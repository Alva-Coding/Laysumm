10.1016/j.engappai.2019.02.015

FULLTEXT

TITLE

Sequential hypothesis tests for streaming data via symbolic time-series analysis

SECTION

Introduction

PARAGRAPH

Recently there has been an unprecedented increase in the volume and speed of temporal data being generated by physical systems, due to the improvements in low-cost sensing and high-speed computation & communication.

Typical machine learning tools, used for monitoring of physical processes, extract features from a fixed number of consecutive observations and pose a supervised learning problem for detection or classification using those features (Bishop, 2006).

Similarly, fixed-sample-size (FSS) tests from the estimation theory literature (Poor, 1994) compute likelihood or posterior probability of an observed sequence of fixed length and select the class which maximizes this statistic.

However, in order to perform well on all feasible sequences, the chosen sample length in these approaches is made longer than needed for most of the easily separable cases.

For example, in time-critical online monitoring systems such as detection of combustion instabilities in aircraft gas-turbine engines, an early detection can enhance mitigation of structural damage in engines by avoiding the thermo-acoustic resonance and may even prevent accidents due to engine shutdown.

A dynamic data-driven approach (Darema, 2004) for sequential detection and classification is needed, which can adapt the observed length of time series without any significant computational burden.

Sequential hypothesis tests offer one such efficient and adaptive framework, which would allow online detection of anomalies, faults, and mode transitions in dynamical systems, where high-speed streaming data are generated.

This paper presents a sequential hypothesis testing procedure for a class of Markov models of temporal data inferred by using symbolic time-series analysis (STSA) (Daw et al., 2003; Beim Graben, 2001).

PARAGRAPH

A sequential detector is a statistical decision function that uses a random number of samples depending on the observation sequence to detect the underlying hypothesis.

A sequential detector, on the average, would need a much smaller sequence length than FSS tests (Poor, 1994).

Sequential probability ratio test (SPRT) (Wald and Wolfowitz, 1948; Shiryaev, 1978) has been traditionally used for binary hypothesis testing and is known to be an optimal detector when the observation sequences are independent and identically distributed (IID) (Burkholder and Wijsman, 1963).

However, sequential data from physical systems are typically not independent as causal relations, because of the underlying physics, may lead to statistical dependencies.

PARAGRAPH

Hidden Markov models (HMMs) (Rabiner, 1989) undergo temporal evolution, which are learned from data using iterative techniques to capture some of the statistical dependencies (Bishop, 2006).

In Grossi and Lops (2008), Chen and Willett (2000), Fuh (2003), the concept of SPRT was extended for data generated from a HMM.

In contrast, symbolic time-series analysis-based Markov modeling makes use of concepts from symbolic dynamics to represent an observation sequence as a discrete Markov model.

This technique consists of two critical steps, namely, discretization (also called partitioning) of the measurement space of a dynamical system (Kennel and Buhl, 2003; Virani et al., 2016) and memory-size estimation (Srivastav, 2014; Jha et al., 2015).

The time-series data are then approximated as a D-Markov model, which is a Markov chain, with finite memory (or order which is denoted as D), over the discrete state-space of the system (Ray, 2004; Mukherjee and Ray, 2014); however, the D-Markov model is different from a hidden Markov model (HMM) in the sense that the state-space of a D-Markov model is always observable.

The structure of a D-Markov model can be inferred by searching for the optimal discretization and the corresponding order of the Markov chain.

Thus, the parameters associated with the model structure are: the size of partitioning set (also called alphabet in symbolic dynamics literature), location of partitioning segments, and the memory of the associated Markov chain.

Once the memory of the discrete symbol sequence is estimated, the state-space of the Markov model is represented by the corresponding collection of memory words (or collections of symbols of length equal to the estimated memory for the discrete sequence).

This paper presents a technique to sequentially estimate the log-posterior ratio (LPR) for a binary hypothesis test, where each hypothesis is represented by a D-Markov model.

PARAGRAPH

Contributions: This paper extends the results of classical SPRT for IID observations to sequential hypothesis tests for observations from D-Markov models of time-series data.

D-Markov modeling for time-series data via STSA is an existing technique (Ray, 2004) with applications in target detection and classification in surveillance (Virani et al., 2013; Mukherjee et al., 2011), prognostics and health monitoring of physical systems (e.g., nuclear plants (Jin et al., 2011) and electronic products (Kumar and Pecht, 2007)), and others.

The novel contribution of this work is to enable the use of D-Markov models for streaming data analysis in detection and classification problems by formulating their real-time sequential estimation and hypothesis testing problem in a Bayesian framework.

In this paper, the probability density of the D-Markov model is represented as a product of categorical distributions; and the parameters of this distribution themselves have a Dirichlet distribution.

Then, using the fact that the Dirichlet distribution is the conjugate prior of the categorical distribution, a sequential update rule is developed for posterior probability ratios of D-Markov models to test a time-series model against an alternate time-series model for binary classification problems.

Expected increment of the log-posterior ratios are explicitly provided under each hypothesis; and it is shown that the sequential tests for the Markov models terminate in finite time with probability one.

The efficacy of the proposed method for online monitoring with streaming data is first illustrated with numerical simulation and then validated on experimental data from a lean-premixed swirl-stabilized combustor apparatus (Kim et al., 2010).

The performance of the sequential tests is also compared with that of a (maximum likelihood classifier) FSS test to demonstrate that the proposed method is capable of making more accurate decisions using fewer observations.

PARAGRAPH

Organization: The paper is organized in seven sections including the present one.

Section 2 provides mathematical background for the rest of paper.

Section 3 presents the problem formulation and explains the difference with respect to the SPRT for IID sequences.

Section 4 explains the proposed approach to address the sequential hypothesis testing problem and shows the theoretical justification for the proposed technique.

Section 5 presents the results and inferences from a simulation example to explain the underlying algorithms.

Section 6 shows the validation results based on experimental data of combustion instability along with a description of the test apparatus.

Section 7 concludes the paper along with recommendations for future research.

SECTION

Mathematical preliminaries

PARAGRAPH

This section introduces mathematical concepts that are used throughout the paper.

Symbolic time-series analysis (STSA) (Beim Graben, 2001; Daw et al., 2003) partitions the measurement space of a dynamical system, where the partitioning is a mapping from a continuous space of measurements to a discrete space of symbols.

In machine learning literature, discretization is generally studied as a feature extraction technique.

In mathematics literature, the partitioning or discretization is characterized by the extent to which a dynamical system can be represented by a symbolic one.

The data are symbolized based on the choice of a partitioning technique and then, the dynamics of the discrete process are studied.

Upon symbolization of statistically stationary (or quasi-stationary) time-series data, the resulting symbol sequences are converted to probabilistic finite state automata (PFSA) for information compression.

While the details are reported in Ray (2004), Mukherjee and Ray (2014), the essential information is presented below for completeness of the paper.

PARAGRAPH

DFSA

PARAGRAPH

A deterministic finite state automaton (DFSA) is a 3-tuple G=(Σ,Q,δ) where:

Σ is a non-empty finite set, called the alphabet, with cardinality |Σ|;

Q is a non-empty finite set, called the set of states, with cardinality |Q|;

δ:Q×Σ→Q is the state transition map.

PARAGRAPH

It is noted that Σ⋆ is a countable collection of all finite-length strings with symbols from the alphabet Σ and the (zero-length) empty string ε.

PARAGRAPH

PFSA

PARAGRAPH

A probabilistic finite state automaton (PFSA) is constructed upon a DFSA G=(Σ,Q,δ) as a pair M=(G,M), that is, the PFSA M is a 4-tuple M=(Σ,Q,δ,M), where:

Σ,Q, and δ are the same as in  Definition 2.1;

M:Q×Σ→[0,1] is the morph function that satisfies the condition ∑s∈Σm(q,s)=1∀q∈Q.

In matrix form M is called the morph matrix and its entries mij denotes the probability of emitting a symbol sj∈Σ from the state qi∈Q.

PARAGRAPH

A PFSA is viewed as a generative model capable of probabilistic generation of a symbol string through state transitions; it has found many applications ranging from pattern recognition and machine learning to computational linguistics (Vidal et al., 2005).

For symbolic analysis of time-series data, a class of PFSAs called the D-Markov machine has been proposed in Ray (2004) as a sub-optimal but computationally efficient approach for encoding the dynamics of symbol sequences as a finite state automaton.

The main assumption, which is the reason for sub-optimality, is that the symbolic process can be approximated as a Dth-order Markov chain.

The assumption of finite memory (or order) is reasonable for stable and controlled engineering systems that tend to eventually forget their initial conditions.

The states of this PFSA are words of length D (or less) over an alphabet Σ; and state transitions are described by a sliding block code of memory D and anticipation length of one or higher (Lind and Marcus, 1995).

PARAGRAPH

D-Markov Machine (Ray, 2004; Mukherjee and Ray, 2014)

PARAGRAPH

A D-Markov machine is a statistically stationary stochastic process S=⋯s−1s0s1⋯, where the probability of occurrence of a new symbol depends only on the last D symbols, that is, P(sn∣⋯sn−D⋯sn−1)=P(sn∣sn−D⋯sn−1)where D is called the depth of the Markov machine.

PARAGRAPH

A D-Markov machine is thus a Dth-order Markov approximation of the discrete symbolic process.

It is also noted that the presented formalism for statistical learning is different from the standard hidden Markov models in the following sense:

PARAGRAPH

Although the framework of a D-Markov machine is restrictive due to the constraint of (finitely many) observable states, it can be conveniently used for online in-situ monitoring applications due to the simplicity of the inference algorithms.

For finite-order and finite-state Markov chains, the conditional symbol emission probabilities and the initial state summarize all of the relevant information supplied by an appropriate sample (i.e., time series) (Laurence, 1993).

Under stationarity assumptions, the initial state becomes unnecessary and thus, the sufficient statistic is provided by the emission probabilities given by the morph matrix.

PARAGRAPH

In information theory, Kullback–Leibler divergence (or K–L divergence) between two probability mass functions (Cover and Thomas, 1991) is defined as: dKL(p(q)∥p̄(q))=∑q∈Qp(q)log(p(q)p̄(q)).

PARAGRAPH

In this paper, a measure of difference between two morph matrices, which are conditional distributions, is obtained in terms of the conditional relative entropy (Cover and Thomas, 1991) as: d(p(s|q)∥p̄(s|q))=∑q∈Qp(q)∑s∈Σp(s|q)log(p(s|q)p̄(s|q)).

SECTION

Problem formulation

PARAGRAPH

Sequential detectors for independent and identically distributed (IID) observations to infer which of the known probability densities is generating the data have been well-studied in literature (Poor, 1994).

This section first addresses the sequential detection problem for discrete-valued IID observations and then extends it to the case in which the observation sequence is generated by a D-Markov model.

SECTION

Sequential detection: IID Case

PARAGRAPH

The discrete-valued IID observations {St;t=1,2,…} are tested according to the following binary hypotheses: H0:St∼P0,t=1,2,…versusH1:St∼P1,t=1,2,…, where P0 and P1 are two known probability distributions (or measures) on the measurable space (Σ,2Σ).

Here, Σ is a finite set of symbols, each symbol s∈Σ is a discrete observation, and 2Σ denotes the power set of Σ, which consists of all possible subsets of Σ including the empty set and the set Σ itself.

PARAGRAPH

A sequential decision rule is formulated as a pair of (time-dependent) sequences (ϕ̄,μ̄).

In this setting, ϕ̄≜{ϕj:j=0,1,2,…} is called a stopping rule with ϕj:Σj→{0,1} and μ̄≜{μj:j=0,1,2,…} is called a terminal decision rule with μj:Σj→X; for binary hypothesis (i.e., either H0 or H1) testing, X is represented as {0,1}.

For an observed symbol string of finite length St={si∈Σ:i=1,2,…,t} at time t, either the stopping rule is executed if ϕt(St)=1) to make a decision, or sampling is continued if ϕt(St)=0) to observe the next symbol st+1.

If the decision is to stop sampling (i.e., ϕt(St)=1), then the terminal decision rule selects the hypothesis for St, which yields the decision μt(St)∈X.

SECTION

Sequential detection: D-Markov case

PARAGRAPH

In the hypothesis testing problem under consideration, the observations may not necessarily be IID as they are often generated from a physical process.

Therefore, a D-Markov model is assigned corresponding to each hypothesis, instead of a probability distribution.

As stated earlier in Section 2, these D-Markov models are represented as Mk=(Q,Σ,δ,Mk) for each hypothesis Hk with k∈X, where Q represents the set of states, Σ is the alphabet set, δ:Q×Σ→Q is the transition function, and Mk is the class-specific morph matrix, which consists of entries [Mk]ij=mijk=pk(sj|qi).

Each row of the matrix Mk represents the probability of emitting a symbol sj∈Σ from a particular state qi∈Q in the model k∈X.

Thus, each row of the morph matrix is a categorical distribution (Papoulis and Pillai, 2002).

PARAGRAPH

It is noted that all of these models have a similar algebraic structure and they differ only in the symbol emission probabilities.

It is assumed that the training instances for nominal and anomalous conditions have been obtained as sufficiently long symbol sequences, which allows the convergence of the morph matrix parameters in the training phase (Meyn and Tweedie, 2012).

However, in the operation phase, it is necessary to make accurate decisions with (possibly) fewer streaming observations, because early detection (or prediction using precursors) of anomalies/faults and instabilities are crucial for taking corrective actions in the physical process.

PARAGRAPH

Let a Markov model over the state-symbol pair, which generates the symbol sequence, be given as: p(qt+1,st+1∣qt,st)=p(st+1∣qt,st,qt+1)p(qt+1∣qt,st)=p(st+1∣qt+1)p(qt+1∣qt,st) where p(qt+1∣qt,st) is obtained from the transition function δ as: p(qt+1∣qt,st)=1if δ(qt,st)=qt+10otherwise

PARAGRAPH

In Eq. (3), p(st+1∣qt+1) is obtained from the morph matrix M. Since, the state transition is deterministic (see Definition 2.1), the emission of a symbol sequence from hypothesis Hk is governed only by the morph matrix Mk[i,j]=[pk(st+1=j∣qt+1=i)].

Thus, the hypothesis test is given as follows:

PARAGRAPH

The discrete-valued observation sequence St={si∈Σ:i=1,2,…,t} is generated according to H0:St∼M0,t=1,2,…versusH1:St∼M1,t=1,2,…, where Mk is the morph matrix of the PFSA model Mk for k∈{0,1}.

It is noted that, in a D-Markov model, each state is represented by a finite history of at most D symbols, thus after observing the first at most D symbols, the initial state will be known.

Hence, the sequential detector is initiated after at most D symbols have been observed.

The sequential detector in this case is also defined as the pair of sequences of stopping rule and terminal decision rule as discussed in Section 3.1.

SECTION

Problem statement

PARAGRAPH

In this paper, a sequential detector is constructed to identify the D-Markov model that generates symbol sequences, as shown in the hypothesis test in Section 3.2.

Given these models, the problem is to execute the following tasks:

SECTION

Technical approach

PARAGRAPH

This section first summarizes a time-series modeling method by using symbolic analysis along with the process for hyper-parameter estimation and a Bayesian approach for parameter estimation of these time-series models.

The posterior distribution of the PFSA model Mk, given the observed symbol sequence St, is then computed.

Subsequently, the sequential decision rule is developed for a binary hypothesis testing problem.

A summary of the proposed approach during training and during operation with streaming data is also provided.

Some analysis is shown to study the evolution of log-posterior ratio (LPR) statistic and to obtain estimates of stopping time and performance bounds.

SECTION

Time-series modeling and hyper-parameter estimation

PARAGRAPH

Symbolic time-series analysis (STSA) has been adopted as a tool for time-series modeling, which consists of two critical steps: (i) discretization, where the continuous attributes of the sequential data are projected onto a symbolic space, which is followed by (ii) identification of concise probabilistic patterns to compress the information embedded in the discretized symbol sequences.

Specifically, a finite-memory Markov model is built upon the memory estimate of the symbol sequence, which is represented by a state transition matrix.

This leads to identification of the causal dynamical structure, which is intrinsic to the symbolic process under consideration; such a dynamical structure is suitable for feature extraction and pattern classification.

PARAGRAPH

The hyper-parameters for model inference under the current class of Markov models include: (i) alphabet size and location of partition boundaries for discretization (i.e., symbolization) of the continuous real-valued signals, and (ii) memory of the corresponding discrete data.

Symbolization is carried out via partitioning of the phase-space of the system within which the system dynamics evolves.

In general, there are two main lines of thought behind the symbolization process, one inspired by dynamical systems theory and the other inspired by machine learning objectives.

A brief formal introduction of discretization is presented next.

PARAGRAPH

Let the time-series data be denoted as the sequence {yt}t∈N where yt∈Y⊆R and let Φ represent the partitioning function such that Φ:yt↦st where st∈Σ for all t∈N and the alphabet size ∣Σ∣∈N is known and fixed (see Fig. 1).

Then the partitioning function, which is determined by the set RΦ={R1,…,R|Σ|}, has mutually exclusive and exhaustive segments Ri¯,i=1,…,|Σ| (i.e., Y=R1¯∪⋯∪R|Σ|¯ and Ri¯∩Rj¯=0̸∀i≠j).

The dynamics of the discrete system is governed by the function Φ and thus, to find a useful discrete stochastic system, it is important to find a good partitioning function.

Some approaches inspired by the dynamical systems theory could be found in Buhl and Kennel (2005), Adler (1998), Kennel and Buhl (2003).

Several other partitioning methods have been proposed in literature such as maximum entropy partition (MEP) (Rajagopalan and Ray, 2006), symbolic aggregate approximation (SAX) (Lin et al., 2007), maximally-bijective partition (Sarkar et al., 2013), and sparse density estimation-based partition (Virani et al., 2016).

In general, most of the machine learning applications use a cross-validation-based approach to select the partitioning size and boundaries.

This paper has used (unsupervised) maximum entropy partitioning (MEP) (Rajagopalan and Ray, 2006), where approximately equal number of points are assigned to each of the partitioning segments; thus, it leads to a unique solution for a one-dimensional time-series.

For its simplicity, MEP is the most widely used partitioning technique.

For a detailed discussion on partitioning techniques, interested readers are referred to Jha (2016).

PARAGRAPH

Working in the symbolic domain, the task is to identify concise probabilistic models that relate the past, present and the future states of the system under consideration.

For Markov modeling of the symbol sequence, this is achieved by first estimating the depth (or size of memory) for the discrete symbolic process and then, estimating the approximate stochastic model from the observed sequence.

Various approaches have been reported in literature for order estimation of Markov chains (Jha, 2016; Jha et al., 2018).

For machine learning applications, the estimation process follows the wrapper approach (Bishop, 2006), where a search algorithm with a certain stopping criterion calls the main modeling module to build several temporal models with varying depths; and the search is stopped when the stopping criterion (e.g., information gain or entropy rate) show marginal improvement for the added complexity.

This paper does not address optimization of the depth of the underlying models — it is assumed to be known.

Interested readers are referred to Jha et al. (2015, 2018) for a detailed discussion on estimation of depth for D-Markov models.

For completeness of the paper, a recent and computation-efficient algorithm based on spectral properties of the Markov model has been provided in Appendix.

PARAGRAPH

The work presented in this paper is independent of the choice of hyper-parameters of the model; in other words, the presented results hold true for all choices of hyper-parameters.

Once these hyper-parameters are estimated, the time-series data are represented by a stochastic matrix of the inferred Markov chain.

The stochastic matrix is estimated in a sequential fashion by a Bayesian approach with conjugate priors as explained in the next section.

SECTION

Bayesian approach for parameter estimation

PARAGRAPH

The D-Markov model for each hypothesis is represented by a morph matrix, as discussed in Section 2.

Each row of the morph matrix of a D-Markov model is a discrete probability mass function denoting the probability of emission of a symbol from a given state.

Given the states, the rows of a morph matrix represent a set of independent categorical distributions (Papoulis and Pillai, 2002) over the alphabet set.

It is known that the Dirichlet distribution is the conjugate prior of the categorical distribution (Bishop, 2006), i.e., if the prior density over parameters of the categorical distribution is represented as a Dirichlet distribution, then the posterior density after a Bayesian update with a new observation follows a Dirichlet distribution too.

Thus, for a given state, the density over parameters of a particular row of the morph matrix is given by Dirichlet distribution (Wen et al., 2013).

PARAGRAPH

For any t∈N, let St denote the symbol sequence (s1,s2,…,st) of length t.

The posterior distribution of the model parameters of Mk, given an observed symbol sequence St, is computed as: P(Mk∣St)=∏i=1|Q|(∏j=1|Σ|(pijk)αijt−1B(ᾱit))where the hyper-parameter αijt is initialized at αij0; the value of (αijt−αij0) is the count of occurrences of the symbol sj at state qi in a symbol sequence St; the vector ᾱit represents [αi1t,αi2t,…,αi|Σ|t] for all i∈{1,2,…,|Q|}; the scalar B(ᾱit)=∏j=1|Σ|Γ(αijt)Γ(∑j=1|Σ|αijt); and Γ(⋅) is the standard Gamma function.

Using the posterior distribution and given St, the expected value of the morph map for the PFSA M is: mt(qi,sj)=mijt≜αijt∑ℓ=1|Σ|αiℓt=αij0+nijt∑ℓ=1|Σ|αiℓ0+∑ℓ=1|Σ|niℓtwhere nijt is the count of occurrence of symbol sj at state qi in a sequence St; and αij0 is the initial value of the hyper-parameter αijt of the Dirichlet distribution.

During parameter estimation in training phase, the following observations are made:

PARAGRAPH

This parameter estimation process is repeated to obtain a model for each of the hypotheses.

One may also create reduced-order Markov models with fewer states for each hypothesis which can lead to faster training and potentially quicker termination of sequential test by using the tools prescribed in Jha et al. (2018).

In the next part, these models are used to obtain a test statistic and their sequential update rules for use in the testing phase.

SECTION

Initialization and sequential update during testing

PARAGRAPH

In binary hypothesis testing, such as Bayes or Neyman–Pearson hypothesis testing, the log-likelihood ratio is computed from observed data and compared with a predetermined threshold to choose a likely hypothesis (Poor, 1994).

The proposed method uses Eq. (4) to compute the log-posterior ratio for as follows: Λt=log(P(M1∣St)P(M0∣St))=log(∏i=1|Q|∏j=1|Σ|(mij1)αijt−1∏i=1|Q|∏j=1|Σ|(mij0)αijt−1)=log(∏i=1|Q|∏j=1|Σ|(mij1mij0)αijt−1)=∑i=1|Q|∑j=1|Σ|(αijt−1)log(mij1mij0),Λt=∑i=1|Q|∑j=1|Σ|wij(αijt−1), where the constant wij denotes log(mij1mij0).

The intuition behind this statistic is as follows:

PARAGRAPH

In the absence of any initial count of emissions of symbols from states, the hyper-parameters αijt in Eq. (7) are initialized to be 1, i.e., αij0=1∀i,j.

This is consistent with the uniform prior assumption to obtain an estimate of the Markov model parameters as discussed in Section 4.2.

However, if prior observations of the number, Nij0, of emissions of symbol sj from state qi are available before the start of the hypothesis test, then the initial value is assigned as αij0=Nij0+1.

Specifically, in nested binary classification problems for anomaly/fault detection and then anomaly/fault classification, the Bayesian formulation allows to use observation counts from the first hypothesis test in the Dirichlet priors during initialization of the second test for faster classification.

The fact that the Dirichlet class allows usage of non-informative priors as well as previous symbol counts to impose priors in a straightforward way is another reason for adopting the Bayesian approach.

PARAGRAPH

Alternatively, one may derive an expression for log-likelihood ratio using the D-Markov models and choose to use log-likelihood ratio as the test statistic too.

The log-likelihood ratio statistic is derived next.

PARAGRAPH

Let St for t∈N denote the symbol sequence (s1,s2,…,st) of length t in the testing phase.

The likelihood of an observed symbol sequence St for a given model Mk is computed as: P(St∣Mk)=mq0sD+1kmq1sD+2k…mqt−Dstk=∏i=1|Q|∏j=1|Σ|(mijk)nijtwhere the state qi represents the D-length word (si+1si+2…si+D), the value of nijt is the count of occurrences of symbol sj at state qi in a sequence St.

Similar to the derivation of Eq. (7) from Eq. (4), a log-likelihood ratio is derived from Eq. (8) as: Λ˜t=∑i=1|Q|∑j=1|Σ|wijnijt.

PARAGRAPH

A simple relation to compute the log-posterior ratio was obtained in Eq. (7).

Its relationship to log-likelihood ratio from Eq. (9) will be shown ahead.

It follows from Eq. (7), the relationship αijt=αij0+nijt, and Eq. (9) that: Λt=∑i=1|Q|∑j=1|Σ|wij(αij0+nijt−1)=∑i=1|Q|∑j=1|Σ|wijnijt+∑i=1|Q|∑j=1|Σ|wij(αij0−1)=Λ˜t+∑i=1|Q|∑j=1|Σ|wij(αij0−1).

PARAGRAPH

The above relationship highlights that, without any informative prior, where αij0=1, the statistic log-likelihood ratio and log-posterior ratio are identical and initialize at zero.

However, any prior symbol occurrence counts can be used to bias the test statistic initially by assigning a non-zero value.

The sequential update rule for log-posterior ratio is formulated next.

PARAGRAPH

Sequential Update

PARAGRAPH

Given that the log-posterior ratio at time t is Λt and the state at time t+1 is qt+1, ifst+1 is the emitted symbol, then the updated log-posterior ratio Λt+1 is given by : Λt+1=Λt+wqt+1st+1,where wqt+1st+1={wij:qt+1=i and st+1=j}.

PARAGRAPH

PARAGRAPH

Recalling the hyper-parameter αijt+1−αij0 to be the count of occurrence of symbol sj at the state qi in the observed symbol sequence St+1, it follows that: αijt+1=αijt+1{i}(qt+1)1{j}(st+1),where 1A(⋅) is the indicator function with set A, that is, 1A(x)=1, if x belongs to A; otherwise, 1A(x)=0.

Then, by using the log-posterior ratio given in Eq. (7) for time t+1, and Eq. (12), it follows that: Λt+1=∑i=1|Q|∑j=1|Σ|wij(αijt+1{i}(qt+1)1{j}(st+1)−1)=∑i=1|Q|∑j=1|Σ|wij(αijt−1)+∑i=1|Q|∑j=1|Σ|wij1{i}(qt+1)1{j}(st+1)=Λt+wqt+1st+1.□

PARAGRAPH

The simplicity of sequential update of the log-posterior ratio for D-Markov models enables online update of the statistic Λt(St).

The time complexity of this sequential update is O(log(|Σ|)) as explained below.

PARAGRAPH

Using binary search, the new measurement is assigned a symbol st+1 by efficiently identifying its location in a sorted sequence of real numbers.

This sorted sequence represents a partition of the measurement range.

The time complexity of symbolization via binary search is known to be O(log(|Σ|)).

Given the symbol st+1 and previous state qt+1, the relation wqt+1st+1=wij1{i}(qt+1)1{j}(st+1) implies that wqt+1st+1 can be obtained directly from a look-up table and added to previous statistic Λt in constant time.

Thus, time complexity of sequential update is given by O(log(|Σ|)).

PARAGRAPH

Since alphabet size |Σ| is typically small, the time complexity of update shows that the approach is feasible for real-time monitoring of physical systems.

It is noted that, under the current framework of Dirichlet distribution for the class of D-Markov models, the expression for likelihood ratio computation as well as its update is significantly simplified when compared to the same for hidden Markov models as proposed in Fuh (2003).

This simplification is attributed to the perfect information of the states being observed in the current class of models.

In the next part, the computed log-posterior ratio is used to obtain a sequential decision rule.

SECTION

Sequential hypothesis test for D-Markov models

PARAGRAPH

As mentioned in Section 3.2, a sequential decision rule is a pair of sequences (ϕ̄,μ̄), where ϕ̄≜{ϕj:j=0,1,2,…} is called a stopping rule and μ̄≜{μj:j=0,1,2,…} is called a terminal decision rule.

By choosing two thresholds γ0 and γ1 with γ0<0<γ1, the sequential hypothesis test SHT(γ0,γ1) is constructed from the sequence of stopping rule ϕj and terminal decision rule μj as follows: ϕj(Sj)=0;ifγ0<Λj(Sj)<γ1,1;otherwise.μj(Sj)=0;ifΛj(Sj)≤γ0,1;ifΛj(Sj)≥γ1.

PARAGRAPH

Let pd be the probability of detection and pfa be the probability of false alarm of the detector.

In order to choose the thresholds, the following relations from Wald’s SPRT (Poor, 1994) are used: γ1≤log(pdpfa) and γ0≥log(1−pd1−pfa).These relations hold even without the assumption of independence (Grossi and Lops, 2008) and the underlying inequalities not only relate the performance with the chosen thresholds but also help to choose the threshold to guarantee a particular desired performance.

SECTION

Summary of the proposed approach

PARAGRAPH

The proposed approach is summarized in this subsection and a schematic overview is provided in Fig. 1.

In the training phase, the pre-recorded time-series data are used to learn the corresponding D-Markov models.

In this learning process, the model hyper-parameters (i.e., alphabet size, boundary locations of partition segments, and depth) are estimated using the techniques describes in Section 4.1.

The parameters of the time-series model, i.e. the entries of the morph matrix, are then obtained using a Bayesian approach given in Section 4.2.

The maximum a posteriori probability (MAP) estimates of the model parameters for each hypothesis are then used to compute a weight matrix W=[wij], where wij≜log(mij1)∕log(mij0), which is used in the sequential update step during operation.

Finally, the user-defined performance is used to obtain thresholds for the sequential test.

PARAGRAPH

During operation with streaming data at initialization of the test, the log-posterior ratio is initialized using suitable hyper-parameters αij0 for all i∈Q and j∈Σ.

Based on the first D symbols, the state qD+1 is initialized to be (s1s2…sD).

After initialization, each observation from the data acquisition system is symbolized, used to obtain next state, and also update the log-posterior ratio statistic using Eq. (11).

The statistic is then compared with the predefined thresholds in the stopping rule and terminal decision rule to either declare the hypothesis or continue sampling.

These computations enable online monitoring and hypothesis testing with streaming data.

PARAGRAPH

Fig. 1 summarizes the process during training and during operation.

In the next part of this section, the sequential detector is analyzed in detail and certain convergence and test completion results have been derived.

SECTION

Analysis of the sequential detector

PARAGRAPH

The sequential detector SHT(γ0,γ1) has been shown in Eq. (13).

This part explores the stochastic evolution and asymptotic behavior of the log-posterior ratio (LPR) statistic for D-Markov models as well as prove that the sequential detector will terminate in finite time with probability one.

PARAGRAPH

The sequential update rule for LPR in Eq. (11) suggests that its stochastic evolution has some resemblance with a random walk on the real line (Papoulis and Pillai, 2002).

This stochastic evolution is characterized in the following Proposition.

PARAGRAPH

Random Walk on a DFSA

Random Walk on a DFSA

PARAGRAPH

Given that DFSA G is the common algebraic structure in both models M0 and M1, andSt is a (finite-length) symbol sequence over the alphabet Σ, then the stochastic evolution of the log-posterior ratio (LPR)Λt=log(P(M1∣St)P(M0∣St)), is given by a weighted random walk on the DFSA G.

PARAGRAPH

PARAGRAPH

Using Eq. (11), it follows that: Λt+2=Λt+1+wqt+2st+2,=Λt+wqt+1st+1+wqt+2st+2. Given qt+1 and st+1, the next state qt+2=δ(qt+1,st+1), thus, qt+2 is not random and is governed by the algebraic structure of the DFSA.

The symbol emission probabilities are obtained from the morph matrix of true model, given the current state.

The step size wqtst at time step t can take |Q|||Σ| different values depending on the state and emitted symbol pair.

Thus, the evolution of LPR is a weighted random walk over a DFSA.

□

PARAGRAPH

Given the stochastic evolution of the LPR statistic from Proposition 4.2, the expected value of the LPR statistic is presented in the following theorem.

PARAGRAPH

Expected Value of Log-Posterior Ratio (LPR)

Expected Value of Log-Posterior Ratio (LPR)

PARAGRAPH

Given that pk(q1) is a row vector of initial state probabilities, Tk is the state transition probability matrix and Mk is the morph matrix for model Mk, and W=[wij]=[log(mij1mij0)], then the expectation (over all possible symbol sequences) of the log-posterior ratio (LPR) statistic aftert updates is given as: E[Λt∣Mk]=∑l=1tpk(q1)Tkl−1(Mk⊙W)1|Σ|,where ⊙ is element-wise multiplication operator and 1n is column vector of n ones.

PARAGRAPH

PARAGRAPH

Using Λt+1=Λt+wqt+1st+1 from Eq. (11), it follows that: E[Λt+1−Λt∣Mk]=E[wst+1qt+1∣Mk]=E[∑j=1|Q|∑i=1|Σ|wij1{i}(qt+1)1{j}(st+1)∣Mk]=∑j=1|Q|∑i=1|Σ|wijpk(qt+1=j)pk(st+1=i|qt+1=j)=∑j=1|Q|pk(qt+1=j)∑i=1|Σ|wijpk(st+1=i|qt+1=j)=pk(qt+1)(Mk⊙W)1|Σ|. Using pk(qt+1)=pk(q1)Tkt, it follows that: E[Λt+1−Λt∣Mk]=pk(q1)Tkt(Mk⊙W)1|Σ|.Thus, the expression for the expected increment in LPR is derived after t updates.

Finally, adding up the expected increments, the desired result is obtained for the expected LPR as: E[Λt∣Mk]=∑l=1tpk(q1)Tkl−1(Mk⊙W)1|Σ|.□

PARAGRAPH

The information-theoretic interpretation of the result in Theorem 4.3 is explained by the following remark.

PARAGRAPH

Information-Theoretic Interpretation

PARAGRAPH

Using Eq. (16) and the relation for conditional relative entropy Eq. (2), it follows that: E[Λt+1−Λt∣M1]=p1(qt+1)(M1⊙W)1|Σ|=∑j=1|Q|∑i=1|Σ|p1(qt+1=j)mij1log(mij1mij0)=Ep1(st+1,qt+1)[log(p1(st+1∣qt+1)p0(st+1∣qt+1))]=dt+1(p1(st+1∣qt+1)∥p0(st+1∣qt+1))=dt+1(M1∥M0). Similarly, using non-negativity of conditional relative entropy, it follows that: E[Λt+1−Λt∣M0]=−dt+1(M0∥M1).Thus, one may infer that expected increment in log-posterior ratio at time t+1 is the conditional relative entropy between the given models at time t+1, where time dependence is introduced because of state probability vector p1(qt+1).

Thus, considering conditional relative entropy to be a distance function, the expected increment is the statistical distance between the two different models.

Moreover, the derived relationship shows that the expected value of the statistic at time t is E[Λt∣Mk]=(−1)1−k∑l=1tdl(Mk∥M1−k)Here the magnitude of the expected value of the statistic is directly proportional to the distance between the models.

Thus, this result implies that models which are closer will need more incremental updates to cross a prescribed threshold.

In other words, models which are closer will need larger number of observations to terminate the sequential hypothesis test for a given desired performance.

PARAGRAPH

There are two consequences of the result in this remark:

The expected increment and the rate of sequential update depends directly on the statistical distance between models of the hypotheses.

Hence, the speed of sequential update cannot be tuned with any parameter.

However, an alternative approach to create reduced-order Markov models (Jha et al., 2018) of time-series data with smaller state set can be employed.

It can provide sufficiently accurate models that maybe further apart and yield high expected increment in test statistic leading to faster termination of the test.

The work in Beim Graben (2001) showed that symbolic dynamics approach shows good robustness to measurement noise as additive noise leads to uncertainty in the assigned symbol for a measurement, which are near the partition boundaries only.

The noise statistics observed in the training data sequence are already captured in the Markov models.

Addition of noise in data for each hypothesis leads to reduction in conditional relative entropy between models giving shorter expected increments.

Thus, noisy signals will lead to slower termination of test to guarantee similar performance.

PARAGRAPH

The results from Theorem 4.3 are used in next two remarks to derive the asymptotic behavior of the test statistic and to obtain insights on special case where the D-Markov models are stationary.

PARAGRAPH

Asymptotic Behavior of Log-Posterior Ratio

Asymptotic Behavior of Log-Posterior Ratio

PARAGRAPH

Using the result from Theorem 4.3 and computing the limit as number of observations tend to infinity, it follows that: limt→∞E[Λt∣Mk]t=π̃k(Mk⊙W)1|Σ|where π̃k=limt→∞(1t∑l=1tpk(q1)Tkl−1) is the Césaro limit.

The constant value on the right hand side of this result implies that the growth of log-posterior ratio becomes linear asymptotically.

PARAGRAPH

Stationary Markov Models

PARAGRAPH

Let two Markov models M0 and M1 be stationary and let their stationary distributions be given as π0 and π1, respectively.

Thus, the state probability vector is the same as the stationary distribution of the Markov model at all time epochs, i.e., pk(qt)=πk for all t∈N and k=0,1.

Using πk in Eq. (16), it follows that: E[Λt+1−Λt∣Mk]=πk(Mk⊙W)1|Σ|,which has a constant value.

Hence, the expectation under model Mk is given by: E[Λt∣Mk]=tπk(Mk⊙W)1|Σ|.Since, it is known that the Césaro limit is equal to the stationary distribution for irreducible aperiodic Markov models (Meyn and Tweedie, 2012), this result matches the intuition from Remark 4.5 with π̃k=πk.

Using the interpretation from Remark 4.4, it follows that: E[Λt∣Mk]=(−1)1−ktd(Mk∥M1−k),for k=0,1.

This relation for the stationary case shows a direct relationship between the expected value of log-posterior ratio and the distance between the models.

SECTION

Completion of the test and estimation of the sequence length

PARAGRAPH

This subsection proves that the sequential hypothesis test for D-Markov models terminates in finite time with probability one and also provides an estimate of the average observation sequence length.

Similar to the methodology in Grossi and Lops (2008), the notion of stopping time under each hypothesis for a symbol sequence St is given as follows: τ0(St)=inf{t∈N:Λt(St)≤γ0},τ1(St)=inf{t∈N:Λt(St)≥γ1}. Then, the stopping time for the sequential test is given by τ(St)=min{τ0(St),τ1(St)}.

The formal result for the guarantee of completion of the sequential test in finite time is given below as Theorem 4.7.

PARAGRAPH

Completion of the Test

Completion of the Test

PARAGRAPH

If the discrete finite-order Markov process generating the symbols is ergodic, then the sequential test with decision rule shown in Section 4.4 terminates in finite time almost surely under both hypothesis, i.e., P({τ(St)<+∞}|Mk)=1, for k=0,1.

PARAGRAPH

PARAGRAPH

Ergodicity of the symbol sequence generated by a Markov model implies that the sequence of log-posterior ratio (LPR) is also ergodic for the model, because the LPR is a deterministic function of the stochastic sequence.

Thus, the sequence of time averages of the LPR converges to the ensemble average (i.e., expected value) of the LPR almost surely, that is, limt→∞E[Λt∣Mk]t=aslimt→∞1tΛt(St)under hypothesis k∈X.

Using Eq. (17) for k=1, it follows that: limt→∞1tΛt(St)=asπ̃1(M1⊙W)1|Σ|The constant value in this result implies that asymptotic growth rate of test statistic sequence is linear.

Thus, eventually the statistic will be greater than any finite threshold, i.e., P({limt→∞Λt(St)>γ1}|M1)=1.

Similarly, for k=0, it follows that: P({limt→∞Λt(St)<γ0}|M0)=1Using Eq. (21) for k=0,1, it follows that: P({τk(St)<+∞}|Mk)=1The proof follows from τ(St)=min{τ0(St),τ1(St)}.

□

PARAGRAPH

It follows from Theorem 4.7 that the sequential tests for D-Markov machines terminate in finite time with probability one.

Now an estimate of this stopping time is obtained with respect to the expected behavior of the LPR sequence.

For each hypothesis, this sequence length estimate is defined by using E[Λt∣Mk] from Eq. (15) as: τ˜1≜inft∈Nt:∑l=1tp1(q1)T1l−1(M1⊙W)1|Σ|≥γ1,τ˜0≜inft∈Nt:∑l=1tp0(q1)T0l−1(M0⊙W)1|Σ|≤γ0.

PARAGRAPH

The utility of these estimates and other theoretical results from this section are elucidated by numerical simulation and validation with experimental data in the next two sections.

SECTION

Numerical simulation

PARAGRAPH

A theoretical framework for sequential hypothesis testing with D-Markov models has been developed in the previous sections.

This section presents the results of numerical simulation to elucidate the underlying principles of the proposed sequential hypothesis testing procedure.

SECTION

Description of the simulation scenarios

PARAGRAPH

An anomaly detection scenario is simulated by two D-Markov models, having a common binary alphabet set (i.e., Σ={0,1}) and the same depth D=2.

These models have been trained with sufficiently long data.

The first model, M0, represents the nominal behavior and the second model, M1, represents the anomalous behavior.

The state set Q for each model is given by the words {00,01,10,11}.

The simulation has been performed for two different cases: Case 1, where models M0 and M1 are largely similar; and Case 2, where models M0 and M1 are reasonably different.

The morph matrices for the models from both the cases are shown in Table 1.

The distance between the models, in terms of conditional relative entropy, using the stationary distribution for the state probabilities, is also given in Table 1.

It is verified that Case 1 models are closer than the models in Case 2.

PARAGRAPH

Each model simulates 2500 symbol sequences of length 1000 and uses the sequential detector algorithms from Section 4.4 to make decisions.

The results from this analysis are discussed next.

SECTION

PARAGRAPH

Simulation results

PARAGRAPH

Simulated symbol sequences have been used to sequentially estimate the log-posterior ratio (LPR) statistic.

In order to visualize the behavior of the LPR statistics under different hypotheses, 1000 LPR trajectories are shown under each hypothesis for both cases in Fig. 2.

The expected value of the LPR is computed using the derived equation Eq. (15) and it is also shown in Fig. 2.

It can be qualitatively verified that the LPR trajectories under the two hypothesis begin to differ as more observations are used in both cases.

Moreover, if the models are further apart (as in Case 2), then the distinction between the LPR trajectories is discernible using fewer observations.

The models in Case 1 have identical emission probabilities for 3 out of 4 states (see Table 1), thus the discriminatory information is only available when the model is in state 4.

Hence, in Fig. 2(a), it is seen that the LPR value remains constant for several consecutive observations.

PARAGRAPH

In Remark 4.5, the asymptotic value of the average increment of expected value LPR has been derived under each hypothesis.

Fig. 3 qualitatively verifies the convergence of the time average of increment in expected LPR to the limit computed in Remark 4.5 under both hypotheses for Case 2.

PARAGRAPH

An estimate of the sequence length is given in Eq. (24), which is shown to be a reasonable estimate of the average sample length; this is seen in Fig. 4 that compares the estimated length with the average sample length by repeating the simulation for 40 different choices of probability of detection between 0.8 and 0.999, while keeping probability of false alarm (pfa) fixed at 0.001.

Fig. 4 also verifies the intuition that more observations are needed to achieve better performance.

PARAGRAPH

Fig. 5 shows that the histograms of stopping length under H0 and that the histograms of stopping length under H1 for Case 1 of the simulation models.

From Remark 4.4, it follows that magnitude of expected increment in LPR, under a given hypothesis, say Hk, is equal to the distance of the true model from the alternate model, i.e., d(Mk∥M1−k).

Thus, if d(M1∥M0)>d(M0∥M1) (see Table 1 for Case 1), then on average it is expected the sequential test to terminate with fewer observations under hypothesis H1.

This can be verified in Fig. 5, where the distribution for H1 is to the left of H0.

It is noted that the empirical distribution is not symmetric, thus, the characterization of the stopping time for the test requires further investigation by estimating the higher moments of the respective distributions (Grossi and Lops, 2008).

PARAGRAPH

The efficacy of a sequential detector is characterized by the average sample length and the detection performance.

The desired performance in terms of probability of detection and false alarm is used to design the sequential detector, as shown in Section 4.4.

In Fig. 6, it is seen that the probability of detection of the sequential detector is better than the desired pd and the probability of false alarm was also lesser than 0.001.

Thus, the claim that sequential detector with D-Markov models achieves the desired performance has been verified with simulation.

PARAGRAPH

The performance of the proposed sequential approach is compared with the maximum likelihood classifier, which is a fixed-sample-size (FSS) test.

The classification rule for a symbol sequence SN of length N is given in terms of the likelihood as follows: x∗(SN)=argmaxx∈Xp(SN|Mx).

The comparison of performance of the classifier with the developed approach for different probability of detection (pd) is shown in Table 2.

It is inferred that, for the same pd, on average the FSS test needs several more observations as compared to the proposed sequential detector.

It is noted that the desired pfa is chosen to be 0.001 for the SHT and the observed pfa is 0.0004, which is lower than all cases of ML classifier.

Thus, better performance is achieved with lesser number of observations (on average) with the proposed sequential detector.

PARAGRAPH

In summary, the inferences from the results of numerical simulation are given as follows:

SECTION

Validation on a laboratory apparatus

PARAGRAPH

This section describes and analyzes the data used for detection of unstable behavior in lean pre-mixed combustion for validation on a laboratory apparatus, where thermo-acoustic instabilities are a consequence of nonlinear coupling between the acoustic and thermal oscillations during combustion and have undesirable effects on durability of structural components as well as performance of gas-turbine engines.

The dynamics of the thermo-acoustic phenomena are very fast and the underlying physics is still not completely understood (O’Connor et al., 2015; Jha et al., 2016).

In this context, the detection system should be capable of identifying the onset of thermo-acoustic instabilities as early as possible with negligible false alarm rates.

SECTION

Experimental apparatus and data

PARAGRAPH

This subsection very briefly discusses the data collected for detection of thermo-acoustic instabilities in a laboratory environment.

Fig. 7 presents a schematic diagram of the test apparatus that is a swirl-stabilized, lean-premixed, laboratory-scale combustor (see Kim et al. (2010) for details).

The apparatus consists of an inlet section, an injector, a combustion chamber, and an exhaust section.

The combustor chamber consists of an optically-accessible quartz section followed by a variable length steel section.

Tests have been conducted at a nominal combustor pressure of 1 atmosphere over a range of operating conditions, as listed in Table 3.

PARAGRAPH

In each test, dynamic pressure and global OH and CH chemiluminescence intensity in the combustion chamber were measured to study the mechanisms of combustion instability.

The measurements were made simultaneously at a sampling rate of 8192 Hz (per channel), and data were collected for 8 s, which include a total of 65,536 measurements (per channel).

A total of 780 cases for pressure data are collected (each sequence 65,536 long); however, the label for stable/unstable case is not available.

In this work, the labels are created by clustering the Markov models of pressure data and using some domain expertise.

The Markov models are clustered in an unsupervised fashion using symmetric KL-distance and hierarchical clustering for the states of the Markov models.

The states of the Markov model are, to some extent, similar to each other in the stable case — however, as the lean premixed combustion locks onto an unstable limit cycle, the states of the corresponding Markov models become more distinct.

This behavior of the Markov models is used to cluster the stable and unstable classes and provides almost perfect separation.

More details are available in a previous publication (Jha et al., 2018).

Finally, 125 stable and 125 unstable cases have been used for analysis presented in this paper.

SECTION

Time-series modeling and sequential tests on experimental data

PARAGRAPH

The time-series data set is first normalized by subtracting the mean and then dividing by the standard deviation of its elements; this step corresponds to bias removal and variance normalization.

Data from engineering systems are typically over-sampled to ensure that the underlying dynamics can be captured.

Due to coarse-graining in the symbolization process, an over-sampled time-series may mask the true nature of the system dynamics in the symbolic domain (e.g., occurrence of self loops and irrelevant spurious transitions in the Markov chain).

A time-series is first down-sampled to find the next crucial observation.

The first minimum of auto-correlation function R(τ) generated from the observed time-series is obtained to find the uncorrelated samples in time.

The data sets are then down-sampled by this lag (it is noted that different sequence will have different lags).

Fig. 8 shows the auto-correlation function for a typical time-series in the region of unstable combustion, where the data are down-sampled by the lag marked in red rectangles.

To avoid discarding significant amount of data due to down-sampling, the down-sampled data using different initial conditions (or offsets) are concatenated.

Using this approach, the number of data points lost if reduced significantly to mod(n,τ)≤τ−1 where, n is the original data length and τ is the lag value.

For example, if the original time-series is given by x1,x2,x3,…, after this pre-processing it could rearranged as x1,xτ+1,…,xn−τ+1,x2,…,xn−τ+2,….

An example is shown in Fig. 9 for τ=4.

Further details of this pre-processing are reported in Srivastav (2014), Jha et al. (2015).

PARAGRAPH

The measurement space of the continuous time-series is then partitioned using maximum entropy partitioning (MEP) (Rajagopalan and Ray, 2006), where the information rich regions of the measurement space are partitioned finer and those with sparse information are partitioned coarser.

In essence, each cell in the partitioned set contains (approximately) equal number of data points under MEP.

A ternary alphabet with Σ={0,1,2} has been used to symbolize the data.

As discussed in Section 6.1, data sets are analyzed from two different modes of combustion, stable and unstable, with the aim of detecting the unstable modes.

PARAGRAPH

To perform the sequential ratio tests using the combustion data, a single-depth Markov model is first trained for stable and unstable cases on the discretized sequence.

A single symbol sequence of length ∼65,000 is used to train Markov models for the two hypotheses — stable and unstable.

This model is then used to detect the unstable modes from a test data set with 250 discrete sequences with equal proportions of stable and unstable cases.

Figs. 10 and 11 display the pertinent results of sequential tests.

Fig. 10 compares the theoretical estimate of stopping time for the sequential tests with the average sample length obtained over the test data set corresponding to different operating conditions specified by the desired probability of detection (pd) and a constant false alarm rate, pfa=0.01.

Fig. 11 shows the probability of detection achieved by the sequential tests for different desired detection rates.

Observed probability of false alarm is 0.016 for all test cases, which is slightly higher than the desired value of pfa=0.01.

Out of 125 stable and 125 unstable cases, there are 5 misdetections and 2 false alarms as the best performance.

It is noted that the sequential tests for Markov models are able to achieve desired performance for most cases (except for cases when desired pd>0.95).

PARAGRAPH

The results from sequential approach using experimental data are also compared with fixed-sample-size (FSS) maximum likelihood classifier.

The desired pd was chosen to be 0.95 for the SHT, the observed pd is greater than 0.95 and it is also higher than all cases of ML classifier (See Table 4).

Thus, the comparison shown in Table 4 verifies the claim with experiments that sequential tests need fewer observations than FSS tests on average to give the same (or even better) level of performance.

PARAGRAPH

PARAGRAPH

Even though the sequential detection procedure is tested using a dataset with equal number of stable and unstable samples (i.e., a balanced dataset), the Markov models are learned using just one such sample from the stable and unstable class (as mentioned earlier in the section that the models are trained using one long time-series).

The theoretical results presented in the paper are independent of the number of samples available from the two classes (see Theorem 4.7).

Hence, as long as one representative symbol sequence for each of the hypotheses under consideration is available, the proposed technique is expected to work well.

Thus, the proposed sequential detection test is immune to data imbalance (which is common in a lot of industrial applications).

PARAGRAPH

In contrast to the inferences from numerical simulation in Section 5.2, the sequential tests on experimental data could not always guarantee the desired performance.

This observation can be attributed to the fact that the test set contains (experimental) data from a wide range of operating conditions as was discussed in Section 6.1.

Thus, the test samples cannot be represented accurately by a single D-Markov model used for training.

PARAGRAPH

Multi-Dimensional Time-Series:

PARAGRAPH

The experimental example in this section considers only a single-dimensional time-series data for simplicity of presentation.

However, the results presented in the paper are applicable to any symbol sequence with D-Markov property and thus can treat multi-dimensional data with any additional constraints.

In essence, generating symbol sequences from multi-dimensional time-series is a non-trivial task and is a topic of ongoing research (Virani et al., 2016).

Illustration of the proposed approach with multi-dimensional data is thus out-of-scope of the current paper and is recommended as a topic of future research.

SECTION

Summary, conclusions, and future work

PARAGRAPH

Symbolic time-series analysis-based Markov modeling provides learning and inference capabilities that can be used for on-line monitoring of critical physical systems.

Sequential hypothesis tests are useful for fast and in-situ detection of anomalies and events in systems where measurement data are available in a streaming fashion.

This paper formulates and validates a concept of sequential hypothesis testing for symbolic analysis-based Markov models of time-series data.

First, a Bayesian approach for parameter estimation D-Markov models is presented using Dirichlet and categorical distributions.

This approach has been used to estimate the log-posterior ratio (LPR) for hypothesis testing.

The LPR is then used to develop a sequential hypothesis test for the D-Markov model.

The stochastic evolution of the LPR statistic is studied and it is proved that the sequential test will terminate in finite time with probability one.

The underlying concept is illustrated on two cases of PFSA models with a binary symbol alphabet and depth D=2.

Finally, the proposed sequential test has been used to detect occurrence of unstable combustion on experimental data of pressure time-series collected from a laboratory apparatus of swirl-stabilized combustion.

The sequential tests are able to achieve 95% detection accuracy at 1.6% false alarm for detecting instabilities with short-length data over a wide range of operating conditions.

The performance of the proposed sequential test for D-Markov models is also compared with fixed-sample-size test (maximum likelihood) to verify that sequential tests need fewer observations than FSS tests on average to give the same level of performance, which is important for real-time monitoring of time-critical processes.

PARAGRAPH

Although the performance of the proposed sequential hypothesis test is apparently promising, much theoretical and experimental research is necessary before its real-life applications.

To this end, the authors propose the following topics for future research.