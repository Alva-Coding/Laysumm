10.1016/j.engappai.2019.05.009

FULLTEXT

TITLE

Online probabilistic goal recognition and its application in dynamic shortest-path local network interdiction

SECTION

Introduction

PARAGRAPH

Goal recognition, the ability to recognize the plans and goals of other agents, enables humans, AI agents or command and control systems to reason about what the others are doing, why they are doing it, and what they will do next (Sukthankar et al., 2014).

Until now, goal recognition system works well in many applications like human–robot interaction (Hofmann and Williams, 2007), intelligent tutoring (Min et al., 2014), system intrusion detection (Geib and Goldman, 2001) and security applications (Jarvis et al., 2005).

However, the inability to take full use of the goal recognition results poses another challenge for domains like Game AI (Synnaeve and Bessiere, 2012) and Command and Control system (Xu et al., 2017).

PARAGRAPH

In this work, we select the Shortest-Path Network Interdiction (SPNI) as the problem upon which the goal recognition would be used to provide real-time situation awareness knowledge.

As one of classic decision-making problems in domains like public transportation (Laporte et al., 2010) and public security (Cappanera and Scaparra, 2011), network interdiction is most often modeled in the form of a static two-player, two-stage, master–slave game with perfect information (i.e., a Stackelberg game) (Lunday and Sherali, 2010).

However, these assumptions are not valid in real-life scenarios where the evader’s possible termini are neither single nor static.

This work seeks to orient the knowledge generated by goal recognition system into the decision-making process of the interdictor, and thus allow a dynamic shortest-path local network interdiction.

PARAGRAPH

We first introduce a novel Dynamic Shortest-Path Local Network Interdiction (DSPLNI) model so as to incorporate useful real-time knowledge acquired from goal recognition system.

As the problem is a typical bilevel mixed-integer program (BLMIP), a BLMIP solvable dual form is then proposed as the DSPLNI’s reformulation.

PARAGRAPH

Then we introduce a Markov Decision Process-based goal recognition model, its dynamic Bayesian network representation and the applied goal inference method.

As an extension to the original work (Xu et al., 2017), and in order to retrieve the agent behavior model which would be used during the goal inference, we propose two behavior modeling approaches, i.e., a data-driven learning method based on Inverse Reinforcement Learning (IRL) as well as a heuristic method taking advantage of the network information, to help solve both the data-intensive and no available data situations.

As for the heuristic method, the paper designs a scalable technique in maintaining action utility map for fast goal inference.

This is mainly designed to get and update action utilities more efficiently under the dynamic changing network environment.

PARAGRAPH

Lastly, we introduce a heuristic named Subjective Confidence to design a resource assignment mechanism for DSPLNI, where the interdiction resource could be more flexibly and effectively allocated at each confrontation stage.

PARAGRAPH

To illustrate the general idea of dynamic shortest-path local network interdiction using online probabilistic goal recognition, see the framework shown in Fig. 1.

The circle consisting of four modules formulates a cognition loop.

Specifically, the sensors first feed the observation information into probabilistic goal recognition, inside which the evader’s goal probabilities would be evaluated using state estimation techniques, e.g. particle filters.

Then this information would be further fed into the DSPLNI module for the interdictor making real-time decisions.

Finally, the decisions would be sent to actuators where the course of actions are implemented.

PARAGRAPH

As for the evader’s behavior modeling, we provide two approaches, covering two situations, as shown in the framework.

Both methods consisting of two modules, one for off-line pre-computations and the other designed to accommodate the dynamic changing situations.

For no data situation, we use either heuristic or planning method to generate the evader behaviors, and propose the action utility map maintenance method to speed up online goal inference.

While for the data-intensive situation, the IRL method is first used to learn the weights of the behavior features from the opponent behavior database, followed by the reinforcement learning (RL) method to learn the evader’s action policy from its reward function.

When confronting with the network interdictions and the changing features, we once again use the RL to update the policy.

PARAGRAPH

The rest of this article is organized as follows.

We first review the background and related works in Section 2 of the model-based goal recognition and the shortest path network interdiction problem.

Section 3 formally presents the DSPLNI model, along with the problem reformulation and algorithm.

Then the paper introduces the MDP-based goal recognition, along with the methods for modeling the agent decision behaviors and the resource allocation mechanism using Subjective Confidence.

The experimental results upon two road networks (Chicago and Oldenburgh) show the effectiveness and accuracy of our methods both in goal recognition and the dynamic network interdiction.

SECTION

Background and related work

SECTION

Model-based goal recognition

PARAGRAPH

Generally, the plan recognition (or goal recognition) problem or theory could be defined as follows:

PARAGRAPH

PARAGRAPH

A plan recognition problem or theory is a triplet T=〈P,G,O,Prob〉 where P is the problem domain, G is the set of possible goals G, O=o1,…,om is an observation sequence with each oi being an action, and Prob is a probability distribution over O.

PARAGRAPH

It has been formulated in many ways, as a matching problem over a suitable AND/OR graph (Avrahami-Zilberbrand and Kaminka, 2005), a parsing problem over grammar (Pynadath and Wellman, 1998), a probabilistic inference task over a dynamic Bayesian network (Bui et al., 2002; Liao et al., 2007) and an inverse planning problem over planning models (Baker et al., 2009; Ramırez and Geffner, 2011).

Among these approaches, two formulations solve the problem from different perspectives.

One relies on a suitable library of plans or policies, while another one replaces that by an agent action model and a set of possible goals (Ramırez and Geffner, 2011).

Hidden Markov Models (HMMs) are widely used in goal recognition.

Bui et al. (2002) proposed an Abstract Hidden Markov Model (AHMM) to recognize an agent’s behavior in dynamic, noisy, uncertain domains, and across multiple levels of abstraction.

Comparing to the HMM, Markov Decision Process (MDP) can describe agent actions and interactions between agents and the environment.

Baker et al. (2009) consider the goal recognition problem over a MDP setting where actions are assumed to be stochastic and states fully observable.

Ramırez and Geffner (2011) extend their work to Partially Observable MDP settings where states are partially observable.

Yin et al. (2016) and Yue et al. (2016) further extend to a Semi-MDP setting where durative actions break the Markov property and a Decentralized POMDP setting in multi-agent problem domain.

PARAGRAPH

In this paper, as we focus on orienting goal inference information to the decision making problem of shortest-path network interdiction, we would like to specifically reframe the general Definition 1 in a network interdiction context.

Whereas in task-planning, we seek to determine from the observations of the agent “what is he doing?”,

in network interdiction however, we are trying to discover “where is he going?”

PARAGRAPH

PARAGRAPH

A network interdiction goal recognition problem is a tuple T=〈P,G,s0,O,Prob〉, where:

D=〈V,E,c〉 is a path-planning domain, where V is a non-empty set of vertices (or locations), E⊆V×V is a set of edges between each two vertices, and c:E↦R0+ is a function that returns the non-negative cost of traversing each edge;

G⊆V is the set of possible goal locations;

s0∈V is the start location;

O=o1,…,om, where m≥0 and oi∈V, is the sequence of observations, with o1≠s0; and

Prob represents the prior probabilities of the goals (though we assume in this paper that priors for all goals are equal).

SECTION

Shortest path network interdiction

PARAGRAPH

The network interdiction problem has been examined for several decades within the context of a variety of modeling approaches, optimization objectives, and solution techniques.

The network interdiction problem that we focus on is Maximizing the Shortest Path (MXSP) (Fulkerson and Harding, 1977), which is also frequently referred to as the Shortest Path Network Interdiction (SPNI) problem.

SPNI is defined on a directed graph G=(N,A), where N is the set of nodes, and A, the set of arcs.

The nominal length of arc k is ck≥0; interdiction increases the arc’s length to ck+dk, where dk≥0.

The resource required to interdict arc k is rk≥0. xk

and yk are all binary variables, indicating if the arc k is interdicted by the leader or traversed by the follower.

The mathematical-programming formulation of SPNI is [MXSP − P]z∗=maxx∈Xminy∑k∈A(ck+xkdk)yk ∑k∈FS(i)yk−∑k∈RS(i)yk=1fori=s0∀i∈N∖{s,t}−1fori=t yk≥0∀k∈Awhere X={x∈{0,1}|A||rTx≤R}, FS(i)(RS(i)) are the sets of arcs directed out of (into) node i and R is the total amount of interdiction resource available.

MXSP or SPNI could be viewed as a bilevel mixed-integer program (BLMIP), which is a special case of a static Stackelberg game (Simaan and Cruz, 1973).

PARAGRAPH

To show how SPNI model works in network interdiction, consider the example as shown in Fig. 2.

We first give a simple directed network in Fig. 2(a) consisting of 9 nodes and 14 edges.

The three numbers in the tuple (c,d,r) each represent the original length of the arc, added length after interdiction and the resource required for the arc to be interdicted.

The evader has one fixed starting node (No.1) and two possible goals (No.6 and No.9).

We depict the evader’s shortest path before and after the network interdiction using solid and dashed green lines.

And the interdicted arcs would be labeled with a red cross.

Specifically, we assume that the interdictor has been assigned with a total of 5 units of resource to interdict.

PARAGRAPH

Taking the goal No.6 as an example, the evader would first traverse the optimal route {(1,4),(4,5),(5,6)} with a total length of 7.

However, after the arcs (3,6) and (5,6) being interdicted, the length of the original path would be enlarged to 13 and thus the other route {(1,2),(2,3),(3,6)} with a length of 11 would be selected, as shown in Fig. 2(b).

Similar for the evader taking No.9 as its real goal in Fig. 2(c), a new route {(1,7),(7,8),(8,9)} would be taken other than the original {(1,4),(4,5),(5,9)} after the arcs (7,8) and (5,9) being blocked.

In this case, the path length before and after network interdiction are 10 and 17 respectively, while the new selected path after interdiction is 15.

SECTION

Dynamic shortest-path local network interdiction

SECTION

Model formulation

PARAGRAPH

In previous studies, interdictor is assumed to know the exact location of the source and terminus of evader (Israeli and Wood, 2002; Bayrak and Bailey, 2008; Xiao et al., 2014).

Thus he/she can first allocate resources in the road network, after which evader select the shortest path to traverse; hence, interdictor can make a once-for-all decision to gain an optimal reward.

PARAGRAPH

However, this is not always the case for the interdictor in real scenarios where the evader’s goals and actions are subtle, confusing or even deceptive.

For example in scenarios like drug smuggling, terrorist attacks and tactical movement, where the evader’s real goal cannot be acquired accurately beforehand, the well-studied static network interdiction approaches would certainly fail, as we would discuss about in the end of this section.

Also, strategic evaders who assumes the existence of goal recognizer would hide its real target either by taking misleading actions or allowing additional budgets for diverging from its optimal path.

This once again raises the need of a dynamic network interdiction system driven by online goal recognition.

PARAGRAPH

Before introducing novel model, we first claim assumptions which are different from those in previous MXSP model (Israeli and Wood, 2002): (1) Evader’s current location is observable, only its final goal is uncertain for interdictor; (2) evader may change its final terminus midway; (3) the confrontation is a multi-stage process.

In each stage, interdictor assigns some resources and allocates them to the local area of evader’s current location based on the knowledge of recognition results and subjective confidence, and accordingly evader re-plan the shortest path.

PARAGRAPH

These assumptions are closer to the reality.

To illustrate how the static SPNI model would fail and also why there is the necessity to have an online updated probabilistic estimation over the evader’s possible goals, see the examples shown in Fig. 3.

Once again, we set the amount of total interdiction resource to 5.

Apart from the same meanings of the aforementioned notations, in this case, the red solid and dashed lines each represent the evader’s believed and the actual traversed arcs.

PARAGRAPH

With the set of nodes {6,9} being the evader’s possible goals, Fig. 3(a) and (c) show the situations where the wrong judgment of the evader’s real goal could cause severe performance deterioration under the static SPNI model.

As in Fig. 3(a), the interdicted arcs (7,8) and (5,9) with node No.9 being the evader’s believed goal would have no effect on the evader traversing the shortest path to its real goal node No.6.

This is also true for the case in Fig. 3(c).

PARAGRAPH

This problem could be properly solved after we reformulate the original flow-balance constraint Eq. (1) of SPNI model into a probabilistic goal representation: ∑k∈FS(i)yk−∑k∈RS(i)yk=1fori=s0∀i∈N∖{s,g1,…,gm}−p(gj)∀i=gj,j∈{1,…,m}where p(gj) is the probabilistic distribution over the possible goals g1,…,gm with 0≤p(gj)<1, ∑j=1,…,mp(gj)=1.

PARAGRAPH

Take the uniform goal distribution, where nodes No.6 and No.9 are given equal probabilities during the whole process, as an example, the original result is changed to interdicting the arcs (3,6), (5,6) and (5,9).

In this case, no matter which goal the evader selects, the length of its shortest path would be extended.

Specifically, the length of the evader moving towards node No.6 is changed from 7 to 11, while for node No.9 the data rises from 10 to 12.

Therefore, we could foresee that a new network interdiction model that incorporates the online and updated goal estimation into the network interdiction system would further improve the interdictor’s decision quality.

PARAGRAPH

Also, instead of making its decisions in an offline and once-and-for-all manner, the interdictor needs to have a new flexible resource allocation policy which could be adjusted dynamically according to the real-time goal estimation quality.

This would be talked about further in the next section.

The above problems raise the need to develop a dynamic online SPNI model.

Therefore, the mathematical-programming formulation of Dynamic Shortest-Path Local Network Interdiction (DSPLNI) is modified as follows:

PARAGRAPH

The formulations is: [DSPLNI − P]maxx∈Xminy∑k∈A(ck+xkdk)yk ∑k∈FS(i)yk−∑k∈RS(i)yk=1fori=s0∀i∈N∖{s,g1,…,gm}−p(gj)∀i=gj,j∈{1,…,m} xk∈{0,1},∀k∈FS(s) xk=0,∀k∉FS(s) yk≥0,∀k∈Awhere X={x∈{0,1}|A||rTx≤Rτ}.

The interdictor’s goal is to maximize the expectation length of the shortest path of evader from s to potential termini g1,…,gm.

In each stage, the interdictor selects a set of arcs in FS(s) to interdict guaranteed by constraints in Eqs. (4), (5) and resource limit Rτ, after which the evader re-plan the path to traverse.

Eq. (3) is the flow-balance constraint that drives the evader to start from s and end at g, where p(gj) over potential termini is obtained from goal recognition results.

SECTION

Reformulation and algorithm

PARAGRAPH

The problem of DSPLNI is a typical BLMIP, which cannot be solved directly using MIP approaches; thus a proper reformulation is necessary for the optimal solution.

Here we propose a dual reformulation of DSPLNI.

We first reformulate [DSPLNI-P] as follows: [DSPLNI−P1]maxx∈Xminy∑k∈A(c+Dx)Ty s.t.Ky=b xk∈{0,1},∀k∈FS(s) xk=0,∀k∉FS(s) y≥0where D=diag(d1,…,d|A|), Eq. (7) is the vector-form of flow-balance constraint of Eq. (3), b=(1,0,…,0,−p(g1),…,−p(gm))T.

PARAGRAPH

Since the inner minimization of DSPLNI is a standard shortest-path model, linear dual theory can be used to get the dual of it.

We first fix the outer variable x, and then take the dual of the inner minimization in [DSPLNI-P1], after which release x and make some simple modifications.

The final reformulated MIP results: [DSPLNI−D]maxx∈X,π→bTπ→ s.t.KTπ→≤c+Dx πs=0 xk∈{0,1},∀k∈FS(s) xk=0,∀k∉FS(s)where X={x∈{0,1}|A||rTx≤Rτ}, π→ is the vector form of dual variables.

Hence, [DSPLNI-D], a simple MIP, can be solved directly using a standard LP-based branch-and-bound algorithm.

SECTION

MDP-based goal recognition

SECTION

Model formalization and goal inference

PARAGRAPH

Our MDP-based model is a tuple 〈s0,S,G,e,A,Pa(s′|s),O〉, where s0 is the initial state, S is a non-empty state space, G⊆S is a non-empty set of goal states, e is the goal termination variable where e={0,1}, A is the set of actions, Pa(s′|s) represents the probability of agent reaching state s′ after taking action a at state s with a∈A, s,s′∈S, and O is a non-empty observation set.

PARAGRAPH

Taking advantage of the dynamic Bayesian network, in which all causalities could be depicted, we first introduce a sub-network for agent goal in Fig. 4 to explain causal influences among different variables.

Then a full DBN structure depicting two time slices is presented in Fig. 5.

The behaviors of system evolution are described using functions or parameters.

PARAGRAPH

Fig. 4 shows the sub-network for goal of the evader.

As shown in Fig. 4(a), the full dependency of the goal gτ+1 would include no more than original goal gτ, goal termination variable eτ and the current state sτ at confrontation state τ.

When eτ takes on 0 at τ, showing that the evader’s intention is not terminated, gτ+1 would remain the same as gτ.

While if eτ takes on 1, the evader would select another goal according to goal selection function Γ with Pgτ.

In our evader-indicator scenario, it means that the evader would change its goal with the consideration of their inner and outer situations.

PARAGRAPH

Recognizing the evader’s goal is an inference problem trying to find the real goal behind agent actions based on observations online.

In essence, the task is to compute the posterior distribution P(gτ|oτ) of goal gτ given observation oτ.

Widely applied in sequential state estimation, particle filter is a kind of approximate inference methods which are good at non-Gaussian and nonlinear problems (Chen et al., 2003).

In this work, we assume that the MDP or agent action model is known by both players in goal recognition task, and the set of possible goals G containing the real goal gτ is given along with the priors P(G).

Different to the work (Ramırez and Geffner, 2011) where they obtain the goal posterior P(G|O) from Bayes rule P(G|O)=αP(O|G)P(G)with α is a normalizing constant, in particle system however, a posterior is empirically represented using a weighted sum of Np samples (Chen et al., 2003) drawn from the proposal distribution: p(gτ|oτ)≈∑i=1NpWτ(i)δ(gτ−gτ(i))where gτ(i) are assumed to be i.i.d drawn from q(gτ|oi).

The importance weights Wτ(i) should be updated recursively Wτ(i)≈Wτ−1(i)p(oτ|gτ(i))p(gτ(i)|gτ−1(i))q(gτ(i)|g0:τ−1(i),oτ).

PARAGRAPH

As we use simplest sampling, the q(gτ(i)|g0:τ−1(i),oτ) is set to be p(gτ(i)|gτ−1(i)), which could be computed directly using the agent action model: p(gτ(i)|gτ−1(i))=∫aτ−1(i)∫sτ−1(i)∫eτ−1(i)pgτ(i)peτ−1(i)psτ−1(i)paτ−1(i).

PARAGRAPH

Thus the gτ in Eq. (16) would be sampled from p(gτ(i)|gτ−1(i)).

As the observation oτ only depends on sτ, the importance weights Wτ(i) can be updated by Wτ(i)=Wτ−1(i)⋅p(oτ|sτ(i)).

PARAGRAPH

In this work, a sequential importance sampling (SIS) filter with resampling is used in evader’s goal inference.

Notably, in the resampling process, we apply the estimated effective sample size, N̂eff, according to Nˆeff=1∑i=1Np(W̃t(i))2where W̃t(i) is the normalized weight.

The resampling process returns if Nˆeff>NT, where NT is the pre-defined threshold which is set to Np∕3, otherwise generates a new particle set by resampling with replacement Np times from the previous set with probabilities W̃t(i), and then reset the weights to 1∕Np.

PARAGRAPH

To illustrate the effects of online goal recognition on DSPLNI model, see the example shown in Fig. 6.

The network details (c,d,r) are the same as the above examples and omitted for clarity. [P1;P2;...]

marked at each node is the probabilistic goal estimation obtained from goal recognition algorithm, and is further used in the dynamic network interdiction in a stage-by-stage manner.

In this case, we use red solid and dashed lines to denote traversed arcs before and after the static network interdiction, while green dashed lines to depict those traversed arcs that are affected under the DSPLNI model using online goal recognition.

Besides, we use blue cross to mark those offline interdicted arcs and red one for online interdicted ones.

PARAGRAPH

Using original simple map as in Figs. 2 and 3, we could find that the p(goal1) and p(goal2) along the paths {(1,4),(4,5),(5,6)} and {(1,4),(4,5),(5,9)} at τ=0,1,2 are not distinctive enough to show the evader’s real goal, thus the interdictor cannot allocate resource precisely as in Fig. 6(a) and (c).

This is because the optimal paths to possible goals share a common prefix.

As this problem is not the one we are going to solve in this paper, the original network is changed a little bit to concentrate on our topic.

PARAGRAPH

As in Fig. 6(b) and (d), the length of the arc (7,8) is reduced from 6 to 3.

For both tasks, the evader’s real goal could be correctly recognized as the first observation coming in.

Especially in Fig. 6(d), compared to static SPNI model which needs to balance between the resource costs and the cover of all three possible paths to the goal, DSPLNI using online goal recognition save its resource till the last to interdict the arc (8,9) instead of (7,8) and (5,9), and thus rising the evader’s shortest-path length from 12 straight up to 17.

PARAGRAPH

With no further examples, online goal recognition would help the decision makers of the network interdiction perfectly responds to dynamically changing situations, e.g., changing goals or network reformulation.

SECTION

The Evader’s behavior modeling

PARAGRAPH

It should be noted that, goal inference method would only work if and only if the evade’s state transition knowledge, or decision model could be known beforehand.

In this section, we would introduce two behavior modeling approaches, i.e., a data-driven learning method as well as a heuristic method using network information, to help solve both the data-intensive and no available data situations.

Also, as for the heuristic method, the paper designs an action utility map maintenance method to improve the goal inference’s real-time efficiency.

SECTION

Heuristic method and action utility map maintenance

PARAGRAPH

Many model-based goal recognition (Baker et al., 2009; Ramırez and Geffner, 2011; Yin et al., 2016) share a key assumption, that if the agent is pursuing the goal G, the probability P(a|b,G) of choosing action a in the state b is given by the Boltzmann policy P(a|b,G)=α′exp{βQG(a,b)} where α′ is a normalizing constant and β captures a ’soft rationality’ assumption (Baker et al., 2009), which assumes that the larger β, the greedier agent would be on QG.

And the term QG(a,b) expresses the expected cost to reach the goal G from b starting with the action a in b, i.e., QG(a,b)=c(a,b)+∑o∈Oba(o)VG(bao)where VG is the value function for the (PO)MDP planner assuming that the goal states are those in which G is true, c(a,b) is the expected cost of action a in b,

PARAGRAPH

In this work, we formulate this assumption as p(vi|vτ,gτ)=α′exp(βugτ(vτ,vi)) where ugτ(vτ,vi) is the utility of agent in the vertex vτ choosing vi under the goal gτ at the confrontation stage τ.

In SPNI, we define ugτ(vτ,vi)=1∕(rvτ,vi+rvi,gτ), where the rvτ,vi is the nominal integer length ck of the arc k where k=(τ,i) and rvi,gτ is the shortest path length from vi to the target gτ computed by the Dijkstra algorithm.

However, this value has to be recomputed continuously as we interdict the network, which greatly weakens the scalability of our goal inference algorithm in other larger road maps, or more complex and time-consuming agent action model.

PARAGRAPH

In this section, using source s and target t as the map key where s,t∈N, we design an action trace map to restore the trace solutions generated by the agent action model.

For SPNI, the trace solution is the s−t shortest path computed by Dijkstra algorithm.

Also, we prove that only a small portion of vτ, not all vertexes, whose utilities need to be updated, as in Theorem 1.

PARAGRAPH

PARAGRAPH

Given the network G and a fixed target g, letT〈s,g〉(Ns,As) be the shortest path trace of a source–target pair 〈s,g〉, where Ns={1,2,…,ns} andAs={(i,i+1)|i∈Ns∕ns}.

For anyv∈Ns, there exists at least one T〈v,g〉(Nv,Av) in the v−g shortest path set S, in which the Av satisfies Av⊆As.

PARAGRAPH

PARAGRAPH

Assuming there is no T〈v,g〉(Nv,Av) in the v−g shortest path set S where Av⊆As, then any element T〈v,g〉′ from vertex v to g in S satisfies T〈v,g〉′≤T〈v,g〉.

According to the properties of shortest path network, T〈s,v〉+T〈v,g〉′≤T〈s,g〉.

Thus the T〈v,g〉(Nv,Av) is not the shortest path of the pair 〈v,g〉.

□

PARAGRAPH

Theorem 1 enables the action trace map to be updated only in a limited portion of vertexes.

It could be further cut to the set of un-duplicated vertexes from the filtering result.

Making use of the fact that the agent actual trace only covers very a limited number of vertexes, we propose a dynamic action utility map maintenance algorithm as in Algorithm 1.

PARAGRAPH

In Algorithm 1, the set of all possible evader destinations Ter, action trace map Mapτ−1, the network adjacent matrix NetAdjτ−1 of the network, the interdiction result XGτ and the particle system particleτ are algorithm’s inputs.

Duplicate agent positions of particle system would be pruned out in line (2).

From line (4) to line (14), the algorithm checks whether the moving trace of each unduplicated positions contains the interdicted edge ei of XGτ.

Those agent positions whose trace containing ei would be added into updateSet which is initialized in line (1).

Finally, the trace map is updated using Mapτ−1, updated NetAdjτ−1, updateSet and Ter in line (15).

Also should be noted that, MapUpdate operation not only updates elements in trace map whose source vertex s∈updateSet, but also recomputes elements whose s′ locates along the way from s∈updateSet to the entering vertex of the interdicted edge.

SECTION

Inverse reinforcement learning based behavior modeling

PARAGRAPH

The learning process of IRL is based on a cycle of: (1) forward–backward passes to calculate the expected feature counts for a given set of weights, and (2) gradient-based feature weight updates.

Our method starts with collecting observation data from a human opponent and separating trajectories into several fragments by the goals, if the goal changes in a trajectory.

Given all the observed data, the goal is to find the policy π that best matches the data under the maximum entropy assumption.

PARAGRAPH

Note that the policy depends on the reward at each step, and the reward is assumed to be the linear of features multiplied by a weight vector: R(f,ω̃)=ω̃⋅f.

The opponent’s policy π is defined as πω→≔P(a|s,ω→), where ω→ is given by argmaxω→L(ω→)=∑m=1MlogP(trajm|ω→).

Fortunately, this function is convex for deterministic MDPs (such as game worlds) and therefore can be solved using gradient descent: dL(ω→)=E[fevader]−E[fπω→] E[fevader] is straightforward from observation data and assumes Csi is the expected visiting count for si.

E[fπω→]=∑siCsifsi|πω→

PARAGRAPH

The forward–backward pass algorithm for computing expected occupancy counts based on weights can be found in Ziebart et al. (2008).

With this information we can compute the expected feature count of the whole policy using Eq. (23).

The gradient descent method (Eq. (22)) is used to improve ω→ until it reaches its termination criteria.

PARAGRAPH

The policy can be retrieved by calculating the value for each state s at time t using the Bellman equation: V(st)=maxast{R(st,ast)+∑st+1P(st+1|st,ast)V(st+1)}

PARAGRAPH

The Bellman equation formulation for value iteration presented above would give us a fixed sequence of actions for each state that achieves the maximum value for the MDP.

A similar state–action value function Q:S×A→R, can be defined as the expected return starting from state s, taking action a and thereafter following policy: Q(s,a)=∑s′P(s′|st,ast)(R(s,a,s′)+V(s′))

PARAGRAPH

Rather than simply choosing the maximum, humans exhibit flexibility when choosing the actions, especially when several options are all strong.

To accommodate this behavior, we assume that the opponent selects from a distribution of actions in each state.

The distribution with maximum information entropy will minimize the information loss and thus result in more human-like behavior.

Thus, rather than just selecting the action with the maximum reward, the probability of an action can be weighted by the expected exponential values P(aj|si)∝expQ(si,aj).

The corresponding Bellman equation assuming that the opponent draws from distribution of actions is given in Eq. (25) where R(s,a)=ω→⋅f.

PARAGRAPH

The process of calculating reward function using Maximum Entropy IRL is shown in Algorithm 2.

After ω→ is initialized randomly, the fg is updated according to current state s and goal g.

Then the features are added up from behavior trajectories.

Finally, ω→ could be computed using the forward–backward pass algorithm and gradient-based feature weight updates, as in Eq. (22).

PARAGRAPH

The process of applying IRL-assisted dynamic goal recognition in DSPLNI at one confrontation stage is shown in Algorithm 3.

Variables including Q value, action selection Policy, the adjacent matrix Adj describing current state in a form of vertexes and arcs, feature value f and particle system Particle serve as inputs at stage τ−1 and outputs at τ.

In line 3 of Algorithm 3, after observing the evader’s action at τ from environment, the goal recognition module uses the Policyτ−1 computed at stage τ−1 to advance the particle system from Particleτ−1 to Particleτ.

The weights are updated using Eq. (19) by comparing the current states of particles in Particleτ and the real observation Obs.

PARAGRAPH

After that, the probabilistic distribution P(G|O) of goals are computed according to Eq. (16).

In line 4, P(G|O) along with Adjτ−1 are fed into the DSPLNI module to generate the set of arcs Arcs to be interdicted.

Further, states would be updated based on Arcs, Adjτ−1 and fτ−1.

In lines 6 and 7, we compute the Q value using reinforcement learning, and the Policy under the maximum entropy assumption for the next stage.

SECTION

Resource assignment

PARAGRAPH

Now, we introduce the flexible resource assignment in each confrontation stage.

It is assumed that a total amount of resource R is available during the whole process of interdiction.

Therefore, the resource needs to be dynamically assigned to each stage for the purpose of high utilization levels.

For goal recognizer, we refer to the level of certainty of some specific facts as the agent’s subjective confidence (SC) of those facts.

In this paper, we use SC to compute the Rτ.

In particle system, the SC could be represented using the weighted variance of estimated distribution of goals, Varτ=∑i=1nωτi(gτi−gˆτ)(gτi−gˆτ)Twhere ωτi is the weight of particle xτi and gˆτ is the estimated goal distribution.

This metric was frequently used to evaluate the performance of two goal inference algorithms (Chen et al., 2003).

PARAGRAPH

According to the definition of SC, its value would be at the maximum before any observation comes in.

In particle system, this happens just after all particles are initialized according to goal priors.

For example, when the particles are sampled randomly with three possible termini, the upper bound of the SC Varupper=2∕3.

Based on that, we compute Rτ linearly at each confrontation stage τ using Rτ=Varupper−VarτVarupper⋅(R−∑t=1τRt)∕Hwhere H is estimated as the remaining number of arcs that the evader needs to traverse to the estimated terminus.

SECTION

Experiments

PARAGRAPH

We conducted extensive experiments on the basis of a synthetic evader action data upon two real cases of transportation networks.

We first prove the effectiveness of our goal recognition method, then the performance of DSPLNI and that of classic MXSP algorithm is also compared under several different conditions.

SECTION

Scenarios and experiment settings

PARAGRAPH

The experiment settings are as follows.

The program was written in Matlab script and is run in a computer with an Intel i7 CPU (3.40 GHz) and 8 GB memory.

PARAGRAPH

The real cases of transportation networks evaluated are the Chicago Sketch Road Network (Bar-Gera, 2016) and the Oldenburg Road Network (Brinkhoff, 2002).

The Chicago Sketch Road Network shown in Fig. 7(a) contains 933 nodes and 2950 arcs, and there are 6105 nodes and 14 058 arcs in the Oldenburg Road Network shown in Fig. 7(b).

Here, the initial origin node and potential destination nodes are marked in the road networks, and the length of each arc ck is an approximate integer of the real distance between two nodes.

The initial and potential destination nodes’ index in the Chicago Sketch Road Network are 368 and {377,575,900,882,597,912,867,919} respectively, while that for the Oldenburg Road Network are 2450 and {2896,4225,2295,5752,6022,829,5354,4460}.

These nodes are uniformly selected across each roadmap, so as to minimize the ambiguity exists among multiple targets.

PARAGRAPH

The evader has one starting point and several predefined possible destination which would be selected with equal probability at the beginning.

We simplify the goal termination function Peτ as follows: if evader reaches its terminus, then the goal is achieved, otherwise it would be terminated with the probability pend for every state.

Also in experiments, the goal selection function Pgτ is interpreted as follows: if the gτ−1 is not terminated, then the goal gτ at confrontation stage τ remains the same as gτ−1, otherwise gτ would be selected equal probably among the remaining goals.

The observation, containing the evader’s current position, of the recognizer would be missing with a probability of pmissing.

The computation of the SPNI is formulated into a BLMIP and solved using the MIP solvers of CPLEX 11.5 and YALMIP toolbox of MATLAB (Lofberg, 2005).

SECTION

Tests on goal recognition

SECTION

Goal recognition of specific traces

PARAGRAPH

We run the agent decision model repeatedly and collect two test datasets with each consisting of 100 labeled traces on two road networks.

The traces collected from Chicago Sketch Road Network possesses an average of 23.31 steps, and there are approximately 16% traces where the evader’s goal is changed at least once during the half way.

Accordingly, the numbers for the dataset on Oldenburg Road Network is 103.32 and 42% respectively. pmissing

is set to 0.05 and the number of target is set to 4, in both two scenarios.

The Np of the particle system is set to 300.

PARAGRAPH

To show details of the recognition results, from each dataset we select two special traces.

As listed in Table 1, traces No.1 collected upon Chicago Sketch Road Network and No.1 upon Oldenburg Road Network are the first traces in each scenario where the evader’s goals remain unchanged.

While traces No.9 and No.4 upon each two scenarios are the first traces in the datasets whose goals have been changed in the midway.

PARAGRAPH

As shown in Table 1, the evader in the trace No.9 upon the Chicago network selected Target 2 to be its first terminus before changed it to Target 1 at τ=22 and eventually achieved the goal at τ=44, while the evader in trace No.1 kept its initial goal (Target 2) till the end.

Similarly, the evader in the trace No. 1 from the Oldenburg network kept its initial goal (Target 2) till the end, while the evader in the trace No.4 changed its terminus two times (Target 4 – Target 1 – Target 4) before achieved its final goal.

PARAGRAPH

Recognition results of the above four traces are shown in Fig. 8.

As the size of the Oldenburg road network is several times larger than that of the Chicago road network, so is the densities of goal recognition traces between two scenarios.

In Fig. 8(a) and (c), the correct estimates (Target 2 in Chicago and Target 2 in Oldenburg) reach their peaks within 4 and 7 stages respectively, and the probabilities of these estimates maintains nearly 100% except for several misleading observations.

While in Fig. 8(b) and (d), when the goals are changed during the half way, at τ=22 in trace No.9 in Chicago, τ=76 and τ=88 in trace No.4 in Oldenburg, our method responded not only fast but also accurate.

What is more, the correct estimates all keep their dominance till the end.

PARAGRAPH

In the following tests, we would further evaluate our goal inference method from different perspectives statistically using the metrics of precision, recall and F-measure, which are frequently used to measure the overall accuracy of the recognizer (Sukthankar et al., 2014; Yin et al., 2016; Yue et al., 2016).

PARAGRAPH

SECTION

Goal recognition over multiple goals and noisy observation

PARAGRAPH

As we believe, the larger the number of possible targets, the greater the ambiguity would be introduced into the goal recognition system under a fixed length of observation sequence.

This phenomenon has already been frequently observed during the above experiments.

Similarly, the larger amount of noise coming from the environment or the observation devices would also degrade the goal recognition quality.

In this section, we further investigate the effectiveness of our method over the two above mentioned influencing factors, i.e. the number of possible targets and the amount of observation noise.

PARAGRAPH

In the following experiments, datasets each consisting of 100 labeled traces, are collected upon both the Chicago Sketch network and the Oldenburg road network.

All parameters including particle population Np and pend remain the same as the above experiments.

For clarity, the partition stage Nstage is set to 5 in these two tests.

The experimental results of the above two factors are shown in Figs. 9 and 10 respectively.

PARAGRAPH

As in Fig. 9, we show the three metrics, precision, recall and F-measure, in the same scenario vertically, and compare the performance of all three metrics in different scenarios horizontally.

Within each sub-figure, we further show the changing patterns of recognition quality with different number of goals.

The experiments show that, generally the three metrics would degrade along with the increasing number of goals in one fixed roadmap.

As in Fig. 9(a), (c) and (e), the recognition quality for 2 targets is far better than that of 8 targets.

However, this trend is weakened when we expand the size of the observing map, as in Fig. 9(b), (d) and (f) where the degradation of three metrics facing larger number of targets became less obvious.

What is more, the selection mode of the source and targets in the Oldenburg road network, which is “Source in the middle, and goals scattered around” and different from that of the Chicago, also contribute to this appearance.

It should be noted that, this selection mode also explains, to some extent, the reason why the performance for 8 targets is better than that of 6 ones as in Fig. 9(b), (d) and (f).

PARAGRAPH

As in Fig. 10, we simply use the control variable pmissing to model the noise, which may come from multiple sources, in the observation module. pmissing

is set to 0.0 initially and then increased to 0.2,0.4 and 0.6 incrementally in each test.

It represents the probability of data missing from the observation sequence in each confrontation stage.

According to Fig. 10(a) and (b), the precision of goal recognition is indeed affected by the amount of noise in the system, as we talked about at the beginning of this section.

But still, from the overall performance upon both road networks in Fig. 10(a)–(f), we could find that our goal inference method based on particle filter is robust enough to encounter noisy environment.

What is more, the performance of F-measure for both maps all exceed 0.8, even when more than half of observation sequence is missing.

SECTION

Comparison of MXSP and DSPLNI

PARAGRAPH

As we have proved the effectiveness of our goal recognition method in the last section, in this section we will formally compare the interdiction results between the original MXSP, as in Israeli and Wood (2002), and our proposed DSPLNI model.

Specifically, four factors will be considered, including the total amount of interdicting resource that could be used, the level of action randomness after the decision making of the network evader and the number of target that should be taken care of during the whole process.

PARAGRAPH

In this section, parameters including the initial terminus of evader, the resource required to interdict arcs r, added integer delays d and the total interdiction resource R, remain the same between each two comparative cases of MXSP and DSPLNI, though they could be different between two test instances.

In this section, tests would be focused on the scenario of Chicago sketch road network.

PARAGRAPH

Fig. 11 is the demonstration of the interdiction performance carried out by MXSP and DSPLNI models, where the interdicted arcs are labeled in the middle by a solid square and the actual paths evaders selected are depicted by the bold lines.

In this simple test, we control the evader changing its initial terminus (Target 3) to a predefined one (Target 2) at a fixed time step (τ=20).

As shown in Fig. 11(a), MXSP only deploys its resource according to the initial distribution of all possible termini once and for all.

While using goal estimation in Fig. 11(c) and subjective confidence in Fig. 11(d), the behavior of the dynamic DSPLNI is much more concentrated and effective as in Fig. 11(b).

It also shows the relationship between the resource allocation per step and the subjective confidence.

During the early prediction when τ<4 and the goal changes by approximately τ=20, the subjective confidence is at a high position accompanied with low resource allocation as in Fig. 11(d).

SECTION

Comparison under different resource

PARAGRAPH

After the demonstration test, in this section we first compare the two models under different amounts of total available interdicting resource R.

The value of R is set to a fixed percentage of resource ∑krk ranging from 0.2% to 1.0%.

Using each R, 20 instances with randomly generated r and d are exercised.

As we have mentioned, r and d remains the same in each two comparative cases between the MXSP and DSPLNI.

The evaders are assumed not to alter their targets from the beginning till they reach destinations.

Besides, they are completely rational, meaning that the evaders would select actions to their best interests.

Also, target 1 to 3 as depicted in Fig. 7(a) are used in this test.

PARAGRAPH

In examining the resource factor, two situations are considered, where the first one concentrates on giving exact pre-known goal information to models, and the latter one focuses on probabilistic goal information, as shown in two box graphs in Fig. 12(a) and (b).

Specifically, in Fig. 12(a) we set the goal distribution as the true goal of the evader, e.g. Pgoal=[0,0,1] when the evader’s goal is the third target, both for MXSP and DSPLNI models.

While in Fig. 12(b), Pgoal is an unknown variable, and is set to Pgoal=[1∕3,1∕3,1∕3] in MXSP and the online goal inference estimation P(G|O) in DSPLNI.

PARAGRAPH

Compared to themselves, different models all perform better when they are granted with increased amounts of interdiction resource, ranging from 0.2% to 1.0%.

More specifically, the median numbers of added length caused by network interdiction for MXSP model with exact goal information rise from less than 150 to more than 250, while those numbers for DSPLNI rise straightly from less than 200 to almost 450.

For the latter case, the numbers for MXSP rise from less than 100 to almost 200, while those for DSPLNI have a similar performance with the same DSPLNI model using exact goal information.

Apparently, DSPLNI model always could achieve a superior performance compared with MXSP, under each amount of total resource R. Also, as shown in Fig. 12(a) and (b), the performance of MXSP using probabilistic goal information during its interdiction decision-making process degrades a lot compared with the same model using exact pre-known goal information instead.

PARAGRAPH

It should be noted that, the DSPLNI model with probabilistic goal estimates always performs better than the MXSP with exact pre-known goal distribution, while at the same time and as being mentioned above, it further has a similar performance with the same model using exact goal information.

This, on one hand, shows the superiority of our model compared with the traditional MXSP, while on the other hand once again proves the effectiveness, including accuracy and fast-responding capability, of our goal recognition methods.

PARAGRAPH

Further, the line series graphs depicting case-to-case comparison between the MXSP and the DSPLNI models are shown in Fig. 12(c) and (d), where each two comparative cases share the same r and d.

It could be found that, the DSPLNI wins in all two cases.

SECTION

Comparison under different number of targets

PARAGRAPH

In this section, we compare the MXSP and DSPLNI models under different number (from 2 to 8) of possible targets.

Target selection follows the order in Fig. 7(a).

Other experiment settings including parameters, assumptions, and comparison mode all stay the same as the above several tests.

Detailed experiments are shown in Fig. 13.

PARAGRAPH

From the experiments, we could find that models with exact goal information is not very sensitive to the increase of target numbers, both for MXSP and DSPLNI.

This is because exact goal information excludes the effect from target number changes, as shown in Fig. 13(a).

While when models are given probabilistic goal information, things are different.

For DSPLNI, its performance is relatively steady and little change is witnessed as this probabilistic goal information is replaced by an accurate goal estimation.

While for MXSP model, the added length continuously drops down as more possible targets are available, with its median numbers drop from 175 to less than 150.

This is because, without pre-known goal information, static MXSP allocate its resource evenly to arcs along all possible goals.

We also give the case-to-case comparison in Fig. 13(c) and (d).

PARAGRAPH

SECTION

Comparison under action randomness

PARAGRAPH

Our last section focuses on the comparison of MXSP and DSPLNI under action randomness, which on one hand may come from stochastic environment or the evaders’ bounded rationality that impedes them from taking optimal actions.

While on the other hand, the evaders’ certain deceptive tactics or misleading behaviors, could also be interpreted as the action randomness from the observer’s perspective, if the observer has no idea of these tactics and believes the evader would select the shortest path.

In this section, we test the robustness of our DSPLNI model under the above mentioned circumstances.

PARAGRAPH

In Fig. 14, we set the available interdiction resource to be 0.6% of ∑krk, and a path deviation probability PRandom, ranging from 0% to 20%, is introduced to describe the action randomness.

Specifically, instead of selecting the optimal one according to paction=1−PRandom, the evader would evenly choose other non-optimal neighboring arcs using paction=PRandom∕Nneigh, where Nneigh is the number of neighboring arcs.

PARAGRAPH

Generally, compared to the above sections, the added length in this experiment setting increases a lot because of the randomness in action selection.

Once again, DSPLNI model performs better than the MXSP under situations no matter goal information is exact or not, as shown in Fig. 14(a) and (b).

However, there is only a relative small superiority of our DSPLNI model performance compared to that of the MXSP with probabilistic goal information, as shown in Fig. 14(b) and the case-to-case comparison shown in Fig. 14(d).

There are two reasons for this performance deterioration.

Firstly, action randomness gives more uncertainty to agents’ action selection, and larger randomness means their behavior would further deviate from the optimal one.

Therefore, each two comparative cases would have totally different moving traces, as we could find in Fig. 14(c) and (d).

Secondly, as the randomness grows higher, more influence would be given to our goal inference method, e.g. increased failure rates and frequently changing estimated goals.

This in turn would lower down the quality of interdictor’s decision making, and thus affects the DSPLNI performance.

PARAGRAPH

The test results show that, too much randomness in agent action selection does affect the effectiveness of our DSPLNI model in network interdiction.

The evader could use certain deceptive tactics to handle the goal recognition module of the DSPLNI model.

Correspondingly, this result also calls upon the practical requirements of a more robust goal recognition method in adversarial settings.

SECTION

Test on IRL-based behavior learning

PARAGRAPH

Lastly, different from the above experiments using a simple heuristic method, in this section we test the IRL-based behavior modeling for goal recognition during the dynamic shortest path local network interdiction.

The experiments are conducted on the basis of a human evader action upon the Chicago Sketch Road Network upon an interactive environment, as shown in Fig. 15(a), generated by the Unreal Engine 4 (Engine, 2016).

The human evader has 1 starting point and 3 predefined possible goals which would be selected with equal probability at the beginning.

Along with an IRL-learned model using human behavioral data, we give another two similar models predefined by human experts for comparison.

The assumptions are same as those in Section 5.1, and pend and pmissing are set to 0.01 and 0.2.

The Np of the particle system is set to 600.

SECTION

Tests on IRL-based behavior learning

PARAGRAPH

The features are used to discriminate between locations in the environment.

Without loss of generality, in this task we simply use the following features: (1) distance d to the evader’s goal; (2) the value of coordinate X; and (3) the value of coordinate Y. Thus, R=ω1×d+ω2×X+ω3×Y, where ω→=(ω1,ω2,ω3) are the weights of three features.

Based on these features, we compute the weight values for three predefined goals, as shown in Table 2, using the Maximum Entropy IRL in Algorithm 2.

PARAGRAPH

Table 2 shows that the policy learned by IRL for Target 1 is most closely related to the test trajectories, compared to Target 2 and 3.

This is because there exists several non-distinctive paths for the latter two targets, which introduces ambiguity not only to human observer, but also to the IRL learner.

Also, we show the final value map for Target 2 computed by reinforcement learning using IRL-generated rewards, as shown in Fig. 15(b).

These relative values are computed using Eq. (24).

They increase from blue (value 0) to red (value 1), and show a general trend of the evader moving from the source point to Target 2.

PARAGRAPH

Further, applying the behavior model to network interdiction, we can compute the change in the Q value for each fixed vertex according to Eq. (25), after its neighboring edges are interdicted as shown in Fig. 16.

We select the index No.572 vertex at the position (68,189) to analyze, and know Target 2 is located in the east of this mini-map in advance.

The relative Q values for No.572’s neighboring vertices, which represent the set of evader’s possible action selections and are connected using bold lines, are shown in the white boxes.

Fig. 16(a) shows the original values before network interdiction.

PARAGRAPH

After the edge, labeled in the middle by a solid square as shown in Fig. 16(b) along the vertex No.572 and 573, is interdicted, relative Q values change accordingly.

Specifically, among 7 neighboring vertices, the relative Q values increase at No.576, No.637, No.569 and No.570, and decrease at No.26 and No.571.

Though the relative value for vertex No.573 is still the highest, its real value decreases after the interdiction, as the total increased Q value of 4 vertices is higher than that of the decreased value.

This is because the interdiction increases the cost for evader to traverse from No.572 to Target 2 taking actions to No.573.

This also explains the relative value changes for the other 6 vertices.

SECTION

Tests on IRL-based goal recognition for DSPLNI

PARAGRAPH

In this test, we first verify the effectiveness of IRL learned behavior model in goal recognition, using the dataset from the same group of subjects.

Then we test the overall performance of Algorithm 3 in DSPLNI problem.

The dataset consists of 100 labeled traces with each one possessing an average of 20.65 steps.

There are approximately 19% traces where the evader’s goal is changed at least once during half way.

To show the details of goal recognition results using IRL learned behavior model, and especially in a situation where the goal has been changed midway, we select the first trace that has goal changes as an example, which is trace No.9, as shown in Table 3.

PARAGRAPH

As shown in Table 3, the evader in trace No.9 selected Target 1 to be its first terminus before changing it to Target 3 at τ=8, and eventually reaching the goal at τ=20.

Recognition results are shown in Fig. 17(d).

The probability of the real goal Target 1 increases quickly during the initial period.

When the goal changes at τ=8, our method responds very quickly and the correct estimate is maintained until the end.

Further, we compare the performance of goal recognizers using different behavior models in Fig. 17(a–c) by statistic metrics of precision, recall and F-measure.

The weights of the other two cases are ω→1=(0.1,1.0,1.0) and ω→2=(1.0,−1.0,−1.0) respectively.

The goal recognizer using IRL learned model has a faster reaction time and performs the best till the end in all three metrics.

PARAGRAPH

Finally, we test the overall performance of Algorithm 3 in the DSPLNI problem, as shown in Fig. 18, which compares the original static MXSP solver with DSPLNI solvers using different behavior models, i.e., IRL learned model (DSPLNI-IRL) and models (DSPLNI-R1 and DSPLNI-R2) with human specified ω→1 and ω→2.

In both goal changing and unchanged situations, the DSPLNI solver using IRL learned behavior model performs the best.

SECTION

Conclusion

PARAGRAPH

We have tested the positive effects of goal recognition in helping improve the performance of a more realistic SPNI problem.

Several methods, including two behavior modeling approaches, subjective confidence-directed resource assignment and a reformulated dynamic network interdiction are proposed.

The framework above from the goal recognition to decision making is simple but inspiring, and helps to solve both the data-intensive and no available data situations in many real-time decision making tasks.

PARAGRAPH

This work, on one hand, is an effective application of artificial intelligence in traditional operational researches, on the other hand, grounds the behaviors of the agents on clear, well-founded computational models.

Though this work is quite suggestive but still simple, problems with higher dimensionality, more complex state description and adversarial goal recognition are still open in our future work.