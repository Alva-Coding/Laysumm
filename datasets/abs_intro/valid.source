In this paper, a diagnostic evaluation of the state of the art of archaeological waterlogged foundation piles in Riga Cathedral (1211 CE) was carried out. Microscopic, chemical and instrumental methods were applied to study the impact of deterioration of piles leading to the deformations of the Cathedral building. Severe biodeterioration by microorganisms in the majority of pile samples was determined. Chemical analyses showed an extensive depletion of hemicellulose and cellulose sugars while lignin seemed to be unaffected. Restricted degradation of hemicellulose sugars –arabinose and galactose – was characteristic of bacterial degradation of wood. FTIR spectroscopy proved itself as a quick and efficient method for determination of changes in wood components in foundation piles in comparison with chemical method. The increased ash content (up to 70%) in waterlogged wood consisted of deposited salts, oxides and other inorganic compounds. The X-ray diffraction method determined the main inorganic impurities, including calcite, quartz, sodium magnesium silicate and muscovite.The historic centre of Riga is included in the UNESCO World Heritage list. Riga Cathedral (1211) is one of the oldest sacred buildings of the medieval period in Latvia and also the Baltics. In recent years, deformations (cracks) have been observed in a number of historic buildings, including the Cathedral. Those buildings are supported by wooden foundation piles. Extensive research work with many expert institutions has been carried out to understand the reasons for the deterioration of Cathedral constructions. It is found that the mortar foundation is stable but the ground complex underneath, containing wooden piles and ground material, could be the main reason for super-normative deformations. The piles are situated under permanent groundwater; however, because of water level fluctuations, the upper parts of piles could be exposed above the water for longer periods. The main geotechnical problem is the declined load bearing of damaged piles which is unpredictable and can lead to an increased construction's settlement in future.
Two patients at our center experienced florid visual hallucinations following hemispherectomy. The first patient had drug-resistant left hemispheric focal seizures at 20 months of age from a previous stroke. Following functional hemispherectomy at age 3, he experienced frightening hallucinations 1 month post-operatively lasting 3.5 months. Our second patient underwent subtotal hemispherectomy at age 6 for drug-resistant focal seizures from right hemispheric cortical dysplasia. Eighteen months later he developed scary visual hallucinations during which he would shout and throw things. Hallucinations recurred for 6 months. In our experience in these patients, even though symptoms were florid, they were transient and subsided 3–6 months later.To our knowledge visual hallucinations following hemispherectomy have not been reported [1,2]. Visual hallucinations refer to visual images that are not in fact present, and can be either elemental (occipital origin) or formed (temporal origin) representing visual memories. There have been reports of hallucinations in adults following intracranial surgeries, such as musical hallucinations and visual hallucinations associated with Charles-Bonnet syndrome [3,4]. Here we present two pediatric patients out of 24 consecutive patients operated at our center, with visual hallucinations after hemispherectomy. Hemispherectomy was functional in one person and subtotal in another. The hallucinations were disturbing to patients and families, but that eventually proved to be transient. SECTION Case report SECTION Patient 1
Chronic cannabis use impacts memory functioning, even while users are not acutely intoxicated. The impact of cannabis use on Wada or intracarotid amobarbital testing (IAT) has not previously been described. We reviewed cannabis consumption in epilepsy patients undergoing IAT during pre-surgical work-up. Of 58 patients reviewed, 16 patients (28%) indicated regular use. During IAT, five regular cannabis users with suspected temporal lobe epilepsy exhibited poor memory while testing their presumptively healthy temporal lobe (i.e., the side opposite that targeted for epilepsy surgery), indicating the potential for an amnestic syndrome post-operatively. It was suspected that the pattern of IAT results for these patients was attributable to the deleterious impact of cannabis use on cognition. Thus, three of the five underwent repeat IAT after a period of enforced abstinence. On repeat IAT, each of the three patients exhibited improved memory performance while testing their healthy temporal lobe, suggesting that the healthy temporal lobe of each mediated sufficient memory ability to allow for epilepsy surgery. These findings raised concerns that frequent cannabis use may alter IAT results, leading to incorrect assessments regarding potential post-operative cognitive deficits, and led to a mandate at our institution that patients must stop cannabis use before IAT.The Wada test or intracarotid amobarbital test (IAT) is used for language lateralization and to assess memory function in patients with drug-resistant epilepsy who are considering anterior temporal lobectomy with hippocampal resection as treatment [1]. During the test, amobarbital is injected into the internal carotid artery and inactivates brain function in the dependent vascular territory, typically including hippocampal function. It is intended to mimic the effect of removing the epileptogenic temporal lobe and its medial structures, and assesses whether the remaining, contralateral temporal lobe can provide sufficient memory function to compensate for the loss of the ipsilateral hippocampus. If a patient exhibits poor memory performance while testing the presumptively healthy side, an anterior temporal resection with removal of mesial structures cannot be recommended as the results indicate that the patient may have inadequate memory function post-operatively. Conceivably, systemic factors that impair memory, such as medications or drugs like cannabis, can interfere with IAT performance and cause misleading results.
Pyrometamorphic rocks produced by natural coal combustion appear at archaeological sites across North America but have received little archaeological attention regarding provenance studies. Tertiary Hills Clinker is a distinct pyrometamorphic rock from Subarctic Canada utilized by hunter-gatherers from 10,000 years ago to European contact. We employ X-ray diffraction, thin section analyses, and electron probe microanalyses to characterise Tertiary Hills Clinker and inform archaeometric studies of rock produced by combustion metamorphism. We geochemically compare pyrometamorphic rocks used by pre-contact people across North America to demonstrate that Tertiary Hills Clinker can be sourced using portable X-ray fluorescence. Results indicate that Late Pleistocene/Early Holocene exchange networks in North America were larger than previously thought. A later change in the distribution of Tertiary Hills Clinker may relate to a Late Holocene volcanic eruption (White River Ash east) that fragmented modes of lithic exchange and associated social networks with potential stimulus for a subsequent large-scale migration of northern hunter-gatherers across the continent. Provenance studies of pyrometamorphic artifacts offer untapped opportunities to study social networks in coal-bearing regions across the world.Pyrometamorphism (also called combustion metamorphism) generally occurs when coal, oil, or gas burn with sufficient energy to bake or fuse neighbouring rock (Allen, 1874; Bentor et al., 1981; Cosca et al., 1989; Grapes, 2011:21; Stracher et al., 2010). Beds of fused rock were targeted for stone tool production because of the raw materials' internal uniformity (Cinq-Mars, 1973; Clark, 1986; Curran et al., 2001; Fredlund, 1976). Pyrometamorphic rock has received geological attention in North and South America (Hefern and Coates, 2004; Henao et al., 2010), Europe (Žáček et al., 2015), Russia (Sokol et al., 1998), China (Song and Kuenzer, 2017), and Africa (Pone et al., 2017). Despite the global distribution of rock produced by coal combustion and the use of it by people, comparatively few efforts have been made by archaeologists to formally identify pyrometamorphic rock in archaeological assemblages (Estes et al., 2010; Hughes and Peterson, 2009; Le Blanc, 1997; Vapnik et al., 2015).
Cova Bonica has yielded one of the few assemblages of Cardial Neolithic records of directly dated human remains (c. 5470 and 5220 years cal. BC – unmodelled) in the Iberian Peninsula and has provided the first complete genome of an Iberian farmer. A minimum of seven individuals and six age clusters have been ascribed on the basis of the disarticulated human bones. A large number of archaeological artifacts have likewise been identified in the same layer, preserved in a small number of remnants in different areas of the cave. This study presents the results of a multi-proxy archaeological analysis of the spatial distribution, human remains, small and large mammals, palaeobotanical remains, lithics, ceramics and radiocarbon dating, with the aim of reconstructing the cave's history and the context of the layer containing the human remains. The results suggest the cave was used for at least two distinct purposes: one related to its use for funerary practices, as documented by a small group of artifacts (ornamental objects, ceramics, tools), charcoal and small mammals; the other related to its use as a sheep pen as indicated by reworked fumier, the results of a zooarchaeological study and an ovicaprine palaeodemographic profile. The paper concludes that the funerary and ritualistic practices of the Cardial Neolithic in SW Europe are difficult to reconstruct because human remains are often scattered in archaeological layers where other human activities may also have been conducted. For this reason, artifacts associated with human remains do not constitute a solid foundation on which to reconstruct funerary practices. Indeed, only a multi-proxy analysis of the archaeological material is capable of evaluating different geological and/or archaeological processes and their associated activities.The transition from hunter-gatherer to farmer populations meant radical economic and social changes from the ways of nomadism to more sedentary ways of life (Guilaine, 2013). According to palaeogenetic data (Gamba et al., 2014; Olalde et al., 2015; Rivollat et al., 2015), farming appears to have been introduced into central and western Europe around 8000 years ago by new migrant populations, while the archaeological evidence suggests at least two distinct, but well-defined, migration routes: up the Danube valley and along the Mediterranean shoreline (Cruz Berrocal, 2012; Guilaine, 2013; Zilhão, 2001). The Neolithic period in the associated archaeological sites shares close links with the Linearbandkeramik (LBK) culture in central Europe and with the Impressa and Cardial traditions in southern Europe, characterized by the domestication of plants and animals, and the use of a new lithic technology and raw materials, among other features. Scholars have employed a range of approaches in their study of Neolithic phenomena in Europe, including, the determination of the rhythms of this process, radiocarbon dating, stratigraphy, and analyses of the social structure, economy, funerary practices, etc. (Bickle and Whittle, 2013; Binder, 2000; Bogaard, 2004; Cruz Berrocal, 2012; Davison et al., 2006; Gronenborn, 1999; Oross and Bánffy, 2009).
Hemispherotomy is a surgical treatment indicated in patients with drug-resistant epilepsy due to unilateral hemispheric pathology. Hemispherotomy is less invasive compared with hemispherectomy. We reviewed our experience performing 24 hemispherotomy and report the results of 16 patients with prolonged follow-up of this relatively uncommon procedure in two centers in Indonesia. This is a retrospective observational study conducted from 1999 to July 2019 in two epilepsy neurosurgical centers in Semarang, Indonesia. Surgical techniques included vertical parasagittal hemispherotomy (VPH), peri-insular hemispherotomy (PIH), and modified PIH called the Shimizu approach (SA). The postoperative assessment was carried out using the Engel classification system of seizure outcome. Seizure freedom (Engel class I) outcome was achieved in 10 patients (62.5%), class II in 3 patients (18.7%), class III in 2 patients (12.5%), and class IV in 1 patient (6.3%) with follow-up duration spanning from 24 to 160 months. To the best of our knowledge, this series is the most extensive documentation of hemispherotomy in an Indonesian population.Hemispherotomy is a potential surgical treatment indicated for patients with drug-resistant epilepsy due to unilateral hemispheric pathology [1,2]. The underlying etiology for unilateral hemispheric pathology may include conditions such as Rasmussen syndrome, Sturge–Weber syndrome, porencephaly, perinatal stroke and disturbances in neuronal migration (e.g., hemimegalencephaly, cortical dysplasia, and hemiconvulsion–hemiplegia–epilepsy syndrome) [3,4].
We propose SECUR-AMA, an Active Malware Analysis (AMA) framework for Android. (AMA) is a technique that aims at acquiring knowledge about target applications by executing actions on the system that trigger responses from the targets. The main strength of this approach is the capability of extracting behaviors that would otherwise remain invisible. A key difference from other analysis techniques is that the triggering actions are not selected randomly or sequentially, but following strategies that aim at maximizing the information acquired about the behavior of the target application. Specifically, we design SECUR-AMA as a framework implementing a stochastic game between two agents: an analyzer and a target application. The strategy of the analyzer consists in a reinforcement learning algorithm based on Monte Carlo Tree Search (MCTS) to efficiently search the state and action spaces taking into account previous interactions in order to obtain more information on the target. The target model instead is created online while playing the game, using the information acquired so far by the analyzer and using it to guide the remainder of the analysis in an iterative process. We conduct an extensive evaluation of SECUR-AMA analyzing about 1200 real Android malware divided into 24 families (classes) from a publicly available dataset, and we compare our approach with multiple state-of-the-art techniques of different types, including passive and active approaches. Results show that SECUR-AMA creates more informative models that allow to reach better classification results for most of the malware families in our dataset.In recent years the increasing reliance on computer systems and the increasing use of Internet, wireless networks, autonomous systems, e.g., cars, boats, as well as the growth of smart and tiny devices as part of the Internet of Things (IoT) resulted in a corresponding increase in the number of cyber-security flaws. In particular, Android is one of the most diffused operating systems employed in smartphones and IoT devices, making it the preferred target for cyber-criminals due to its huge market share (Cheung, 2018). Android malware are then one of the biggest threats in IT security nowadays, with millions of malicious applications released every year. Analyzing such amount of threats have become almost impossible for human security experts, and consequently, tools based on machine learning are fundamental to automate and speed up the process. In this work we aim to extend an analysis technique we proposed in Sartea and Farinelli (2017), by creating a fully fledged automated framework for Android malware analysis that substitutes the manual analysis of an unknown application, i.e., by performing automated test interactions and adapting to what is observed. Broadly, the concept of executing specific actions to perform a better analysis can be linked to the general framework of active learning, and recently there has been a specific interest in applying active learning techniques to malware analysis (Nissim et al., 2014). In that work, authors propose the use of an SVM classifier to select which samples (already analyzed) should be fed to the classifier, so to refine the classification bounds. In this work, our aim is to generate malware models that can be studied by a human security expert or processed by automated techniques (clustering, classification) for comparison. Hence, we focus on the decision making side of the analysis by devising an intelligent strategy for the analyzer action selection. For this reason, SECUR-AMA differs from active learning approaches where the methodology is usually tied to the specific choice of classifier in order to improve the classification bounds, e.g., k-NN and naïve Bayes (Wei et al., 2015), logistic regression (Guo and Schuurmans, 2007), linear regression (Yu et al., 2006), SVM (Tong and Koller, 2002).
Cannabis use is associated with changes in brain structure and function; its neurotoxic effects are largely attributed to Δ9-tetrahydrocannabidiol. Whether such effects are present in patients with epilepsy exposed to a highly-purified cannabidiol isolate (CBD; Epidiolex®; Greenwich Biosciences, Inc.) has not been investigated to date. This preliminary study examines whether daily CBD dose of 15–25 mg/kg produces cerebral macrostructure changes and, if present, how they relate to changes in seizure frequency. Twenty-seven patients with treatment-resistant epilepsy were recruited from the University of Alabama at Birmingham CBD Program. Participants provided seizure frequency diaries (SF), completed the Chalfont Seizure Severity Scale (CSSS) and Adverse Events Profile (AEP), and underwent MRI before CBD (baseline) and after achieving a stable CBD dosage (on-CBD). We examined T1-weighted structural images for gray matter volume (GMV) and cortical thickness changes from baseline to on-CBD in 18 participants. Repeated measures t-tests confirmed decreases in SF [t(17) = 3.08, p = 0.0069], CSSS [t(17) = 5.77, p < 0.001], and AEP [t(17) = 3.04, p = 0.0074] between the two time-points. Voxel-level paired samples t-tests did not identify significant changes in GMV or cortical thickness between these two time-points. In conclusion, short-term exposure to highly purified CBD may not affect cortical macrostructure.Of the 1.2% of the population that suffers from epilepsy, one-third has treatment-resistant epilepsy (TRE) in which anti-seizure drug (ASD) mono- or poly-therapy does not control seizures [1]. In TRE, the primary tissue insult from chronic, uncontrolled seizures, combined with the secondary effects of failed ASDs, results in on-going insult to brain structure and function [2,3]. Patients with TRE are thus at increased risk for epilepsy-related mortality (e.g., sudden unexpected death in epilepsy; SUDEP), as well as more severe cognitive and neuropsychological impairments [2,3]. While ASDs treat seizures, they do not interrupt or reverse the underlying epileptogenesis [1] which underscores the necessity of finding treatments that interrupt or reverse the pathophysiology that underlies epilepsy. Recent evidence points to chronic neuroinflammation as one of the potential drivers of epileptogenesis [4–8]. Perpetual activation of the neuroinflammatory cascade can lower seizure threshold, resulting in dysfunction of the blood–brain-barrier and chronic neuronal hyperexcitability [4–8]. This notion highlights an under-exploited therapeutic target: the development of treatments that interrupt the neuroinflammatory cascade to provide seizure freedom to patients with TRE.
Focal neuroinflammation is considered one of the hypotheses for the cause of temporal lobe epilepsy (TLE) with amygdala enlargement (AE). Here, we report a case involving an adult female patient with TLE-AE characterized by late-onset seizures and cognitive impairment. Anti-N-methyl-d-aspartate receptor (NMDAR) antibodies were detected in her cerebrospinal fluid. However, administration of appropriate anti-seizure drugs (ASD), without immunotherapy, improved TLE-AE associated with NMDAR antibodies. In the present case, two clinically significant observations were made: 1) anti-NMDAR antibody-mediated autoimmune processes may be associated with TLE-AE, and 2) appropriate administration of ASD alone can improve clinical symptoms in mild cases of autoimmune epilepsy.In recent years, an increasing number of reports have indicated an association between amygdala enlargement (AE) and temporal lobe epilepsy (TLE) [1]. Focal neuroinflammation is considered one of the hypotheses for the cause of TLE-AE. However, the etiology is not fully understood, and there is no consensus regarding its treatment. Here, we describe a patient with TLE-AE associated with Anti-N-methyl-d-aspartate receptor (NMDAR) antibody. Administration of appropriate anti-seizure drugs (ASD), without immunotherapy, improved her clinical symptoms. SECTION Case
A 54-year-old man was admitted to the intensive care unit with an aneurysmal subarachnoid hemorrhage and subsequently underwent mechanical ventilation and received neuromuscular blocking drugs to control refractory elevated intracranial pressure. During quantitative EEG monitoring, an automated alert was triggered by the train of four peripheral nerve stimulation artifacts. Real-time feedback was made possible due to remote monitoring. This case illustrates how computerized, automated artificial intelligence algorithms can be used beyond typical seizure detection in the intensive care unit for remote monitoring to benefit patient care.Since the introduction of digital electroencephalogram (EEG) and computer-based microprocessing, methods of EEG recording have evolved from paper records to dedicated EEG digital servers that can be accessed remotely, similar to telemedicine in the intensive care unit (ICU) [1,2]. Further, raw EEG data can now be processed via computerized software by Fast Fourier transform techniques into condensed quantitative EEG (QEEG) displays with numerous mathematical derivatives for seizure detection and even surrogate cerebral blood flow inferences. The technology for EEG data analysis has advanced rapidly in the last decade, using an array of sophisticated software and artificial intelligence algorithms for seizure detection based on the EEG waveform morphology (i.e., spike detection) combined with spike frequency (> 2–3 Hz), or on a combination of amplitude, morphology, and frequency (seizure detection and artifact rejection).
The process of dating ancient obsidian artifacts converts the quantity of surface diffused molecular water to a calendar age using an experimentally derived diffusion coefficient predicted from glass composition. The internal structural water content of rhyolitic obsidian has been identified as a highly influential variable that controls the rate of water diffusion at ambient temperature. We demonstrate through the use of infrared spectroscopy and specific gravity (density) measurements on samples from 34 obsidian sources that total structural water (H2Ot) concentrations between sources can range from 0.07% to 1.66%. Structural water concentration within individual sources may also vary significantly and impact the accuracy of estimated ages for artifact manufacture if not properly monitored. A calibration for the water determination on individual samples by density measurement is developed here and the impact of structural water variation on obsidian chronometric dates is discussed.Geological flows of rhyolitic glass, or obsidian, were routine sources of tool raw material for humans in the Lower and Middle Paleolithic (Adler et al., 2014; Merrick and Brown, 1984) and afterward (Shackley, 1998). Easily accessible from primary surface outcroppings, or within secondarily deposited erosional contexts (Doleman et al., 2012), obsidian was knapped into functional tools, symbols of cultural interconnectedness (Torrence, 2011), or markers of social inequality (Pierce, 2015). The abundance of obsidian in the archaeological record, created by its use within many types of cultural contexts, provides an opportunity for obsidian hydration age determinations to play a significant role in archaeological interpretation. High resolution temporal reconstructions of past events benefit from chronometric assays where the standard errors associated with dates are small. Rogers (2008b, 2010) discusses four major error sources that can impact the final age determination: hydration rim measurement precision, estimation of temperature history (effective hydration temperature), variation in the experimentally determined source-specific diffusion coefficient, and intra-source variability in intrinsic water content; all of which contribute to the final age uncertainty. The fourth variable, the variability in obsidian structural water content, and its influence on the dating outcome, is the factor explored here.
This work deals with the application of archaeological methods and modern methodologies of point clouds survey to structural analysis, with the purpose of creating a series of products, such as Elevation Maps, Orthophotos, 3D Models, in order to highlight the building and mechanical past of the examined buildings and to further the knowledge of the territory's seismic history. These products are to be used as a base for the study of the cognitive process of the material structure, the constructive techniques and the restoration of a specific context, of importance for future vulnerability and restoration analyses. The present paper will focus on the trinomial “technology-archaeoseismology-earthquakes” in order to bring to the attention of the scientific community the advantages and critical issues of an innovative point of view. The historical center of Florence and, specifically, the church of San Remigio, has been chosen as a case study to illustrate the methodology.In a vast, heterogenous and averagely seismic landscape like the Italian one, characterized by a significant presence of historical edifices that require safeguarding, the recent issuing by the MiBAC of the “Linee Guida per la valutazione e riduzione del rischio sismico del patrimonio culturale allineate alle nuove Norme tecniche per le costruzioni (d.m. 14 gennaio 2008) (Mibac, 2011)” has attempted to regulate interventions in the field of architecture (Pugi and Galassi, 2013; Paradiso et al., 2014a). This paper specifically outlines the need of a multidisciplinary approach towards the study of past monuments, achieved from the interaction of analyses conducted by different disciplines related to the fields of the sciences and the humanities. In this panorama, the systematic use of archaeoseismic research applied to historical buildings in broad territorial districts (Arrighetti, 2015, 2016) has produced new groundbreaking data obtained through archaeological methods and tools that are in themselves perfectly integrable with data provided by other disciplines (e.g. historical seismology, structural engineering, earth sciences, restoration, etc.). In a preliminary stage of investigation, the operational praxis developed for the analysis of such contexts has considered that specific attention should be given to the study of seismologic databases and seismic maps. This is followed, in the operational phase, by the integration of archaeological data and macro-features reading (Doglioni et al., 1994; Doglioni, 2003; Doglioni and Mazzotti, 2007) along with the documented evidence and analysis of disruptions and restorations, consequently leading to a stratigraphic “breakdown” and interpretation of the constructive history and mechanics of the buildings. SECTION Research aim
The late antique/early medieval age in Central Italy is a well-suited context to verify the implications of the end of the natron glass supplies, and to explore the beginnings of the new plant-ash glass technology. We present the results of a LA-ICP-MS analysis campaign conducted on archaeological glass finds excavated at the Santa Maria della Scala hospital site in Siena and in Donoratico. This provided us with major, minor and trace element quantitative data for 49 glass samples belonging to drinking vessels and lamps, dated mainly between the 5th and the 8th century. On the basis of these data, we have sought to identify the working processes and possible glassware trade that are reflected in the glass composition. Major and minor element contents revealed that most samples, also at the later boundary of the explored timeframe, fit well within known late Roman glass classifications (e.g. HIMT, Levantine). Trace element analysis provided further information on the raw materials that were used in the glassmaking process, indicating the use of coastal sands as a silica source and allowing us to formulate different hypotheses on the materials used for the colouring process.This study aims to increase the insight into the working processes and glassware trade in Italy during the transition time between the end of the Roman period and the beginning of the Middle Ages, including the type of materials used for glassmaking and for improving its quality. Both Roman and medieval glass have been extensively studied in the past (see e.g. Janssens, 2013) and further studies have been dedicated to the exploration of the effective onset of the new processes leading to typical Medieval glass compositions„ i.e., based on vegetable ash as a flux in place of natural evaporites. Recent studies on late Antique and early Medieval glass from Italy have revealed co-existence of several glass compositions from the 4-5th century onwards, almost all natron-based and mainly compatible with the previously identified HIMT, Strong HIMT, Levantine and Série 3.2 glass groups, next to earlier Roman blue-green glass (Gliozzo et al., 2015, 2016, 2017; Silvestri et al., 2017; Gallo et al., 2014; Maltoni et al., 2015, 2016; Silvestri and Marcante, 2011), while a few objects dated to the 6-7th century, and 6-11th, excavated in the Tuscan site of San Genesio already featured a vegetable ash glass composition (Cagno et al., 2012). In order to expand our knowledge on this transition, in particular in the Tuscan area, we have selected glass samples that are precisely dated to the period from the 5th to 8th century and originate from the Santa Maria della Scala site in Siena, Italy, next to a small set of later, clearly early medieval glasses dated to the 10–11th century from the same site and to the 11-12th century from the village of Donoratico, in the same region. SECTION The historical sites of Santa Maria della Scala and Donoratico
Mechatronics design is complex by nature as it involves a large number of couplings and interdependencies between subsystems and components alongside a variety of sometimes contradicting objectives and design constraints. Mechatronics design activity requires cross-disciplinary and multi-objective thinking. In this paper, a fuzzy-based approach for the modeling of a unified performance evaluation index in the detailed design phase is presented. This index acts as a multidisciplinary objective function aggregating all the design criteria and requirements from various disciplines and subsystems while taking into account the interactions and correlations among the objectives. Then this function is optimized using a particle swarm optimization algorithm alongside all the constraints facing each subsystem. As an application, the mechatronics design of a vision-guided quadrotor unmanned aerial vehicle is carried out to demonstrate the effectiveness of the proposed method. Thus, a thorough modeling of system dynamics, structure, aerodynamics, flight control and visual servoing system is carried out to provide the designer with all necessary design variables and requirements. The final results and related computer simulations show the effectiveness of the proposed method in finding solutions for an optimal mechatronic design.Mechatronic systems are multidisciplinary products, that incorporate an interactive and synergistic application of various domains such as mechanics, electronics, controls, and computer engineering. Due to the large number of couplings and dynamic interdependencies between subsystems and components, the design of mechatronic systems is considered to be a challenging and complex task, which requires a cross disciplinary design thought process (Torry-Smith et al., 2013; van Amerongen, 2003). This calls for a more systematic and multi-objective design approach to mechatronics (Mohebbi et al., 2014d). More precisely, a concurrent and integrated design method is needed to obtain more efficient, reliable and flexible products in less complex ways and at a lower cost (Mohebbi et al., 2014a). A number of research efforts have demonstrated that designing the structure and control concurrently, improves the system’s performance and efficiency (Cruz-Villar et al., 2009; de Silva and Behbahani, 2013; Li et al., 2001; Van Brussel, 1996; Zhang et al., 1999). Although, in most of these efforts, the mechanical structures of the system were usually determined in advance without considering the future aspects of the controller design. Therefore, a perfect control action may be far from practical concerns, due to limitations imposed by the poorly designed mechanical structure.
The quality of meat products is traditionally assessed by chemical or sensorial analysis, which are time consuming, need specialized technicians and destroy the products. The development of new technologies to monitor meat pieces using non-destructive methods in order to establish their quality is earning importance in the last years. An increasing number of studies have been carried out on meat pieces combining Magnetic Resonance Imaging (MRI), texture descriptors and regression techniques to predict several physico-chemical or sensorial attributes of the meat, mainly different types of pig ham and loins. In spite of the importance of the problem, the conclusions of these works are still preliminary because they only use the most classical texture descriptors and regressors instead of stronger methods, and because the methodology used to measure the performance is optimistic. In this work, we test a wide range of texture analysis techniques and regression methods using a realistic methodology to predict several physico-chemical and sensorial attributes of different meat pieces of Iberian pigs. The texture descriptors include statistical techniques, like Haralick descriptors, local binary patterns, fractal features and frequential descriptors, like Gabor or wavelet features. The regression techniques include linear regressors, neural networks, deep learning, support vector machines, regression trees, ensembles, boosting machines and random forests, among others. We developed experiments using 15 texture feature vectors, 28 regressors over 4 datasets of Iberian pig meat pieces to predict 39 physico-chemical and sensorial attributes, summarizing16,380 experiments. There is not any combination of texture vector and regressor which provides the best result for all attributes tested. Nevertheless, all these experiments provided the following conclusions: (1) the regressor performance, measured using the squared correlation (R2), is from good to excellent (above 0.5625) for 29 out of 39 attributes tested; (2) the WAPE (Weighted Absolute Percent Error) is lower than 2% for 32 out of 37 attributes; (3) the dispersion in computer predictions around the true attributes is lower or similar than the dispersion in the labeling expert’s for the majority of attributes (85%); and (4) differences between predicted and true values are not statistically significant for 29 out of 37 attributes using the Wilcoxon ranksum statistical test. We can conclude that these results provide a high reliability for an automatic system to predict the quality of meat pieces, which may operate on-line in the meat industries in the future.Hams and loins from Iberian pig, which is an autochthonous porcine breed developed traditionally in the SouthWest of Spain, are one of the most valuable meat products in this country. This is mainly ascribed to their exceptional sensorial attributes that depend on characteristics of raw material and processing conditions. Thus, not only characteristics of fresh pieces but also their modifications during the processing are important parameters to control the technological process of dry-cured hams and loins (Pérez-Palacios et al., 2011b). Temperature and relative humidity conditions during the processing lead to dehydration and, hence, to weight loss and a water activity decrease. Meat from Iberian pigs should contain plenty of intramuscular fat, which is an important characteristic, due to its positive influence on quality parameters on the final product, such as marbling, juiciness, odor, and aroma (Ruiz et al., 2002). The determination of salt content is important from a microbiological point of view, but it also influences on the texture and flavor of the final product (Toldrá et al., 1997). Color is also one of the most interesting characteristics of meat products (Resurrección, 2003), and for dry cured meat products it is the most relevant appearance property (Gandemer, 2002). It is also important to study the final sensory quality of Iberian meat products, considering their most distinguished sensorial attributes, such as appearance, odor, taste and flavor (García and Carrapiso, 2001). Scientific studies on these meat products have carried out the sensory analysis objectively, with trained panellists and following standardized conditions.
Repair and maintenance services are among the most lucrative aspects of the entire automobile business chain. However, in the context of fierce competition, customer churns have led to the bankruptcy of several 4S (sales, spare parts, services, and surveys) shops. In this regard, a six-year dataset is utilized to study customer behaviors to aid managers identify and retain valuable but potential customer churn through a customized retention solution. First, we define the absence and presence behaviors of customers and thereafter generate absence data according to customer habits; this makes it possible to treat the customer absence prediction problem as a classification problem. Second, the repeated absence and presence behaviors of customers are considered as a whole from a lifecycle perspective. A modified recurrent neural network (RNN-2L) is proposed; it is more efficient and reasonable in structure compared with traditional RNN. The time-invariant customer features and the sequential lifecycle features are handled separately; this provides a more sensible specification of the RNN structure from a behavioral interpretation perspective. Third, a customized retention solution is proposed. By comparing the proposed model with those that are conventional, it is found that the former outperforms the latter in terms of area under the curve (AUC), confusion matrix, and amount of time consumed. The proposed customized retention solution can achieve significant profit increase. This paper not only elucidates the customer relationship management in the automobile aftermarket (where the absence and presence behaviors are infrequently considered), but also presents an efficient solution to increase the predictive power of conventional machine learning models. The latter is achieved by considering behavioral and business perspectives.Background and motivation
In this work we propose a minimax probability extreme learning machine framework (MPME), which combines the benefits of minimax probability machine (MPM) with extreme learning machine (ELM). For binary classification problems, we illustrate that the proposed MPME can be interpreted geometrically by minimizing the maximum of Mahalanobis distances to the two classes. Then two variants of the MPME are presented based on the l2-norm loss and l1-norm loss functions (called LSEMPME and LADMPME) respectively. Without making specific assumption on the data distribution, the proposed methods can provide explicit upper-bounds for the generalization error, moreover the LSEMPME and LADMPME minimize empirical risk simultaneously. The decision hyperplanes of the proposed methods pass through the origin in ELM feature space with few decision variables. By using the multivariate Chebyshev–Cantelli inequality, all the proposed problems can be reformulated as second-order cone programming (SOCP) with global solutions. Furthermore, numerical experiments have been carried out on two databases that are drawn from UCI benchmark database and a practical application database. First, the proposed methods are evaluated for a practical application consisting on the analysis of licorice seeds using near-infrared spectral (NIR) data. Experiments in six different spectral regions illustrate that the proposed methods can improve generalization in most cases. Then the proposed methods are evaluated on benchmark datasets. In comparison with traditional methods including MPM, ELM and support vector machine (SVM), experiments show that the proposed methods achieve comparable results in generalization. With few decision variables, the proposed methods are easy to implement for nonlinear classification and to estimate a lower-bound on the prediction accuracy.Machine learning algorithms have been widely used in various fields such as data mining and pattern recognition, where support vector machine (SVM) (Vapnik, 1998; Frenay and Verleysen, 2010; Shawe-Taylor and Sun, 2011; Liu et al., 2012; Yang and Dong, 2018), neural network algorithms (Li et al., 2015; Kolbaek et al., 2017), ensemble learning (Rojarath et al., 2017), deep learning (Deng and Yu, 2014) and logistic regression (Yang and Qian, 2016) have been successfully used in classification, regression, function approximation, feature selection, feature extraction and so on.
Locating the subtle and uneven deposition of human activities across the landscape continues to challenge archaeologists. Existing tools (e.g. excavation, shovel testing, pedestrian survey, and terrestrial geophysics) have proven effective at locating many types of archaeological features but remain time-consuming and difficult to undertake on densely vegetated or topographically complex terrain. As a result of these limitations, key aspects of past communities remain largely outside of archaeological detection and interpretation. This flattening of past lifeways not only affects broader understandings of these communities, but can also negatively impact the preservation of archaeological sites. This paper presents the detection of archaeological features through an analysis of drone-acquired thermal, multispectral, and visible light imagery, alongside historical aerial photography, in the area surrounding Middle Grant Creek (11WI2739), a late prehistoric village located at Midewin National Tallgrass Prairie in Will County, IL. Our investigations discovered a probable housing area and a ritual enclosure, increasing the area of the site from 3.4 ha to 20 ha. The proposed housing and ritual areas of this village also help contextualize finds from the ongoing archaeological excavations at Middle Grant Creek. More broadly, results demonstrate the valuable contributions that these relatively new archaeological survey methods have in shaping our understandings of the archaeological landscape and highlight the importance of integrating them into the archaeological toolkit.Archaeologists agree that narrowly defined sites, areas with high concentrations of artifacts, are small components of the complex and textured ways in which people make use of and interact with broader landscapes (e.g. Basso, 1996; Erickson, 2008; Gordillo, 2014; Ingold, 1993; Lelièvre, 2017; Yaeger and Canuto, 2000). However, documenting the varied traces of past human activities, which are often spread over large areas and produce little readily identifiable surface signatures, remains a perennial challenge. These challenges are heightened in locations where materials used for prehistoric construction (e.g. wood) easily degrade and where industry, housing, and agriculture have heavy impacts. Locating the often fragmented archaeological record in these contexts has previously required time intensive and expensive survey techniques, like pedestrian survey and shovel testing, that are poorly suited to documenting broadly dispersed archaeological landscape features.
A study about pre–Hispanic ceramics (pastes and pigments) coming from archaeological sites of the Valley of Metzontla, Mexico (Iglesia Vieja San Sebastian, Coronilla Hill, Agua Socoya Hill and Metzontla Hill), from sites of the vicinity (Cutha, Teteles de Santo Nombre, and Tehuacan Viejo), and from one more distant site (Champayan) is presented. Raw materials such as clays, tempers and pigments were studied as well. Nuclear activation analyses, energy dispersion spectroscopy, and X–ray diffraction were applied. According to the chemical composition of the pastes six groups of ceramics were identified: one of them includes pre–Hispanic Popoloca Orange ceramic, one other group is similar to pre–Hispanic Brown ceramic and present–day Los Reyes Metzontla's samples, and several other exemplars were quite different to these groups. The brown ethnographic pottery of Los Reyes Metzontla town is chemically identical to those pre–Hispanic pottery; apparently raw materials have been the same for a long period of time, whereas Popoloca Orange ceramic is no longer manufactured in the region. Similarities and differences were found among the ceramics of the Metzontla Valley and those of the sites located within the Popoloca area and beyond.The term Popoloca became confused because of the connotation given by the Aztecs to many groups that were not Nahuas (Jäcklein, 1991). At present this denomination (ib.) applies to indigenous groups of Puebla (Mexico) that linguistically belong to the Otomangue family. The term “Historical Popoloca” used by Jäcklein refers to the group that existed during the Early Classic period (ca. 200 CE), and he considers (ib.) to the proto–Otomangues to be the predecessors, who in ca. 7000 BCE began the domestication of several plants. The Popoloca people lived in the southern and central regions of the State of Puebla, the northern zone of Oaxaca, and perhaps in the eastern zone of Guerrero and the south of Tlaxcala. However, so far, insufficient studies have been conducted regarding about this ethnic group.
Feature subset selection is an essential machine learning approach aimed at the process of dimensionality reduction of the input space. By removing irrelevant and/or redundant variables, not only it enhances model performance, but also facilitates its improved interpretability. The fuzzy set and the rough set are two different but complementary theories that apply the fuzzy rough dependency as a criterion for performing feature subset selection. However, this concept can only maintain a maximal dependency function. It cannot preferably illustrate the differences in object classification and does not fit a particular dataset well. This problem was handled by using a fitting model for feature selection with fuzzy rough sets. However, intuitionistic fuzzy set theory can deal with uncertainty in a much better way when compared to fuzzy set theory as it considers positive, negative and hesitancy degree of an object simultaneously to belong to a particular set. Therefore, in the current study, a novel intuitionistic fuzzy rough set model is proposed for handling above mentioned problems. This model fits the data well and prevents misclassification. Firstly, intuitionistic fuzzy decision of a sample is introduced using neighborhood concept. Then, intuitionistic fuzzy lower and upper approximations are constructed using intuitionistic fuzzy decision and parameterized intuitionistic fuzzy granule. Furthermore, a new dependency function is established. Moreover, a greedy forward algorithm is given using the proposed concept to calculate reduct set. Finally, this algorithm is applied to the benchmark datasets and a comparative study with the existing algorithm is presented. From the experimental results, it can be observed that the proposed model provides more accurate reduct set than existing model.Millions of data is generated in multiple scenarios, including weather, census, health care, government, social networking, production, business, and scientific research. Such high dimensional data may increase inefficiency of classifiers, as they possess several irrelevant or redundant features. Therefore, it is necessary to preprocess the dataset before applying any classification algorithm. Feature selection is a preprocessing step to remove irrelevant and/or redundant features and offers more concise and explicit descriptions of data. Feature selection has got wide applications in data mining, signal processing, bioinformatics, machine learning, etc. (Iannarilli and Rubin, 2003; Jaeger et al., 2002; Jain et al., 2000; Kohavi and John, 1997; Kwak and Chong-Ho, 2002; Langley, 1994; Webb and Copsey, 2011; Xiong et al., 2001).
The fixed-dose combination of sofosbuvir/velpatasvir was highly efficacious in patients infected with genotype (GT)1–6 hepatitis C virus (HCV) in the ASTRAL studies. This analysis evaluated the impact of baseline resistance-associated substitutions (RASs) on treatment outcome and emergence of RASs in patients infected with HCV GT1-6 who were treated with sofosbuvir/velpatasvir. Non-structural protein 5A and 5B (NS5A and NS5B) deep sequencing was performed at baseline and at the time of relapse for all patients treated with sofosbuvir/velpatasvir for 12 weeks (n = 1,778) in the ASTRAL-1–3, ASTRAL-5 and POLARIS-2–3 studies. Patients with 37 known and 19 novel HCV subtypes were included in these analyses. Overall, 28% (range 9% to 61% depending on genotype) had detectable NS5A class RASs at baseline, using a 15% sequencing assay cut-off. There was no significant effect of baseline NS5A class RASs on sustained virologic response at week 12 (SVR12) with sofosbuvir/velpatasvir; the SVR12 rate in the presence of NS5A class RASs was 100% and 97%, in patients with GT1a and GT1b infection, respectively, and 100% in patients with GT2 and GT4–6 infections. In GT3 infection, the SVR rate was 93% and 98% in patients with and without baseline NS5A class RASs, respectively. The overall virologic failure rate was low (20/1,778 = 1.1%) in patients treated with sofosbuvir/velpatasvir. Single NS5A class resistance was observed at virologic failure in 17 of the 20 patients. Sofosbuvir/velpatasvir taken for 12 weeks once daily resulted in high SVR rates in patients infected with GT1–6 HCV, irrespective of baseline NS5A RASs. NS5A inhibitor resistance, but not sofosbuvir resistance, was detected in the few patients with virologic failure. These data highlight the high barrier to resistance of this regimen for the treatment of chronic HCV across all genotypes in the vast majority of patients.Hepatitis C virus (HCV) infection is a global health burden with an estimated 71–150 million individuals infected worldwide.1–3 Development of direct-acting antivirals (DAAs) in recent years has dramatically enhanced sustained virologic response (SVR) rates in HCV genotype (GT)1 chronically infected patients.4
More than 90% of cases of hepatocellular carcinoma (HCC) occur in patients with cirrhosis, of which alcohol is a major cause. The CIRRAL cohort aimed to assess the burden of complications in patients with alcoholic cirrhosis, particularly the occurrence of HCC. Patients with biopsy-proven compensated alcoholic cirrhosis were included then prospectively followed. The main endpoint was the incidence of HCC. Secondary outcomes were incidence of hepatic focal lesions, overall survival (OS), liver-related mortality and event-free survival (EFS). From October 2010 to April 2016, 652 patients were included in 22 French and Belgian centers. During follow-up (median 29 months), HCC was diagnosed in 43 patients. With the limitation derived from the uncertainty of consecutive patients’ inclusion and from a sizable proportion of dropouts (153/652), the incidence of HCC was 2.9 per 100 patient-years, and one- and two-year cumulative incidences of 1.8% and 5.2%, respectively. Although HCC fulfilled the Milan criteria in 33 cases (77%), only 24 patients (56%) underwent curative treatment. An explorative prognostic analysis showed that age, male gender, baseline alpha-fetoprotein, bilirubin and prothrombin were significantly associated with the risk of HCC occurrence. Among 73 deaths, 61 had a recorded cause and 27 were directly attributable to liver disease. At two years, OS, EFS and cumulative incidences of liver-related deaths were 93% (95% CI 90.5–95.4), 80.3% (95% CI 76.9–83.9), and 3.2% (95% CI 1.6–4.8) respectively. This large prospective cohort incompletely representative of the whole population with alcoholic cirrhosis showed: a) an annual incidence of HCC of up to 2.9 per 100 patient-years, suggesting that surveillance might be cost effective in these patients; b) a high proportion of HCC detected within the Milan criteria, but only one-half of detected HCC cases were referred for curative treatments; c) a two-year mortality rate of up to 7%.Primary liver cancer has a high incidence in Europe, especially in France, which has almost 9,000 cases per year.1 In more than 90% of cases, hepatocellular carcinoma (HCC) occurs in patients with cirrhosis. Alcohol is the leading cause of the underlying cirrhosis that is associated with HCC in France; it is responsible for more than 60% of the cases,2 which is much higher than the proportion of cases from hepatitis B and C and non-alcoholic steatohepatitis. Although precise data are lacking, as France has lost its longstanding European leadership in alcohol consumption, one can speculate that alcoholic cirrhosis is currently the main cause of HCC occurrence in Europe, and it will gain an even more important predominance as alcoholic consumption is on the rise in many countries, while active viral infection is on the decline.
The effectiveness of direct-acting antivirals (DAAs) against hepatitis C virus (HCV), following successful treatment of early hepatocellular carcinoma (HCC), has been studied extensively. However, the benefit in terms of overall survival (OS) remains to be conclusively demonstrated. The aim of this study was to assess the impact of DAAs on OS, HCC recurrence, and hepatic decompensation. We prospectively enrolled 163 consecutive patients with HCV-related cirrhosis and a first diagnosis of early Barcelona Clinic Liver Cancer stage 0/A HCC, who had achieved a complete radiologic response after curative resection or ablation and were subsequently treated with DAAs. DAA-untreated patients from the ITA.LI.CA. cohort (n = 328) served as controls. After propensity score matching, outcomes of 102 DAA-treated (DAA group) and 102 DAA-untreated patients (No DAA group) were compared. In the DAA group, 7/102 patients (6.9%) died, HCC recurred in 28/102 patients (27.5%) and hepatic decompensation occurred in 6/102 patients (5.9%), after a mean follow-up of 21.4 months. OS was significantly higher in the DAA group compared to the No DAA group (hazard ratio [HR] 0.39; 95% CI 0.17–0.91; p = 0.03). HCC recurrence was not significantly different between the DAA and No DAA groups (HR 0.70; 95% CI 0.44–1.13; p = 0.15). A significant reduction in the rate of hepatic decompensation was observed in the DAA group compared with the No DAA group (HR 0.32; 95% CI 0.13–0.84; p = 0.02). In the DAA group, sustained virologic response was a significant predictor of OS (HR 0.02; 95% CI 0.00–0.19; p <0.001), HCC recurrence (HR 0.25; 95% CI 0.11–0.57; p <0.001) and hepatic decompensation (HR 0.12; 95% CI 0.02–0.38; p = 0.02). In patients with HCV-related cirrhosis who had been successfully treated for early HCC, DAAs significantly improved OS compared with No DAA treatment.Hepatocellular carcinoma (HCC) is the third leading cause of cancer-related death globally, and the leading cause of mortality in cirrhotic patients, with hepatitis C virus (HCV) being the major risk factor in the Western world and Japan.1 Orthotopic liver transplantation (OLT) is the definitive treatment for HCC and cirrhotic liver, but this approach cannot be offered to all patients due to limited graft availability and rigorous selection criteria.2 Alternative curative treatment options for patients with compensated cirrhosis are surgical resection and loco-regional ablation of early HCC (i.e. Barcelona Clinic Liver Cancer [BCLC] stage 0/A).2
The development of non-invasive liver fibrosis tests may enable earlier identification of patients with non-alcoholic fatty liver disease (NAFLD) requiring referral to secondary care. We developed and evaluated a pathway for the management of patients with NAFLD, aimed at improving the detection of cases of advanced fibrosis and cirrhosis, and avoiding unnecessary referrals. This was a prospective longitudinal cohort study, with analyses performed before and after introduction of the pathway, and comparisons made to unexposed controls. We used a 2-step algorithm combining the use of Fibrosis-4 score followed by the ELF™ test if required. In total, 3,012 patients were analysed. Use of the pathway detected 5 times more cases of advanced fibrosis (Kleiner F3) and cirrhosis (odds ratio [OR] 5.18; 95% CI 2.97–9.04; p <0.0001), while reducing unnecessary referrals from primary care to secondary care by 81% (OR 0.193; 95% CI 0.111–0.337; p <0.0001). Although it was used for only 48% of referrals, significant benefits were observed in practices exposed to the pathway compared to those which were not, with unnecessary referrals falling by 77% (OR 0.23; 95% CI 0.658–0.082; p = 0.006) and a 4-fold improvement in detection of cases of advanced fibrosis and cirrhosis (OR 4.32; 95% CI 1.52–12.25; p = 0.006). Compared to referrals made before the introduction of the pathway, unnecessary referrals fell from 79/83 referrals (95.2%) to 107/152 (70.4%), representing an 88% reduction in unnecessary referrals when the pathway was followed (OR 0.12; 95% CI 0.042–0.349; p <0.0001). The use of non-invasive blood tests for liver fibrosis improves the detection of advanced fibrosis and cirrhosis, while reducing unnecessary referrals in patients with NAFLD. This strategy improves resource use and benefits patients.Non-alcoholic fatty liver disease (NAFLD) is the commonest cause of deranged liver blood tests (LFTs) in primary care in Europe and North America,1 with an estimated prevalence of 25–30% in the adult population.2 Only a minority of people with NAFLD (5%) develop clinically significant liver disease,2 but the burden is such that NAFLD is predicted to be the leading indication for liver transplantation within a decade.3
Eight-week glecaprevir/pibrentasvir leads to high rates of sustained virological response at post-treatment week 12 (SVR12) across HCV genotypes (GT) 1–6 in treatment-naïve patients without cirrhosis. We evaluated glecaprevir/pibrentasvir once daily for 8 weeks in treatment-naïve patients with compensated cirrhosis. EXPEDITION-8 was a single-arm, multicenter, phase IIIb trial. The primary and key secondary efficacy analyses were to compare the lower bound of the 95% CI of the SVR12 rate in i) patients with GT1,2,4–6 in the per protocol (PP) population, ii) patients with GT1,2,4–6 in the intention-to-treat (ITT) population, iii) patients with GT1–6 in the PP population, and iv) patients with GT1–6 in the ITT population, to pre-defined efficacy thresholds based on historical SVR12 rates for 12 weeks of glecaprevir/pibrentasvir in the same populations. Safety was also assessed. A total of 343 patients were enrolled. Most patients were male (63%), white (83%), and had GT1 (67%). The SVR12 rate in patients with GT1–6 was 99.7% (n/N = 334/335; 95% CI 98.3–99.9) in the PP population and 97.7% (n/N = 335/343; 95% CI 96.1–99.3) in the ITT population. All primary and key secondary efficacy analyses were achieved. One patient (GT3a) experienced relapse (0.3%) at post-treatment week 4. Common adverse events (≥5%) were fatigue (9%), pruritus (8%), headache (8%), and nausea (6%). Serious adverse events (none related) occurred in 2% of patients. No adverse event led to study drug discontinuation. Clinically significant laboratory abnormalities were infrequent. Eight-week glecaprevir/pibrentasvir was well tolerated and led to a similarly high SVR12 rate as the 12-week regimen in treatment-naïve patients with chronic HCV GT1–6 infection and compensated cirrhosis. Trial registration: ClinicalTrials.gov, NCT03089944.Chronic HCV infection is a major public health threat, with an estimated 71 million individuals affected worldwide.1 In 2015, it was estimated that only 20% of these individuals were aware of their HCV infection.1 In 2016, approximately 13% of those aware of having chronic HCV infection were being treated.2 Approximately 15–30% of patients with chronic HCV infection will develop cirrhosis within 20 years3 and, if left untreated, these patients are at risk of developing hepatic decompensation and hepatocellular carcinoma, ultimately leading to increased liver-related mortality.4 Successful treatment of chronic HCV infection can significantly reduce disease progression, as well as rates of HCV transmission.5
Recreational ketamine use has emerged as an important health and social issue worldwide. Although ketamine is associated with biliary tract damage, the clinical and radiological profiles of ketamine-related cholangiopathy have not been well described. Chinese individuals who had used ketamine recreationally at least twice per month for six months in the previous two years via a territory-wide community network of charitable organizations tackling substance abuse were recruited. Magnetic resonance cholangiography (MRC) was performed, and the findings were interpreted independently by two radiologists, with the findings analysed in association with clinical characteristics. Among the 343 ketamine users referred, 257 (74.9%) were recruited. The mean age and ketamine exposure duration were 28.7 (±5.8) and 10.5 (±3.7) years, respectively. A total of 159 (61.9%) had biliary tract anomalies on MRC, categorized as diffuse extrahepatic dilatation (n = 73), fusiform extrahepatic dilatation (n = 64), and intrahepatic ductal changes (n = 22) with no extrahepatic involvement. Serum alkaline phosphatase (ALP) level (odds ratio [OR] 1.007; 95% CI 1.002–1.102), lack of concomitant recreational drug use (OR 1.99; 95% CI 1.11–3.58), and prior emergency attendance for urinary symptoms (OR 1.95; 95% CI 1.03–3.70) had high predictive values for biliary anomalies on MRC. Among sole ketamine users, ALP level had an AUC of 0.800 in predicting biliary anomalies, with an optimal level of ≥113 U/L having a positive predictive value of 85.4%. Cholangiographic anomalies were reversible after ketamine abstinence, whereas decompensated cirrhosis and death were possible after prolonged exposure. We have identified distinctive MRC patterns in a large cohort of ketamine users. ALP level and lack of concomitant drug use predicted biliary anomalies, which were reversible after abstinence. The study findings may aid public health efforts in combating the growing epidemic of ketamine abuse.Recreational inhalation of ketamine is emerging as a major global social and health issue.1,2 Although ketamine, an N-methyl-d-aspartate receptor antagonist, has medical uses in anaesthesia and chronic pain control, its highly addictive nature has led to a massive increase in recreational consumption worldwide. Because of the ease of production and low cost, the non-medical use of ketamine is increasing especially in East and South-East Asia, with its lifetime prevalence in the general population ranging from 0.3% to 2.0%,3 comprising up to 39.7% of total recreational drugs users in these regions.4 The self-reported recreational use of ketamine in Western countries, including the UK, Australia, and Canada, is also increasing.5,6 From 2008 to 2014, law enforcement seizures of ketamine worldwide increased by more than threefold.3
Population-level evidence for the impact of direct-acting antiviral (DAA) therapy on hepatitis C virus (HCV)-related disease burden is lacking. We aimed to evaluate trends in HCV-related decompensated cirrhosis and hepatocellular carcinoma (HCC) hospitalisation, and liver-related and all-cause mortality in the pre-DAA (2001–2014) and DAA therapy (2015–2017) eras in New South Wales, Australia. HCV notifications (1993–2016) were linked to hospital admissions (2001–2017) and mortality (1995–2017). Segmented Poisson regressions and Poisson regression were used to assess the impact of DAA era and factors associated with liver-related mortality, respectively. Among 99,910 people with an HCV notification, 3.8% had a decompensated cirrhosis diagnosis and 1.8% had an HCC diagnosis, while 3.3% and 10.5% died of liver-related and all-cause mortality, respectively. In the pre-DAA era, the number of decompensated cirrhosis and HCC diagnoses, and liver-related and all-cause mortality consistently increased (incidence rate ratios 1.04 [95% CI 1.04–1.05], 1.08 [95% CI 1.07–1.08], 1.07 [95% CI 1.06–1.07], and 1.05 [95% CI 1.04–1.05], respectively) over each 6-monthly band. In the DAA era, decompensated cirrhosis diagnosis and liver-related mortality numbers declined (incidence rate ratios 0.97 [95% CI 0.95–0.99] and 0.96 [95% CI 0.94–0.98], respectively), and HCC diagnosis and all-cause mortality numbers plateaued (incidence rate ratio 1.00 [95% CI 0.97–1.03] and 1.01 [95% CI 1.00–1.02], respectively) over each 6-monthly band. In the DAA era, alcohol-use disorder (AUD) was common in patients diagnosed with decompensated cirrhosis and HCC (65% and 46% had a history of AUD, respectively). AUD was independently associated with liver-related mortality (incidence rate ratio 3.35; 95% CI 3.14–3.58). In the DAA era, there has been a sharp decline in liver disease morbidity and mortality in New South Wales, Australia. AUD remains a major contributor to HCV-related liver disease burden, highlighting the need to address comorbidities.The advent of highly curative, tolerable, short-duration direct-acting antiviral (DAA) therapy for hepatitis C virus (HCV) infection has transformed clinical management, and provided great optimism for the global HCV response.1 The World Health Organization (WHO) have developed a global health strategy on viral hepatitis that incorporates key service and impact targets, including declines in HCV-related mortality of 10% by 2020 and 65% by 2030.2
Myeloid cell leukemia 1 (MCL1), a prosurvival member of the BCL2 protein family, has a pivotal role in human cholangiocarcinoma (CCA) cell survival. We previously reported that fibroblast growth factor receptor (FGFR) signalling mediates MCL1-dependent survival of CCA cells in vitro and in vivo. However, the mode and mechanisms of cell death in this model were not delineated. Human CCA cell lines were treated with the pan-FGFR inhibitor LY2874455 and the mode of cell death examined by several complementary assays. Mitochondrial oxidative metabolism was examined using a XF24 extracellular flux analyser. The efficiency of FGFR inhibition in patient-derived xenografts (PDX) was also assessed. CCA cells expressed two species of MCL1, a full-length form localised to the outer mitochondrial membrane, and an N terminus-truncated species compartmentalised within the mitochondrial matrix. The pan-FGFR inhibitor LY2874455 induced non-apoptotic cell death in the CCA cell lines associated with cellular depletion of both MCL1 species. The cell death was accompanied by failure of mitochondrial oxidative metabolism and was most consistent with necrosis. Enforced expression of N terminus-truncated MCL1 targeted to the mitochondrial matrix, but not full-length MCL1 targeted to the outer mitochondrial membrane, rescued cell death and mitochondrial function. LY2874455 treatment of PDX-bearing mice was associated with tumour cell loss of MCL1 and cell necrosis. FGFR inhibition induces loss of matrix MCL1, resulting in cell necrosis. These observations support a heretofore unidentified, alternative MCL1 survival function, namely prevention of cell necrosis, and have implications for treatment of human CCA.Cholangiocarcinoma (CCA) is a lethal hepatobiliary malignancy with limited therapeutic options.1,2 Advances in CCA therapy will require an understanding of oncogenic signalling networks that contribute to CCA pathogenesis and could be disrupted therapeutically. Similar to other malignancies, a cardinal feature of CCA is inhibition of cell death pathways that are engaged by oncogenic signalling pathways.3 Members of the BCL2 gene family encode proteins that regulate the mitochondrial or intrinsic apoptotic pathway.4 Of the BCL2 prosurvival members, myeloid cell leukemia 1 (MCL1) is most frequently amplified and overexpressed in CCA,5–7 and has a pivotal role in CCA cell survival.8 Hence, targeting MCL1 is a strategy for the treatment of CCA and other malignancies.9,10
Treatment with nucleos(t)ide analogues (NA) leads to hepatitis B virus (HBV) DNA suppression in most patients with chronic hepatitis B (CHB), but HBV surface antigen (HBsAg) loss rates are low. Upon NA discontinuation, HBV DNA can return rapidly with ensuing alanine aminotransferase flares and induction of cytokines. Several studies reported higher HBsAg loss rates after stopping therapy, but at present it is unclear if cell-mediated immune responses are altered after treatment discontinuation. The aim of this study was to characterise T cell responses during the early phase of virological relapse, following discontinuation of NA therapy in HBeAg-negative patients. A total of 15 HBeAg-negative patients with CHB on long-term NA treatment were included in a prospective study and subjected to structured NA discontinuation. T cell responses were studied at the end of NA therapy and 4, 8 and 12 weeks thereafter. The T cell phenotype of patients with CHB on long-term NA therapy was markedly different compared to healthy individuals, but was only slightly altered after discontinuation of therapy. T cells from patients with HBsAg loss expressed low levels of KLRG1 and PD-1 at all time-points and high levels of Ki-67 and CD38 at week 12 after treatment cessation. In vitro peptide stimulated HBV-specific T cell responses were increased in several patients after NA cessation. Blocking of PD-L1 further enhanced HBV-specific T cell responses, especially after discontinuation of therapy. Relapse of active HBV replication after stopping therapy may trigger an immunological environment that enhances the responsiveness of HBV-specific T cells in vitro. Together with other immune interventions, this approach might be of interest for the development of novel therapeutic options to induce HBsAg loss in CHB.Hepatitis B virus (HBV) chronically infects around 250 million individuals worldwide and can cause liver cirrhosis as well as hepatocellular carcinoma (HCC).1 The treatment options for chronic hepatitis B (CHB) are either pegylated interferon-alfa (PEG-IFNα) or nucleos(t)ide analogues (NA). PEG-IFNα has the advantage of finite treatment with higher rates of anti-HBs seroconversion of around 10%, but is associated with considerable side effects.2 Thus, well-tolerated NAs are most commonly used. NAs efficiently suppress viral replication in most patients, have a beneficial effect on disease progression, and reduce the risk of HCC development.2 Still, NAs only have minor, if any, effect on covalently closed circular DNA (cccDNA). For this reason, functional cure is a rare event and most patients with CHB require life-long therapy, particularly HBV e antigen (HBeAg)-negative patients.2
Progressive familial intrahepatic cholestasis type 3 (PFIC3), for which there are limited therapeutic options, often leads to end-stage liver disease before adulthood due to impaired ABCB4-dependent phospholipid transport to bile. Using adeno-associated virus serotype 8 (AAV8)-mediated gene therapy, we aimed to restore the phospholipid content in bile to levels that prevent liver damage, thereby enabling stable hepatic ABCB4 expression and long-term correction of the phenotype in a murine model of PFIC3. Ten-week-old Abcb4−/− mice received a single dose of AAV8-hABCB4 (n = 10) or AAV8-GFP (n = 7) under control of a liver specific promoter via tail vein injection. Animals were sacrificed either 10 or 26 weeks after vector administration to assess transgene persistence, after being challenged with a 0.1% cholate diet for 2 weeks. Periodic evaluation of plasma cholestatic markers was performed and bile duct cannulation enabled analysis of biliary phospholipids. Liver fibrosis and the Ki67 proliferation index were assessed by immunohistochemistry. Stable transgene expression was achieved in all animals that received AAV8-hABCB4 up to 26 weeks after administration. AAV8-hABCB4 expression restored biliary phospholipid excretion, increasing the phospholipid and cholesterol content in bile to levels that ameliorate liver damage. This resulted in normalization of the plasma cholestatic markers, alkaline phosphatase and bilirubin. In addition, AAV8-hABCB4 prevented progressive liver fibrosis and reduced hepatocyte proliferation for the duration of the study. Liver-directed gene therapy provides stable hepatic ABCB4 expression and long-term correction of the phenotype in a murine model of PFIC3. Translational studies that verify the clinical feasibility of this approach are warranted.Progressive familial intrahepatic cholestasis type 3 (PFIC3) is an autosomal-recessive liver disorder. Patients with PFIC3 present with cholestasis at a young age, which progresses to cirrhosis and end-stage liver disease before adulthood.1–3 PFIC3 is caused by impairment of phosphatidylcholine (PC) translocation to bile by the canalicular membrane protein ATP binding cassette subfamily B member 4 (ABCB4), formerly known as multidrug resistance protein 3 (MDR3), encoded by the ABCB4 gene.4–6 In bile, PC is essential in the formation of mixed micelles with bile salts that protect the lining of the biliary tree from the detergent properties of bile salts. In the absence of PC transport, bile salt–induced cytotoxicity causes progressive destruction of cholangiocytes, mainly of small bile ducts, and hepatocytes leading to intrahepatic cholestasis and progressive liver damage.4,7
Biliary atresia (BA) results from a neonatal inflammatory and fibrosing obstruction of bile ducts of unknown etiology. Although the innate immune system has been linked to the virally induced mechanism of disease, the role of inflammasome-mediated epithelial injury remains largely undefined. Here, we hypothesized that disruption of the inflammasome suppresses the neonatal proinflammatory response and prevents experimental BA. We determined the expression of key inflammasome-related genes in livers from infants at diagnosis of BA and in extrahepatic bile ducts (EHBDs) of neonatal mice after infection with rotavirus (RRV) immediately after birth. Then, we determined the impact of the wholesale inactivation of the genes encoding IL-1R1 (Il1r1−/−), NLRP3 (Nlrp3−/−) or caspase-1 (Casp1−/−) on epithelial injury and bile duct obstruction. IL1R1, NLRP3 and CASP1 mRNA increased significantly in human livers at the time of diagnosis, and in EHBDs of RRV-infected mice. In Il1r1−/− mice, the epithelial injury of EHBDs induced by RRV was suppressed, with dendritic cells unable to activate natural killer cells. A similar protection was observed in Nlrp3−/− mice, with decreased injury and inflammation of livers and EHBDs. Long-term survival was also improved. In contrast, the inactivation of the Casp1 gene had no impact on tissue injury, and all mice died. Tissue analyses in Il1r1−/− and Nlrp3−/− mice showed decreased populations of dendritic cells and natural killer cells and suppressed expression of type-1 cytokines and chemokines. Genes of the inflammasome are overexpressed at diagnosis of BA in humans and in the BA mouse model. In the experimental model, the targeted loss of IL-1R1 or NLRP3, but not of caspase-1, protected neonatal mice against RRV-induced bile duct obstruction.Biliary atresia (BA) results from a rapidly progressing inflammation and obstruction of extrahepatic bile ducts (EHBDs) in early infancy and is the most common indication for pediatric liver transplantation.1–3 The etiology of BA includes environmental triggers in the genetically susceptible host,4,5 followed by an over-activation of the neonatal immune response in the liver and EHBD.6,7 We and others previously reported several cellular (dendritic cells [DCs], natural killer [NK] cells and CD8+ T cells) and molecular (IL-8, IL-15, IFN-γ, TNFα) effectors of bile duct epithelial injury.8–16 Despite this progress, very little is known about how molecular sensors and related circuits regulate the hepatobiliary injury and duct obstruction in BA.
Acute-on-chronic liver failure (ACLF) is a clinical syndrome defined by liver failure on preexisting chronic liver disease and is often associated with bacterial infection with high short-term mortality. Experimental models that fully reproduce ACLF and effective pharmacological therapies are lacking. To mimic ACLF conditions, we developed a severe liver injury model by combining chronic injury (chronic carbon tetrachloride [CCl4] injection), acute hepatic insult (injection of a double dose CCl4), and bacterial infection (intraperitoneal injection of bacteria). Serum and liver samples from patients with ACLF or acute drug-induced liver injury (DILI) were used. Liver injury and regeneration were assessed to ascertain for potential benefits of interleukin-22 (IL-22Fc) administration. This severe liver injury model developed acute-on-chronic liver injury, bacterial infection, multi-organ injury, and high mortality, recapitulating some features of clinical ACLF. Liver regeneration in this model was severely impaired due to the shift from the activation of pro-regenerative IL-6/STAT3 to anti-regenerative IFN-γ/STAT1 pathway. The impaired IL-6/STAT3 activation was due to Kupffer cell inability to produce IL-6; whereas the enhanced STAT1 activation was due to strong innate immune response and subsequent production of IFN-γ. Compared to DILI patients, ACLF patients had higher levels of IFN-γ but lower liver regeneration. IL-22Fc treatment improved survival of the ACLF mice by reversing the STAT1/STAT3 pathway imbalance and enhancing expression of many anti-bacterial genes in a manner involving the anti-apoptotic protein BCL2. Acute-on-chronic liver injury or bacterial infection is associated with impaired liver regeneration due to a shift from the pro-regenerative to anti-regenerative pathways, IL-22Fc therapy reverses this shift and attenuates bacterial infection, thus IL-22Fc may have therapeutic potential for ACLF treatment.Acute-on-chronic liver failure (ACLF) is generally accepted as a clinical syndrome characterized by an acute hepatic insult and rapid deterioration of liver function in patients with pre-existing chronic liver disease in combination with multi-organ failure with high short-term mortality.1-5 The main etiologies of the pre-existing chronic liver disease are alcoholism and chronic hepatitis B virus (HBV) infection, while the most frequently documented acute insults that induce acute injury in ACLF include excessive alcohol drinking, HBV activation, drug-induced liver injury (DILI) etc.1-6 Bacterial infections were detected in up to 2/3 ACLF patients and contributed to the poor outcome of ACLF.7-9 Although the poor outcome is closely associated with bacterial infection,7,8 whether bacterial infection is a consequence of ACLF or a trigger as an acute insult to induce ACLF is still a question of debate.7-9 Collectively, the outcome of ACLF likely depends on two major aspects: the recovery of multi-organ injury and the control of bacterial infection.
Diabetes occurring as a direct consequence of loss of liver function is usually characterized by non-diabetic fasting plasma glucose (FPG) and haemoglobin A1c (HbA1c) levels and should regress after orthotopic liver transplantation (OLT). This observational, longitudinal study investigated the relationship between the time-courses of changes in all 3 direct determinants of glucose regulation, i.e., β-cell function, insulin clearance and insulin sensitivity, and diabetes regression after OLT. Eighty cirrhotic patients with non-diabetic FPG and HbA1c levels underwent an extended oral glucose tolerance test (OGTT) before and 3, 6, 12 and 24 months after OLT. The OGTT data were analysed with a mathematical model to estimate derivative control (DC) and proportional control (PC) of β-cell function and insulin clearance (which determine insulin bioavailability), and with the Oral Glucose Insulin Sensitivity (OGIS)-2 h index to estimate insulin sensitivity. At baseline, 36 patients were diabetic (45%) and 44 were non-diabetic (55%). Over the 2-year follow-up, 23 diabetic patients (63.9%) regressed to non-diabetic glucose regulation, whereas 13 did not (36.1%); moreover, 4 non-diabetic individuals progressed to diabetes (9.1%), whereas 40 did not (90.9%). Both DC and PC increased in regressors (from month 3 and 24, respectively) and decreased in progressors, whereas they remained stable in non-regressors and only PC decreased in non-progressors. Insulin clearance increased in all groups, apart from progressors. Likewise, OGIS-2 h improved at month 3 in all groups, but thereafter it continued to improve only in regressors, whereas it returned to baseline values in the other groups. Increased insulin bioavailability driven by improved β-cell function plays a central role in favouring diabetes regression after OLT, in the presence of a sustained improvement of insulin sensitivity.Diabetes mellitus (DM) is a common feature in cirrhotic individuals, due to the bidirectional relationship between impaired glucose metabolism and chronic liver disease.1 On the one hand, type 2 DM is a risk factor for non-alcoholic fatty liver disease (NAFLD)2 and, though not included in the most widely used prognostic tools,3 is a major predictor of adverse outcomes in cirrhotic individuals both before4 and after5 orthotopic liver transplantation (OLT). On the other hand, certain aetiological agents of liver disease, including HCV and NAFLD, may cause β-cell dysfunction and/or insulin resistance, thus favouring development of DM even prior to the onset of cirrhosis.1 Moreover, DM may be a direct consequence of loss of liver function, which impairs insulin secretion and sensitivity via several, partly unrecognized, mechanisms.6 This is the so-called hepatogenous DM, which is not considered a separate clinical entity, despite distinguishing pathophysiological and clinical features.7 We have previously shown that, compared with non-DM cirrhotic individuals, those with hepatogenous DM are characterized by worse β-cell function, which deteriorates in parallel with severity of liver disease.8 In addition, they present with fasting plasma glucose (FPG) and haemoglobin A1c (HbA1c) levels not in the DM range, due to impaired glucose metabolism and reduced lifespan of erythrocytes, respectively.6,7 This “subclinical” presentation implies that, in cirrhotic patients, an oral glucose tolerance test (OGTT) is required for DM diagnosis6,9 and explains the differences in prevalence estimates of DM according to the method(s) of assessment.9
HCV subtypes which are unusual in Europe are more prevalent in the African region, but little is known of their response to direct-acting antivirals (DAAs). These include non-1a/1b/ non-subtypeable genotype 1 (G1) or non-4a/4d (G4). In this report we aimed to describe the genotype distribution and treatment outcome in a south London cohort of African patients. We identified all patients born in Africa who attended our clinic from 2010-2018. Information on HCV genotype, treatment regimen and outcome were obtained. Non-subtypeable samples were analysed using Glasgow NimbleGen next-generation sequencing (NGS). Phylogenetic analysis was carried out by generating an uncorrected nucleotide p-distance tree from the complete coding regions of our sequences. Of 91 African patients, 47 (52%) were infected with an unusual subtype. Fourteen novel, as yet undesignated subtypes (G1*), were identified by NGS. Three individuals were infected with the same subtype, now designated as subtype 1p. Baseline sequences were available for 22 patients; 18/22 (82%) had baseline NS5A resistance-associated substitutions (RASs). Sustained virological response (SVR) was achieved in 56/63 (89%) overall, yet only in 21/28 (75%) of those with unusual G1 subtypes, with failure in 3/16 G1*, 1/2 G1p and 3/3 in G1l. Six treatment failures occurred with sofosbuvir/ledipasvir compared to 1 failure on a PI-based regimen. The SVR rate for all other genotypes and subtypes was 35/35 (100%). Most individuals in an unselected cohort of African patients were infected with an unusual genotype, including novel subtype 1p. The SVR rate of those with unusual G1 subtypes was 75%, raising concern about expansion of DAAs across Africa. Depending on the regimen used, higher failure rates in African cohorts could jeopardise HCV elimination.Direct-acting antiviral therapy (DAA) therapy has revolutionised hepatitis C (HCV) treatment. High cure rates with short courses of treatment make global eradication of HCV feasible; consequently, the World Health Organisation has promulgated a call for elimination of viral hepatitis as a public health threat by 2030.1
Cholestasis often occurs after burn injuries. However, the prevalence of cholestasis and its effect on outcomes in patients with severe burn injuries are unknown. The aim of this study was to describe the course and the burden of cholestasis in a cohort of severely burned adult patients. We investigated the relationship between burn-associated cholestasis (BAC) and clinical outcomes in a retrospective cohort of patients admitted to our unit for severe burn injuries between 2012 and 2015. BAC was defined as an increased level of serum alkaline phosphatase (ALP) ≥1.5x the upper limit of normal (ULN) with an increased level of gamma-glutamyltransferase (GGT) ≥3x ULN, or as an increased level of total bilirubin ≥2x ULN. A total of 214 patients were included: 111 (52%) patients developed BAC after a median (IQR) stay of 9 (5–16) days. At 90 days, the mortality rate was 20%, including 34 and 9 patients with and without BAC (p <0.001), respectively, which corresponded to a 2.5-fold higher (95% CI 1.2–5.2, p = 0.012) risk of 90-day mortality for patients with BAC. After being adjusted for severity of illness, patients with BAC, hyperbilirubinemia and without elevated ALP and GGT levels had a hazard ratio of 4.51 (95% CI 1.87–10.87) for 90-day mortality. BAC was associated with the severity of the burn injury, shock and bacteraemia. BAC was present in 38 (51%) patients at discharge, and 7 (18%) patients had secondary sclerosing cholangitis. These patients maintained elevated levels of ALP and GGT that were 5.8x (1.7–15) the ULN and 11x the ULN (4.5–22), respectively, 20 months (3.5–35) after discharge. BAC is prevalent among patients with severe burn injuries and is associated with worse short-term outcomes, especially when total bilirubin levels were increased without elevated ALP and GGT levels. BAC survivors are at risk of developing sclerosing cholangitis.Although the outcomes of severe burn injuries have improved over recent decades, morbidity and mortality remain high: almost 40% of patients develop acute respiratory distress syndrome,1,2 up to 50% of patients develop acute kidney injury3,4 and the reported mortality rates range from 15%1,3,5 to 25–30%.2,4 Liver necrosis and hepatic dysfunction after burn injuries have been reported since the late 1930s.6 An autopsy series from the 1980s described increased liver size and lipid infiltrations after burn injuries.7 Later studies showed that these infiltrations were prevalent among burn patients and correlated with the total body surface area (TBSA) of the burn8 and were associated with liver dysfunction.9 In a paediatric series, increases in alkaline phosphatase (ALP) and gamma glutamyltransferase (GGT) levels were reported 10 days after sustaining a burn injury.10 Furthermore, cases of sclerosing cholangitis have been reported after burn injuries.11 However, the prevalence of cholestasis and the relationship between cholestasis and outcomes in patients with burns is unknown. The purpose of this study was to describe the course and the burden of cholestasis in a cohort of severely burned adult patients. According to the standard definitions of cholestasis12,13 based on ALP, GGT, and bilirubin levels, we propose the term/entity of burn-associated cholestasis (BAC) with a subtype classification according to the patterns of ALP, GGT, and total bilirubin levels and their associations with clinical outcomes. SECTION Patients and methods SECTION Study design
Routine HEV testing of blood products has recently been implemented in Great Britain and the Netherlands. The relevance of transfusion-transmitted HEV infections is still controversially discussed in Europe. All blood donations at the University Medical Center Hamburg-Eppendorf were prospectively tested for HEV RNA by pooled PCR from October 2016 to May 2017. Reactive samples were individually retested. Additionally, stored samples from previous donations of positive donors were tested to determine the duration of HEV viraemia. HEV RNA-positive donors and a control cohort were asked to answer a questionnaire. Twenty-three out of 18,737 HEV RNA-positive donors were identified (0.12%). Only two of the positive donors (8.7%) presented with elevated aminotransferases at time of donation (alanine aminotransferase: 192 and 101 U/L). The retrospective analysis of all positive donors revealed that four asymptomatic donors had been HEV viraemic for up to three months with the longest duration of HEV viraemia exceeding four months. Despite the HEV-testing efforts, 14 HEV RNA-positive blood products were transfused into 12 immunocompromised and two immunocompetent patients. One recipient of these products developed fatal acute-on-chronic liver failure complicated by Pseudomonas septicemia. The questionnaire revealed that HEV RNA-positive donors significantly more often consumed raw pork meat (12 out of 18; 67%) than controls (89 out of 256; 35%; p = 0.01). In two donors, undercooked pork liver dishes were identified as the source of infection. HEV genotyping was possible in 7 out of 23 of HEV viraemic donors and six out of seven isolates belonged to HEV Genotype 3, Group 2. Prolonged HEV viraemia can be detected at a relatively high rate in Northern German blood donors, leading to transfusion-transmitted HEV infections in several patients with the risk of severe and fatal complications. Eating raw pork tartare represented a relevant risk for the acquisition of HEV infection.HEV infections are present worldwide.1 Consumption of pork meat has been considered to be the major source of HEV Genotype (GT) 3 infections in Europe. In addition to zoonotic transmission, blood products were shown to be a potential source of acute and chronic HEV infection in industrialised countries.2,3 The anti-HEV seroprevalence rate in Europe, depicting the number of people previously exposed to HEV, varies largely depending on the region and the assay used.4 A high seroprevalence rate of 30% was found in healthy German individuals using the sensitive Wantai anti-HEV IgG assay.5 In addition to serological studies, blood donors were tested for HEV viraemia by PCR in various European studies, and rates of HEV positivity from 1:726 to 1:3,333 have been determined.6–11 The largest of these studies was conducted in the UK. In this study, 225,000 blood donations were tested in pools of 24 samples by an in-house assay, and 79 specimens tested positive for HEV PCR (1 out of 2,850). The likelihood of developing clinically relevant hepatitis E after transfusion of an HEV-contaminated blood product was 42%,10 while in a Japanese study, 50% of recipients of HEV-contaminated blood products developed HEV infection.12 Since it became apparent how frequently HEV was found in blood products, routine testing of blood products has been discussed controversially in many European countries. In the vast majority of these countries, HEV screening has not been implemented, while authorities in the UK, Ireland, and the Netherlands decided to test all blood products for HEV.13 The aim of the present study was to determine the prevalence of blood-borne HEV infections at our academic tertiary care centre in Northern Germany and to evaluate whether routine HEV testing of blood products should be performed. SECTION Materials and methods SECTION Routine screening
Gasdermin D (GSDMD)-executed programmed necrosis is involved in inflammation and controls interleukin (IL)-1β release. However, the role of GSDMD in non-alcoholic steatohepatitis (NASH) remains unclear. We investigated the role of GSDMD in the pathogenesis of steatohepatitis. Human liver tissues from patients with non-alcoholic fatty liver disease (NAFLD) and control individuals were obtained to evaluate GSDMD expression. Gsdmd knockout (Gsdmd−/−) mice, obese db/db mice and their wild-type (WT) littermates were fed with methionine-choline deficient (MCD) or control diet to induce steatohepatitis. The Gsdmd−/− and WT mice were also used in a high-fat diet (HFD)-induced NAFLD model. In addition, Alb-Cre mice were administered an adeno-associated virus (AAV) vector that expressed the gasdermin-N domain (AAV9-FLEX-GSDMD-N) and were fed with either MCD or control diet for 10 days. GSDMD and its pyroptosis-inducing fragment GSDMD-N were upregulated in liver tissues of human NAFLD/NASH. Importantly, hepatic GSDMD-N protein levels were significantly higher in human NASH and correlated with the NAFLD activity score and fibrosis. GSDMD-N remained a potential biomarker for the diagnosis of NASH. MCD-fed Gsdmd−/− mice exhibit decreased severity of steatosis and inflammation compared with WT littermates. GSDMD was associated with the secretion of pro-inflammatory cytokines (IL-1β, TNF-α, and MCP-1 [CCL2]) and persistent activation of the NF-ĸB signaling pathway. Gsdmd−/− mice showed lower steatosis, mainly because of reduced expression of the lipogenic gene Srebp1c (Srebf1) and upregulated expression of lipolytic genes, including Pparα, Aco [Klk15], Lcad [Acadl], Cyp4a10 and Cyp4a14. Alb-Cre mice administered with AAV9-FLEX-GSDMD-N showed significantly aggravated steatohepatitis when fed with MCD diet. As an executor of pyroptosis, GSDMD plays a key role in the pathogenesis of steatohepatitis, by controlling cytokine secretion, NF-ĸB activation, and lipogenesis.Non-alcoholic fatty liver disease (NAFLD) represents a multi-step biological disorder in the liver, increasing the risk of cirrhosis and tumorigenesis.1,2 The key aspects of steatohepatitis have been precisely mimicked by extensive basic and translation research. This has enabled the reductionist assessment of genes and dietary factors involved in the pathogenesis of NAFLD.3,4 Toxic lipid accumulation in the liver acts as the primary insult which initiates and propagates damage, leading to hepatocyte injury and resultant inflammation.5,6 It is important to note that inflammation in the liver is believed to be the compelling feature that transforms simple steatosis to steatohepatitis, perpetuating hepatocellular injury and subsequent cell death, and promoting liver fibrosis.7–9 However, the molecular basis behind the inflammatory response leading to steatohepatitis is still largely unknown.
Donation after circulatory death (DCD) in the UK has tripled in the last decade. However, outcomes following DCD liver transplantation are worse than for donation after brainstem death (DBD) liver transplants. This study examines whether a recipient should accept a “poorer quality” DCD organ or wait longer for a “better” DBD organ. Data were collected on 5,825 patients who were registered on the elective waiting list for a first adult liver-only transplant and 3,949 patients who received a liver-only transplant in the UK between 1 January 2008 and 31 December 2015. Survival following deceased donor liver transplantation performed between 2008 and 2015 was compared by Cox regression modelling to assess the impact on patient survival of accepting a DCD liver compared to deferring for a potential DBD transplant. A total of 953 (23%) of the 3,949 liver transplantations performed utilised DCD donors. Five-year post-transplant survival was worse following DCD than DBD transplantation (69.1% [DCD] vs. 78.3% [DBD]; p <0.0001: adjusted hazard ratio [HR] 1.65; 95% CI 1.40–1.94). Of the 5,798 patients registered on the transplant list, 1,325 (23%) died or were removed from the list without receiving a transplant. Patients who received DCD livers had a lower risk-adjusted hazard of death than those who remained on the waiting list for a potential DBD organ (adjusted HR 0.55; 95% CI 0.47–0.65). The greatest survival benefit was in those with the most advanced liver disease (adjusted HR 0.19; 95% CI 0.07–0.50). Although DCD liver transplantation leads to worse transplant outcomes than DBD transplantation, the individual’s survival is enhanced by accepting a DCD offer, particularly for patients with more severe liver disease. DCD liver transplantation improves overall survival for UK listed patients and should be encouraged.Rates of liver failure are increasing dramatically in the UK1 and over a million people worldwide die of cirrhosis every year.2 Liver transplantation is the only effective treatment for end-stage liver disease and provides an average of 17–22 years of additional life.1,3,4 Access to liver transplantation is limited by donor organ availability, and over the last decade, as the incidence of liver disease has increased,1 the number of patients on the liver transplant waiting list in the UK has roughly doubled. Consequently, after 2 years, about 13% of listed patients will no longer be eligible for liver transplantation because of death or deterioration in their condition.5,6 These waiting list pressures have prompted a focus on the use of organs from donation after circulatory death (DCD) donors, with numbers of DCD donors increasing markedly over the last decade, such that they now almost match annual numbers of donation after brain death (DBD) donors in the UK.5 Worldwide, only the Netherlands achieves similar numbers of DCD donors per million population.7
Around 10–20% of patients with non-alcoholic fatty liver disease (NAFLD) are non-obese. The benefit of weight reduction in such patients is unclear. We aim to study the efficacy of lifestyle intervention in non-obese patients with NAFLD and to identify factors that predict treatment response. A total of 154 community NAFLD patients were randomised to a 12-month lifestyle intervention programme involving regular exercise, or to standard care. The primary outcome was remission of NAFLD at Month 12 by proton-magnetic resonance spectroscopy. After the programme, the patients were prospectively followed until Year 6. The Asian body mass index (BMI) cut-off of 25 kg/m2 was used to define non-obese NAFLD. Patients were assigned to the intervention (n = 77) and control (n = 77) groups (39 and 38 in each group had baseline BMI <25 and ≥25 kg/m2, respectively). More patients in the intervention group achieved the primary outcome than the control group regardless of baseline BMI (non-obese: 67% vs. 18%, p <0.001; obese: 61% vs. 21%, p <0.001). Lifestyle intervention, lower baseline intrahepatic triglyceride, and reduction in body weight and waist circumference were independent factors associated with remission of NAFLD in non-obese patients. Half of non-obese patients achieved remission of NAFLD with 3–5% weight reduction; the same could only be achieved in obese patients with 7–10% weight reduction. By Year 6, non-obese patients in the intervention group remained more likely to maintain weight reduction and alanine aminotransferase normalisation than the control group. Lifestyle intervention is effective in treating NAFLD in both non-obese and obese patients. Weight reduction predicts remission of NAFLD in non-obese patients, but a modest weight reduction may be sufficient in this population.Non-alcoholic fatty liver disease (NAFLD) is currently the most common chronic liver disease and is one of the leading causes of end-stage liver disease and hepatocellular carcinoma worldwide.1,2 Although NAFLD is strongly associated with metabolic syndrome and obesity,3 around 10–20% of patients with NAFLD have a relatively normal body mass index (BMI), a condition often described as non-obese or lean NAFLD.4 Studies based on liver histology or non-invasive tests of fibrosis suggest that these non-obese patients may also harbour non-alcoholic steatohepatitis (NASH) and advanced fibrosis.5–8
Immunotherapy for metastatic cancer can be complicated by the onset of hepatic immune-related adverse events (IRAEs). This study compared hepatic IRAEs associated with anti-programmed cell death protein 1 (PD-1)/PD ligand 1 (PD-L1) and anti-cytotoxic T lymphocyte antigen 4 (CTLA-4) monoclonal antibodies (mAbs). Among 536 patients treated with anti-PD-1/PD-L1 or CTLA-4 immunotherapies, 19 (3.5%) were referred to the liver unit for grade ≥3 hepatitis. Of these patients, nine had received anti-PD-1/PD-L1 and seven had received anti-CTLA-4 mAbs, in monotherapy or in combination with anti-PD-1. Liver investigations were undertaken in these 16 patients, including viral assays, autoimmune tests and liver biopsy, histological review, and immunostaining of liver specimens. In the 16 patients included in this study, median age was 63 (range 33-84) years, and nine (56%) were female. Time between therapy initiation and hepatitis was five (range, 1–49) weeks and median number of immunotherapy injections was two (range, 1–36). No patients developed hepatic failure. Histology related to anti-CTLA-4 mAbs demonstrated granulomatous hepatitis including fibrin ring granulomas and central vein endotheliitis. Histology related to anti-PD-1/PD-L1 mAbs was characterised by lobular hepatitis. The management of hepatic IRAEs was tailored according to the severity of both the biology and histology of liver injury: six patients improved spontaneously; seven received oral corticosteroids at 0.5–1 mg/kg/day; two were maintained on 0.2 mg/kg/day corticosteroids; and one patient required pulses and 2.5 mg/kg/day of corticosteroids, and the addition of a second immunosuppressive drug. In three patients, immunotherapy was reintroduced without recurrence of liver dysfunction. Acute hepatitis resulting from immunotherapy for metastatic cancer is rare (3.5%) and, in most cases, not severe. Histological assessment can distinguish between anti-PD-1/PD-L1 and anti-CTLA-4 mAb toxicity. The severity of liver injury is helpful for tailoring patient management, which does not require systematic corticosteroid administration.Immune-modulatory therapies have dramatically improved the survival of patients with metastatic tumours.1,2 During the development of cancer, the immune system becomes naturally ‘tolerant’ towards cancer cells, which are seen as part of the ‘self’. This tolerance is maintained by immune checkpoint pathways that downregulate immune functions, permitting cancer cells to evade immune attacks.3,4 Monoclonal antibodies (mAbs) directed against regulatory immune checkpoint molecules that inhibit T cell activation enhance antitumour immunity.5 Ipilimumab, a human Ig-G1 mAb, blocks cytotoxic T lymphocyte antigen 4 (CTLA-4).6 Pembrolizumab and nivolumab, humanized IgG4 kappa and human IgG4 mAbs, respectively, block the interaction between programmed cell death protein 1 (PD-1) and the two PD ligands, PD-L1 and PD-L2, by selectively binding the PD-1 receptor.7,8 Durvalumab, a human IgG1 kappa mAb, targets PD-L1.9
Living-donor liver transplantation (LDLT) can simultaneously cure hepatocellular carcinoma (HCC) and underlying liver cirrhosis, improving long-term results in patients with HCC. ABO-incompatible LDLT could expand the living-donor pool, reduce waiting times for deceased-donor liver transplantation, and improve long-term survival for some patients with HCC. We retrospectively reviewed the medical records of patients undergoing LDLT for HCC from November 2008 to December 2015 at a single institution in Korea. In total, 165 patients underwent ABO-incompatible and 753 patients underwent ABO-compatible LDLT for HCC. ABO-incompatible recipients underwent desensitization to overcome the ABO blood group barrier, including pretransplant plasma exchange and rituximab administration (300–375 mg/m2 /body surface area). We performed 1:1 propensity score matching and included 165 patients in each group. 82.4% of ABO-incompatible and 83.0% of -compatible LDLT groups had HCC within conventional Milan criteria, respectively, and 92.1% and 92.7% of patients in each group had a Child-Pugh score of A or B. ABO-incompatible and -compatible LDLT groups were followed up for 48.0 and 48.7 months, respectively, with both groups showing comparable recurrence-free survival rates (hazard ratio [HR] 1.14; 95% CI 0.68–1.90; p = 0.630) and overall patient-survival outcomes (HR 1.10; 95% CI 0.60–2.00; p = 0.763). These findings suggested that ABO-incompatible liver transplantation is a feasible option for patients with HCC, especially for those with compensated cirrhosis with HCC within conventional Milan criteria.Although several potentially curative treatments for hepatocellular carcinoma (HCC) are known, such as liver resection and local ablation, HCC is difficult to manage, because the tumor mostly develops on a background of cirrhosis, and the limited functional reserves of the liver making their use difficult.1 In recent years, liver transplantation (LT) has generally been considered a feasible treatment capable of simultaneously curing HCC and the underlying liver cirrhosis, with living-donor liver transplantation (LDLT) performed on select patients with HCC as a practical alternative to deceased-donor liver transplantation (DDLT).2–4 However, many patients with HCC who may benefit from long-term survival through LT still do not have a chance of transplantation due to a lack of organs, especially in Asia where HCC incidence combined with chronic HBV- and HCV-related liver diseases is high.5
The effect of hepatocellular carcinoma (HCC) on the response to interferon-free direct-acting antiviral (DAA) therapy in patients with chronic hepatitis C (CHC) infection remains unclear. Using a systematic review and meta-analysis approach, we aimed to investigate the effect of DAA therapy on sustained virologic response (SVR) among patients with CHC and either active, inactive or no HCC. PubMed, Embase, Web of Science, and the Cochrane Central Register of Controlled Trials were searched from 1/1/2013 to 9/24/2018. The pooled SVR rates were computed using DerSimonian-Laird random-effects models. We included 49 studies from 15 countries, comprised of 3,341 patients with HCC and 35,701 without HCC. Overall, the pooled SVR was lower in patients with HCC than in those without HCC (89.6%, 95% CI 86.8–92.1%, I2 = 79.1% vs. 93.3%, 95% CI 91.9–94.7%, I2 = 95.0%, p = 0.0012), translating to a 4.8% (95% CI 0.2–7.4%) SVR reduction by meta-regression analysis. The largest SVR reduction (18.8%) occurred in patients with active/residual HCC vs. inactive/ablated HCC (SVR 73.1% vs. 92.6%, p = 0.002). Meanwhile, patients with HCC who received a prior liver transplant had higher SVR rates than those who did not (p <0.001). Regarding specific DAA regimens, patients with HCC treated with ledipasvir/sofosbuvir had lower SVR rates than patients without HCC (92.6%, n = 884 vs. 97.8%, n = 13,141, p = 0.026), but heterogeneity was high (I2 = 84.7%, p <0.001). The SVR rate was similar in patients with/without HCC who were treated with ombitasvir/paritaprevir/ritonavir ± dasabuvir (n = 101) (97.2% vs. 94.8%, p = 0.79), or daclatasvir/asunaprevir (91.7% vs. 89.8%, p = 0.66). Overall, SVR rates were lower in patients with HCC, especially with active HCC, compared to those without HCC, though heterogeneity was high. Continued efforts are needed to aggressively screen, diagnose, and treat HCC to ensure higher CHC cure rates.Chronic hepatitis C virus (HCV) infection affected an estimated 71.1 million patients worldwide in 2015 and is a leading cause of liver cirrhosis and hepatocellular carcinoma (HCC).1 Among patients who have undergone treatment with curative intent for HCC, early hepatic decompensation and HCC recurrence were the major drivers of mortality.2 In recent studies of chronic hepatitis B-related HCC, antiviral therapy was shown to significantly reduce overall long-term mortality even in patients with very advanced HCC or decompensated cirrhosis, including those who were only receiving palliative treatment for HCC.3–6 Prior to the advent of interferon (IFN)-free direct-acting antiviral (DAA) therapy, patients with HCV-related HCC were often excluded from anti-HCV therapy as they tended to be older and had multiple non-liver and liver comorbidities, many of which rendered them unsuitable candidates for IFN-based therapy. Since 2014, many of these patients with HCC became treatment candidates for their chronic hepatitis C (CHC), despite the presence of advanced liver disease and comorbidities, as DAA therapy is not only highly efficacious but well tolerated.7 Individual real-world studies to date have included patients with HCC from both the East and West, and some have reported significantly lower cure rates.8–14 However, most studies had small sample sizes and heterogeneous patient demographic and clinical characteristics.
Non-alcoholic fatty liver disease and alcohol-related liver disease pose an important challenge to current clinical healthcare pathways because of the large number of at-risk patients. Therefore, we aimed to explore the cost-effectiveness of transient elastography (TE) as a screening method to detect liver fibrosis in a primary care pathway. Cost-effectiveness analysis was performed using real-life individual patient data from 6 independent prospective cohorts (5 from Europe and 1 from Asia). A diagnostic algorithm with conditional inference trees was developed to explore the relationships between liver stiffness, socio-demographics, comorbidities, and hepatic fibrosis, the latter assessed by fibrosis scores (FIB-4, NFS) and liver biopsies in a subset of 352 patients. We compared the incremental cost-effectiveness of a screening strategy against standard of care alongside the numbers needed to screen to diagnose a patient with fibrosis stage ≥F2. The data set encompassed 6,295 participants (mean age 55 ± 12 years, BMI 27 ± 5 kg/m2, liver stiffness 5.6 ± 5.0 kPa). A 9.1 kPa TE cut-off provided the best accuracy for the diagnosis of significant fibrosis (≥F2) in general population settings, whereas a threshold of 9.5 kPa was optimal for populations at-risk of alcohol-related liver disease. TE with the proposed cut-offs outperformed fibrosis scores in terms of accuracy. Screening with TE was cost-effective with mean incremental cost-effectiveness ratios ranging from 2,570 €/QALY (95% CI 2,456–2,683) for a population at-risk of alcohol-related liver disease (age ≥45 years) to 6,217 €/QALY (95% CI 5,832–6,601) in the general population. Overall, there was a 12% chance of TE screening being cost saving across countries and populations. Screening for liver fibrosis with TE in primary care is a cost-effective intervention for European and Asian populations and may even be cost saving.Alcohol-related liver disease (ALD) and non-alcoholic fatty liver disease (NAFLD) are leading causes of chronic liver diseases, hepatocellular carcinoma (HCC) and liver-related deaths worldwide.1,2 While the causes, consequences and treatment strategies for ALD and NAFLD are being studied and developed,3–5 the majority of patients are still diagnosed at an advanced stage of disease.6 Consequently, the course of action towards early disease detection from a public health perspective remains a grey area in hepatology.7
A major limitation in the field of liver transplantation is the shortage of transplantable organs. Chimeric animals carrying human tissue have the potential to solve this problem. However, currently available chimeric organs retain a high level of xenogeneic cells, and the transplantation of impure organs needs to be tested. We created chimeric livers by injecting Lewis rat hepatocytes into C57Bl/6Fah−/−Rag2−/−Il2rg−/− mice, and further transplanted them into newly weaned Lewis rats (45 ± 3 g) with or without suboptimal immunosuppression (tacrolimus 0.6 mg/kg/day for 56 or 112 days). Control donors included wild-type C57Bl/6 mice (xenogeneic) and Lewis rats (syngeneic). Without immunosuppression, recipients of chimeric livers experienced acute rejection, and died within 8 to 11 days. With immunosuppression, they all survived for >112 days with normal weight gain compared to syngeneic controls, while all xenogeneic controls died within 98 days due to rejection with Banff scores >6 (p = 0.0014). The chimeric grafts underwent post-transplant remodelling, growing by 670% on average. Rat hepatocytes fully replaced mouse hepatocytes starting from day 56 (absence of detectable mouse serum albumin, histological clearance of mouse hepatocytes). In addition, rat albumin levels reached those of syngeneic recipients. Four months after transplantation of chimeric livers, we observed the development of diffuse mature rat bile ducts through transdifferentiation of hepatocytes (up to 72% of cholangiocytes), and patchy areas of portal endothelium originating from the host (seen in one out of five recipients). Taken together, these data demonstrate the efficacy of transplanting rat-to-mouse chimeric livers into rats, with a high potential for post-transplant recipient-oriented graft remodelling. Validation in a large animal model is still needed.The use of chimeric animals with organs compatible with specific patients in need of transplantation has the potential to solve the chronic lack of organ donors. The idea of using animals as incubators of human tissue is becoming more and more realistic, especially with the recent observation that human induced pluripotent stem cells (hiPSCs) can lead to chimeras after injection into pig blastocysts.1 At this stage, the experiment was terminated before birth, and the contribution of human cells to the final chimeric pig embryos has remained low, but future studies are looking at replacing entire organs, as has been the case for various rodent combinations.1
It has been proposed that serum hepatitis B core-related antigen (HBcrAg) reflects intrahepatic covalently closed circular (ccc)DNA levels. However, the correlation of HBcrAg with serum and intrahepatic viral markers and liver histology has not been comprehensively investigated in a large sample. We aimed to determine if HBcrAg could be a useful therapeutic marker in patients with chronic hepatitis B. HBcrAg was measured by chemiluminescent enzyme immunoassay in 130 (36 hepatitis B e antigen [HBeAg]+ and 94 HBeAg−) biopsy proven, untreated, patients with chronic hepatitis B. HBcrAg levels were correlated with: a) serum hepatitis B virus (HBV)-DNA, quantitative hepatitis B surface antigen and alanine aminotransferase levels; b) intrahepatic total (t)HBV-DNA, cccDNA, pregenomic (pg)RNA and cccDNA transcriptional activity (defined as pgRNA/cccDNA ratio); c) fibrosis and necroinflammatory activity scores. HBcrAg levels were significantly higher in HBeAg+ vs. HBeAg− patients and correlated with serum HBV-DNA, intrahepatic tHBV-DNA, pgRNA and cccDNA levels, and transcriptional activity. Patients who were negative for HBcrAg (<3 LogU/ml) had less liver cccDNA and lower cccDNA activity than the HBcrAg+ group. Principal component analysis coupled with unsupervised clustering identified that in a subgroup of HBeAg− patients, higher HBcrAg levels were associated with higher serum HBV-DNA, intrahepatic tHBV-DNA, pgRNA, cccDNA transcriptional activity and with higher fibrosis and necroinflammatory activity scores. Our results indicate that HBcrAg is a surrogate marker of both intrahepatic cccDNA and its transcriptional activity. HBcrAg could be useful in the evaluation of new antiviral therapies aiming at a functional cure of HBV infection either by directly or indirectly targeting the intrahepatic cccDNA pool.The development of novel antiviral agents and immunomodulatory approaches to cure hepatitis B virus (HBV) infection requires new biomarkers capable of reflecting the intrahepatic activity of the virus and chronic HBV (CHB) stages and defining new meaningful treatment endpoints. Indeed, there is an unmet need for standardized assays able to provide mechanistic insights into the effects of the novel antiviral and immunomodulatory agents and to assess treatment efficacy.1
Recently the Amsterdam-Oxford model (AOM) was introduced as a prognostic model to assess the risk of death and/or liver transplantation (LT) in primary sclerosing cholangitis (PSC). We aimed to validate and assess the utility of the AOM. Clinical and laboratory data were collected from the time of PSC diagnosis until the last visit or time of LT or death. The AOM was calculated at yearly intervals following PSC diagnosis. Discriminatory performance was assessed by calculation of the C-statistic and prediction accuracy by comparing the predicted survival with the observed survival in Kaplan-Meier estimates. A grid search was performed to identify the most discriminatory AOM threshold. A total of 534 patients with PSC and a mean (SD) age of 39.2 (13.1) years were included. The diagnosis was large duct PSC in 466 (87%), PSC with features of autoimmune hepatitis in 52 (10%) and small-duct PSC in 16 (3%). During the median (IQR) follow-up of 7.8 (4.0–12.6) years, 167 patients underwent LT and 65 died. The median LT-free survival was 13.2 (11.8–14.7) years. The C-statistic of the AOM ranged from 0.67 at baseline to 0.75 at 5 years of follow-up. The difference between the predicted and observed survival ranged from −1.6% at 1 year to + 3.9% at 5 years of follow-up. Patients that developed AOM scores >2.0 were at significant risk of LT or death (time-dependent hazard ratio 4.09; 95% CI 2.99–5.61). In this large cohort of patients with PSC, the AOM showed an adequate discriminative performance and good prediction accuracy at PSC diagnosis and during follow-up. This study further validates the AOM as a valuable risk stratification tool in PSC and extends its utility.Primary sclerosing cholangitis (PSC) is a chronic, variably progressive cholestatic liver disease characterized by inflammation of the intrahepatic and extrahepatic bile ducts, sclerosis and destruction of the biliary tract.1–4 This leads to chronic cholestasis, biliary fibrosis and (decompensated) cirrhosis, which may eventually culminate into liver failure requiring liver transplantation; the only potential curative treatment for PSC.2,3 Following a PSC diagnosis a median transplant-free survival of 13 years has been reported in studies from tertiary referral centres, although this may be longer in a population-based setting.5
Acute-on-chronic liver failure (ACLF), which develops in patients with cirrhosis, is characterized by intense systemic inflammation and organ failure(s). Because systemic inflammation is energetically expensive, its metabolic costs may result in organ dysfunction/failure. Therefore, we aimed to analyze blood metabolome in patients with cirrhosis, with and without ACLF. We performed untargeted metabolomics using liquid chromatography coupled to high-resolution mass spectrometry in serum from 650 patients with AD (acute decompensation of cirrhosis, without ACLF), 181 with ACLF, 43 with compensated cirrhosis, and 29 healthy subjects. Of the 137 annotated metabolites identified, 100 were increased in patients with ACLF of any grade, relative to those with AD, and 38 composed a distinctive blood metabolite fingerprint for ACLF. Among patients with ACLF, the intensity of the fingerprint increased across ACLF grades, and was similar in patients with kidney failure and in those without, indicating that the fingerprint reflected not only decreased kidney excretion but also altered cell metabolism. The higher the ACLF-associated fingerprint intensity, the higher plasma levels of inflammatory markers, tumor necrosis factor α, soluble CD206, and soluble CD163. ACLF was characterized by intense proteolysis and lipolysis; amino acid catabolism; extra-mitochondrial glucose metabolism through glycolysis, pentose phosphate, and D-glucuronate pathways; depressed mitochondrial ATP-producing fatty acid β-oxidation; and extra-mitochondrial amino acid metabolism giving rise to metabolites which are metabotoxins. In ACLF, intense systemic inflammation is associated with blood metabolite accumulation witnessing profound alterations in major metabolic pathways, in particular inhibition in mitochondrial energy production, which may contribute to the existence of organ failures.The results of the CANONIC study were used to redefine acute-on-chronic liver failure (ACLF) as a syndrome which develops in patients with cirrhosis and acute decompensation (AD) and is characterized by an intense systemic inflammation1,2-4 associated with different combinations of organ failures among the six major organ systems (liver, kidney, brain, coagulation, circulation, and respiration), giving rise to different clinical phenotypes1. ACLF is very frequent, affecting 30%-40% of hospitalized patients, and is associated with high short-term mortality rate (30% at 28 days)1. In most cases of ACLF, the activation of innate immune cells by pathogen-associated molecular patterns (PAMPs) are thought to play a major role in the induction of systemic inflammation.5 However, the intrinsic mechanisms of ACLF at the tissue and cellular levels that contribute to the development and maintenance of organ failures are unknown. Understanding these mechanisms should not only result in progresses in the knowledge on pathophysiology of ACLF, but also could provide clues to the development of new biomarkers of organ dysfunction/failure and identification of targets for new therapies of organ failures which are urgently needed.
Around 5% of patients with chronic hepatitis C virus (HCV) infection treated with direct-acting antiviral (DAA) agents do not achieve sustained virological response (SVR). The currently approved retreatment regimen for prior DAA failure is a combination of sofosbuvir, velpatasvir, and voxilaprevir (SOF/VEL/VOX), although there is little data on its use in clinical practice. The aim of this study was to analyse the effectiveness and safety of SOF/VEL/VOX in the real-world setting. This was a prospective multicentre study assessing the efficacy of retreatment with SOF/VEL/VOX in patients who had experienced a prior DAA treatment failure. The primary endpoint was SVR 12 weeks after the completion of treatment (SVR12). Data on safety and tolerability were also recorded. A total of 137 patients were included: 75% men, 35% with liver cirrhosis. Most were infected with HCV genotype (GT) 1 or 3. The most common prior DAA combinations were sofosbuvir plus an NS5A inhibitor or ombitasvir/paritaprevir/r+dasabuvir. A total of 136 (99%) patients achieved undetectable HCV RNA at the end of treatment. Overall SVR12 was 95% in the 135 patients reaching this point. SVR12 was lower in patients with cirrhosis (89%, p = 0.05) and those with GT3 infection (80%, p <0.001). Patients with GT3 infection and cirrhosis had the lowest SVR12 rate (69%). Of the patients who did not achieve SVR12, 1 was reinfected and 7 experienced treatment failure (6 GT3, 1 GT1a). The presence of resistance-associated substitutions did not impact SVR12. Adverse effects were mild and non-specific. Real-world data show that SOF/VEL/VOX is an effective, safe rescue therapy for patients with prior DAA treatment failure despite the presence of resistance-associated substitutions. However, patients with liver cirrhosis infected by GT3 remain the most-difficult-to-treat group.Current treatments with direct-acting antivirals (DAAs) for hepatitis C virus (HCV) infection lead to elimination of the virus in more than 95% of patients, regardless of the HCV genotype or presence of advanced liver fibrosis.1 The American Association for the Study of Liver Diseases (AASLD) and the European Association for the Study of the Liver (EASL) guidelines both recommend combinations including an NS5A inhibitor with either a NS3/4 protease inhibitor, such as grazoprevir/elbasvir or glecaprevir/pibrentasvir, or a nucleotide analogue plus an NS5A inhibitor, such as sofosbuvir/velpatasvir, for durations ranging from 8 to 12 weeks.2–4 Sofosbuvir/velpatasvir and glecaprevir/pibrentasvir combinations are pangenotypic and therefore, they are the preferred regimens to simplify HCV therapy.5–7 Despite the high efficacy of these new combinations, the options for patients who do not achieve a sustained virological response (SVR) are limited.8 The latest approved retreatment regimen is combined therapy with sofosbuvir plus the NS55 inhibitor, velpatasvir, and the NS3/4 protease inhibitor, voxilaprevir (SOF/VEL/VOX),9 which is recommended in the AASLD and EASL guidelines for retreating patients previously failing DAA regimens.2–4
A comprehensive analysis of changes in symptoms and functioning during and after direct-acting antiviral (DAA) therapy for chronic hepatitis C virus (HCV) infection has not been conducted for patients treated in real-world clinical settings. Therefore, we evaluated patient-reported outcomes (PROs) in a diverse cohort of patients with HCV treated with commonly prescribed DAAs. PROP UP is a US multicenter observational study of 1,601 patients with HCV treated with DAAs in 2016-2017. PRO data were collected at baseline (T1), early on-treatment (T2), late on-treatment (T3) and 3-months post-treatment (T4). PRO mean change scores were calculated from baseline and a minimally important change (MIC) threshold was set at 5%. Regression analyses investigated patient and treatment characteristics independently associated with PRO changes on-treatment and post-treatment. Of 1,564 patients, 55% were male, 39% non-white, 47% had cirrhosis. Sofosbuvir/ledipasvir was prescribed to 63%, sofosbuvir/velpatasvir to 21%, grazoprevir/elbasvir to 11%, and paritaprevir/ombitasvir/ritonavir + dasabuvir to 5%. During DAA therapy, mean PRO scores improved slightly in the overall cohort, but did not reach the 5% MIC threshold. Between 21–53% of patients experienced >5% improved PROs while 23–36% experienced >5% worse symptoms. Of 1,410 patients with evaluable sustained virologic response (SVR) data, 95% achieved SVR. Among those with SVR, all mean PRO scores improved, with the 5% MIC threshold met for fatigue, sleep disturbance, and functioning well-being. Regression analyses identified subgroups, defined by age 35–55, baseline mental health issues and a higher number of health comorbidities as predictors of PRO improvements. In real-world clinical practices, we observed heterogeneous patient experiences during and after DAA treatment. Symptom improvements were more pronounced in younger patients, those with baseline mental health issues and multiple comorbidities.Patients with chronic hepatitis C virus (HCV) infection often report neuropsychiatric, somatic, and gastrointestinal symptoms including fatigue, sleep disturbance, musculoskeletal pain, depression, and abdominal pain.1–3 Patients may attribute these symptoms to HCV, a chronic viral infection associated with several extrahepatic disorders. Recent studies show that health-related quality of life and other patient-reported outcomes (PROs) improve during all-oral direct-acting antiviral (DAA) therapy and after patients achieve a sustained virologic response (SVR).4–6 These studies were based exclusively on data derived from industry-sponsored registration trials. It remains critical to determine if these findings can be generalized to patients treated in real-world clinical practices given inherent biases of registration trial data.7,8
Placement of an irradiation stent has been demonstrated to offer longer patency and survival than an uncovered self-expandable metallic stent (SEMS) in patients with unresectable malignant biliary obstruction (MBO). We aim to further assess the efficacy of an irradiation stent compared to an uncovered SEMS in those patients. We performed a randomized, open-label trial of participants with unresectable MBO at 20 centers in China. A total of 328 participants were allocated in parallel to the irradiation stent group (ISG) or the uncovered SEMS group (USG). Endpoints included stent patency (primary), technical success, relief of jaundice, overall survival, and complications. The first quartile stent patency time (when 25% of the patients experienced stent restenosis) was 212 days for the ISG and 104 days for the USG. Irradiation stents were significantly associated with a decrease in the rate of stent restenosis (9% vs. 15% at 90 days; 16% vs. 27% at 180 days; 21% vs. 33% at 360 days; p = 0.010). Patients in the ISG obtained longer survival time (median 202 days vs. 140 days; p = 0.020). No significant results were observed in technical success rate (93% vs. 95%; p = 0.499), relief of jaundice (85% vs. 80%; p = 0.308), and the incidence of grade 3 and 4 complications (8.5% vs. 7.9%; p = 0.841). Insertion of irradiation stents instead of uncovered SEMS could improve patency and overall survival in patients with unresectable MBO.Malignant biliary obstruction (MBO) is a common condition caused by biliary tract cancer (cholangiocarcinoma, gallbladder carcinoma, or ampulla carcinoma), pancreatic cancer, or metastatic lymph nodes.1 The incidence of MBO is modest in the Western world, although consistently rise; while in East and Southeast Asia, the incidence is remarkably high and poses significant public health issues.2 Surgical excision of detectable tumor is associated with improvemed survival.1 However, less than 20% of patients are surgical candidates once obstructive jaundice has occurred,3 and the long-term survival remains dismal.4,5 For unresectable tumors, stenting is considered the preferred palliative modality to relieve pruritus, cholangitis, pain, and jaundice, with or without chemotherapy.3,6 Meanwhile, conventional self-expandable metallic stent (SEMS) has a high incidence of restenosis which limits the survival benefit. Despite many advances in stent design over the past decade, none of them was acknowledged as a substitute for conventional SEMS.7 An irradiation biliary stent loaded with iodine 125 (125I) seeds has been demonstrated to offer significantly longer survival in patients with unresectable MBO when compared to an uncovered SEMS in a single-center randomized trial.8 This phase III trial aimed to further assess the efficacy of this irradiation stent in those patients. SECTION Patients and methods SECTION Study design and participants
Hepatocellular carcinoma (HCC) risk varies dramatically in patients with cirrhosis according to well-described, readily available predictors. We aimed to develop simple models estimating HCC risk in patients with alcohol-related liver disease (ALD)-cirrhosis or non-alcoholic fatty liver disease (NAFLD)-cirrhosis and calculate the net benefit that would be derived by implementing HCC surveillance strategies based on HCC risk as predicted by our models. We identified 7,068 patients with NAFLD-cirrhosis and 16,175 with ALD-cirrhosis who received care in the Veterans Affairs (VA) healthcare system in 2012. We retrospectively followed them for the development of incident HCC until January 2018. We used Cox proportional hazards regression to develop and internally validate models predicting HCC risk using baseline characteristics at entry into the cohort in 2012. We plotted decision curves of net benefit against HCC screening thresholds. We identified 1,278 incident cases of HCC during a mean follow-up period of 3.7 years. Mean annualized HCC incidence was 1.56% in NAFLD-cirrhosis and 1.44% in ALD-cirrhosis. The final models estimating HCC were developed separately for NAFLD-cirrhosis and ALD-cirrhosis and included 7 predictors: age, gender, diabetes, body mass index, platelet count, serum albumin and aspartate aminotransferase to √alanine aminotransferase ratio. The models exhibited very good measures of discrimination and calibration and an area under the receiver operating characteristic curve of 0.75 for NAFLD-cirrhosis and 0.76 for ALD-cirrhosis. Decision curves showed higher standardized net benefit of risk-based screening using our prediction models compared to the screen-all approach. We developed simple models estimating HCC risk in patients with NAFLD-cirrhosis or ALD-cirrhosis, which are available as web-based tools (www.hccrisk.com). Risk stratification can be used to inform risk-based HCC surveillance strategies in individual patients or healthcare systems or to identify high-risk patients for clinical trials.Annual HCC risk varies greatly in patients with cirrhosis ranging from as little as <0.2% to as high as >5%. Although this variability is well recognized, few models exist to estimate HCC risk in patients with cirrhosis and none are commonly used. Liver societies recommend the same screening strategy (abdominal ultrasonography every 6 months with or without concomitant serum alpha-fetoprotein [AFP]) irrespective of HCC risk.1–3 Studies show poor compliance with these screening recommendations.4,5 Stratification of HCC risk in patients with cirrhosis into low (e.g. <1% per year), medium (e.g. 1–3% per year) and high (e.g. >3% per year) would enable optimization and individualization of outreach efforts and screening strategies in patients with cirrhosis. It would also enable identification of high-risk patients for clinical trials of HCC screening.
To compare the overall survival (OS) and disease progression free survival (PFS) in patients with advanced hepatocellular carcinoma (Ad-HCC) who are undergoing hepatic arterial infusion (HAI) of oxaliplatin, fluorouracil/leucovorin (FOLFOX) treatment vs. sorafenib. This retrospective study was approved by the ethical review committee, and informed consent was obtained from all patients before treatment. HAI of FOLFOX (HAIF) was recommended as an alternative treatment option for patients who refused sorafenib. Of the 412 patients with Ad-HCC (376 men and 36 women) between Jan 2012 to Dec 2015, 232 patients were treated with sorafenib; 180 patients were given HAIF therapy. The median age was 51 years (range, 16–82 years). Propensity-score matched estimates were used to reduce bias when evaluating survival. Survival curves were calculated by performing the Kaplan-Meier method and compared by using the log-rank test and Cox regression models. The median PFS and OS in the HAIF group were significantly longer than those in the sorafenib group (PFS 7.1 vs. 3.3 months [RECIST]/7.4 vs. 3.6 months [mRECIST], respectively; OS 14.5 vs. 7.0 months; p <0.001 for each). In the propensity-score matched cohorts (147 pairs), both PFS and OS in the HAIF group were longer than those in the sorafenib group (p <0.001). At multivariate analysis, HAIF treatment was an independent factor for PFS (hazard ratio [HR] 0.389 [RECIST]/0.402 [mRECIST]; p <0.001 for each) and OS (HR 0.129; p <0.001). HAIF therapy may improve survival compared to sorafenib in patients with Ad-HCC. A prospective randomized trial is ongoing to confirm this finding.Hepatocellular carcinoma (HCC) is the fourth leading cause of cancer worldwide.1 A total of 25%–70% of HCC is diagnosed at an advanced stage, with a median overall survival (OS) of only 4.2–7.9 months, because of limited treatment options.2,3 To date, sorafenib is still the only treatment shown to extend OS for advanced HCC (Ad-HCC).4 However limitations including, low response rates,2 modest survival advantages,3 high-level heterogeneity of individual response5 and insensitivity for populations with hepatitis B virus (HBV) infection,6 prohibit sorafenib’s widespread use in Ad-HCC. Thus, alternative therapies for Ad-HCC are urgently required.1,7,8
Sterile inflammation resulting in alcoholic hepatitis (AH) occurs unpredictably after many years of excess alcohol intake. The factors responsible for the development of AH are not known but mitochondrial damage with loss of mitochondrial function are common features. Hcar2 is a G-protein coupled receptor which is activated by β-hydroxybutyrate (BHB). We aimed to determine the relevance of the BHB-Hcar2 pathway in alcoholic liver disease. We tested if loss of BHB production can result in increased liver inflammation. We further tested if BHB supplementation is protective in AH through interaction with Hcar2, and analyzed the immune and cellular basis for protection. Humans with AH have reduced hepatic BHB, and inhibition of BHB production in mice aggravated ethanol-induced AH, with higher plasma alanine aminotransferase levels, increased steatosis and greater neutrophil influx. Conversely supplementation of BHB had the opposite effects with reduced alanine aminotransferase levels, reduced steatosis and neutrophil influx. This therapeutic effect of BHB is dependent on the receptor Hcar2. BHB treatment increased liver Il10 transcripts, and promoted the M2 phenotype of intrahepatic macrophages. BHB also increased the transcriptional level of M2 related genes in vitro bone marrow derived macrophages. This skewing towards M2 related genes is dependent on lower mitochondrial membrane potential (Δψ) induced by BHB. Collectively, our data shows that BHB production during excess alcohol consumption has an anti-inflammatory and hepatoprotective role through an Hcar2 dependent pathway. This introduces the concept of metabolite-based therapy for AH.Excess alcohol intake has many effects on the liver and can present with several clinical syndromes.1 One of the most serious is acute alcoholic hepatitis (AH), which occurs unexpectedly after decades of high levels of alcohol consumption, and is characterized by sterile liver inflammation, jaundice and can progress to a systemic inflammatory response.2 There is a mixed inflammatory infiltrate characterized by neutrophils, and upregulation of a variety of inflammatory cytokines including IL-1β, TNF-α and IL-6. Due to the ubiquitous development of hepatocyte steatosis there has been sustained interest in aspects of mitochondrial function related to lipid metabolism, particularly mitochondrial β-oxidation.3 In vivo and in vitro models have shown diverse effects of alcohol on mitochondrial biology including abnormal mitochondrial morphology (giant mitochondria), mitochondrial DNA fragmentation, deceased mitochondrial protein synthesis by inhibition of mitochondrial ribosome activity, and oxidation of mitochondrial proteins.3 Measurement of the steps in β-oxidation has demonstrated inhibition after acute and chronic alcohol exposure, which is caused by downregulation of genes involved in fatty acid uptake as well as oxidation.4,5
In the sera of infected patients, hepatitis C virus (HCV) particles display heterogeneous forms with low-buoyant densities (<1.08), underscoring their lipidation via association with apoB-containing lipoproteins, which was proposed to occur during assembly or secretion from infected hepatocytes. However, the mechanisms inducing this association remain poorly-defined and most cell culture grown HCV (HCVcc) particles exhibit higher density (>1.08) and poor/no association with apoB. We aimed to elucidate the mechanisms of lipidation and to produce HCVcc particles resembling those in infected sera. We produced HCVcc particles of Jc1 or H77 strains from Huh-7.5 hepatoma cells cultured in standard conditions (10%-fetal calf serum) vs. in serum-free or human serum conditions before comparing their density profiles to patient-derived virus. We also characterized wild-type and Jc1/H77 hypervariable region 1 (HVR1)-swapped mutant HCVcc particles produced in serum-free media and incubated with different serum types or with purified lipoproteins. Compared to serum-free or fetal calf serum conditions, production with human serum redistributed most HCVcc infectious particles to low density (<1.08) or very-low density (<1.04) ranges. In addition, short-time incubation with human serum was sufficient to shift HCVcc physical particles to low-density fractions, in time- and dose-dependent manners, which increased their specific infectivity, promoted apoB-association and induced neutralization-resistance. Moreover, compared to Jc1, we detected higher levels of H77 HCVcc infectious particles in very-low-density fractions, which could unambiguously be attributed to strain-specific features of the HVR1 sequence. Finally, all 3 lipoprotein classes, i.e., very-low-density, low-density and high-density lipoproteins, could synergistically induce low-density shift of HCV particles; yet, this required additional non-lipid serum factor(s) that include albumin. The association of HCV particles with lipids may occur in the extracellular milieu. The lipidation level depends on serum composition as well as on HVR1-specific properties. These simple culture conditions allow production of infectious HCV particles resembling those of chronically-infected patients.Hepatitis C virus (HCV) infection is a major cause of chronic liver diseases worldwide. Although direct-acting antivirals (DAAs) can now cure most patients, there remain major challenges in basic, translational and clinical research.1 As DAAs are only curative, the development of a protective vaccine remains an important goal; yet, this requires deeper knowledge of the HCV particle’s structure. Indeed, the HCV virion has unusually heterogenous morphology, size and properties.2 Immunocapture of its surface proteins revealed particles of 50–80 nm without symmetrical arrangement.3–6 HCV particles harbor 2 envelope glycoproteins, E1 and E2, inserted on a membranous envelope that surrounds a nucleocapsid, composed of a core protein multimer and RNA+ viral genome; yet, the organization of the virion surface remains elusive and there is currently no clear model of the HCV particle’s topology.
Fibrosis, a cardinal feature of a dysfunctional liver, significantly contributes to the ever-increasing mortality due to end-stage chronic liver diseases. The crosstalk between hepatocytes and hepatic stellate cells (HSCs) plays a key role in the progression of fibrosis. Although ample efforts have been devoted to elucidate the functions of HSCs during liver fibrosis, the regulatory functions of hepatocytes remain elusive. Using an unbiased functional microRNA (miRNA) screening, we investigated the ability of hepatocytes to regulate fibrosis by fine-tuning gene expression via miRNA modulation. The in vivo functional analyses were performed by inhibiting miRNA in hepatocytes using adeno-associated virus in carbon-tetrachloride- and 3,5-di-diethoxycarbonyl-1,4-dihydrocollidine-induced liver fibrosis. Blocking miRNA-221-3p function in hepatocytes during chronic liver injury facilitated recovery of the liver and faster resolution of the deposited extracellular matrix. Furthermore, we demonstrate that reduced secretion of C–C motif chemokine ligand 2, as a result of post-transcriptional regulation of GNAI2 (G protein alpha inhibiting activity polypeptide 2) by miRNA-221-3p, mitigates liver fibrosis. Collectively, miRNA modulation in hepatocytes, an easy-to-target cell type in the liver, may serve as a potential therapeutic approach for liver fibrosis.Liver fibrosis and cirrhosis contribute to more than 1 million deaths per year worldwide1,2 (particularly, 170,000 per year in Europe3 and 33,539 per year in the United States4). The underlying pathologies leading to fibrosis and subsequently cirrhosis are chronic virus infection, alcoholic steatohepatitis, and non-alcoholic steatohepatitis.5 The manifestation of fibrosis is accompanied by the activation of quiescent HSCs, accumulation of excessive extracellular matrix, and hepatocyte dysfunction leading to liver failure.6 While the majority of efforts have been concentrated on the elucidation of HSCs function during fibrosis, the regulatory functions of hepatocyte, the main parenchymal cells of the liver, remain to be understood further. Specifically, how gene expression alterations, particularly at the post-transcriptional level in hepatocytes, regulate fibrosis remains to be investigated. MiRNAs, one of the post-transcriptional regulators of gene expression,7,8 have been reported to be deregulated in liver fibrosis.7,9 The majority of studies have examined the functions of miRNAs in HSCs; however, identification and functional analysis of hepatocyte miRNAs, which are capable of regulating fibrosis, remain to be investigated. Elucidation of such key fibrosis-regulating miRNAs in hepatocytes, an easy-to-target cell type in the liver, would allow development of successful therapeutics for liver fibrosis. SECTION Materials and methods SECTION Animals
Long noncoding RNAs (lncRNAs) play important roles in various biological processes, regulating gene expression by diverse mechanisms. However, how lncRNAs regulate liver repopulation is unknown. Herein, we aimed to identify lncRNAs that regulate liver repopulation and elucidate the signaling pathways involved. Herein, we performed 70% partial hepatectomy in wild-type and gene knockout mice. We then performed transcriptomic analyses to identify a divergent lncRNA termed lncHand2 that is highly expressed during liver regeneration. LncHand2 is constitutively expressed in the nuclei of pericentral hepatocytes in mouse and human livers. LncHand2 knockout abrogates liver regeneration and repopulation capacity. Mechanistically, lncHand2 recruits the Ino80 remodeling complex to initiate expression of Nkx1-2 in trans, which triggers c-Met (Met) expression in hepatocytes. Finally, knockout of both Nkx1-2 and c-Met causes more severe liver injury and poorer repopulation ability. Thus, lncHand2 promotes liver repopulation via initiating Nkx1-2-induced c-Met signaling. Our findings reveal that lncHand2 acts as a critical mediator regulating liver repopulation. It does this by inducing Nkx1-2 expression, which in turn triggers c-Met signaling.The liver is a central organ for homeostasis with considerable regenerative capacity.1–3 Liver transplantation is the only effective therapy for advanced liver disease. However, a shortage of donor organs is a real problem across the world. Hepatocyte cell therapy is thus an attractive alternative to liver transplantation. Mature hepatocytes harbor a remarkable capacity to proliferate upon liver injury.4 Liver regeneration occurs after the loss of hepatic tissue as a fundamental parameter. However, the molecular regulatory mechanisms of liver regeneration are still elusive.
