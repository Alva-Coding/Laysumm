We present a case series of three boys with childhood epilepsy with myoclonic–atonic seizures (EMAS) who achieved complete remission during childhood only to develop absence seizures during early adolescence. In all three cases, the recurrent seizures resolved again with antiseizure drugs, and two are currently medication-free for a second time. Doose syndrome (DS), otherwise known as epilepsy with myoclonic-atonic seizures (EMAS) and previously myoclonic-astatic epilepsy, is a seizure syndrome of young childhood associated with prominent myoclonic and astatic seizures [1]. EEG findings include bursts of 2- to 5-Hz spike and polyspike and wave complexes, with normal sleep architecture and posterior basic rhythm. EMAS typically presents with a peak onset between the ages of 3–4 years, with a range of 7 months and 6 years of age [1]. Antiseizure drugs reported to be the most effective for EMAS include ethosuximide, lamotrigine, and valproic acid, although levetiracetam is also widely used [2]. Certain antiseizure drugs may cause paradoxical worsening, including carbamazepine, oxcarbazepine, phenytoin, and vigabatrin. The ketogenic diet has been shown to be highly efficacious, and reports have suggested that it could be even considered as a first line therapy in these patients [3]. Close to two-thirds of patients achieve seizure remission, within 5–6 years of onset [4,5].
Rasmussen syndrome (RS) is a chronic encephalopathy with uncertain etiology and immune-mediated pathogenesis. The only definitive treatment is represented by functional hemispherectomy. We describe the case of a 6.5-year-old female patient who developed several episodes of focal, unilateral clonic seizure. Following laboratory and instrumental investigations, the patient was diagnosed with RS. A treatment with corticosteroids, intravenous immunoglobulin and the antiseizure therapy (carbamazepine and levitiracetam) did not completely control the seizures. Therefore, the patient was treated with mycophenolate mofetil (MMF), showing a good clinical response, with reduction of the seizure episodes, and stability of the radiological findings. This case suggests the potential utility of MMF in the immune approach to RS. 
Micro-blog has changed people’s life, study, and work styles. Every day, we want to know what public opinion news happens and how it evolves. Extracting and tracking these topics correctly help us better understand the latest public opinions and pay attention to their evolution. To extract topics from Microblog posts accurately, we adopt five unique features of micro-blogs to drive the joint probability distributions of all words and topics, and improve LDA into our topic extraction model(named MF-LDA). To track evolution trend of the topic, we propose a hot topic life cycle model (named HTLCM). We divide the HTLCM into five stages, namely, birth, growth, maturity, decline, and disappearance. The HTLCM determines whether a topic is the candidate hot topic or not and estimates hot topic evolution stages. On the other hand, we propose a hot topic tracking (shorten for HTT) algorithm which integrates MF-LDA and HTLCM. First, the HTT assigns candidate hot topics, which are labeled by HTLCM, to the corresponding time window according to the release time. Second, to obtain the hot topic in each time window, we input Micro-blog posts of each time window into MF-LDA in order. By analyzing changes in these hot topics, we track the changes in their contents. The experiment results show that MF-LDA has a lower perplexity and higher coverage rate than LDA under the same conditions. We conclude parameters of the Transition regions of our proposed HTLCM model. The MR, FR of our proposed HTLCM model are lower than 18%. The average P, R, F of the HTT algorithm are 85.64%, 84.97%, 85.66%, respectively. A practical application on topicFemale driver beats male driver in chengdu shows an excellent effect and practical significance of HTLCM model and HTT algorithm in extracting and tracking hot topics. With rapid development of communication technologies and popularization of smartphones, more and more people begin to use mobile Internet. On December 2017, the number of Internet users in China reached 731 million, among which 695 million are mobile Internet users. This proportion increases from 90.1% (the end of 2015) to 95.1% (Anon., 2019). The high-speed development of the mobile Internet network rapidly rise development of social network platforms, such as Sina Micro-blog.
Individual stock trend prediction is extremely valuable for investment management. Previous studies mainly focused on proposing effective approaches to make profits. However, there is an ineffectiveness in model evaluation due to the inconsistency between model’s performance and profitability. We name this inconsistency profit bias. In order to address the profit bias in model evaluation, this paper proposes a new effective metric, Mean Profit Rate (MPR). The effectiveness of metric is measured based on the correlation between the metric value and profit of the model. Experiments on five stock daily index data among four countries show that MPR outperforms the classification metrics in correlating to profit. In view of these findings, we suggest that MPR is a more effective metric than the classification metrics in stock trend prediction. The individual stock trend prediction is challenging due to the high volatility, irregularity, and noisy signal in the environment of the stock markets. In recent years, it draws a lot of attention from researchers in various areas, especially Artificial intelligence. Some studies treat it as a regression problem (Bollen et al., 2011; Schumaker and Chen, 2009), aiming to predict the future value of stock price or profits. While other studies treat it as a classification problem (Hsieh et al., 2011; Huang et al., 2008), aiming to predict the trend of stock price movement. In most cases, the classification approaches achieve higher profits than the regression ones (Leung et al., 2000). The effectiveness of different classification approaches in individual stock prediction has been widely explored (Choudhry and Garg, 2008; Lin et al., 2013; Wang and Choi, 0000; O’Connor and Madden, 2006; Lee, 2009a).
We present the case of a child with long-standing, super-refractory status epilepticus (SRSE) who manifested prompt and complete resolution of SRSE upon exposure to pure cannabidiol. SRSE emerged in the context of remote suspected encephalitis with previously well-controlled epilepsy. We discuss the extent to which response may be specifically attributed to cannabidiol, with consideration and discussion of multiple potential drug–drug interactions. Based on this case, we propose that adjunctive cannabidiol be considered in the treatment of SRSE. Super-refractory status epilepticus (SRSE) is defined as status epilepticus that continues or recurs at least 24 h after the onset of anesthesia [1]. SRSE is an important focus of ongoing research given the exceptionally high morbidity and mortality [2, 3], and the absence of evidence-based therapeutic options. Given the relatively low incidence of SRSE, clinical trials are quite challenging from a feasibility standpoint. Accordingly, although there are no randomized controlled trials supporting any specific treatment modality, there are abundant contemporary case reports and case series suggesting effectiveness of multiple therapies including ketamine [4], ketogenic diet therapy [5, 6], electroconvulsive therapy [7], thalamic deep brain stimulation [8], repetitive transcranial magnetic stimulation [9], surgical resection (even hemispherectomy) [10, 11], immunotherapy (i.e. corticosteroids) [12], and most recently, allopregnanolone [13–15].
Archaeological research has highlighted the importance of population movement and interaction in promoting cultural change and interaction in past societies. Strontium isotope ratios (87Sr/86Sr) are used worldwide to track prehistoric human population movement, and recent studies have provided new insight into the role of population diversity in the pre-Columbian American midcontinent. To track such movement, we have analyzed enamel from 222 small nonmigratory terrestrial and semiaquatic fauna from a series of midcontinental geographic locations to provide initial baseline regional 87Sr/86Sr information. Results of this study reveal considerable overlap in the strontium isotope ranges within the midcontinent, but also identify important isotopic differences between regions. We conclude that sufficient Sr variation exists within the midcontinent to identify the movement of individuals, however, the lack of regional specificity in Sr currently limits our ability to identify specific place(s) of origin for these individuals using Sr alone. Continued isotopic research offers the potential to produce a more detailed midcontinental isoscape, which combined with other geochemical, biological, and archaeological data, allows us to refine our understanding of the movement of people in pre-Columbian America. In light of this new information, we revisited our earlier case study of Cahokian immigration, reassessing new samples (558 teeth representing 338 individuals), and confirmed that the Cahokian population included a large number of nonlocal residents. Interpretations that employ population movement as a factor in cultural change have a long and uneven history in archaeology from a common use as an explanation of cultural transitions and distribution (via diffusion or migration) in the nineteenth century to their rapid abandonment in the early twentieth century (Adams et al., 1978:525–526). This decline became especially steep with the ascendancy of processual archaeology and its widespread application of environmental and neoevolutionary models of change (Chapman and Hamerow, 1997). Only recently has population movement become academically acceptable as an explanative for cultural transitions. In the United States, for example, consideration of migration has become common in the Southwest (e.g., Clark, 2001:1–2). Despite this resurgence, the antimigration mantra continues among postmodernists. For example, Storey and Jones (2011:19) cite Peter Ucko's (1995:12) statement that culture history is “at its most insidious (when coupled with theories of migration) [that have] deprive[d] whole peoples of any legitimate past.” In this strange twist, recognizing migration, in the eyes of some proponents, becomes potentially demeaning to modern descendants' self-identities (citing Arnold, 2007). Consequently, for both social and political reasons, interpretive models postulating migration remain underevaluated.
Mortality associated with cannabis used for treatment of epilepsy is not well documented. We discuss two fatalities in the setting of epilepsy and self-determined therapy with cannabis (SDTC). One patient had probable sudden unexpected death in epilepsy, the second death was due to seizure-associated drowning. Both directed SDTC over conventional anti-seizure medications. Where recreational cannabis is legal, decisions to use cannabis are often self-directed and independent of physician advice of cannabis risks, in part because physicians may not be aware of the risk of SDTC. Further study of morbidity and mortality of SDTC in patients with epilepsy is needed. In Washington State, marijuana legalization occurred for medicinal purposes in 1996 and for recreational use in 2012 [1]. Cannabis based therapies have been suggested for a broad range or problems including anxiety, insomnia, chronic pain, depression and epilepsy [2]. Publication of prospective trials supporting adjunctive effective use of cannabidiol oil (CBD) for the reduction of convulsive seizures in patients with Dravet (DS) and Lennox–Gastaut syndromes (LGS) occurred in 2017 [3,4]. In our Epilepsy clinic, patient inquiries regarding efficacy of cannabis have increased since 2012 and even more since the landmark trials and further with the USA Food and Drug Administration's rescheduling of CBD in 2018. Specifically, many patients are prepared to discuss cannabis use in the epilepsy clinic and are well versed through reviews available from various online and anecdotal resources, but with limited understanding of available peer-reviewed literature, including side effects demonstrated in the 2017 LGS and DS trials or from studies of artisanal cannabis use in epilepsy [1,3,4].
The ability to determine body mass from skulls is valuable for understanding various ecological, physiological, and evolutionary factors. In the Canidae, numerous methods to reconstruct body mass from measurements of the skull have been proposed, however there is no one-size fits all approach that can be applied across all species and subspecies. Added to this, current methods of reconstructing body mass are often complex, and have relatively high error rates. We aimed to produce a multivariate regression equation to estimate body mass of the Australian dingo (Canis dingo) from simple measurements of the skull, whilst ensuring that it could also be used in studies of encephalisation. To do this, we focussed on palate length (PL), palate width (PW) and the length of the first upper molar (M1). A total of 128 adult dingo (64 male; 64 female) crania from one region of Australia with known body mass were measured. Overall, the combination of PL and PW was the best predictor of body mass, with M1 having poor predictability. The model, mass (kg) = 0.246 ∗ (PL) + 0.320 ∗ (PW) − 24.757 produced a prediction error of 8.05%. Thus, these two measures of the palate provide simple and accurate predictors of body mass for the dingo. This will be useful for modern dingo specimens, as well as those found at archaeological sites and in museum collections that often consist of incomplete cranial material. The reconstruction of dingo body size is useful for evaluating variation in body mass through time, and across the Australian continent, particularly in the context of human activity. Body mass and size have long been linked to various ecological, physiological and evolutionary factors (Gittleman, 1985), with the aim to understand an animal or species' life history, general appearance, metabolic needs, and bite force (Damuth and MacFadden, 1990). In the Canidae specifically, knowledge of body mass can reveal information about geographic variation and adaptation, as well as the life histories and roles of canids in past human societies, particularly during domestication (Losey et al., 2015; Losey et al., 2017). Changes in body size across time and space may offer an insight into such aspects as geographic variation, changes to climate, and resource availability, such as the size of prey.
A large amount of well-preserved timbers was found during several archaeological excavations of the Faravel mining site (Southern French Alps, between 1950- and 2150 m a.s.l.). 232 of these timbers were sampled for dendrochronological analysis and 67% of them were dated. These 156 larch (Larix decidua Mill.) series, crossdated against existing reference chronologies, were averaged for a site chronology spanning from 777 to 1243. From this dataset, 33 timbers with (almost) complete sapwood allowed us to obtain tree felling years with seasonal resolution. The chronological distribution of these felling years highlights nine distinct mining phases that occurred between 1059 and 1243, revealing a discontinuous exploitation of the study site during the medieval period. In addition, the presence of late wood in the vast majority of complete samples, demonstrates that logging mainly occurred during late fall and early winter. These results, combined with historical, palynological and archaeological investigations, plead for short, seasonal, and low-intensity, mining campaigns, mainly carried out after the bulk of agropastoral activities using rudimentary techniques with limited impact on the forest cover. Medieval wood is frequently perfectly preserved and abundant (Bailly-Maître, 2008a; Tegel, 2012). At that time, wood was required for carpentry works and manufacturing equipment (floors, ladders, hoists, dragging roads, timbered shafts etc.). Mines are generally closed environments saturated with humidity and protected from light and temperature variations. Therefore, wood does not decompose as fast as it would in open-air, particularly when it is located in submerged and/or backfilled works. Numerous pieces of preserved wood have allowed dendrochronologists to build reliable and powerful cross-dated site chronologies for dating and for revealing phases of mining activity. Moreover, wood analyses can provide information about how species were selected for different uses, the mechanical properties and qualities of the selected wood, and forest management. In addition, tool traces may provide information about the tools and techniques used by miners for tree felling, debarking, limb removal, length reduction, and shaping. Consequently, dendrochronological analyses of mining sites are of high interest not only to dendrochronologists but also to archaeologists and historians.
Recent archaeological investigations (2009–2016) in the Sistema 7 Venado, a ceremonial complex located on the south part of the Monte Albán hill, Oaxaca, Mexico, demonstrates that occupation and ceremonial activity on the site can be traced back until 800 BCE. Around 200–300 CE, this site was ritually abandoned and its structures covered by a thick layer of soil. This process indirectly offered a good protection of architectural structures and occupation levels. The chemical composition of 416 obsidian artefacts discovered during the excavations campaigns were analysed by PIXE technique to determine their origin and to replace the Sistema 7 Venado development into the complex trade network of the central valleys of Oaxaca. The results led us to twelve obsidian sources, Otumba, Pacheco, Zacualtipán/Metzquititlán, Sierra de Pachuca, Malpaís, Paredón, Zaragoza, Guadalupe Victoria, Pico de Orizaba, Altotonga, Ucareo and Magdalena, and also revealed variations in the areas of supplies through time. 
Along with the constantly increasing complexity of industrial automation systems, machine learning methods have been widely applied to detecting abnormal states in such systems. Anomaly detection tasks can be treated as one-class classification problems in machine learning. Geometric methods can give an intuitive solution to such problems. In this paper, we propose a new geometric structure, oriented non-convex hulls, to represent decision boundaries used for one-class classification. Based on this geometric structure, a novel boundary based one-class classification algorithm is developed to solve the anomaly detection problem. Compared with traditional boundary-based approaches such as convex hulls based methods and one-class support vector machines, the proposed approach can better reflect the true geometry of target data and needs little effort for parameter tuning. The effectiveness of this approach is evaluated with artificial and real world data sets to solve the anomaly detection problem in Cyber–Physical-Production-Systems (CPPS). The evaluation results also show that the proposed approach has higher generality than the used baseline algorithms. Improving the reliability of Cyber–Physical Production Systems (CPPS) has become a central issue in research and industry to ensure high product quality and low cost (Gao et al., 2015). In safety-critical industrial automation systems, such as chemical production systems, wind power plants and nuclear energy systems, performance degradation, which may lead to terrible accidents, is normally not tolerated. Detecting system anomalies as early as possible to reduce unplanned machine breakdown is one of the most effective way to ensure system reliability.
Ninety years ago in the Zagros foothills of Iraq, Dorothy Garrod and her team excavated Zarzi cave, the type site of the Epipalaeolithic “Zarzian” lithic industry. Garrod reported the existence of “two small fragments of obsidian” in the principally chert-based microlithic assemblage. One of the two artifacts from Zarzi was analyzed by Renfrew and colleagues in a pioneering application of obsidian sourcing to the Near East, which elucidated links between Neolithic villages. It was, unfortunately, ambiguously assigned to their “Group 4c” obsidian, which occurs at two different sources, ∼120 km apart, in eastern Turkey. New interpretive methods — agent-based models, least-cost path analysis, and others — have been applied to the datasets of Renfrew and colleagues, furthering work on the mechanisms of Neolithicization. With respect to the Epipalaeolithic, though, all of these studies rely entirely on this single Zarzi artifact with an inconclusive attribution. Fortunately, the second Zarzi obsidian “fragment” — a burin spall — was “rediscovered” at the Peabody Museum of Archaeology and Ethnology at Harvard. Our study establishes that both artifacts came from Nemrut Dağ volcano, 400 km linearly and ≳650 km on foot. To do so, multivariate analyses were applied to the original spectroscopic data of Renfrew and colleagues, while state-of-the-art portable XRF was used to source the burin spall at Harvard's Peabody Museum. Comparison to two Epipalaeolithic sites in the Caucasus begins to reveal a patchwork of interaction spheres that highlight not only the potential of obsidian sourcing but also the considerable amount of work yet to be done. In the Mesopotamian highlands, the Epipalaeolithic (EP) — that is, the portion of the Upper Palaeolithic that falls between the Last Glacial Maximum (LGM, 26.5 to ∼20 ka) and the start of the Neolithic (12 ka) — is a crucial period for investigating the rise of food production, sedentism, and other aspects of the so-called “Neolithic package.” This timeframe is even considered “Period 0” in the Atlas des Sites du Proche-Orient (ASPO) chronological scheme, devised by the Maison de l'Orient et de la Méditerranée, as a means to conceptualize the rise of Neolithic phenomena that culminated in urbanism (Hours et al., 1994; Aurenche et al., 2001). A variety of stimuli have been proposed for this “revolution” that appears, at least, to coincide with the beginning of the Holocene (∼11.65 ka). Braidwood's (1951) “hilly flanks” hypothesis posited that agriculture and sedentism arose in the foothills of the Taurus and Zagros mountain ranges, where fertile land enabled grain gathering and which coincided with the natural habitats for the wild forms of domesticates. Binford (1968) and Flannery (1969), in response to Braidwood, held that demographic pressures drove a shift to food production. Hayden (1992) suggested that feasting and other opulent displays drove agriculture. Individuals who could amass a food surplus were able to transform it into, for example, exotic and desirable objects, facilitating the rise of social inequality. Price and Bar-Yosef (2011) point out that each of these scenarios is “very much a chicken-egg question, like the issue of population pressure, of which came first” (S167). There is, though, an increasing recognition that certain aspects of the “Neolithic” package may have emerged at different times and places during the EP (e.g., Akkermans, 2004; Watkins, 2010; Weide et al., 2018). Consequently, “Period 0” is an important time in which to seek archaeological evidence that can be contrasted against Neolithic datasets.
Applications in various fields of neutrosophic soft set theory enhance its appreciation to the researchers. Although, this uncertain solving tool can accomplish several types of real-life problems, but not able to deal with the decision-making problems in which considering an additional information of a corresponding parameter is needed to get a proper decision from a problem. But, complex neutrosophic soft sets can handle such type of decision-making problems. Consequently, in this study, we have given concentration on solving soft set based decision-making problems in the field of complex neutrosophic environment. Firstly, we have introduced some basic set-theoretic operations of complex neutrosophic sets including different types of unions, intersections and aggregations. Then, a new definition of score function of a complex neutrosophic number has been proposed. Additionally, the above-defined unions and intersections of complex neutrosophic sets have been stated for complex neutrosophic soft sets. Finally, by utilizing these proposed notions, we have offered a complex neutrosophic soft VIKOR approach to get a compromise optimal solution for single as well as multiple decision-maker based problems. Our proposed approach has been clarified by several real life-related problems including medical diagnosis problem, sustainable manufacturing material selection problem, company’s manager selection problem, etc. The feasibility and effectiveness of our proposed approach have also been included in the article. One of the important and challenging issues in our day-to-day life is the presence of uncertainty to be addressed. Neutrosophic set theory (Smarandache, 2003, 2005) is one of the innovative generalization of a bunch of theories, including, fuzzy set theory (Zadeh, 1965), intuitionistic fuzzy set theory (Atanassov, 1986), vague set theory (Gau and Buehrer, 1993) etc., through three independent membership grades (truth membership (T(x)), indeterminate membership (I(x)) and false membership (F(x)) where, T(x),I(x),F(x)∈[0,1] and 0≤T(x)+I(x)+F(x)≤3). In the last few decades, neutrosophic set has been improved very quickly and has been successfully used to solve various types of real-life problems. For instance, Ye (2015) used the cosine similarity measure of neutrosophic sets in medical diagnosis problem, Zhang et al. (2015) utilized the advantages of correlation coefficient of neutrosophic sets in solving multi-criteria decision-making problems, Wu et al. (2018) developed multi attribute group decision-making by using single-valued neutrosophic 2-tuple linguistic sets etc.
Cannabis use is associated with changes in brain structure and function; its neurotoxic effects are largely attributed to Δ9-tetrahydrocannabidiol. Whether such effects are present in patients with epilepsy exposed to a highly-purified cannabidiol isolate (CBD; Epidiolex®; Greenwich Biosciences, Inc.) has not been investigated to date. This preliminary study examines whether daily CBD dose of 15–25 mg/kg produces cerebral macrostructure changes and, if present, how they relate to changes in seizure frequency. Twenty-seven patients with treatment-resistant epilepsy were recruited from the University of Alabama at Birmingham CBD Program. Participants provided seizure frequency diaries (SF), completed the Chalfont Seizure Severity Scale (CSSS) and Adverse Events Profile (AEP), and underwent MRI before CBD (baseline) and after achieving a stable CBD dosage (on-CBD). We examined T1-weighted structural images for gray matter volume (GMV) and cortical thickness changes from baseline to on-CBD in 18 participants. Repeated measures t-tests confirmed decreases in SF [t(17) = 3.08, p = 0.0069], CSSS [t(17) = 5.77, p < 0.001], and AEP [t(17) = 3.04, p = 0.0074] between the two time-points. Voxel-level paired samples t-tests did not identify significant changes in GMV or cortical thickness between these two time-points. In conclusion, short-term exposure to highly purified CBD may not affect cortical macrostructure. Of the 1.2% of the population that suffers from epilepsy, one-third has treatment-resistant epilepsy (TRE) in which anti-seizure drug (ASD) mono- or poly-therapy does not control seizures [1]. In TRE, the primary tissue insult from chronic, uncontrolled seizures, combined with the secondary effects of failed ASDs, results in on-going insult to brain structure and function [2,3]. Patients with TRE are thus at increased risk for epilepsy-related mortality (e.g., sudden unexpected death in epilepsy; SUDEP), as well as more severe cognitive and neuropsychological impairments [2,3]. While ASDs treat seizures, they do not interrupt or reverse the underlying epileptogenesis [1] which underscores the necessity of finding treatments that interrupt or reverse the pathophysiology that underlies epilepsy. Recent evidence points to chronic neuroinflammation as one of the potential drivers of epileptogenesis [4–8]. Perpetual activation of the neuroinflammatory cascade can lower seizure threshold, resulting in dysfunction of the blood–brain-barrier and chronic neuronal hyperexcitability [4–8]. This notion highlights an under-exploited therapeutic target: the development of treatments that interrupt the neuroinflammatory cascade to provide seizure freedom to patients with TRE.
Beads were an important form of personal ornamentation in southern Levantine Pre-Pottery Neolithic societies and hold information on symbolic practices and exchange networks engaged by these communities. Establishing the exact mineralogical composition of stone beads and the precise methods used to manufacture beads are key to documenting shifts in these systems, but so far visual inspection has served as the primary mode of analysis for most assemblages. Here, we apply XRD and 3D digital optical microscopy to investigate raw material selection and bead manufacture at the Pre-Pottery Neolithic (PPN) settlement of el-Hemmeh, Jordan. Analyses reveal that PPNA bead assemblages emphasize the color green but included a wide variety of mineral types. The LPPNB assemblage exhibits a greater diversity in shape and color, including an emphasis on the use of red minerals, but consist of a more restricted range of minerals and lack the rectangular beads found in the PPNA. Color rather than suitability of material properties for bead making appears to have motivated choices for mineral selection. Bidirectional perforation was codified into the technological system and was consistently used to produce beads during the PPNA regardless of their size, thickness, or hardness. Stone beads are ubiquitous objects of material culture present in settlement and mortuary deposits that hold information on symbolic systems and participation in exchange networks. As personal adornment items, beads accrue social and symbolic meaning and are used by individuals to express group affiliation, reify social roles and status, and exchange cultural capital (Dubin, 2009; Jones, 2004; Lankton, 2003). In addition to signaling qualities associated with the use of beads as ornamentation, the series of technological and stylistic choices with bead manufacture are determined by both the physical properties of the raw material as well as cultural traditions, symbolic practices, and social networks (Lechtman, 1977; Lechtman, 1999; Hosler, 1994).
Temporary sites were a critical component of the prehistoric Near Eastern economy but, because of their ephemeral nature, are less frequently the focus of research than sedentary settlements. The present article presents the results of neutron activation analysis conducted on pottery from the temporary site of Saruq al-Hadid, United Arab Emirates. The results identified both continuity and change in the pottery consumed at the site in the Bronze and Iron Ages, which suggests that the peoples gathering here were integrated into economic practices observed at sedentary sites throughout southeastern Arabia. Pottery is ubiquitous at sites in the ancient Near East by the third millennium BCE. Ceramic vessels are used for the preparation, storage, and consumption of food as well as a variety of less well understood ritual practices. As a result, the study of pottery is an essential tool for illuminating past lifeways (Rice, 1987; Sinopoli, 1991). In particular, the geochemical analysis of pottery has proved to be an effective means of gaining insight into patterns of production and exchange (Speakman and Glascock, 2007). In the archaeology of the Near East, there is a tendency for the compositional analysis of ceramics to focus on pottery recovered from sedentary settlements, best typified by the man-made tells of mudbrick and stone that, following generation after generation of construction, collapse, and rebuilding, have come to loom over the surrounding agricultural plains. However, integral segments of ancient Near Eastern society moved between these sedentary settlements and temporary sites (Bernbeck, 2008; Porter, 2012). Much occurred beyond the sedentary villages, towns, and cities that is often not integrated into reconstructions of social and economic practice because of the difficulties associated with identifying temporary occupations and the allure of the mound. Compositional analysis of pottery from a temporary site has the potential to fill-in this gap and tie the groups that gathered at these locations into the social and economic practices documented at sedentary settlements, creating a more accurate and complete picture of the past.
This paper mainly proposes a novel method to construct a risk matrix for assessing safety risks in oil and gas industry. There are often multiple experts and multiple criteria involved in safety risk assessment problems and the assessment data are often given in the form of interval numbers. In order to better assess risks, the definition of interval number with distribution function and utility function is proposed in this paper. The frequency and the consequence of risk are only two needed indicators in risk matrix and their values are needed in the form of crisp values. So a multi-expert and multi-criterion information fusion based on interval number(MEMCIF-IN) model is built in this paper. Firstly, a multi-expert and multi-criterion fusion model is constructed to combine individual interval numbers into a collective interval number and integrate multiple criteria into a comprehensive index. In the fusion model, the weights of assessment experts are calculated based on the objective weights and the subjective weights simultaneously and the information of individual interval numbers is preserved without information loss in the final result. Secondly, a Continuous Weighted Ordered Weighted Aggregation(C-WOWA) operator is proposed. In the C-WOWA operator, the position weights which are generated by utility function and the importance weights which are generated by probability density function are considered at the same time. The position weights in the C-WOWA operator can correct the impact of experts’ risk attitudes and the importance weights can reflect the importance of the points themselves in an interval number. Finally, a risk matrix is constructed to show which risk is high and which is low. In addition, an application is implemented to show the practicality and rationality of the proposed method. To prevent safety accidents from happening, effective safety risk assessment is required in oil and gas industry (Markowski and Mannan, 2008). In general, quantitative way (Haldar and Mahadevan, 2000; Azizsoltani and Sadeghi, 2018; Azizsoltani and Haldar, 2018; Roy et al., 2019), semi-quantitative way (Ni et al., 2010; Mahamid, 2011; Ruan et al., 2015; Duan et al., 2016; Tian et al., 2018) and qualitative way (Goetz et al., 2017; Kelly et al., 2018) are three ways to implement risk assessment. The semi-quantitative way which has the advantages of both quantitative and qualitative ways is often used in risk assessment problems. Risk matrix approach, a semi-quantitative way, is used for risk assessment in this paper.
Recent research on plant and insect impressions on Jomon-period pottery strongly suggests not only the presence of household pests such as the maize weevil (Sitophilus zeamais), but also the existence of storage facilities for acorns and nuts inside pit houses. The presence of insect pests provides indirect evidence for human-mediated crop propagation on a large scale. The evidence also suggests the possibility of introduction of maize weevils into north-eastern Japan by the Jomon people via the transportation of infested foodstuffs. However, this ‘diffusion by man’ hypothesis has remained unproven because it is possible that these insect pests and their preferred foods originated locally. Aiming to resolve this issue, we examined pottery vessels discovered at a Jomon site in Hokkaido that contain the remains of adult maize weevils. The geographical and temporal distribution of these vessels suggests a possibility of the human-mediated diffusion of maize weevils and chestnut cultivation across oceanic barriers by the Jomon people. Ultimately, we could not prove this hypothesis because of lack of entomological evidence, but we could prove that colonization of the cultivated plants had played a role in the spread of stored food pests. A lot of discoveries of the maize weevil by further impression survey will be expected as a clue to trace the relationship history between people, cultivated plant and pest insects. Description of modern Sitophilus spp. SECTION Origin and diffusion
Hepatic resection and liver transplantation with adjuvant chemo- and radiotherapy are the mainstay of hepatocellular carcinoma (HCC) treatment, but the 5-year survival rate remains poor because of frequent recurrence and intrahepatic metastasis. Only sorafenib and lenvatinib are currently approved for the first-line treatment of advanced, unresected HCC, but they yield modest survival benefits. Thus, there is a need to identify new therapeutic targets to improve current HCC treatment modalities. The HCC tumor model was generated by hydrodynamic transfection of AKT1 and β-catenin (CTNNB1) oncogenes. Cancer cells with stemness properties were characterized following isolation using side population (SP) and CD44 surface markers by flow cytometry. The effect of Jak/Stat inhibitors was analyzed in vitro by using tumorsphere culture and in vivo using an allograft mouse model. Co-activation of both Wnt/β-catenin and Akt/mTOR pathways was found in 14.4% of our HCC patient cohort. More importantly, these patients showed poorer survival than those with either Wnt/β-catenin or Akt/mTOR pathway activation alone, demonstrating the clinical relevance of our study. In addition, we observed that Akt/β-catenin tumors contained a subpopulation of cells with stem/progenitor-like characteristics identified through SP analysis and expression of the cancer stem cell-like marker CD44, which may contribute to tumor self-renewal and drug resistance. Consequently, we identified small molecule inhibitors of the Jak/Stat pathway that demonstrated efficacy in mitigating tumor proliferation and formation in Akt/β-catenin-driven HCC. In conclusion, we have shown that Akt/β-catenin tumors contain a subpopulation of tumor-initiating cells with stem/progenitor-like characteristics which can be effectively targeted with inhibitors of the Jak/Stat pathway, demonstrating that inhibition of the Jak/Stat pathway could be an alternative method to overcome drug resistance and effectively treat Akt/β-catenin-driven HCC tumors. Hepatocellular carcinoma (HCC) is the fourth leading cause of cancer-related death worldwide with a very high mortality rate.1 Hepatitis B and C viral infections, aflatoxin B1, and alcohol abuse are main risks factors for HCC development.2 Despite treatment with advanced surgical resection, liver transplantation, or ablation, the 5-year survival rate for HCC patients remains poor due to frequent recurrence and intrahepatic metastasis.3 Currently, the first-line FDA-approved drugs for the treatment of advanced HCC are sorafenib and lenvatinib, both of which are multi-kinase inhibitors of Raf, VEGF, PDGF and c-kit signaling with anti-proliferative and anti-angiogenic activity.4–6 However, patients treated with these multi-kinase inhibitors showed marginal overall survival improvements of 2 to 3 months compared to placebo group, with low partial treatment response rates due to drug resistance in most instances.7 Therefore, there is a need to understand the biology of hepatocarcinogenesis and identify new therapeutic targets to improve current HCC treatment modalities.
Glycogen synthase kinase 3β (Gsk3β [Gsk3b]) is a ubiquitously expressed kinase with distinctive functions in different types of cells. Although its roles in regulating innate immune activation and ischaemia and reperfusion injuries (IRIs) have been well documented, the underlying mechanisms remain ambiguous, in part because of the lack of cell-specific tools in vivo. We created a myeloid-specific Gsk3b knockout (KO) strain to study the function of Gsk3β in macrophages in a murine liver partial warm ischaemia model. Compared with controls, myeloid Gsk3b KO mice were protected from IRI, with diminished proinflammatory but enhanced anti-inflammatory immune responses in livers. In bone marrow-derived macrophages, Gsk3β deficiency resulted in an early reduction of Tnf gene transcription but sustained increase of Il10 gene transcription on Toll-like receptor 4 stimulation in vitro. These effects were associated with enhanced AMP-activated protein kinase (AMPK) activation, which led to an accelerated and higher level of induction of the novel innate immune negative regulator small heterodimer partner (SHP [Nr0b2]). The regulatory function of Gsk3β on AMPK activation and SHP induction was confirmed in wild-type bone marrow-derived macrophages with a Gsk3 inhibitor. Furthermore, we found that this immune regulatory mechanism was independent of Gsk3β Ser9 phosphorylation and the phosphoinositide 3-kinase–Akt signalling pathway. In vivo, myeloid Gsk3β deficiency facilitated SHP upregulation by ischaemia–reperfusion in liver macrophages. Treatment of Gsk3b KO mice with either AMPK inhibitor or SHP small interfering RNA before the onset of liver ischaemia restored liver proinflammatory immune activation and IRI in these otherwise protected hosts. Additionally, pharmacological activation of AMPK protected wild-type mice from liver IRI, with reduced proinflammatory immune activation. Inhibition of the AMPK–SHP pathway by liver ischaemia was demonstrated in tumour resection patients. Gsk3β promotes innate proinflammatory immune activation by restraining AMPK activation. Liver inflammation triggered by ischaemia–reperfusion (IR) is an innate immune-dominated response, mediated by the sentinel pattern recognition receptor (PRR) system.1,2 Damage-associated molecular patterns (DAMPs),3 such as high-mobility group box 1, released from necrotic/stressed cells,4–6 activate PRRs, including Toll-like receptors (TLRs) and nucleotide-binding oligomerization domain-like receptors (NLRs),7–10 to initiate the proinflammatory immune response. At the cellular level, macrophages, the predominant innate immune cells in the liver, play a major role in reacting to DAMPs to trigger tissue inflammatory immune response.
Macrophages contribute to liver disease, but their role in cholestatic liver injury, including primary sclerosing cholangitis (PSC), is unclear. We tested the hypothesis that macrophages contribute to the pathogenesis of, and are therapeutic targets for, PSC. Immune cell profile, hepatic macrophage number, localization and polarization, fibrosis, and serum markers of liver injury and cholestasis were measured in an acute (intrabiliary injection of the inhibitor of apoptosis antagonist BV6) and chronic (Mdr2−/− mice) mouse model of sclerosing cholangitis (SC). Selected observations were confirmed in liver specimens from patients with PSC. Because of the known role of the CCR2/CCL2 axis in monocyte/macrophage chemotaxis, therapeutic effects of the CCR2/5 antagonist cenicriviroc (CVC), or genetic deletion of CCR2 (Ccr2−/− mice) were determined in BV6-injected mice. We found increased peribiliary pro-inflammatory (M1-like) and alternatively-activated (M2-like) monocyte-derived macrophages in PSC compared to normal livers. In both SC models, genetic profiling of liver immune cells identified a predominance of monocytes/macrophages; immunohistochemistry confirmed peribiliary monocyte-derived macrophage recruitment (M1>M2-polarized), which paralleled injury onset and was reversed upon resolution in acute SC mice. PSC, senescent and BV6-treated human cholangiocytes released monocyte chemoattractants (CCL2, IL-8) and macrophage-activating factors in vitro. Pharmacological inhibition of monocyte recruitment by CVC treatment or CCR2 genetic deletion attenuated macrophage accumulation, liver injury and fibrosis in acute SC. Peribiliary recruited macrophages are a feature of both PSC and acute and chronic murine SC models. Pharmacologic and genetic inhibition of peribiliary macrophage recruitment decreases liver injury and fibrosis in mouse SC. These observations suggest monocyte-derived macrophages contribute to the development of SC in mice and in PSC pathogenesis, and support their potential as a therapeutic target. Primary sclerosing cholangitis (PSC) is a progressive cholestatic liver disease, characterized by chronic inflammation of the biliary epithelium and fibrous obliterative cholangitis of the intra- and extra-hepatic bile ducts. Its etiology and pathogenic mechanisms are still largely unknown, which contributes to the lack of effective therapies aside from liver transplantation. Patients with PSC are at high risk for the development of cirrhosis and its sequela of portal hypertension, end-stage liver disease, and cholangiocarcinoma.1 Animal models contribute to our knowledge of disease pathogenesis and malignant transformation of the biliary epithelia, and provide insight regarding new therapeutic strategies. However, given the multifactorial nature of PSC, a single animal model recapitulating all the aspects of the disease may be difficult to achieve. Therefore, the study of multiple models may represent the best approach to investigate the molecular mechanisms of disease pathogenesis.
It is important to know which patients with hepatitis C are likely to develop liver-related complications after achieving a sustained virological response (SVR) to direct-acting antiviral (DAA) therapy. We aimed to describe the incidence of liver-related events in a population of patients with HCV-associated compensated advanced chronic liver disease (cACLD) who achieved SVR and to identify non-invasive parameters that predict the occurrence of liver-related events. This 2-center prospective study included 572 patients with cACLD who had been treated with DAAs and had achieved SVR. Patients had liver stiffness measurement (LSM) ≥10 kPa at baseline and had never decompensated (Child-Pugh class A). Laboratory work-up and LSM were performed at baseline and at 1 year of follow-up. The median follow-up was 2.8 years during which 32 patients (5.6%) presented with a liver-related event. The incidence rate (IR) of portal hypertension-related decompensation was 0.34/100 patient-years. These patients all had baseline LSM >20 kPa, and LSM did not improve during follow-up in 4 out of 5 of them. Hepatocellular carcinoma (HCC) occurred in 25 patients (IR 1.5/100 patient-years). Albumin levels at follow-up (hazard ratio [HR] 0.08; 95% CI 0.02–0.25) and LSM <10 kPa at follow-up (HR 0.33; 95% CI 0.11–0.96) were independently associated with the risk of HCC. Combining both predictors identified 2 groups with differing risk of HCC occurrence: those with LSM ≥20 kPa at follow-up or those with LSM between 10–20 kPa and albumin levels <4.4 g/dl were at the highest risk (IR ≥1.9/100 patient-years). Visual nomograms predicting HCC risk based on LSM and albumin at 1 year of follow-up were constructed. In patients with HCV-related cACLD who have achieved SVR with DAAs, HCC is the most frequent liver-related event. Both albumin levels and LSM are useful for stratifying patients based on their risk of developing HCC during follow-up. Direct-acting antivirals (DAAs) have become the new standard of care for patients with chronic hepatitis C virus (HCV) infection, demonstrating high efficacy, with most patients achieving a sustained virological response (SVR) regardless of HCV genotype. Due to their good safety profile, any patient in any stage of chronic liver disease (from mild fibrosis to decompensated cirrhosis) can be treated with DAAs.1 Therefore, it is important to know which patients will be prone to developing liver-related complications, such as hepatocellular carcinoma (HCC) or liver decompensation, requiring lifelong follow-up and which patients can be safely discharged from follow-up.
People who inject drugs (PWID) and are on opioid agonist therapy (OAT) might have lower adherence to direct-acting antivirals (DAAs) against hepatitis C virus (HCV) and, therefore, lower rates of sustained virologic response (SVR). Because of this, we compared the SVR rates to interferon-free DAA combinations in individuals receiving OAT and those not receiving OAT in a real-world setting. The HEPAVIR-DAA cohort, recruiting HIV/HCV-coinfected patients (NCT02057003), and the GEHEP-MONO cohort (NCT02333292), including HCV-monoinfected individuals, are ongoing prospective multicenter cohorts of patients receiving DAAs in clinical practice. We compared SVR 12 weeks after treatment (SVR12) in non-drug users and PWID, including those receiving or not receiving OAT. Intention-to-treat and per protocol analyses were performed. Overall, 1,752 patients started interferon-free DAA treatment. By intention-to-treat analysis, 778 (95%, 95% CI 93%–96%) never injectors, 673 (92%, 95% CI 89%–93%) PWID not on OAT and 177 (89%, 95% CI 83%–92%) PWID on OAT achieved SVR12 (p = 0.002). SVR12 rates for ongoing drug users (with or without OAT) were 68 (79%) compared with 1,548 (95%) for non-drug users (p <0.001). Among ongoing drug users, 15 (17%) were lost-to-follow-up, and 3 (3.5%) became reinfected. In the per protocol analysis, 97% never injectors, 95% PWID not on OAT and 95% PWID on OAT achieved SVR12 (p = 0.246). After adjustment, ongoing drug use was associated with SVR12 (intention-to-treat) and OAT use was not. HCV-infected PWID achieve high SVR12 rates with DAAs whether they are on OAT or not, but their response rates are lower than those of patients who never used drugs. This is mainly attributable to more frequent loss to follow-up. Accounting for active drug use during DAA therapy nearly closed the gap in SVR rates between the study groups. The epidemic of hepatitis C virus (HCV) infection has been driven in many countries by parenteral transmission through shared needles among people who inject drugs (PWID).1,2 Some reports have found that many PWID have a high willingness to receive HCV treatment.3,4 However, treatment uptake in the interferon era among PWID was very low, with about 1–2% of all HCV-infected PWID treated yearly.5,6 Most probably, this low incorporation of PWID to HCV treatment was due to barriers at different levels, including the poor tolerability of interferon. Simple, highly effective and safer all-oral direct-acting antivirals (DAAs) should have increased the uptake of HCV treatment among PWID. Indeed, current guidelines support the need to scale up treatment in PWID to effectively impact the HCV epidemic.7,8 However, patients with ongoing drug use or on opioid agonist therapy (OAT) are ineligible in some settings9 or might not be considered suitable to receive DAAs by some practitioners.10 To further complicate the scenario, PWID with HCV infection have been underrepresented in most clinical trials with DAAs.11–14 Indeed, only a few trials have specifically assessed treatment with all-oral DAA regimens in individuals on OAT.15–18
Data regarding the nephrotoxicity of sofosbuvir (SOF) remain controversial. We compared the changes of estimated glomerular filtration rate (eGFR) in patients with chronic hepatitis C virus (HCV) infection receiving SOF-based or SOF-free direct acting antivirals (DAAs). 481 patients with compensated liver diseases and eGFR ≥ 30 mL/min/1.73m2 receiving SOF-based (n = 308) or SOF-free (n = 173) DAAs for 12 weeks were prospectively enrolled. The eGFR was assessed from baseline to off-therapy week 24 by Chronic Kidney Disease Epidemiology Collaboration (CKD-EPI) equation. The eGFR evolutions between regimens were compared by generalized linear mixed-effects model (GLMM). Multivariate analysis was performed for factors affecting eGFR evolution. Patients receiving SOF-based DAAs had a significant on-treatment eGFR decline (adjusted slope coefficient difference: -1.24 mL/min/1.73m2/month [95% CI: -1.35 to -1.13]; p < 0.001) and a significant off-therapy eGFR improvement (adjusted slope coefficient difference: 0.14 mL/min/1.73m2/month [95% CI: 0.08 to 0.21]; p = 0.004) than patients receiving SOF-free DAAs. Multivariate analysis showed age per 1-year increase (adjusted slope coefficient difference: -0.05 mL/min/1.73m2/month [95% CI: -0.05 to -0.04]; p < 0.001), SOF-based DAAs (adjusted slope coefficient difference: -0.33 mL/min/1.73m2/month [95% CI: -0.49 to -0.17]; p < 0.001), and CKD stage (adjusted slope coefficient difference: -1.44 mL/min/1.73m2/month [95% CI: -1.58 to -1.30] and -3.59 mL/min/1.73m2/month [95% CI: -3.88 to -3.30] for stage 3 and stage 2 vs. stage 1; p < 0.001) were independent factors affecting eGFR evolution from baseline to SVR24. Patients receiving SOF-based DAAs have a quadratic trend with on-treatment worsening and off-therapy improving eGFR. Increasing age, SOF-based DAAs, and more advanced CKD stages are independently associated with eGFR decline in HCV patients receiving DAAs. Chronic hepatitis C virus (HCV) infection is a major health problem which affects 71 million people worldwide.1 Patients with chronic HCV infection may present with various hepatic and extrahepatic manifestations that lead to substantial morbidity and mortality.2-5 In contrast, the long-term health outcome improves following successful HCV eradication by antiviral therapies.6-8
Although the role of inflammation to combat infection is known, the contribution of metabolic changes in response to sepsis is poorly understood. Sepsis induces the release of lipid mediators, many of which activate nuclear receptors such as the peroxisome proliferator-activated receptor (PPAR)α, which controls both lipid metabolism and inflammation. We aimed to elucidate the previously unknown role of hepatic PPARα in the response to sepsis. Sepsis was induced by intraperitoneal injection of Escherichia coli in different models of cell-specific Ppara-deficiency and their controls. The systemic and hepatic metabolic response was analyzed using biochemical, transcriptomic and functional assays. PPARα expression was analyzed in livers from elective surgery and critically ill patients and correlated with hepatic gene expression and blood parameters. Both whole body and non-hematopoietic Ppara-deficiency in mice decreased survival upon bacterial infection. Livers of septic Ppara-deficient mice displayed an impaired metabolic shift from glucose to lipid utilization resulting in more severe hypoglycemia, impaired induction of hyperketonemia and increased steatosis due to lower expression of genes involved in fatty acid catabolism and ketogenesis. Hepatocyte-specific deletion of PPARα impaired the metabolic response to sepsis and was sufficient to decrease survival upon bacterial infection. Hepatic PPARA expression was lower in critically ill patients and correlated positively with expression of lipid metabolism genes, but not with systemic inflammatory markers. During sepsis, Ppara-deficiency in hepatocytes is deleterious as it impairs the adaptive metabolic shift from glucose to FA utilization. Metabolic control by PPARα in hepatocytes plays a key role in the host defense against infection. Sepsis, the systemic inflammatory response to poorly controlled infection, causes significant morbidity/mortality.1 Sepsis is often complicated by multiple organ failure, requiring intensive care. Recently, mortality in sepsis has decreased largely due to improved supportive strategies for critically ill patients, such as mechanical ventilation, renal replacement therapy and antibiotics. While current therapeutic strategies targeting the inflammatory response have been disappointing,1,2 metabolic interventions, such as intensive insulin therapy3 and controlled caloric deficit through delayed administration of parenteral nutrition,4 have shown some promise, suggesting that appropriate adaptation of energy metabolism contributes to proper defense against pathogens.5
The nuclear farnesoid X receptor (FXR) agonist obeticholic acid (OCA) has been developed for the treatment of liver diseases. We aimed to determine whether OCA treatment increases the risk of gallstone formation. Twenty patients awaiting laparoscopic cholecystectomy were randomized to treatment with OCA (25 mg/day) or placebo for 3 weeks until the day before surgery. Serum bile acids (BAs), the BA synthesis marker C4 (7α-hydroxy-4-cholesten-3-one), and fibroblast growth factor 19 (FGF19) were measured before and after treatment. During surgery, biopsies from the liver and the whole bile-filled gallbladder were collected for analyses of gene expression, biliary lipids and FGF19. In serum, OCA increased FGF19 (from 95.0 ± 8.5 to 234.4 ± 35.6 ng/L) and decreased C4 (from 31.4 ± 22.8 to 2.8 ± 4.0 nmol/L) and endogenous BAs (from 1,312.2 ± 236.2 to 517.7 ± 178.9 nmol/L; all p <0.05). At surgery, BAs in gallbladder bile were lower in patients that received OCA than in controls (OCA, 77.9 ± 53.6 mmol/L; placebo, 196.4 ± 99.3 mmol/L; p <0.01), resulting in a higher cholesterol saturation index (OCA, 2.8 ± 1.1; placebo, 1.8 ± 0.8; p <0.05). In addition, hydrophobic OCA conjugates accounted for 13.6 ± 5.0% of gallbladder BAs after OCA treatment, resulting in a higher hydrophobicity index (OCA, 0.43 ± 0.09; placebo, 0.34 ± 0.07, p <0.05). Gallbladder FGF19 levels were 3-fold higher in OCA patients than in controls (OCA, 40.3 ± 16.5 ng/L; placebo, 13.5 ± 13.1 ng/ml; p <0.005). Gene expression analysis indicated that FGF19 mainly originated from the gallbladder epithelium. Our results show for the first time an enrichment of FGF19 in human bile after OCA treatment. In accordance with its murine homolog FGF15, FGF19 might trigger relaxation and filling of the gallbladder which, in combination with increased cholesterol saturation and BA hydrophobicity, would enhance the risk of gallstone development. Bile acids (BAs) are amphipathic molecules that are synthesized from cholesterol in the liver. Once synthesized, they are conjugated with glycine or taurine and then excreted with bile into the small bowel from where about 95% are reabsorbed in the terminal ileum via the enterohepatic circulation. In addition to their detergent properties that aid lipid digestion, BAs serve as signaling molecules by activating various nuclear and membrane-bound receptors, in particular the nuclear farnesoid X receptor (FXR), which regulates BA, glucose and lipid metabolism.1,2 BA homeostasis is maintained through negative feedback activation of FXR both within the liver (via SHP-LRH-1/HNF-4α) and from the small intestine (via FGF19-FGFR4/ß-klotho) by decreasing the expression of cholesterol 7α-hydroxylase (CYP7A1), the key enzyme in BA synthesis.3,4
There is a paucity of data regarding antiviral therapy in hepatitis B virus (HBV)-infected infants aged <1 year who have elevated alanine aminotransferase. This study aims to assess the efficacy and safety of antiviral therapy initiated in infancy. A real-world cohort study was conducted from January 2010 to December 2017. HBV-infected infants under 1 year of age, with persistent elevation of alanine aminotransferase and high viral load, were recruited and divided into 2 groups. Group I included 18 infants whose parents chose to initiate antiviral therapy with lamivudine before 1 year of age. Group II included 11 infants whose parents chose to initiate antiviral therapy with interferon-α after 1 year of age and not to receive any antiviral therapies before 1 year of age. The main outcome measure was rate of serum HBV surface antigen (HBsAg) loss at month 12 of treatment. There were no statistical differences between Groups I and II regarding baseline characteristics. No infants in Group II developed spontaneous HBsAg loss before 1 year of age. In Group I, the cumulative rates of HBsAg loss at month 3, 6, 9 and 12 of treatment were 39%, 67%, 78% and 83%, respectively. In Group II, the cumulative rates of HBsAg loss at month 3, 6, 9 and 12 of treatment were 18%, 27%, 27% and 36%, respectively. Statistical differences existed in the cumulative rates of HBsAg loss between the 2 groups (log-rank test, p = 0.0023). No serious adverse events occurred in the study. Early initiation of antiviral therapy for infantile-onset hepatitis B contributes to a rapid and significant loss of HBsAg. Further trials with larger cohorts are needed to verify our results. Hepatitis B virus (HBV) infection is a global public health problem that causes liver-related morbidity and mortality.1–3 Despite passive-active immunoprophylaxis using hepatitis B vaccination with or without hepatitis B immunoglobulin (HBIg), up to 8%–10% of newborns of HBV surface antigen (HBsAg)-positive mothers still acquire HBV infection.4 Most infected infants acquire HBV infection asymptomatically during perinatal period and have normal alanine aminotransferase (ALT) levels; however, some cases may present with onset hepatitis with elevated ALT.5 As an unusual yet serious condition, infantile-onset hepatitis B after neonatal immunoprophylaxis has scarcely been studied.
Acute-on-chronic liver failure (ACLF), which develops in patients with cirrhosis, is characterized by intense systemic inflammation and organ failure(s). Because systemic inflammation is energetically expensive, its metabolic costs may result in organ dysfunction/failure. Therefore, we aimed to analyze blood metabolome in patients with cirrhosis, with and without ACLF. We performed untargeted metabolomics using liquid chromatography coupled to high-resolution mass spectrometry in serum from 650 patients with AD (acute decompensation of cirrhosis, without ACLF), 181 with ACLF, 43 with compensated cirrhosis, and 29 healthy subjects. Of the 137 annotated metabolites identified, 100 were increased in patients with ACLF of any grade, relative to those with AD, and 38 composed a distinctive blood metabolite fingerprint for ACLF. Among patients with ACLF, the intensity of the fingerprint increased across ACLF grades, and was similar in patients with kidney failure and in those without, indicating that the fingerprint reflected not only decreased kidney excretion but also altered cell metabolism. The higher the ACLF-associated fingerprint intensity, the higher plasma levels of inflammatory markers, tumor necrosis factor α, soluble CD206, and soluble CD163. ACLF was characterized by intense proteolysis and lipolysis; amino acid catabolism; extra-mitochondrial glucose metabolism through glycolysis, pentose phosphate, and D-glucuronate pathways; depressed mitochondrial ATP-producing fatty acid β-oxidation; and extra-mitochondrial amino acid metabolism giving rise to metabolites which are metabotoxins. In ACLF, intense systemic inflammation is associated with blood metabolite accumulation witnessing profound alterations in major metabolic pathways, in particular inhibition in mitochondrial energy production, which may contribute to the existence of organ failures. The results of the CANONIC study were used to redefine acute-on-chronic liver failure (ACLF) as a syndrome which develops in patients with cirrhosis and acute decompensation (AD) and is characterized by an intense systemic inflammation1,2-4 associated with different combinations of organ failures among the six major organ systems (liver, kidney, brain, coagulation, circulation, and respiration), giving rise to different clinical phenotypes1. ACLF is very frequent, affecting 30%-40% of hospitalized patients, and is associated with high short-term mortality rate (30% at 28 days)1. In most cases of ACLF, the activation of innate immune cells by pathogen-associated molecular patterns (PAMPs) are thought to play a major role in the induction of systemic inflammation.5 However, the intrinsic mechanisms of ACLF at the tissue and cellular levels that contribute to the development and maintenance of organ failures are unknown. Understanding these mechanisms should not only result in progresses in the knowledge on pathophysiology of ACLF, but also could provide clues to the development of new biomarkers of organ dysfunction/failure and identification of targets for new therapies of organ failures which are urgently needed.
To date, evidence on the association between physical activity and risk of hepatobiliary cancers has been inconclusive. We examined this association in the European Prospective Investigation into Cancer and Nutrition cohort (EPIC). We identified 275 hepatocellular carcinoma (HCC) cases, 93 intrahepatic bile duct cancers (IHBCs), and 164 non-gallbladder extrahepatic bile duct cancers (NGBCs) among 467,336 EPIC participants (median follow-up 14.9 years). We estimated cause-specific hazard ratios (HRs) for total physical activity and vigorous physical activity and performed mediation analysis and secondary analyses to assess robustness to confounding (e.g. due to hepatitis virus infection). In the EPIC cohort, the multivariable-adjusted HR of HCC was 0.55 (95% CI 0.38–0.80) comparing active and inactive individuals. Regarding vigorous physical activity, for those reporting >2 hours/week compared to those with no vigorous activity, the HR for HCC was 0.50 (95% CI 0.33–0.76). Estimates were similar in sensitivity analyses for confounding. Total and vigorous physical activity were unrelated to IHBC and NGBC. In mediation analysis, waist circumference explained about 40% and body mass index 30% of the overall association of total physical activity and HCC. These findings suggest an inverse association between physical activity and risk of HCC, which is potentially mediated by obesity. Liver cancer was the fourth leading cause of cancer death in 2015.1 Liver cancer is responsible for around 47,000 deaths per year in the European Union.2 Hepatocellular carcinoma (HCC) is the most common type of primary liver cancer derived from hepatocytes and it accounts for 85–90% of all primary liver cancers worldwide. It is the fifth most common cancer in men and the seventh most common cancer in women.1 The distribution of HCC varies greatly according to geographic location and it is more common in low- and middle-income countries than in developed countries. HCC more frequently occurs in Asia and Africa than in Europe and the United States. The strongest risk factor for HCC is cirrhosis, a condition that is related to hepatitis B virus (HBV), hepatitis C virus (HCV), excessive consumption of alcohol, and exposure to aflatoxin B1.1 The geographic variability of HCC incidence has been widely associated to the different distribution of HBV and HCV infections.1,3 In high-income countries, the main risk factors for HCC are smoking, alcoholic cirrhosis, diabetes, obesity, and non-alcoholic hepatic steatosis.1,4,5 The recent increase in HCC incidence is thought to be caused by increases in obesity, diabetes, and physical inactivity.6,7 The Physical Activity Collaboration of the National Cancer Institute’s Cohort Consortium performed a pooled analysis of 10 prospective US and European cohorts and found that high compared with low leisure-time physical activity was associated with a 27% lower risk of liver cancer incidence.8 Other prospective studies from the United States and East Asian countries support an association between physical activity and lower risk of hepatobiliary cancers.8–13 However, the World Cancer Research Fund International judged that the evidence was not convincing to support an effect of physical activity on liver cancer.14 Similarly, an umbrella review provided limited evidence for an association between physical activity and liver cancer.15 We report results from the European Prospective Investigation into Cancer and Nutrition (EPIC) cohort to provide additional evidence on the relationship between physical activity and HCC and other hepatobiliary cancers. SECTION Patients and methods SECTION Study population and data collection
Excessive alcohol consumption is one of the major causes of hepatocellular carcinoma (HCC). Approximately 30–40% of the Asian population are deficient for aldehyde dehydrogenase 2 (ALDH2), a key enzyme that detoxifies the ethanol metabolite acetaldehyde. However, how ALDH2 deficiency affects alcohol-related HCC remains unclear. ALDH2 polymorphisms were studied in 646 patients with viral hepatitis B (HBV) infection, who did or did not drink alcohol. A new model of HCC induced by chronic carbon tetrachloride (CCl4) and alcohol administration was developed and studied in 3 lines of Aldh2-deficient mice: including Aldh2 global knockout (KO) mice, Aldh2*1/*2 knock-in mutant mice, and liver-specific Aldh2 KO mice. We demonstrated that ALDH2 deficiency was not associated with liver disease progression but was associated with an increased risk of HCC development in cirrhotic patients with HBV who consumed excessive alcohol. The mechanisms underlying HCC development associated with cirrhosis and alcohol consumption were studied in Aldh2-deficient mice. We found that all 3 lines of Aldh2-deficient mice were more susceptible to CCl4 plus alcohol-associated liver fibrosis and HCC development. Furthermore, our results from in vivo and in vitro mechanistic studies revealed that after CCl4 plus ethanol exposure, Aldh2-deficient hepatocytes produced a large amount of harmful oxidized mitochondrial DNA via extracellular vesicles, which were then transferred into neighboring HCC cells and together with acetaldehyde activated multiple oncogenic pathways (JNK, STAT3, BCL-2, and TAZ), thereby promoting HCC. ALDH2 deficiency is associated with an increased risk of alcohol-related HCC development from fibrosis in patients and in mice. Mechanistic studies reveal a novel mechanism that Aldh2-deficient hepatocytes promote alcohol-associated HCC by transferring harmful oxidized mitochondrial DNA-enriched extracellular vesicles into HCC and subsequently activating multiple oncogenic pathways in HCC. Hepatocellular carcinoma (HCC) is a leading cause of cancer-related death worldwide. Chronic alcohol abuse is a major cause of HCC development; its metabolite, acetaldehyde is believed to play an important role in inducing HCC.1 Mitochondrial aldehyde dehydrogenase (ALDH2) is a major enzyme for acetaldehyde elimination. The Glu487Lys polymorphism (also named rs671, with the glutamate corresponding to *1 allele, and lysine corresponding to *2 allele) at codon 487 in the ALDH2 gene causes the substitution of glutamate (Glu) by lysine (Lys), which exists in approximately 40% of east Asian populations.2 Such a polymorphism (Glu to Lys, or G to A, or *1 to *2) disrupts ALDH2 activity, causing high blood acetaldehyde concentration and “alcohol flush reactions” after alcohol consumption.1 Alcoholics with heterozygous ALDH2*1/*2 or homozygous ALDH2*2/*2 polymorphism have ALDH2 deficiency and have an increased risk of developing digestive tract cancers,3 however, the association of this polymorphism with HCC development and how acetaldehyde affects HCC still remain obscure.
The role of hepatitis B virus (HBV)-specific CD4 T cells in patients with chronic HBV infection is not clear. Thus, we aimed to elucidate this in patients with chronic infection, and those with hepatitis B flares. Through intracellular IFN-γ and TNF-α staining, HBV-specific CD4 T cells were analyzed in 68 patients with chronic HBV infection and alanine aminotransferase (ALT) <2x the upper limit of normal (ULN), and 28 patients with a hepatitis B flare. HBV-specific HLA-DRB1*0803/HLA-DRB1*1202-restricted CD4 T cell epitopes were identified. TNF-α producing cells were the dominant population in patients’ HBV-specific CD4 T cells. In patients with ALT <2xULN, both the frequency and the dominance of HBV-specific IFN-γ producing CD4 T cells increased sequentially in patients with elevated levels of viral clearance: HBV e antigen (HBeAg) positive, HBeAg negative, and HBV surface antigen (HBsAg) negative. In patients with a hepatitis B flare, the frequency of HBV core-specific TNF-α producing CD4 T cells was positively correlated with patients’ ALT and total bilirubin levels, and the frequency of those cells changed in parallel with the severity of liver damage. Patients with HBeAg/HBsAg loss after flare showed higher frequency and dominance of HBV-specific IFN-γ producing CD4 T cells, compared to patients without HBeAg/HBsAg loss. Both the frequency and the dominance of HBV S-specific IFN-γ producing CD4 T cells were positively correlated with the decrease of HBsAg during flare. A differentiation process from TNF-α producing cells to IFN-γ producing cells in HBV-specific CD4 T cells was observed during flare. Eight and 9 HBV-derived peptides/pairs were identified as HLA-DRB1*0803 restricted epitopes and HLA-DRB1*1202 restricted epitopes, respectively. HBV-specific TNF-α producing CD4 T cells are associated with liver damage, while HBV-specific IFN-γ producing CD4 T cells are associated with viral clearance in patients with chronic HBV infection. In chronic hepatitis B virus (HBV) infection, HBV-specific T cells plays a pivotal role in viral clearance and liver damage. Studies on a chimpanzee model of HBV acute infection have demonstrated an irreplaceable role of CD8 T cells in HBV clearance and disease pathogenesis.1 In patients with chronic HBV infection, HBV-specific CD8 T cells usually show functional exhaustion or even clonal deletion,2–7 which is deemed a major reason for the HBV persistence in those patients.
Interventions aimed at lifestyle changes are pivotal for the treatment of non-alcoholic fatty liver disease (NAFLD), and web-based programs might help remove barriers in both patients and therapists. In the period 2010–15, 716 consecutive NAFLD cases (mean age, 52; type 2 diabetes, 33%) were treated in our Department with structured programs. The usual protocol included motivational interviewing and a group-based intervention (GBI), chaired by physicians, dietitians and psychologists (five weekly meetings, n = 438). Individuals who could not attend GBI entered a web-based intervention (WBI, n = 278) derived from GBI, with interactive games, learning tests, motivational tests, and mail contacts with the center. The primary outcome was weight loss ≥10%; secondary outcomes were alanine aminotransferase within normal limits, changes in lifestyle, weight, alanine aminotransferase, and surrogate markers of steatosis and fibrosis. GBI and WBI cohorts had similar body mass index (mean, 33 kg/m2), with more males (67% vs. 45%), younger age, higher education, and more physical activity in the WBI group. The two-year attrition rate was higher in the WBI group. Healthy lifestyle changes were observed in both groups and body mass index decreased by almost two points; the 10% weight target was reached in 20% of WBI cases vs. 15% in GBI (not significant). In logistic regression analysis, after adjustment for confounders and attrition rates, WBI was not associated with a reduction of patients reaching short- and long-term 10% weight targets. Liver enzymes decreased in both groups, and normalized more frequently in WBI. Fatty liver index was reduced, whereas fibrosis remained stable (NAFLD fibrosis score) or similarly decreased (Fib-4). WBI is not less effective than common lifestyle programs, as measured by significant clinical outcomes associated with improved histological outcomes in NAFLD. eHealth programs may effectively contribute to NAFLD control. The burden associated with non-alcoholic fatty liver disease (NAFLD) is becoming a major problem for health systems worldwide.1 As part of the metabolic syndrome, NAFLD prevalence is increasing in parallel with the epidemics of obesity and diabetes;2 although in most cases NAFLD remains a non-progressive disease, in some cases non-alcoholic steatohepatitis (NASH) and progressive fibrosis may occur, finally progressing to cirrhosis and hepatocellular carcinoma.3 Thus, the costs associated with liver disease of metabolic origin and its complications are likely to soon outweigh the costs of liver diseases of viral origin.4
Although gadoxetate disodium-enhanced magnetic resonance imaging (MRI) shows higher sensitivity for diagnosing hepatocellular carcinoma (HCC), its arterial-phase images may be unsatisfactory because of weak arterial enhancement. We investigated the clinical effectiveness of arterial subtraction images from gadoxetate disodium-enhanced MRI for diagnosing early-stage HCC using the Liver Imaging Reporting and Data System (LI-RADS) v2018. In 258 patients at risk of HCC who underwent gadoxetate disodium-enhanced MRI in 2016, a total of 372 hepatic nodules (273 HCCs, 18 other malignancies, and 81 benign nodules) of 3.0 cm or smaller were retrospectively analyzed. Final diagnosis was assessed histopathologically or clinically (marginal recurrence after treatment or change in lesion size on follow-up imaging). The detection rate for arterial hyperenhancement was compared between ordinary arterial-phase and arterial subtraction images, and the benefit of arterial subtraction images in diagnosing HCC using LI-RADS was assessed. Arterial subtraction images had a significantly higher detection rate for arterial hyperenhancement than ordinary arterial-phase images, both for all hepatic nodules (72.3% vs. 62.4%, p <0.001) and HCCs (91.9% vs. 80.6%, p <0.001). Compared with ordinary arterial-phase images, arterial subtraction images significantly increased the sensitivity of LI-RADS category 5 for diagnosis of HCC (64.1% [173/270] vs. 55.9% [151/270], p <0.001), without significantly decreasing specificity (92.9% [91/98] vs. 94.9% [93/98], p = 0.155). For histopathologically confirmed lesions, arterial subtraction images significantly increased sensitivity to 68.8% (128/186) from the 61.3% (114/186) of ordinary arterial-phase images (p <0.001), with a minimal decrease in specificity to 84.8% (39/46) from 89.1% (41/46) (p = 0.151). Arterial subtraction images of gadoxetate disodium-enhanced MRI can significantly improve the sensitivity of early-stage HCC diagnosis using LI-RADS, without a significant decrease in specificity. Hepatocellular carcinoma (HCC) is the most common primary hepatic malignancy and the third most frequent cause of cancer-related deaths.1,2 Although the prognosis of patients with advanced HCC remains poor, patients with early-stage HCC are eligible for curative treatments such as surgical resection, local ablation, and liver transplantation.3,4 Therefore, accurate imaging diagnosis of early-stage HCC is important.
Treatment programs for people who inject drugs (PWID), including prisoners, are important for achieving hepatitis C elimination targets. There are multiple barriers to treatment of hepatitis C in prisons, including access to specialist physicians, testing and antiviral therapy, short prison sentences, and frequent inter-prison transfer. We aimed to assess the effectiveness of a nurse-led model of care for the treatment of prisoners with hepatitis C. A statewide program for assessment and management of hepatitis C was developed in Victoria, Australia to improve access to care for prisoners. This nurse-led model of care is supported by telemedicine to provide decentralized care within all prisons in the state. We prospectively evaluated the feasibility and efficacy of this nurse-led model of care for hepatitis C within the 14 adult prisons over a 13-month period. The primary endpoint was sustained virological response at post-treatment week 12 (SVR12) using per protocol analysis. There were 416 prisoners included in the analysis. The median age was 41 years, 90% were male, 50% had genotype 3 and 44% genotype 1 hepatitis C and 21% had cirrhosis. Injecting drug use was reported by 68% in the month prior to prison entry, 54% were receiving opioid substitution therapy, and 86% reported never previously engaging with specialist HCV care. Treatment duration was 8 weeks in 24%, 12 weeks in 59%, and 24 weeks in 17% of treatment courses. The SVR12 rate was 96% (301/313) per protocol. Inter-prison transfer occurred during 26% of treatment courses but was not associated with lower SVR12 rates. No treatment-related serious adverse events occurred. Hepatitis C treatment using a decentralized, nurse-led model of care is highly effective and can reach large numbers of prisoners. Large scale prison treatment programs should be considered to support hepatitis C elimination efforts. The World Health Organization (WHO) has set global targets for the elimination of viral hepatitis as a public health threat.1 For hepatitis C, the goals are to reduce incidence by 80% and mortality by 65% by 2030. In Western countries where injecting drug use is the dominant risk factor for hepatitis C transmission,2 eliminating incident infection will require coordinated efforts using harm reduction strategies as well as “treatment as prevention” to interrupt transmission among people who inject drugs (PWID). This population has not typically been well engaged with specialist care, highlighting the need to develop new models of care for hepatitis C among marginalized, high transmitting populations.
Gasdermin D (GSDMD)-executed programmed necrosis is involved in inflammation and controls interleukin (IL)-1β release. However, the role of GSDMD in non-alcoholic steatohepatitis (NASH) remains unclear. We investigated the role of GSDMD in the pathogenesis of steatohepatitis. Human liver tissues from patients with non-alcoholic fatty liver disease (NAFLD) and control individuals were obtained to evaluate GSDMD expression. Gsdmd knockout (Gsdmd−/−) mice, obese db/db mice and their wild-type (WT) littermates were fed with methionine-choline deficient (MCD) or control diet to induce steatohepatitis. The Gsdmd−/− and WT mice were also used in a high-fat diet (HFD)-induced NAFLD model. In addition, Alb-Cre mice were administered an adeno-associated virus (AAV) vector that expressed the gasdermin-N domain (AAV9-FLEX-GSDMD-N) and were fed with either MCD or control diet for 10 days. GSDMD and its pyroptosis-inducing fragment GSDMD-N were upregulated in liver tissues of human NAFLD/NASH. Importantly, hepatic GSDMD-N protein levels were significantly higher in human NASH and correlated with the NAFLD activity score and fibrosis. GSDMD-N remained a potential biomarker for the diagnosis of NASH. MCD-fed Gsdmd−/− mice exhibit decreased severity of steatosis and inflammation compared with WT littermates. GSDMD was associated with the secretion of pro-inflammatory cytokines (IL-1β, TNF-α, and MCP-1 [CCL2]) and persistent activation of the NF-ĸB signaling pathway. Gsdmd−/− mice showed lower steatosis, mainly because of reduced expression of the lipogenic gene Srebp1c (Srebf1) and upregulated expression of lipolytic genes, including Pparα, Aco [Klk15], Lcad [Acadl], Cyp4a10 and Cyp4a14. Alb-Cre mice administered with AAV9-FLEX-GSDMD-N showed significantly aggravated steatohepatitis when fed with MCD diet. As an executor of pyroptosis, GSDMD plays a key role in the pathogenesis of steatohepatitis, by controlling cytokine secretion, NF-ĸB activation, and lipogenesis. Non-alcoholic fatty liver disease (NAFLD) represents a multi-step biological disorder in the liver, increasing the risk of cirrhosis and tumorigenesis.1,2 The key aspects of steatohepatitis have been precisely mimicked by extensive basic and translation research. This has enabled the reductionist assessment of genes and dietary factors involved in the pathogenesis of NAFLD.3,4 Toxic lipid accumulation in the liver acts as the primary insult which initiates and propagates damage, leading to hepatocyte injury and resultant inflammation.5,6 It is important to note that inflammation in the liver is believed to be the compelling feature that transforms simple steatosis to steatohepatitis, perpetuating hepatocellular injury and subsequent cell death, and promoting liver fibrosis.7–9 However, the molecular basis behind the inflammatory response leading to steatohepatitis is still largely unknown.
Direct-acting antiviral (DAA) therapy for HCV has high efficacy and limited toxicity. We hypothesised that the efficacy of glecaprevir-pibrentasvir for chronic HCV with a simplified treatment monitoring schedule would be non-inferior to a standard treatment monitoring schedule. In this open-label multicentre phase IIIb trial, treatment-naïve adults with chronic HCV without cirrhosis were randomly assigned (2:1) to receive glecaprevir-pibrentasvir 300 mg–120 mg daily for 8 weeks administered with a simplified or standard monitoring strategy. Clinic visits occurred at baseline and post-treatment week 12 in the simplified arm, and at baseline, week 4, week 8, and post-treatment week 12 in the standard arm. Study nurse phone contact occurred at week 4 and week 8 in both arms. Participants requiring adherence support were not eligible, including those reporting recent injecting drug use. The primary endpoint was sustained virological response at post-treatment week 12 (SVR12), with a non-inferiority margin of 6%. Overall, 380 participants (60% male, 47% genotype 1, 32% genotype 3) with chronic HCV were randomised and treated with glecaprevir-pibrentasvir in the simplified (n = 253) and standard (n = 127) arms. In the intention-to-treat population, SVR12 was 92% (95% CI 89%–95%) in the simplified and 95% (95% CI 92%–99%) in the standard arm (difference between arms −3.2%; 95% CI −8.2% to 1.8%) and did not reach non-inferiority. In the per-protocol population, SVR12 was 97% (95% CI 96%–99%) in the simplified and 98% (95% CI 96%–100%) in the standard arm. No treatment-related serious adverse events were reported. In patients with chronic HCV infection without cirrhosis, treatment with glecaprevir-pibrentasvir was safe and effective. In comparison to standard monitoring, a simplified monitoring schedule did not achieve non-inferiority. Trial Registration: clinicaltrials.gov Identifier: NCT03117569. Globally, an estimated 71 million people have chronic HCV infection.1 HCV treatment was interferon-based for more than 2 decades, with the addition of ribavirin,2 pegylated-interferon,3 and first-generation protease inhibitor direct-acting antiviral (DAA) therapies (telaprevir, boceprevir),4,5 providing stepwise improvements in efficacy as defined by sustained virological response (SVR). Despite these improvements, treatment uptake remained low in most countries, with <1% to 5% of people with chronic HCV initiating therapy each year.6
Biliary tract cancers (BTCs) are clinically and pathologically heterogeneous and respond poorly to treatment. Genomic profiling can offer a clearer understanding of their carcinogenesis, classification and treatment strategy. We performed large-scale genome sequencing analyses on BTCs to investigate their somatic and germline driver events and characterize their genomic landscape. We analyzed 412 BTC samples from Japanese and Italian populations, 107 by whole-exome sequencing (WES), 39 by whole-genome sequencing (WGS), and a further 266 samples by targeted sequencing. The subtypes were 136 intrahepatic cholangiocarcinomas (ICCs), 101 distal cholangiocarcinomas (DCCs), 109 peri-hilar type cholangiocarcinomas (PHCs), and 66 gallbladder or cystic duct cancers (GBCs/CDCs). We identified somatic alterations and searched for driver genes in BTCs, finding pathogenic germline variants of cancer-predisposing genes. We predicted cell-of-origin for BTCs by combining somatic mutation patterns and epigenetic features. We identified 32 significantly and commonly mutated genes including TP53, KRAS, SMAD4, NF1, ARID1A, PBRM1, and ATR, some of which negatively affected patient prognosis. A novel deletion of MUC17 at 7q22.1 affected patient prognosis. Cell-of-origin predictions using WGS and epigenetic features suggest hepatocyte-origin of hepatitis-related ICCs. Deleterious germline mutations of cancer-predisposing genes such as BRCA1, BRCA2, RAD51D, MLH1, or MSH2 were detected in 11% (16/146) of BTC patients. BTCs have distinct genetic features including somatic events and germline predisposition. These findings could be useful to establish treatment and diagnostic strategies for BTCs based on genetic information. Biliary tract cancer (BTC) or cholangiocarcinoma (CC) is a rare cancer worldwide, but prevalent in some areas, where a specific risk factor of environmental exposure is involved in BTC development, such as chronic cholangitis,1,2 liver fluke infection in Thailand,1,2 viral hepatitis,1,2 aflatoxin exposure in Chile,3 or other chemical exposures.2,4 According to its anatomical location, BTCs are mainly classified as intrahepatic cholangiocarcinoma (ICC), extrahepatic bile duct cancer, or gallbladder cancer. The extrahepatic form is composed of peri-hilar type cholangiocarcinoma (PHC or Klatskin tumor) and distal cholangiocarcinoma (DCC), while gallbladder cancer (GBC) also includes cystic duct carcinoma (CDC). There is some debate about the cellular origin of ICC. BTC cells are presumed to originate from cholangiocytes, but the presence of mixed tumor types in ICC and HCC (hepatocellular carcinoma) with intermediate characteristics between ICC and HCC suggests that a subset of intrahepatic tumors could share a common hepatic progenitor cell origin.5 Regardless of its location or pathology, BTCs are very aggressive with high metastatic and invasive potential and are difficult to completely resect by surgery because of their anatomical location and spread along the bile ducts. Standard of practice for advanced CC is cisplatin or gemcitabine, but the response to these chemotherapies is poor, and consequently they show poor prognosis with only 5–10% five-year survival.1
Endothelial dysfunction plays an essential role in liver injury, yet the phenotypic regulation of liver sinusoidal endothelial cells (LSECs) remains unknown. Autophagy is an endogenous protective system whose loss could undermine LSEC integrity and phenotype. The aim of our study was to investigate the role of autophagy in the regulation of endothelial dysfunction and the impact of its manipulation during liver injury. We analyzed primary isolated LSECs from Atg7control and Atg7endo mice as well as rats after CCl4 induced liver injury. Liver tissue and primary isolated stellate cells were used to analyze liver fibrosis. Autophagy flux, microvascular function, nitric oxide bioavailability, cellular superoxide content and the antioxidant response were evaluated in endothelial cells. Autophagy maintains LSEC homeostasis and is rapidly upregulated during capillarization in vitro and in vivo. Pharmacological and genetic downregulation of endothelial autophagy increases oxidative stress in vitro. During liver injury in vivo, the selective loss of endothelial autophagy leads to cellular dysfunction and reduced intrahepatic nitric oxide. The loss of autophagy also impairs LSECs ability to handle oxidative stress and aggravates fibrosis. Autophagy contributes to maintaining endothelial phenotype and protecting LSECs from oxidative stress during early phases of liver disease. Selectively potentiating autophagy in LSECs during early stages of liver disease may be an attractive approach to modify the disease course and prevent fibrosis progression. Chronic liver injury from any source leads to progressive fibrosis, yet treatments are elusive. A better understanding of the early changes that disrupt cellular homeostasis, and initiate and perpetuate fibrogenesis following liver injury is needed. Liver sinusoidal endothelial cells (LSECs) constitute the liver’s first barrier of defense because of their unique position lining the sinusoidal lumen. They are also the initial liver cell type to sense injury. Maintenance of the LSEC phenotype associated with cellular pores, or fenestrae, is critical to maintaining homeostasis in the whole liver parenchyma. Following hepatic damage, sinusoidal endothelial dysfunction may arise and it is characterized by the loss of both fenestrae (capillarization) and of its anti-fibrotic, anti-thrombotic and anti-vasodilatory properties, which are essential for the maintenance of liver integrity.1 LSEC injury also plays an essential role in initiation and progression of liver injury. Indeed, signals derived from the sinusoidal endothelium during liver damage determine the outcome of pro-regenerative vs. pro-fibrotic processes.2,3 Despite its primary role in maladaptive healing and liver fibrosis,4,5 the phenotypic regulation of endothelial dysfunction is not fully understood.
CD100 is constitutively expressed on T cells and can be cleaved from the cell surface by matrix metalloproteases (MMPs) to become soluble CD100 (sCD100). Both membrane-bound CD100 (mCD100) and sCD100 have important immune regulatory functions that promote immune cell activation and responses. This study investigated the expression and role of mCD100 and sCD100 in regulating antiviral immune responses during HBV infection. mCD100 expression on T cells, sCD100 levels in the serum, and MMP expression in the liver and serum were analysed in patients with chronic HBV (CHB) and in HBV-replicating mice. The ability of sCD100 to mediate antigen-presenting cell maturation, HBV-specific T cell activation, and HBV clearance were analysed in HBV-replicating mice and patients with CHB. Patients with CHB had higher mCD100 expression on T cells and lower serum sCD100 levels compared with healthy controls. Therapeutic sCD100 treatment resulted in the activation of DCs and liver sinusoidal endothelial cells, enhanced HBV-specific CD8 T cell responses, and accelerated HBV clearance, whereas blockade of its receptor CD72 attenuated the intrahepatic anti-HBV CD8 T cell response. Together with MMP9, MMP2 mediated mCD100 shedding from the T cell surface. Patients with CHB had significantly lower serum MMP2 levels, which positively correlated with serum sCD100 levels, compared with healthy controls. Inhibition of MMP2/9 activity resulted in an attenuated anti-HBV T cell response and delayed HBV clearance in mice. MMP2/9-mediated sCD100 release has an important role in regulating intrahepatic anti-HBV CD8 T cell responses, thus mediating subsequent viral clearance during HBV infection. Chronic HBV infection continues to be a major public health burden worldwide. The persistence of HBV infection increases the risk of end-stage liver diseases, such as liver cirrhosis and hepatocellular carcinoma.1 Exposure to HBV in neonates usually leads to viral persistence, whereas most infected adults spontaneously clear the virus.2 The clearance of HBV relies largely on a potent and diverse T cell immune response, which usually becomes dysregulated in chronic HBV infection.3–5 However, the mechanism by which a favourable anti-HBV T cell response is generated in an infected individual remains largely unknown.6
It is widely believed that autoimmune hepatitis accumulates in families, but the degree of familial clustering has not been clarified. We conducted a population-based study on the family occurrence of autoimmune hepatitis. Through Danish nationwide registries we identified 8,582 first-degree and 9,230 second-degree relatives of index patients diagnosed with autoimmune hepatitis in 1994–2015; and 64 co-twins of index patients diagnosed with autoimmune hepatitis in 1977–2011. For first- and second-degree relatives we calculated the sex- and age-adjusted standardized incidence ratio of autoimmune hepatitis relative to the general population, and we calculated the cumulative risk, i.e. the cumulative incidence, of developing autoimmune hepatitis from the time of the index patient’s diagnosis. For co-twins, we estimated the standardized incidence ratio and the concordance rate of autoimmune hepatitis. In first-degree relatives, there were six incident autoimmune hepatitis diagnoses during 64,020 years of follow-up: the standardized incidence ratio was 4.9 (95% CI 1.8–10.7), and the 10-year cumulative risk was 0.10% (95% CI 0.04–0.23). In the second-degree relatives, there were no incident autoimmune hepatitis diagnoses (expected number assuming incidence rate as in the Danish general population = 0.8). In the co-twins, there was one incident autoimmune hepatitis diagnosis during 1,112 years of follow-up, and the standardized incidence ratio was 53.9 (95% CI 1.4–300.4). The probandwise concordance rate, a measure of heritability, was higher in monozygotic than in dizygotic twins (8.7% [95% CI 1.1–28.0] vs. 0%). This nationwide study indicates that only first-degree relatives of index patients with autoimmune hepatitis are at increased risk of autoimmune hepatitis from the time of the index patient’s diagnosis, but the absolute risk is very low. Autoimmune hepatitis (AIH) is a chronic autoimmune liver disease with an incidence of 1 per 100,000 population per year around the world.1 It is a frequently held notion that AIH accumulates in families,2 but the degree of familial clustering has rarely been studied. The existing literature suggests that AIH accumulates in twins,3,4 siblings,5 parents and children,3,6–8 second-degree relatives,3 and family members of unspecified relationship,9,10 but those studies were limited by small numbers of patients and did not compare AIH incidence with the general population. Numerous genetic risk factors for AIH have been suggested,11,12 but genome-wide association studies indicate that genetics plays only a minor role.12 The basis for AIH family counselling thus remains inadequate.
Death rates on liver transplant waiting lists range from 5%–25%. Herein, we report a unique experience with 50 anonymous individuals who volunteered to address this gap by offering to donate part of their liver to a recipient with whom they had no biological connection or prior relationship, so called anonymous live liver donation (A-LLD). Candidates were screened to confirm excellent physical, mental, social, and financial health. Demographics and surgical outcomes were analyzed. Qualitative interviews after donation examined motivation and experiences. Validated self-reported questionnaires assessed personality traits and psychological impact. A total of 50 A-LLD liver transplants were performed between 2005 and 2017. Most donors had a university education, a middle-class income, and a history of prior altruism. Half were women. Median age was 38.5 years (range 20–59). Thirty-three (70%) learned about this opportunity through public or social media. Saving a life, helping others, generativity, and reciprocity for past generosity were motivators. Social, financial, healthcare, and legal support in Canada were identified as facilitators. A-LLD identified most with the personality traits of agreeableness and conscientiousness. The median hospital stay was 6 days. One donor experienced a Dindo-Clavien Grade 3 complication that completely resolved. One-year recipient survival was 91% in 22 adults and 97% in 28 children. No A-LLD reported regretting their decision. This is the first and only report of the characteristics, motivations and facilitators of A-LLD in a large cohort. With rigorous protocols, outcomes are excellent. A-LLD has significant potential to reduce the gap between transplant organ demand and availability. Death rates on liver transplant (LT) waiting lists in the Western world range from 5-25%.1–4 This is disheartening since most LT recipients now survive for decades with good health and near normal quality of life.1,5,6 In selected locations, live liver donation (LLD) has been used to mitigate the shortage of deceased donor livers with excellent recipient outcomes. LLD is associated with a 30% morbidity rate and an estimated 0.3% donor mortality risk.5,7–11,12 Our program and others have confirmed that donors with biological relationships or close emotional bonds with the recipient have few regrets and good outcomes.8,13
Human induced pluripotent stem cell (hiPSC)-derived liver modeling systems have the potential to overcome the shortage of donors for clinical application and become a model for drug development. Although several strategies are available to generate hepatic micro-tissues, few have succeeded in generating a liver organoid with hepatobiliary structure from hiPSCs. At differentiation stages I and II (day 1–15), 25% of mTeSR™ culture medium was added to hepatic differentiation medium to induce endodermal and mesodermal commitment and thereafter hepatic and biliary co-differentiation. At stage III (day 15–45), 10% cholesterol+ MIX was added to the maturation medium to promote the formation and maturation of the hepatobiliary organoids. Phenotypes and functions of organoids were determined by specific markers and multiple functional assays both in vitro and in vivo. In this system, hiPSCs were induced to form 3D hepatobiliary organoids and to some extent recapitulated key aspects of early hepatogenesis in a parallel fashion. The organoids displayed a series of functional attributes. Specifically, the induced hepatocyte-like cells could take up indocyanine green, accumulate lipid and glycogen, and displayed appropriate secretion ability (albumin and urea) and drug metabolic ability (CYP3A4 activity and inducibility); the biliary structures in the system showed gamma glutamyltransferase activity and the ability to efflux rhodamine and store bile acids. Furthermore, after transplantation into the immune-deficient mice, the organoids survived for more than 8 weeks. This is the first time that functional hepatobiliary organoids have been generated from hiPSCs. The organoid model will be useful for in vitro studies of the molecular mechanisms of liver development and has important potential in the therapy of liver diseases. To date, most of the pluripotent stem cell (PSC)-based liver modeling systems have initiated differentiation from highly purified definitive endoderm (DE) and have generated monolayer hepatocyte-like cells.1–5 However, preclinical predictions from such models are usually difficult to interpret, and may be misleading, because homogeneous DE populations are insufficient to accurately replicate the complex regulation of signals among cells and tissues during liver organogenesis.6 A recent study, which mimicked liver development by combining hepatic endoderm cells with endothelial cells and mesenchymal progenitors, resulted in the generation of a liver bud-like structure with improved function.7 Similarly, co-culture of fetal liver progenitor cells and liver extracellular matrix triggered the formation of hepatocytes with bile duct-like structures.8 These findings highlighted the importance of multicellular interactions during early liver development. However, these studies combined cells that were isolated from multiple and postnatal individuals, suggesting a severe limitation of the transplanting applications based on these strategies.
Liver cancer is the second leading cause of cancer death worldwide. Hepatocellular carcinoma (HCC) is the most common type of primary liver cancer in adults. The aim of this study was to define the role of the long non-coding RNA lncHDAC2 in the tumorigenesis of HCC. CD13+CD133+ cells (hereafter called liver cancer stem cells [CSCs]) and CD13-CD133- cells (referred to as non-CSCs) were sorted from 3 primary HCC tumor tissues and followed by transcriptome microarray. The expression and function of lncHDAC2 were further assessed by northern blot, sphere formation and xenograft tumor models. LncHDAC2 is highly expressed in HCC tumors and liver CSCs. LncHDAC2 promotes the self-renewal of liver CSCs and tumor propagation. In liver CSCs, lncHDAC2 recruits the NuRD complex onto the promoter of PTCH1 to inhibit its expression, leading to activation of Hedgehog signaling. Moreover, HDAC2 expression levels are positively related to HCC severity and PTCH1 levels are negatively related to HCC severity. Additionally, the Smo inhibitor cyclopamine was shown to impair the self-renewal of liver CSCs and suppress tumor propagation. Our findings reveal that lncHDAC2 promotes the self-renewal of liver CSCs and tumor propagation by activating the Hedgehog signaling pathway. Downregulating lncHDAC2 is a promising antitumor strategy in HCC. Hepatocellular carcinoma (HCC), the most common type of primary liver cancer, is one of the leading causes of cancer death globally. The highest incidence of HCC is in East and South-East Asia and Northern and Western Africa.1 However, the incidence of liver cancer, including HCC, has risen in areas with historically low rates, for instance, Western Europe, Northern America and parts of Oceania. Infection with hepatitis B virus and hepatitis C virus, as well as metabolic disorders, are etiologically responsible for HCC.2 The high rate of recurrence and heterogeneity render HCC intractable.3 Therefore, the mechanism underlying liver carcinogenesis remain elusive.
Treatment with nucleos(t)ide analogues (NA) leads to hepatitis B virus (HBV) DNA suppression in most patients with chronic hepatitis B (CHB), but HBV surface antigen (HBsAg) loss rates are low. Upon NA discontinuation, HBV DNA can return rapidly with ensuing alanine aminotransferase flares and induction of cytokines. Several studies reported higher HBsAg loss rates after stopping therapy, but at present it is unclear if cell-mediated immune responses are altered after treatment discontinuation. The aim of this study was to characterise T cell responses during the early phase of virological relapse, following discontinuation of NA therapy in HBeAg-negative patients. A total of 15 HBeAg-negative patients with CHB on long-term NA treatment were included in a prospective study and subjected to structured NA discontinuation. T cell responses were studied at the end of NA therapy and 4, 8 and 12 weeks thereafter. The T cell phenotype of patients with CHB on long-term NA therapy was markedly different compared to healthy individuals, but was only slightly altered after discontinuation of therapy. T cells from patients with HBsAg loss expressed low levels of KLRG1 and PD-1 at all time-points and high levels of Ki-67 and CD38 at week 12 after treatment cessation. In vitro peptide stimulated HBV-specific T cell responses were increased in several patients after NA cessation. Blocking of PD-L1 further enhanced HBV-specific T cell responses, especially after discontinuation of therapy. Relapse of active HBV replication after stopping therapy may trigger an immunological environment that enhances the responsiveness of HBV-specific T cells in vitro. Together with other immune interventions, this approach might be of interest for the development of novel therapeutic options to induce HBsAg loss in CHB. Hepatitis B virus (HBV) chronically infects around 250 million individuals worldwide and can cause liver cirrhosis as well as hepatocellular carcinoma (HCC).1 The treatment options for chronic hepatitis B (CHB) are either pegylated interferon-alfa (PEG-IFNα) or nucleos(t)ide analogues (NA). PEG-IFNα has the advantage of finite treatment with higher rates of anti-HBs seroconversion of around 10%, but is associated with considerable side effects.2 Thus, well-tolerated NAs are most commonly used. NAs efficiently suppress viral replication in most patients, have a beneficial effect on disease progression, and reduce the risk of HCC development.2 Still, NAs only have minor, if any, effect on covalently closed circular DNA (cccDNA). For this reason, functional cure is a rare event and most patients with CHB require life-long therapy, particularly HBV e antigen (HBeAg)-negative patients.2
Acetaminophen-protein adducts are specific biomarkers of toxic acetaminophen (paracetamol) metabolite exposure. In patients with hepatotoxicity (alanine aminotransferase [ALT] >1,000 U/L), an adduct concentration ≥1.0 nmol/ml is sensitive and specific for identifying cases secondary to acetaminophen. Our aim was to characterise acetaminophen-protein adduct concentrations in patients following acetaminophen overdose and determine if they predict toxicity. We performed a multicentre prospective observational study, recruiting patients 14 years of age or older with acetaminophen overdose regardless of intent or formulation. Three serum samples were obtained within the first 24 h of presentation and analysed for acetaminophen-protein adducts. Acetaminophen-protein adduct concentrations were compared to ALT and other indicators of toxicity. Of the 240 patients who participated, 204 (85%) presented following acute ingestions, with a median ingested dose of 20 g (IQR 10–40), and 228 (95%) were treated with intravenous acetylcysteine at a median time of 6 h (IQR 3.5–10.5) post-ingestion. Thirty-six (15%) patients developed hepatotoxicity, of whom 22 had an ALT ≤1,000 U/L at the time of initial acetaminophen-protein adduct measurement. Those who developed hepatotoxicity had a higher initial acetaminophen-protein adduct concentration compared to those who did not, 1.63 nmol/ml (IQR 0.76–2.02, n = 22) vs. 0.26 nmol/ml (IQR 0.15–0.41; n = 204; p <0.0001), respectively. The AUROC for hepatotoxicity was 0.98 (95% CI 0.96–1.00; n = 226; p <0.0001) with acetaminophen-protein adduct concentration and 0.89 (95% CI 0.82–0.96; n = 219; p <0.0001) with ALT. An acetaminophen-protein adduct concentration of 0.58 nmol/ml was 100% sensitive and 91% specific for identifying patients with an initial ALT ≤1,000 U/L who would develop hepatotoxicity. Adding acetaminophen-protein adduct concentrations to risk prediction models improved prediction of hepatotoxicity to a level similar to that obtained by more complex models. Acetaminophen-protein adduct concentration on presentation predicted which patients with acetaminophen overdose subsequently developed hepatotoxicity, regardless of time of ingestion. An adduct threshold of 0.58 nmol/L was required for optimal prediction. Acetaminophen (APAP), also called paracetamol, is one of the most common medications resulting in hospital presentations and admissions following deliberate self-poisoning and accidental overdose worldwide.1 Its major toxic effect is acute liver injury (ALI) and it is the most common cause of acute liver failure (ALF) in North America, Europe and Australia.2–4 The American Association of Poison Control Centres, which annually provides over 2.1 million tele-consults for the US and associated territories, received over 100,000 calls related to APAP exposure in 2017.5
Cirrhosis, the prevalence of which is increasing, is a risk factor for osteoporosis and fractures. However, little is known of the actual risk of hip fractures in patients with alcoholic cirrhosis. Using linked primary and secondary care data from the English and Danish nationwide registries, we quantified the hip fracture risk in two national cohorts of patients with alcoholic cirrhosis. We followed 3,706 English and 17,779 Danish patients with a diagnosis of alcoholic cirrhosis, and we identified matched controls from the general populations. We estimated hazard ratios (HR) of hip fracture for patients vs. controls, adjusted for age, sex and comorbidity. The five-year hip fracture risk was raised both in England (2.9% vs. 0.8% for controls) and Denmark (4.6% vs. 0.9% for controls). With confounder adjustment, patients with cirrhosis had fivefold (adjusted HR 5.5; 95% CI 4.3–6.9), and 8.5-fold (adjusted HR 8.5; 95% CI 7.8–9.3) increased rates of hip fracture, in England and Denmark, respectively. This association between alcoholic cirrhosis and risk of hip fracture showed significant interaction with age (p <0.001), being stronger in younger age groups (under 45 years, HR 17.9 and 16.6 for English and Danish patients, respectively) than in patients over 75 years (HR 2.1 and 2.9, respectively). In patients with alcoholic cirrhosis, 30-day mortality following a hip fracture was 11.1% in England and 10.0% in Denmark, giving age-adjusted post-fracture mortality rate ratios of 2.8 (95% CI 1.9–3.9) and 2.0 (95% CI 1.5–2.7), respectively. Patients with alcoholic cirrhosis have a markedly increased risk of hip fracture and post-hip fracture mortality compared with the general population. These findings support the need for more effort towards fracture prevention in this population, to benefit individuals and reduce the societal burden. An important and often studied complication of chronic liver disease is osteoporosis.1–5 Osteoporosis may be asymptomatic, but increases the risk that minor accidents result in bone fractures, particularly of the distal radius and proximal femur.1,2 Hip fractures in particular have a significant impact on health, productivity and life expectancy,6,7 and the 30-day mortality is estimated to be up to 10%.8,9 Specifically in patients with alcoholic cirrhosis, the risk of these fractures may be further increased by the direct effect of high levels of alcohol use,10 or minimal hepatic encephalopathy.11 However, owing to a dearth of studies that have actually quantified the incidence of fractures in chronic liver disease, the absolute risk of hip fractures in people with alcoholic cirrhosis is unknown.
Tenofovir alafenamide (TAF) is a new prodrug of tenofovir developed to treat patients with chronic hepatitis B virus (HBV) infection at a lower dose than tenofovir disoproxil fumarate (TDF) through more efficient delivery of tenofovir to hepatocytes. In 48-week results from two ongoing, double-blind, randomized phase III trials, TAF was non-inferior to TDF in efficacy with improved renal and bone safety. We report 96-week outcomes for both trials. In two international trials, patients with chronic HBV infection were randomized 2:1 to receive 25 mg TAF or 300 mg TDF in a double-blinded fashion. One study enrolled HBeAg-positive patients and the other HBeAg-negative patients. We assessed efficacy in each study, and safety in the pooled population. At week 96, the differences in the rates of viral suppression were similar in HBeAg-positive patients receiving TAF and TDF (73% vs. 75%, respectively, adjusted difference −2.2% (95% CI −8.3 to 3.9%; p = 0.47), and in HBeAg-negative patients receiving TAF and TDF (90% vs. 91%, respectively, adjusted difference −0.6% (95% CI −7.0 to 5.8%; p = 0.84). In both studies the proportions of patients with alanine aminotransferase above the upper limit of normal at baseline, who had normal alanine aminotransferase at week 96 of treatment, were significantly higher in patients receiving TAF than in those receiving TDF. In the pooled safety population, patients receiving TAF had significantly smaller decreases in bone mineral density than those receiving TDF in the hip (mean % change −0.33% vs. −2.51%; p <0.001) and lumbar spine (mean % change −0.75% vs. −2.57%; p <0.001), as well as a significantly smaller median change in estimated glomerular filtration rate by Cockcroft-Gault method (−1.2 vs. −4.8 mg/dl; p <0.001). In patients with HBV infection, TAF remained as effective as TDF, with continued improved renal and bone safety, two years after the initiation of treatment. Clinicaltrials.gov number: NCT01940471 and NCT01940341. . The World Health Organization estimates that approximately 240 million people worldwide are chronically infected with the hepatitis B virus (HBV).1 Without treatment, chronic HBV infection can cause progressive liver fibrosis, which may lead to cirrhosis, decompensation, and hepatocellular carcinoma.2–4 Suppressive antiviral treatment has been shown to reduce the risk of liver-related complications, and can halt or even reverse disease progression.5–7 However, since few patients achieve seroclearance of the hepatitis B surface antigen (HBsAg), which is considered the hallmark of functional cure, treatment is generally life-long.6–9 In an aging population with comorbidities, side effects of treatment such as renal and bone complications can be problematic with long-term treatment.9–13
Chronic liver diseases are characterized by expansion of the small immature cholangiocytes – a mechanism named ductular reaction (DR) – which have the capacity to differentiate into hepatocytes. We investigated the kinetics of this differentiation, as well as analyzing several important features of the newly formed hepatocytes, such as functional maturity, clonal expansion and resistance to stress in mice with long-term liver damage. We tracked cholangiocytes using osteopontin-iCreERT2 and hepatocytes with AAV8-TBG-Cre. Mice received carbon tetrachloride (CCl4) for >24 weeks to induce chronic liver injury. Livers were collected for the analysis of reporter proteins, cell proliferation and death, DNA damage, and nuclear ploidy; hepatocytes were also isolated for RNA sequencing. During liver injury we observed a transient DR and the differentiation of DR cells into hepatocytes as clones that expanded to occupy 12% of the liver parenchyma by week 8. By lineage tracing, we confirmed that these new hepatocytes derived from cholangiocytes but not from native hepatocytes. They had all the features of mature functional hepatocytes. In contrast to the exhausted native hepatocytes, these newly formed hepatocytes had higher proliferative capability, less apoptosis, a lower proportion of highly polyploid nuclei and were better at eliminating DNA damage. In chronic liver injury, DR cells differentiate into stress-resistant hepatocytes that repopulate the liver. The process might account for the observed parenchymal reconstitution in livers of patients with advanced-stage hepatitis and could be a target for regenerative purposes. Persistent injury of the hepatic tissue leads to fibrosis, which eventually evolves to cirrhosis, the end-stage of any chronic liver diseases. Cirrhosis is characterized by distortion of hepatic architecture, regenerative nodules and hepatocyte dysfunction and is associated with life-threatening complications such as hepatocellular insufficiency and hepatocellular carcinoma (HCC).1 Liver cirrhosis is estimated to cause around 170,000 deaths annually.2 So far, liver transplantation represents the only curative therapeutic solution for many chronic liver diseases.
Immunotherapy for metastatic cancer can be complicated by the onset of hepatic immune-related adverse events (IRAEs). This study compared hepatic IRAEs associated with anti-programmed cell death protein 1 (PD-1)/PD ligand 1 (PD-L1) and anti-cytotoxic T lymphocyte antigen 4 (CTLA-4) monoclonal antibodies (mAbs). Among 536 patients treated with anti-PD-1/PD-L1 or CTLA-4 immunotherapies, 19 (3.5%) were referred to the liver unit for grade ≥3 hepatitis. Of these patients, nine had received anti-PD-1/PD-L1 and seven had received anti-CTLA-4 mAbs, in monotherapy or in combination with anti-PD-1. Liver investigations were undertaken in these 16 patients, including viral assays, autoimmune tests and liver biopsy, histological review, and immunostaining of liver specimens. In the 16 patients included in this study, median age was 63 (range 33-84) years, and nine (56%) were female. Time between therapy initiation and hepatitis was five (range, 1–49) weeks and median number of immunotherapy injections was two (range, 1–36). No patients developed hepatic failure. Histology related to anti-CTLA-4 mAbs demonstrated granulomatous hepatitis including fibrin ring granulomas and central vein endotheliitis. Histology related to anti-PD-1/PD-L1 mAbs was characterised by lobular hepatitis. The management of hepatic IRAEs was tailored according to the severity of both the biology and histology of liver injury: six patients improved spontaneously; seven received oral corticosteroids at 0.5–1 mg/kg/day; two were maintained on 0.2 mg/kg/day corticosteroids; and one patient required pulses and 2.5 mg/kg/day of corticosteroids, and the addition of a second immunosuppressive drug. In three patients, immunotherapy was reintroduced without recurrence of liver dysfunction. Acute hepatitis resulting from immunotherapy for metastatic cancer is rare (3.5%) and, in most cases, not severe. Histological assessment can distinguish between anti-PD-1/PD-L1 and anti-CTLA-4 mAb toxicity. The severity of liver injury is helpful for tailoring patient management, which does not require systematic corticosteroid administration. Immune-modulatory therapies have dramatically improved the survival of patients with metastatic tumours.1,2 During the development of cancer, the immune system becomes naturally ‘tolerant’ towards cancer cells, which are seen as part of the ‘self’. This tolerance is maintained by immune checkpoint pathways that downregulate immune functions, permitting cancer cells to evade immune attacks.3,4 Monoclonal antibodies (mAbs) directed against regulatory immune checkpoint molecules that inhibit T cell activation enhance antitumour immunity.5 Ipilimumab, a human Ig-G1 mAb, blocks cytotoxic T lymphocyte antigen 4 (CTLA-4).6 Pembrolizumab and nivolumab, humanized IgG4 kappa and human IgG4 mAbs, respectively, block the interaction between programmed cell death protein 1 (PD-1) and the two PD ligands, PD-L1 and PD-L2, by selectively binding the PD-1 receptor.7,8 Durvalumab, a human IgG1 kappa mAb, targets PD-L1.9
Before antiviral therapy, kidney transplant recipients infected with hepatitis B virus (HBV) or hepatitis C virus (HCV) had poor outcomes. Since the 90s, nucleos(t)ide analogues have been widely used in HBV-infected patients, while interferon-based therapy was rarely used in HCV-infected patients. The aim of this study was to assess the impact of HBV and HCV on patient and graft survival, according to viral replication status. Data from January 1993 to December 2010 were extracted from the French national database CRISTAL. A total of 31,433 kidney transplant recipients were included, of whom 575, 1,060 and 29,798 had chronic hepatitis B, C, or were not infected, respectively. Ten-year survival was lower in HCV-infected (71.3%) than in HBV-infected (81.2%, p = 0.0004) or non-infected kidney transplant recipients (82.7%, p <0.0001). Ten-year kidney graft survival was lower in HCV-infected (50.6%) than in HBV-infected (62.3%, p <0.0001) or non-infected kidney transplant recipients (64.7%, p <0.0001). A random analysis of the medical records of 184 patients with HBV and 504 patients with HCV showed a control of viral replication in 94% and 35% of cases, respectively. Ten-year patient and graft survival in patients with detectable HCV RNA was lower than in their matching controls. Conversely, patients with HCV and undetectable HCV RNA had higher 10-year survival than their matched controls without significant differences in graft survival. Chronic HBV infection does not impact 10-year patient and kidney graft survival thanks to control of viral replication with nucleos(t)ide analogues. In kidney transplant recipients infected with HCV, patients with detectable RNA had worse outcomes, whereas the outcomes of those with undetectable RNA were at least as good as non-infected patients. Thus, direct-acting antivirals should be systematically offered to HCV-infected patients. Kidney transplantation is the best treatment for patients with end-stage renal disease (ESRD) because of a significant survival benefit conferred compared to patients who remain on haemodialysis.1 Although the prevalence of hepatitis B virus (HBV) and hepatitis C virus (HCV) infection in patients with ESRD has significantly declined over time, it remains at least 4-times higher than in the general population.2,3 Chronic HCV or HBV infection can result in chronic liver disease, cirrhosis, and hepatocellular carcinoma4–6 and increase the risk of chronic kidney disease (CKD).7–10 Before the use of antiviral therapy, HBV and HCV infection were associated with a poor outcome in kidney transplant recipients (KTRs).11–14 This poorer outcome has been reported in untreated patients.11–14 With the development of new treatments against viral hepatitis, an update of data in large cohorts of KTRs with long-term follow-up is warranted.
Myeloid cell leukemia 1 (MCL1), a prosurvival member of the BCL2 protein family, has a pivotal role in human cholangiocarcinoma (CCA) cell survival. We previously reported that fibroblast growth factor receptor (FGFR) signalling mediates MCL1-dependent survival of CCA cells in vitro and in vivo. However, the mode and mechanisms of cell death in this model were not delineated. Human CCA cell lines were treated with the pan-FGFR inhibitor LY2874455 and the mode of cell death examined by several complementary assays. Mitochondrial oxidative metabolism was examined using a XF24 extracellular flux analyser. The efficiency of FGFR inhibition in patient-derived xenografts (PDX) was also assessed. CCA cells expressed two species of MCL1, a full-length form localised to the outer mitochondrial membrane, and an N terminus-truncated species compartmentalised within the mitochondrial matrix. The pan-FGFR inhibitor LY2874455 induced non-apoptotic cell death in the CCA cell lines associated with cellular depletion of both MCL1 species. The cell death was accompanied by failure of mitochondrial oxidative metabolism and was most consistent with necrosis. Enforced expression of N terminus-truncated MCL1 targeted to the mitochondrial matrix, but not full-length MCL1 targeted to the outer mitochondrial membrane, rescued cell death and mitochondrial function. LY2874455 treatment of PDX-bearing mice was associated with tumour cell loss of MCL1 and cell necrosis. FGFR inhibition induces loss of matrix MCL1, resulting in cell necrosis. These observations support a heretofore unidentified, alternative MCL1 survival function, namely prevention of cell necrosis, and have implications for treatment of human CCA. Cholangiocarcinoma (CCA) is a lethal hepatobiliary malignancy with limited therapeutic options.1,2 Advances in CCA therapy will require an understanding of oncogenic signalling networks that contribute to CCA pathogenesis and could be disrupted therapeutically. Similar to other malignancies, a cardinal feature of CCA is inhibition of cell death pathways that are engaged by oncogenic signalling pathways.3 Members of the BCL2 gene family encode proteins that regulate the mitochondrial or intrinsic apoptotic pathway.4 Of the BCL2 prosurvival members, myeloid cell leukemia 1 (MCL1) is most frequently amplified and overexpressed in CCA,5–7 and has a pivotal role in CCA cell survival.8 Hence, targeting MCL1 is a strategy for the treatment of CCA and other malignancies.9,10
A major limitation in the field of liver transplantation is the shortage of transplantable organs. Chimeric animals carrying human tissue have the potential to solve this problem. However, currently available chimeric organs retain a high level of xenogeneic cells, and the transplantation of impure organs needs to be tested. We created chimeric livers by injecting Lewis rat hepatocytes into C57Bl/6Fah−/−Rag2−/−Il2rg−/− mice, and further transplanted them into newly weaned Lewis rats (45 ± 3 g) with or without suboptimal immunosuppression (tacrolimus 0.6 mg/kg/day for 56 or 112 days). Control donors included wild-type C57Bl/6 mice (xenogeneic) and Lewis rats (syngeneic). Without immunosuppression, recipients of chimeric livers experienced acute rejection, and died within 8 to 11 days. With immunosuppression, they all survived for >112 days with normal weight gain compared to syngeneic controls, while all xenogeneic controls died within 98 days due to rejection with Banff scores >6 (p = 0.0014). The chimeric grafts underwent post-transplant remodelling, growing by 670% on average. Rat hepatocytes fully replaced mouse hepatocytes starting from day 56 (absence of detectable mouse serum albumin, histological clearance of mouse hepatocytes). In addition, rat albumin levels reached those of syngeneic recipients. Four months after transplantation of chimeric livers, we observed the development of diffuse mature rat bile ducts through transdifferentiation of hepatocytes (up to 72% of cholangiocytes), and patchy areas of portal endothelium originating from the host (seen in one out of five recipients). Taken together, these data demonstrate the efficacy of transplanting rat-to-mouse chimeric livers into rats, with a high potential for post-transplant recipient-oriented graft remodelling. Validation in a large animal model is still needed. The use of chimeric animals with organs compatible with specific patients in need of transplantation has the potential to solve the chronic lack of organ donors. The idea of using animals as incubators of human tissue is becoming more and more realistic, especially with the recent observation that human induced pluripotent stem cells (hiPSCs) can lead to chimeras after injection into pig blastocysts.1 At this stage, the experiment was terminated before birth, and the contribution of human cells to the final chimeric pig embryos has remained low, but future studies are looking at replacing entire organs, as has been the case for various rodent combinations.1
Non-invasive tools for monitoring treatment response and disease progression in non-alcoholic steatohepatitis (NASH) are needed. Our objective was to evaluate the utility of magnetic resonance (MR)-based hepatic imaging measures for the assessment of liver histology in patients with NASH. We analyzed data from patients with NASH and stage 2 or 3 fibrosis enrolled in a phase II study of selonsertib. Pre- and post-treatment assessments included centrally read MR elastography (MRE)-estimated liver stiffness, MR imaging-estimated proton density fat fraction (MRI-PDFF), and liver biopsies evaluated according to the NASH Clinical Research Network classification and the non-alcoholic fatty liver disease activity score (NAS). Among 54 patients with MRE and biopsies at baseline and week 24, 18 (33%) had fibrosis improvement (≥1-stage reduction) after undergoing 24 weeks of treatment with the study drug. The area under the receiver operating characteristic curve (AUROC) of MRE-stiffness to predict fibrosis improvement was 0.62 (95% CI 0.46–0.78) and the optimal threshold was a ≥0% relative reduction. At this threshold, MRE had 67% sensitivity, 64% specificity, 48% positive predictive value, 79% negative predictive value. Among 65 patients with MRI-PDFF and biopsies at baseline and week 24, a ≥1-grade reduction in steatosis was observed in 18 (28%). The AUROC of MRI-PDFF to predict steatosis response was 0.70 (95% CI 0.57–0.83) and the optimal threshold was a ≥0% relative reduction. At this threshold, MRI-PDFF had 89% sensitivity and 47% specificity, 39% positive predictive value, and 92% negative predictive value. These preliminary data support the further evaluation of MRE-stiffness and MRI-PDFF for the longitudinal assessment of histologic response in patients with NASH. Non-alcoholic steatohepatitis (NASH) is a progressive form of non-alcoholic fatty liver disease (NAFLD) characterized by the presence of hepatic steatosis, inflammation, and hepatocyte ballooning with or without fibrosis.1 Patients with NASH and advanced fibrosis are at greatest risk of liver-related morbidity and mortality.2–5 The global prevalence of NASH is between 1.5% and 6.5%.6 Widespread screening is not currently feasible given that a definitive diagnosis of NASH can only be made through identification of the characteristic histopathologic pattern on liver biopsy.7 While liver biopsy remains the gold standard, its disadvantages are well known. It is an invasive procedure that can lead to serious complications, including hospitalization for pain, bleeding, and, in rare cases, death.8,9 Few patients are willing to undergo repeat biopsies necessary to monitor disease progression and to evaluate treatment response in trials of experimental therapeutic agents. Moreover, the interpretation of biopsy results is subject to variability due in part to inter- and intra-observer differences; the risk of sampling error is high due to the patchy nature of the histopathologic lesions in NASH; and biopsy is not able to discriminate intermediate stages of fibrosis.10–13 There is a pressing unmet medical need for reliable and accurate non-invasive tools to evaluate steatosis and fibrosis in patients with NASH, and to monitor response to treatment.
Acute-on-chronic liver failure (ACLF) is a clinical syndrome defined by liver failure on preexisting chronic liver disease and is often associated with bacterial infection with high short-term mortality. Experimental models that fully reproduce ACLF and effective pharmacological therapies are lacking. To mimic ACLF conditions, we developed a severe liver injury model by combining chronic injury (chronic carbon tetrachloride [CCl4] injection), acute hepatic insult (injection of a double dose CCl4), and bacterial infection (intraperitoneal injection of bacteria). Serum and liver samples from patients with ACLF or acute drug-induced liver injury (DILI) were used. Liver injury and regeneration were assessed to ascertain for potential benefits of interleukin-22 (IL-22Fc) administration. This severe liver injury model developed acute-on-chronic liver injury, bacterial infection, multi-organ injury, and high mortality, recapitulating some features of clinical ACLF. Liver regeneration in this model was severely impaired due to the shift from the activation of pro-regenerative IL-6/STAT3 to anti-regenerative IFN-γ/STAT1 pathway. The impaired IL-6/STAT3 activation was due to Kupffer cell inability to produce IL-6; whereas the enhanced STAT1 activation was due to strong innate immune response and subsequent production of IFN-γ. Compared to DILI patients, ACLF patients had higher levels of IFN-γ but lower liver regeneration. IL-22Fc treatment improved survival of the ACLF mice by reversing the STAT1/STAT3 pathway imbalance and enhancing expression of many anti-bacterial genes in a manner involving the anti-apoptotic protein BCL2. Acute-on-chronic liver injury or bacterial infection is associated with impaired liver regeneration due to a shift from the pro-regenerative to anti-regenerative pathways, IL-22Fc therapy reverses this shift and attenuates bacterial infection, thus IL-22Fc may have therapeutic potential for ACLF treatment. Acute-on-chronic liver failure (ACLF) is generally accepted as a clinical syndrome characterized by an acute hepatic insult and rapid deterioration of liver function in patients with pre-existing chronic liver disease in combination with multi-organ failure with high short-term mortality.1-5 The main etiologies of the pre-existing chronic liver disease are alcoholism and chronic hepatitis B virus (HBV) infection, while the most frequently documented acute insults that induce acute injury in ACLF include excessive alcohol drinking, HBV activation, drug-induced liver injury (DILI) etc.1-6 Bacterial infections were detected in up to 2/3 ACLF patients and contributed to the poor outcome of ACLF.7-9 Although the poor outcome is closely associated with bacterial infection,7,8 whether bacterial infection is a consequence of ACLF or a trigger as an acute insult to induce ACLF is still a question of debate.7-9 Collectively, the outcome of ACLF likely depends on two major aspects: the recovery of multi-organ injury and the control of bacterial infection.
Neutrophil extracellular traps (NETs) are an important strategy utilized by neutrophils to immobilize and kill invading microorganisms. Herein, we studied NET formation and the process of neutrophil cell death (NETosis), as well as the clearance of NETs by macrophages (MΦ) (efferocytosis) in acute sepsis following binge drinking. Healthy volunteers consumed 2 ml of vodka/kg body weight, before blood endotoxin and 16 s rDNA were measured. Peripheral neutrophils were isolated and exposed to alcohol followed by phorbol 12-myristate 13-acetate (PMA) stimulation. Mice were treated with three alcohol binges and intraperitoneal lipopolysaccharide (LPS) to assess the dynamics of NET formation and efferocytosis. In vivo, anti-Ly6G antibody (IA8) was used for neutrophil depletion. Inducers of NETs (endotoxin and bacterial DNA) significantly increased in the circulation after binge alcohol drinking in humans. Ex vivo, alcohol alone increased NET formation, but upon PMA stimulation alcohol attenuated NET formation. Binge alcohol in mice resulted in a biphasic response to LPS. Initially, binge alcohol reduced LPS-induced NET formation and resulted in a diffuse distribution of neutrophils in the liver compared to alcohol-naïve mice. Moreover, indicators of NET formation including citrullinated histone H3, neutrophil elastase, and neutrophil myeloperoxidase were decreased at an early time point after LPS challenge in mice receiving binge alcohol, suggesting decreased NET formation. However, in the efferocytosis phase (15 h after LPS) citrullinated histone-H3 was increased in the liver in alcohol binge mice, suggesting decreased clearance of NETs. In vitro alcohol treatment reduced efferocytosis and phagocytosis of NETotic neutrophils and promoted expression of CD206 on MΦ. Finally, depletion of neutrophils prior to binge alcohol ameliorated LPS-induced systemic inflammation and liver injury in mice. Dysfunctional NETosis and efferocytosis following binge drinking exacerbate liver injury associated with sepsis. Alcoholic liver disease (ALD) affects millions of people worldwide. The multifaceted disease spectrum is characterized by increased liver inflammation and steatosis, as a direct effect of alcohol, its metabolites and increased hepatic oxidative stress.1–3 Sustained dysregulated hepatic inflammation during ALD is mediated by increased mobilization and recruitment of inflammatory cells to the liver. This occurrence normally precedes the breakdown of the gut barrier integrity and increased serum endotoxin levels, leading to prolonged hepatic inflammation and cell death.3,4 During alcoholic hepatitis (AH), innate immune cells play a crucial role not only in recognizing and responding to pathogen-associated molecular patterns (PAMPs) but also in contributing to the activation of the inflammatory cascade that correlates with disease severity in AH.5–7 Most studies on innate immune cells in AH and ALD have focused on the dysregulated migration, phagocytosis, and inflammatory cytokine release properties of macrophages (MΦ) and neutrophils.8–10 During ALD associated with clinical sepsis, neutrophils are recruited to the liver within hours, adhering to activated blood vessels or migrating to the parenchyma. Neutrophil recruitment is increased under systemic inflammatory conditions.1,11 Increased liver neutrophil infiltration correlates with mortality in acute AH; however, little is known about the functional capacity of neutrophils in AH. Mechanisms of neutrophil activation, recruitment, and innate immune functions, as well as their contribution to hepatic inflammation and injury during AH/ALD also remain incompletely understood.
Aberrant oncogenic mRNA translation and protein O-linked β-N-acetylglucosaminylation (O-GlcNAcylation) are general features during tumorigenesis. Nevertheless, whether and how these two pathways are interlinked remain unknown. Our previous study indicated that ribosomal receptor for activated C-kinase 1 (RACK1) promoted chemoresistance and growth in hepatocellular carcinoma (HCC). The aim of this study is to examine the role of RACK1 O-GlcNAcylation in oncogene translation and HCC carcinogenesis. The site(s) of RACK1 for O-GlcNAcylation was mapped by mass spectrometry analysis. HCC cell lines were employed to examine the effects of RACK1 O-GlcNAcylation on the translation of oncogenic factors and behaviors of tumor cells in vitro. Transgenic knock-in mice were used to detect the role of RACK1 O-GlcNAcylation in modulating HCC tumorigenesis in vivo. The correlation of RACK1 O-GlcNAcylation with tumor progression and relapse were analyzed in clinical HCC samples. We found that ribosomal RACK1 was highly modified by O-GlcNAc at Ser122. O-GlcNAcylation of RACK1 enhanced its protein stability, ribosome binding and interaction with PKCβII (PRKCB), leading to increased eukaryotic translation initiation factor 4E phosphorylation and translation of potent oncogenes in HCC cells. Genetic ablation of RACK1 O-GlcNAcylation at Ser122 dramatically suppressed tumorigenesis, angiogenesis, and metastasis in vitro and in diethylnitrosamine (DEN)-induced HCC mouse model. Increased RACK1 O-GlcNAcylation was also observed in HCC patient samples and correlated with tumor development and recurrence after chemotherapy. These findings demonstrate that RACK1 acts as key mediator linking O-GlcNAc metabolism to cap-dependent translation during HCC tumorigenesis. Targeting RACK1 O-GlcNAcylation provides promising options for HCC treatment. Hepatocellular carcinoma (HCC) is the sixth-most frequent and the second-most lethal cancer worldwide, with a rising incidence in developing and industrialized countries.1 Until now, surgery has remained the most effective treatment with curative potential. Nevertheless, most patients are still diagnosed at an advanced/late stage when surgery is no longer applicable, and display symptoms of intrahepatic and extrahepatic metastasis.2 Few effective treatment options exist for patients with advanced HCC, with a five-year survival rate of only 30%–40%.3 A high rate of postsurgical metastasis and relapse remains a major challenge in HCC, owing to the fact that this disease is highly resistant to conventional chemotherapy and radiation.4 Therefore, there is an urgent need to better understand the mechanisms contributing to the pathogenesis of HCC, to identify possible preventive strategies and therapeutic targets.
