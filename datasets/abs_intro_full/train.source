Individual stock trend prediction is extremely valuable for investment management. Previous studies mainly focused on proposing effective approaches to make profits. However, there is an ineffectiveness in model evaluation due to the inconsistency between model’s performance and profitability. We name this inconsistency profit bias. In order to address the profit bias in model evaluation, this paper proposes a new effective metric, Mean Profit Rate (MPR). The effectiveness of metric is measured based on the correlation between the metric value and profit of the model. Experiments on five stock daily index data among four countries show that MPR outperforms the classification metrics in correlating to profit. In view of these findings, we suggest that MPR is a more effective metric than the classification metrics in stock trend prediction. The individual stock trend prediction is challenging due to the high volatility, irregularity, and noisy signal in the environment of the stock markets.In recent years, it draws a lot of attention from researchers in various areas, especially Artificial intelligence.Some studies treat it as a regression problem (Bollen et al., 2011; Schumaker and Chen, 2009), aiming to predict the future value of stock price or profits.While other studies treat it as a classification problem (Hsieh et al., 2011; Huang et al., 2008), aiming to predict the trend of stock price movement.In most cases, the classification approaches achieve higher profits than the regression ones (Leung et al., 2000).The effectiveness of different classification approaches in individual stock prediction has been widely explored (Choudhry and Garg, 2008; Lin et al., 2013; Wang and Choi, 0000; O’Connor and Madden, 2006; Lee, 2009a).
Individual stock trend prediction is extremely valuable for investment management. Previous studies mainly focused on proposing effective approaches to make profits. However, there is an ineffectiveness in model evaluation due to the inconsistency between model’s performance and profitability. We name this inconsistency profit bias. In order to address the profit bias in model evaluation, this paper proposes a new effective metric, Mean Profit Rate (MPR). The effectiveness of metric is measured based on the correlation between the metric value and profit of the model. Experiments on five stock daily index data among four countries show that MPR outperforms the classification metrics in correlating to profit. In view of these findings, we suggest that MPR is a more effective metric than the classification metrics in stock trend prediction. Evaluation metric plays a key role in model improvement despite of which specifical model is used to solve the problem.Therefore, it is essential to understand and account for the evaluation process.At present, the evaluation of a model for individual stock trend prediction consists of two stages (Kaastra and Boyd, 1996a; Ding et al., 2015; Li and Tsang, 1999; Zhai et al., 2007; Atsalakis and Valavanis, 2009a; Schumaker and Chen, 2009).The first stage is the model selection.At this stage, the optimal model is chosen by the performance under a given metric, especially the classification metric.Traditionally, Accuracy (Qian and Rasheed, 2007) (or Hit ratio (Huang et al., 2005)) and F-measure (F1) (Patel et al., 2015a), which are utilized to evaluate models both in binary and multiple classifications, are used to select the optimal model.Recently, the Matthews Correlation Coefficient (MCC) (Ding et al., 2015) is used to evaluate the effect of models in binary stock trend classification.The second stage estimates the profitability of the optimal model.The profitability of the selected model is often estimated by simulated trading which has many variables (Kaastra and Boyd, 1996a; Leung et al., 2000; O’Connor and Madden, 2006).It is obvious that the consistency between classification performance and profitability of the model is fairly important, i.e. the model with the highest metric value should achieve the biggest profit.
Individual stock trend prediction is extremely valuable for investment management. Previous studies mainly focused on proposing effective approaches to make profits. However, there is an ineffectiveness in model evaluation due to the inconsistency between model’s performance and profitability. We name this inconsistency profit bias. In order to address the profit bias in model evaluation, this paper proposes a new effective metric, Mean Profit Rate (MPR). The effectiveness of metric is measured based on the correlation between the metric value and profit of the model. Experiments on five stock daily index data among four countries show that MPR outperforms the classification metrics in correlating to profit. In view of these findings, we suggest that MPR is a more effective metric than the classification metrics in stock trend prediction. However, there are usually  inconsistencies (Brownstone, 1996; Chang et al., 2009;  Schumaker and Chen, 2009;  Teixeira and De Oliveira, 2010a), as shown in Fig. 1.We call the inconsistency profit bias.Traditionally, model-oriented and data-oriented methods are used to address the profit bias.In model-oriented methods, researchers aim to alleviate profit bias by incorporating profit information into model’s learning targets.In Saad et al. (1998), they assign different weights to samples in the loss calculation to avoid false alarm.However, this also makes the model difficult to converge and affects the performance of other classes.In Moody and Saffell (2001) and Deng et al. (2017a), they use the direct reinforcement learning model to learn the trading signals directly.However, this method is less flexible in strategy settings.In the data-oriented methods, researchers aim to reduce the probability of profit bias by changing the original data distribution.In Kaastra and Boyd (1996b), they suggest removing some small changes from the dataset to avoid profit bias.In Luo and Chen (2013), they use the statistical-based method to segment and label the datasets.All data-oriented methods change the actual distribution of the original stock data.However, these above methods fail to fully address the profit bias.The key factor to trigger the profit bias is the inconsistency between the metrics used in the two-stage evaluation methods.
Individual stock trend prediction is extremely valuable for investment management. Previous studies mainly focused on proposing effective approaches to make profits. However, there is an ineffectiveness in model evaluation due to the inconsistency between model’s performance and profitability. We name this inconsistency profit bias. In order to address the profit bias in model evaluation, this paper proposes a new effective metric, Mean Profit Rate (MPR). The effectiveness of metric is measured based on the correlation between the metric value and profit of the model. Experiments on five stock daily index data among four countries show that MPR outperforms the classification metrics in correlating to profit. In view of these findings, we suggest that MPR is a more effective metric than the classification metrics in stock trend prediction. A simple example is given to demonstrate the profit bias.Suppose two models (A and B) are used to predict the trend of a stock in three consecutive trading days.These trading days have profit rate 1.00%, 2.00%, and −4.00%, respectively.The results of each model are used as the trade signals for a day trader which invests $100 on each trade.Model A gives results of up, up and up.It suggests holding a long position each day.The profit of model A is −1.Model B gives a result of down, down, down.It suggests holding a short position for three days.The profit of Model B is 1.Model A’s accuracy is 0.667 (2 out of 3), while model B is 0.333.On one hand, model A has a higher accuracy, but a lower level of profit.On the other hand, model B with the lower accuracy but achieves more profits.So, there is an inconsistency between the model’s performance and the model’s profitability.This is just a very simple example of profit bias, a more detailed example of the problem will be discussed in the next section.
Individual stock trend prediction is extremely valuable for investment management. Previous studies mainly focused on proposing effective approaches to make profits. However, there is an ineffectiveness in model evaluation due to the inconsistency between model’s performance and profitability. We name this inconsistency profit bias. In order to address the profit bias in model evaluation, this paper proposes a new effective metric, Mean Profit Rate (MPR). The effectiveness of metric is measured based on the correlation between the metric value and profit of the model. Experiments on five stock daily index data among four countries show that MPR outperforms the classification metrics in correlating to profit. In view of these findings, we suggest that MPR is a more effective metric than the classification metrics in stock trend prediction. The present study is aimed at overcoming profit bias by improving the effectiveness in the evaluation of individual stock trend prediction.Unlike previous works, we believe the present models can learn the profit information within stock data, and the profit bias is due to the ineffectiveness in selecting the optimal model.Therefore, this paper proposes a new metric, Mean Profit Rate (MPR), to evaluate models without profit bias.Experiments on five stock index data among four countries show that our metric can effectively select models due to its consistency with model’s profitability.The findings of the present study may be of some help in improving the effectiveness during the evaluation of individual stock trend prediction.
This study focuses on the analysis of a group of decapitated crania, which date to the Epiclassic period (ca. 600–900 CE). The crania were excavated from a small ritual site in the former lake bed of Lake Xaltocan, in the Basin of Mexico. Using non-metric cranial attributes, we undertook an intergroup analysis using an MMD biodistance. To place the Lake Xaltocan remains in a regional and historical biological context, we examined and recorded 13 non-metric, cranial features in 276 skulls from sites throughout central Mexico, including Tlatilco (N1 = 78), Teotihuacan (N2 = 66), Valle de Toluca (N3 = 33), and Xaltocan (N4 = 118). The results indicate that the most closely associated groups are Xaltocan and Valle de Toluca. Nevertheless, biodistance data indicate that all the groups significantly differ from one another, with a level of confidence of α = 6.6 × 10−6. This finding points to the existence of biological discontinuity among all the samples. Our results fall in line with existing proposals asserting that, after the fall of Teotihuacan, an intense population mobility occurred in central Mexico. In the Basin of Mexico, the Epiclassic period (ca. 600–900 CE) was marked by social, political, economic, and cultural change.Archaeological data suggest that most of the regional population during this time was organized around small political centers, which were separated by areas of low population and small hamlets (e.g. Parsons, 1989, 2008).Key population concentrations may have been tied to particular political and economic spheres around (1) Teotihuacan, (2) Cuauhtitlan-Atzcapotzalco, (3) Portezuelo–Cerro de la Estrella–Xico, (4) Cerro de la Mesa Ahumada, and (5) Tula, to the north (Parsons, 1989; García-Chávez, 2004; García-Chávez and Martínez-Yrízar, 2006; Crider et al., 2007).Several scholars have speculated that the political fragmentation in the region led to competition and, perhaps, to conflict (e.g. Kelly, 1978; Diehl and Berlo, 1989).It is likely that these conditions shaped the regional population dynamics (Cowgill, 2013; Beekman, 2015; Beekman and Christensen, 2003, 2011).
This study focuses on the analysis of a group of decapitated crania, which date to the Epiclassic period (ca. 600–900 CE). The crania were excavated from a small ritual site in the former lake bed of Lake Xaltocan, in the Basin of Mexico. Using non-metric cranial attributes, we undertook an intergroup analysis using an MMD biodistance. To place the Lake Xaltocan remains in a regional and historical biological context, we examined and recorded 13 non-metric, cranial features in 276 skulls from sites throughout central Mexico, including Tlatilco (N1 = 78), Teotihuacan (N2 = 66), Valle de Toluca (N3 = 33), and Xaltocan (N4 = 118). The results indicate that the most closely associated groups are Xaltocan and Valle de Toluca. Nevertheless, biodistance data indicate that all the groups significantly differ from one another, with a level of confidence of α = 6.6 × 10−6. This finding points to the existence of biological discontinuity among all the samples. Our results fall in line with existing proposals asserting that, after the fall of Teotihuacan, an intense population mobility occurred in central Mexico. This study focuses on a ritual site from the Epiclassic period in former Lake Xaltocan (designated Non-Grid 4 by surveyors), just south of the community that bears the same name today.The remains, represented by crania, of over 180 individuals, in various states of preservation, were recovered through excavations.Prior to work at Non-Grid 4, most of our knowledge of Xaltocan was associated to occupations dating to the later Postclassic period (900–1519 CE) (Brumfiel, 2003, 2005; Morehart and Crider, 2016).Several historic sources suggest that Xaltocan was an Otomi community, at least during the Early to Middle Postclassic period (ca. 900 CE/1000–1350) (Brumfiel, 2003, 2005).According to Alva Ixtlilxóchitl and Vázquez (1985), Xaltocan was the capital of an Otomi nation, and it was one of the major political centers prior to the formation of the Aztec Empire in the early 15th century, during the Late Postclassic period (ca. 1350–1519 CE) (Carrasco, 1950; Lorenzo-Monterrubio, 2001).When the town was incorporated into the Aztec Empire, a community-wide demographic shift occurred (Mata-Miguez et al., 2012).
The decision-making trial and evaluation laboratory (DEMATEL) method is a hot issue in industrial engineering field for it can help determine critical factors in complex system. Although lots of efforts have been spent on improving the DEMATEL, they are just the extensions from the subjective perspective but lack of the objective perspective. This study focuses on providing a new improved DEMATEL method based on both subjective experience and objective data. In order to reasonably determine the initial direct-relation (IDR) matrix, the basic probability assignment (BPA) function is employed to extract expert experience and the Dempster’s rule with Shafer’s discounting is employed to make combination to derive the subjective IDR matrix. Then the path analysis is suggested to test each possible influence relation included in the subjective IDR matrix, and the objective IDR matrix consisting of path coefficients of any two factors is derived by training the sample data of factors. Following the principle of one-vote negation, the Dempster’s rule is once again used to make combination for two kinds of IDR matrices, based on which an algorithm for the new improved DEMATEL is summarized to find the major factors in a complex system. Finally, numerical comparison and discussion are proposed to demonstrate the applicability and superiority of the prosed method. The decision-making trial and evaluation laboratory (DEMATEL) method was originally developed by the Science and Human Affairs Program of the Battelle Memorial Institute of Geneva between 1972 and 1976.The DEMATEL as a common multi-criteria decision making (MCDM) method is aimed at describing the basic concept of contextual relations and identifying cause–effect chain components/factors for a complex decision problem (Gabus, 1973; Fontela, 1974).
The decision-making trial and evaluation laboratory (DEMATEL) method is a hot issue in industrial engineering field for it can help determine critical factors in complex system. Although lots of efforts have been spent on improving the DEMATEL, they are just the extensions from the subjective perspective but lack of the objective perspective. This study focuses on providing a new improved DEMATEL method based on both subjective experience and objective data. In order to reasonably determine the initial direct-relation (IDR) matrix, the basic probability assignment (BPA) function is employed to extract expert experience and the Dempster’s rule with Shafer’s discounting is employed to make combination to derive the subjective IDR matrix. Then the path analysis is suggested to test each possible influence relation included in the subjective IDR matrix, and the objective IDR matrix consisting of path coefficients of any two factors is derived by training the sample data of factors. Following the principle of one-vote negation, the Dempster’s rule is once again used to make combination for two kinds of IDR matrices, based on which an algorithm for the new improved DEMATEL is summarized to find the major factors in a complex system. Finally, numerical comparison and discussion are proposed to demonstrate the applicability and superiority of the prosed method. The DEMATEL is mainly used to solve the actual group decision making (GDM) problem from the perspective of identifying the critical factors that have the greatest influence on a special system.With the help of this method, some critical factors have been successfully identified in the systems such as disaster operations management, hospital service quality improvement, industrial symbiosis networks, sustainable supply chain adoption or management, emergency management, supplier selection, truck selection, electric vehicles diffusion, and so on (Celik, 2017; Shieh et al., 2010; Ghaemi Rad et al., 2018; Bacudio et al., 2016; Li and Mathiyazhagan, 2016; Zhou et al., 2017; Mirmousa and Dehnavi, 2016; Lin et al., 2018; Song et al., 2017; Ding and Liu, 2018; Liu et al., 2017).Specially, the DEMATEL method has been widely applied into industrial applications due to its simplicity and effectiveness.Yadegaridehkordi et al. (2018) applied the DEMATEL to discover the interdependencies among the adoption factors and reveal the importance level of these adoption factors towards big data adoption.Raz and Gabis (2009) demonstrated the application of DEMATEL in identifying and analyzing barriers for implementing industrial symbiosis in an industrial park of Laguna.Bhatia and Srivastava (2018) applied the gray-DEMATEL to analyze external barriers to remanufacturing and found that the sustainable technology (providing environmental, economic and social benefits) was one of the key factors.
The decision-making trial and evaluation laboratory (DEMATEL) method is a hot issue in industrial engineering field for it can help determine critical factors in complex system. Although lots of efforts have been spent on improving the DEMATEL, they are just the extensions from the subjective perspective but lack of the objective perspective. This study focuses on providing a new improved DEMATEL method based on both subjective experience and objective data. In order to reasonably determine the initial direct-relation (IDR) matrix, the basic probability assignment (BPA) function is employed to extract expert experience and the Dempster’s rule with Shafer’s discounting is employed to make combination to derive the subjective IDR matrix. Then the path analysis is suggested to test each possible influence relation included in the subjective IDR matrix, and the objective IDR matrix consisting of path coefficients of any two factors is derived by training the sample data of factors. Following the principle of one-vote negation, the Dempster’s rule is once again used to make combination for two kinds of IDR matrices, based on which an algorithm for the new improved DEMATEL is summarized to find the major factors in a complex system. Finally, numerical comparison and discussion are proposed to demonstrate the applicability and superiority of the prosed method. In recent years, the more complex decision situations are considered and fuzzy set has been popularly employed to solve the MCDM problems.For examples, Li and Chen (2019) integrated fuzzy belief structure and gray relational projection method to propose a novel failure mode and effects analysis method.Arora and Garg (2018) presented a novel MCDM method based on the intuitionistic fuzzy soft set theory.Similarly, the fuzzy set is also employed to solve DEMATEL problems in the situation of insufficient subjective knowledge and thus several kinds of fuzzy DEMATEL methods are proposed.The mechanism of fuzzy DEMATEL methods can be summarized as follows (Si et al., 2018).The fuzzy set is used to represent subjective assessments for comparisons between each pair of factors in initial direct-relation (IDR) matrix, then some operation rules of fuzzy set theory are carried out in the steps of DEMATAL method.For examples, Suo et al. (2012) presented a formula to transform correlation information from uncertain linguistic terms to trapezoidal fuzzy numbers, and proposed a novel DEMATEL method in an uncertain linguistic environment.Fan et al. (2012) combined a 2-tuple fuzzy linguistic representation model with the classical DEMATEL to identify the importance together with the classification of risk factors of IT sourcing.Liu et al. (2015) proposed a novel hybrid MCDM model by integrating the 2-tuple DEMATEL technique and fuzzy MULTIMOORA method for selection of health-care waste treatment alternatives.Shieh and Wu (2016) proposed an integrated DEMATEL approach of using corrected item-total correlation and split-half methods to evaluate the consistency from the survey data.Wu et al. (2017) used the fuzzy and gray Delphi methods to identify a set of reliable attributes, and applied both fuzzy and gray DEMATEL to determine the causal relations for supply chain risks and uncertainties.Asan et al. (2018) proposed a new interval-valued hesitant fuzzy approach to DEMATEL, which has the ability to explicitly deal with hesitancy in expert assessments and offer a better representation of uncertainty.
The decision-making trial and evaluation laboratory (DEMATEL) method is a hot issue in industrial engineering field for it can help determine critical factors in complex system. Although lots of efforts have been spent on improving the DEMATEL, they are just the extensions from the subjective perspective but lack of the objective perspective. This study focuses on providing a new improved DEMATEL method based on both subjective experience and objective data. In order to reasonably determine the initial direct-relation (IDR) matrix, the basic probability assignment (BPA) function is employed to extract expert experience and the Dempster’s rule with Shafer’s discounting is employed to make combination to derive the subjective IDR matrix. Then the path analysis is suggested to test each possible influence relation included in the subjective IDR matrix, and the objective IDR matrix consisting of path coefficients of any two factors is derived by training the sample data of factors. Following the principle of one-vote negation, the Dempster’s rule is once again used to make combination for two kinds of IDR matrices, based on which an algorithm for the new improved DEMATEL is summarized to find the major factors in a complex system. Finally, numerical comparison and discussion are proposed to demonstrate the applicability and superiority of the prosed method. The DEMATEL has been further developed by integrating it with other kinds of MCDM methods to solve contextual decision problems.The DEMATEL is mainly used to establish contextual relations among criteria, then some other kinds of MCDM methods such as analytic hierarchy process (AHP), analytic network process (ANP), or technique for order preference by similarity to ideal solution (TOPSIS) are used to derive the holistic priorities for the evaluated alternatives.For examples, the DEMATEL and the AHP were successfully combined to solve the management problems in human resource development, human resource for science and technology, and so on (Abdullah and Zulkifli, 2015; Chou et al., 2012).The DEMATEL was also employed to modify the ANP in the step of examining causal relations among element groups and a new combined method called DEMATEL-ANP was proposed (Gölcük and Baykasoğlu, 2016).Since the DEMATEL-ANP is capable of dealing with the decision issues of complexity and dependency among different criteria, it has been widely used to solve the decision making problems in customer relationship management partner evaluation, financial reporting supply chain management, outsourcing provider evaluation and selection for a telecommunication company, internal hospital supply chain performance evaluation, alternatives evaluation regardless of dilemmas in the decision making process, renewable energy resources selection, and so on (Büyüközkan et al., 2017; Lan and Zhong, 2016; Uygun et al., 2014; Supeekit et al., 2016; Pamučar et al., 2017; Gülçin and SezinGüleryüz, 2016).In addition, the DEMATEL was integrated with the hierarchical TOPSIS and the combined method was employed to solve SWOT-based strategy selection problems and truck selection problems (Baykasoğlu and Gölcük, 2017; Baykasoǧlu et al., 2013).
The decision-making trial and evaluation laboratory (DEMATEL) method is a hot issue in industrial engineering field for it can help determine critical factors in complex system. Although lots of efforts have been spent on improving the DEMATEL, they are just the extensions from the subjective perspective but lack of the objective perspective. This study focuses on providing a new improved DEMATEL method based on both subjective experience and objective data. In order to reasonably determine the initial direct-relation (IDR) matrix, the basic probability assignment (BPA) function is employed to extract expert experience and the Dempster’s rule with Shafer’s discounting is employed to make combination to derive the subjective IDR matrix. Then the path analysis is suggested to test each possible influence relation included in the subjective IDR matrix, and the objective IDR matrix consisting of path coefficients of any two factors is derived by training the sample data of factors. Following the principle of one-vote negation, the Dempster’s rule is once again used to make combination for two kinds of IDR matrices, based on which an algorithm for the new improved DEMATEL is summarized to find the major factors in a complex system. Finally, numerical comparison and discussion are proposed to demonstrate the applicability and superiority of the prosed method. The initial decision information used in DEMATEL is always given by experts subjectively.It is obvious to find that the effectiveness of decision-making may be restricted by experts’ domain knowledge, bounded rationalities or other aspects.If the initial decision information given by expert is unreliable, then decision making result may be debatable.A lot of efforts have been spent on improving these weaknesses by either extending the DEMATEL with fuzzy set theory or integrating it with other kinds of MCDM methods.We believe that these efforts can improve the accuracy of the decision results to a certain extent, however, previous researches are still the extensions of DEMATEL improved from merely subjective perspective and they fail to consider decision making problems from the objective perspective.In our opinions, the objective data on factors, which can be easily observed in the era of big data, are beneficial to improve the decision making qualities of DEMATEL.For instance, if experts believe that there exists an influence relation between a pair of factors, but such a hypothesis is rejected by objective data, then the decision relying on subjective experience is undoubtedly to be wrong.On the contrary, is it reasonable to make a decision completely relying on objective data?From any two data sets of variables, we can compute the relation (e.g., correlation coefficient or regression coefficient) between the two variables by statistical analysis technologies.But whether the relation is significant or not should require to be tested in practice.The spurious regression often exists in statistical practice in the absence of subjective experience, just as the chief editor of MIS Quarterly argued that “The researcher may follow a process of changing and re-estimating the model until it fits the data.The final model is mistakenly believed to be correct”.(Chin, 1998)
Applications in various fields of neutrosophic soft set theory enhance its appreciation to the researchers. Although, this uncertain solving tool can accomplish several types of real-life problems, but not able to deal with the decision-making problems in which considering an additional information of a corresponding parameter is needed to get a proper decision from a problem. But, complex neutrosophic soft sets can handle such type of decision-making problems. Consequently, in this study, we have given concentration on solving soft set based decision-making problems in the field of complex neutrosophic environment. Firstly, we have introduced some basic set-theoretic operations of complex neutrosophic sets including different types of unions, intersections and aggregations. Then, a new definition of score function of a complex neutrosophic number has been proposed. Additionally, the above-defined unions and intersections of complex neutrosophic sets have been stated for complex neutrosophic soft sets. Finally, by utilizing these proposed notions, we have offered a complex neutrosophic soft VIKOR approach to get a compromise optimal solution for single as well as multiple decision-maker based problems. Our proposed approach has been clarified by several real life-related problems including medical diagnosis problem, sustainable manufacturing material selection problem, company’s manager selection problem, etc. The feasibility and effectiveness of our proposed approach have also been included in the article. One of the important and challenging issues in our day-to-day life is the presence of uncertainty to be addressed.Neutrosophic set theory (Smarandache, 2003, 2005) is one of the innovative generalization of a bunch of theories, including, fuzzy set theory (Zadeh, 1965), intuitionistic fuzzy set theory (Atanassov, 1986), vague set theory (Gau and Buehrer, 1993) etc., through three independent membership grades (truth membership (T(x)), indeterminate membership (I(x)) and false membership (F(x)) where, T(x),I(x),F(x)∈[0,1] and 0≤T(x)+I(x)+F(x)≤3).In the last few decades, neutrosophic set has been improved very quickly and has been successfully used to solve various types of real-life problems.For instance, Ye (2015) used the cosine similarity measure of neutrosophic sets in medical diagnosis problem, Zhang et al. (2015) utilized the advantages of correlation coefficient of neutrosophic sets in solving multi-criteria decision-making problems, Wu et al. (2018) developed multi attribute group decision-making by using single-valued neutrosophic 2-tuple linguistic sets etc.
Applications in various fields of neutrosophic soft set theory enhance its appreciation to the researchers. Although, this uncertain solving tool can accomplish several types of real-life problems, but not able to deal with the decision-making problems in which considering an additional information of a corresponding parameter is needed to get a proper decision from a problem. But, complex neutrosophic soft sets can handle such type of decision-making problems. Consequently, in this study, we have given concentration on solving soft set based decision-making problems in the field of complex neutrosophic environment. Firstly, we have introduced some basic set-theoretic operations of complex neutrosophic sets including different types of unions, intersections and aggregations. Then, a new definition of score function of a complex neutrosophic number has been proposed. Additionally, the above-defined unions and intersections of complex neutrosophic sets have been stated for complex neutrosophic soft sets. Finally, by utilizing these proposed notions, we have offered a complex neutrosophic soft VIKOR approach to get a compromise optimal solution for single as well as multiple decision-maker based problems. Our proposed approach has been clarified by several real life-related problems including medical diagnosis problem, sustainable manufacturing material selection problem, company’s manager selection problem, etc. The feasibility and effectiveness of our proposed approach have also been included in the article. All the above-aforementioned discussions are based on real-valued membership magnitudes like membership, non membership, indeterminate membership, which are limited in the closed interval [0,1].Then, Romot et al. (2002) extended a real-valued membership degree of a fuzzy set to a complex-valued membership degree (μC(x)) by adding a second dimension u(x) as, μC(x)=r(x)eiu(x) and introduced the concept of complex fuzzy set, where, r(x)∈[0,1] is the amplitude term and u(x), a real number, is the phase term.So, complex fuzzy set can be applied to the problems where adding a second decision information of an object over an attribute is needed to get a better and accurate result.For instance, in medical science, during disease diagnosis of a patient, the decision information about a symptom in a patient is perfect if the doctor will consider the two information ‘belongingness of a symptom’ and ‘time duration of the symptom’ together.In the earlier section, Romot et al. (2002) developed some basic set theoretic operations and properties of complex fuzzy sets along with some of its application fields.Then, Zhang et al. (2009) worked on complex fuzzy set theory, when the range of the phase term is limited to [0,2π].They also introduced δ-equalities of complex fuzzy sets.After that, a systematic review of complex fuzzy sets was presented by the researchers Yazdanbakhsh and Disk (2018).Latter, Alkouri and Salleh (2012) proposed complex intuitionistic fuzzy set by using complex sense in the both of membership degree and non membership degree.Then, Rani and Garg (2017) developed some distance measures to complex intuitionistic fuzzy sets such as Hamming distance, Hausdorff distance etc.Furthermore, Ali and Smarandache (2017) introduced complex neutrosophic set by using complex-valued truth membership, complex-valued indeterminate membership and complex-valued false membership.Then, Dat et al. (2019) solved some decision-making problems by using interval complex neutrosophic sets.
Applications in various fields of neutrosophic soft set theory enhance its appreciation to the researchers. Although, this uncertain solving tool can accomplish several types of real-life problems, but not able to deal with the decision-making problems in which considering an additional information of a corresponding parameter is needed to get a proper decision from a problem. But, complex neutrosophic soft sets can handle such type of decision-making problems. Consequently, in this study, we have given concentration on solving soft set based decision-making problems in the field of complex neutrosophic environment. Firstly, we have introduced some basic set-theoretic operations of complex neutrosophic sets including different types of unions, intersections and aggregations. Then, a new definition of score function of a complex neutrosophic number has been proposed. Additionally, the above-defined unions and intersections of complex neutrosophic sets have been stated for complex neutrosophic soft sets. Finally, by utilizing these proposed notions, we have offered a complex neutrosophic soft VIKOR approach to get a compromise optimal solution for single as well as multiple decision-maker based problems. Our proposed approach has been clarified by several real life-related problems including medical diagnosis problem, sustainable manufacturing material selection problem, company’s manager selection problem, etc. The feasibility and effectiveness of our proposed approach have also been included in the article. Even though, probability theory, fuzzy set theory, vague set theory etc. can be effectively employed to cover up uncertain situations, Molodsov (1999) said that, due to the lack of parameterization, the above theories are not sufficient to solve all types of problems.Then, as a solution, he (Molodsov, 1999) proposed the idea soft set theory by utilizing the notion of parameterization.Then, several types of theoretical developments of soft set theory have been done by the researchers.For instance, Ali et al. (2009) developed some set-theoretic operations on soft sets, Babitha and Sunil (2010) introduced soft relations and soft functions, Majumdar and Samanta (2010) offered the notion of soft mappings, Cagman and Enginoglu (2010) introduced soft matrices associated with soft sets, etc.
Applications in various fields of neutrosophic soft set theory enhance its appreciation to the researchers. Although, this uncertain solving tool can accomplish several types of real-life problems, but not able to deal with the decision-making problems in which considering an additional information of a corresponding parameter is needed to get a proper decision from a problem. But, complex neutrosophic soft sets can handle such type of decision-making problems. Consequently, in this study, we have given concentration on solving soft set based decision-making problems in the field of complex neutrosophic environment. Firstly, we have introduced some basic set-theoretic operations of complex neutrosophic sets including different types of unions, intersections and aggregations. Then, a new definition of score function of a complex neutrosophic number has been proposed. Additionally, the above-defined unions and intersections of complex neutrosophic sets have been stated for complex neutrosophic soft sets. Finally, by utilizing these proposed notions, we have offered a complex neutrosophic soft VIKOR approach to get a compromise optimal solution for single as well as multiple decision-maker based problems. Our proposed approach has been clarified by several real life-related problems including medical diagnosis problem, sustainable manufacturing material selection problem, company’s manager selection problem, etc. The feasibility and effectiveness of our proposed approach have also been included in the article. Besides, researchers have extended soft set theory to different environments.Fuzzy soft set theory (Basu et al., 2012; Paik and Mondal, 2019; Manna et al., 2017), intuitionistic fuzzy soft set theory (Manna et al., 2019), neutrosophic soft set theory (Basu and Mondal, 2015), trapezoidal interval type-2 fuzzy soft set theory (Manna et al., 2018) etc. are some important extensions of soft set theory.Then, several types of decision-making have been disposed by using soft set theory.For example, Basu et al. (2012) developed a balanced solution by using fuzzy soft set and used it in medical science, Manna et al. (2019) proposed an algorithm through generalized trapezoidal intuitionistic fuzzy soft sets and applied it in diabetes disease diagnosis problem, Karaaslan (2017) used single-valued neutrosophic redefined soft sets in clustering analysis system, etc.
Applications in various fields of neutrosophic soft set theory enhance its appreciation to the researchers. Although, this uncertain solving tool can accomplish several types of real-life problems, but not able to deal with the decision-making problems in which considering an additional information of a corresponding parameter is needed to get a proper decision from a problem. But, complex neutrosophic soft sets can handle such type of decision-making problems. Consequently, in this study, we have given concentration on solving soft set based decision-making problems in the field of complex neutrosophic environment. Firstly, we have introduced some basic set-theoretic operations of complex neutrosophic sets including different types of unions, intersections and aggregations. Then, a new definition of score function of a complex neutrosophic number has been proposed. Additionally, the above-defined unions and intersections of complex neutrosophic sets have been stated for complex neutrosophic soft sets. Finally, by utilizing these proposed notions, we have offered a complex neutrosophic soft VIKOR approach to get a compromise optimal solution for single as well as multiple decision-maker based problems. Our proposed approach has been clarified by several real life-related problems including medical diagnosis problem, sustainable manufacturing material selection problem, company’s manager selection problem, etc. The feasibility and effectiveness of our proposed approach have also been included in the article. On the other hand, complex-valued generalization of soft set theory have also been developed by the researchers.For instance, Thirunavukarasu et al. (2017) introduced the concept of complex fuzzy soft set theory by taking all the parameters in a soft set in complex fuzzy sense, Selvachandran et al. (2016a) initiated complex vague soft set theory by taking all the parameters in a vague soft set in complex fuzzy sense, Broumi et al. (2017) introduced complex neutrosophic soft set theory by taking all the parameters in a soft set in complex neutrosophic sense, etc.Then, Kumar and Bajaj (2014) defined some distance measures and entropies on complex intuitionistic fuzzy soft sets and applied it in solving multi-criteria decision-making, Selvachandran et al. (2016a) solved pattern recognition problems by using some different types of distance measures of complex vague soft sets.Furthermore, the relations between complex vague soft sets have been thrived by Selvachandran et al. (2016b).
Applications in various fields of neutrosophic soft set theory enhance its appreciation to the researchers. Although, this uncertain solving tool can accomplish several types of real-life problems, but not able to deal with the decision-making problems in which considering an additional information of a corresponding parameter is needed to get a proper decision from a problem. But, complex neutrosophic soft sets can handle such type of decision-making problems. Consequently, in this study, we have given concentration on solving soft set based decision-making problems in the field of complex neutrosophic environment. Firstly, we have introduced some basic set-theoretic operations of complex neutrosophic sets including different types of unions, intersections and aggregations. Then, a new definition of score function of a complex neutrosophic number has been proposed. Additionally, the above-defined unions and intersections of complex neutrosophic sets have been stated for complex neutrosophic soft sets. Finally, by utilizing these proposed notions, we have offered a complex neutrosophic soft VIKOR approach to get a compromise optimal solution for single as well as multiple decision-maker based problems. Our proposed approach has been clarified by several real life-related problems including medical diagnosis problem, sustainable manufacturing material selection problem, company’s manager selection problem, etc. The feasibility and effectiveness of our proposed approach have also been included in the article. After surveying the above literature, complex neutrosophic set theory, introduced by Ali and Smarandache (2017), first motivates us to use this concept in soft set theory, developed by Molodsov (1999), to solve some real-life based decision making problems.Then, we have studied complex neutrosophic soft set in decision-making, proposed by Broumi et al. (2017), from which some shortcomings have been raised in our mind such as,
Applications in various fields of neutrosophic soft set theory enhance its appreciation to the researchers. Although, this uncertain solving tool can accomplish several types of real-life problems, but not able to deal with the decision-making problems in which considering an additional information of a corresponding parameter is needed to get a proper decision from a problem. But, complex neutrosophic soft sets can handle such type of decision-making problems. Consequently, in this study, we have given concentration on solving soft set based decision-making problems in the field of complex neutrosophic environment. Firstly, we have introduced some basic set-theoretic operations of complex neutrosophic sets including different types of unions, intersections and aggregations. Then, a new definition of score function of a complex neutrosophic number has been proposed. Additionally, the above-defined unions and intersections of complex neutrosophic sets have been stated for complex neutrosophic soft sets. Finally, by utilizing these proposed notions, we have offered a complex neutrosophic soft VIKOR approach to get a compromise optimal solution for single as well as multiple decision-maker based problems. Our proposed approach has been clarified by several real life-related problems including medical diagnosis problem, sustainable manufacturing material selection problem, company’s manager selection problem, etc. The feasibility and effectiveness of our proposed approach have also been included in the article. Therefore, to fulfill these research gaps, we are motivated to develop some decision-making methods in soft set theory under complex neutrosophic environment to take a quality of decision from several types of real-life related problems.Moreover, aggregation operator is a very significant tool to take a decision from a group of evaluations.But till now, no aggregation operator exists on the basis of complex neutrosophic sets.Therefore, the introduction of aggregation operator on complex neutrosophic sets is required.However, to avoid complexity in solving real-life data based problems, transformation from an uncertain value into a real value by using a score function is very useful to handle complicated problems.But there is no definition of score function of a complex neutrosophic number.Besides, VIKOR method (Park et al., 2013) is a popular multi-criteria decision-making methodology which provides a compromise optimal solution of a decision-making problem.So, if this familiar optimization method is used in solving complex neutrosophic soft set based decision-making problems, then compromise optimal solution would be obtained as a solution instead of a general optimal solution.
Applications in various fields of neutrosophic soft set theory enhance its appreciation to the researchers. Although, this uncertain solving tool can accomplish several types of real-life problems, but not able to deal with the decision-making problems in which considering an additional information of a corresponding parameter is needed to get a proper decision from a problem. But, complex neutrosophic soft sets can handle such type of decision-making problems. Consequently, in this study, we have given concentration on solving soft set based decision-making problems in the field of complex neutrosophic environment. Firstly, we have introduced some basic set-theoretic operations of complex neutrosophic sets including different types of unions, intersections and aggregations. Then, a new definition of score function of a complex neutrosophic number has been proposed. Additionally, the above-defined unions and intersections of complex neutrosophic sets have been stated for complex neutrosophic soft sets. Finally, by utilizing these proposed notions, we have offered a complex neutrosophic soft VIKOR approach to get a compromise optimal solution for single as well as multiple decision-maker based problems. Our proposed approach has been clarified by several real life-related problems including medical diagnosis problem, sustainable manufacturing material selection problem, company’s manager selection problem, etc. The feasibility and effectiveness of our proposed approach have also been included in the article. From the above aforementioned analysis, our main contributions can be highlighted as following:
The present archaeometric study of the Punic black-gloss ware found at the “Roman Temple” of Nora (south-western Sardinia, Italy), dated at the end of the IV century BCE and the beginning of the II century BCE, was addressed to better define the exchanges of Punic ware, ideas and production skills within the west Mediterranean Sea. Petrographic and microstructural analyses at the scanning electron microscope (SEM) clearly indicate that the analysed pottery can be referred to two different productions, for which different base-clays were used, indicating different geological origin. On the basis of chemical composition, the black-gloss ware found at Nora can be traced back in part to the northern African area and in part to the western Sardinia coast at Tharros. None local production was identified. Moreover, on the basis of both the mineralogical composition and the microstructural features, samples produced in the northern African area and in western Sardinia definitely differ in terms of firing temperature. Therefore, on the basis of these results, a more complete scenario can be drawn on the commercial traffic active in the Western Mediterranean and in Sardinia between the end of the IV century and the first half of the II century BCE. Under a social perspective, Sardinia, thanks to its Phoenician, Punic and Italic influences, became an incubator of ideas, techniques and production knowledge that, from the interaction of the preceding cultures and their influences, gave life to many local productions, which were part of the greater phenomenon of Punic black-gloss ware in the western Mediterranean (Sardinia, Sicily, Northern Africa and the southern coasts of the Iberian Peninsula). Punic black-gloss ware, dating to the Hellenistic Age, is a ceramic class extensively found in the whole Punic (referred to the culture of Carthage) area as well as in those under the Punic influence.The development of this ceramic class finds its origins in the need of counteracting the decrease of tableware caused by the progressive downfall of the exports of Attic pottery, between the end of the IV century and the first half of the II century BCE.The term Punic refers to all local productions, the production techniques and shapes related both to the knowledge inherent to the Punic cultural substratum and to different external influences, initially from the Greeks and then from the Italics.
The present archaeometric study of the Punic black-gloss ware found at the “Roman Temple” of Nora (south-western Sardinia, Italy), dated at the end of the IV century BCE and the beginning of the II century BCE, was addressed to better define the exchanges of Punic ware, ideas and production skills within the west Mediterranean Sea. Petrographic and microstructural analyses at the scanning electron microscope (SEM) clearly indicate that the analysed pottery can be referred to two different productions, for which different base-clays were used, indicating different geological origin. On the basis of chemical composition, the black-gloss ware found at Nora can be traced back in part to the northern African area and in part to the western Sardinia coast at Tharros. None local production was identified. Moreover, on the basis of both the mineralogical composition and the microstructural features, samples produced in the northern African area and in western Sardinia definitely differ in terms of firing temperature. Therefore, on the basis of these results, a more complete scenario can be drawn on the commercial traffic active in the Western Mediterranean and in Sardinia between the end of the IV century and the first half of the II century BCE. Under a social perspective, Sardinia, thanks to its Phoenician, Punic and Italic influences, became an incubator of ideas, techniques and production knowledge that, from the interaction of the preceding cultures and their influences, gave life to many local productions, which were part of the greater phenomenon of Punic black-gloss ware in the western Mediterranean (Sardinia, Sicily, Northern Africa and the southern coasts of the Iberian Peninsula). In the western Mediterranean, Punic black-gloss wares are attested, in addition to the northern African regions (in the modern Morocco and Tunisia), also in southern Spain and Italy (Morel, 1963, 1986; Lancel, 1987; Chelbi, 1992; Castanyer et al., 1993; Righini Cantelli, 1983; Tronchetti, 2001, 2014; Del Vais, 2000; Michetti, 2007; De Luca, 2017; Bechtold et al., 1999).In this latter, it is attested especially in the Tyrrhenian islands, particularly in southern Sardinia, Sicily, Aeolian Islands and Ischia (Fig. 1).Since the sixties of the last century, this ceramic class has been identified in Sardinia (Morel, 1963) and has been then defined as “ceramique punique á vernis noir” (Morel, 1986); the documents were characterised by “original” features and considered “often of high quality”, reflecting a Hellenistic repertoire and representing an imitation of the widely distributed and highly appreciated black-gloss ware of Greek production (Morel, 1986).
The present archaeometric study of the Punic black-gloss ware found at the “Roman Temple” of Nora (south-western Sardinia, Italy), dated at the end of the IV century BCE and the beginning of the II century BCE, was addressed to better define the exchanges of Punic ware, ideas and production skills within the west Mediterranean Sea. Petrographic and microstructural analyses at the scanning electron microscope (SEM) clearly indicate that the analysed pottery can be referred to two different productions, for which different base-clays were used, indicating different geological origin. On the basis of chemical composition, the black-gloss ware found at Nora can be traced back in part to the northern African area and in part to the western Sardinia coast at Tharros. None local production was identified. Moreover, on the basis of both the mineralogical composition and the microstructural features, samples produced in the northern African area and in western Sardinia definitely differ in terms of firing temperature. Therefore, on the basis of these results, a more complete scenario can be drawn on the commercial traffic active in the Western Mediterranean and in Sardinia between the end of the IV century and the first half of the II century BCE. Under a social perspective, Sardinia, thanks to its Phoenician, Punic and Italic influences, became an incubator of ideas, techniques and production knowledge that, from the interaction of the preceding cultures and their influences, gave life to many local productions, which were part of the greater phenomenon of Punic black-gloss ware in the western Mediterranean (Sardinia, Sicily, Northern Africa and the southern coasts of the Iberian Peninsula). Within the recent excavation of the so called “Roman Temple” in the ancient settlement of Nora (Pula, south-western coast of Sardinia, Italy), a significant amount of black-gloss pottery sherds was found.The town was inhabited since the second half of the VIII century BCE, when Phoenician merchants used the small headland for their seasonal stays; developed into an urban settlement after the aggressive Carthaginian strategies at the end of the VI century BCE, Nora became eventually a medium-size town in the Roman Age after the constitution of the Provincia Sardiniae et Corsicae in 227 BCE (Bonetto and Ghiotto, 2013).
The present archaeometric study of the Punic black-gloss ware found at the “Roman Temple” of Nora (south-western Sardinia, Italy), dated at the end of the IV century BCE and the beginning of the II century BCE, was addressed to better define the exchanges of Punic ware, ideas and production skills within the west Mediterranean Sea. Petrographic and microstructural analyses at the scanning electron microscope (SEM) clearly indicate that the analysed pottery can be referred to two different productions, for which different base-clays were used, indicating different geological origin. On the basis of chemical composition, the black-gloss ware found at Nora can be traced back in part to the northern African area and in part to the western Sardinia coast at Tharros. None local production was identified. Moreover, on the basis of both the mineralogical composition and the microstructural features, samples produced in the northern African area and in western Sardinia definitely differ in terms of firing temperature. Therefore, on the basis of these results, a more complete scenario can be drawn on the commercial traffic active in the Western Mediterranean and in Sardinia between the end of the IV century and the first half of the II century BCE. Under a social perspective, Sardinia, thanks to its Phoenician, Punic and Italic influences, became an incubator of ideas, techniques and production knowledge that, from the interaction of the preceding cultures and their influences, gave life to many local productions, which were part of the greater phenomenon of Punic black-gloss ware in the western Mediterranean (Sardinia, Sicily, Northern Africa and the southern coasts of the Iberian Peninsula). The black-gloss ware sherds recovered from the deep and continuous stratigraphy of the so called "Roman Temple", share quite a few common features under a macroscopic and morphological viewpoints: they have a quite pliable ceramic body and a non-uniform, blackish glaze (Fig. 2a).The main shapes of Punic black-gloss ware attested at Nora are small cups, bowls and plates (Fig. 2b) belonging to the Morel type 2600 (Morel, 1986).
We describe a patient with unilateral periventricular nodular heterotopia (PNH) and drug-resistant epilepsy, whose SEEG revealed that seizures were arising from the PNH, with the almost simultaneous involvement of heterotopic neurons (“micronodules”) scattered within the white matter, and subsequently the overlying cortex. Laser ablation of heterotopic nodules and the adjacent white matter rendered the patient seizure free. This case elucidates that “micronodules” scattered in white matter between heterotopic nodules and overlying cortex might be another contributor in complex epileptogenicity of heterotopia. Detecting patient-specific targets in the epileptic network of heterotopia creates the possibility to disrupt the pathological circuit by minimally invasive procedures. Periventricular nodular heterotopia (PNH) is a malformation of neuronal migration characterized by masses of neurons and glial cells with a rudimentary laminar organization located close to the periventricular germinal matrix [1,2].PNH may be bilateral or unilateral.Genetic factors can play a major role in bilateral cases while acquired factors may be more important in the latter [3].Focal epilepsy, most commonly drug-resistant, presents in childhood or early adulthood [4].PNH often has intrinsic epileptogenicity but may not always be primarily involved in the generation of seizures [4–8].Stereo-EEG (SEEG) studies found diverse, patient-specific networks, with seizure-onset being simultaneous in nodules and overlying or widespread cortical structures, or simultaneous in mesial temporal structures and ipsilateral adjacent heterotopia, or onset in the overlying cortex or the nodules alone [4,5,7,8].Although an earlier study has characterized PNH as poorly responsive to traditional epilepsy surgery [9], others reported a high rate of seizure-freedom, similar to other lesional epilepsies [5].Conflicting results were published recently on SEEG-guided thermocoagulation: as a very effective therapeutic approach for drug-resistant epilepsy related to PNH [7] and on contrary, having transitory, mild, or no effect [8].There are only a few cases describing laser ablation in PNH (summaries and outcomes are shown in Table 1).
In this study, bulk bone collagen carbon (δ13C) and nitrogen (δ15N) isotope data from 49 individuals, recovered from two Medieval burial grounds in Hereford, England, are coupled with incremental dentine data from five individuals with high δ15N bone values who survived into old age, to see whether the high δ15N values were consistent throughout their childhood and adolescence. There are statistically insignificant differences between mean bone δ13C and δ15N values from the two Hereford populations, exhumed at Cathedral Close and St. Guthlac's Priory, despite temporal and demographic differences (St Guthlac's mean: δ13C −19.4 ± 0.5‰ and δ15N 10.9 ± 1.2‰; Hereford Cathedral mean: δ13C −19.6 ± 0.4‰ and δ15N 10.4 ± 0.9‰, 1σ). In comparison to other contemporary urban populations, the Hereford individuals present significantly lower but more variable δ15N values, suggesting a diet low in protein from high trophic level foods such as meat and milk, possibly the result of differing social status or geographic factors. The approximately 23-year long incremental dentine profiles all show considerable fluctuation in stable isotope values during childhood and adolescence for all individuals until around age 20, suggesting possible influence by physiological processes related to growth and development. Coupled with archaeological, historical, and palaeopathological observations, stable isotope analysis is an increasingly valuable tool to examine palaeodietary, geographic, and temporal differences in past human populations (Lee-Thorp, 2008; Müldner and Richards, 2005, 2006, 2007a, 2007b).Typically, such analyses have been undertaken through sampling bone collagen, focusing on the ratios of carbon (13C/12C, δ13C) and nitrogen (15N/14N, δ15N), to estimate the proportion of plant, animal, and fish protein in a population's diet, along with possible indicators of stress or illness, based on several well-established principles (Balasse and Ambrose, 2005; Beaumont and Montgomery, 2016; Cerling, 1993; Lee-Thorp, 2008).As such, the δ15N values of sampled collagen will increase significantly when an individual's diet includes protein from fish, meat or animal secondary products (Schoeninger and DeNiro, 1984).These values may further be lowered by anabolic events, such as growth and pregnancy, and increased by catabolic events, such as malnutrition and illness (Fuller et al., 2004, 2005, 2006; Mekota et al., 2006; Waters-Rist and Katzenberg, 2010).Typically, δ13C values will predominantly reflect the photosynthetic pathway (C3 or C4) of plants at the bottom of the food chain (Farquhar et al., 1989).Increased δ13C values can also indicate the presence of marine protein in a person's diet, due to the heightening effect of carbon reservoirs on the δ13C values of marine organisms, or access to imported plant protein such as millet (Schoeninger and DeNiro, 1984).
In this study, bulk bone collagen carbon (δ13C) and nitrogen (δ15N) isotope data from 49 individuals, recovered from two Medieval burial grounds in Hereford, England, are coupled with incremental dentine data from five individuals with high δ15N bone values who survived into old age, to see whether the high δ15N values were consistent throughout their childhood and adolescence. There are statistically insignificant differences between mean bone δ13C and δ15N values from the two Hereford populations, exhumed at Cathedral Close and St. Guthlac's Priory, despite temporal and demographic differences (St Guthlac's mean: δ13C −19.4 ± 0.5‰ and δ15N 10.9 ± 1.2‰; Hereford Cathedral mean: δ13C −19.6 ± 0.4‰ and δ15N 10.4 ± 0.9‰, 1σ). In comparison to other contemporary urban populations, the Hereford individuals present significantly lower but more variable δ15N values, suggesting a diet low in protein from high trophic level foods such as meat and milk, possibly the result of differing social status or geographic factors. The approximately 23-year long incremental dentine profiles all show considerable fluctuation in stable isotope values during childhood and adolescence for all individuals until around age 20, suggesting possible influence by physiological processes related to growth and development. Using these principles, δ13C and δ15N profiles for burial populations have been created to estimate the relationship between the diet, health, and socio-economic status of past populations and to provide comparisons with historical evidence (e.g. Curtis-Summers et al., 2014; Katzenberg and Lovell, 1999; Müldner and Richards, 2005, 2006, 2007a, 2007b; Thompson et al., 2008).Most commonly, these have been carried out on bone collagen, but such isotope values face the challenge that they may reflect an average of the last twenty years or more of an individual's life, depending on the rate of bone remodelling (Cox and Sealy, 1997; Hedges et al., 2007).These challenges are exacerbated by factors such as imprecise assessments of age at death, failure to resolve short-term events, and limitations in fully representative sampling, due to the inherent biases in the demography of burial populations (AlQahtani et al., 2010; Beaumont et al., 2015; Reynard and Tuross, 2015; Wood et al., 1992).However, recently refined techniques, utilising incremental primary dentine instead of bone collagen have overcome many of these issues.Primary dentine does not remodel and mineralises in a highly regular manner, recording discrete isotope values at incremental lines of growth during formation (Beaumont and Montgomery, 2015; Beaumont et al., 2013, 2014; Lee-Thorp, 2008).By sectioning primary dentine at specific intervals, isotope values can be assigned approximate known formation times and the age at which each section formed can be estimated (Beaumont and Montgomery, 2015).Using this approach, it is possible to point out specific occurrences in past individuals' lives, often related to specific events such as periods of famine (Beaumont and Montgomery, 2016), illness (Fuller et al., 2005) or weaning (Fuller et al., 2006), predominantly during childhood (cf. Beaumont and Montgomery, 2015; Beaumont et al., 2013, 2014, 2015; Eerkens et al., 2016; Montgomery et al., 2013; Sandberg et al., 2014; van der Sluis et al., 2015).
We describe a case of mesial temporal extraventricular neurocytoma (mtEVN) in a 23-year-old male presenting with drug-resistant seizures and review the literature on this rare tumor. A PubMed search was queried using the MeSH term “neurocytoma” and key search terms “extraventricular”, “temporal”, and “epilepsy”. Titles and abstracts were screened for temporal neurocytomas. References were reviewed to identify further studies. Twenty case reports were selected comparing the presentation, radiological, histopathological, and surgical outcomes of neocortex temporal EVNs (ntEVN) and mtEVNs. Gross total resection of mtEVNs under intraoperative electrocorticography monitoring typically affords an excellent prognosis and successful seizure control. Drug-resistant focal impaired awareness seizures (FIAS) are most commonly associated with mesial temporal sclerosis [1].Fortunately, epilepsy secondary to mesial temporal sclerosis is amenable to surgical treatment with satisfactory seizure control and improvement in life of quality.However, 20–35% of patients with temporal FIAS have an underlying brain tumor responsible [2].Correctly identifying the etiology of FIAS is imperative to obtaining a successful outcome and seizure control [3].Thus, the diagnostic dilemma of mesial temporal FIAS has been a challenge for epileptologists and epilepsy surgeons.
We describe a case of mesial temporal extraventricular neurocytoma (mtEVN) in a 23-year-old male presenting with drug-resistant seizures and review the literature on this rare tumor. A PubMed search was queried using the MeSH term “neurocytoma” and key search terms “extraventricular”, “temporal”, and “epilepsy”. Titles and abstracts were screened for temporal neurocytomas. References were reviewed to identify further studies. Twenty case reports were selected comparing the presentation, radiological, histopathological, and surgical outcomes of neocortex temporal EVNs (ntEVN) and mtEVNs. Gross total resection of mtEVNs under intraoperative electrocorticography monitoring typically affords an excellent prognosis and successful seizure control. Extraventricular neurocytomas (EVN) have been reported and classified as a distinct entity that can cause drug-resistant seizures [4].These benign tumors of contentious origin generally afflict children and young adults and can present in a variety of locations in the CNS [5].In particular, EVNs arising in the temporal lobe most often present with FIAS.Temporally-located EVNs can occur in the neocortex or derive from the mesial temporal structures [6].Mesial temporal extraventricular neurocytomas (mtEVN) are a subtype of temporal EVNs and have not been well documented in medical literature.It is important to highlight the individual characteristics of mtEVNs as these tumors differ from those of extratemporal EVNs and even neocortical temporal EVNs (ntEVN) in terms of their presentation, radiology, pathology, and surgical outcomes [6].
Market prediction has been an important research problem for decades. Having better predictive models that are both more accurate and faster has been attractive for both researchers and traders. Among many approaches, semi-supervised graph-based prediction has been used as a solution in recent researches. Based on this approach, we present two prediction models. In the first model, a new network structure is introduced that can capture more information about markets’ direction of movements compared to the previous state of the art methods. Based on this novel network, a new algorithm for semi-supervised label propagation is designed that is able to prediction the direction of movement faster and more accurately. The second model is a mixture of experts system that decides between supervised or semi-supervised approaches. Besides this, the model gives us the ability to identify the markets that their data are helpful in constructing the network. Our models are shown to be both faster regarding computational complexity and running time and more accurate in prediction comparing to best rival models in literature of graph-based semi-supervised prediction. The results are also tested to be statistically significant. Direction of movement prediction is an important research topic in financial time series studies.Many traders prefer to know about the direction of movement of a market rather than a precise value for the prices or indices (Yao and Tan, 2000).A traders’ decision of hold, buy, or sell comes from his/her perception of future direction rather than exact values of price or index.That is the reason for development of several direction of movement prediction models in many researches like Kia et al. (2018), Li and Liao (2017), Patel et al. (2015), Imandoust and Bolandraftar (2014), Kara et al. (2011) and Huang et al. (2005).
Market prediction has been an important research problem for decades. Having better predictive models that are both more accurate and faster has been attractive for both researchers and traders. Among many approaches, semi-supervised graph-based prediction has been used as a solution in recent researches. Based on this approach, we present two prediction models. In the first model, a new network structure is introduced that can capture more information about markets’ direction of movements compared to the previous state of the art methods. Based on this novel network, a new algorithm for semi-supervised label propagation is designed that is able to prediction the direction of movement faster and more accurately. The second model is a mixture of experts system that decides between supervised or semi-supervised approaches. Besides this, the model gives us the ability to identify the markets that their data are helpful in constructing the network. Our models are shown to be both faster regarding computational complexity and running time and more accurate in prediction comparing to best rival models in literature of graph-based semi-supervised prediction. The results are also tested to be statistically significant. Efficient market hypothesis (EMH) indicates that all information revealed and related, immediately affect the price of a financial time series (Fama et al., 1969).One of the results from the EMH is that predicting prices and stock indices from historical data will not provide profit for the traders (Timmermann and Granger, 2004).EMH has many critics and measuring the level of efficiency in markets has been a challenging topic of research for the decades.There are also many researches in the field of finance and economic that bring evidences in favor of using prediction models to gain profit and show anomalies in EMH (Naseer and Bin Tariq, 2015; Malkiel, 2003; Lo and MacKinlay, 1988).Many researches in the field of financial prediction along with different methods as well as the success of several algorithmic trading companies are evidences that show the traders have found predictions valuable.
Market prediction has been an important research problem for decades. Having better predictive models that are both more accurate and faster has been attractive for both researchers and traders. Among many approaches, semi-supervised graph-based prediction has been used as a solution in recent researches. Based on this approach, we present two prediction models. In the first model, a new network structure is introduced that can capture more information about markets’ direction of movements compared to the previous state of the art methods. Based on this novel network, a new algorithm for semi-supervised label propagation is designed that is able to prediction the direction of movement faster and more accurately. The second model is a mixture of experts system that decides between supervised or semi-supervised approaches. Besides this, the model gives us the ability to identify the markets that their data are helpful in constructing the network. Our models are shown to be both faster regarding computational complexity and running time and more accurate in prediction comparing to best rival models in literature of graph-based semi-supervised prediction. The results are also tested to be statistically significant. There are different categories of methods in the literature for financial time series prediction.The oldest ones are technical analysis methods which use traditional statistical methods to calculate some indicators for prediction.A good survey on these methods can be found in the work of Atsalakis and Valavanis (2013).Some other researches use the financial statements of markets and companies to predict their future.This approach that is called fundamental analysis method has been used in many other researches like Yan and Zheng (2017), Chen et al. (2017), Shen and Tzeng (2015) and Abarbanell and Bushee (1997).Both technical and fundamental analysis are conventional methods used by the traders for decades.With emergence of the machine learning and data mining science, many researchers turned into using these algorithms and techniques for financial time series forecasting.These methods and algorithms have shown to outperform the conventional methods mentioned before in different studies (Atsalakis and Valavanis, 2009).Several surveys have compared machine learning and hybrid methods that use machine learning with conventional methods for market prediction (Rather et al., 2017; Cavalcante et al., 2016; Atsalakis and Valavanis, 2009; Preethi and Santhi, 2012; Soni, 2011).
Market prediction has been an important research problem for decades. Having better predictive models that are both more accurate and faster has been attractive for both researchers and traders. Among many approaches, semi-supervised graph-based prediction has been used as a solution in recent researches. Based on this approach, we present two prediction models. In the first model, a new network structure is introduced that can capture more information about markets’ direction of movements compared to the previous state of the art methods. Based on this novel network, a new algorithm for semi-supervised label propagation is designed that is able to prediction the direction of movement faster and more accurately. The second model is a mixture of experts system that decides between supervised or semi-supervised approaches. Besides this, the model gives us the ability to identify the markets that their data are helpful in constructing the network. Our models are shown to be both faster regarding computational complexity and running time and more accurate in prediction comparing to best rival models in literature of graph-based semi-supervised prediction. The results are also tested to be statistically significant. From one perspective, machine learning algorithms are divided into three categories of supervised, unsupervised, and semi-supervised learning (Zhu, 2006).Supervised learning has been used mostly for regression and classification models.In supervised prediction, the model is learned by sample instances that have known class labels, then the learned model is used to predict the label of new instances with unknown labels.The labels can be direction of movement of markets time series.In semi-supervised learning there are a few samples with known labels and many samples without labels.Semi-supervised learning tries to label the unlabeled samples using both the labeled ones as well as the underlying structure of all training samples.A network-based approach to find out this underlying structure is called graph-based semi-supervised learning (Goldberg and Zhu, 2006).Using supervised learning models trained with historical data of the market in prediction means that we assume past patterns in the market data can be helpful in prediction of its future.As mentioned before EMH challenges this assumption by reminding us that efficient markets behave as a random-walk processes.In our experiments we found out that many financial time series cannot be predicted better than a fair coin model with their own historical data and this may be an evidence of their efficiency.But some other financial time series were predictable with their historical data.We also observed that many of the stock market indices were predictable using the data from other stock markets and commodity prices.This led us to use semi-supervised learning models where we knew the direction of movements for some markets and some other markets had to be predicted.
Market prediction has been an important research problem for decades. Having better predictive models that are both more accurate and faster has been attractive for both researchers and traders. Among many approaches, semi-supervised graph-based prediction has been used as a solution in recent researches. Based on this approach, we present two prediction models. In the first model, a new network structure is introduced that can capture more information about markets’ direction of movements compared to the previous state of the art methods. Based on this novel network, a new algorithm for semi-supervised label propagation is designed that is able to prediction the direction of movement faster and more accurately. The second model is a mixture of experts system that decides between supervised or semi-supervised approaches. Besides this, the model gives us the ability to identify the markets that their data are helpful in constructing the network. Our models are shown to be both faster regarding computational complexity and running time and more accurate in prediction comparing to best rival models in literature of graph-based semi-supervised prediction. The results are also tested to be statistically significant. The interrelationship among markets with known and unknown labels can be modeled using graph structures (a network).The network models have shown their superiority in representing financial markets interrelations in the past researches (Pereira et al., 2017; Jovanovic and Schinckus, 2017; McCauley, 2004; Mantegna and Stanley, 1999).Some researches like Kia et al. (2018), Saumya et al. (2016), Skabar (2013), Shin et al. (2013), Park and Shin (2013) and Upstill et al. (2003) have tried to forecast financial time series using network models.Different network structures have been used and different semi-supervised learning algorithm over the network have been applied in these studies.
Market prediction has been an important research problem for decades. Having better predictive models that are both more accurate and faster has been attractive for both researchers and traders. Among many approaches, semi-supervised graph-based prediction has been used as a solution in recent researches. Based on this approach, we present two prediction models. In the first model, a new network structure is introduced that can capture more information about markets’ direction of movements compared to the previous state of the art methods. Based on this novel network, a new algorithm for semi-supervised label propagation is designed that is able to prediction the direction of movement faster and more accurately. The second model is a mixture of experts system that decides between supervised or semi-supervised approaches. Besides this, the model gives us the ability to identify the markets that their data are helpful in constructing the network. Our models are shown to be both faster regarding computational complexity and running time and more accurate in prediction comparing to best rival models in literature of graph-based semi-supervised prediction. The results are also tested to be statistically significant. In the first part of this research, we introduce a new approach for modeling the financial markets data as a network structure that captures the information that is relevant and important regarding the desired prediction task.Also we show a novel approach for analyzing the suggested network structure in order to forecast different markets faster and more accurately.As we will see, the experiments shows the superiority of the suggested approach in capturing relevant movement relationship between different markets and using it for movement prediction, compared to previous networks.
Market prediction has been an important research problem for decades. Having better predictive models that are both more accurate and faster has been attractive for both researchers and traders. Among many approaches, semi-supervised graph-based prediction has been used as a solution in recent researches. Based on this approach, we present two prediction models. In the first model, a new network structure is introduced that can capture more information about markets’ direction of movements compared to the previous state of the art methods. Based on this novel network, a new algorithm for semi-supervised label propagation is designed that is able to prediction the direction of movement faster and more accurately. The second model is a mixture of experts system that decides between supervised or semi-supervised approaches. Besides this, the model gives us the ability to identify the markets that their data are helpful in constructing the network. Our models are shown to be both faster regarding computational complexity and running time and more accurate in prediction comparing to best rival models in literature of graph-based semi-supervised prediction. The results are also tested to be statistically significant. The new network structure used in our research is built upon the information of association rules such as ifmarketAgoesup⟹marketBgoesdown that are extracted from the dataset.A novel weighting method is used for weighting the links between markets in the network.The weighting of the links or edges of the network is important in graph-based semi-supervised learning.Our idea of weighting the edges is taken from the concept of lift of rules in association rules mining domain.The nodes of the network represent up and down directions of each market.Then known information of the markets in time zones that their markets are open, are used to predict the direction of markets in the closed time zones.
Market prediction has been an important research problem for decades. Having better predictive models that are both more accurate and faster has been attractive for both researchers and traders. Among many approaches, semi-supervised graph-based prediction has been used as a solution in recent researches. Based on this approach, we present two prediction models. In the first model, a new network structure is introduced that can capture more information about markets’ direction of movements compared to the previous state of the art methods. Based on this novel network, a new algorithm for semi-supervised label propagation is designed that is able to prediction the direction of movement faster and more accurately. The second model is a mixture of experts system that decides between supervised or semi-supervised approaches. Besides this, the model gives us the ability to identify the markets that their data are helpful in constructing the network. Our models are shown to be both faster regarding computational complexity and running time and more accurate in prediction comparing to best rival models in literature of graph-based semi-supervised prediction. The results are also tested to be statistically significant. In this study we are going to predict next time-step direction of movements in financial time series of stock markets and commodity prices with a novel semi-supervised label propagation approach using a modified personalized PageRank-based algorithm on a network structure of different time series direction of movements interrelations.A novel mixture of experts model is then designed to decide between supervised or semi-supervised prediction approach.The ability of a hybrid model that uses both the benefits of supervised and semi-supervised methods, for movement prediction has been shown in previous studies (Kia et al., 2018).In this study we suggest a new model that achieves better results using a new semi-supervised approach for analyzing a network model with a novel design.But again if in some cases a market is predicted better with its own historical data, the mixture of experts model guides us to use a fully supervised model.This model also helps us find out which markets should be in the network structure.We will show the superiority of our models over other models in terms of prediction performance, computational complexity, and running time in the next sections of the paper.
Market prediction has been an important research problem for decades. Having better predictive models that are both more accurate and faster has been attractive for both researchers and traders. Among many approaches, semi-supervised graph-based prediction has been used as a solution in recent researches. Based on this approach, we present two prediction models. In the first model, a new network structure is introduced that can capture more information about markets’ direction of movements compared to the previous state of the art methods. Based on this novel network, a new algorithm for semi-supervised label propagation is designed that is able to prediction the direction of movement faster and more accurately. The second model is a mixture of experts system that decides between supervised or semi-supervised approaches. Besides this, the model gives us the ability to identify the markets that their data are helpful in constructing the network. Our models are shown to be both faster regarding computational complexity and running time and more accurate in prediction comparing to best rival models in literature of graph-based semi-supervised prediction. The results are also tested to be statistically significant. We will name the proposed models DiMexRank and MixDiMex.DiMexRank (Direction of Movement prediction with extended lift network using modified PageRank) and MixDiMex, (Mixture of experts model using supervised learning and DiMexRank) are faster, and more accurate comparing to their best rival models in the literature.MixDiMex is also a way to find out whether a market’s data is helpful in prediction of other markets future direction or not.
The eruption of the Laacher See volcano ca. 13.000 years ago profoundly influenced the lifeways of Final Palaeolithic foragers inhabiting the fallout area. Apart from the substantial devastation that affected the proximal area around the eruptive centre (<50 km), substantial amounts of tephra covered the medial (50–500 km) and distal (500–1000 km) zones of the eruption. In particular, substantial amounts of volcanic ash were transported towards the northeast across Germany and into the Baltic region. In order to find new sites that would allow us to investigate the far-field effects of this cataclysmic event in detail, a predictive model using a legacy dataset of rock shelters in the Federal State of Hesse in Central Germany was developed. Hitherto, only few sites where Laacher See tephra is directly stratigraphically associated with Final Palaeolithic archaeology are known in the region. Following the in silico evaluation of the archaeological potential, two survey campaigns were conducted which resulted in the discovery of several locations that in turn will be subject to keyhole excavations in a subsequent field campaign. Environmental change is often regarded to be one of the major triggers of prehistoric culture change and technological adaptation.Particularly, long-term changes and corresponding human responses are typically at the centre of archaeological investigation.However, intense and rapid changes, such as environmental disruptions triggered by volcanic events, may also induce shifts in human lifeways (e.g. Cashman and Giordano, 2008; Oppenheimer, 2011; Riede, 2016b; Sheets, 2015).
The eruption of the Laacher See volcano ca. 13.000 years ago profoundly influenced the lifeways of Final Palaeolithic foragers inhabiting the fallout area. Apart from the substantial devastation that affected the proximal area around the eruptive centre (<50 km), substantial amounts of tephra covered the medial (50–500 km) and distal (500–1000 km) zones of the eruption. In particular, substantial amounts of volcanic ash were transported towards the northeast across Germany and into the Baltic region. In order to find new sites that would allow us to investigate the far-field effects of this cataclysmic event in detail, a predictive model using a legacy dataset of rock shelters in the Federal State of Hesse in Central Germany was developed. Hitherto, only few sites where Laacher See tephra is directly stratigraphically associated with Final Palaeolithic archaeology are known in the region. Following the in silico evaluation of the archaeological potential, two survey campaigns were conducted which resulted in the discovery of several locations that in turn will be subject to keyhole excavations in a subsequent field campaign. During the Late Glacial Interstadial Complex, the eruption of the Laacher See volcano in Germany at around 13,000 cal BP destabilized the ecological framework throughout large parts of Central Europe (Baales et al., 2002; Schmincke, 2006).Among other things, it likely induced a cooling of the northern hemisphere estimated at 0.5 to 2 °C (Graf and Timmreck, 2001; Riede, 2017).In fact, a multitude of hazards can be attributed to the volcanic event and in particular to the ejected tephra which was deposited across Europe in a swath ranging from Italy in the south to north-western Russia in the east (Riede, 2017).At the time of the eruption, Europe was occupied by hunter-gatherer groups generally assigned to the Final Palaeolithic arch-backed point techno-complex, although many workers see regional variation emerging at this time (see Sauer and Riede, 2018 for a summary of arguments for and against regionalisation in this period).Foragers operating in Central Europe at the time of the eruption were subject to the sudden and intense change of environmental conditions. >300,000 km2 were covered with ash fallout (Fischer and Schmincke, 1984; Riede et al., 2011).For these Final Palaeolithic hunter-gatherers, the effects of the Laacher See eruption (LSE) likely extended throughout most of their range and affected their lifeways in various ways.
The eruption of the Laacher See volcano ca. 13.000 years ago profoundly influenced the lifeways of Final Palaeolithic foragers inhabiting the fallout area. Apart from the substantial devastation that affected the proximal area around the eruptive centre (<50 km), substantial amounts of tephra covered the medial (50–500 km) and distal (500–1000 km) zones of the eruption. In particular, substantial amounts of volcanic ash were transported towards the northeast across Germany and into the Baltic region. In order to find new sites that would allow us to investigate the far-field effects of this cataclysmic event in detail, a predictive model using a legacy dataset of rock shelters in the Federal State of Hesse in Central Germany was developed. Hitherto, only few sites where Laacher See tephra is directly stratigraphically associated with Final Palaeolithic archaeology are known in the region. Following the in silico evaluation of the archaeological potential, two survey campaigns were conducted which resulted in the discovery of several locations that in turn will be subject to keyhole excavations in a subsequent field campaign. In the proximal zone (0–50 km) around the eruptive centre, numerous Final Palaeolithic sites predating the LSE are known (Baales, 1999; Baales, 2002; Baales et al., 1996; Bolus, 1992; Heinen, 2008; Kegler, 2002).This situation is at least in part related to the massive pumice cover, which led to a favourable preservation of the often ephemeral remains of these highly mobile foragers.Furthermore, the site of Bad Breisig, which post-dates the eruption and lies just the north of the eruptive centre, attests to the recolonization of the desolated near-vent region some decades or centuries after the eruption (Waldmann et al., 2001).In the medial zone (50–500 km), however, excavated sites – and particularly those showing a clear stratigraphic association of LSE tephra and Final Palaeolithic archaeology – are less frequent (Riede, 2012).At the same time, a plethora of surface collections can be attributed to the Final Palaeolithic arch-backed point techno-complex, indicating the presence of these foragers in the Central European uplands during the Late Glacial Interstadial Complex (Sauer, 2018).
The eruption of the Laacher See volcano ca. 13.000 years ago profoundly influenced the lifeways of Final Palaeolithic foragers inhabiting the fallout area. Apart from the substantial devastation that affected the proximal area around the eruptive centre (<50 km), substantial amounts of tephra covered the medial (50–500 km) and distal (500–1000 km) zones of the eruption. In particular, substantial amounts of volcanic ash were transported towards the northeast across Germany and into the Baltic region. In order to find new sites that would allow us to investigate the far-field effects of this cataclysmic event in detail, a predictive model using a legacy dataset of rock shelters in the Federal State of Hesse in Central Germany was developed. Hitherto, only few sites where Laacher See tephra is directly stratigraphically associated with Final Palaeolithic archaeology are known in the region. Following the in silico evaluation of the archaeological potential, two survey campaigns were conducted which resulted in the discovery of several locations that in turn will be subject to keyhole excavations in a subsequent field campaign. Using radiocarbon dates and Bayesian modelling (Riede and Edinborough, 2012), network approaches (Riede, 2014) and lithic studies (Dev and Riede, 2012; Riede, 2009), previous research has suggested considerable long-range impacts of the LSE on the culture-historical trajectories of forager groups in northern Europe (Riede, 2008; Riede, 2017).At the same time, this ‘Laacher See hypothesis’ predicts a depopulation or at least substantial reduction in land-use in the region north-east between the eruptive centre and the North European Plain (Riede, 2012; Riede, 2016a).
A set of 37 overfired ceramic samples was collected from the dump of two kilns sited in the productive area FF1 in the acropolis of Selinunte (south western Sicily), being specifically active in the period 409–250 BCE. The ceramic samples were analysed by thin-section petrography and chemical analysis, with the aim to establish a valuable ‘reference group’ representative of the ceramic produced at Selinunte during the Punic phase. The petrographic and chemical analyses allowed to state that the ceramic manufactures from the kilns operating in the FF1 insula are characterized by rather homogeneous textural/compositional features. The daily-use common ware here produced is characterized by aplastic inclusions mainly falling in the size classes of coarse silt and medium sand, with relative abundance ranging between 15 and 25% area. The inclusions are composed of monocrystalline quartz and, subordinately, of calcareous bioclasts, polycrystalline quartz, K-feldspar, plagioclase, chert, sandstones and acid rock fragments. The relatively low total chemical variability of the ceramic sample set reflects the specific incidence of the above-mentioned mineralogical and textural features. The variable amount of quartz-rich sand used for tempering the local raw clays produces slight variations in the SiO2/CaO concentration ratio. Nonetheless, the chemical ‘reference group’ defined through this study seems to be consistent and characterized by satisfactory low standard deviations and it is fully congruent with the geo-lithological background of the area. This new chemical ‘reference group’ might be applied to studies that are aimed to define the trade networks in that time in south western Sicily. It could also represent a useful starting point for future systematic studies concerning various ceramics classes (i.e. tableware, cooking ware, transport amphorae, etc.), taking into account the consumption and insular/extra insular trade dynamics of the ceramic products of Punic Selinunte. Selinus (or Selinous, modern: Selinunte) is one of the rare “second generation” Sicilian Greek colonies.It is in fact related to the Greek mother-city Megara Nysaea and its first Sicilian colony Megara Hyblaea, laying the foundation of the sub-colony Heraclea Minoa as well.Selinunte was the farthest western outpost of the Greek civilization in the central Mediterranean being located on the border of the Phoenico-Punic and the Elymian territories (Fig. 1).
A set of 37 overfired ceramic samples was collected from the dump of two kilns sited in the productive area FF1 in the acropolis of Selinunte (south western Sicily), being specifically active in the period 409–250 BCE. The ceramic samples were analysed by thin-section petrography and chemical analysis, with the aim to establish a valuable ‘reference group’ representative of the ceramic produced at Selinunte during the Punic phase. The petrographic and chemical analyses allowed to state that the ceramic manufactures from the kilns operating in the FF1 insula are characterized by rather homogeneous textural/compositional features. The daily-use common ware here produced is characterized by aplastic inclusions mainly falling in the size classes of coarse silt and medium sand, with relative abundance ranging between 15 and 25% area. The inclusions are composed of monocrystalline quartz and, subordinately, of calcareous bioclasts, polycrystalline quartz, K-feldspar, plagioclase, chert, sandstones and acid rock fragments. The relatively low total chemical variability of the ceramic sample set reflects the specific incidence of the above-mentioned mineralogical and textural features. The variable amount of quartz-rich sand used for tempering the local raw clays produces slight variations in the SiO2/CaO concentration ratio. Nonetheless, the chemical ‘reference group’ defined through this study seems to be consistent and characterized by satisfactory low standard deviations and it is fully congruent with the geo-lithological background of the area. This new chemical ‘reference group’ might be applied to studies that are aimed to define the trade networks in that time in south western Sicily. It could also represent a useful starting point for future systematic studies concerning various ceramics classes (i.e. tableware, cooking ware, transport amphorae, etc.), taking into account the consumption and insular/extra insular trade dynamics of the ceramic products of Punic Selinunte. In 480 BCE, the Greek world was threatened by the Persians at east and by the Carthaginians at west.In that historical frame, Sicily's fate was decided at the Battle of Himera.In this conflict Selinunte is the only major Greek colony that did not participate to the battle on the Greek side.The ambiguous neutrality of Selinunte towards the Carthaginians, who were defeated at Himera, did not prevent its destruction in the 409 BCE.In the following decades several treaties redefined continuously the borders between the Greek and the Punic parts of the island and, finally, the city passed under the Carthaginian control.During the 4th-3rd century BCE, as soon as the limits of the city were redefined, Selinunte was deeply influenced by both Greek and Punic culture and a prosperity period with rising commercial businesses and craft activities took place (Fresina and Bonanno, 2013; Fourmont, 2014).
A set of 37 overfired ceramic samples was collected from the dump of two kilns sited in the productive area FF1 in the acropolis of Selinunte (south western Sicily), being specifically active in the period 409–250 BCE. The ceramic samples were analysed by thin-section petrography and chemical analysis, with the aim to establish a valuable ‘reference group’ representative of the ceramic produced at Selinunte during the Punic phase. The petrographic and chemical analyses allowed to state that the ceramic manufactures from the kilns operating in the FF1 insula are characterized by rather homogeneous textural/compositional features. The daily-use common ware here produced is characterized by aplastic inclusions mainly falling in the size classes of coarse silt and medium sand, with relative abundance ranging between 15 and 25% area. The inclusions are composed of monocrystalline quartz and, subordinately, of calcareous bioclasts, polycrystalline quartz, K-feldspar, plagioclase, chert, sandstones and acid rock fragments. The relatively low total chemical variability of the ceramic sample set reflects the specific incidence of the above-mentioned mineralogical and textural features. The variable amount of quartz-rich sand used for tempering the local raw clays produces slight variations in the SiO2/CaO concentration ratio. Nonetheless, the chemical ‘reference group’ defined through this study seems to be consistent and characterized by satisfactory low standard deviations and it is fully congruent with the geo-lithological background of the area. This new chemical ‘reference group’ might be applied to studies that are aimed to define the trade networks in that time in south western Sicily. It could also represent a useful starting point for future systematic studies concerning various ceramics classes (i.e. tableware, cooking ware, transport amphorae, etc.), taking into account the consumption and insular/extra insular trade dynamics of the ceramic products of Punic Selinunte. The vocation of Selinunte for pottery production roughly dates back to the period of its foundation, as it was recently evidenced by a research of the University of Bonn.In fact, on the eastern flank of the hill named Manuzza, several bricks, tiles and pottery manufacture structures were found that date back to the beginning of the 6th century BCE (Bentz et al., 2013).In this relatively small area of the eastern site of the city, extensive geophysical surveys have identified at least 80 kilns for ceramic firing that represent a unique case in Antiquity.The workshop area of Manuzza, however, no longer produced after the destruction of 409 BCE.Despite of that, astonishing evidences of pottery making activities for the period following 409 BCE were documented, in the last quarter of the 19th century (Fourmont, 1991, 2005).In fact, in the part of the excavations of Selinunte named FF1 North, the presence of several potter's workshop producing vessels and terracotta figurines was attested (Fourmont, 1991).These workshops are particularly relevant as they are fairly well preserved.In their vicinity, the discovery of many overfired vessels has allowed to know the pottery forms therein produced.In particular, different types of Punic amphorae were found which represent the first evidence of this production in Selinunte (Fourmont, 2013).Another unique aspect is the discovery of a kiln even with part of its load still inside and several melted artefacts.Together with all the pots and the remains of the structure, catapult balls were found inside the kiln, thus dating in an exceptional way the moment when the event should be placed, that is to say 250 BCE, when ‘Carthaginian’ Selinunte fell after the Roman siege during the First Punic War (Fourmont, 2005).
A set of 37 overfired ceramic samples was collected from the dump of two kilns sited in the productive area FF1 in the acropolis of Selinunte (south western Sicily), being specifically active in the period 409–250 BCE. The ceramic samples were analysed by thin-section petrography and chemical analysis, with the aim to establish a valuable ‘reference group’ representative of the ceramic produced at Selinunte during the Punic phase. The petrographic and chemical analyses allowed to state that the ceramic manufactures from the kilns operating in the FF1 insula are characterized by rather homogeneous textural/compositional features. The daily-use common ware here produced is characterized by aplastic inclusions mainly falling in the size classes of coarse silt and medium sand, with relative abundance ranging between 15 and 25% area. The inclusions are composed of monocrystalline quartz and, subordinately, of calcareous bioclasts, polycrystalline quartz, K-feldspar, plagioclase, chert, sandstones and acid rock fragments. The relatively low total chemical variability of the ceramic sample set reflects the specific incidence of the above-mentioned mineralogical and textural features. The variable amount of quartz-rich sand used for tempering the local raw clays produces slight variations in the SiO2/CaO concentration ratio. Nonetheless, the chemical ‘reference group’ defined through this study seems to be consistent and characterized by satisfactory low standard deviations and it is fully congruent with the geo-lithological background of the area. This new chemical ‘reference group’ might be applied to studies that are aimed to define the trade networks in that time in south western Sicily. It could also represent a useful starting point for future systematic studies concerning various ceramics classes (i.e. tableware, cooking ware, transport amphorae, etc.), taking into account the consumption and insular/extra insular trade dynamics of the ceramic products of Punic Selinunte. The aim of this paper is to identify the chemical Reference Group/Groups (RG or RGs) and the petrographical fabric/fabrics of the ceramic production of the sector FF1 in Selinunte.As well established, a RG is defined on the basis of the chemical composition of pottery sampled at kilns and/or kiln waste dumps (Buxeda i Garrigós et al., 2001; Buxeda i Garrigós and Madrid i Fernandez, 2017).The high fired pottery wastes normally found in kiln dumps offer much higher certainty of local production, therefore they are especially important for the identification of chemical reference groups.Nevertheless, the chemical reference groups do not relate in a straight line pottery to possible local geological sources for the raw materials employed, as different steps in the processing of the same raw materials and/or firing conditions result in different chemical composition.Technological processes, on the other hand, are directly related to the functional purposes of the ceramics, since they provide specific physical properties to the final product.Therefore, when RGs are identified, the researchers have to take into consideration that a pottery workshop may produce artefacts having different functional purposes, even for a limited period of time, thus the raw materials may also have undergone different processing steps and so, that can result in more than one RG for the same kiln/workshop.Accordingly any eligible ‘reference group’ should be as close as possible to a specific ceramic class or at least macro category (e.g. coarse tableware) and it should possess a relatively low internal variability in terms of chemical composition (Buxeda i Garrigós and Madrid i Fernandez, 2017; Waksman, 2017 and references therein).A decisive input in this field can be given by the thin-section microscopy technique.It helps to interpret the concentration values and the variation intervals shown by the chemical elements that compose the ceramic sample (major and trace elements) in light of both mineralogical composition and textural aspects, such as the relative abundance of the aplastic constituents (packing) and their grain-size distribution (Quinn, 2009).Therefore complementing chemistry with petrography can provide much more information and more accurate results (Montana et al., 2012; Cau Ontiveros et al., 2015; Tsantini et al., 2017).
We present a case series of three boys with childhood epilepsy with myoclonic–atonic seizures (EMAS) who achieved complete remission during childhood only to develop absence seizures during early adolescence. In all three cases, the recurrent seizures resolved again with antiseizure drugs, and two are currently medication-free for a second time. Doose syndrome (DS), otherwise known as epilepsy with myoclonic-atonic seizures (EMAS) and previously myoclonic-astatic epilepsy, is a seizure syndrome of young childhood associated with prominent myoclonic and astatic seizures [1].EEG findings include bursts of 2- to 5-Hz spike and polyspike and wave complexes, with normal sleep architecture and posterior basic rhythm.EMAS typically presents with a peak onset between the ages of 3–4 years, with a range of 7 months and 6 years of age [1].Antiseizure drugs reported to be the most effective for EMAS include ethosuximide, lamotrigine, and valproic acid, although levetiracetam is also widely used [2].Certain antiseizure drugs may cause paradoxical worsening, including carbamazepine, oxcarbazepine, phenytoin, and vigabatrin.The ketogenic diet has been shown to be highly efficacious, and reports have suggested that it could be even considered as a first line therapy in these patients [3].Close to two-thirds of patients achieve seizure remission, within 5–6 years of onset [4,5].
We present a case series of three boys with childhood epilepsy with myoclonic–atonic seizures (EMAS) who achieved complete remission during childhood only to develop absence seizures during early adolescence. In all three cases, the recurrent seizures resolved again with antiseizure drugs, and two are currently medication-free for a second time. The long-term outcomes from EMAS are not widely described.Although most patients will achieve remission, it is not clear how many will have recurrence years later.Additionally, it is known that other epilepsy syndromes can remit, only to have other seizure types or even different syndromes occur months to years later [6–10].
A new bio-inspired optimization technique, named Manta Ray Foraging Optimization (MRFO) algorithm, is proposed and presented, aiming to providing a novel algorithm that provides an alternate optimization approach for addressing real-world engineering issues. The inspiration of this algorithm is based on intelligent behaviors of manta rays. This work mimics three unique foraging strategies of manta rays, including chain foraging, cyclone foraging, and somersault foraging, to develop an efficient optimization paradigm for solving different optimization problems. The performance of MRFO is evaluated, through comparisons with other state-of-the-art optimizers, on benchmark optimization functions and eight real-world engineering design cases. The comparison results on the benchmark functions suggest that MRFO is far superior to its competitors. In addition, the real-world engineering applications show the merits of this algorithm in tackling challenging problems in terms of computational cost and solution precision. The MATLAB codes of the MRFO algorithm are available at https://www.mathworks.com/matlabcentral/fileexchange/73130-manta-ray-foraging-optimization-mrfo. Many real-world optimization problems are increasingly becoming challenging as they often concern a big number of decision variables, complex nonlinear constraints and objective functions.The global optimization using traditional approaches like numerical methods becomes less powerful especially when objective functions or constraints have multiple peaks.Metaheuristic algorithms, powerful tools for handling challenging optimization problems, are increasingly becoming popular.The popularity drives from the following aspects.
A new bio-inspired optimization technique, named Manta Ray Foraging Optimization (MRFO) algorithm, is proposed and presented, aiming to providing a novel algorithm that provides an alternate optimization approach for addressing real-world engineering issues. The inspiration of this algorithm is based on intelligent behaviors of manta rays. This work mimics three unique foraging strategies of manta rays, including chain foraging, cyclone foraging, and somersault foraging, to develop an efficient optimization paradigm for solving different optimization problems. The performance of MRFO is evaluated, through comparisons with other state-of-the-art optimizers, on benchmark optimization functions and eight real-world engineering design cases. The comparison results on the benchmark functions suggest that MRFO is far superior to its competitors. In addition, the real-world engineering applications show the merits of this algorithm in tackling challenging problems in terms of computational cost and solution precision. The MATLAB codes of the MRFO algorithm are available at https://www.mathworks.com/matlabcentral/fileexchange/73130-manta-ray-foraging-optimization-mrfo. First, the most outstanding characteristic of metaheuristic algorithms is their simplicity.These metaheuristic methods possess basic theories or mathematical models which derive from nature.These methods are generally simple and easy to implement.The ease-to-use allows one to apply metaheuristics to solve real-world problems.Moreover, it is also easy to develop their variants according to existing methods.Second, these optimization technologies can be viewed as a black box, meaning that it is able to offer a set of outputs for a given problem for a set of inputs.Furthermore, scholars may easily modify the structures and parameters of these methods in order to obtain satisfactory solutions.Third, randomness is one the most important characteristics of metaheuristic algorithms.This allows metaheuristic algorithms to explore the entire search space and prevent them from trapping into local optima effectively.More specially, it makes many metaheuristics successful to solve problems with unknown search space or multiple local optima.Finally, these metaheuristics are highly versatile and flexible, implying their practicability to various different types of optimization problems including non-linear problems, non-differentiable problems, or complex numerical problems with plentiful of local minima.Many metaheuristic algorithms have been presented and successfully applied to different areas.These algorithms are mainly categorized into three classes (Hare et al., 2013): evolution-based (Mühlenbein et al., 1988), physics-based (Geem et al., 2001), and swarm-based (Krause et al., 2013).
A new bio-inspired optimization technique, named Manta Ray Foraging Optimization (MRFO) algorithm, is proposed and presented, aiming to providing a novel algorithm that provides an alternate optimization approach for addressing real-world engineering issues. The inspiration of this algorithm is based on intelligent behaviors of manta rays. This work mimics three unique foraging strategies of manta rays, including chain foraging, cyclone foraging, and somersault foraging, to develop an efficient optimization paradigm for solving different optimization problems. The performance of MRFO is evaluated, through comparisons with other state-of-the-art optimizers, on benchmark optimization functions and eight real-world engineering design cases. The comparison results on the benchmark functions suggest that MRFO is far superior to its competitors. In addition, the real-world engineering applications show the merits of this algorithm in tackling challenging problems in terms of computational cost and solution precision. The MATLAB codes of the MRFO algorithm are available at https://www.mathworks.com/matlabcentral/fileexchange/73130-manta-ray-foraging-optimization-mrfo. Evolution-based algorithms simulate natural evolution like chemotaxis, reproduction elimination and dispersal, and migration (Passino, 2002; De Falco et al., 2012).Genetic algorithm (GA), proposed by Holland (1975), is a famous and widely used evolutionary algorithm (EA).One of the main features of GA is that it does not require derivative in the search space that is existing in mathematical optimization approaches.GA evolves a population by emulating survival of the fittest in nature, it may provide efficient solutions and avoid local optima.Since its emergence, a range of variants have been developed to improve GA.With its popularity, many other evolutionary algorithms, including differential evolution (DE) (Rocca et al., 2011), evolutionary programming (EP) (Juste et al., 1999), evolutionary strategies (ES) (Beyer and Schwefel, 2002), memetic algorithm (MA) (Moscato et al., 2007), and so on, have been proposed.In addition, scores of novel EAs have been presented recently, including biogeography-based optimization (BBO) (Simon, 2009), bacterial foraging optimization (BFO) (Passino, 2002), artificial algae algorithm (AAA) (Uymaz et al., 2015), bat algorithm (BA) (Yang and Hossein Gandomi, 2012), monkey king evolutionary (MKE) (Meng and Pan, 2016), et al. (Punnathanam and Kotecha, 2016; Pan, 2012; Mehrabian and Lucas, 2006; Civicioglu, 2013).
A new bio-inspired optimization technique, named Manta Ray Foraging Optimization (MRFO) algorithm, is proposed and presented, aiming to providing a novel algorithm that provides an alternate optimization approach for addressing real-world engineering issues. The inspiration of this algorithm is based on intelligent behaviors of manta rays. This work mimics three unique foraging strategies of manta rays, including chain foraging, cyclone foraging, and somersault foraging, to develop an efficient optimization paradigm for solving different optimization problems. The performance of MRFO is evaluated, through comparisons with other state-of-the-art optimizers, on benchmark optimization functions and eight real-world engineering design cases. The comparison results on the benchmark functions suggest that MRFO is far superior to its competitors. In addition, the real-world engineering applications show the merits of this algorithm in tackling challenging problems in terms of computational cost and solution precision. The MATLAB codes of the MRFO algorithm are available at https://www.mathworks.com/matlabcentral/fileexchange/73130-manta-ray-foraging-optimization-mrfo. Physics-based algorithms mimic physical laws in universe.Simulated annealing (SA) (Kirkpatrick et al., 1983) is one of the most well-known physics-based techniques.SA is analogy with thermodynamics in physical material.Annealing is to minimize energy use, specifically with the way that heated metals cool and crystallize.Recently, multiple new physics-inspired techniques are developed, including gravitational search algorithm (GSA) (Rashedi et al., 2009), electromagnetism-like mechanism (EM) algorithm (Birbil and Fang, 2003), particle collision algorithm (PCA) (Sacco and De Oliveira, 2005), vortex search algorithm (VSA) (Doğan and Ölmez, 2015), water evaporation optimization (WEO) (Kaveh and Bakhshpoori, 2016), atom search optimization (ASO) (Zhao et al., 2019a), big bang–big crunch algorithm (BB-BC) (Genç et al., 2010), et al. (Shah-Hosseini, 2011; Chuang and Jiang, 2007; Shah-Hosseini, 2009; Kaveh and Dadras, 2017; Kaveh and Talatahari, 2010; Zheng et al., 2010; Javidy et al., 2015; Mirjalili and Hashim, 2012; Zheng, 2015; Flores et al., 2011; Tamura and Yasuda, 2011; Rao et al., 2012; Zarand et al., 2002; Shen and Li, 2009; Kripka and Kripka, 2008; Patel and Savsani, 2015; Eskandar et al., 2012; Moghaddam et al., 2012).
A new bio-inspired optimization technique, named Manta Ray Foraging Optimization (MRFO) algorithm, is proposed and presented, aiming to providing a novel algorithm that provides an alternate optimization approach for addressing real-world engineering issues. The inspiration of this algorithm is based on intelligent behaviors of manta rays. This work mimics three unique foraging strategies of manta rays, including chain foraging, cyclone foraging, and somersault foraging, to develop an efficient optimization paradigm for solving different optimization problems. The performance of MRFO is evaluated, through comparisons with other state-of-the-art optimizers, on benchmark optimization functions and eight real-world engineering design cases. The comparison results on the benchmark functions suggest that MRFO is far superior to its competitors. In addition, the real-world engineering applications show the merits of this algorithm in tackling challenging problems in terms of computational cost and solution precision. The MATLAB codes of the MRFO algorithm are available at https://www.mathworks.com/matlabcentral/fileexchange/73130-manta-ray-foraging-optimization-mrfo. Swarm-based algorithms simulate social behaviors of species like self-organization and division of labor (Beni and Wang, 1993; Ab Wahab et al., 2015).Two outstanding examples are particle swarm optimization (PSO) (Kennedy and Eberhart, 1995) and ant colony optimization (ACO) (Dorigo et al., 1996).PSO inspired by bird flocking behaviors updates each agent in a population by its best individual agent and the best global agent.ACO is inspired by the foraging behaviors of ant swarms, ants search for the most effective route from their nest to the food source by the intensity of pheromones which reduces over time.Other examples of swarm-inspired algorithms include glowworm swarm optimization (GSO) (Krihnanand and Ghose, 2009), grey wolf optimization (GWO) (Mirjalili et al., 2014), artificial ecosystem-based optimization (AEO) (Zhao et al., 2019b), shark smell optimization (SSO) (Abedinia et al., 2014), firefly algorithm (FA) (Yang, 2010), supply–demand-based optimization (SDO) (Zhao et al., 2019c), spotted hyena optimization (SHO) (Gaurav and Vijay, 2017), and so on (Yang and Deb, 2009; Oftadeh et al., 2010; Kiran, 2015; Mohamed et al., 2017; Cuevas et al., 2013; Askarzadeh, 2014; Saremi et al., 2017; Akay and Karaboga, 2012a, b; Mirjalili, 2016; Kaveh and Farhoudi, 2013; Yang, 2010; Mucherino and Seref, 2007; Gandomi and Alavi, 2012).
A new bio-inspired optimization technique, named Manta Ray Foraging Optimization (MRFO) algorithm, is proposed and presented, aiming to providing a novel algorithm that provides an alternate optimization approach for addressing real-world engineering issues. The inspiration of this algorithm is based on intelligent behaviors of manta rays. This work mimics three unique foraging strategies of manta rays, including chain foraging, cyclone foraging, and somersault foraging, to develop an efficient optimization paradigm for solving different optimization problems. The performance of MRFO is evaluated, through comparisons with other state-of-the-art optimizers, on benchmark optimization functions and eight real-world engineering design cases. The comparison results on the benchmark functions suggest that MRFO is far superior to its competitors. In addition, the real-world engineering applications show the merits of this algorithm in tackling challenging problems in terms of computational cost and solution precision. The MATLAB codes of the MRFO algorithm are available at https://www.mathworks.com/matlabcentral/fileexchange/73130-manta-ray-foraging-optimization-mrfo. It is worth mentioning that there are other recently developed metaheuristics motivated from social behaviors and ideology in humans.Some of the most well-known ones include parliamentary optimization algorithm (POA) (Borji, 2007), artificial human optimization (AHO) (Gajawada, 2016), continuous opinion dynamics optimization (CODO) (Kaur et al., 2013), league championship algorithm (LCA) (Kashan, 2009), social group optimization (SGO) (Satapathy and Naik, 2016), ideology algorithm (IA) (Huan et al., 2016), and so on (Kuo and Lin, 2013; Moosavian and Roodsari, 2014; Kumar et al., 2018; Xu et al., 2010; Ray and Liew, 2003).
A new bio-inspired optimization technique, named Manta Ray Foraging Optimization (MRFO) algorithm, is proposed and presented, aiming to providing a novel algorithm that provides an alternate optimization approach for addressing real-world engineering issues. The inspiration of this algorithm is based on intelligent behaviors of manta rays. This work mimics three unique foraging strategies of manta rays, including chain foraging, cyclone foraging, and somersault foraging, to develop an efficient optimization paradigm for solving different optimization problems. The performance of MRFO is evaluated, through comparisons with other state-of-the-art optimizers, on benchmark optimization functions and eight real-world engineering design cases. The comparison results on the benchmark functions suggest that MRFO is far superior to its competitors. In addition, the real-world engineering applications show the merits of this algorithm in tackling challenging problems in terms of computational cost and solution precision. The MATLAB codes of the MRFO algorithm are available at https://www.mathworks.com/matlabcentral/fileexchange/73130-manta-ray-foraging-optimization-mrfo. Swarm-based algorithms have some unique features.Some historical information about the swarm is restored to provide a basis for every agent to update individual positions by using interaction rules over subsequent iterations.In general, swarm-based techniques share two specific behaviors, exploration and exploitation (Alba and Dorronsoro, 2005; Lynn and Suganthan, 2015).Exploration is to search a wide variable space for promising solutions which are not neighbor to the current solution, and this search should be as extensive and random as possible.This behavior generally contributes to escaping local optima.Exploitation is to confine the search to a small region found in the exploration process to refine the solution.Essentially this behavior implements local search in a promising space.Based on these two behaviors, swarm-based algorithms have superiority over other two types of algorithms.
A new bio-inspired optimization technique, named Manta Ray Foraging Optimization (MRFO) algorithm, is proposed and presented, aiming to providing a novel algorithm that provides an alternate optimization approach for addressing real-world engineering issues. The inspiration of this algorithm is based on intelligent behaviors of manta rays. This work mimics three unique foraging strategies of manta rays, including chain foraging, cyclone foraging, and somersault foraging, to develop an efficient optimization paradigm for solving different optimization problems. The performance of MRFO is evaluated, through comparisons with other state-of-the-art optimizers, on benchmark optimization functions and eight real-world engineering design cases. The comparison results on the benchmark functions suggest that MRFO is far superior to its competitors. In addition, the real-world engineering applications show the merits of this algorithm in tackling challenging problems in terms of computational cost and solution precision. The MATLAB codes of the MRFO algorithm are available at https://www.mathworks.com/matlabcentral/fileexchange/73130-manta-ray-foraging-optimization-mrfo. Some might question why new optimization algorithms are still developed despite of so many existing algorithms.The answer can be found in the No Free Lunch Theorem of Optimization (Wolpert and Macready, 1997), which describes that there is no optimizer performing the best for all optimization problems.So developing an effective swarm-inspired optimizer to solve specific real-world problems motivates this work.
A new bio-inspired optimization technique, named Manta Ray Foraging Optimization (MRFO) algorithm, is proposed and presented, aiming to providing a novel algorithm that provides an alternate optimization approach for addressing real-world engineering issues. The inspiration of this algorithm is based on intelligent behaviors of manta rays. This work mimics three unique foraging strategies of manta rays, including chain foraging, cyclone foraging, and somersault foraging, to develop an efficient optimization paradigm for solving different optimization problems. The performance of MRFO is evaluated, through comparisons with other state-of-the-art optimizers, on benchmark optimization functions and eight real-world engineering design cases. The comparison results on the benchmark functions suggest that MRFO is far superior to its competitors. In addition, the real-world engineering applications show the merits of this algorithm in tackling challenging problems in terms of computational cost and solution precision. The MATLAB codes of the MRFO algorithm are available at https://www.mathworks.com/matlabcentral/fileexchange/73130-manta-ray-foraging-optimization-mrfo. This paper proposes a new metaheuristic algorithm, named manta ray foraging optimization (MRFO), which simulates the foraging behaviors of manta rays.This algorithm has three foraging operators, including chain foraging, cyclone foraging, and somersault foraging.The performance of MRFO is tested using 31 test functions and 8 engineering problems.The test results discover that the proposed method significantly outperforms those well-known metaheuristics.
The use of Knowledge Engineering (KE) processes to analyze and configure domains in automated planning is becoming more appealing since it was noticed that this issue could make a difference to solve real problems. The contrast between a generic domain independent approach, taken as canonical in AI, and alternative processes that include knowledge engineering – eventually adding specific knowledge – has been discussed by Computer and Engineering communities. A big impact has been noticed mainly in the early phase of requirement analysis when KE approach is normally introduced. Requirement analysis is responsible for carrying out the Knowledge modeling of both problem and work domains, which is a key issue to guide different planner algorithms to come out with efficient solutions. Also, there is the scalability issue that appear in most real problems. To face that, hierarchical methods played an important hole in the history of planning and inspired several solutions since the proposal of NONLIN in the 70’s. Since then, the idea of associating hierarchical relational nets with partial ordered actions has prevailed when large systems were considered. However, there is still a gap between the hierarchical approach and the state of art of requirements analysis to allow features anticipated by KE approach to really appear in the requirements of a planning process. This paper proposes a pathway to solve this gap starting with requirements elicitation represented first in the conventional semi-formal (diagrammatic) language – UML – that is translated to Hierarchical Petri Nets (HPNs) by a new enhanced algorithm. The proposed process was installed in a software tool – developed by one of the authors – that analyzes the performance of the KE planning model: itSIMPLE (Integrated Tools Software Interface for Modeling Planning Environment). This tool was initially designed to use classic Place/Transition nets and an old version of UML (2.1). It is now enhanced to use UML 2.4 and a hierarchical Petri Net extension, also developed by the authors. Realistic examples illustrate the process which is now being applied to larger problems related to the manufacturing of car sequencing domain, one of challenge of ROADEF 2005 (French Operations Research & Decision Support Society). Finally, we consider the possibility to introduce another approach to the KE process by using KAOS (Keep All Object Satisfied) to make the planning design more accurate. Planning defines a specific type of state-transition problem where the goal is to find an admissible sequence of actions to bring the system from a given initial state to a target final state.Some approaches in the literature aim to improve the performance of intelligent automated planners by trying to optimize search algorithms for a general solution (Edelkamp and Jabbar, 2006).In addition, most existing work on AI planning use a domain independent approach where specific knowledge and restrictions of the target problem are not modeled and analyzed in the planning domain.However, even domain independent approaches lead to very smart solution frameworks – normally based on STRIPS – and in practice, can be adapted to solve real problems.In this work, a domain-independent general approach is still used as inspiration for planners algorithms, but before planners start the search for a sequence of actions that lead to the final state the whole planning domain is modeled and analyzed based on requirements (Vaquero et al., 2013b).
The use of Knowledge Engineering (KE) processes to analyze and configure domains in automated planning is becoming more appealing since it was noticed that this issue could make a difference to solve real problems. The contrast between a generic domain independent approach, taken as canonical in AI, and alternative processes that include knowledge engineering – eventually adding specific knowledge – has been discussed by Computer and Engineering communities. A big impact has been noticed mainly in the early phase of requirement analysis when KE approach is normally introduced. Requirement analysis is responsible for carrying out the Knowledge modeling of both problem and work domains, which is a key issue to guide different planner algorithms to come out with efficient solutions. Also, there is the scalability issue that appear in most real problems. To face that, hierarchical methods played an important hole in the history of planning and inspired several solutions since the proposal of NONLIN in the 70’s. Since then, the idea of associating hierarchical relational nets with partial ordered actions has prevailed when large systems were considered. However, there is still a gap between the hierarchical approach and the state of art of requirements analysis to allow features anticipated by KE approach to really appear in the requirements of a planning process. This paper proposes a pathway to solve this gap starting with requirements elicitation represented first in the conventional semi-formal (diagrammatic) language – UML – that is translated to Hierarchical Petri Nets (HPNs) by a new enhanced algorithm. The proposed process was installed in a software tool – developed by one of the authors – that analyzes the performance of the KE planning model: itSIMPLE (Integrated Tools Software Interface for Modeling Planning Environment). This tool was initially designed to use classic Place/Transition nets and an old version of UML (2.1). It is now enhanced to use UML 2.4 and a hierarchical Petri Net extension, also developed by the authors. Realistic examples illustrate the process which is now being applied to larger problems related to the manufacturing of car sequencing domain, one of challenge of ROADEF 2005 (French Operations Research & Decision Support Society). Finally, we consider the possibility to introduce another approach to the KE process by using KAOS (Keep All Object Satisfied) to make the planning design more accurate. After extensive development combining domain independent and domain specific approaches some authors started to apply planning techniques to real world problems – as real logistic systems – with a great amount of variables, where a sole domain independent approach would be computationally prohibitive (Vaquero et al., 2012).That opened some space to alternative approaches that use also specific knowledge and a direct approach to the knowledge engineering process by building and modeling the problem domain.Such approach could lead to good results treating challenge problems and give some feedback on how to solve fully automated domain-independent problems.
The use of Knowledge Engineering (KE) processes to analyze and configure domains in automated planning is becoming more appealing since it was noticed that this issue could make a difference to solve real problems. The contrast between a generic domain independent approach, taken as canonical in AI, and alternative processes that include knowledge engineering – eventually adding specific knowledge – has been discussed by Computer and Engineering communities. A big impact has been noticed mainly in the early phase of requirement analysis when KE approach is normally introduced. Requirement analysis is responsible for carrying out the Knowledge modeling of both problem and work domains, which is a key issue to guide different planner algorithms to come out with efficient solutions. Also, there is the scalability issue that appear in most real problems. To face that, hierarchical methods played an important hole in the history of planning and inspired several solutions since the proposal of NONLIN in the 70’s. Since then, the idea of associating hierarchical relational nets with partial ordered actions has prevailed when large systems were considered. However, there is still a gap between the hierarchical approach and the state of art of requirements analysis to allow features anticipated by KE approach to really appear in the requirements of a planning process. This paper proposes a pathway to solve this gap starting with requirements elicitation represented first in the conventional semi-formal (diagrammatic) language – UML – that is translated to Hierarchical Petri Nets (HPNs) by a new enhanced algorithm. The proposed process was installed in a software tool – developed by one of the authors – that analyzes the performance of the KE planning model: itSIMPLE (Integrated Tools Software Interface for Modeling Planning Environment). This tool was initially designed to use classic Place/Transition nets and an old version of UML (2.1). It is now enhanced to use UML 2.4 and a hierarchical Petri Net extension, also developed by the authors. Realistic examples illustrate the process which is now being applied to larger problems related to the manufacturing of car sequencing domain, one of challenge of ROADEF 2005 (French Operations Research & Decision Support Society). Finally, we consider the possibility to introduce another approach to the KE process by using KAOS (Keep All Object Satisfied) to make the planning design more accurate. Therefore, the automated planning area carries a dual challenge:
The use of Knowledge Engineering (KE) processes to analyze and configure domains in automated planning is becoming more appealing since it was noticed that this issue could make a difference to solve real problems. The contrast between a generic domain independent approach, taken as canonical in AI, and alternative processes that include knowledge engineering – eventually adding specific knowledge – has been discussed by Computer and Engineering communities. A big impact has been noticed mainly in the early phase of requirement analysis when KE approach is normally introduced. Requirement analysis is responsible for carrying out the Knowledge modeling of both problem and work domains, which is a key issue to guide different planner algorithms to come out with efficient solutions. Also, there is the scalability issue that appear in most real problems. To face that, hierarchical methods played an important hole in the history of planning and inspired several solutions since the proposal of NONLIN in the 70’s. Since then, the idea of associating hierarchical relational nets with partial ordered actions has prevailed when large systems were considered. However, there is still a gap between the hierarchical approach and the state of art of requirements analysis to allow features anticipated by KE approach to really appear in the requirements of a planning process. This paper proposes a pathway to solve this gap starting with requirements elicitation represented first in the conventional semi-formal (diagrammatic) language – UML – that is translated to Hierarchical Petri Nets (HPNs) by a new enhanced algorithm. The proposed process was installed in a software tool – developed by one of the authors – that analyzes the performance of the KE planning model: itSIMPLE (Integrated Tools Software Interface for Modeling Planning Environment). This tool was initially designed to use classic Place/Transition nets and an old version of UML (2.1). It is now enhanced to use UML 2.4 and a hierarchical Petri Net extension, also developed by the authors. Realistic examples illustrate the process which is now being applied to larger problems related to the manufacturing of car sequencing domain, one of challenge of ROADEF 2005 (French Operations Research & Decision Support Society). Finally, we consider the possibility to introduce another approach to the KE process by using KAOS (Keep All Object Satisfied) to make the planning design more accurate. Indeed, complex domains are hard to deal with when no abstraction is provided.In such domains, a hierarchical decomposition of the problem, based on a topological structure, can lead to better performance.The use of formal approaches to domain analysis such as Petri Net can then be justified.
The use of Knowledge Engineering (KE) processes to analyze and configure domains in automated planning is becoming more appealing since it was noticed that this issue could make a difference to solve real problems. The contrast between a generic domain independent approach, taken as canonical in AI, and alternative processes that include knowledge engineering – eventually adding specific knowledge – has been discussed by Computer and Engineering communities. A big impact has been noticed mainly in the early phase of requirement analysis when KE approach is normally introduced. Requirement analysis is responsible for carrying out the Knowledge modeling of both problem and work domains, which is a key issue to guide different planner algorithms to come out with efficient solutions. Also, there is the scalability issue that appear in most real problems. To face that, hierarchical methods played an important hole in the history of planning and inspired several solutions since the proposal of NONLIN in the 70’s. Since then, the idea of associating hierarchical relational nets with partial ordered actions has prevailed when large systems were considered. However, there is still a gap between the hierarchical approach and the state of art of requirements analysis to allow features anticipated by KE approach to really appear in the requirements of a planning process. This paper proposes a pathway to solve this gap starting with requirements elicitation represented first in the conventional semi-formal (diagrammatic) language – UML – that is translated to Hierarchical Petri Nets (HPNs) by a new enhanced algorithm. The proposed process was installed in a software tool – developed by one of the authors – that analyzes the performance of the KE planning model: itSIMPLE (Integrated Tools Software Interface for Modeling Planning Environment). This tool was initially designed to use classic Place/Transition nets and an old version of UML (2.1). It is now enhanced to use UML 2.4 and a hierarchical Petri Net extension, also developed by the authors. Realistic examples illustrate the process which is now being applied to larger problems related to the manufacturing of car sequencing domain, one of challenge of ROADEF 2005 (French Operations Research & Decision Support Society). Finally, we consider the possibility to introduce another approach to the KE process by using KAOS (Keep All Object Satisfied) to make the planning design more accurate. For a case study we will take the challenge launched in ROADEF 2005 (Solnon et al., 2008), which brings a problem domain synthesized from automotive industry where a significant improvement in performance is achieved when hierarchical models are introduced.The advantage of hierarchical approach relies in the possibility to see abstract integration analysis – for assembling, painting, etc – without a sacrifice of the formalism.Details can be added (or reused) by the insertion of hierarchical components.
The use of Knowledge Engineering (KE) processes to analyze and configure domains in automated planning is becoming more appealing since it was noticed that this issue could make a difference to solve real problems. The contrast between a generic domain independent approach, taken as canonical in AI, and alternative processes that include knowledge engineering – eventually adding specific knowledge – has been discussed by Computer and Engineering communities. A big impact has been noticed mainly in the early phase of requirement analysis when KE approach is normally introduced. Requirement analysis is responsible for carrying out the Knowledge modeling of both problem and work domains, which is a key issue to guide different planner algorithms to come out with efficient solutions. Also, there is the scalability issue that appear in most real problems. To face that, hierarchical methods played an important hole in the history of planning and inspired several solutions since the proposal of NONLIN in the 70’s. Since then, the idea of associating hierarchical relational nets with partial ordered actions has prevailed when large systems were considered. However, there is still a gap between the hierarchical approach and the state of art of requirements analysis to allow features anticipated by KE approach to really appear in the requirements of a planning process. This paper proposes a pathway to solve this gap starting with requirements elicitation represented first in the conventional semi-formal (diagrammatic) language – UML – that is translated to Hierarchical Petri Nets (HPNs) by a new enhanced algorithm. The proposed process was installed in a software tool – developed by one of the authors – that analyzes the performance of the KE planning model: itSIMPLE (Integrated Tools Software Interface for Modeling Planning Environment). This tool was initially designed to use classic Place/Transition nets and an old version of UML (2.1). It is now enhanced to use UML 2.4 and a hierarchical Petri Net extension, also developed by the authors. Realistic examples illustrate the process which is now being applied to larger problems related to the manufacturing of car sequencing domain, one of challenge of ROADEF 2005 (French Operations Research & Decision Support Society). Finally, we consider the possibility to introduce another approach to the KE process by using KAOS (Keep All Object Satisfied) to make the planning design more accurate. Another important domain is called Logistics which is also proposed in ROADEF, where a topological abstraction of the real world is the key issue to define problem requirements.In Logistics, several packages must be transported by planes from their initial location to various destinations.It is part of the input knowledge a map of cities connected by airline routes.Transportation inside cities can be done by trucks located on each city.Details about the cities are abstracted and treated as a set of connected streets — some of which are temporarily blocked or permanently unavailable.Inside a city, a truck can go from any point to any destination at no cost — in a simplified mode1  (Botea et al., 2003).
The use of Knowledge Engineering (KE) processes to analyze and configure domains in automated planning is becoming more appealing since it was noticed that this issue could make a difference to solve real problems. The contrast between a generic domain independent approach, taken as canonical in AI, and alternative processes that include knowledge engineering – eventually adding specific knowledge – has been discussed by Computer and Engineering communities. A big impact has been noticed mainly in the early phase of requirement analysis when KE approach is normally introduced. Requirement analysis is responsible for carrying out the Knowledge modeling of both problem and work domains, which is a key issue to guide different planner algorithms to come out with efficient solutions. Also, there is the scalability issue that appear in most real problems. To face that, hierarchical methods played an important hole in the history of planning and inspired several solutions since the proposal of NONLIN in the 70’s. Since then, the idea of associating hierarchical relational nets with partial ordered actions has prevailed when large systems were considered. However, there is still a gap between the hierarchical approach and the state of art of requirements analysis to allow features anticipated by KE approach to really appear in the requirements of a planning process. This paper proposes a pathway to solve this gap starting with requirements elicitation represented first in the conventional semi-formal (diagrammatic) language – UML – that is translated to Hierarchical Petri Nets (HPNs) by a new enhanced algorithm. The proposed process was installed in a software tool – developed by one of the authors – that analyzes the performance of the KE planning model: itSIMPLE (Integrated Tools Software Interface for Modeling Planning Environment). This tool was initially designed to use classic Place/Transition nets and an old version of UML (2.1). It is now enhanced to use UML 2.4 and a hierarchical Petri Net extension, also developed by the authors. Realistic examples illustrate the process which is now being applied to larger problems related to the manufacturing of car sequencing domain, one of challenge of ROADEF 2005 (French Operations Research & Decision Support Society). Finally, we consider the possibility to introduce another approach to the KE process by using KAOS (Keep All Object Satisfied) to make the planning design more accurate. We will use both domains to illustrate requirement analysis using a formal procedure.In fact, this process starts by eliciting and representing requirements in UML 2.4 – a semi-formal presentation – before the formal modeling based on classic Petri Nets (Murata, 1989).This approach was embedded in a knowledge based tool called itSIMPLE (Integrated Tool and Knowledge Interface to the Modeling of Planning Environments) (Vaquero et al., 2013b, a, 2007).The new approach presented in this paper update the representation of elicited requirements to UML 2.4 (originally UML 2.1 was used) and introduces the discussion about which would be the proper set of diagrams to be used besides analyzing pors and contras of this process.Formal requirements analysis is based on unified Petri Nets, that is, on nets that follows ISO/IEC 15.909 standard, which implies in having in the same environment a classic Place/Transition, High-Level and Asymmetric net and user extensions such as the hierarchical approach presented here.A transfer language based on XML is part of the standard – PNML (Petri Net Markup Language) (Weber and Kindler, 2003) – and is used to link UML specification and domain model.
The use of Knowledge Engineering (KE) processes to analyze and configure domains in automated planning is becoming more appealing since it was noticed that this issue could make a difference to solve real problems. The contrast between a generic domain independent approach, taken as canonical in AI, and alternative processes that include knowledge engineering – eventually adding specific knowledge – has been discussed by Computer and Engineering communities. A big impact has been noticed mainly in the early phase of requirement analysis when KE approach is normally introduced. Requirement analysis is responsible for carrying out the Knowledge modeling of both problem and work domains, which is a key issue to guide different planner algorithms to come out with efficient solutions. Also, there is the scalability issue that appear in most real problems. To face that, hierarchical methods played an important hole in the history of planning and inspired several solutions since the proposal of NONLIN in the 70’s. Since then, the idea of associating hierarchical relational nets with partial ordered actions has prevailed when large systems were considered. However, there is still a gap between the hierarchical approach and the state of art of requirements analysis to allow features anticipated by KE approach to really appear in the requirements of a planning process. This paper proposes a pathway to solve this gap starting with requirements elicitation represented first in the conventional semi-formal (diagrammatic) language – UML – that is translated to Hierarchical Petri Nets (HPNs) by a new enhanced algorithm. The proposed process was installed in a software tool – developed by one of the authors – that analyzes the performance of the KE planning model: itSIMPLE (Integrated Tools Software Interface for Modeling Planning Environment). This tool was initially designed to use classic Place/Transition nets and an old version of UML (2.1). It is now enhanced to use UML 2.4 and a hierarchical Petri Net extension, also developed by the authors. Realistic examples illustrate the process which is now being applied to larger problems related to the manufacturing of car sequencing domain, one of challenge of ROADEF 2005 (French Operations Research & Decision Support Society). Finally, we consider the possibility to introduce another approach to the KE process by using KAOS (Keep All Object Satisfied) to make the planning design more accurate. In the original itSIMPLE system requirements are analyzed with hierarchical nets should be translated to a transfer language understood by planners.If the analysis is done in classic Petri Nets this transfer language is the Planning Domain Definition Language — PDDL (Kovacs, 2011; Strobel and Kirsch, 2014), but to hierarchical approach the specification should be adapted to SHOP2 Nau et al. (2013) to obtain practical results.
The environment-induced multi-phase trajectory optimization problem is studied in this paper, and the underwater target tracking task is focused on. The task is finished by an aerial-aquatic coaxial eight-rotor vehicle and is divided into two phases, i.e., the diving phase and the underwater navigation phase. The dynamic model and constraints on angular velocity of rotor in each phase are established to understand the motion characteristic. Then the model of navigation information and terrain matching are contained in the trajectory optimization model to reflect the influence of underwater navigation error on the quality of trajectory. Correspondingly, the forms of collision detection and cost function are changed to adapt to the inaccurate navigation information. To obtain the trajectory with the minimum terminal position error, an improved teach & learn-based optimization (ITLBO) algorithm is developed to strengthen the influence of individual historical optimal solution. Besides, Chebyshev collocation points are applied to determine the locations of control variables. Simulation results demonstrate that the established navigation error-based trajectory optimization model can reflect the real situation of multi-phase task. Especially, it is able to calculate the collision probability between the vehicle and the obstacle when GPS is unavailable underwater, thus ensuring the safety of underwater navigation. Compare to other common effective algorithms, the proposed ITLBO algorithm is in general more suitable for solving this problem because it is swarm-based and can obtain good solution without worrying about the inappropriate values of user-defined parameters. Trajectory optimization for aerial vehicle has been a research focus for many years because it is important to improve the autonomous flight capability.Among so many types of aircraft and tasks, trajectory optimization for unmanned aerial vehicle (UAV) and hypersonic vehicle are paid special attentions.For example, when UAVs are equipped in military, they can execute attack and reconnaissance tasks (Ropero et al., 2019).In civil use, UAVs are often assigned to search and rescue tasks (Juan et al., 2018).As for the hypersonic vehicles, they can make precise strike towards target instantly (Sushnigdha and Joshi, 2018).In each of the above situations, an optimal trajectory is expected to realize a safe and efficient flight.It is well known that a complete flight is composed of several phases, i.e., take-off, climb, cruise, descent and landing, and existing literatures mainly concentrate on generating the trajectory for one single flight phase (especially the cruise phase).The trajectory in other phases is often fixed in advance, which is far from satisfaction in a real world scenario.
The environment-induced multi-phase trajectory optimization problem is studied in this paper, and the underwater target tracking task is focused on. The task is finished by an aerial-aquatic coaxial eight-rotor vehicle and is divided into two phases, i.e., the diving phase and the underwater navigation phase. The dynamic model and constraints on angular velocity of rotor in each phase are established to understand the motion characteristic. Then the model of navigation information and terrain matching are contained in the trajectory optimization model to reflect the influence of underwater navigation error on the quality of trajectory. Correspondingly, the forms of collision detection and cost function are changed to adapt to the inaccurate navigation information. To obtain the trajectory with the minimum terminal position error, an improved teach & learn-based optimization (ITLBO) algorithm is developed to strengthen the influence of individual historical optimal solution. Besides, Chebyshev collocation points are applied to determine the locations of control variables. Simulation results demonstrate that the established navigation error-based trajectory optimization model can reflect the real situation of multi-phase task. Especially, it is able to calculate the collision probability between the vehicle and the obstacle when GPS is unavailable underwater, thus ensuring the safety of underwater navigation. Compare to other common effective algorithms, the proposed ITLBO algorithm is in general more suitable for solving this problem because it is swarm-based and can obtain good solution without worrying about the inappropriate values of user-defined parameters. In existing studies on multi-phase trajectory optimization for aerial vehicle, task-induced problems are mainly reported, and they are the issues on commercial transportation, lunar landing and space mission.In the commercial transportation, the goal is always minimizing the total fuel consumption (Hartjes et al., 2018).The whole flight is divided into three phases, i.e., climb, cruise, and descent considering standard airline procedures and air traffic control regulations (Franco and Rivas, 2015; Rivas et al., 2013).In each phase, trajectory optimization is formulated into an optimal control problem, and the switch between two consecutive phases is autonomous when certain conditions on continuous states and time are satisfied (Soler et al., 2011).Besides, the influence of actual take-off weight is analyzed, as well as the effects of wind and non-standard temperatures.In a lunar landing task, there are three phases, i.e., the braking with rough navigation (from the altitude of 18 km to 7 km), attitude hold (holding the attitude for 35 s) and braking with precise navigation (from the end of phase 2 to 100 m altitude over the landing site) (Mathavaraj et al., 2016, 2017).Different constraints are set according to the characteristic of each phase, and the trajectory segments are all obtained by the Legendre Pseudospectral method.As for the space mission, the idea is to integrate multiple phases into a single-phase trajectory optimization problem (Saranathan and Grant, 2018).To be specific, the original trajectory optimization problem is relaxed by using saturation functions to approximate the piecewise dynamics and cost functional as continuous equations (Rexius et al., 2013).The new equations can describe the motion of vehicle for all flight phases, and the intermediate boundary conditions and corresponding change in flight dynamics during the phase switching are inherently satisfied.Besides, the new continuous cost functional is also valid for all flight phases.Then the hybrid optimal control method is developed to solve the single-phase trajectory optimization problem, and the problem is divided into an outer and inner loop (Ogawa et al., 2014; Chilan and Conway, 2013).In the outer loop, the discrete states are optimized using evolutionary algorithms, and the inner loop performed optimization on the continuous dynamics using nonlinear programming.In summary of the above studies, the differences among each phase lie in dynamic equations, constraints and optimization goals, and the strategies of automatic phase switch and hierarchical planning are applied according to the characteristic of specific task.However, in some studies (Mathavaraj et al., 2016; Rexius et al., 2013; Ogawa et al., 2014), the same method is used to generate each trajectory segment (Wang et al., 2014), which fail to abstract the key elements of each flight phase and make no difference with the single-phase trajectory optimization.
The environment-induced multi-phase trajectory optimization problem is studied in this paper, and the underwater target tracking task is focused on. The task is finished by an aerial-aquatic coaxial eight-rotor vehicle and is divided into two phases, i.e., the diving phase and the underwater navigation phase. The dynamic model and constraints on angular velocity of rotor in each phase are established to understand the motion characteristic. Then the model of navigation information and terrain matching are contained in the trajectory optimization model to reflect the influence of underwater navigation error on the quality of trajectory. Correspondingly, the forms of collision detection and cost function are changed to adapt to the inaccurate navigation information. To obtain the trajectory with the minimum terminal position error, an improved teach & learn-based optimization (ITLBO) algorithm is developed to strengthen the influence of individual historical optimal solution. Besides, Chebyshev collocation points are applied to determine the locations of control variables. Simulation results demonstrate that the established navigation error-based trajectory optimization model can reflect the real situation of multi-phase task. Especially, it is able to calculate the collision probability between the vehicle and the obstacle when GPS is unavailable underwater, thus ensuring the safety of underwater navigation. Compare to other common effective algorithms, the proposed ITLBO algorithm is in general more suitable for solving this problem because it is swarm-based and can obtain good solution without worrying about the inappropriate values of user-defined parameters. During the flight, not only the flight phases are related to specific task, but also the environment plays a role.The environment-induced multi-phase trajectory optimization problems are rarely concerned and the relevant literatures are limited.The flight across blackout zone (the space from the altitude of 80 km to 45 km from the surface of the earth) and underwater navigation are typical cases.When considering the reentry trajectory optimization problem for hypersonic vehicle, the position information is provided by Global Positioning System (GPS)/Inertial Navigation System (INS), which has high precision (Li et al., 2016b).However, GPS is invalid in the blackout zone because the received radio signal is shielded, and only the INS works in the navigation system (Makarov et al., 2005), which makes the navigation error accumulated.An adaptive reentry guidance method is proposed in Wu et al. (2018), and the whole reentry guidance task is divided into two phases, i.e., the trajectory updating phase and the trajectory planning phase.In the first phase, the receding optimization procedure ensures the optimal trajectory in the next few seconds.In the trajectory planning phase, after the vehicle has flown out of the blackout zone, the optimal reentry trajectory is obtained by online planning to adapt to the navigation information.As for the seaplane, an unmanned aerial system is established to decompose a cyclic mission into the following segments: drift, acceleration, take-off, climb, cruise, descent, landing, drift (Eubank et al., 2009).In each phase, a desired trajectory can be defined along with switching strategy between phases.This system enables fully-autonomous operations, and the phase-switching logic relies on geometric and temporal rules to select the appropriate flight mode.Actually, the motion of an aerial-aquatic vehicle is also a multi-phase task, and the underwater navigation error occurs as GPS is forbidden for communication from air to underwater (and from underwater to air) because the GPS signal is easy to be detected, which makes underwater navigation unsafe (Murphy et al., 2008).
The environment-induced multi-phase trajectory optimization problem is studied in this paper, and the underwater target tracking task is focused on. The task is finished by an aerial-aquatic coaxial eight-rotor vehicle and is divided into two phases, i.e., the diving phase and the underwater navigation phase. The dynamic model and constraints on angular velocity of rotor in each phase are established to understand the motion characteristic. Then the model of navigation information and terrain matching are contained in the trajectory optimization model to reflect the influence of underwater navigation error on the quality of trajectory. Correspondingly, the forms of collision detection and cost function are changed to adapt to the inaccurate navigation information. To obtain the trajectory with the minimum terminal position error, an improved teach & learn-based optimization (ITLBO) algorithm is developed to strengthen the influence of individual historical optimal solution. Besides, Chebyshev collocation points are applied to determine the locations of control variables. Simulation results demonstrate that the established navigation error-based trajectory optimization model can reflect the real situation of multi-phase task. Especially, it is able to calculate the collision probability between the vehicle and the obstacle when GPS is unavailable underwater, thus ensuring the safety of underwater navigation. Compare to other common effective algorithms, the proposed ITLBO algorithm is in general more suitable for solving this problem because it is swarm-based and can obtain good solution without worrying about the inappropriate values of user-defined parameters. As for trajectory optimization algorithms, meta-heuristic algorithms have been paid more and more attentions in recent years due to their advantages in making a balance between the solution quality and computing speed (Karagöz and Yıldız, 2017; Yıldız and Yıldız, 2018), and they have a wide range of application in control system (Ontiveros-Robles et al., 2018; Sanchez et al., 2015), milling operations (Yildiz, 2013b) and so on.Particle swarm optimization (PSO) algorithm, genetic algorithm (GA) and ant colony optimization (ACO) algorithm are the most popular ones.Besides, there are many new algorithms emerging in the last few years, such as the chicken swarm optimization (CSO) algorithm (Li et al., 2017), farmland fertility optimization algorithm (Shayanfar and Gharehchopogh, 2018), falcon optimization algorithm (de Vasconcelos Segundo et al., 2019), sailfish optimizer (Shadravan et al., 2019) and interactive search algorithm (Mortazavi et al., 2018).The frames of the above algorithms are similar, and the differences among them lie in the principles of updating solutions.There are random numbers in those algorithms to make the solutions diversity, and some algorithm parameters must be determined to result in best performance.Current studies on meta-heuristic algorithms are conducted mainly from three aspects, i.e., application of standard algorithms (Hu et al., 2019), improvements of one specific algorithm (Yildiz, 2013a) and combination of several algorithms (Wang and Li, 2017).
The environment-induced multi-phase trajectory optimization problem is studied in this paper, and the underwater target tracking task is focused on. The task is finished by an aerial-aquatic coaxial eight-rotor vehicle and is divided into two phases, i.e., the diving phase and the underwater navigation phase. The dynamic model and constraints on angular velocity of rotor in each phase are established to understand the motion characteristic. Then the model of navigation information and terrain matching are contained in the trajectory optimization model to reflect the influence of underwater navigation error on the quality of trajectory. Correspondingly, the forms of collision detection and cost function are changed to adapt to the inaccurate navigation information. To obtain the trajectory with the minimum terminal position error, an improved teach & learn-based optimization (ITLBO) algorithm is developed to strengthen the influence of individual historical optimal solution. Besides, Chebyshev collocation points are applied to determine the locations of control variables. Simulation results demonstrate that the established navigation error-based trajectory optimization model can reflect the real situation of multi-phase task. Especially, it is able to calculate the collision probability between the vehicle and the obstacle when GPS is unavailable underwater, thus ensuring the safety of underwater navigation. Compare to other common effective algorithms, the proposed ITLBO algorithm is in general more suitable for solving this problem because it is swarm-based and can obtain good solution without worrying about the inappropriate values of user-defined parameters. In this paper, the multi-phase trajectory optimization problem in different media is studied considering the influence of navigation error.This topic is interesting and significant, and as far as the authors know, there is no public literature concerning this issue.A coaxial eight-rotor vehicle is selected to execute the task (Drews et al., 2014; Peng et al., 2015) because compared to the fixed wing aircraft, there is no need for the coaxial eight-rotor vehicle to change the configuration when dividing into water (Wu et al., 2019; Wu, 2019).Seamless transition between air and underwater can be realized by adjusting the angular velocity of each rotor (Maia et al., 2015).The aim is to realize a safe and efficient navigation both in air and underwater especially when GPS is forbidden underwater.New trajectory optimization model and algorithm are developed to avoid collision between the vehicle and obstacle and achieve an optimal trajectory to the destination.
The environment-induced multi-phase trajectory optimization problem is studied in this paper, and the underwater target tracking task is focused on. The task is finished by an aerial-aquatic coaxial eight-rotor vehicle and is divided into two phases, i.e., the diving phase and the underwater navigation phase. The dynamic model and constraints on angular velocity of rotor in each phase are established to understand the motion characteristic. Then the model of navigation information and terrain matching are contained in the trajectory optimization model to reflect the influence of underwater navigation error on the quality of trajectory. Correspondingly, the forms of collision detection and cost function are changed to adapt to the inaccurate navigation information. To obtain the trajectory with the minimum terminal position error, an improved teach & learn-based optimization (ITLBO) algorithm is developed to strengthen the influence of individual historical optimal solution. Besides, Chebyshev collocation points are applied to determine the locations of control variables. Simulation results demonstrate that the established navigation error-based trajectory optimization model can reflect the real situation of multi-phase task. Especially, it is able to calculate the collision probability between the vehicle and the obstacle when GPS is unavailable underwater, thus ensuring the safety of underwater navigation. Compare to other common effective algorithms, the proposed ITLBO algorithm is in general more suitable for solving this problem because it is swarm-based and can obtain good solution without worrying about the inappropriate values of user-defined parameters. In an underwater target tracking task, the vehicle must dive into water and then navigate to the destination.Firstly, the nonlinear dynamical model of vehicle is established, and the vehicle is ordered to dive vertically to reduce the impact of water.The constraints on angular velocity of rotor in different media are proposed to realize a smooth diving.In the underwater navigation, only the INS combined with terrain matching is used to provide the navigation information.However, INS will cause accumulated navigation error due to senor drift, and the error can be corrected by terrain matching (Jekeli and Christopher, 2001).With inaccurate state information, there may be erroneous judgments for relative position between the vehicle and underwater obstacles or between the vehicle and the destination, which will lead to collision or large derivation, thus reducing the safety level and quality of trajectory.In view of the above mentioned problems, the models of inaccurate navigation information and terrain matching are established, and current states of vehicle are denoted by an expectation and a standard deviation.Correspondingly, the representations of states lead to the new strategy of collision detection and the change in the form of cost function.Both of them are described by a new way to improve the safety level and accuracy of trajectory.To obtain the trajectory in each phase, teach & learn-based optimization (TLBO) algorithm (Rao et al., 2012) is applied, and a modified version is developed to improve its performance.TLBO is a population-based heuristic stochastic optimization algorithm, and its structure is similar to other swarm intelligence algorithms.The difference is that there are some user-defined parameters in many algorithms except for common parameters such as population size, solution dimension and maximum iterations.For example, PSO uses the inertia weight and acceleration factor, and ACO uses evaporation rate of pheromone and effectiveness factors.Those user-defined parameters need to be well set and adjusted according to the characteristic of specific problem to result in best performance, which have a great influence on the searching ability of algorithms.In TLBO algorithm, the above problems do not arise as there is no additional parameter setting artificially (Qu et al., 2017).This mechanism makes the algorithm’s performance unaffected by the inappropriate parameter setting.Due to the advantages such as simple concept, without algorithm-specific parameters and effectiveness, TLBO has been widely applied in civil engineering, control engineering and so on (Zou et al., 2019).However, the deficiency of slow convergence rate and trapping into the local optimum easily weaken its performance (Niu et al., 2018).The operator influencing its solution quality is modified to improve the performance in this study.The main contributions of this paper are summarized as follows:
The environment-induced multi-phase trajectory optimization problem is studied in this paper, and the underwater target tracking task is focused on. The task is finished by an aerial-aquatic coaxial eight-rotor vehicle and is divided into two phases, i.e., the diving phase and the underwater navigation phase. The dynamic model and constraints on angular velocity of rotor in each phase are established to understand the motion characteristic. Then the model of navigation information and terrain matching are contained in the trajectory optimization model to reflect the influence of underwater navigation error on the quality of trajectory. Correspondingly, the forms of collision detection and cost function are changed to adapt to the inaccurate navigation information. To obtain the trajectory with the minimum terminal position error, an improved teach & learn-based optimization (ITLBO) algorithm is developed to strengthen the influence of individual historical optimal solution. Besides, Chebyshev collocation points are applied to determine the locations of control variables. Simulation results demonstrate that the established navigation error-based trajectory optimization model can reflect the real situation of multi-phase task. Especially, it is able to calculate the collision probability between the vehicle and the obstacle when GPS is unavailable underwater, thus ensuring the safety of underwater navigation. Compare to other common effective algorithms, the proposed ITLBO algorithm is in general more suitable for solving this problem because it is swarm-based and can obtain good solution without worrying about the inappropriate values of user-defined parameters. 1.A multi-phase trajectory optimization model is developed.The underwater target tracking task is described and is divided into two phases.The nonlinear dynamical model of vehicle is established, and based on which, the control variables in different phases are defined to reduce the number of inputs.The constraints on the control variables in air and water are formulated to reflect different requirements.
The environment-induced multi-phase trajectory optimization problem is studied in this paper, and the underwater target tracking task is focused on. The task is finished by an aerial-aquatic coaxial eight-rotor vehicle and is divided into two phases, i.e., the diving phase and the underwater navigation phase. The dynamic model and constraints on angular velocity of rotor in each phase are established to understand the motion characteristic. Then the model of navigation information and terrain matching are contained in the trajectory optimization model to reflect the influence of underwater navigation error on the quality of trajectory. Correspondingly, the forms of collision detection and cost function are changed to adapt to the inaccurate navigation information. To obtain the trajectory with the minimum terminal position error, an improved teach & learn-based optimization (ITLBO) algorithm is developed to strengthen the influence of individual historical optimal solution. Besides, Chebyshev collocation points are applied to determine the locations of control variables. Simulation results demonstrate that the established navigation error-based trajectory optimization model can reflect the real situation of multi-phase task. Especially, it is able to calculate the collision probability between the vehicle and the obstacle when GPS is unavailable underwater, thus ensuring the safety of underwater navigation. Compare to other common effective algorithms, the proposed ITLBO algorithm is in general more suitable for solving this problem because it is swarm-based and can obtain good solution without worrying about the inappropriate values of user-defined parameters. 2.The models caused by inaccurate navigation information are established, and those models are introduced to the trajectory optimization problem.The navigation information is denoted by random variables with an expectation and a standard deviation.The result of collision detection is indicated by collision probability, which is obtained by integrating the probability density function of navigation position in the obstacle space.The cost of trajectory is calculated by applying Monte Carlo Simulation, which gets the average value from certain number of samples.
The automatic regulation of blood glucose for Type 1 diabetes patients is the main goal of the artificial pancreas, a closed-loop system that exploits continue glucose monitoring data to define an optimal insulin therapy. One of the most successful approaches for developing the artificial pancreas is the model predictive control, which exhibits promising results on both virtual and real patients. The performance of such controller is highly dependent on the reliability of the glucose–insulin model used for prediction purpose, which is usually implemented with classic mathematical models. The main limitation of these models consists in the difficulties of modeling the physiological nonlinear dynamics typical of this system. The availability of big amount of in silico and in vivo data moved the attention to new data-driven methods which are able to easily overcome this problem. In this paper we propose Deep Glucose Forecasting, a deep learning approach for forecasting glucose levels, based on a novel, two-headed Long-Short Term Memory implementation. It takes in input the previous values obtained through continue glucose monitoring, the carbohydrate intake, the suggested insulin therapy and forecasts the interstitial glucose level of the patient. The proposed architecture has been trained on 100 virtual adult patients of the UVA/Padova simulator, and tested on both virtual and real patients. The proposed solution is able to generalize to new unseen data, outperforms classical population models and reaches performance comparable to classical personalized models when fine-tuning is exploited on real patients. Type 1 Diabetes (T1D) is a chronic metabolic disease characterized by high Blood Glucose (BG) level, known as hyperglycaemia.Hyperglycaemia can cause long-term complications including damage to blood vessels, eyes, kidneys, and nerves and it is caused by the dysfunction of pancreatic β-cells responsible for the production of insulin.This hormone regulates the BG concentration by allowing cells and tissues to absorb glucose from the bloodstream.T1D patients need exogenous insulin injections to keep the glucose concentration in the euglycemic range.Their goal is to minimize diabetes complications related to hyperglycemia and simultaneously avoid hypoglycemia, a condition that could be caused by excessive insulin administration.The automatic regulation of the BG concentration for people affected by T1D through exogenous insulin administrations (Cobelli et al., 2011; Cameron et al., 2011; Thabit and Hovorka, 2016) is the main purpose of the so-called artificial pancreas.The artificial pancreas is a closed-loop system that exploits the glucose measurements obtained via Continuous Glucose Monitor (CGM) to compute and automatically deliver the proper amount of insulin via subcutaneous insulin pump.The core of the artificial pancreas is the control algorithm that defines the optimal insulin amount to infuse.The Model Predictive Control (MPC) resulted into one the most promising approach to this problem in the last years, obtaining successful results both in silico and in vivo (Renard et al., 2016; Thabit et al., 2015; Kropff et al., 2015; Anderson et al., 2016; Bergenstal et al., 2016; Dassau et al., 2012; Pinsker et al., 2018).The MPC approach exploits a glucose–insulin model to forecast the BG values in order to compute the optimal insulin therapy.For this reason, the predictive performance of the model plays a key role in the overall control performance.Classical mathematical model used in these applications are not able to fully describe the nonlinear glucose–insulin dynamics.In order to overcome this limitation, the complexity of the model has to be increased and new effective identification techniques are required.Recently, a branch of the research was moved towards new identification techniques in order to have more effective models to be used for both the control algorithms and the safety systems.A complete review can be found in Zarkogianni et al. (2015).Data-driven approaches have been successfully applied to real-life applications (Baghban et al., 2019; Samadianfard et al., 2019; Wu and Chau, 2011; Moazenzadeh et al., 2018).Depending on the task at hand, the aim of these approaches is to learn a model directly from the data.Thanks to the availability of a huge amount of data collected during long-period trials in free-living conditions new data-driven approaches have also been studied in the artificial pancreas research field, with promising results (Toffanin et al., 2018, 2019).However, their performance are limited by the use of a fixed and simple structure of the chosen model.Data-driven approaches based on deep learning architecture have received an increasing attention in the last few years mainly because of the remarkable performance obtained in several research fields (Krizhevsky et al., 2012; Ronneberger et al., 2015).Among these approaches, recurrent neural networks represent a family of deep learning architectures which have been explicitly designed to model the evolution over time of a phenomenon.In particular, given an input composed of a sequence of observations from a signal, such as the BG level in our scenario, these models try to predict its future value or values.
The automatic regulation of blood glucose for Type 1 diabetes patients is the main goal of the artificial pancreas, a closed-loop system that exploits continue glucose monitoring data to define an optimal insulin therapy. One of the most successful approaches for developing the artificial pancreas is the model predictive control, which exhibits promising results on both virtual and real patients. The performance of such controller is highly dependent on the reliability of the glucose–insulin model used for prediction purpose, which is usually implemented with classic mathematical models. The main limitation of these models consists in the difficulties of modeling the physiological nonlinear dynamics typical of this system. The availability of big amount of in silico and in vivo data moved the attention to new data-driven methods which are able to easily overcome this problem. In this paper we propose Deep Glucose Forecasting, a deep learning approach for forecasting glucose levels, based on a novel, two-headed Long-Short Term Memory implementation. It takes in input the previous values obtained through continue glucose monitoring, the carbohydrate intake, the suggested insulin therapy and forecasts the interstitial glucose level of the patient. The proposed architecture has been trained on 100 virtual adult patients of the UVA/Padova simulator, and tested on both virtual and real patients. The proposed solution is able to generalize to new unseen data, outperforms classical population models and reaches performance comparable to classical personalized models when fine-tuning is exploited on real patients. The main goal of this work is the development of a new forecasting model able to predict the future BG of a patient subject to several possible insulin treatments in order to define his/her optimal future insulin therapy.In this perspective, we propose a deep learning architecture which is able to forecast the BG level of T1D patients.Our architecture is composed of two models, one observing the CGM measurements, insulin injections and carbohydrate intakes up to a given time t and a second model that receives as input the future insulin that will be administered to the patient and the future carbohydrates that he/she will assume.Both models are composed of stacked Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks.The output of the two models is combined and given as input to a Fully Connected (FC) layer which is used to predict the future values of the Interstitial Glucose (IG), considering a fixed prediction horizon.Training is performed in a supervised fashion on a subset of identities, separated from those that will be considered as test, in order to obtain a model which is able to generalize to new unseen data.The proposed architecture obtains state-of-the-art performance on both in silico and in vivo data, considering several prediction horizons.
Extensive 2004 excavation of Čḯxwicən (pronounced ch-WHEET-son), traditional home of the Lower Elwha Klallam Tribe in northwest Washington State, U.S.A., documented human occupation spanning the last 2700 years with fine geo-stratigraphic control and 102 radiocarbon samples. Remains of multiple plankhouses were documented. Occupation spans large-magnitude earthquakes, periods of climate change, and change in nearshore habitat. Our project began in 2012 as a case study to explore the value of human ecodynamics in explaining change and stability in human-animal relationships on the Northwest Coast through analysis of faunal and geo-archaeological records. Field sampling was explicitly designed to allow for integration of all faunal classes (birds, fish, mammals, and invertebrates), thus facilitating our ability to track how different taxa were affected by external factors and cultural processes. With over one million specimens, the faunal assemblage represents one of the largest on the North Pacific Coast. Invertebrate records reveal striking changes in intertidal habitat that are linked to the formation of the sheltered harbor and catastrophic events such as tsunamis. Analysis suggests a high level of consistency in the structure of resource use (evenness and richness) across 2150 years of occupation, despite increase in intensity of human use and a shift to plankhouse occupation. Trends in fish and invertebrate representation do not correspond to changing ocean conditions, while changes in abundance of herring, salmon, burrowing bivalves and urchins are consistent with impacts from tsunamis. Comparison of resource use between two well-sampled houses before and after one tsunami suggests that while both households were resilient, they negotiated the event in different ways. We are in the midst of a paradigm shift in our conception of human-environmental relationships and explanatory models for cultural change on the north Pacific coast of North America.At the time of European contact, relatively large concentrations of people lived in substantial plankhouses, participated in elaborate ceremonies, and were organized into ranked social units—attributes generally associated with agriculturalists.Until recently, anthropologists viewed the hunter-fisher-gatherer peoples of the Pacific Northwest as atypical foragers.Accounting for this outlier status became the focus of attention.Explanations first emphasized inherent abundance of resources such as salmon, then the spatial and temporal patchiness of resources and the importance of technological and social means of increasing productivity (Ames and Maschner, 1999; Matson and Coupland, 1995).Competition among social groups for prestige, recruitment, resource rights, and control of storable commodities, exchanged through feasting or trade, were viewed as key factors in driving changes in social organization and subsistence adaptation (Coupland, 1985; Hayden, 1995).Changes in the use of animals were routinely used to explain many of these trends, for example, increased use of salmon (Oncorhynchus spp.) or marine mammals in some locations has been used as an explanation for observed changes in artifact distributions and/or household organization (Matson, 1992; Matson and Coupland, 1995).
Extensive 2004 excavation of Čḯxwicən (pronounced ch-WHEET-son), traditional home of the Lower Elwha Klallam Tribe in northwest Washington State, U.S.A., documented human occupation spanning the last 2700 years with fine geo-stratigraphic control and 102 radiocarbon samples. Remains of multiple plankhouses were documented. Occupation spans large-magnitude earthquakes, periods of climate change, and change in nearshore habitat. Our project began in 2012 as a case study to explore the value of human ecodynamics in explaining change and stability in human-animal relationships on the Northwest Coast through analysis of faunal and geo-archaeological records. Field sampling was explicitly designed to allow for integration of all faunal classes (birds, fish, mammals, and invertebrates), thus facilitating our ability to track how different taxa were affected by external factors and cultural processes. With over one million specimens, the faunal assemblage represents one of the largest on the North Pacific Coast. Invertebrate records reveal striking changes in intertidal habitat that are linked to the formation of the sheltered harbor and catastrophic events such as tsunamis. Analysis suggests a high level of consistency in the structure of resource use (evenness and richness) across 2150 years of occupation, despite increase in intensity of human use and a shift to plankhouse occupation. Trends in fish and invertebrate representation do not correspond to changing ocean conditions, while changes in abundance of herring, salmon, burrowing bivalves and urchins are consistent with impacts from tsunamis. Comparison of resource use between two well-sampled houses before and after one tsunami suggests that while both households were resilient, they negotiated the event in different ways. Several shifts have occurred to challenge this thinking and research focus.The limits of simple models of resource intensification on resources like salmon are exposed empirically by close comparative analysis of faunal assemblages demonstrating the breadth of resources used (Butler and Campbell, 2004), and further by the great environmental diversity in the region (Moss, 2012).Increased use of fine-mesh screens has demonstrated the importance of a range of small-bodied fishes such as herring (Clupea pallasii) that had previously been overlooked (McKechnie et al., 2014; McKechnie and Moss, 2016).As well, there has been an increased appreciation for the role of plants in Northwest Coast lifeways—based on ethnohistoric records, archaeological evidence, and on-going practices (Deur and Turner, 2005 and references therein).Most striking, people were engaged in a range of activities such as transplanting plants and amending soils, which constitute gardening, blurring the line between “foragers” and “farmers” (Deur and Turner, 2005; Smith, 2005).In effect, this understanding has upended views that Northwest Coast peoples were atypical at all, because it challenges the validity of the dichotomy between hunter-gatherers and agriculturalists.Both terrestrial and nearshore landscapes were deliberately manipulated to increase habitable area (Grier, 2014) or in the case of clam gardens, to increase shellfish productivity (Groesbeck et al., 2014).As archaeologists collaborate more with Indigenous knowledge holders, they are learning that animals and plants hold a range of values beyond subsistence, further expanding our conception of human-environmental connections (Lepofsky et al., 2017; Moss et al., 2016).
Extensive 2004 excavation of Čḯxwicən (pronounced ch-WHEET-son), traditional home of the Lower Elwha Klallam Tribe in northwest Washington State, U.S.A., documented human occupation spanning the last 2700 years with fine geo-stratigraphic control and 102 radiocarbon samples. Remains of multiple plankhouses were documented. Occupation spans large-magnitude earthquakes, periods of climate change, and change in nearshore habitat. Our project began in 2012 as a case study to explore the value of human ecodynamics in explaining change and stability in human-animal relationships on the Northwest Coast through analysis of faunal and geo-archaeological records. Field sampling was explicitly designed to allow for integration of all faunal classes (birds, fish, mammals, and invertebrates), thus facilitating our ability to track how different taxa were affected by external factors and cultural processes. With over one million specimens, the faunal assemblage represents one of the largest on the North Pacific Coast. Invertebrate records reveal striking changes in intertidal habitat that are linked to the formation of the sheltered harbor and catastrophic events such as tsunamis. Analysis suggests a high level of consistency in the structure of resource use (evenness and richness) across 2150 years of occupation, despite increase in intensity of human use and a shift to plankhouse occupation. Trends in fish and invertebrate representation do not correspond to changing ocean conditions, while changes in abundance of herring, salmon, burrowing bivalves and urchins are consistent with impacts from tsunamis. Comparison of resource use between two well-sampled houses before and after one tsunami suggests that while both households were resilient, they negotiated the event in different ways. To a large extent, scholars are shifting from seeking region-wide, unifying economic or environmental explanations for cultural changes or complexity, what Moss (2012) has termed ‘master narratives’—to a greater emphasis on understanding the role of local historical process and human agency in accounting for cultural changes (e.g., Hopt and Grier, 2018).Economic and environmental constraints are still relevant and require attention (e.g., Hutchinson et al., 2018, in this issue; Prentiss et al., 2018), but how these manifest in local settings must be considered.
Extensive 2004 excavation of Čḯxwicən (pronounced ch-WHEET-son), traditional home of the Lower Elwha Klallam Tribe in northwest Washington State, U.S.A., documented human occupation spanning the last 2700 years with fine geo-stratigraphic control and 102 radiocarbon samples. Remains of multiple plankhouses were documented. Occupation spans large-magnitude earthquakes, periods of climate change, and change in nearshore habitat. Our project began in 2012 as a case study to explore the value of human ecodynamics in explaining change and stability in human-animal relationships on the Northwest Coast through analysis of faunal and geo-archaeological records. Field sampling was explicitly designed to allow for integration of all faunal classes (birds, fish, mammals, and invertebrates), thus facilitating our ability to track how different taxa were affected by external factors and cultural processes. With over one million specimens, the faunal assemblage represents one of the largest on the North Pacific Coast. Invertebrate records reveal striking changes in intertidal habitat that are linked to the formation of the sheltered harbor and catastrophic events such as tsunamis. Analysis suggests a high level of consistency in the structure of resource use (evenness and richness) across 2150 years of occupation, despite increase in intensity of human use and a shift to plankhouse occupation. Trends in fish and invertebrate representation do not correspond to changing ocean conditions, while changes in abundance of herring, salmon, burrowing bivalves and urchins are consistent with impacts from tsunamis. Comparison of resource use between two well-sampled houses before and after one tsunami suggests that while both households were resilient, they negotiated the event in different ways. The developing scholarly area known as Human Ecodynamics (H.E.) captures many of the ideas and methods that are part of this paradigm shift.Such scholarship joins concepts from historical ecology, resilience theory, human behavioral ecology, and Indigenous archaeology to explore the complex and dynamic relationships between physical (e.g., climate change, natural hazards), ecological (e.g., nutrient cycling, predator-prey relations), and social (e.g., economic, technological, organizational, ideological) processes (e.g., Fitzhugh et al., in this issue; Kirch, 2007).The need to consider historical contingency and human agency is integral to H.E. research.
Extensive 2004 excavation of Čḯxwicən (pronounced ch-WHEET-son), traditional home of the Lower Elwha Klallam Tribe in northwest Washington State, U.S.A., documented human occupation spanning the last 2700 years with fine geo-stratigraphic control and 102 radiocarbon samples. Remains of multiple plankhouses were documented. Occupation spans large-magnitude earthquakes, periods of climate change, and change in nearshore habitat. Our project began in 2012 as a case study to explore the value of human ecodynamics in explaining change and stability in human-animal relationships on the Northwest Coast through analysis of faunal and geo-archaeological records. Field sampling was explicitly designed to allow for integration of all faunal classes (birds, fish, mammals, and invertebrates), thus facilitating our ability to track how different taxa were affected by external factors and cultural processes. With over one million specimens, the faunal assemblage represents one of the largest on the North Pacific Coast. Invertebrate records reveal striking changes in intertidal habitat that are linked to the formation of the sheltered harbor and catastrophic events such as tsunamis. Analysis suggests a high level of consistency in the structure of resource use (evenness and richness) across 2150 years of occupation, despite increase in intensity of human use and a shift to plankhouse occupation. Trends in fish and invertebrate representation do not correspond to changing ocean conditions, while changes in abundance of herring, salmon, burrowing bivalves and urchins are consistent with impacts from tsunamis. Comparison of resource use between two well-sampled houses before and after one tsunami suggests that while both households were resilient, they negotiated the event in different ways. The Northwest Coast is an appropriate place to apply the H.E. framework.The western edge of the North American continent, located on a plate boundary, is tectonically and volcanically active and has been subject to glaciation and sea level change, as well as dynamic patterns of ocean circulation.Cascade Subduction Zone (CSZ) earthquakes (of magnitude 8.0 and greater), and shifts in ocean productivity (Hutchinson et al., 2018, in this issue; Monks, 2017a) are of particular interest for our project.The region is characterized by multi-house villages with long-duration households, offering the opportunity to study social changes in the context of a dynamic environment.
Extensive 2004 excavation of Čḯxwicən (pronounced ch-WHEET-son), traditional home of the Lower Elwha Klallam Tribe in northwest Washington State, U.S.A., documented human occupation spanning the last 2700 years with fine geo-stratigraphic control and 102 radiocarbon samples. Remains of multiple plankhouses were documented. Occupation spans large-magnitude earthquakes, periods of climate change, and change in nearshore habitat. Our project began in 2012 as a case study to explore the value of human ecodynamics in explaining change and stability in human-animal relationships on the Northwest Coast through analysis of faunal and geo-archaeological records. Field sampling was explicitly designed to allow for integration of all faunal classes (birds, fish, mammals, and invertebrates), thus facilitating our ability to track how different taxa were affected by external factors and cultural processes. With over one million specimens, the faunal assemblage represents one of the largest on the North Pacific Coast. Invertebrate records reveal striking changes in intertidal habitat that are linked to the formation of the sheltered harbor and catastrophic events such as tsunamis. Analysis suggests a high level of consistency in the structure of resource use (evenness and richness) across 2150 years of occupation, despite increase in intensity of human use and a shift to plankhouse occupation. Trends in fish and invertebrate representation do not correspond to changing ocean conditions, while changes in abundance of herring, salmon, burrowing bivalves and urchins are consistent with impacts from tsunamis. Comparison of resource use between two well-sampled houses before and after one tsunami suggests that while both households were resilient, they negotiated the event in different ways. Our research project began in 2012 as a case study to explore the value of H.E. in explaining change and stability in animal-human relationships on the Northwest Coast at the site of Čḯxwicən1 (pronounced ch-WHEET-son).The site is a traditional village of the Lower Elwha Klallam Tribe (LEKT) and is located on the southern shore of the Strait of Juan de Fuca (SJDF), on the northwest coast of Washington State (U.S.A.) (Fig. 1).Large-scale excavations in 2004 yielded enormous faunal samples associated with plankhouses, extramural activity areas, and midden deposits.Detailed geoarchaeological field recording of deposits and 102 radiocarbon ages have produced a high-resolution view of changes in the socio-ecological system over the past 2700 years (Campbell et al., 2018b).In situ records suggest the site was overtopped by multiple tsunamis.Thus, the site and faunal record provided an opportunity to study ways Indigenous people altered resource use in response to external drivers of change; and how social forces mediated animal use in the context of potentially catastrophic events.
Extensive 2004 excavation of Čḯxwicən (pronounced ch-WHEET-son), traditional home of the Lower Elwha Klallam Tribe in northwest Washington State, U.S.A., documented human occupation spanning the last 2700 years with fine geo-stratigraphic control and 102 radiocarbon samples. Remains of multiple plankhouses were documented. Occupation spans large-magnitude earthquakes, periods of climate change, and change in nearshore habitat. Our project began in 2012 as a case study to explore the value of human ecodynamics in explaining change and stability in human-animal relationships on the Northwest Coast through analysis of faunal and geo-archaeological records. Field sampling was explicitly designed to allow for integration of all faunal classes (birds, fish, mammals, and invertebrates), thus facilitating our ability to track how different taxa were affected by external factors and cultural processes. With over one million specimens, the faunal assemblage represents one of the largest on the North Pacific Coast. Invertebrate records reveal striking changes in intertidal habitat that are linked to the formation of the sheltered harbor and catastrophic events such as tsunamis. Analysis suggests a high level of consistency in the structure of resource use (evenness and richness) across 2150 years of occupation, despite increase in intensity of human use and a shift to plankhouse occupation. Trends in fish and invertebrate representation do not correspond to changing ocean conditions, while changes in abundance of herring, salmon, burrowing bivalves and urchins are consistent with impacts from tsunamis. Comparison of resource use between two well-sampled houses before and after one tsunami suggests that while both households were resilient, they negotiated the event in different ways. Our project focused on faunal remains for their potential to contribute to understanding resilience in human adaptive strategies in the face of a range of environmental and social changes, focusing on the past 2150 years of occupation.Faunal remains are more directly linked to resource use areas and environments than many other artifact types, and have been central to explanations for cultural change in our region.Field sampling was explicitly designed to allow for integration of all faunal classes (birds, fish, mammals, and invertebrates), thus addressing a common limitation with Northwest Coast faunal sampling, where remains of different faunal classes are retained and studied from different site matrix and volumes (Butler and Campbell, 2004).
Extensive 2004 excavation of Čḯxwicən (pronounced ch-WHEET-son), traditional home of the Lower Elwha Klallam Tribe in northwest Washington State, U.S.A., documented human occupation spanning the last 2700 years with fine geo-stratigraphic control and 102 radiocarbon samples. Remains of multiple plankhouses were documented. Occupation spans large-magnitude earthquakes, periods of climate change, and change in nearshore habitat. Our project began in 2012 as a case study to explore the value of human ecodynamics in explaining change and stability in human-animal relationships on the Northwest Coast through analysis of faunal and geo-archaeological records. Field sampling was explicitly designed to allow for integration of all faunal classes (birds, fish, mammals, and invertebrates), thus facilitating our ability to track how different taxa were affected by external factors and cultural processes. With over one million specimens, the faunal assemblage represents one of the largest on the North Pacific Coast. Invertebrate records reveal striking changes in intertidal habitat that are linked to the formation of the sheltered harbor and catastrophic events such as tsunamis. Analysis suggests a high level of consistency in the structure of resource use (evenness and richness) across 2150 years of occupation, despite increase in intensity of human use and a shift to plankhouse occupation. Trends in fish and invertebrate representation do not correspond to changing ocean conditions, while changes in abundance of herring, salmon, burrowing bivalves and urchins are consistent with impacts from tsunamis. Comparison of resource use between two well-sampled houses before and after one tsunami suggests that while both households were resilient, they negotiated the event in different ways. Several central questions that drove the larger research project are the subject of this paper:Are there changes in the faunal record that indicate local landform evolution?Are there changes in resource use that correspond to the transition to plankhouse occupation?Are there changes in animal representation that correspond to changes in ocean conditions?To what extent did earthquakes affect nearshore habitats, animal populations, or cultural procurement?Did the degree of communalism in animal use vary across households?If so, how resilient was this social structure in response to an earthquake?
Archaeologists have paid substantial attention to the social transformations coinciding with the widespread adoption of bow and arrow technologies. Social network analysis (SNA) is used to examine stone tool assemblages from the Salish Sea. SNA while widely applied a wide range of problems in lithic technologies has been an underutilized approach in the Pacific Northwest. Based on an application of cultural transmission theory, ethnography, and Coast Salish ontology, that haft styles reflect corporate group connections. Changes in the social networks are examined as reflected in haft styles from 3500 to 1000 BP, a time of shifts towards large plank house villages and the emergence of hereditary forms of social inequality in the region. Five social networks were constructed, each covering a 500-year period, to assess shifts in regional connections through time. There appears to be increased elaboration of social networks throughout the Salish Sea until 1600 BP, when the bow and arrow become widely adopted. These data suggest SNA of lithic haft styles shows a shift in hunting organization from a collective corporate group level activity to an individualized pursuit. The findings show the utility of SNA to address oscillations in Salish Sea society over time. New directions for future studies to examine shifts in corporate group relations in other aspects of precontact Coast Salish society are also provided. Social network analysis (SNA) has been applied to examine social transformations in non-state societies (Borck et al., 2015; Brughmans, 2010; papers in Knappett, 2013; Mills et al., 2015; Mills, 2017) such as demographic collapse (Birch and Hart, 2018; Knappett, 2011).Many of these studies have emphasized ceramic styles, as they are argued to reflect active social symbols, while others have integrated ceramic sourcing approaches to their studies (Gjesfjeld, 2014, 2015).In terms of lithic studies, analyses of lithic material sources have been employed by archaeologists in varied settings to assess the relationships between assemblages (Buchanan et al., 2016; Golitko et al., 2012; Golitko and Feinman, 2015; Mills et al., 2013; Phillips, 2011).Linking SNA with a geographic information system (GIS) has enabled archaeologists to engage with data in new ways using multiple spatial and temporal scales to understand social connections.
Archaeologists have paid substantial attention to the social transformations coinciding with the widespread adoption of bow and arrow technologies. Social network analysis (SNA) is used to examine stone tool assemblages from the Salish Sea. SNA while widely applied a wide range of problems in lithic technologies has been an underutilized approach in the Pacific Northwest. Based on an application of cultural transmission theory, ethnography, and Coast Salish ontology, that haft styles reflect corporate group connections. Changes in the social networks are examined as reflected in haft styles from 3500 to 1000 BP, a time of shifts towards large plank house villages and the emergence of hereditary forms of social inequality in the region. Five social networks were constructed, each covering a 500-year period, to assess shifts in regional connections through time. There appears to be increased elaboration of social networks throughout the Salish Sea until 1600 BP, when the bow and arrow become widely adopted. These data suggest SNA of lithic haft styles shows a shift in hunting organization from a collective corporate group level activity to an individualized pursuit. The findings show the utility of SNA to address oscillations in Salish Sea society over time. New directions for future studies to examine shifts in corporate group relations in other aspects of precontact Coast Salish society are also provided. Here, shifts in cultural transmission in hafted lithic tool styles are examined, during a time when substantial social transformations are argued to have occurred on the Salish Sea of the Pacific Northwest Coast of North America.The period from 3200 to 1000 cal BP is marked by transitions in residences towards large plank house villages (Lepofsky et al., 2009), hereditary forms of social inequality and shifts in the ways that people actively managed the landscape for food production (Bilton, 2014; Butler and Campbell, 2004; Campbell and Butler, 2010; Grier et al., 2017; Hopt and Grier, 2018; Martindale and Letham, 2011; McKechnie, 2014; Moss, 2011).
Archaeologists have paid substantial attention to the social transformations coinciding with the widespread adoption of bow and arrow technologies. Social network analysis (SNA) is used to examine stone tool assemblages from the Salish Sea. SNA while widely applied a wide range of problems in lithic technologies has been an underutilized approach in the Pacific Northwest. Based on an application of cultural transmission theory, ethnography, and Coast Salish ontology, that haft styles reflect corporate group connections. Changes in the social networks are examined as reflected in haft styles from 3500 to 1000 BP, a time of shifts towards large plank house villages and the emergence of hereditary forms of social inequality in the region. Five social networks were constructed, each covering a 500-year period, to assess shifts in regional connections through time. There appears to be increased elaboration of social networks throughout the Salish Sea until 1600 BP, when the bow and arrow become widely adopted. These data suggest SNA of lithic haft styles shows a shift in hunting organization from a collective corporate group level activity to an individualized pursuit. The findings show the utility of SNA to address oscillations in Salish Sea society over time. New directions for future studies to examine shifts in corporate group relations in other aspects of precontact Coast Salish society are also provided. Informed by ethnography (e.g. Duff, 1952; Elmendorf, 1992; Haeberlin and Gunther, 1930; Smith, 1940; Suttles, 1958, 1960) and archaeological studies, current theorizing has argued for the emergence of an expansive network of hereditary elites in Marpole-age (2400–1000 cal BP) sites (Angelbeck and Grier, 2012; Grier, 2003; Rorabaugh and Shantry, 2017; Schaepe, 2009) and resiliency to political centralization (Angelbeck, 2009; Angelbeck and Grier, 2012).Instead of focusing on elites in the Salish Sea during this critical transition, I explore shifts in social networks (e.g., Jordan and O'Neill, 2010).Specifically, this study focuses on corporate group ties traced through the cultural transmission of lithic manufacture.One fundamental question is, “What structure do social networks have in the Salish Sea?”This question is intricate in its complexities, particularly in light of arguments emphasizing regional connections (Grier, 2003; Rorabaugh and Shantry, 2017), subregional variability (Clark, 2010), and political autonomy (Angelbeck, 2009).A cultural transmission framework is combined with Coast Salish ontology to develop expectations for these patterns in the archaeological data.
Many Reinforcement Learning (RL) real-world applications have multi-dimensional action spaces which suffer from the combinatorial explosion of complexity. Then, it may turn infeasible to implement Centralized RL (CRL) systems due to the exponential increasing of dimensionality in both the state space and the action space, and the large number of training trials. In order to address this, this paper proposes to deal with these issues by using Decentralized Reinforcement Learning (DRL) to alleviate the effects of the curse of dimensionality on the action space, and by transferring knowledge to reduce the training episodes so that asymptotic converge can be achieved. Three DRL schemes are compared: DRL with independent learners and no prior-coordination (DRL-Ind); DRL accelerated-coordinated by using the Control Sharing (DRL+CoSh) Knowledge Transfer approach; and a proposed DRL scheme using the CoSh-based variant Nearby Action Sharing to include a measure of the uncertainty into the CoSh procedure (DRL+NeASh). These three schemes are analyzed through an extensive experimental study and validated through two complex real-world problems, namely the inwalk-kicking and the ball-dribbling behaviors, both performed with humanoid biped robots. Obtained results show (empirically): (i) the effectiveness of DRL systems which even without prior-coordination are able to achieve asymptotic convergence throughout indirect coordination; (ii) that by using the proposed knowledge transfer methods, it is possible to reduce the training episodes and to coordinate the DRL process; and (iii) obtained learning times are between 36% and 62% faster than the DRL-Ind schemes in the case studies. Reinforcement Learning (RL) is increasingly being used to learn complex behaviors in robotics.Two of the main challenges to be solved for modeling RL systems acting in the real-world are: (i) the high dimensionality of the state and action spaces, and (ii) the large number of training trials required to learn most of complex behaviors.Many real-world applications have multi-dimensional action spaces (e.g., multiple actuators or effectors).In those cases, RL suffers from the combinatorial explosion of complexity which occurs when a single or centralized RL (CRL) scheme is used.It may turn infeasible to implement CRL systems in terms of computational resources or learning time due to the exponential increasing of dimensionality in both the state space and the action space as in Martín and de Lope Asiaín (2007) and Leottau et al. (2018).Instead, the use of Decentralized Reinforcement Learning (DRL) helps to alleviate this problem as it has been empirically evidenced by Buşoniu et al. (2006) and Leottau et al. (2017, 2018).In DRL, a problem is decomposed into several sub-problems, whose resources are managed separately while working toward a common goal, i.e. learning and performing a behavior.In the case of multidimensional action spaces, a sub-problem corresponds to control one particular variable.For instance, in mobile robotics, a common high-level motion command is the requested velocity vector (e.g., [vx,vy,vθ] for an omni-directional robot).Then, if each speed component of this vector is handled individually, a distributed control scheme can be applied.
Many Reinforcement Learning (RL) real-world applications have multi-dimensional action spaces which suffer from the combinatorial explosion of complexity. Then, it may turn infeasible to implement Centralized RL (CRL) systems due to the exponential increasing of dimensionality in both the state space and the action space, and the large number of training trials. In order to address this, this paper proposes to deal with these issues by using Decentralized Reinforcement Learning (DRL) to alleviate the effects of the curse of dimensionality on the action space, and by transferring knowledge to reduce the training episodes so that asymptotic converge can be achieved. Three DRL schemes are compared: DRL with independent learners and no prior-coordination (DRL-Ind); DRL accelerated-coordinated by using the Control Sharing (DRL+CoSh) Knowledge Transfer approach; and a proposed DRL scheme using the CoSh-based variant Nearby Action Sharing to include a measure of the uncertainty into the CoSh procedure (DRL+NeASh). These three schemes are analyzed through an extensive experimental study and validated through two complex real-world problems, namely the inwalk-kicking and the ball-dribbling behaviors, both performed with humanoid biped robots. Obtained results show (empirically): (i) the effectiveness of DRL systems which even without prior-coordination are able to achieve asymptotic convergence throughout indirect coordination; (ii) that by using the proposed knowledge transfer methods, it is possible to reduce the training episodes and to coordinate the DRL process; and (iii) obtained learning times are between 36% and 62% faster than the DRL-Ind schemes in the case studies. Most of the stochastic DRL systems implemented with independent learners also present two main drawbacks: non-stationary and non-Markovian issues.Laurent et al. (2011) indicate that these drawbacks could be mitigated by: (i) decaying the exploration rate, and (ii) using coordinated exploration techniques for shrinking the action space.Both mechanisms can be accomplished by using Knowledge Transfer (KT) approaches, as Knox and Stone (2010) and Bianchi et al. (2014) reported.KT has evidenced to be able to accelerate the training of Multi-Agent RL (MARL) problems (Vrancx et al., 2011; Boutsioukis et al., 2012; Taylor et al., 2013; Bianchi et al., 2014; Hu et al., 2015), alleviating the second aforementioned challenge: the large number of training trials.KT allows exploring in a subset of the action space limited by a source of knowledge (SoK), which has been previously learned or designed.A SoK for a DRL system should contain at least one branch per decentralized learning agent.If those branches are pre-coordinated, the SoK relieves at the same time the coordination problem, which must be solved in order to extend and take advantage of some potential benefits of Multi-Agent Systems (MAS) to DRL systems.
Many Reinforcement Learning (RL) real-world applications have multi-dimensional action spaces which suffer from the combinatorial explosion of complexity. Then, it may turn infeasible to implement Centralized RL (CRL) systems due to the exponential increasing of dimensionality in both the state space and the action space, and the large number of training trials. In order to address this, this paper proposes to deal with these issues by using Decentralized Reinforcement Learning (DRL) to alleviate the effects of the curse of dimensionality on the action space, and by transferring knowledge to reduce the training episodes so that asymptotic converge can be achieved. Three DRL schemes are compared: DRL with independent learners and no prior-coordination (DRL-Ind); DRL accelerated-coordinated by using the Control Sharing (DRL+CoSh) Knowledge Transfer approach; and a proposed DRL scheme using the CoSh-based variant Nearby Action Sharing to include a measure of the uncertainty into the CoSh procedure (DRL+NeASh). These three schemes are analyzed through an extensive experimental study and validated through two complex real-world problems, namely the inwalk-kicking and the ball-dribbling behaviors, both performed with humanoid biped robots. Obtained results show (empirically): (i) the effectiveness of DRL systems which even without prior-coordination are able to achieve asymptotic convergence throughout indirect coordination; (ii) that by using the proposed knowledge transfer methods, it is possible to reduce the training episodes and to coordinate the DRL process; and (iii) obtained learning times are between 36% and 62% faster than the DRL-Ind schemes in the case studies. In this work, we tackle the high dimensionality of the state and action spaces, and the large number of training trials by using two mechanisms: (i) DRL to alleviate the effects of the curse of dimensionality on the action space, and (ii) KT to reduce the training episodes so that asymptotic convergence can be achieved.The SARSA(λ) with radial basis functions (RBFs) is used as basis algorithm to implement a DRL system with no prior-coordination among agents.This is accelerated-coordinated by using a KT approach called Control Sharing (CoSh) introduced by Knox and Stone (2012), which is extended from the single-agent case to the DRL case.In addition, we introduce a CoSh-based variant called Nearby Action Sharing (NeASh), which is able to include a measure of uncertainty in the action sharing process.
Many Reinforcement Learning (RL) real-world applications have multi-dimensional action spaces which suffer from the combinatorial explosion of complexity. Then, it may turn infeasible to implement Centralized RL (CRL) systems due to the exponential increasing of dimensionality in both the state space and the action space, and the large number of training trials. In order to address this, this paper proposes to deal with these issues by using Decentralized Reinforcement Learning (DRL) to alleviate the effects of the curse of dimensionality on the action space, and by transferring knowledge to reduce the training episodes so that asymptotic converge can be achieved. Three DRL schemes are compared: DRL with independent learners and no prior-coordination (DRL-Ind); DRL accelerated-coordinated by using the Control Sharing (DRL+CoSh) Knowledge Transfer approach; and a proposed DRL scheme using the CoSh-based variant Nearby Action Sharing to include a measure of the uncertainty into the CoSh procedure (DRL+NeASh). These three schemes are analyzed through an extensive experimental study and validated through two complex real-world problems, namely the inwalk-kicking and the ball-dribbling behaviors, both performed with humanoid biped robots. Obtained results show (empirically): (i) the effectiveness of DRL systems which even without prior-coordination are able to achieve asymptotic convergence throughout indirect coordination; (ii) that by using the proposed knowledge transfer methods, it is possible to reduce the training episodes and to coordinate the DRL process; and (iii) obtained learning times are between 36% and 62% faster than the DRL-Ind schemes in the case studies. Since most of the MARL reported studies do not address or validate their proposed approaches with multi-state, stochastic, and real-world problems (Buşoniu et al., 2008), our preliminary goal is to show (empirically) that the benefits of MAS are also applicable to complex problems by using a DRL scheme.For such purpose, two challenging real-world problems for soccer robotics are modeled and implemented: the inwalk-kicking and the ball-dribbling behaviors, both performed by using an omni-directional biped robot.In the inwalk-kicking behavior, the robot must learn to push the ball toward a desired target only by using the inertia of its own gait (Lobos-Tsunekawa et al., 2017).In the ball-dribbling problem, the robot must learn to maneuver the ball in a very controlled way while moving towards a desired target (Leottau et al., 2015).In the case of biped robots, the complexity of these tasks is very high, as in each case the controller must take into account the physical interaction among the ball, the robot’s feet, the ground, and the robot’s gait inertia.Thus, the action is highly dynamic, non-linear, and influenced by several sources of uncertainty.
Many Reinforcement Learning (RL) real-world applications have multi-dimensional action spaces which suffer from the combinatorial explosion of complexity. Then, it may turn infeasible to implement Centralized RL (CRL) systems due to the exponential increasing of dimensionality in both the state space and the action space, and the large number of training trials. In order to address this, this paper proposes to deal with these issues by using Decentralized Reinforcement Learning (DRL) to alleviate the effects of the curse of dimensionality on the action space, and by transferring knowledge to reduce the training episodes so that asymptotic converge can be achieved. Three DRL schemes are compared: DRL with independent learners and no prior-coordination (DRL-Ind); DRL accelerated-coordinated by using the Control Sharing (DRL+CoSh) Knowledge Transfer approach; and a proposed DRL scheme using the CoSh-based variant Nearby Action Sharing to include a measure of the uncertainty into the CoSh procedure (DRL+NeASh). These three schemes are analyzed through an extensive experimental study and validated through two complex real-world problems, namely the inwalk-kicking and the ball-dribbling behaviors, both performed with humanoid biped robots. Obtained results show (empirically): (i) the effectiveness of DRL systems which even without prior-coordination are able to achieve asymptotic convergence throughout indirect coordination; (ii) that by using the proposed knowledge transfer methods, it is possible to reduce the training episodes and to coordinate the DRL process; and (iii) obtained learning times are between 36% and 62% faster than the DRL-Ind schemes in the case studies. Three DRL schemes are analyzed and compared for both test problems, the DRL with independent agents and no prior-coordination (DRL-Ind), the DRL accelerated with CoSh (DRL+CoSh), and the DRL accelerated with NeASh (DRL+NeASh).As it will be shown, DRL+CoSh and DRL+NeASh are able to transfer knowledge and accelerate the DRL-Ind.These three schemes are analyzed through an extensive empirical study carried out in a 3D realistic simulator and demonstrated with physical robots.It is worth mentioning that, to the best of our knowledge, we are the first applying an effective strategy for transferring knowledge and coordinating-accelerating DRL systems.
This publication aims to shed light on the influence that prior heating (burning) of mollusc shells during human activity may have on the results of radiocarbon dating. We compare the geochemical and mineralogical composition of heated and unheated shells of Anadara uropigimelana and Terebralia palustris recovered from Neolithic and Bronze Age archaeological contexts at Kalba, Sharjah Emirate, in the United Arab Emirates (UAE). Our research examined whether the heating of shells impacts on the determination of reservoir effects, or whether in spite of heating, this material remains a viable material for precise 14C measurements. Our results show that both heated and non-heated shells of A. uropigimelana and T. palustris provide consistent results, although the mineral composition of the shells changes from aragonite to calcite. Our results are important, since some of our selection of shells did not initially appear to have been heated. A heating process will then usually be detected as a greyish, marble like structure when cutting the shells. As a result of this work, we have also developed insights into prehistoric cooking practices of shells collected in Arabia. Our results provide archaeologists and associated researchers with confidence when assessing the results of radiocarbon dating during their studies of shells that might have been heated. Shells provide valuable sources of information within archaeology, whether for establishing chronologies for archaeological sites by means of radiocarbon dating and AAR (Kosnik et al., 2017) or through palaeoclimate and palaeoenvironmental studies (Biagi, 1994; Lindauer et al., 2017; Zazzo et al., 2016).This is especially so in arid environments where other organic matter such as peat or humic silt and clay are poorly preserved.Given the abundance of burnt shells in archaeological deposits (Barker et al., 2010; Stringer et al., 2008; Taylor et al., 2011), it is fundamental to understand the effect of heating on the geochemical composition of the shelly material, which in turn, may have consequences for the determination of reservoir effects during radiocarbon dating.
This publication aims to shed light on the influence that prior heating (burning) of mollusc shells during human activity may have on the results of radiocarbon dating. We compare the geochemical and mineralogical composition of heated and unheated shells of Anadara uropigimelana and Terebralia palustris recovered from Neolithic and Bronze Age archaeological contexts at Kalba, Sharjah Emirate, in the United Arab Emirates (UAE). Our research examined whether the heating of shells impacts on the determination of reservoir effects, or whether in spite of heating, this material remains a viable material for precise 14C measurements. Our results show that both heated and non-heated shells of A. uropigimelana and T. palustris provide consistent results, although the mineral composition of the shells changes from aragonite to calcite. Our results are important, since some of our selection of shells did not initially appear to have been heated. A heating process will then usually be detected as a greyish, marble like structure when cutting the shells. As a result of this work, we have also developed insights into prehistoric cooking practices of shells collected in Arabia. Our results provide archaeologists and associated researchers with confidence when assessing the results of radiocarbon dating during their studies of shells that might have been heated. Depending on the species, shells can provide high-resolution information about palaeoenvironments over decades, centuries or millenia (Andrus, 2011; Hallmann et al., 2013; Lindauer et al., 2017; Paul and Mauldin, 2013; Scourse et al., 2012; Wanamaker Jr et al., 2008).The information stored in the shell material contains evidence about environmental conditions and food resources throughout their complete lifetime (Culleton et al., 2006; Hallmann et al., 2013; Leng and Lewis, 2016; Twaddle et al., 2017).Furthermore, shells can aid our understanding of strategies of food procurement by hunter-gatherer communities (Burchell et al., 2013).In archaeological contexts, large quantities of aquatic shells are found along contemporary or past coastlines, either as discrete shell midden deposits, or associated with settlements (Biagi, 1994, 2013; Biagi and Nisbet, 1999; Fernández-López de Pablo, 2016; Parker and Goudie, 2007; Uerpmann, 1990).Sometimes shells are also found in excavations further inland, e.g. as grave goods (Kutterer, 2013).In all these contexts, it is critical to know whether or not heated (burnt) shells can still be used to provide chronological control.Furthermore, sometimes it is not obvious at first sight whether shell remains have been heated or not.The most obvious way that shells would be heating is during the cooking process.
This publication aims to shed light on the influence that prior heating (burning) of mollusc shells during human activity may have on the results of radiocarbon dating. We compare the geochemical and mineralogical composition of heated and unheated shells of Anadara uropigimelana and Terebralia palustris recovered from Neolithic and Bronze Age archaeological contexts at Kalba, Sharjah Emirate, in the United Arab Emirates (UAE). Our research examined whether the heating of shells impacts on the determination of reservoir effects, or whether in spite of heating, this material remains a viable material for precise 14C measurements. Our results show that both heated and non-heated shells of A. uropigimelana and T. palustris provide consistent results, although the mineral composition of the shells changes from aragonite to calcite. Our results are important, since some of our selection of shells did not initially appear to have been heated. A heating process will then usually be detected as a greyish, marble like structure when cutting the shells. As a result of this work, we have also developed insights into prehistoric cooking practices of shells collected in Arabia. Our results provide archaeologists and associated researchers with confidence when assessing the results of radiocarbon dating during their studies of shells that might have been heated. In an earlier paper, Milano et al. (2016) focused upon species Phorcus turbinatus.The behaviour of shell microstructures and changes associated with the temperature of heating were explored; it was shown that above a temperature of 500 °C a mineralogical transformation from aragonite into calcite and major structural alterations occurred.Milano et al. (2016) also found that exposure to high temperatures resulted in a systematic shift in the oxygen stable isotope content (δ18O), which has significant implications for palaeoclimatic interpretations of sea surface temperatures.The information on altered microstructures is relevant when interpreting the radiocarbon data on heated shells as a change in microstructure might lead to an incorporation of surrounding carbon sources like atmosphere or ash.
This publication aims to shed light on the influence that prior heating (burning) of mollusc shells during human activity may have on the results of radiocarbon dating. We compare the geochemical and mineralogical composition of heated and unheated shells of Anadara uropigimelana and Terebralia palustris recovered from Neolithic and Bronze Age archaeological contexts at Kalba, Sharjah Emirate, in the United Arab Emirates (UAE). Our research examined whether the heating of shells impacts on the determination of reservoir effects, or whether in spite of heating, this material remains a viable material for precise 14C measurements. Our results show that both heated and non-heated shells of A. uropigimelana and T. palustris provide consistent results, although the mineral composition of the shells changes from aragonite to calcite. Our results are important, since some of our selection of shells did not initially appear to have been heated. A heating process will then usually be detected as a greyish, marble like structure when cutting the shells. As a result of this work, we have also developed insights into prehistoric cooking practices of shells collected in Arabia. Our results provide archaeologists and associated researchers with confidence when assessing the results of radiocarbon dating during their studies of shells that might have been heated. When investigating ancient shell midden sites or settlements, shells are often used for radiocarbon dating.However, shells that show evidence of burning are generally excluded from such studies.However, it is important to remember that in some cases it may not be possible to detect burning features at first sight.Thus, cooked shells could unintentionally be selected for dating.It is therefore important to know how heating affects the dates that may potentially result when using shells for radiocarbon dating.
Repair and maintenance services are among the most lucrative aspects of the entire automobile business chain. However, in the context of fierce competition, customer churns have led to the bankruptcy of several 4S (sales, spare parts, services, and surveys) shops. In this regard, a six-year dataset is utilized to study customer behaviors to aid managers identify and retain valuable but potential customer churn through a customized retention solution. First, we define the absence and presence behaviors of customers and thereafter generate absence data according to customer habits; this makes it possible to treat the customer absence prediction problem as a classification problem. Second, the repeated absence and presence behaviors of customers are considered as a whole from a lifecycle perspective. A modified recurrent neural network (RNN-2L) is proposed; it is more efficient and reasonable in structure compared with traditional RNN. The time-invariant customer features and the sequential lifecycle features are handled separately; this provides a more sensible specification of the RNN structure from a behavioral interpretation perspective. Third, a customized retention solution is proposed. By comparing the proposed model with those that are conventional, it is found that the former outperforms the latter in terms of area under the curve (AUC), confusion matrix, and amount of time consumed. The proposed customized retention solution can achieve significant profit increase. This paper not only elucidates the customer relationship management in the automobile aftermarket (where the absence and presence behaviors are infrequently considered), but also presents an efficient solution to increase the predictive power of conventional machine learning models. The latter is achieved by considering behavioral and business perspectives. Background and motivation
Repair and maintenance services are among the most lucrative aspects of the entire automobile business chain. However, in the context of fierce competition, customer churns have led to the bankruptcy of several 4S (sales, spare parts, services, and surveys) shops. In this regard, a six-year dataset is utilized to study customer behaviors to aid managers identify and retain valuable but potential customer churn through a customized retention solution. First, we define the absence and presence behaviors of customers and thereafter generate absence data according to customer habits; this makes it possible to treat the customer absence prediction problem as a classification problem. Second, the repeated absence and presence behaviors of customers are considered as a whole from a lifecycle perspective. A modified recurrent neural network (RNN-2L) is proposed; it is more efficient and reasonable in structure compared with traditional RNN. The time-invariant customer features and the sequential lifecycle features are handled separately; this provides a more sensible specification of the RNN structure from a behavioral interpretation perspective. Third, a customized retention solution is proposed. By comparing the proposed model with those that are conventional, it is found that the former outperforms the latter in terms of area under the curve (AUC), confusion matrix, and amount of time consumed. The proposed customized retention solution can achieve significant profit increase. This paper not only elucidates the customer relationship management in the automobile aftermarket (where the absence and presence behaviors are infrequently considered), but also presents an efficient solution to increase the predictive power of conventional machine learning models. The latter is achieved by considering behavioral and business perspectives. China’s automobile market has continued to flourish for more than 20 years.Up until 2017, the total number of vehicles in China reached 217 million, and the country’s vehicle sales ranked number one worldwide for eight years.In 2018, the estimated sales are 25.6 million passenger cars and 2 million commercial vehicles (Wouter et al., 2018).
Repair and maintenance services are among the most lucrative aspects of the entire automobile business chain. However, in the context of fierce competition, customer churns have led to the bankruptcy of several 4S (sales, spare parts, services, and surveys) shops. In this regard, a six-year dataset is utilized to study customer behaviors to aid managers identify and retain valuable but potential customer churn through a customized retention solution. First, we define the absence and presence behaviors of customers and thereafter generate absence data according to customer habits; this makes it possible to treat the customer absence prediction problem as a classification problem. Second, the repeated absence and presence behaviors of customers are considered as a whole from a lifecycle perspective. A modified recurrent neural network (RNN-2L) is proposed; it is more efficient and reasonable in structure compared with traditional RNN. The time-invariant customer features and the sequential lifecycle features are handled separately; this provides a more sensible specification of the RNN structure from a behavioral interpretation perspective. Third, a customized retention solution is proposed. By comparing the proposed model with those that are conventional, it is found that the former outperforms the latter in terms of area under the curve (AUC), confusion matrix, and amount of time consumed. The proposed customized retention solution can achieve significant profit increase. This paper not only elucidates the customer relationship management in the automobile aftermarket (where the absence and presence behaviors are infrequently considered), but also presents an efficient solution to increase the predictive power of conventional machine learning models. The latter is achieved by considering behavioral and business perspectives. The large demand for vehicles not only yields financial gains to manufacturing and sales, but also brings profits to the vehicle aftermarket, such as repair and maintenance.According to experiences gained in the mature markets, such as those in the European Union and the United States, the aftermarket profit margin is 76% higher than that of vehicle sales.For 70% of automobile companies, the net profit of spare part sales can reach 25%, and some can even reach 40%; this makes the repair and maintenance services one of the most lucrative enterprises in the entire automobile business chain.In 1998, the first 4S shop (sales, spare parts, services, and surveys) was established in China by a joint venture company, GAC-Honda,1  followed by Buick and Audi.Thereafter, China’s fast-growing automobile market has attracted practically all brands in the world to establish businesses and 4S shops in the country.
Repair and maintenance services are among the most lucrative aspects of the entire automobile business chain. However, in the context of fierce competition, customer churns have led to the bankruptcy of several 4S (sales, spare parts, services, and surveys) shops. In this regard, a six-year dataset is utilized to study customer behaviors to aid managers identify and retain valuable but potential customer churn through a customized retention solution. First, we define the absence and presence behaviors of customers and thereafter generate absence data according to customer habits; this makes it possible to treat the customer absence prediction problem as a classification problem. Second, the repeated absence and presence behaviors of customers are considered as a whole from a lifecycle perspective. A modified recurrent neural network (RNN-2L) is proposed; it is more efficient and reasonable in structure compared with traditional RNN. The time-invariant customer features and the sequential lifecycle features are handled separately; this provides a more sensible specification of the RNN structure from a behavioral interpretation perspective. Third, a customized retention solution is proposed. By comparing the proposed model with those that are conventional, it is found that the former outperforms the latter in terms of area under the curve (AUC), confusion matrix, and amount of time consumed. The proposed customized retention solution can achieve significant profit increase. This paper not only elucidates the customer relationship management in the automobile aftermarket (where the absence and presence behaviors are infrequently considered), but also presents an efficient solution to increase the predictive power of conventional machine learning models. The latter is achieved by considering behavioral and business perspectives. However, the repair and maintenance business has increasingly become extremely competitive.The local repair shops have not only improved services and provided lower prices to attract customers, but also stimulated the internet and online-to-offline business in China (Wang et al., 2016); it has vastly facilitated the comparison of service quality and prices of several shops for customers.In the past five years, customer churn has become the main reason for the decline or even bankruptcy of several 4S shops in China.
Repair and maintenance services are among the most lucrative aspects of the entire automobile business chain. However, in the context of fierce competition, customer churns have led to the bankruptcy of several 4S (sales, spare parts, services, and surveys) shops. In this regard, a six-year dataset is utilized to study customer behaviors to aid managers identify and retain valuable but potential customer churn through a customized retention solution. First, we define the absence and presence behaviors of customers and thereafter generate absence data according to customer habits; this makes it possible to treat the customer absence prediction problem as a classification problem. Second, the repeated absence and presence behaviors of customers are considered as a whole from a lifecycle perspective. A modified recurrent neural network (RNN-2L) is proposed; it is more efficient and reasonable in structure compared with traditional RNN. The time-invariant customer features and the sequential lifecycle features are handled separately; this provides a more sensible specification of the RNN structure from a behavioral interpretation perspective. Third, a customized retention solution is proposed. By comparing the proposed model with those that are conventional, it is found that the former outperforms the latter in terms of area under the curve (AUC), confusion matrix, and amount of time consumed. The proposed customized retention solution can achieve significant profit increase. This paper not only elucidates the customer relationship management in the automobile aftermarket (where the absence and presence behaviors are infrequently considered), but also presents an efficient solution to increase the predictive power of conventional machine learning models. The latter is achieved by considering behavioral and business perspectives. Consider the 4S shop we surveyed as an example.The shop, established by a large American brand, sells passenger cars worth between 80,000 and 250,000 RMB (approximately between US$11,500 and US$35,700, respectively).It is located in a city that belongs to the rich Yangtze River Delta in East China.According to our investigation, there are significant drawbacks in the customer relationship management (CRM) of this 4S shop that are also common problems in practically all 4S shops in China.
Repair and maintenance services are among the most lucrative aspects of the entire automobile business chain. However, in the context of fierce competition, customer churns have led to the bankruptcy of several 4S (sales, spare parts, services, and surveys) shops. In this regard, a six-year dataset is utilized to study customer behaviors to aid managers identify and retain valuable but potential customer churn through a customized retention solution. First, we define the absence and presence behaviors of customers and thereafter generate absence data according to customer habits; this makes it possible to treat the customer absence prediction problem as a classification problem. Second, the repeated absence and presence behaviors of customers are considered as a whole from a lifecycle perspective. A modified recurrent neural network (RNN-2L) is proposed; it is more efficient and reasonable in structure compared with traditional RNN. The time-invariant customer features and the sequential lifecycle features are handled separately; this provides a more sensible specification of the RNN structure from a behavioral interpretation perspective. Third, a customized retention solution is proposed. By comparing the proposed model with those that are conventional, it is found that the former outperforms the latter in terms of area under the curve (AUC), confusion matrix, and amount of time consumed. The proposed customized retention solution can achieve significant profit increase. This paper not only elucidates the customer relationship management in the automobile aftermarket (where the absence and presence behaviors are infrequently considered), but also presents an efficient solution to increase the predictive power of conventional machine learning models. The latter is achieved by considering behavioral and business perspectives. First, 4S shops are mainly established by overseas automobile companies; because of distance and cultural differences, communication gaps exist between the shops and the headquarters.These gaps delay and make it considerably difficult for 4S shops to alter their business activities to cope with the fast-changing market.
Repair and maintenance services are among the most lucrative aspects of the entire automobile business chain. However, in the context of fierce competition, customer churns have led to the bankruptcy of several 4S (sales, spare parts, services, and surveys) shops. In this regard, a six-year dataset is utilized to study customer behaviors to aid managers identify and retain valuable but potential customer churn through a customized retention solution. First, we define the absence and presence behaviors of customers and thereafter generate absence data according to customer habits; this makes it possible to treat the customer absence prediction problem as a classification problem. Second, the repeated absence and presence behaviors of customers are considered as a whole from a lifecycle perspective. A modified recurrent neural network (RNN-2L) is proposed; it is more efficient and reasonable in structure compared with traditional RNN. The time-invariant customer features and the sequential lifecycle features are handled separately; this provides a more sensible specification of the RNN structure from a behavioral interpretation perspective. Third, a customized retention solution is proposed. By comparing the proposed model with those that are conventional, it is found that the former outperforms the latter in terms of area under the curve (AUC), confusion matrix, and amount of time consumed. The proposed customized retention solution can achieve significant profit increase. This paper not only elucidates the customer relationship management in the automobile aftermarket (where the absence and presence behaviors are infrequently considered), but also presents an efficient solution to increase the predictive power of conventional machine learning models. The latter is achieved by considering behavioral and business perspectives. Second, the management style is qualitative rather than quantitative.Although most 4S shop managers use conventional and lucrative 4S business models, their attention to data mining is extremely limited; customer records are simply stored in the IT system without any thorough investigation of possible opportunities for profit growth.
Web image annotation has became a hot research topic owing to massive image data and abundant semantic context. In this paper, we propose a Tri-relational Graph (TG) model for web image annotation, which comprises the image data graph, the region data graph and the label graph as subgraphs, and connects them by an additional tripartite graph induced from image segmentation results and label assignments. Through analyzing the global visual similarity between images, the visual similarity between regions, the semantic correlations between labels and the relationships between the three subgraphs by TG model, we perform multilevel Random Walk with Restart algorithm on TG to produce vertex-to-vertex relevance, including image-to-region, region-to-label and image-to-label relevances. Then semi-supervised learning is used to predict labels for unannotated image regions by inserting unlabeled images and their regions into TG. In addition, we also analyze the text context information of web image and achieve the semantic and proper nouns for the further label expansion through WordNet. Experiments on public web images datasets demonstrate that our proposed TG model and multilevel RWR algorithm can achieve good performance on image region annotation and outperform the similar image annotation methods. Moreover label expansion by web semantic context analysis can achieve more accurate and abundant annotation results. Web images are exploding at an exponential rate due to the rapid development of image acquisition technology, which makes the analysis and understanding of image becoming more difficult.How to effectively express, classify and manage images from the vast web image database has become a research hotspot in the field of computer vision.Traditional automatic image annotation technology extracts the semantic information through analyzing the low-level features of images by machine learning methods, which could not solve the problem of semantic gap effectively.With the development of the Web2.0, users can share their comments conveniently on online socializing platform, which causes that web images always being around with abundant contextual information (Lu et al., 2016).And these text context information of web images can be effectively used to alleviate the problem of semantic gap in image annotation and understanding.
Web image annotation has became a hot research topic owing to massive image data and abundant semantic context. In this paper, we propose a Tri-relational Graph (TG) model for web image annotation, which comprises the image data graph, the region data graph and the label graph as subgraphs, and connects them by an additional tripartite graph induced from image segmentation results and label assignments. Through analyzing the global visual similarity between images, the visual similarity between regions, the semantic correlations between labels and the relationships between the three subgraphs by TG model, we perform multilevel Random Walk with Restart algorithm on TG to produce vertex-to-vertex relevance, including image-to-region, region-to-label and image-to-label relevances. Then semi-supervised learning is used to predict labels for unannotated image regions by inserting unlabeled images and their regions into TG. In addition, we also analyze the text context information of web image and achieve the semantic and proper nouns for the further label expansion through WordNet. Experiments on public web images datasets demonstrate that our proposed TG model and multilevel RWR algorithm can achieve good performance on image region annotation and outperform the similar image annotation methods. Moreover label expansion by web semantic context analysis can achieve more accurate and abundant annotation results. Understanding the similarity of images is important in many areas such as image retrieval (Yang et al., 2018c, a, b) and image annotation.Deng et al. use the relative relationship of the tags to obtain a more general semantic relevance for image retrieval (Deng et al., 2018).In Deng et al. (2014), they introduce a new co-regularized multi-graph Learning framework, and utilizes the complementary nature of features effectively.
Web image annotation has became a hot research topic owing to massive image data and abundant semantic context. In this paper, we propose a Tri-relational Graph (TG) model for web image annotation, which comprises the image data graph, the region data graph and the label graph as subgraphs, and connects them by an additional tripartite graph induced from image segmentation results and label assignments. Through analyzing the global visual similarity between images, the visual similarity between regions, the semantic correlations between labels and the relationships between the three subgraphs by TG model, we perform multilevel Random Walk with Restart algorithm on TG to produce vertex-to-vertex relevance, including image-to-region, region-to-label and image-to-label relevances. Then semi-supervised learning is used to predict labels for unannotated image regions by inserting unlabeled images and their regions into TG. In addition, we also analyze the text context information of web image and achieve the semantic and proper nouns for the further label expansion through WordNet. Experiments on public web images datasets demonstrate that our proposed TG model and multilevel RWR algorithm can achieve good performance on image region annotation and outperform the similar image annotation methods. Moreover label expansion by web semantic context analysis can achieve more accurate and abundant annotation results. A variety of automatic image annotation (AIA) methods haveachieved good performance in image understanding and semantic annotation (Zhang et al., 2012).However, Most of AIA methods analyze the semantic concept of the whole image without considering the semantics of different regions (Ke et al., 2017), which is called global-based image annotation method.These methods (Hu and Lam, 2013; Wang et al., 2009b; Mahmood et al., 2014; Jian et al., 2014; Yang et al., 2009; Mei et al., 2008) usually cannot locate all relevant semantic concepts accurately, since the characteristics and semantics of different regions in an image are not considered.Region-based image annotation methods detect the semantic concept of each independent region and can describe the visual features more accurately as a semantic concept (Yuan et al., 2007; Memon et al., 2017; Zhang et al., 2015; Souly and Shah, 2016; Zhang et al., 2018), and they also can address the inability to predict small objects owing to the limited discrimination of global visual features (Zhang et al., 2016).
Web image annotation has became a hot research topic owing to massive image data and abundant semantic context. In this paper, we propose a Tri-relational Graph (TG) model for web image annotation, which comprises the image data graph, the region data graph and the label graph as subgraphs, and connects them by an additional tripartite graph induced from image segmentation results and label assignments. Through analyzing the global visual similarity between images, the visual similarity between regions, the semantic correlations between labels and the relationships between the three subgraphs by TG model, we perform multilevel Random Walk with Restart algorithm on TG to produce vertex-to-vertex relevance, including image-to-region, region-to-label and image-to-label relevances. Then semi-supervised learning is used to predict labels for unannotated image regions by inserting unlabeled images and their regions into TG. In addition, we also analyze the text context information of web image and achieve the semantic and proper nouns for the further label expansion through WordNet. Experiments on public web images datasets demonstrate that our proposed TG model and multilevel RWR algorithm can achieve good performance on image region annotation and outperform the similar image annotation methods. Moreover label expansion by web semantic context analysis can achieve more accurate and abundant annotation results. An image usually contains several labels, and these labels are often strongly correlated in terms of semantics.For example, the labels “sky” and “airplane” tend to appear in same image, while the probability of the labels “sea” and “train” appearing at the same time is small.As a result, many multi-label image annotation algorithms exploit label correlations to improve the overall classification performance.For example, label correlations can be used for label refinement (Zhang et al., 2018; Liu et al., 2009; Hou and Lin, 2015; Li and Tang, 2017; Uricchio et al., 2013; Wang et al., 2010a; Uricchio et al., 2017; Wang et al., 2010d; Tsoumakas et al., 2009), or be used to seek discriminative subspaces (Wang et al., 2010c; Blei and Jordan, 2003; Li et al., 2011; Ji et al., 2010; Wang et al., 2010b).In addition, label correlations can also be incorporated into the graph so that the graph has both visual and semantic features (Chen et al., 2008; Kang et al., 2006; Wang et al., 2011; Pham et al., 2014; Wang et al., 2009a, 2010e).
Web image annotation has became a hot research topic owing to massive image data and abundant semantic context. In this paper, we propose a Tri-relational Graph (TG) model for web image annotation, which comprises the image data graph, the region data graph and the label graph as subgraphs, and connects them by an additional tripartite graph induced from image segmentation results and label assignments. Through analyzing the global visual similarity between images, the visual similarity between regions, the semantic correlations between labels and the relationships between the three subgraphs by TG model, we perform multilevel Random Walk with Restart algorithm on TG to produce vertex-to-vertex relevance, including image-to-region, region-to-label and image-to-label relevances. Then semi-supervised learning is used to predict labels for unannotated image regions by inserting unlabeled images and their regions into TG. In addition, we also analyze the text context information of web image and achieve the semantic and proper nouns for the further label expansion through WordNet. Experiments on public web images datasets demonstrate that our proposed TG model and multilevel RWR algorithm can achieve good performance on image region annotation and outperform the similar image annotation methods. Moreover label expansion by web semantic context analysis can achieve more accurate and abundant annotation results. AIA methods (Lin et al., 2013; Li et al., 2014, 2016; Hou and Lin, 2015) usually assume that the images in the training set are completely annotated.In fact, acquiring an annotated image data set is expensive and time consuming.Hence the researchers propose many semi-supervised learning methods for image annotation, which can use large amount unlabeled data and small amount labeled data to train the model.Traditional graph-based semi-supervised methods (Chen et al., 2008; Tong et al., 2006) build data graph only with image visual similarity, which omit the semantic correlations between labels.Wang et al. proposed a Bi-relational Graph (BG) for automatically multi-label image annotation through semi-supervised learning (Wang et al., 2011).The BG contains two subgraphs: data graph and label graph, which are connected by an additional bipartite graph induced from label assignments.Each label semantic class and its labeled image in BG are considered as a semantic group, and Random Walk with Restart (RWR) algorithm (Fellbaum and Miller, 1998) is performed iteratively to calculate the correlation values between semantic groups and images.Then the calculated correlation value can be used to predict unlabeled image tags.In addition, Pham et al. proposed a semi-supervised learning algorithm based on local and global consistency (Pham et al., 2014).This algorithm merges two different entities (images and labels) into a single graph, and performs label propagation based on label correlation to obtain the correlations between unannotated images and labels.
Web image annotation has became a hot research topic owing to massive image data and abundant semantic context. In this paper, we propose a Tri-relational Graph (TG) model for web image annotation, which comprises the image data graph, the region data graph and the label graph as subgraphs, and connects them by an additional tripartite graph induced from image segmentation results and label assignments. Through analyzing the global visual similarity between images, the visual similarity between regions, the semantic correlations between labels and the relationships between the three subgraphs by TG model, we perform multilevel Random Walk with Restart algorithm on TG to produce vertex-to-vertex relevance, including image-to-region, region-to-label and image-to-label relevances. Then semi-supervised learning is used to predict labels for unannotated image regions by inserting unlabeled images and their regions into TG. In addition, we also analyze the text context information of web image and achieve the semantic and proper nouns for the further label expansion through WordNet. Experiments on public web images datasets demonstrate that our proposed TG model and multilevel RWR algorithm can achieve good performance on image region annotation and outperform the similar image annotation methods. Moreover label expansion by web semantic context analysis can achieve more accurate and abundant annotation results. Graph-based semi-supervised automatic image annotationalgorithms proposed in Wang et al. (2011) and Pham et al. (2014) are based on full-image semantic analysis, and do not consider the image region semantics, which makes the annotation results being not as accurate as the region-based image annotation methods.In order to achieve more detailed description of image region and full image, we propose a novel Tri-relational Graph (TG) model for image region annotation and use web social context analysis for full image label expansion.TG model is constructed by analyzing the global similarity, regional similarity among images, the semantic correlation of image labels and the relationships between each subgraph.Then a new multilevel RWR is used to update the label-to-region scores and predict labels for unannotated image regions.Finally, proper noun and keywords extracted by Natural Language Processing (NLP) algorithm from web semantic context and expanded by WordNet (DeviantArt, 0000), are used to extended the annotation results.
Web image annotation has became a hot research topic owing to massive image data and abundant semantic context. In this paper, we propose a Tri-relational Graph (TG) model for web image annotation, which comprises the image data graph, the region data graph and the label graph as subgraphs, and connects them by an additional tripartite graph induced from image segmentation results and label assignments. Through analyzing the global visual similarity between images, the visual similarity between regions, the semantic correlations between labels and the relationships between the three subgraphs by TG model, we perform multilevel Random Walk with Restart algorithm on TG to produce vertex-to-vertex relevance, including image-to-region, region-to-label and image-to-label relevances. Then semi-supervised learning is used to predict labels for unannotated image regions by inserting unlabeled images and their regions into TG. In addition, we also analyze the text context information of web image and achieve the semantic and proper nouns for the further label expansion through WordNet. Experiments on public web images datasets demonstrate that our proposed TG model and multilevel RWR algorithm can achieve good performance on image region annotation and outperform the similar image annotation methods. Moreover label expansion by web semantic context analysis can achieve more accurate and abundant annotation results. The original contributions made in this paper are illustrated as follows:
The Bronze-Iron Age transition in Lika, Croatia is characterized by a seemingly rapid and significant transformation in sociopolitical organization. New hillfort centers were presumably supported by the intensification and specialization of economic activities to a larger degree than in previous periods, though Lika's challenging environment and topography likely made large-scale agriculture and livestock keeping difficult. We present new stable carbon and nitrogen isotope values for domesticated and wild fauna from hillforts and caves dating from the Middle Bronze to Early Iron Ages to examine changing sociopolitical and economic organization during this time. Our results suggest animal husbandry was carried out across multiple spatial and organizational scales to take advantage of finite resources, from the centralized movement of cattle and ovicaprid herds across greater swaths of the landscape to the continued management of pigs by individual households. The European Late Bronze Age was characterized by the rapid reorganization of social, political, and economic lifeways, resulting in larger and denser populations, new regional centers, and expanded trade networks that demanded increased economic specialization to sustain them.The intensification and diversification of agricultural and husbandry practices throughout Europe was one result of these new systems, and was carried out through the production of more durable tools from technological advancements in metal-working; development of intensive practices such as plowing, manuring, and irrigation; and the introduction of either new domesticates or management techniques that increased the productivity of old ones (Harding, 2002; Kristiansen and Larsson, 2005; Stevens and Fuller, 2012; Reed, 2013).Groups did not uniformly adopt these new changes, however, but instead engaged in complex decision-making processes that evaluated the potential benefits of new technologies and systems against perceived hazards and local environmental constraints.In more marginal environments with limited resources, these calculations were especially important.People, plants, and animals all require land for settlement, cultivation, and pasture, and expanding communities faced balancing these competing interests in ways that maximized yields without sacrificing efficiency or value (Allaby et al., 2015; Banks et al., 2013; Zavodny et al., 2017).
The transition from inhumation to cremation is a well-documented phenomenon in Bronze Age Central Europe. However, almost nothing is known about similar transitions taking place in other mortuary practices, such as secondary burials. This study brings new insights into diachronic trends in secondary burials during the Central European Bronze and Iron Age. Diachronic trends in secondary burials are defined here by different kinds of excarnation. The type of excarnation was observed in 23 secondary burials dating to the Early Bronze Age and the turn of the Late Bronze to Early Iron Age at five sites in Moravia (Czech Republic). Osteological and taphonomic assessment of unburned human bones recovered from settlement contexts indicates a changing pattern of secondary burial practice over time. Early Bronze Age human remains bear traces of both passive excarnation by natural agents, such as exposure to carnivores, and excarnation by primary burial. By the Late Bronze Age and Early Iron Age secondary burials show evidence of excarnation with tools. This modification of secondary burial practices, may be connected with a contemporaneous change of primary burial practices from inhumation to cremation. Mortuary practices in Central Europe changed radically during the Bronze Age (Brandt et al., 2014; Harding and Fokkens, 2013; Harding, 2000; Sørensen and Rebay, 2008).By the Early Bronze Age (EBA; 2200–1500 BCE) the dominant ritual is characterized by relatively uniform primary inhumations of complete bodies.During the Middle Bronze Age (1500–1300 BCE) cremation becomes more prevalent (Stuchlík, 1993).This change spread across Continental Europe with the Urnfield culture in Late Bronze and Early Iron Age from 1300 to 700 BCE.The break between the Late Bronze and Early Iron Age (LBA/EIA) in Central Europe corresponds with two phases of the Lusatian culture.During the transition from inhumation to cremation, secondary burials gradually increase with their methods and characteristics changing over time (Rulf, 1996; Stuchlík, 2010).
The transition from inhumation to cremation is a well-documented phenomenon in Bronze Age Central Europe. However, almost nothing is known about similar transitions taking place in other mortuary practices, such as secondary burials. This study brings new insights into diachronic trends in secondary burials during the Central European Bronze and Iron Age. Diachronic trends in secondary burials are defined here by different kinds of excarnation. The type of excarnation was observed in 23 secondary burials dating to the Early Bronze Age and the turn of the Late Bronze to Early Iron Age at five sites in Moravia (Czech Republic). Osteological and taphonomic assessment of unburned human bones recovered from settlement contexts indicates a changing pattern of secondary burial practice over time. Early Bronze Age human remains bear traces of both passive excarnation by natural agents, such as exposure to carnivores, and excarnation by primary burial. By the Late Bronze Age and Early Iron Age secondary burials show evidence of excarnation with tools. This modification of secondary burial practices, may be connected with a contemporaneous change of primary burial practices from inhumation to cremation. The reason for the great transition and function of the cremation is unknown.The traditional explanation of the transition from inhumation to cremation considers population movement and changing ethnicity to be the primary cause (Kimmig, 1964; Reinecke, 1900) with other potential causal factors related to wealth, politics and social stratification (Urban, 2000).More recently, Flohr Sørensen and Bille (2008) and Harris et al. (2013) remark that the transition is not part of any great ideological or political transformation and that changes occurred only on a regional level.The primary function of cremation might have been to quickly and simply fragment a body to acquire clean bone fragments for other ritualistic needs, like curation (Flohr Sørensen and Bille, 2008; Rebay-Salisbury, 2010).
The transition from inhumation to cremation is a well-documented phenomenon in Bronze Age Central Europe. However, almost nothing is known about similar transitions taking place in other mortuary practices, such as secondary burials. This study brings new insights into diachronic trends in secondary burials during the Central European Bronze and Iron Age. Diachronic trends in secondary burials are defined here by different kinds of excarnation. The type of excarnation was observed in 23 secondary burials dating to the Early Bronze Age and the turn of the Late Bronze to Early Iron Age at five sites in Moravia (Czech Republic). Osteological and taphonomic assessment of unburned human bones recovered from settlement contexts indicates a changing pattern of secondary burial practice over time. Early Bronze Age human remains bear traces of both passive excarnation by natural agents, such as exposure to carnivores, and excarnation by primary burial. By the Late Bronze Age and Early Iron Age secondary burials show evidence of excarnation with tools. This modification of secondary burial practices, may be connected with a contemporaneous change of primary burial practices from inhumation to cremation. Changes in body treatment within secondary burials are not well explored.Little, if any, empirical research of diachronic change in these mortuary practices exists for Central Europe.Regardless, it would be surprising to discover that changes in primary mortuary rituals from inhumation to cremation did not coincide with changes in secondary practices.Cremation and the practice of secondary burial of unburnt human bones, may reflect two means to the same ends: to fragment a human body for social needs, such as curation, circulation away from the mortuary site, and redistribution within the community (Cerezo-Román et al., 2017; Cerezo-Román and Williams, 2014; Rebay-Salisbury, 2010).
The transition from inhumation to cremation is a well-documented phenomenon in Bronze Age Central Europe. However, almost nothing is known about similar transitions taking place in other mortuary practices, such as secondary burials. This study brings new insights into diachronic trends in secondary burials during the Central European Bronze and Iron Age. Diachronic trends in secondary burials are defined here by different kinds of excarnation. The type of excarnation was observed in 23 secondary burials dating to the Early Bronze Age and the turn of the Late Bronze to Early Iron Age at five sites in Moravia (Czech Republic). Osteological and taphonomic assessment of unburned human bones recovered from settlement contexts indicates a changing pattern of secondary burial practice over time. Early Bronze Age human remains bear traces of both passive excarnation by natural agents, such as exposure to carnivores, and excarnation by primary burial. By the Late Bronze Age and Early Iron Age secondary burials show evidence of excarnation with tools. This modification of secondary burial practices, may be connected with a contemporaneous change of primary burial practices from inhumation to cremation. Secondary burials, in an archaeological sense, are characterized by skeletal disarticulation which occurred prior to final disposal, and the underrepresentation of certain small bones (Schroeder, 2001).Secondary burial is any subsequent burial, irrespective of the number of previous exhumations or bone relocations (Burns, 1999).Harrisson (1967) defines secondary burial as human skeletal remains broken up in a way presupposing major disintegration of the flesh and ligaments after primary burial, exhumation, retreatment, and redeposition of the bones during secondary rites (Sprague, 2005).According to (Kuijt, 1996), who integrated ethnographic and archaeological data about Levantine Neolithic communities, secondary manipulation with inhumed unburnt bones is a complex and sanctioned social act.He understands secondary manipulation with human remains as the development and expansion of ritual practices that emphasized collective community beliefs and identity.Archaeologically, secondary burials are expressed as intentional removal of bones from one location to another, and are represented by the recovery of disarticulated and incomplete skeletal remains (Kuijt, 1996).Secondary burials also comprise transformation of the body into fragments, and its final integration into a form of custody.In addition, body fragmentation allows the descendants to distribute remains of their relatives within the community in the form of relics (Cerezo-Román, 2015; Ezzo, 2007).The distribution and sharing of bone fragments is reflective of social roles and relationships between the living and the dead.Nevertheless, the term secondary burial is just interpretative; it understands the act as resulting from ritual activities.Aside from ritual activities, secondary burials can result from other, sometimes accidental events including mass graves from epidemics and war (Pérez, 2012), cannibalism (Hurlbut, 2000; Knüsel and Robb, 2016), post-depositional processes (Beckett, 2011; Chroustovský and Průchová, 2011) and disturbance of older graves (Knüsel and Robb, 2016).Therefore, Knüsel and Robb (2016) used the more neutral phrase secondary deposition.
In this article we outline a statistical method for distinguishing ostrich eggshell (OES) beads perforated with a hand turned drill bit and those created with a hafted drill. This distinction has important implications for tracking past bead-making traditions across space and time, and for tracing the first appearance and spread of hafted drilling. Previous efforts to reconstruct the way in which beads were perforated have relied on a common sense approach, usually in combination with an experimental reference. However, without blind-test results or other metrics of reliability it is unclear how accurate these methods are. We argue that the quantitative framework described here provides a much needed answer to this question and helps to further systematize the process of bead analysis. We also define a set of terms which we hope will allow for a more standardized discussion of bead production signatures and techniques. Although it is impossible to know their precise meaning to those who made and wore them, archaeological beads were likely signals of status, prestige and beauty, as they are in contemporary societies.Beads first appeared in the archaeological record as non-standardized perforated aquatic shells that are thought to have been strung and worn suspended on the body (e.g., d'Errico et al., 2009; Henshilwood, 2007).The first deliberately shaped, standardized ornaments are ostrich eggshell (OES) beads that date to the end of the Middle Stone Age (MSA) (Miller and Willoughby, 2014), and which subsequently became common during the Later Stone Age (LSA) of eastern and southern Africa.While beads hold great scholarly significance because of what they represent in terms of human cognition and sociality they are also the products of a complex technological process, which is itself worthy of study.
In this article we outline a statistical method for distinguishing ostrich eggshell (OES) beads perforated with a hand turned drill bit and those created with a hafted drill. This distinction has important implications for tracking past bead-making traditions across space and time, and for tracing the first appearance and spread of hafted drilling. Previous efforts to reconstruct the way in which beads were perforated have relied on a common sense approach, usually in combination with an experimental reference. However, without blind-test results or other metrics of reliability it is unclear how accurate these methods are. We argue that the quantitative framework described here provides a much needed answer to this question and helps to further systematize the process of bead analysis. We also define a set of terms which we hope will allow for a more standardized discussion of bead production signatures and techniques. The manufacture process of OES beads can be organized by archaeologists in a number of ways.One of the most common systems involves determining whether the shell was perforated prior to the shaping of the bead's exterior or after, otherwise known as Pathways 1 and 2, respectively (Orton, 2008).The actual act of perforating the shell, on the other hand, has received less attention.In theory, the perforation of OES can be accomplished using a variety of techniques including pecking (repeatedly tapping the OES with an implement), gouging (pressing an implement into the OES and using a scooping motion to scrape away the surface) or punching (using a small number of directed blows with significant force to push an implement through the OES).However, personal observation by JM indicates that the vast majority of Stone Age OES beads show evidence of rotary drilling, which involves rotating a sharp implement of some kind against the shell.
In this article we outline a statistical method for distinguishing ostrich eggshell (OES) beads perforated with a hand turned drill bit and those created with a hafted drill. This distinction has important implications for tracking past bead-making traditions across space and time, and for tracing the first appearance and spread of hafted drilling. Previous efforts to reconstruct the way in which beads were perforated have relied on a common sense approach, usually in combination with an experimental reference. However, without blind-test results or other metrics of reliability it is unclear how accurate these methods are. We argue that the quantitative framework described here provides a much needed answer to this question and helps to further systematize the process of bead analysis. We also define a set of terms which we hope will allow for a more standardized discussion of bead production signatures and techniques. This article will therefore focus on the two forms of rotary drilling that are well known from ethnographic accounts of OES bead-making and Stone Age artifacts.The first form involves manually twisting the drill bit which is held in the hand, while the second relies on a rapidly rotating a hafted drill (e.g. Wingfield, 2003; Hitchcock, 2012; Marshall, 1976; Schapera, 1965; Stow, 1905).Unlike hand-drilling, we demonstrate that hafted-drilling is highly efficient and those equipped with this technology would have been able to drill large numbers of beads very quickly.The introduction of such specialized technology may have had important social implications such as the concentration of social capital amongst select groups or the establishment of craft specialists.
In this article we outline a statistical method for distinguishing ostrich eggshell (OES) beads perforated with a hand turned drill bit and those created with a hafted drill. This distinction has important implications for tracking past bead-making traditions across space and time, and for tracing the first appearance and spread of hafted drilling. Previous efforts to reconstruct the way in which beads were perforated have relied on a common sense approach, usually in combination with an experimental reference. However, without blind-test results or other metrics of reliability it is unclear how accurate these methods are. We argue that the quantitative framework described here provides a much needed answer to this question and helps to further systematize the process of bead analysis. We also define a set of terms which we hope will allow for a more standardized discussion of bead production signatures and techniques. Nevertheless, the precise origins of hafted-drilling are unknown.In Africa and the Near East it is assumed to have been present by the early Holocene/late Pleistocene (Gorelick and Gwinnett, 1990; Gwinnett and Gorelick, 1998; Wright et al., 2008), while in China, Yang et al. (2016) have identified hafted-drilled beads dating to the early Holocene from the Shuidonggou site (see also Wei et al., 2017).Even so, it is possible that this technique has a much greater time depth, as both rotary-drilled OES beads and hafted tools originated much earlier in the MSA.It is also unclear which social, environmental and historical conditions might have stimulated the development and adoption of hafted-drilling.Resolving these questions would provide valuable insight into Stone Age technology and social life.However, many of the necessary analytical methods are either absent or lacking solid experimental validation.
In this article we outline a statistical method for distinguishing ostrich eggshell (OES) beads perforated with a hand turned drill bit and those created with a hafted drill. This distinction has important implications for tracking past bead-making traditions across space and time, and for tracing the first appearance and spread of hafted drilling. Previous efforts to reconstruct the way in which beads were perforated have relied on a common sense approach, usually in combination with an experimental reference. However, without blind-test results or other metrics of reliability it is unclear how accurate these methods are. We argue that the quantitative framework described here provides a much needed answer to this question and helps to further systematize the process of bead analysis. We also define a set of terms which we hope will allow for a more standardized discussion of bead production signatures and techniques. Some experimental research has been devoted to the study of drilling, while only a few studies have examined the rotary drill impressions left on beads.Much of the existing literature has instead focused on the micro-wear traces left on experimental drill bits (e.g. Beyin, 2010; Coșkunsu, 2009; Kenoyer and Vidale, 1992; Unger-Hamilton et al., 1987; Yerkes, 1983).Other studies have attempted to replicate archaeological techniques by experimentally punch-piercing aquatic shells (e.g. d'Errico et al., 2005; Stiner et al., 2013; Tátá et al., 2014), and others have analyzed the efficiency of drilling with respect to variables like lithic material and shell heating (e.g. Arnold and Rachal, 2002; Nigra and Arnold, 2013).Only a handful of experiments have directly addressed the difference in aperture characteristics between beads drilled by a bit held in the hand and beads drilled with a composite tool such as a bow drill.These projects generally report that hand-turned perforations are irregular with asymmetrical/eccentric apertures (Coșkunsu, 2009; Gwinnett and Gorelick, 1991; Yang et al., 2016; Yerkes, 1983).Other characteristics which suggest hand drilling include the formation of striations that do not form complete circles (Coșkunsu, 2009), the presence of a notch within the aperture (Gwinnett and Gorelick, 1991), and the presence of “waving/fluting” inside the aperture (Yang et al., 2016).
In this article we outline a statistical method for distinguishing ostrich eggshell (OES) beads perforated with a hand turned drill bit and those created with a hafted drill. This distinction has important implications for tracking past bead-making traditions across space and time, and for tracing the first appearance and spread of hafted drilling. Previous efforts to reconstruct the way in which beads were perforated have relied on a common sense approach, usually in combination with an experimental reference. However, without blind-test results or other metrics of reliability it is unclear how accurate these methods are. We argue that the quantitative framework described here provides a much needed answer to this question and helps to further systematize the process of bead analysis. We also define a set of terms which we hope will allow for a more standardized discussion of bead production signatures and techniques. This study builds upon previous work in a number of important ways.Firstly, most existing research relies upon a semi-systematic visual survey of the bead aperture, usually in conjunction with informal experimentation.While we do not doubt the value of such observations, these methods of bead differentiation are in some part subjective and rely largely on the skill of the analyst to be successful.Their comparability is also limited by a lack of standardized language to describe aperture characteristics.Perhaps most concerning, a blind testing program of bead drilling techniques has not been published, so it is uncertain how well these studies are actually able to predict drilling type.In any analytical field, particularly one that relies on subjective judgement, the importance of testing to validate potential methods cannot be understated (Eren et al., 2016; Evans, 2014).By making our results available this study provides a much needed experimental underpinning for existing and future research.
In this article we outline a statistical method for distinguishing ostrich eggshell (OES) beads perforated with a hand turned drill bit and those created with a hafted drill. This distinction has important implications for tracking past bead-making traditions across space and time, and for tracing the first appearance and spread of hafted drilling. Previous efforts to reconstruct the way in which beads were perforated have relied on a common sense approach, usually in combination with an experimental reference. However, without blind-test results or other metrics of reliability it is unclear how accurate these methods are. We argue that the quantitative framework described here provides a much needed answer to this question and helps to further systematize the process of bead analysis. We also define a set of terms which we hope will allow for a more standardized discussion of bead production signatures and techniques. Secondly, we believe that dialogue between researchers could be improved by an attempt to systematize the language that is used to describe the anatomy of bead perforations.Having reviewed the literature, we therefore propose a synthesis of some terms that we hope will permit a more nuanced and mutually intelligible conversation about bead technology.We also supplement descriptions of features with detailed images to help limit uncertainty.
Fault prognostics aims at predicting the degradation of equipment for estimating the Remaining Useful Life (RUL). Traditional data-driven fault prognostic approaches face the challenge of dealing with incomplete and noisy data collected at irregular time steps, e.g. in correspondence of the occurrence of triggering events in the system. Since the values of all the signals are missing at the same time and the number of missing data largely exceeds the number of triggering events, missing data reconstruction approaches are difficult to apply. In this context, the objective of the present work is to develop a one-step method, which directly receives in input the event-based measurement and produces in output the system RUL with the associated uncertainty. Two strategies based on the use of ensembles of Echo State Networks (ESNs), properly adapted to deal with data collected at irregular time steps, have been proposed to this aim. A synthetic and a real-world case study are used to show their effectiveness and their superior performance with respect to state-of-the-art prognostic methods. Prognostics aims at predicting the degradation of equipment for estimating the Remaining Useful Life (RUL) (Zio and Di Maio, 2010; Zio, 2012; Liao and Köttig, 2014; Palacios et al., 2015; Prytz et al., 2015; Lei et al., 2018).Prognostic methods are classified into model-based and data-driven (Schwabacher and Goebel, 2007; Zio and Di Maio, 2010).Model-based approaches, which are based on the use of physics-based models of the degradation processes, are typically applied to safety-critical and slow-degrading equipment whose degradation mechanisms have been extensively studied (Cai et al., 2017b).By contrast, data-driven approaches are typically used when accurate physics-based models of the, possibly competing, degradation processes which the components of industrial systems are subjected to are not available (Hu et al., 2012a, b; Medjaher et al., 2012; Rigamonti et al., 2017; Sardá-Espinosa et al., 2017; Huang et al., 2019).Since they require a large amount of run-to-failure degradation trajectories for model training (Hu et al., 2012a, b), data-driven approaches are typically applied to non-safety critical systems characterized by relatively short mean times to failure.They are distinguished into two approaches (i) degradation-based, which indirectly predict the system RUL by estimating the future evolution of the component degradation until a failure threshold is reached (Rigamonti et al., 2016a, b; Lim et al., 2017) and (ii) direct RUL prediction-based, which predict the system RUL by developing a direct mapping from the condition monitoring signals to the system RUL (Khelif et al., 2017).Although degradation-based approaches are closer to physics-based reasoning, they are more difficult to develop than direct RUL prediction-based approaches when the quantification of the component degradation and the identification of a failure threshold are not straightforward (Medjaher et al., 2012).
Fault prognostics aims at predicting the degradation of equipment for estimating the Remaining Useful Life (RUL). Traditional data-driven fault prognostic approaches face the challenge of dealing with incomplete and noisy data collected at irregular time steps, e.g. in correspondence of the occurrence of triggering events in the system. Since the values of all the signals are missing at the same time and the number of missing data largely exceeds the number of triggering events, missing data reconstruction approaches are difficult to apply. In this context, the objective of the present work is to develop a one-step method, which directly receives in input the event-based measurement and produces in output the system RUL with the associated uncertainty. Two strategies based on the use of ensembles of Echo State Networks (ESNs), properly adapted to deal with data collected at irregular time steps, have been proposed to this aim. A synthetic and a real-world case study are used to show their effectiveness and their superior performance with respect to state-of-the-art prognostic methods. Since prognostics requires to catch the dynamic behavior of the degradation process, static approaches based on the prediction of the equipment RUL at a given time as a function of the signal measurements at the same time typically provide unsatisfactory performances.An attempt to catch the system dynamics is to provide in input to the prognostic model the current and past signal values collected in a sliding time window.The main limitation of this approach is the difficulty in identifying a proper length of the time window, which allows representing the degradation dynamics without dramatically increasing the computational cost of the models, particularly if there are many input variables (Geraci and Gnabo, 2018).An alternative solution to the problem of learning the system dynamic for RUL prediction is the use of Recurrent Neural Networks (RNNs) (Zio et al., 2009; Malhi et al., 2011; Guo et al., 2017).The recurrent nature of RNNs, obtained by using feedback connections between the neurons of a layer and those of the preceding layers, allows processing dynamic information and makes them different from Feedforward Artificial Neural Networks (FANNs), which provide only a direct functional mapping between input and output data (Samanta and Al-Balushi, 2003; Moustapha and Selmic, 2008).Among the various types of recurrent networks that have been proposed in the last years, Echo State Networks (ESNs) are emerging due to their intrinsic dynamic properties, generalization capability, ability to handle noisy data and easiness of training (Jaeger, 2004).An extensive literature review on the use of RNNs in fault prognostics is reported in Section 2.
Fault prognostics aims at predicting the degradation of equipment for estimating the Remaining Useful Life (RUL). Traditional data-driven fault prognostic approaches face the challenge of dealing with incomplete and noisy data collected at irregular time steps, e.g. in correspondence of the occurrence of triggering events in the system. Since the values of all the signals are missing at the same time and the number of missing data largely exceeds the number of triggering events, missing data reconstruction approaches are difficult to apply. In this context, the objective of the present work is to develop a one-step method, which directly receives in input the event-based measurement and produces in output the system RUL with the associated uncertainty. Two strategies based on the use of ensembles of Echo State Networks (ESNs), properly adapted to deal with data collected at irregular time steps, have been proposed to this aim. A synthetic and a real-world case study are used to show their effectiveness and their superior performance with respect to state-of-the-art prognostic methods. The objectives of the present work are (i) to predict the RUL of a system made by non-repairable interacting components, in which the measurements are collected only when triggering events, such as component faults or extreme operational conditions occur, and (ii) to estimate the uncertainty affecting the RUL prediction.These “snapshot” datasets are often encountered in industrial applications, dominated by the necessity of cost saving in storing and managing the databases (Weijters and van der Aalst, 2003; Liu et al., 2017), and of reducing energy consumption and bandwidth sources (Tsividis, 2010).Since failure events rarely occur during the lifetime of a system, event-based datasets are dominated by the presence of a large number of missing measurements (Fink et al., 2015).Furthermore, the values of all the signals are missing at the same time.Given these characteristics, traditional methods for missing data management, e.g. case deletion (Schafer and Graham, 2002), imputation (Eekhout et al., 2012; Ranjbar et al., 2015; Cheng et al., 2019; Razavi-Far et al., 2019) and maximum likelihood estimation (Baraldi and Enders, 2010), are difficult to apply.For instance, since Case Deletion methods discard patterns whose information is incomplete, they are not useful in case of event-based datasets where a pattern is either present or absent for all signals (Baraldi and Enders, 2010).Imputation techniques, which are based on the idea that a missing value can be replaced by a statistical indicator of the probability distribution generating the data, such as the signal mean (Donders et al., 2006) or a value predicted by a multivariable regression model (Schafer and Graham, 2002), have been shown to be inaccurate in case of large fractions of missing values (Schafer, 1999; Vergouw et al., 2010).Maximum Likelihood methods use the available data to identify the values of the probability distribution parameters with the largest probability of producing the sample data (Schafer, 1999).They typically require the Missing At Random (MAR) assumption, i.e. the probability of having a missing value is not dependent on the missing values (Little and Rubin, 2002; Donders et al., 2006; Honaker and King, 2010), which is not met in event-based datasets.
Fault prognostics aims at predicting the degradation of equipment for estimating the Remaining Useful Life (RUL). Traditional data-driven fault prognostic approaches face the challenge of dealing with incomplete and noisy data collected at irregular time steps, e.g. in correspondence of the occurrence of triggering events in the system. Since the values of all the signals are missing at the same time and the number of missing data largely exceeds the number of triggering events, missing data reconstruction approaches are difficult to apply. In this context, the objective of the present work is to develop a one-step method, which directly receives in input the event-based measurement and produces in output the system RUL with the associated uncertainty. Two strategies based on the use of ensembles of Echo State Networks (ESNs), properly adapted to deal with data collected at irregular time steps, have been proposed to this aim. A synthetic and a real-world case study are used to show their effectiveness and their superior performance with respect to state-of-the-art prognostic methods. To the best of our knowledge, few research works have considered fault prognostics in presence of missing data.A model based on Auto-Regressive Moving Average (ARMA) and an auto-associative neural networks, is developed for fault diagnostics and prognostics of water processes with incomplete data (Xiao et al., 2017) and an hybrid architecture including physics-based and data-driven approaches are proposed to deal with missing data in case of rotating machinery (Leturiondo et al., 2017).In the medical field, a Bayesian simulator is used to generate missing data for developing prognostic models (Marshall et al., 2010) and a Multiple Imputation approach is used within a prognostic model for assessing overall survival of ovarian cancer in presence of missing covariate data (Clark and Altman, 2003).Notice that all these methods are based on the two successive steps of missing data reconstruction and prediction.
Fault prognostics aims at predicting the degradation of equipment for estimating the Remaining Useful Life (RUL). Traditional data-driven fault prognostic approaches face the challenge of dealing with incomplete and noisy data collected at irregular time steps, e.g. in correspondence of the occurrence of triggering events in the system. Since the values of all the signals are missing at the same time and the number of missing data largely exceeds the number of triggering events, missing data reconstruction approaches are difficult to apply. In this context, the objective of the present work is to develop a one-step method, which directly receives in input the event-based measurement and produces in output the system RUL with the associated uncertainty. Two strategies based on the use of ensembles of Echo State Networks (ESNs), properly adapted to deal with data collected at irregular time steps, have been proposed to this aim. A synthetic and a real-world case study are used to show their effectiveness and their superior performance with respect to state-of-the-art prognostic methods. In this work, we consider the possibility of developing a method which is able to directly predict the equipment RUL without requiring a missing data reconstruction step.To this aim, the use of ESNs is considered due to their ability of maintaining information about the input history inside the reservoir state.The main difficulty to be addressed is that, contrarily to the typical applications of ESNs, the time intervals at which the data become available are irregular.Two different strategies are considered to cope with the event-based data collection.In strategy 1, the ESN receives an input pattern only when an event occurs.The pattern is formed by the measured signals and the time at which the event has occurred.In strategy 2, the reservoir states are excited at each time step.If an event has occurred, the reservoir states are excited both by the previous reservoir states and the measured signals, whereas, if an event has not occurred, they are excited only by the previous reservoir states.Therefore, it is expected that the connection loops in the reservoir allow reconstructing the dynamic degradation behavior of the system at those time steps in which events do not occur.
Fault prognostics aims at predicting the degradation of equipment for estimating the Remaining Useful Life (RUL). Traditional data-driven fault prognostic approaches face the challenge of dealing with incomplete and noisy data collected at irregular time steps, e.g. in correspondence of the occurrence of triggering events in the system. Since the values of all the signals are missing at the same time and the number of missing data largely exceeds the number of triggering events, missing data reconstruction approaches are difficult to apply. In this context, the objective of the present work is to develop a one-step method, which directly receives in input the event-based measurement and produces in output the system RUL with the associated uncertainty. Two strategies based on the use of ensembles of Echo State Networks (ESNs), properly adapted to deal with data collected at irregular time steps, have been proposed to this aim. A synthetic and a real-world case study are used to show their effectiveness and their superior performance with respect to state-of-the-art prognostic methods. In both proposed strategies, a Multi-Objective Differential Evolution (MODE) algorithm based on a Self-adaptive Differential Evolution with Neighborhood Search (SaNSDE) (Yang et al., 2008) is used to optimize the ESN hyper-parameters.The Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) (Yoon and Hwang, 1995) is, then, used to select the optimal solution from the obtained Pareto solutions.Furthermore, a bootstrap aggregating (Bagging) ensemble method is applied to improve the RUL prediction accuracy and estimate the RUL prediction uncertainty.Given that ESNs cannot be fed by random sequences of patterns, the traditional Bagging sampling mechanism used to create the bootstrap training sets has been modified.In the proposed solution, the bootstrap training sets are obtained by concatenating entire run-to-failure trajectories, randomly sampled with replacement,
Fault prognostics aims at predicting the degradation of equipment for estimating the Remaining Useful Life (RUL). Traditional data-driven fault prognostic approaches face the challenge of dealing with incomplete and noisy data collected at irregular time steps, e.g. in correspondence of the occurrence of triggering events in the system. Since the values of all the signals are missing at the same time and the number of missing data largely exceeds the number of triggering events, missing data reconstruction approaches are difficult to apply. In this context, the objective of the present work is to develop a one-step method, which directly receives in input the event-based measurement and produces in output the system RUL with the associated uncertainty. Two strategies based on the use of ensembles of Echo State Networks (ESNs), properly adapted to deal with data collected at irregular time steps, have been proposed to this aim. A synthetic and a real-world case study are used to show their effectiveness and their superior performance with respect to state-of-the-art prognostic methods. The two proposed strategies are applied to a synthetic case study, properly designed to mimic run-to-failure trajectories of a system of non-repairable interacting components in which the measurements are collected when events occur.The benefits of the proposed approaches are further shown by their application to a real-world case study concerning the prediction of the RUL of a sliding bearing of a turbine unit.The accuracy of the two strategies is evaluated considering the Cumulative Relative Accuracy (CRA) and Alpha-Lambda (α-λ) metrics (Saxena et al., 2010), and compared to that of a traditional feedforward neural network and a state-of-the-art ensemble of ESNs.
Fault prognostics aims at predicting the degradation of equipment for estimating the Remaining Useful Life (RUL). Traditional data-driven fault prognostic approaches face the challenge of dealing with incomplete and noisy data collected at irregular time steps, e.g. in correspondence of the occurrence of triggering events in the system. Since the values of all the signals are missing at the same time and the number of missing data largely exceeds the number of triggering events, missing data reconstruction approaches are difficult to apply. In this context, the objective of the present work is to develop a one-step method, which directly receives in input the event-based measurement and produces in output the system RUL with the associated uncertainty. Two strategies based on the use of ensembles of Echo State Networks (ESNs), properly adapted to deal with data collected at irregular time steps, have been proposed to this aim. A synthetic and a real-world case study are used to show their effectiveness and their superior performance with respect to state-of-the-art prognostic methods. The original contributions of this work are:
We report a case of auditory disturbance in an adult female that developed after starting lacosamide treatment for epilepsy. While carbamazepine is known to change auditory pitch perception in some patients, that has not been previously reported as a side effect of lacosamide administration. In our description of pitch perception deficit associated with lacosamide, we outline features seen in our patient and compare our findings with those of previous reports describing carbamazepine-associated auditory disturbance. Lacosamide, a third-generation anti-seizure drug administered for focal epilepsy [1], blocks sodium channels, as also seen with the traditional anti-seizure drug carbamazepine.However, in contrast to that drug, lacosamide blocks the slow inactivation state of sodium channels, though has no effect on fast inactivation.In a large-scale double-blind trial, lacosamide was shown to be non-inferior to carbamazepine-CR and also well tolerated when used as first-line monotherapy in patients aged 16 years or older with newly diagnosed epilepsy [1].Moreover, lacosamide is a non-enzyme-inducing anti-seizure drug with a predictable pharmacokinetic profile and low potential for interactions with other drugs, indicating its suitability for first-line monotherapy in patients with focal epilepsy.
The key factor in selecting appropriate forecasting model is accuracy. Given the deficiencies of single models in processing various patterns and relationships latent in data, hybrid approaches have been known as promising techniques to achieve more accurate results for time series modeling and forecasting. Therefore, a rapid development has been evolved in time series forecasting fields in order to access accurate results. While, numerous review papers have been concentrated on the use of hybrid models and their advantages in improving forecasting accuracy versus individual models in wide variety of areas, no study is concerned to categorize and review papers from the structural point of view in numerous developed studies. The main goal of this paper is to analyze hybrid structures by surveying more than 150 papers employed various hybrid models in time series modeling and forecasting domains. In this paper, the classification of hybrid models is made based on three main combination structures: parallel, series, and parallel–series. Then, reviewed papers are analyzed comprehensively with respect to their specific features of employed hybrid structure. Through reviewed articles, it can be observed that combined methods are viable and accurate approaches for time series forecasting and also the parallel–series hybrid structure can obtain more accurate and promising results than other those hybrid structures. Besides this paper provides the viable research directions for each hybrid structure to help the researchers in time series forecasting area. Time series forecasting is a broad and active research area which is drawn considerable attention from wide variety of fields such as finance, engineering, statistics and etc.Therefore, a large amount of literature has focused on approaches that can get accurate forecasts in numerous practical applications.In general, two main ways can be traced in the literature in order to achieve desired and accurate results and/or improve the accuracy of obtained results: (1) developing and proposing new forecasting models and (2) hybridization of existing forecasting models.Hybridization is generally performed due to the lacking of the comprehensive individual model in capturing various patterns in the data, concurrently.Undoubtedly, one model is not sufficient to deal with complex real world systems with unknown mixed patterns.Combining different models is one of the most common remedy introduced in the literature aiming to take the strength of individual models in patterns modeling and recognition applied in large amount of time series forecasting articles.Pioneer researchers such as Bates and Granger (1969), Clemen (1989); Granger and Ramanathan (1984), Hibon and Evgeniou (2005), Timmermann (2006), Winkler and Markakis (1983), Bunn (1989) and Armstrong (2001) claimed that combining different models can enhance forecasting accuracy compared with individual models used separately.
The key factor in selecting appropriate forecasting model is accuracy. Given the deficiencies of single models in processing various patterns and relationships latent in data, hybrid approaches have been known as promising techniques to achieve more accurate results for time series modeling and forecasting. Therefore, a rapid development has been evolved in time series forecasting fields in order to access accurate results. While, numerous review papers have been concentrated on the use of hybrid models and their advantages in improving forecasting accuracy versus individual models in wide variety of areas, no study is concerned to categorize and review papers from the structural point of view in numerous developed studies. The main goal of this paper is to analyze hybrid structures by surveying more than 150 papers employed various hybrid models in time series modeling and forecasting domains. In this paper, the classification of hybrid models is made based on three main combination structures: parallel, series, and parallel–series. Then, reviewed papers are analyzed comprehensively with respect to their specific features of employed hybrid structure. Through reviewed articles, it can be observed that combined methods are viable and accurate approaches for time series forecasting and also the parallel–series hybrid structure can obtain more accurate and promising results than other those hybrid structures. Besides this paper provides the viable research directions for each hybrid structure to help the researchers in time series forecasting area. Main advantages of hybrid models enumerated in related articles can be summarized in three substantial points:
The key factor in selecting appropriate forecasting model is accuracy. Given the deficiencies of single models in processing various patterns and relationships latent in data, hybrid approaches have been known as promising techniques to achieve more accurate results for time series modeling and forecasting. Therefore, a rapid development has been evolved in time series forecasting fields in order to access accurate results. While, numerous review papers have been concentrated on the use of hybrid models and their advantages in improving forecasting accuracy versus individual models in wide variety of areas, no study is concerned to categorize and review papers from the structural point of view in numerous developed studies. The main goal of this paper is to analyze hybrid structures by surveying more than 150 papers employed various hybrid models in time series modeling and forecasting domains. In this paper, the classification of hybrid models is made based on three main combination structures: parallel, series, and parallel–series. Then, reviewed papers are analyzed comprehensively with respect to their specific features of employed hybrid structure. Through reviewed articles, it can be observed that combined methods are viable and accurate approaches for time series forecasting and also the parallel–series hybrid structure can obtain more accurate and promising results than other those hybrid structures. Besides this paper provides the viable research directions for each hybrid structure to help the researchers in time series forecasting area. According to the importance of the forecasting with high degree of accuracy, applying hybrid models for time series modeling and forecasting continues to grow in recent studies.Several review articles overviewed the hybrid forecasting models and their applications.These review papers provided different classifications of the hybrid models in specific field of time series forecasting.Tascikaraoglu and Uzunoglu (2014) generally reviewed the most widely-used hybrid models and developments in the short term wind power and speed forecasting field namely weighted based hybrid model, data preprocessing, parameter selection and optimization and error processing techniques.Deb et al. (2017) presented the comprehensive overview on the machine learning approaches and their hybrid models which are constructed from two or more machine learning techniques for energy consumption forecasting.Pradeepkumar and Ravi presented (Pradeepkumar and Ravi, 2018) the review on soft computing hybrid models for FOREX rate prediction.They categorized the soft computing hybrid models in to ANN based, evolutionary computation based, fuzzy logic and SVM based hybrid models.The final conclusion of this study is that ANN-based hybrids are more pervasive and more accurate hybrid techniques in the pertinent area.Rather et al. (2017) surveyed important published papers for stock market forecasting and portfolio selection.They divided reviewed articles in to two general classes including single and hybrid models.Antonanzas et al. (2016) reviewed the different techniques to generate power forecasts for photovoltaics.The hybrid models surveyed in this study followed two hybridization approaches: combining two or more statistical models (statistical hybrid models) and combining statistical models and photovoltaic performance model (physical hybrid model).
The key factor in selecting appropriate forecasting model is accuracy. Given the deficiencies of single models in processing various patterns and relationships latent in data, hybrid approaches have been known as promising techniques to achieve more accurate results for time series modeling and forecasting. Therefore, a rapid development has been evolved in time series forecasting fields in order to access accurate results. While, numerous review papers have been concentrated on the use of hybrid models and their advantages in improving forecasting accuracy versus individual models in wide variety of areas, no study is concerned to categorize and review papers from the structural point of view in numerous developed studies. The main goal of this paper is to analyze hybrid structures by surveying more than 150 papers employed various hybrid models in time series modeling and forecasting domains. In this paper, the classification of hybrid models is made based on three main combination structures: parallel, series, and parallel–series. Then, reviewed papers are analyzed comprehensively with respect to their specific features of employed hybrid structure. Through reviewed articles, it can be observed that combined methods are viable and accurate approaches for time series forecasting and also the parallel–series hybrid structure can obtain more accurate and promising results than other those hybrid structures. Besides this paper provides the viable research directions for each hybrid structure to help the researchers in time series forecasting area. Even though some recent review articles surveyed the specific branch of hybrid models and make classification in the particular fields, none of them pay comprehensively attention to all combining structures in various hybrid time series forecasting models.Therefore, the best of our knowledge is to fill the gap in the literature by reviewing various hybrid models based on the three main structures proposed in the literature: parallel, series and parallel–series regardless of the field of study.In the next phase, each research study is analyzed based on the specific features of used combination structure.Two highlighted goals followed in this review paper are:
In this paper, a diagnostic evaluation of the state of the art of archaeological waterlogged foundation piles in Riga Cathedral (1211 CE) was carried out. Microscopic, chemical and instrumental methods were applied to study the impact of deterioration of piles leading to the deformations of the Cathedral building. Severe biodeterioration by microorganisms in the majority of pile samples was determined. Chemical analyses showed an extensive depletion of hemicellulose and cellulose sugars while lignin seemed to be unaffected. Restricted degradation of hemicellulose sugars –arabinose and galactose – was characteristic of bacterial degradation of wood. FTIR spectroscopy proved itself as a quick and efficient method for determination of changes in wood components in foundation piles in comparison with chemical method. The increased ash content (up to 70%) in waterlogged wood consisted of deposited salts, oxides and other inorganic compounds. The X-ray diffraction method determined the main inorganic impurities, including calcite, quartz, sodium magnesium silicate and muscovite. The historic centre of Riga is included in the UNESCO World Heritage list.Riga Cathedral (1211) is one of the oldest sacred buildings of the medieval period in Latvia and also the Baltics.In recent years, deformations (cracks) have been observed in a number of historic buildings, including the Cathedral.Those buildings are supported by wooden foundation piles.Extensive research work with many expert institutions has been carried out to understand the reasons for the deterioration of Cathedral constructions.It is found that the mortar foundation is stable but the ground complex underneath, containing wooden piles and ground material, could be the main reason for super-normative deformations.The piles are situated under permanent groundwater; however, because of water level fluctuations, the upper parts of piles could be exposed above the water for longer periods.The main geotechnical problem is the declined load bearing of damaged piles which is unpredictable and can lead to an increased construction's settlement in future.
In this paper, a diagnostic evaluation of the state of the art of archaeological waterlogged foundation piles in Riga Cathedral (1211 CE) was carried out. Microscopic, chemical and instrumental methods were applied to study the impact of deterioration of piles leading to the deformations of the Cathedral building. Severe biodeterioration by microorganisms in the majority of pile samples was determined. Chemical analyses showed an extensive depletion of hemicellulose and cellulose sugars while lignin seemed to be unaffected. Restricted degradation of hemicellulose sugars –arabinose and galactose – was characteristic of bacterial degradation of wood. FTIR spectroscopy proved itself as a quick and efficient method for determination of changes in wood components in foundation piles in comparison with chemical method. The increased ash content (up to 70%) in waterlogged wood consisted of deposited salts, oxides and other inorganic compounds. The X-ray diffraction method determined the main inorganic impurities, including calcite, quartz, sodium magnesium silicate and muscovite. Wooden foundation piles have been used to stabilise urban settlements for thousands of years.Depending on the local soil conditions and the building above, pile foundations differ in construction type, pile length, timber species and timber quality applied, and the degree of conservation.It is estimated that millions of wooden foundation piles are still in service, carrying small buildings like family houses, or bigger buildings like churches and palaces or constructions in water-like quay walls or bridges.Many of these buildings are old and therefore wooden foundations are an important asset for cultural heritage.This is not always realised probably because foundations are hidden in the soil and therefore not visual as part of the building and because the wooden pile has been replaced by concrete since the 1950s (Klaassen and Creemers, 2012).A number of these buildings have suffered from structural instability because their foundations have lost supporting strength.The strength loss is associated with the activity of bacterial degradation that can colonise and destroy wood under low-oxygen conditions below the groundwater level (Huisman et al., 2008).Studies on the microbial decay of foundation piles and archaeological wood from sites in the Netherlands (Klaassen, 2008a), Germany, Italy (Huisman et al., 2008), the Mediterranean Sea area (Palla et al., 2013) and Sweden (Björdal, 2012) have been carried out.
In this paper, a diagnostic evaluation of the state of the art of archaeological waterlogged foundation piles in Riga Cathedral (1211 CE) was carried out. Microscopic, chemical and instrumental methods were applied to study the impact of deterioration of piles leading to the deformations of the Cathedral building. Severe biodeterioration by microorganisms in the majority of pile samples was determined. Chemical analyses showed an extensive depletion of hemicellulose and cellulose sugars while lignin seemed to be unaffected. Restricted degradation of hemicellulose sugars –arabinose and galactose – was characteristic of bacterial degradation of wood. FTIR spectroscopy proved itself as a quick and efficient method for determination of changes in wood components in foundation piles in comparison with chemical method. The increased ash content (up to 70%) in waterlogged wood consisted of deposited salts, oxides and other inorganic compounds. The X-ray diffraction method determined the main inorganic impurities, including calcite, quartz, sodium magnesium silicate and muscovite. Degradation of wood cell walls by bacteria was confirmed only in the 1980s.Using light and electron microscopy, isolated bacterial consortia were observed, degrading lignified wood cells (Daniel and Nilsson, 1986).Two main groups of wood-degrading bacteria are erosion bacteria (EB) and tunnelling bacteria (TB) (Nilsson and Daniel, 1992).EB represent the dominant form of attack in waterlogged wood and contribute significantly to the deterioration of wooden foundation piles and waterlogged archaeological wood.EB tolerate near anaerobic or fully anoxic environments, whereas TB appear to have a higher demand for oxygen (Björdal et al., 1999).Modern molecular DNA technology results show that sequences representing bacteria from waterlogged archaeological wood belong to the Cytophaga-Flavobacterium-Bacteroides (CFB) complex and the Pseudomonas group, with relatives of the Cellvibrio and Brevundimonas groups also present (Landy et al., 2008).As a result of degradation by EB, wet wood is damaged easily and is also susceptible to damage and deformation during drying out.Sites where oxygen was available showed evidence of degradation by soft rot (SR) fungi (Huisman et al., 2008).SR fungi are present in aquatic and terrestrial environments in large numbers.They are considered to have lower requirements for oxygen than basidiomycetes, which are rare in aquatic environments (Eaton and Hale, 1993).
This study represents the first starch grain analysis undertaken in Palau, performed on a sediment core extracted from a sinkhole on Ulong Island. Radiocarbon dating indicates the core spans the likely period of human occupation on Ulong (ca. 3000 years) as established by prior archaeological evidence. Samples were analysed for macrocharcoal, starch content, and geochemical composition. The results of the analyses indicate an initial period of intensive clearance and gardening from ca. 3000–2000 BP, during which banana (Musa spp.), yams (Dioscorea spp.), Polynesian arrowroot (Tacca leontopetaloides), Tahitian chestnut (Inocarpus fagifer), and breadfruit (Artocarpus sp.) were being utilised and/or cultivated. This initial phase was then followed by a period of reduced and stabilized gardening activity until ca. 1000 BP, during which banana (Musa spp.) disappears from the starch record. The period after 1000 BP represents the transition between the first permanent settlements on Ulong, abandoned between 500 and 300 BP, and the arrival of Europeans in 1783. This period is marked by a dearth of charcoal indicating the absence of significant burning, as well as a decrease in the variety of starch grains from cultigens. Starch grain analysis in the Pacific Islands
This study represents the first starch grain analysis undertaken in Palau, performed on a sediment core extracted from a sinkhole on Ulong Island. Radiocarbon dating indicates the core spans the likely period of human occupation on Ulong (ca. 3000 years) as established by prior archaeological evidence. Samples were analysed for macrocharcoal, starch content, and geochemical composition. The results of the analyses indicate an initial period of intensive clearance and gardening from ca. 3000–2000 BP, during which banana (Musa spp.), yams (Dioscorea spp.), Polynesian arrowroot (Tacca leontopetaloides), Tahitian chestnut (Inocarpus fagifer), and breadfruit (Artocarpus sp.) were being utilised and/or cultivated. This initial phase was then followed by a period of reduced and stabilized gardening activity until ca. 1000 BP, during which banana (Musa spp.) disappears from the starch record. The period after 1000 BP represents the transition between the first permanent settlements on Ulong, abandoned between 500 and 300 BP, and the arrival of Europeans in 1783. This period is marked by a dearth of charcoal indicating the absence of significant burning, as well as a decrease in the variety of starch grains from cultigens. The analysis of starch grains preserved in sediments as a means of palaeoenvironmental reconstruction is a relatively new but potentially important technique (Lentfer et al., 2002).Exploratory studies have shown that starch grains preserve particularly well in sediments from tropical environments, and they have been utilised successfully for palaeoenvironmental reconstruction in the Pacific Islands (e.g., Denham et al., 2003; Fullagar et al., 2006; Horrocks and Nunn, 2007; Horrocks and Rechtman, 2009; Horrocks and Weisler, 2006; Lentfer et al., 2002).This study applies, for the first time, starch grain and geochemical analyses of ponded sediments from Palau as proxies to better understand human arrival and environmental changes in this archipelago.
This study represents the first starch grain analysis undertaken in Palau, performed on a sediment core extracted from a sinkhole on Ulong Island. Radiocarbon dating indicates the core spans the likely period of human occupation on Ulong (ca. 3000 years) as established by prior archaeological evidence. Samples were analysed for macrocharcoal, starch content, and geochemical composition. The results of the analyses indicate an initial period of intensive clearance and gardening from ca. 3000–2000 BP, during which banana (Musa spp.), yams (Dioscorea spp.), Polynesian arrowroot (Tacca leontopetaloides), Tahitian chestnut (Inocarpus fagifer), and breadfruit (Artocarpus sp.) were being utilised and/or cultivated. This initial phase was then followed by a period of reduced and stabilized gardening activity until ca. 1000 BP, during which banana (Musa spp.) disappears from the starch record. The period after 1000 BP represents the transition between the first permanent settlements on Ulong, abandoned between 500 and 300 BP, and the arrival of Europeans in 1783. This period is marked by a dearth of charcoal indicating the absence of significant burning, as well as a decrease in the variety of starch grains from cultigens. Starch is a complex, insoluble carbohydrate that is the main substance of food storage for plants and is most commonly found in underground stems (i.e. rhizomes and tubers), roots, and seeds.Because their semi-crystalline nature makes them birefringent, an extinction cross is visible in each grain when viewed under cross-polarized light, and the presence of this cross allows starch granules to be differentiated from other microscopic plant fossils with relative ease (Horrocks et al., 2004).
This study represents the first starch grain analysis undertaken in Palau, performed on a sediment core extracted from a sinkhole on Ulong Island. Radiocarbon dating indicates the core spans the likely period of human occupation on Ulong (ca. 3000 years) as established by prior archaeological evidence. Samples were analysed for macrocharcoal, starch content, and geochemical composition. The results of the analyses indicate an initial period of intensive clearance and gardening from ca. 3000–2000 BP, during which banana (Musa spp.), yams (Dioscorea spp.), Polynesian arrowroot (Tacca leontopetaloides), Tahitian chestnut (Inocarpus fagifer), and breadfruit (Artocarpus sp.) were being utilised and/or cultivated. This initial phase was then followed by a period of reduced and stabilized gardening activity until ca. 1000 BP, during which banana (Musa spp.) disappears from the starch record. The period after 1000 BP represents the transition between the first permanent settlements on Ulong, abandoned between 500 and 300 BP, and the arrival of Europeans in 1783. This period is marked by a dearth of charcoal indicating the absence of significant burning, as well as a decrease in the variety of starch grains from cultigens. In the past, starch grains were often assigned to species based on their morphological characteristics, relying upon the visual comparison between individual archaeological granules with reference granules (Wilson et al., 2010).However, this approach is not ideal as the comparison of discrete granules is likely to be unreliable (Torrence et al., 2004).To address this problem, multivariate analysis of data recorded from digital images can be used to construct a system of classification that facilitates the discrimination of the starch grains among different plant types with a high degree of consistency by recording a number of metric and nominal variables (Torrence et al., 2004).However, a potential limitation to this method for analysing starch is the representation of three-dimensional starch granules in a two-dimensional image.Since any species of plant starch will contain a variety of granule shapes, a population approach to the analysis can address this problem, resulting in more reliable starch grain identifications than can be achieved with single-grain identification (Wilson et al., 2010).
This study represents the first starch grain analysis undertaken in Palau, performed on a sediment core extracted from a sinkhole on Ulong Island. Radiocarbon dating indicates the core spans the likely period of human occupation on Ulong (ca. 3000 years) as established by prior archaeological evidence. Samples were analysed for macrocharcoal, starch content, and geochemical composition. The results of the analyses indicate an initial period of intensive clearance and gardening from ca. 3000–2000 BP, during which banana (Musa spp.), yams (Dioscorea spp.), Polynesian arrowroot (Tacca leontopetaloides), Tahitian chestnut (Inocarpus fagifer), and breadfruit (Artocarpus sp.) were being utilised and/or cultivated. This initial phase was then followed by a period of reduced and stabilized gardening activity until ca. 1000 BP, during which banana (Musa spp.) disappears from the starch record. The period after 1000 BP represents the transition between the first permanent settlements on Ulong, abandoned between 500 and 300 BP, and the arrival of Europeans in 1783. This period is marked by a dearth of charcoal indicating the absence of significant burning, as well as a decrease in the variety of starch grains from cultigens. Starch grain analysis is a particularly useful technique in the Pacific region, where research on the age, development, and diversity of early agriculture has been hindered by an archaeological scarcity of crop fossils (Horrocks and Weisler, 2006).Furthermore, archaeological deposits in the Pacific often have limited pollen and phytolith production, and the more robust starch grains are therefore ideal proxies to add to the line of evidence for cultivation and other environmental changes in that region (Horrocks et al., 2004).
Within the evolutionary dynamics of post-Gravettian techno-complexes, one can observe an intense regionalization phenomenon, both on a European scale, with the creation of two main provinces, and within the Italian peninsula. To date, typological studies have led to the recognition of several Italian Epigravettian facies, identifying trends, similarities, and differences in the lithic complexes. An important contribution was made by the technological method which in recent years has allowed us to identify the evolutionary processes of the lithic industries in numerous deposits of northern Italy. It is the intent of this reporting to add information which contributes to the debate on the latest Pleistocene complexes, expanding the area to southern Italy. The Grotta della Serratura, by virtue of a large and well-detailed stratigraphy, the optimum state of conservation of the findings, and for previous interdisciplinary studies that have been undertaken, represents an excellent reference site for the lower Tyrrhenian Italian coast. Thus it begins to bring new technological data to already known collections in an area rich in archaeological evidence that can make important contributions to the discussion of the lithic complexes in the Mediterranean area from the late Upper Paleolithic. The paleogeographic changes which followed the last glacial maximum raised natural barriers and boundaries that probably form the basis of the division into two large post-Gravettian techno-complexes: that of central-western Europe, where the chrono-cultural unity of the Solutrean, Badegoulian, Magdalenian, Epimaddalenian/Azilian are developed, and the eastern and Mediterranean Europe, defined on a techno-typological and chrono-cultural scale as “Epigravettian province”.
Within the evolutionary dynamics of post-Gravettian techno-complexes, one can observe an intense regionalization phenomenon, both on a European scale, with the creation of two main provinces, and within the Italian peninsula. To date, typological studies have led to the recognition of several Italian Epigravettian facies, identifying trends, similarities, and differences in the lithic complexes. An important contribution was made by the technological method which in recent years has allowed us to identify the evolutionary processes of the lithic industries in numerous deposits of northern Italy. It is the intent of this reporting to add information which contributes to the debate on the latest Pleistocene complexes, expanding the area to southern Italy. The Grotta della Serratura, by virtue of a large and well-detailed stratigraphy, the optimum state of conservation of the findings, and for previous interdisciplinary studies that have been undertaken, represents an excellent reference site for the lower Tyrrhenian Italian coast. Thus it begins to bring new technological data to already known collections in an area rich in archaeological evidence that can make important contributions to the discussion of the lithic complexes in the Mediterranean area from the late Upper Paleolithic. Originally defined by the Italian industries (Laplace, 1964), the Epigravettian includes the lithic complexes subsequent to the Gravettian; over time this taxonomical unity has been identified in an increasingly broader area, which now stretches from Provence to Armenia (Montet-White and Kozlowski, 1983; Montet-White, 1996; Djindjan et al., 1999; Kozlowski and Kaczanowska, 2004; Mihailović and Mihailović, 2007; Montoya et al., 2013; Tomasso, 2014).
Within the evolutionary dynamics of post-Gravettian techno-complexes, one can observe an intense regionalization phenomenon, both on a European scale, with the creation of two main provinces, and within the Italian peninsula. To date, typological studies have led to the recognition of several Italian Epigravettian facies, identifying trends, similarities, and differences in the lithic complexes. An important contribution was made by the technological method which in recent years has allowed us to identify the evolutionary processes of the lithic industries in numerous deposits of northern Italy. It is the intent of this reporting to add information which contributes to the debate on the latest Pleistocene complexes, expanding the area to southern Italy. The Grotta della Serratura, by virtue of a large and well-detailed stratigraphy, the optimum state of conservation of the findings, and for previous interdisciplinary studies that have been undertaken, represents an excellent reference site for the lower Tyrrhenian Italian coast. Thus it begins to bring new technological data to already known collections in an area rich in archaeological evidence that can make important contributions to the discussion of the lithic complexes in the Mediterranean area from the late Upper Paleolithic. The Epigravettian in Italy was initially characterized by means of typological studies that established a chronological and structural framework, which are in part still valid today (Laplace, 1964; Palma di Cesnola, 1983, 1993; Bietti, 1997, Bietti et al., 1983; Martini, 1993; Broglio, 1997; Dini and Tozzi, 2005; Martini and Martino, 2005; Bertola et al., 2007; Martini et al., 2007).The Epigravettian, with a chronological division into three phases, early (approximately 20,000–16,000 bp), evolved (approximately 16,000 to 15,000/14,500 bp) and final (approximately 15,000/14.500–10,000 bp), in turn divided into subphases on the basis of typological structures, show the extensive variability and configuration of the techno-complexes that compose it.
Within the evolutionary dynamics of post-Gravettian techno-complexes, one can observe an intense regionalization phenomenon, both on a European scale, with the creation of two main provinces, and within the Italian peninsula. To date, typological studies have led to the recognition of several Italian Epigravettian facies, identifying trends, similarities, and differences in the lithic complexes. An important contribution was made by the technological method which in recent years has allowed us to identify the evolutionary processes of the lithic industries in numerous deposits of northern Italy. It is the intent of this reporting to add information which contributes to the debate on the latest Pleistocene complexes, expanding the area to southern Italy. The Grotta della Serratura, by virtue of a large and well-detailed stratigraphy, the optimum state of conservation of the findings, and for previous interdisciplinary studies that have been undertaken, represents an excellent reference site for the lower Tyrrhenian Italian coast. Thus it begins to bring new technological data to already known collections in an area rich in archaeological evidence that can make important contributions to the discussion of the lithic complexes in the Mediterranean area from the late Upper Paleolithic. The appearance of the regionalization phenomenon that burst forth during the Late Glacial in Italy is probably due to paleoenvironmental changes that have, at least in part, influenced the communication between human groups, who, by the time they settled into the different ecological niches in which they lived, gave rise to differentiated technical complexes.This is why the attribution of “environmental wisdom” (Martini, 2015, p. 73), or the human capacity to interact with -a given territory, without adapting passively, but rather exploiting the resources and fully integrating, has determined that regionalization is reflected in the different industrial facies of Laplace (Laplace, 1964a, 1966).However, within this “mosaic of cultures” common trends can be identified such as the microlithic progression of stone tools, certain stylistic and structural parameters, and the circulation of artistic-cultural manifestations, showing generalized trends throughout the peninsula (Palma di Cesnola, 1993).In an attempt to understand the origin, evolution, and spread of these common trends that we find within a clearly indicated differentiation process, technological analytic reseach may provide new results.In fact it is with the technological approach (Leroi-Gourhan, 1943, 1945; Lemonnier, 1976; Cresswell, 1983; Inizan et al., 1995), which has, at least in part, redefined the serialization of the Italian Epigravettian (Montoya, 2004, 2008b; Montoya, 2008a; Montoya and Peresani, 2005; Bertola et al., 2007; Tozzi and Dini, 2007; Cancellieri, 2010; Serradimigni, 2013; Tomasso et al., 2014).Montoya (2004) proposed a new sequence for the early Epigravettian of northern Italy, subdividing it into three phases (ER1, ER2, ER3), based on the objectives of the lithic production and the methods adopted to achieve them.These are organized in a chronological scheme that has recently been confirmed and completed by other studies concerning the Ligurian-Provencal arc and northern Tuscany (Tomasso, 2014).
Residue analysis can be a useful way to determine the past functions of archaeological tools, particularly when teamed with other functional investigations such as usewear and technological analyses. The most common approach for residue analysis is through the use of optical microscopes, which can be used to visually identify in situ residues directly from the tool surface (with reflected light microscopes, RLM), as well as in water extractions sampled from the utilised tool edges (with transmitted light microscopes, TLM). Recently, the scanning electron microscope with energy dispersive X-ray spectroscopy (SEM-EDS) has shown great potential for archaeological residue analysis as it can provide high-resolution images at very high magnifications as well as elemental analysis of adhering material. An advantage of this instrument is that it is capable of operating in low vacuum or “environmental” mode, allowing specimens to be examined uncoated and without additional preparation, so that residues can be documented/analysed in situ on the stone. In this paper, we propose a sequential protocol for the identification of tool residues using various optical light microscopes in combination with the SEM-EDS, on residues documented both in situ on stone tools and those removed from the stone substrate in solvent extractions. We also propose a new method for analysing extracted residues using the SEM-EDS that permits high resolution images of micro-residues, particularly starch and other fibres. We argue that, although both methods have limitations and instrumental challenges, when used in combination, provide a complementary means for documenting tool residues. The identification and characterisation of tool residues makes an important contribution to studies of functional analysis and the comprehension of past tool use activities.While usewear analysis allows identifying the use motion of the stone tool and the worked material (e.g. bone, wood, antler, hide, pigments, etc.), the correct identification of a residue can potentially provide more specific information about the processed material, including the plant/animal taxa/species, or the geological origin of coloured pigments, on the condition that the functional cause of residue deposition is correctly identified (Rots et al., 2016).
Residue analysis can be a useful way to determine the past functions of archaeological tools, particularly when teamed with other functional investigations such as usewear and technological analyses. The most common approach for residue analysis is through the use of optical microscopes, which can be used to visually identify in situ residues directly from the tool surface (with reflected light microscopes, RLM), as well as in water extractions sampled from the utilised tool edges (with transmitted light microscopes, TLM). Recently, the scanning electron microscope with energy dispersive X-ray spectroscopy (SEM-EDS) has shown great potential for archaeological residue analysis as it can provide high-resolution images at very high magnifications as well as elemental analysis of adhering material. An advantage of this instrument is that it is capable of operating in low vacuum or “environmental” mode, allowing specimens to be examined uncoated and without additional preparation, so that residues can be documented/analysed in situ on the stone. In this paper, we propose a sequential protocol for the identification of tool residues using various optical light microscopes in combination with the SEM-EDS, on residues documented both in situ on stone tools and those removed from the stone substrate in solvent extractions. We also propose a new method for analysing extracted residues using the SEM-EDS that permits high resolution images of micro-residues, particularly starch and other fibres. We argue that, although both methods have limitations and instrumental challenges, when used in combination, provide a complementary means for documenting tool residues. Past experimental work has indicated that intact residues, when present in large quantities, may be characterised in situ on stone tools by their morphological features when viewed with reflected light microscopes (RLMs), including stereomicroscopes with external light sources and high magnification vertical incident-light microscopes equipped with various polarising filters (e.g. Wadley et al., 2004; Langejans, 2012; Rots et al., 2016; Pedergnana and Ollé, 2018).Other experiments have shown that macerated or smeared residues, which may be more analogous to what one may encounter archaeologically, are more difficult to identify on stone tools using optical microscopy alone (e.g. Croft et al., 2016, 2018; Lombard and Wadley, 2007; Monnier et al., 2012).To accurately identify these physically altered residues, additional methods and instruments might be required, including those that provide enhanced visualisation of the residue on the stone tool (e.g. scanning electron microscope, SEM) or elemental/molecular residue characterisation (e.g. SEM with energy dispersive X-ray spectroscopy (EDS), Attenuated Total Reflectance – Fourier-transform infrared (ATR-FTIR) and Raman spectroscopes).ATR-FTIR and Raman spectroscopes are used to quantify residues by means of spectroscopy, where the measured substance is stimulated with a focused laser beam (in the case of Raman spectroscopy) and infrared radiation passed through a dispersing element prism made from a single crystal (e.g. diamond) (in the case of ATR-FTIR) to generate a “fingerprint” spectrum of the measured sample, which may then be compared and identified using reference spectra libraries.Elemental data may also be gathered using the SEM-EDS, which is also capable of producing high quality images at very high magnifications, much higher than that of any optical microscope.Using this approach, analysts have successfully characterised archaeological residues on stone artefacts (e.g. Anderson, 1980; Anderson-Gerfaud, 1990; Cârciumaru et al., 2012; Cristiani et al., 2009; Helwig et al., 2014; Monnier et al., 2013; Smith et al., 2015; Vergès and Ollé, 2011) and in dental calculus (e.g. Power et al., 2014); confirmed the organic nature of possible glue remains (e.g. Bradtmöller et al., 2016; Dinnis et al., 2009; Pawlik and Thissen, 2011; Pawlik, 2004), and characterised adhesives on an antler ornament (Ruminski and Osipowicz, 2014).The instrument works by scanning the surface of a specimen with a focussed beam of electrons that interact with the atoms in the sample to produce an image and the compositional information of the sample being viewed.Adhering residues that are composed of elements with higher average atomic weights appear lighter/brighter than those residues with lower average atomic weights.Organic residues (made up predominately of carbon and oxygen bonds) will appear dark on stone tools and are therefore easily distinguished on a stone substrate.Historically, the use of the SEM required a sample to be covered in an electrically conductive coating (typically carbon, platinum or gold) for optimal imaging and analysis.More recently, the use of the variable pressure “environmental” SEM has been used to analyse uncoated samples in low vacuum mode (typically using a backscattered electron detector, BED), eliminating the need for such coating and thus making it more attractive to residue analysts who wish to undertake residue examination in situ.
Residue analysis can be a useful way to determine the past functions of archaeological tools, particularly when teamed with other functional investigations such as usewear and technological analyses. The most common approach for residue analysis is through the use of optical microscopes, which can be used to visually identify in situ residues directly from the tool surface (with reflected light microscopes, RLM), as well as in water extractions sampled from the utilised tool edges (with transmitted light microscopes, TLM). Recently, the scanning electron microscope with energy dispersive X-ray spectroscopy (SEM-EDS) has shown great potential for archaeological residue analysis as it can provide high-resolution images at very high magnifications as well as elemental analysis of adhering material. An advantage of this instrument is that it is capable of operating in low vacuum or “environmental” mode, allowing specimens to be examined uncoated and without additional preparation, so that residues can be documented/analysed in situ on the stone. In this paper, we propose a sequential protocol for the identification of tool residues using various optical light microscopes in combination with the SEM-EDS, on residues documented both in situ on stone tools and those removed from the stone substrate in solvent extractions. We also propose a new method for analysing extracted residues using the SEM-EDS that permits high resolution images of micro-residues, particularly starch and other fibres. We argue that, although both methods have limitations and instrumental challenges, when used in combination, provide a complementary means for documenting tool residues. Although the use of the ATR-FTIR and Raman microscope has been shown to be successful for the identification of residues on stone tools on both experimental and archaeological artefacts (e.g. Blee et al., 2010; Bordes et al., 2017, 2018; Monnier et al., 2017a, 2017b; Wojcieszak and Wadley, 2018), most analysts do not have access to such equipment, nor do they possess the expert skills to use the instruments and interpret the data.Most residue analyses are therefore limited to the use of optical microscopes and less commonly, SEMs for in situ examination.When residues are removed from the stone surface, they can also be characterised using other optical microscopes such as the transmitted light microscope (TLM).Residues are usually removed from the stone by suspending adhering material in an aqueous solution that can then be prepared on a glass slide and examined under transmitted light using a range of magnifications (usually up to ×1000) (e.g. Fullagar, 2014; Cnuts and Rots, 2017).Residues are typically easier to characterise once isolated from the stone substrate as they can be examined flat (rather than on an uneven stone surface) enabling their morphological features (and optical properties) to become easier to document, measure and photograph in focus.Additionally, residues can be manipulated on glass slides, viewed in three-dimensions (e.g. various granules may be rotated), and stained to further identify a residue that may be mechanically damaged or otherwise unrecognisable based on their morphological features alone (see Fullagar et al., 2015; Rots et al., 2016).Finally, residues residing in cracks, scars or other flaws within a stone surface that are otherwise “hidden” on the stone when examined in situ with RLM can be extracted and potentially identified.
Residue analysis can be a useful way to determine the past functions of archaeological tools, particularly when teamed with other functional investigations such as usewear and technological analyses. The most common approach for residue analysis is through the use of optical microscopes, which can be used to visually identify in situ residues directly from the tool surface (with reflected light microscopes, RLM), as well as in water extractions sampled from the utilised tool edges (with transmitted light microscopes, TLM). Recently, the scanning electron microscope with energy dispersive X-ray spectroscopy (SEM-EDS) has shown great potential for archaeological residue analysis as it can provide high-resolution images at very high magnifications as well as elemental analysis of adhering material. An advantage of this instrument is that it is capable of operating in low vacuum or “environmental” mode, allowing specimens to be examined uncoated and without additional preparation, so that residues can be documented/analysed in situ on the stone. In this paper, we propose a sequential protocol for the identification of tool residues using various optical light microscopes in combination with the SEM-EDS, on residues documented both in situ on stone tools and those removed from the stone substrate in solvent extractions. We also propose a new method for analysing extracted residues using the SEM-EDS that permits high resolution images of micro-residues, particularly starch and other fibres. We argue that, although both methods have limitations and instrumental challenges, when used in combination, provide a complementary means for documenting tool residues. The extraction of residues is not yet standard procedure in residue analysis.Langejans (2011) has argued that it gives a biased interpretation of the worked material as only a portion of the residues are extracted, resulting in an incomplete view of what is found on the stone tool.A more recent study, however, has challenged this view by demonstrating that most residue types can be successfully recovered if an adequate extraction technique is chosen (Cnuts and Rots, 2017).Water is usually sufficient for extracting most residues.Residues that are extracted via pipette are dislodged through gentle agitation of the pipette tip, which then become suspended in the fluid medium before removal with the pipette and prepared on a glass slide.The water will not cause damage to residues unless the residue is soluble in water (such as plant gums).Similarly, other solvents (such as ethanol, acetonitrile, chlorofom and hexane) should only cause damage to residues that are soluble in that particular substance (for example, beeswax is soluble in chloroform).Other damage to residues during sampling may result from overly aggressive removal techniques, such as damage from scratching with the pipette tip, or mechanical damage resulting in the break-up of fragile residues during sonication (Cnuts and Rots, 2017).
Residue analysis can be a useful way to determine the past functions of archaeological tools, particularly when teamed with other functional investigations such as usewear and technological analyses. The most common approach for residue analysis is through the use of optical microscopes, which can be used to visually identify in situ residues directly from the tool surface (with reflected light microscopes, RLM), as well as in water extractions sampled from the utilised tool edges (with transmitted light microscopes, TLM). Recently, the scanning electron microscope with energy dispersive X-ray spectroscopy (SEM-EDS) has shown great potential for archaeological residue analysis as it can provide high-resolution images at very high magnifications as well as elemental analysis of adhering material. An advantage of this instrument is that it is capable of operating in low vacuum or “environmental” mode, allowing specimens to be examined uncoated and without additional preparation, so that residues can be documented/analysed in situ on the stone. In this paper, we propose a sequential protocol for the identification of tool residues using various optical light microscopes in combination with the SEM-EDS, on residues documented both in situ on stone tools and those removed from the stone substrate in solvent extractions. We also propose a new method for analysing extracted residues using the SEM-EDS that permits high resolution images of micro-residues, particularly starch and other fibres. We argue that, although both methods have limitations and instrumental challenges, when used in combination, provide a complementary means for documenting tool residues. We have argued elsewhere that residue extraction and examination under the TLM strongly enhances analyst ability to adequately identify residues, particularly when stains are used to differentiate between morphologically indiscrete residues (see Hayes et al., 2017; Rots et al., 2016).Researchers who claim that in situ analyses using RLMs and/or the SEM-EDS are sufficient for residue identification have mostly only been able to demonstrate its effectiveness through limited experimental data sets in which residues were observed under pristine conditions, i.e. in fresh, non-degraded state, often in large enough accumulations that they were macroscopically visible and uncomplicated by contaminant sediments that one may expect on buried archaeological tools (e.g. Lynch and Miotti, 2017; Monnier et al., 2012; Pedergnana and Ollé, 2018).While these studies were important for the initial documentation of intact, non-damaged residues; such residues (apart from those derived from modern contamination) are often not documented on archaeological tools.Consequently, whether or not such instruments are sufficient for identifying cultural residues in situ under circumstances where residues may be highly degraded, damaged, scarce, or obscured by sediment, is yet to be adequately demonstrated.
Residue analysis can be a useful way to determine the past functions of archaeological tools, particularly when teamed with other functional investigations such as usewear and technological analyses. The most common approach for residue analysis is through the use of optical microscopes, which can be used to visually identify in situ residues directly from the tool surface (with reflected light microscopes, RLM), as well as in water extractions sampled from the utilised tool edges (with transmitted light microscopes, TLM). Recently, the scanning electron microscope with energy dispersive X-ray spectroscopy (SEM-EDS) has shown great potential for archaeological residue analysis as it can provide high-resolution images at very high magnifications as well as elemental analysis of adhering material. An advantage of this instrument is that it is capable of operating in low vacuum or “environmental” mode, allowing specimens to be examined uncoated and without additional preparation, so that residues can be documented/analysed in situ on the stone. In this paper, we propose a sequential protocol for the identification of tool residues using various optical light microscopes in combination with the SEM-EDS, on residues documented both in situ on stone tools and those removed from the stone substrate in solvent extractions. We also propose a new method for analysing extracted residues using the SEM-EDS that permits high resolution images of micro-residues, particularly starch and other fibres. We argue that, although both methods have limitations and instrumental challenges, when used in combination, provide a complementary means for documenting tool residues. Relatively few studies have been published that investigate the effects of burial on stone tools and how degradation and contaminant particles may alter/obscure the physical appearance of residues under RLMs (e.g. Wadley et al., 2004; Langejans, 2011; Rots et al., 2016) and the SEM-EDS (e.g. Croft et al., 2016; Hayes and Rots, in press).In these studies, authors agree that residue identification becomes significantly more challenging in these situations.Picture reference libraries of degraded or otherwise altered residues will be highly important for the accurate identification of archaeological residues.Several publications include of picture catalogues and EDS spectra of altered residues that provide a useful resource (e.g. Croft et al., 2016, 2018; Hayes and Rots, in press; Jahren et al., 1997).However, an adequate methodology for studying and identifying such residues is yet to be described.
The task of learning behaviors of dynamical systems heavily involves time series analysis. Most often, to set up a classification problem, the analysis in time is seen as the main and most natural option. In general, working in the time domain entails a manual, time-consuming phase dealing with signal processing, features engineering and selection processes. Extracted features may also lead to a final result that is heavily dependent of subjective choices, making it hard to state whether the current solution is optimal under any perspective. In this work, leveraging a recent proposal to use the cepstrum as a frequency-based learning framework for time series analysis, we show how such an approach can handle classification with multiple input signals, combining them to yield very accurate results. Notably, the approach makes the whole design flow automatic, freeing it from the cumbersome and subjective step of handcrafting and selecting the most effective features. The method is validated on experimental data addressing the automatic classification of whether a car driver is using the smartphone while driving. In the analysis of dynamical systems, time plays a key role.In fact, all the related signals evolve over time and any information that has to be inferred from data, ranging from mathematical models, to fault isolation, to systems use mode detection, can be considered as the result of a learning process with time series as predictors.
The task of learning behaviors of dynamical systems heavily involves time series analysis. Most often, to set up a classification problem, the analysis in time is seen as the main and most natural option. In general, working in the time domain entails a manual, time-consuming phase dealing with signal processing, features engineering and selection processes. Extracted features may also lead to a final result that is heavily dependent of subjective choices, making it hard to state whether the current solution is optimal under any perspective. In this work, leveraging a recent proposal to use the cepstrum as a frequency-based learning framework for time series analysis, we show how such an approach can handle classification with multiple input signals, combining them to yield very accurate results. Notably, the approach makes the whole design flow automatic, freeing it from the cumbersome and subjective step of handcrafting and selecting the most effective features. The method is validated on experimental data addressing the automatic classification of whether a car driver is using the smartphone while driving. Modeling from time series has been the objective of system identification for more than 70 years so far, Ljung (1999).However, in fault or use-mode detection, the learning task can be more properly formulated as a classification problem and such an issue has been less deeply investigated within the control community, due to the categorical nature of the final result which is opposed to a general continuous output of control processes.Time series classification has been widely studied for problems related to fault detection in dynamic system (Fassois and Sakellariou, 2006; Staszewski et al., 1997), but contributions can be found also in health-monitoring (Kampouraki et al., 2008; Begum et al., 2014) and predictive maintenance (Susto et al., 2014; Pedregal et al., 2004).These classification algorithms have been used not only in the most traditional engineering fields, but also in more broad sense for anomaly detection, prediction and forecasts problems such as, e.g., economics (Detotto and Otranto, 2012), finance (Heaton et al., 2017), climate monitoring (Zhang et al., 2016).As the interested range of applications has spiked with the introduction of new technologies that benefit from a digital description, the problem of classifying structured time signals has increased and a large number of innovative time series classification algorithms have been proposed in the data-mining literature in the last years, see, e.g., Bagnall et al. (2017).
The task of learning behaviors of dynamical systems heavily involves time series analysis. Most often, to set up a classification problem, the analysis in time is seen as the main and most natural option. In general, working in the time domain entails a manual, time-consuming phase dealing with signal processing, features engineering and selection processes. Extracted features may also lead to a final result that is heavily dependent of subjective choices, making it hard to state whether the current solution is optimal under any perspective. In this work, leveraging a recent proposal to use the cepstrum as a frequency-based learning framework for time series analysis, we show how such an approach can handle classification with multiple input signals, combining them to yield very accurate results. Notably, the approach makes the whole design flow automatic, freeing it from the cumbersome and subjective step of handcrafting and selecting the most effective features. The method is validated on experimental data addressing the automatic classification of whether a car driver is using the smartphone while driving. As better illustrated in Section 2.1, the research community is constantly seeking for improvements of what currently represents the state of the art, though the performance of some these algorithms are difficult to beat.For instance, authors of Tran et al. (2019) have recently proposed an innovative approach based on the widely used pair 1-Nearest Neighbor and Dynamic Time Warping algorithms, improving what is already known to outperform most of other time series classification algorithms.Furthermore, time series classification becomes even more challenging when dealing with multivariate time series.In fact, as discussed in Wang et al. (2016) and its references, many attribute-value representation methods (i.e., methods that extracts a set of attributes from the data set, reducing the problem dimensionality and/or the length of the time series) are extensively discussed in the literature, though “these approaches may require in-depth domain knowledge for designing, may be labor intensive, and/or time consuming”, as remarked in Kadous and Sammut (2005).An attempt to solve this problem is proposed by the authors of Wang et al. (2016), in which they combine the benefits of recurrent neural networks (RNN) and adaptive differential evolution (ADE) algorithms, showing promising results, with non negligible shortcomings.
The task of learning behaviors of dynamical systems heavily involves time series analysis. Most often, to set up a classification problem, the analysis in time is seen as the main and most natural option. In general, working in the time domain entails a manual, time-consuming phase dealing with signal processing, features engineering and selection processes. Extracted features may also lead to a final result that is heavily dependent of subjective choices, making it hard to state whether the current solution is optimal under any perspective. In this work, leveraging a recent proposal to use the cepstrum as a frequency-based learning framework for time series analysis, we show how such an approach can handle classification with multiple input signals, combining them to yield very accurate results. Notably, the approach makes the whole design flow automatic, freeing it from the cumbersome and subjective step of handcrafting and selecting the most effective features. The method is validated on experimental data addressing the automatic classification of whether a car driver is using the smartphone while driving. In general, the existing classification methods can be classified in two main classes: model-based and data-driven techniques.Model-based classification algorithms (see, e.g., Bagnall and Janacek, 2014) aim to fit a dynamical model to each time series and then measure the similarity between the original series and the ones generated by their models.Such techniques have shown a great potential against over-fitting and allow comparison among signals of different lengths; however, they usually provide worse overall performance with respect to other classification methods, Bagnall et al. (2017).
The task of learning behaviors of dynamical systems heavily involves time series analysis. Most often, to set up a classification problem, the analysis in time is seen as the main and most natural option. In general, working in the time domain entails a manual, time-consuming phase dealing with signal processing, features engineering and selection processes. Extracted features may also lead to a final result that is heavily dependent of subjective choices, making it hard to state whether the current solution is optimal under any perspective. In this work, leveraging a recent proposal to use the cepstrum as a frequency-based learning framework for time series analysis, we show how such an approach can handle classification with multiple input signals, combining them to yield very accurate results. Notably, the approach makes the whole design flow automatic, freeing it from the cumbersome and subjective step of handcrafting and selecting the most effective features. The method is validated on experimental data addressing the automatic classification of whether a car driver is using the smartphone while driving. Data-driven approaches include a large variety of techniques.In some cases, all the time samples of the series of interest can be taken as predictors (see, e.g., Rath and Manmatha, 2003).Such a naïve solution is intuitive, but usually also computationally intense (and sometimes unfeasible).More frequently, a set of features characterizing the time series (e.g., mean, variance, peak values, just to name a few) are used as predictors to which static classifiers are applied.Even if such approaches are apparently model-free, the choice of the most interesting features to train a classification model is all but straightforward and it usually relies upon some prior knowledge of the system behavior, and in any case introduces a subjective step which at best determines the shape of the final solution, and in the worst can prevents finding a satisfactory answer to the original problem.
The task of learning behaviors of dynamical systems heavily involves time series analysis. Most often, to set up a classification problem, the analysis in time is seen as the main and most natural option. In general, working in the time domain entails a manual, time-consuming phase dealing with signal processing, features engineering and selection processes. Extracted features may also lead to a final result that is heavily dependent of subjective choices, making it hard to state whether the current solution is optimal under any perspective. In this work, leveraging a recent proposal to use the cepstrum as a frequency-based learning framework for time series analysis, we show how such an approach can handle classification with multiple input signals, combining them to yield very accurate results. Notably, the approach makes the whole design flow automatic, freeing it from the cumbersome and subjective step of handcrafting and selecting the most effective features. The method is validated on experimental data addressing the automatic classification of whether a car driver is using the smartphone while driving. In general when dealing with time series data, a frequency-domain analysis is also possible, which allows inferring information on the underlying physical phenomenon by inspecting the frequency ranges on which it acts.In control systems design, analysis in the frequency domain is a must.Inspired by this custom habit, in this work we propose a novel frequency-based approach for classification using time series data, leveraging the computation of the cepstrum coefficients, Boets et al. (2005).We refer to this classifier as fierClass, using the same lexical anagram that links spectrum to cepstrum to connect the terms classifier and fierClass.The proposed approach combines the flexibility of data-driven classifiers, meaning that the model identification phase is not necessary, with features that capture the underlying dynamics in the frequency-domain.
The task of learning behaviors of dynamical systems heavily involves time series analysis. Most often, to set up a classification problem, the analysis in time is seen as the main and most natural option. In general, working in the time domain entails a manual, time-consuming phase dealing with signal processing, features engineering and selection processes. Extracted features may also lead to a final result that is heavily dependent of subjective choices, making it hard to state whether the current solution is optimal under any perspective. In this work, leveraging a recent proposal to use the cepstrum as a frequency-based learning framework for time series analysis, we show how such an approach can handle classification with multiple input signals, combining them to yield very accurate results. Notably, the approach makes the whole design flow automatic, freeing it from the cumbersome and subjective step of handcrafting and selecting the most effective features. The method is validated on experimental data addressing the automatic classification of whether a car driver is using the smartphone while driving. Although the cepstrum was first defined in the signal processing community as early as in 1963 (Bogert et al., 1963), its use for time series clustering and modeling of dynamical systems has been only recently prompted by B. De Moor and coauthors, see, e.g., the recent (Lauwers and De Moor, 2017).More specifically, the proposed cepstrum-based classification has several advantages: first of all it allows removing the subjective (and time-consuming) step of feature engineering and selection, offering as native features the cepstrum coefficients themselves, and their distance, properly defined, as similarity measure to guide the classification process.The analysis of time series in this setting becomes easier and the classification policy straightforward.Additionally, thanks to this mathematical framework, the choice of the most informative frequency range is simply obtained tuning the hyperparameters.Further, thanks to the novel extension presented in this work, a cepstrum-based classifier may be shown to handle a vector-valued input, to deal with applications in which many different sensors provide the time series that can be used as inputs in the classification process, one of the most important problems in multivariate time series classification problems.In fact, we show how the combination of the cepstrum coefficients in the classification problem amounts to the convolution of the original signals in the time domain.
The task of learning behaviors of dynamical systems heavily involves time series analysis. Most often, to set up a classification problem, the analysis in time is seen as the main and most natural option. In general, working in the time domain entails a manual, time-consuming phase dealing with signal processing, features engineering and selection processes. Extracted features may also lead to a final result that is heavily dependent of subjective choices, making it hard to state whether the current solution is optimal under any perspective. In this work, leveraging a recent proposal to use the cepstrum as a frequency-based learning framework for time series analysis, we show how such an approach can handle classification with multiple input signals, combining them to yield very accurate results. Notably, the approach makes the whole design flow automatic, freeing it from the cumbersome and subjective step of handcrafting and selecting the most effective features. The method is validated on experimental data addressing the automatic classification of whether a car driver is using the smartphone while driving. To test the capabilities of the proposed approach within a challenging setting, in this work the multi-signal cepstrum-based classification approach is applied to the problem of automatic detection of phone-usage while driving.This is a complex classification problem, the relevance of which is motivated by the need of enlarging the current driving-style estimation algorithms adding the important information of the amount of time spent using the phone during a journey.Of course, this is a major source of danger, and must thus be considered to form a reliable risk-index of a driver.This is of interest, for example, in the field of insurance telematics, Wahlström et al. (2015) and Li et al. (2016).
The task of learning behaviors of dynamical systems heavily involves time series analysis. Most often, to set up a classification problem, the analysis in time is seen as the main and most natural option. In general, working in the time domain entails a manual, time-consuming phase dealing with signal processing, features engineering and selection processes. Extracted features may also lead to a final result that is heavily dependent of subjective choices, making it hard to state whether the current solution is optimal under any perspective. In this work, leveraging a recent proposal to use the cepstrum as a frequency-based learning framework for time series analysis, we show how such an approach can handle classification with multiple input signals, combining them to yield very accurate results. Notably, the approach makes the whole design flow automatic, freeing it from the cumbersome and subjective step of handcrafting and selecting the most effective features. The method is validated on experimental data addressing the automatic classification of whether a car driver is using the smartphone while driving. To perform such a classification, the time series measured by the smartphone sensors are used, that is three accelerations and three angular velocities.Based on the resulting time series, the novel approach is tested, and compared to a more classical SVM-based classifier.To our best knowledge, this is the first work where cepstrum-based signal processing is used for use mode detection via classification.
The task of learning behaviors of dynamical systems heavily involves time series analysis. Most often, to set up a classification problem, the analysis in time is seen as the main and most natural option. In general, working in the time domain entails a manual, time-consuming phase dealing with signal processing, features engineering and selection processes. Extracted features may also lead to a final result that is heavily dependent of subjective choices, making it hard to state whether the current solution is optimal under any perspective. In this work, leveraging a recent proposal to use the cepstrum as a frequency-based learning framework for time series analysis, we show how such an approach can handle classification with multiple input signals, combining them to yield very accurate results. Notably, the approach makes the whole design flow automatic, freeing it from the cumbersome and subjective step of handcrafting and selecting the most effective features. The method is validated on experimental data addressing the automatic classification of whether a car driver is using the smartphone while driving. The paper shows that the cepstrum-based approach yields results which are directly comparable to the best SVM classifier, with the significant advantage of avoiding the qualitative step of feature selection, which is in general highly problem-dependent, and requires a deep knowledge of the problem domain.Furthermore, such a selection step inevitably adds arbitrariness to the final results, while the proposed cepstrum-based approach is fully automatic and quantitative.Moreover, the multi-signal approach allows one to automatically select which is the best combination of the available signals to be used for the classification task.
Cingulate epilepsy is a rare form of epilepsy. Seizures from the anterior cingulate may present with mood change, fear, hypermotor activity, and autonomic signs, while posterior cingulate seizures resemble temporal lobe seizures. We describe a child with cingulate epilepsy who experienced unpleasant/painful sensory phenomenon. The sensations were described as spiders crawling on his forehead/right leg, ladybugs causing right ear pain and bees stinging his head/right extremities. Unpleasant sensory phenomenon/pain are rarely reported in cingulate epilepsy. Recognizing the role of the cingulate in producing pain/unusual sensory phenomenon is important, and may have localizing value when evaluating children for epilepsy surgery. Cingulate gyrus epilepsy is a rare and diagnostically challenging form of epilepsy, with a myriad of clinical manifestations [1–4].The diagnosis is challenging given the location of the cingulate gyrus, capacity of scalp electroencephalogram (EEG) to record epileptiform discharges and ictal data from this region, as well as its overlap clinically with frontal lobe epilepsy [1,2,4].Semiologically, anterior cingulate seizures are characterized by: fright, vocalizations, hypermotor activity, complex motor manifestations, automatisms, autonomic signs and changes in mood/affect [1–4].Gelastic seizures have also been reported to arise from the anterior cingulate gyrus [5–6].Seizures from the posterior cingulate are less well described and may resemble temporal lobe seizures [1].
Despite the powerful feature extraction capability of Convolutional Neural Networks, there are still some challenges in saliency detection. In this paper, we focus on two aspects of challenges: i) Since salient objects appear in various sizes, using single-scale convolution would not capture the right size. Moreover, using multi-scale convolutions without considering their importance may confuse the model. ii) Employing multi-level features helps the model use both local and global context. However, treating all features equally results in information redundancy. Therefore, there needs to be a mechanism to intelligently select which features in different levels are useful. To address the first challenge, we propose a Multi-scale Attention Guided Module. This module not only extracts multi-scale features effectively but also gives more attention to more discriminative feature maps corresponding to the scale of the salient object. To address the second challenge, we propose an Attention-based Multi-level Integrator Module to give the model the ability to assign different weights to multi-level feature maps. Furthermore, our Sharpening Loss function guides our network to output saliency maps with higher certainty and less blurry salient objects, and it has far better performance than the Cross-entropy loss. For the first time, we adopt four different backbones to show the generalization of our method. Experiments on five challenging datasets prove that our method achieves the state-of-the-art performance. Our approach is fast as well and can run at a real-time speed. Saliency detection in computer vision is the process to determine the most prominent and conspicuous parts of an image.Selective attention is embedded in our cognitive system and a lot of the tasks we do in every day life depend on it.Saliency detection has applications in a variety of supervised and unsupervised tasks (Frintrop and Jensfelt, 2008; Walther and Koch, 2006; Zdziarski and Dahyot, 2012; Bi et al., 2014; Hong et al., 2015; Mahadevan and Vasconcelos, 2009; Ren et al., 2013).For example, salient object detection can provide informative prior knowledge to objectness detection.The extracted bounding box locations which are more prominent and salient in an image would be more likely to contain the objects of interest (Han et al., 2018).Due to this fact, some objectness detection methods use saliency cues to detect objects of interest (Alexe et al., 2012; Erhan et al., 2014).
Despite the powerful feature extraction capability of Convolutional Neural Networks, there are still some challenges in saliency detection. In this paper, we focus on two aspects of challenges: i) Since salient objects appear in various sizes, using single-scale convolution would not capture the right size. Moreover, using multi-scale convolutions without considering their importance may confuse the model. ii) Employing multi-level features helps the model use both local and global context. However, treating all features equally results in information redundancy. Therefore, there needs to be a mechanism to intelligently select which features in different levels are useful. To address the first challenge, we propose a Multi-scale Attention Guided Module. This module not only extracts multi-scale features effectively but also gives more attention to more discriminative feature maps corresponding to the scale of the salient object. To address the second challenge, we propose an Attention-based Multi-level Integrator Module to give the model the ability to assign different weights to multi-level feature maps. Furthermore, our Sharpening Loss function guides our network to output saliency maps with higher certainty and less blurry salient objects, and it has far better performance than the Cross-entropy loss. For the first time, we adopt four different backbones to show the generalization of our method. Experiments on five challenging datasets prove that our method achieves the state-of-the-art performance. Our approach is fast as well and can run at a real-time speed. The traditional computer vision approach to saliency detection is to identify parts of the image that have different contextual information with respect to their surroundings.To identify salient parts of an image, we would require both local and global contextual information.While local contextual features can help to reconstruct the object boundaries, global contextual features are beneficial for getting an abstract description of the salient object.
Despite the powerful feature extraction capability of Convolutional Neural Networks, there are still some challenges in saliency detection. In this paper, we focus on two aspects of challenges: i) Since salient objects appear in various sizes, using single-scale convolution would not capture the right size. Moreover, using multi-scale convolutions without considering their importance may confuse the model. ii) Employing multi-level features helps the model use both local and global context. However, treating all features equally results in information redundancy. Therefore, there needs to be a mechanism to intelligently select which features in different levels are useful. To address the first challenge, we propose a Multi-scale Attention Guided Module. This module not only extracts multi-scale features effectively but also gives more attention to more discriminative feature maps corresponding to the scale of the salient object. To address the second challenge, we propose an Attention-based Multi-level Integrator Module to give the model the ability to assign different weights to multi-level feature maps. Furthermore, our Sharpening Loss function guides our network to output saliency maps with higher certainty and less blurry salient objects, and it has far better performance than the Cross-entropy loss. For the first time, we adopt four different backbones to show the generalization of our method. Experiments on five challenging datasets prove that our method achieves the state-of-the-art performance. Our approach is fast as well and can run at a real-time speed. With the ability of deep learning models in extracting high-level features, some early papers used these models to extract features from candidate image regions at different resolutions to extract local and global representations of the salient objects (Lee et al., 2016; Zhao et al., 2015; Li and Yu, 2015; Wang et al., 2015).Despite their success, due to the use of dense layers, these methods were not very efficient.However, deep neural networks inherently extract increasingly complex features from low-level to high-level and so in recent years, many papers have tried to use features from different levels of abstraction to incorporate low-level features and the more global high-level features.Figuring out how to combine the two information is still an open question.While the conventional way is to concatenate the low-level features with high-level features, and thus treating all feature maps equally, we propose to use an adaptive concatenation functionality where conditioned on the input, the model re-weights the concatenating features.To achieve this purpose, we introduce the Attention-based Multi-level Integrator (AMI) Module, which first weights the concatenated multi-level features by using a Channel Attention (CA) Block, and then it refines the resulted features by using a convolutional layer.Note that the CA block is similar to the recently introduced squeeze and excitation (SE) networks (Hu et al., 2018).
Despite the powerful feature extraction capability of Convolutional Neural Networks, there are still some challenges in saliency detection. In this paper, we focus on two aspects of challenges: i) Since salient objects appear in various sizes, using single-scale convolution would not capture the right size. Moreover, using multi-scale convolutions without considering their importance may confuse the model. ii) Employing multi-level features helps the model use both local and global context. However, treating all features equally results in information redundancy. Therefore, there needs to be a mechanism to intelligently select which features in different levels are useful. To address the first challenge, we propose a Multi-scale Attention Guided Module. This module not only extracts multi-scale features effectively but also gives more attention to more discriminative feature maps corresponding to the scale of the salient object. To address the second challenge, we propose an Attention-based Multi-level Integrator Module to give the model the ability to assign different weights to multi-level feature maps. Furthermore, our Sharpening Loss function guides our network to output saliency maps with higher certainty and less blurry salient objects, and it has far better performance than the Cross-entropy loss. For the first time, we adopt four different backbones to show the generalization of our method. Experiments on five challenging datasets prove that our method achieves the state-of-the-art performance. Our approach is fast as well and can run at a real-time speed. To be able to capture salient object in different sizes, Inception like (Szegedy et al., 2015) modules can be used to extract features at different receptive fields (Szegedy et al., 2015).Previous works concatenate features from different scales which means assigning equal importance to all scales.While such functionality is desirable for applications like image segmentation, for saliency detection we usually consider a single scale as the salient object.The ability to capture the right size for the salient object can be achieved by assigning dynamic weights to the output feature maps of the Inception module, where conditioned on the input image, the model gives different importance to different scales.To achieve this functionality, we introduce a Multi-scale Attention Guided (MAG) Module.By using a novel design, this module first extracts multi-scale features effectively, and then it adaptively gives different importance to different scales by adopting the Channel Attention Block.In Fig. 1, two challenging scenarios of saliency detection are shown.In the first scenario, the salient object is globally distributed over the image.In the second one, the salient object is locally distributed.As seen from Fig. 1, our method (denoted as DFNet-R) is able to handle these challenging scenarios, unlike three recent methods.This functionality is achieved by using MAG Modules in our framework.Therefore, if the salient information is spread globally, the model will give more attention to feature maps from larger kernels.While, if the salient information is spread locally the model will emphasize feature maps of smaller kernels.
Despite the powerful feature extraction capability of Convolutional Neural Networks, there are still some challenges in saliency detection. In this paper, we focus on two aspects of challenges: i) Since salient objects appear in various sizes, using single-scale convolution would not capture the right size. Moreover, using multi-scale convolutions without considering their importance may confuse the model. ii) Employing multi-level features helps the model use both local and global context. However, treating all features equally results in information redundancy. Therefore, there needs to be a mechanism to intelligently select which features in different levels are useful. To address the first challenge, we propose a Multi-scale Attention Guided Module. This module not only extracts multi-scale features effectively but also gives more attention to more discriminative feature maps corresponding to the scale of the salient object. To address the second challenge, we propose an Attention-based Multi-level Integrator Module to give the model the ability to assign different weights to multi-level feature maps. Furthermore, our Sharpening Loss function guides our network to output saliency maps with higher certainty and less blurry salient objects, and it has far better performance than the Cross-entropy loss. For the first time, we adopt four different backbones to show the generalization of our method. Experiments on five challenging datasets prove that our method achieves the state-of-the-art performance. Our approach is fast as well and can run at a real-time speed. In this paper, we propose a Discriminative Feature Extraction and Integration Network, which we refer to as DFNet, consisting of two parts; (i) the Feature Extraction Network and (ii) the Feature Integration Network.In the Feature Extraction Network, by adopting the MAG Modules, we extract dynamically weighted multi-scale features from a pre-trained network at various levels of abstraction.These features are then combined together in the Feature Integration Network by employing the AMI Modules.It is interesting to note that while using a single pre-trained network as the backbone is a common practice in saliency detection, for the first time in the literature, we use four different backbones in our framework to prove the robustness and generalization capability of our method.Furthermore, while the Cross-entropy loss is widely used in the literature, we discover that using this loss function leads to blurry predictions, as we show in the ablation study section.To boost the certainty of our proposed model, we design a Sharpening Loss function, which forces our network to generate sharper predictions.Through experiments, we show that our designed loss outperforms the Cross-entropy loss by a large margin.It is worth mentioning that saliency detection is a pre-processing step for various computer vision tasks.Since our method can run at a real-time speed, it can be practically adopted as a pre-processing step.
Suppose you have an inexpensive plant (process) that you have many copies of, and where each requires feedback control to achieve good closed-loop performance. There is then a feedback controller design challenge for each plant. The traditional approach to such a design is to use linear or nonlinear methods for one copy of the plant and implement the designed controller on each copy of the plant. The hope is that the original design is robust so it performs well on all copies. Here, we assume that the plants can be connected on the internet of things and introduce a novel strategy to design, in a distributed fashion, controllers for each plant where the design of the controller for each plant is informed by the success of the designs of all other controllers. We show that a type of robust controller can be designed over the internet of things. Our methods hold promise in practical commercial applications. The “internet of things” (IoT) is the interaction between devices to sense the environment, interpret the information, and react to real-world events autonomously with or without human intervention over a network (Vermesan and Friess, 2011).It promotes the idea that an increasing number of devices are being connected to the internet which allows more efficient monitoring and control of these devices (Anzelmo et al., 2011).Furthermore, smart systems, like the smart lights studied here, are some of the building blocks for the IoT (Kortuem and Kawsar, 2010).With mobile TCP/IP communication in hand, mobile devices such as those in control systems of automobiles, aircraft, and trains can be connected to the IoT to improve various aspects of travel (Ernst and Uehara, 2002).Automobiles can be connected over the IoT to receive updates to controllers such as ones for climate control or cruise control for optimal performance over all connected vehicles.Also, home appliances, such as refrigerators, ovens, and washing machines, will be able to communicate with each other to reach optimum performance in each home connected to the IoT.
Suppose you have an inexpensive plant (process) that you have many copies of, and where each requires feedback control to achieve good closed-loop performance. There is then a feedback controller design challenge for each plant. The traditional approach to such a design is to use linear or nonlinear methods for one copy of the plant and implement the designed controller on each copy of the plant. The hope is that the original design is robust so it performs well on all copies. Here, we assume that the plants can be connected on the internet of things and introduce a novel strategy to design, in a distributed fashion, controllers for each plant where the design of the controller for each plant is informed by the success of the designs of all other controllers. We show that a type of robust controller can be designed over the internet of things. Our methods hold promise in practical commercial applications. There is a vast list of possible applications which will benefit from the IoT, and, with the growing number of devices connected, a more robust controller of these devices can be designed through a distributed non-gradient optimization method.The growing number of devices connected to the IoT translates to a growing number of controllers connected to the IoT.It is expected that the non-gradient algorithms provide near optimal design for large sets of controllers, such as those prevalent in the IoT.If this expectation is confirmed, it is hoped that the non-gradient algorithms tested here can be adapted to design many feedback control systems in the IoT more complex than the control system tested in this article.
Suppose you have an inexpensive plant (process) that you have many copies of, and where each requires feedback control to achieve good closed-loop performance. There is then a feedback controller design challenge for each plant. The traditional approach to such a design is to use linear or nonlinear methods for one copy of the plant and implement the designed controller on each copy of the plant. The hope is that the original design is robust so it performs well on all copies. Here, we assume that the plants can be connected on the internet of things and introduce a novel strategy to design, in a distributed fashion, controllers for each plant where the design of the controller for each plant is informed by the success of the designs of all other controllers. We show that a type of robust controller can be designed over the internet of things. Our methods hold promise in practical commercial applications. Distributing control systems over a network allows for them to be monitored, accessed, and changed in real-time.In this article, a distributed non-gradient optimization algorithm is operated over a network to tune a large set of feedback controllers in a relatively large scale laboratory experiment of 40 lighting control systems through a centralized algorithm.A centralized algorithm is required to share and compare information from all controllers to determine the optimal design of the entire set of controllers and communicate the design to the other controllers for verification and improvements.A localized alternative would cause gaps in communicating optimal designs to other controllers.In the centralized algorithm tested in this article, it is shown that a “robust controller” emerges, one that performs well in all of the feedback control systems.By utilizing non-gradient optimization methods, this experimental approach to designing a controller removes the difficulties in modeling of complicated non-linear systems, and, thus, permits minimal system understanding to obtain a good solution with no need to consider manufacturing variances in system devices.Essentially, the information that is exploited in design is from a large set of operating control systems.
Suppose you have an inexpensive plant (process) that you have many copies of, and where each requires feedback control to achieve good closed-loop performance. There is then a feedback controller design challenge for each plant. The traditional approach to such a design is to use linear or nonlinear methods for one copy of the plant and implement the designed controller on each copy of the plant. The hope is that the original design is robust so it performs well on all copies. Here, we assume that the plants can be connected on the internet of things and introduce a novel strategy to design, in a distributed fashion, controllers for each plant where the design of the controller for each plant is informed by the success of the designs of all other controllers. We show that a type of robust controller can be designed over the internet of things. Our methods hold promise in practical commercial applications. Due to the lighting control systems being non-linear and stochastic and including significant delay, an analytical gradient cannot be determined.Therefore, two standard non-gradient optimization methods, the genetic algorithm (GA) and the set-based stochastic (SBS) optimization method, are tested in this experiment.Based on Darwin’s natural selection theory, the GA is a non-gradient stochastic search method which allows multiple simultaneous search points to reach near a global optimum point.The SBS optimization method creates a set of search points centered about the best point of the previous generation to find a solution near the global optimum point (Passino, 2004).Although doubts about performance assurance surfaced in the past, these algorithms have now reached a stage of maturity to solve complex and conflicting problems which used to be considered “deadlocked” (Man et al., 1996).The GA is now being successfully implemented in a variety of engineering applications from the classic inverted pendulum problem to complicated VLSI circuit design problems (Lee and Takagi, 1993; Dasgupta, 1997).Both of these algorithms have been shown to be appropriate methods for feedback control system design, resulting in stable optimal designs (Fleming and Purshouse, 2002).
The Late Viking Age Swedish runestones are commonly acknowledged as early Christian monuments. Using geostatistical techniques and descriptive statistics, we systematically investigate the regional-to-local spatiotemporal patterns of 1302 ornamentally dated Swedish runestones regarding the timing and speed of the Christianisation process. After quantitative geostatistical analyses of the age distribution patterns of Swedish runestones, we evaluate whether the observed patterns correspond to the pace and pattern of Christianisation, as represented by the presence of mission bishoprics, early church sites, late pagan grave sites and royal estates. We identify seven distinct age groups of runestones and statistically significant regional-to-local spatiotemporal differences in the age and age spread of runestones. The oldest runestones, with the smallest age spread, are found in south-western medieval Sweden, and the youngest, as well as the largest age spread, in the north-east, respectively. We find that runestones are significantly older close to early ecclesiastical sites, regardless of the analytical level, and significantly younger near to late pagan graves. The results obtained are inconclusive as to whether runestones are older near royal estates. Our results support that the spatiotemporal patterns of runestone sites mirror the timing of the Christianisation process and that geostatistical approaches to larger archaeological or historical data sets can add new dimensions to the understanding of the spatial dimensions of past societal changes. Background
The Late Viking Age Swedish runestones are commonly acknowledged as early Christian monuments. Using geostatistical techniques and descriptive statistics, we systematically investigate the regional-to-local spatiotemporal patterns of 1302 ornamentally dated Swedish runestones regarding the timing and speed of the Christianisation process. After quantitative geostatistical analyses of the age distribution patterns of Swedish runestones, we evaluate whether the observed patterns correspond to the pace and pattern of Christianisation, as represented by the presence of mission bishoprics, early church sites, late pagan grave sites and royal estates. We identify seven distinct age groups of runestones and statistically significant regional-to-local spatiotemporal differences in the age and age spread of runestones. The oldest runestones, with the smallest age spread, are found in south-western medieval Sweden, and the youngest, as well as the largest age spread, in the north-east, respectively. We find that runestones are significantly older close to early ecclesiastical sites, regardless of the analytical level, and significantly younger near to late pagan graves. The results obtained are inconclusive as to whether runestones are older near royal estates. Our results support that the spatiotemporal patterns of runestone sites mirror the timing of the Christianisation process and that geostatistical approaches to larger archaeological or historical data sets can add new dimensions to the understanding of the spatial dimensions of past societal changes. Runestones are among the visually most prominent remnants from the Scandinavian Viking Age (c. 790–1100 CE) (Jesch, 1994, 2001; Sawyer, 2000; Imer, 2007; Klos, 2009; Bianchi, 2010; Zilmer, 2010; Stern, 2013).Often decorated with crosses and/or prayers, they are also indicative of the Christianisation process (Lager, 2002, 2003; Zilmer, 2011; Williams, 2016).The majority – over 2800 – of the known Viking Age runic stone inscriptions are within medieval Sweden's borders, and concentrated in the eastern province of Uppland.In comparison, only c. 250 runestones and fewer than 100 are known within the borders of medieval Denmark and Norway, respectively (Table 1; Fig. 1a).They are mainly a Late Viking Age phenomenon – from the late tenth century to the early twelfth century – though about 70 are dated prior to c. 950 CE.The runestones functioned as memorial stones for deceased relatives (Jansson, 1987; Zachrisson, 1998; Sawyer, 2000) and likely served social or political purposes (Randsborg, 1980) and in particular may have attested statements or claims of inheritance rights (Sawyer, 1989, 2000), functioned as boundary markers (Larsson, 1990; Palm, 1992; Wilson, 1994; Johansen, 1997; Stille, 2014) and as a way to express Christianity (Lindqvist, 1915; Gardell, 1937; Palme, 1959; Segelberg, 1983; Gräslund, 1987; Herschend, 1994; Williams, 1999, 2016; Lager, 2002, 2003; Zilmer, 2011, 2012, 2013).
The Late Viking Age Swedish runestones are commonly acknowledged as early Christian monuments. Using geostatistical techniques and descriptive statistics, we systematically investigate the regional-to-local spatiotemporal patterns of 1302 ornamentally dated Swedish runestones regarding the timing and speed of the Christianisation process. After quantitative geostatistical analyses of the age distribution patterns of Swedish runestones, we evaluate whether the observed patterns correspond to the pace and pattern of Christianisation, as represented by the presence of mission bishoprics, early church sites, late pagan grave sites and royal estates. We identify seven distinct age groups of runestones and statistically significant regional-to-local spatiotemporal differences in the age and age spread of runestones. The oldest runestones, with the smallest age spread, are found in south-western medieval Sweden, and the youngest, as well as the largest age spread, in the north-east, respectively. We find that runestones are significantly older close to early ecclesiastical sites, regardless of the analytical level, and significantly younger near to late pagan graves. The results obtained are inconclusive as to whether runestones are older near royal estates. Our results support that the spatiotemporal patterns of runestone sites mirror the timing of the Christianisation process and that geostatistical approaches to larger archaeological or historical data sets can add new dimensions to the understanding of the spatial dimensions of past societal changes. The Late Viking Age runestones have generally been considered a form of Christian monument ever since the publication of Liljegren (1832).Subsequently, von Friesen (1913, 1933) attempted to divide the runestones into age classes, arguing, together with Ljungberg (1938), that their spatiotemporal distribution represented the geographical patterns of the gradual spread of Christianity across Scandinavia.Since then, the consensus has been that the runestone tradition originated in Denmark, spread northward to Norway and north-eastward through Sweden (Section 1.2).
The Late Viking Age Swedish runestones are commonly acknowledged as early Christian monuments. Using geostatistical techniques and descriptive statistics, we systematically investigate the regional-to-local spatiotemporal patterns of 1302 ornamentally dated Swedish runestones regarding the timing and speed of the Christianisation process. After quantitative geostatistical analyses of the age distribution patterns of Swedish runestones, we evaluate whether the observed patterns correspond to the pace and pattern of Christianisation, as represented by the presence of mission bishoprics, early church sites, late pagan grave sites and royal estates. We identify seven distinct age groups of runestones and statistically significant regional-to-local spatiotemporal differences in the age and age spread of runestones. The oldest runestones, with the smallest age spread, are found in south-western medieval Sweden, and the youngest, as well as the largest age spread, in the north-east, respectively. We find that runestones are significantly older close to early ecclesiastical sites, regardless of the analytical level, and significantly younger near to late pagan graves. The results obtained are inconclusive as to whether runestones are older near royal estates. Our results support that the spatiotemporal patterns of runestone sites mirror the timing of the Christianisation process and that geostatistical approaches to larger archaeological or historical data sets can add new dimensions to the understanding of the spatial dimensions of past societal changes. Segelberg (1983), Hultgård (1992), Williams (1996, 2016) and many others have emphasised the importance of the runestone material as a source of the Christianisation process.Still, scholars have interpreted differently how the runestones reflect early Christianity.It has been noted that the end of the erection of runestones in a region approximately coincides with the construction of the first churches (Zachrisson, 1998) and that the custom ceased soon after Christianisation had advanced into an ecclesiastical organisation phase (Lager, 2002, 2003).Since von Friesen (1933) and Ljungberg (1938), it has been commonly held that the Late Viking Age runestones, especially the Swedish ones, reflect the spatiotemporal patterns of Christianisation.In areas with older runestones, Christianity was supposedly established earlier than in those with younger runestones.So far, however, this observation has not been quantitatively verified against different indicators of Christianisation.
During the Last Glacial Maximum (LGM), very specific but rare osseous decorated artifacts were produced using the “pseudo-excise” technique. These artifacts present a large geographical distribution, extending at least from the Aquitaine basin to Asturias. While in France a Badegoulian age is traditionally accepted for the “pseudo-excise” technique, this is mostly based on arguable data from old excavations and/or problematic archaeostratigraphic contexts. Since it is a key-site for this matter we have focused our attention on Pégourié Cave (Lot, France) in order to establish the chronocultural attribution of pseudo-excised pieces in southwest France. The interdisciplinary reassessment of the lithic and osseous industries from Layer 8 and 9, including inter-layer refittings, has shown (1) the irrelevance of previous stratigraphic subdivisions and (2) the strong cultural heterogeneity of this assemblage, which combines Azilian, Magdalenian, Badegoulian, Solutrean and Gravettian components. At the same time, a broad 14C program based on the direct dating of specific bone and antler technical wastes and tools—including a pseudo-excised point—was implemented after 3D recording using photogrammetry. The results obtained have allowed us to establish a new, firm confirmation of the Badegoulian age of pseudo-excised decoration and, in doing so, to more precisely define the time-range of this specific feature's trans-regional dissemination. Finally, by comparing the results with recent data notably obtained at Llonín cave (Asturias, Spain), new light has been shed on the cultural geography of southwestern Europe during the LGM, allowing us to discuss and fuel the still-controversial “Iberian Badegoulian” hypothesis. At the start of the last Glacial Maximum (LGM; 23–19 cal.ka BP: MARGO Project members, 2009; Mix et al., 2001), the Upper Palaeolithic of southwestern Europe experienced several changes whose breadth and chronology highly depends on geography and/or researchers' interpretations.The Solutrean-to-Magdalenian transition (circa 24–19 cal.ka BP; see Table 1 for a synthetic chronocultural framework) is indeed part of a lively current debate where the issue of the existence and geographical extent of Badegoulian technical traditions is a central focus.In short, two contradictory models have been proposed by researchers to interpret the evolving cultural trajectories of hunter-gatherer groups in this area and time-span.From a “regionalized” Solutrean substratum primarily expressed through a coherent geographical distribution of different types of lithic points (e.g., de la Rasilla and Santamaría Álvarez, 2005; Straus, 1977), the first model defends the idea of a general, abrupt and more or less contemporaneous change throughout this geographical area, corresponding to the development of the quite different Badegoulian technical traditions around 23 cal.ka BP.This scenario, which has been broadly documented and long accepted in France, where Badegoulian industries were first defined (e. g. Allain, 1968; Allain and Fritsch, 1967; Cheynier, 1939; Vignard, 1965), was first supported in the Iberian Peninsula in the 1950s with Cheynier's identification of specific Badegoulian tools—such as the so-called “raclettes”—at Parpalló Cave (Gandía, Valencia) (e.g., Aura, 1995, 2007; Breuil, 1954; Cheynier, 1951).This then extended to northwest Spain following the work of Utrilla (1989, 1981) and later, the critical reassessment by Bosselin and Djindjian of published data (Bosselin, 2000; Bosselin and Djindjian, 1999).This scenario is currently supported by research conducted between Asturias and Levantine Spain notably as part of the SOBAMA project (J.-E.Aura dir.; Aura et al., 2012; de la Rasilla et al., 2014).In contrast, other researchers argue that the data available in the Iberian Peninsula does not indicate abrupt changes but a process of progressive replacement of Solutrean hunting technology through the so-called “desolutreanization” process (de la Rasilla, 1989; Straus, 2000), directly leading to the Magdalenian technical traditions observed from around 20.5 cal.ka BP (Corchón Rodríguez et al., 2015; Rios Garaizar et al., 2013; Straus et al., 2014).This second model, which excludes the Badegoulian “stage” in Spain, considers that the technical similarities are non-significant typo-technological convergences also taking place within different kinds of environments, and assumes that, between 23 and 20.5 cal.ka BP, southwestern Europe was marked by a clear cultural geography (i.e., Badegoulian in present-day France versus Upper/Final Solutrean in the Iberian Peninsula; Banks et al., 2009, 2011).Crystallized in the early 2000s (Bosselin and Djindjian, 2000; Straus and Clark, 2000) and much more complex than a simple logomachie issue (Sauvet et al., 2008), the opposition between these two models still generates a lively debate (Álvarez Alonso and Arrizabalaga, 2012).
During the Last Glacial Maximum (LGM), very specific but rare osseous decorated artifacts were produced using the “pseudo-excise” technique. These artifacts present a large geographical distribution, extending at least from the Aquitaine basin to Asturias. While in France a Badegoulian age is traditionally accepted for the “pseudo-excise” technique, this is mostly based on arguable data from old excavations and/or problematic archaeostratigraphic contexts. Since it is a key-site for this matter we have focused our attention on Pégourié Cave (Lot, France) in order to establish the chronocultural attribution of pseudo-excised pieces in southwest France. The interdisciplinary reassessment of the lithic and osseous industries from Layer 8 and 9, including inter-layer refittings, has shown (1) the irrelevance of previous stratigraphic subdivisions and (2) the strong cultural heterogeneity of this assemblage, which combines Azilian, Magdalenian, Badegoulian, Solutrean and Gravettian components. At the same time, a broad 14C program based on the direct dating of specific bone and antler technical wastes and tools—including a pseudo-excised point—was implemented after 3D recording using photogrammetry. The results obtained have allowed us to establish a new, firm confirmation of the Badegoulian age of pseudo-excised decoration and, in doing so, to more precisely define the time-range of this specific feature's trans-regional dissemination. Finally, by comparing the results with recent data notably obtained at Llonín cave (Asturias, Spain), new light has been shed on the cultural geography of southwestern Europe during the LGM, allowing us to discuss and fuel the still-controversial “Iberian Badegoulian” hypothesis. In any case, aside from the discussions regarding the comparability of the lithic subsystems on either side of the Pyrenees, it is admitted that some elements clearly demonstrate links between these two areas during the development of the Badegoulian lithic traditions in southwest France (23.5–20.5 cal.ka BP: Ducasse et al., 2014, 2017).Particularly noteworthy is the large geographical distribution of specific, although rare, antler pieces using the “pseudo-excise” technique to carve specific curvilinear decoration (Fig. 1a).Usually attributed in France to the raclette-yielding Badegoulian, these decorated osseous elements are currently documented from Charente to Asturias and are claimed by proponents of the first model as strong evidence for an Iberian Badegoulian (Aura et al., 2012).On the other hand, if we accept the cultural geography implied by the second model and consider the strong specificity and scarcity of this group of pieces as an indicator of synchronous manufacturing, pseudo-excised decoration can be considered evidence for the dissemination of a specific technique and graphic theme that crossed “cultural” boundaries.
Corpus callosotomy (CC) is used in patients with drug-resistant seizures who are not candidates for excisional surgery and failed neurostimulation. We examined ictal scalp and intracranial electroencephalogram (iEEG) recordings in 16 patients being evaluated for anterior CC alone or CC in combination with focal resection, to determine the role of the iEEG in predicting postoperative seizure outcomes. In our cohort, CC improved generalized atonic seizures and focal seizures with impaired awareness but did not alter outcomes for generalized tonic–clonic or tonic seizures. Invasive EEG prior to CC did not refine the prediction of postsurgical seizure outcomes in patients with inconclusive scalp EEG. Introduced by van Wagenen in 1940, corpus callosotomy (CC) is a palliative disconnection procedure for patients with drug-resistant epilepsy who are not suitable candidates for excisional surgery.Anterior callosotomy, the most commonly used modification of this procedure, involves interruption of the anterior mid-body of the corpus callosum that carries interhemispheric motor connections [1] thought to be essential for the generation of generalized atonic and tonic–clonic seizures (GTCs) [2,3].In epilepsies with these seizure types, the successful postoperative outcomes have been consistently demonstrated.However, the treatment responses in other seizure types have not been well understood.While the previous applications of CC were largely restricted to the patients with disabling generalized seizure syndromes, such as Lennox–Gastaut syndrome (LGS) and infantile spasms, the indications have recently expanded to the patient populations with other epilepsy etiologies [3].These include drug-resistant focal epilepsies in patients without identifiable lesions or those with multiple lesions which are not amenable for resection [4].
Bone modifications are associated with a broad range of agents, including carnivores, stone tools, sediments, etc., and can be categorized as one of two types: conspicuous or inconspicuous. Contrary to the larger, more easily identifiable conspicuous modifications, inconspicuous modifications are small, shallow and almost unnoticeable without the aid of a hand lens and strong light, making them harder and more time consuming to identify. This has led to arguments for their omission from tooth mark counts, even though their presence on the bones of archaeological and paleontological faunal assemblages have been recognized and mentioned in literature as having interpretive potential. This study employs visible light microscopy and high resolution scanning electron microscopy to present evidence that positively identifies inconspicuous carnivore modifications as abrasions that also have diagnostic morphology which differentiates them from abrasion created by other taphonomic agents. These inconspicuous carnivore marks may also be associated with the presence of soft tissue and so may help reconstruct the amount of flesh present at specific stages of carcass consumption. This study shows the interpretive potential of ICA may be substantial, requiring their inclusion in tooth mark counts for more accurate reconstructions of past carnivore behavior. Furthermore, understanding the etiology and implications of inconspicuous carnivore marks may provide a new way to interpret faunal assemblages that exhibit such marks, such as those associated with FLK Zinjanthropus, Tanzania. Such interpretations may be able to help develop stronger inferences regarding methods of hominin carcass acquisition. Conspicuous and inconspicuous modifications
Bone modifications are associated with a broad range of agents, including carnivores, stone tools, sediments, etc., and can be categorized as one of two types: conspicuous or inconspicuous. Contrary to the larger, more easily identifiable conspicuous modifications, inconspicuous modifications are small, shallow and almost unnoticeable without the aid of a hand lens and strong light, making them harder and more time consuming to identify. This has led to arguments for their omission from tooth mark counts, even though their presence on the bones of archaeological and paleontological faunal assemblages have been recognized and mentioned in literature as having interpretive potential. This study employs visible light microscopy and high resolution scanning electron microscopy to present evidence that positively identifies inconspicuous carnivore modifications as abrasions that also have diagnostic morphology which differentiates them from abrasion created by other taphonomic agents. These inconspicuous carnivore marks may also be associated with the presence of soft tissue and so may help reconstruct the amount of flesh present at specific stages of carcass consumption. This study shows the interpretive potential of ICA may be substantial, requiring their inclusion in tooth mark counts for more accurate reconstructions of past carnivore behavior. Furthermore, understanding the etiology and implications of inconspicuous carnivore marks may provide a new way to interpret faunal assemblages that exhibit such marks, such as those associated with FLK Zinjanthropus, Tanzania. Such interpretations may be able to help develop stronger inferences regarding methods of hominin carcass acquisition. One of the ways the science of taphonomy is used during the examination of vertebrate remains by forensic investigators, archaeologists and paleontologists is to identify agents of post mortem processes (Efremov, 1940; Lyman, 1994).When bone is all that remains, taphonomic analysis of surface modifications can reveal evidence of direct interaction between agents of modification and carcasses that can assist in the reconstruction of peri-mortem events (Fisher, 1995; Lyman, 1994).Modifications exist as one of two types: conspicuous or inconspicuous.The difference between the two types is that because of their large size, key morphological features of conspicuous modifications, make them identifiable as tooth marks, cut marks, abrasion, etc., without magnification (Binford, 1981; Blumenschine et al., 1996; Capaldo, 1997; Dominguez-Rodrigo, 1999; Dominguez-Rodrigo et al., 2007; Fisher, 1995; Haynes, 1983; Lyman, 1994, Pobiner, 2007).Conversely, inconspicuous marks refer to shallow, obscure modifications whose variable features are too small to be visible with the naked eye and are only detected by either a slight color change within the interior of the modification or the presence of polish (Amore and Blumenschine, 2016; Blumenschine et al., 1996; Blumenschine, 2007; Capaldo, 1997; Fisher, 1995).Both conspicuous and inconspicuous carnivore tooth marks have been identified on bone assemblages at archaeologically significant sites, such as the FLK Zinjanthropus site in Olduvai Gorge, Tanzania (Blumenschine, 1986; Blumenschine, 2007; Blumenschine, 1995; Blumenschine et al., 1996; Bunn and Kroll, 1986; Capaldo, 1997; Dominguez-Rodrigo and Barba 2006, 2007; Dominguez-Rodrigo et al., 2007; Pante, 2013; Pante et al., 2012, 2015; Pobiner, 2007; Selvaggio, 1994).While problems with collecting congruous bone surface modification data have revolved around both conspicuous and inconspicuous modifications, problems with the latter, such as requiring significantly more time to locate, quanify and identify to an agent, are inherently worse due to their small size and perceived ambiguous morphology (Blumenschine et al., 1996; Fisher, 1995; Lyman, 1994; Cruz-Uribe and Klein, 1994; Dominguez-Rodrigo and Barba, 2006).It is because of these problems that arguments have been made for their exclusion from examinations of bone surfaces (Blumenschine, 1995; Blumenschine et al., 1996; Cruz-Uribe and Klein, 1994; Dominguez-Rodrigo, 1999; Oliver, 1994).Regardless of these challenges, Blumenschine et al. (1996) encourage the inclusion of inconspicuous modifications, suggesting that their omission results in tooth mark counts that fail to accurately reflect the true scope of carnivore scavenging activity on an assemblage.To facilitate the reduction of inter-analyst error and to improve reproducibility, researchers have advocated the consistent use of “a hand lens under strong light, systematically examining all parts of the surface at different angles with respect to the incoming light for conspicuous and inconspicuous marks” (Blumenschine et al., 1996; Pante et al., 2012).
Bone modifications are associated with a broad range of agents, including carnivores, stone tools, sediments, etc., and can be categorized as one of two types: conspicuous or inconspicuous. Contrary to the larger, more easily identifiable conspicuous modifications, inconspicuous modifications are small, shallow and almost unnoticeable without the aid of a hand lens and strong light, making them harder and more time consuming to identify. This has led to arguments for their omission from tooth mark counts, even though their presence on the bones of archaeological and paleontological faunal assemblages have been recognized and mentioned in literature as having interpretive potential. This study employs visible light microscopy and high resolution scanning electron microscopy to present evidence that positively identifies inconspicuous carnivore modifications as abrasions that also have diagnostic morphology which differentiates them from abrasion created by other taphonomic agents. These inconspicuous carnivore marks may also be associated with the presence of soft tissue and so may help reconstruct the amount of flesh present at specific stages of carcass consumption. This study shows the interpretive potential of ICA may be substantial, requiring their inclusion in tooth mark counts for more accurate reconstructions of past carnivore behavior. Furthermore, understanding the etiology and implications of inconspicuous carnivore marks may provide a new way to interpret faunal assemblages that exhibit such marks, such as those associated with FLK Zinjanthropus, Tanzania. Such interpretations may be able to help develop stronger inferences regarding methods of hominin carcass acquisition. But researchers have recognized special circumstances that may require more intensive microscopic analyses, such as microstructural differences among bones of separate species, modification ambiguity or modification mimicry, and when identifying marks of unknown origin (Archer and Braun, 2013; Bell, 1990; Bello et al., 2009; Behrensmeyer et al., 1986; Bromage, 1984; Madgewick, 2014; Pante et al., 2017; Shipman and Rose, 1983).For example, the microstructural differences among the bones of terrestrial and aquatic animals influence the morphology of modifications made by the same agents, making them smaller and more inconspicuous on the bones of aquatic species (Archer and Braun, 2013).This alters how effectively seasoned researchers can identify the smaller, more inconspicuous modifications (Archer et al., 2014; Archer and Braun, 2013).In these cases, the use of a hand lens to observe and identify smaller, inconspicuous modifications is considered insufficient to maintain accurate, reproducible results between observers, but microscopic analysis using visible light microscopy at 40× of the same modifications increased agreement between observers (Archer et al., 2014; Archer and Braun, 2013).
This paper mainly proposes a novel method to construct a risk matrix for assessing safety risks in oil and gas industry. There are often multiple experts and multiple criteria involved in safety risk assessment problems and the assessment data are often given in the form of interval numbers. In order to better assess risks, the definition of interval number with distribution function and utility function is proposed in this paper. The frequency and the consequence of risk are only two needed indicators in risk matrix and their values are needed in the form of crisp values. So a multi-expert and multi-criterion information fusion based on interval number(MEMCIF-IN) model is built in this paper. Firstly, a multi-expert and multi-criterion fusion model is constructed to combine individual interval numbers into a collective interval number and integrate multiple criteria into a comprehensive index. In the fusion model, the weights of assessment experts are calculated based on the objective weights and the subjective weights simultaneously and the information of individual interval numbers is preserved without information loss in the final result. Secondly, a Continuous Weighted Ordered Weighted Aggregation(C-WOWA) operator is proposed. In the C-WOWA operator, the position weights which are generated by utility function and the importance weights which are generated by probability density function are considered at the same time. The position weights in the C-WOWA operator can correct the impact of experts’ risk attitudes and the importance weights can reflect the importance of the points themselves in an interval number. Finally, a risk matrix is constructed to show which risk is high and which is low. In addition, an application is implemented to show the practicality and rationality of the proposed method. To prevent safety accidents from happening, effective safety risk assessment is required in oil and gas industry (Markowski and Mannan, 2008).In general, quantitative way (Haldar and Mahadevan, 2000; Azizsoltani and Sadeghi, 2018; Azizsoltani and Haldar, 2018; Roy et al., 2019), semi-quantitative way (Ni et al., 2010; Mahamid, 2011; Ruan et al., 2015; Duan et al., 2016; Tian et al., 2018) and qualitative way (Goetz et al., 2017; Kelly et al., 2018) are three ways to implement risk assessment.The semi-quantitative way which has the advantages of both quantitative and qualitative ways is often used in risk assessment problems.Risk matrix approach, a semi-quantitative way, is used for risk assessment in this paper.
This paper mainly proposes a novel method to construct a risk matrix for assessing safety risks in oil and gas industry. There are often multiple experts and multiple criteria involved in safety risk assessment problems and the assessment data are often given in the form of interval numbers. In order to better assess risks, the definition of interval number with distribution function and utility function is proposed in this paper. The frequency and the consequence of risk are only two needed indicators in risk matrix and their values are needed in the form of crisp values. So a multi-expert and multi-criterion information fusion based on interval number(MEMCIF-IN) model is built in this paper. Firstly, a multi-expert and multi-criterion fusion model is constructed to combine individual interval numbers into a collective interval number and integrate multiple criteria into a comprehensive index. In the fusion model, the weights of assessment experts are calculated based on the objective weights and the subjective weights simultaneously and the information of individual interval numbers is preserved without information loss in the final result. Secondly, a Continuous Weighted Ordered Weighted Aggregation(C-WOWA) operator is proposed. In the C-WOWA operator, the position weights which are generated by utility function and the importance weights which are generated by probability density function are considered at the same time. The position weights in the C-WOWA operator can correct the impact of experts’ risk attitudes and the importance weights can reflect the importance of the points themselves in an interval number. Finally, a risk matrix is constructed to show which risk is high and which is low. In addition, an application is implemented to show the practicality and rationality of the proposed method. There are only two indicators(Frequency and Consequence) needed in risk matrix (Markowski and Mannan, 2008; Ruan et al., 2015).When the crisp scores of the two indicators are derived and depicted in risk matrix, the rank of risk and the result of risk assessment can be derived.On many occasions, there are often multiple assessment experts and multiple criteria involved in risk assessment problems.Moreover, because of the uncertainty of risk occurrence and the limitation of experts’ knowledge, risk assessment data are often given in the form of interval numbers which cannot be directly used in risk matrix and need to be fused into crisp values.Therefore, safety risk assessment in oil and gas industry is often a multi-expert and multi-criterion information fusion(MEMCIF-IN) problem based on interval numbers.How to implement the risk assessment work based on the MEMCIF-IN problem with using the risk matrix approach is the main task in this paper.
This paper mainly proposes a novel method to construct a risk matrix for assessing safety risks in oil and gas industry. There are often multiple experts and multiple criteria involved in safety risk assessment problems and the assessment data are often given in the form of interval numbers. In order to better assess risks, the definition of interval number with distribution function and utility function is proposed in this paper. The frequency and the consequence of risk are only two needed indicators in risk matrix and their values are needed in the form of crisp values. So a multi-expert and multi-criterion information fusion based on interval number(MEMCIF-IN) model is built in this paper. Firstly, a multi-expert and multi-criterion fusion model is constructed to combine individual interval numbers into a collective interval number and integrate multiple criteria into a comprehensive index. In the fusion model, the weights of assessment experts are calculated based on the objective weights and the subjective weights simultaneously and the information of individual interval numbers is preserved without information loss in the final result. Secondly, a Continuous Weighted Ordered Weighted Aggregation(C-WOWA) operator is proposed. In the C-WOWA operator, the position weights which are generated by utility function and the importance weights which are generated by probability density function are considered at the same time. The position weights in the C-WOWA operator can correct the impact of experts’ risk attitudes and the importance weights can reflect the importance of the points themselves in an interval number. Finally, a risk matrix is constructed to show which risk is high and which is low. In addition, an application is implemented to show the practicality and rationality of the proposed method. There are many scholars have devoted a lot of efforts to establish an effective risk matrix in recent years (Markowski and Mannan, 2008; Ni et al., 2010; Mahamid, 2011; Ruan et al., 2015; Tian et al., 2018).In 2008, Markowski and Mannan developed the fuzzification of frequency and severity of an incident scenario and proposed the fuzzy rules to construct a fuzzy risk matrix with crisp values (Markowski and Mannan, 2008).In 2000, Ni et al. made some extensions on risk matrix approach and incorporated interval numbers which were transformed to crisp values to deal with uncertainty in risk matrix (Ni et al., 2010).Ruan et al. incorporated risk attitudes of decision makers in the establishment of risk matrix with the use of utility indifference curves (Ruan et al., 2015).Tian et al. established a method to construct risk matrix integrating risk attitudes with multiple indicators which are described with crisp values (Tian et al., 2018).
This paper mainly proposes a novel method to construct a risk matrix for assessing safety risks in oil and gas industry. There are often multiple experts and multiple criteria involved in safety risk assessment problems and the assessment data are often given in the form of interval numbers. In order to better assess risks, the definition of interval number with distribution function and utility function is proposed in this paper. The frequency and the consequence of risk are only two needed indicators in risk matrix and their values are needed in the form of crisp values. So a multi-expert and multi-criterion information fusion based on interval number(MEMCIF-IN) model is built in this paper. Firstly, a multi-expert and multi-criterion fusion model is constructed to combine individual interval numbers into a collective interval number and integrate multiple criteria into a comprehensive index. In the fusion model, the weights of assessment experts are calculated based on the objective weights and the subjective weights simultaneously and the information of individual interval numbers is preserved without information loss in the final result. Secondly, a Continuous Weighted Ordered Weighted Aggregation(C-WOWA) operator is proposed. In the C-WOWA operator, the position weights which are generated by utility function and the importance weights which are generated by probability density function are considered at the same time. The position weights in the C-WOWA operator can correct the impact of experts’ risk attitudes and the importance weights can reflect the importance of the points themselves in an interval number. Finally, a risk matrix is constructed to show which risk is high and which is low. In addition, an application is implemented to show the practicality and rationality of the proposed method. Although many scholars have improved the establishment of risk matrix, it still needs to be improved when it is used to assess safety risks in oil and gas industry, for the following reasons:
This paper mainly proposes a novel method to construct a risk matrix for assessing safety risks in oil and gas industry. There are often multiple experts and multiple criteria involved in safety risk assessment problems and the assessment data are often given in the form of interval numbers. In order to better assess risks, the definition of interval number with distribution function and utility function is proposed in this paper. The frequency and the consequence of risk are only two needed indicators in risk matrix and their values are needed in the form of crisp values. So a multi-expert and multi-criterion information fusion based on interval number(MEMCIF-IN) model is built in this paper. Firstly, a multi-expert and multi-criterion fusion model is constructed to combine individual interval numbers into a collective interval number and integrate multiple criteria into a comprehensive index. In the fusion model, the weights of assessment experts are calculated based on the objective weights and the subjective weights simultaneously and the information of individual interval numbers is preserved without information loss in the final result. Secondly, a Continuous Weighted Ordered Weighted Aggregation(C-WOWA) operator is proposed. In the C-WOWA operator, the position weights which are generated by utility function and the importance weights which are generated by probability density function are considered at the same time. The position weights in the C-WOWA operator can correct the impact of experts’ risk attitudes and the importance weights can reflect the importance of the points themselves in an interval number. Finally, a risk matrix is constructed to show which risk is high and which is low. In addition, an application is implemented to show the practicality and rationality of the proposed method. (1) Uncertainty is inevitable in the process of safety risk assessment (Jr, 2010; Duijm, 2015).On many occasions, assessment experts cannot give precise values of risk because of uncertainty.The forms of assessment data in the existing literatures are usually crisp values (Tian et al., 2018), linguistic terms (Li and Zeng, 2017), interval numbers (Ni et al., 2010) and so on.Interval number which can reflect uncertainty is a feasible data form instead of crisp value in the process of safety risk assessment, but few literatures studies how to establish a risk matrix with interval numbers.For example, Ni et al. (Ni et al., 2010) just made a simple transformation from interval numbers to crisp values and the information loss occurred and the nature of safety risk was not reflected well.So it is necessary to propose an appropriate method to establish risk matrix for safety risk assessment in oil and gas industry with interval numbers integrating the nature of safety risk.
This paper mainly proposes a novel method to construct a risk matrix for assessing safety risks in oil and gas industry. There are often multiple experts and multiple criteria involved in safety risk assessment problems and the assessment data are often given in the form of interval numbers. In order to better assess risks, the definition of interval number with distribution function and utility function is proposed in this paper. The frequency and the consequence of risk are only two needed indicators in risk matrix and their values are needed in the form of crisp values. So a multi-expert and multi-criterion information fusion based on interval number(MEMCIF-IN) model is built in this paper. Firstly, a multi-expert and multi-criterion fusion model is constructed to combine individual interval numbers into a collective interval number and integrate multiple criteria into a comprehensive index. In the fusion model, the weights of assessment experts are calculated based on the objective weights and the subjective weights simultaneously and the information of individual interval numbers is preserved without information loss in the final result. Secondly, a Continuous Weighted Ordered Weighted Aggregation(C-WOWA) operator is proposed. In the C-WOWA operator, the position weights which are generated by utility function and the importance weights which are generated by probability density function are considered at the same time. The position weights in the C-WOWA operator can correct the impact of experts’ risk attitudes and the importance weights can reflect the importance of the points themselves in an interval number. Finally, a risk matrix is constructed to show which risk is high and which is low. In addition, an application is implemented to show the practicality and rationality of the proposed method. (2) On many occasions, there are often multiple indicators used to describe the consequence of risk from several aspects in risk assessment (Brito et al., 2010; Zhao et al., 2018; Ruan et al., 2015; Tian et al., 2018), but only one index of risk consequence is needed in risk matrix construction.So combining individual index scores into a comprehensive score is one of the important tasks in the construction of risk matrix (Xu, 2004; Liu et al., 2015; Liu and Li, 2015; Taylan et al., 2016).The attitudes of decision makers to these indicators are possibly different.Decision maker may think the maximum value or the minimum value or the average value of these indicators should be the risk consequence scores regardless of the information source.Different attitudes lead to different consequence scores.So the combination process should be flexible to adjust this kind of attitude.When fusing data from multiple indicators, the attitudes of decision makers to each individual index should be considered.But in the existing literatures, few papers consider this problem to derive a reasonable assessment result in risk matrix construction.
This paper mainly proposes a novel method to construct a risk matrix for assessing safety risks in oil and gas industry. There are often multiple experts and multiple criteria involved in safety risk assessment problems and the assessment data are often given in the form of interval numbers. In order to better assess risks, the definition of interval number with distribution function and utility function is proposed in this paper. The frequency and the consequence of risk are only two needed indicators in risk matrix and their values are needed in the form of crisp values. So a multi-expert and multi-criterion information fusion based on interval number(MEMCIF-IN) model is built in this paper. Firstly, a multi-expert and multi-criterion fusion model is constructed to combine individual interval numbers into a collective interval number and integrate multiple criteria into a comprehensive index. In the fusion model, the weights of assessment experts are calculated based on the objective weights and the subjective weights simultaneously and the information of individual interval numbers is preserved without information loss in the final result. Secondly, a Continuous Weighted Ordered Weighted Aggregation(C-WOWA) operator is proposed. In the C-WOWA operator, the position weights which are generated by utility function and the importance weights which are generated by probability density function are considered at the same time. The position weights in the C-WOWA operator can correct the impact of experts’ risk attitudes and the importance weights can reflect the importance of the points themselves in an interval number. Finally, a risk matrix is constructed to show which risk is high and which is low. In addition, an application is implemented to show the practicality and rationality of the proposed method. (3) When the assessment data are given in the form of interval number in the construction of risk matrix, an aggregation model which aggregates an interval number into a crisp value is needed.There are many literatures study this problem (Cao and Wu, 2011; Yager and Alajlan, 2016; Yager and Xu, 2006; Jin and Qian, 2016; Sayadi et al., 2009; Jiang and Liang, 2017; Yue, 2011), most methods transform an interval number into a specific value.In the process of transformation, the weight function is very important and most scholars consider the probability density function of an interval number being the weight function.But in risk assessment problems, it is not enough to consider only the probability density function.The determination of weight generating function also need to consider the nature of risk assessment.In the assessment process, assessment experts possibly show different risk attitudes and the risk attitudes would affect the values of their assessment data.So it is necessary to propose a novel method generating the weight function when an interval number is aggregated into a crisp value.
This paper mainly proposes a novel method to construct a risk matrix for assessing safety risks in oil and gas industry. There are often multiple experts and multiple criteria involved in safety risk assessment problems and the assessment data are often given in the form of interval numbers. In order to better assess risks, the definition of interval number with distribution function and utility function is proposed in this paper. The frequency and the consequence of risk are only two needed indicators in risk matrix and their values are needed in the form of crisp values. So a multi-expert and multi-criterion information fusion based on interval number(MEMCIF-IN) model is built in this paper. Firstly, a multi-expert and multi-criterion fusion model is constructed to combine individual interval numbers into a collective interval number and integrate multiple criteria into a comprehensive index. In the fusion model, the weights of assessment experts are calculated based on the objective weights and the subjective weights simultaneously and the information of individual interval numbers is preserved without information loss in the final result. Secondly, a Continuous Weighted Ordered Weighted Aggregation(C-WOWA) operator is proposed. In the C-WOWA operator, the position weights which are generated by utility function and the importance weights which are generated by probability density function are considered at the same time. The position weights in the C-WOWA operator can correct the impact of experts’ risk attitudes and the importance weights can reflect the importance of the points themselves in an interval number. Finally, a risk matrix is constructed to show which risk is high and which is low. In addition, an application is implemented to show the practicality and rationality of the proposed method. In a word, a novel method which cannot only deal with the multi-expert and multi-index information fusion problem based on interval numbers but also integrate an interval number into a crisp value with considering the risk attitudes of assessment experts and the importance of the points themselves in interval number simultaneously is needed in this paper.The nature of risk mainly performs in the uncertainty of risk occurrence.Faced with the uncertainty of risk, decision makers have different risk attitudes (Ruan et al., 2015; Tian et al., 2018).The assessment data which are given by different experts with different risk attitudes have different effects on assessment results.Therefore, there will be a deviation between the assessment result and the true level.To derive a real and fair risk assessment result, the influence of experts’ risk attitudes should be corrected.To the authors’ knowledge, there is no existing literature studying this kind of problem.
Case studies can generate hypothesis based on unique clinical patient encounters and provide guidance among populations with limited numbers of patients. However, case studies are not blinded and are susceptible to a variety of factors that can influence study outcomes. One potential solution to minimize this bias is to use an N-of-1 trial. N-of-1 trials are a double-blinded randomized crossover trial within a limited number of patients, often as small as a single patient. These trials borrow many concepts from randomized controlled trials (RCTs), which in turn increases the validity of findings compared with a case report. Situations best suited for an N-of-1 trial include chronic disease states and therapies with quick onset and offset, such as in patients with seizures. There are many opportunities to use N-of-1 trials among patients with epilepsy, and providers are encouraged to explore and employ these methods. The purpose of this article was to describe N-of-1 trials along with considerations for conducting, publishing, and evaluating N-of-1 trials. Case studies have an important place in the medical literature.They can generate hypothesis based on unique clinical patient encounters and provide guidance among populations with limited numbers of patients.Case studies are usually a detailed description of one or more patient experiences.However, case studies are not blinded and are susceptible to a variety of factors that can influence study outcomes [1–4].These include the placebo effect, the patient's desire to please their provider, patient and provider expectations of the therapy, and natural waxing and waning of the condition [2–4].One potential solution to minimize bias found in case studies is to employ the methods from a single-case design, better known as an N-of-1 trial (Table 1).
Case studies can generate hypothesis based on unique clinical patient encounters and provide guidance among populations with limited numbers of patients. However, case studies are not blinded and are susceptible to a variety of factors that can influence study outcomes. One potential solution to minimize this bias is to use an N-of-1 trial. N-of-1 trials are a double-blinded randomized crossover trial within a limited number of patients, often as small as a single patient. These trials borrow many concepts from randomized controlled trials (RCTs), which in turn increases the validity of findings compared with a case report. Situations best suited for an N-of-1 trial include chronic disease states and therapies with quick onset and offset, such as in patients with seizures. There are many opportunities to use N-of-1 trials among patients with epilepsy, and providers are encouraged to explore and employ these methods. The purpose of this article was to describe N-of-1 trials along with considerations for conducting, publishing, and evaluating N-of-1 trials. N-of-1 trials are a double-blinded randomized crossover trial within a limited number of patients, often as small as a single patient [2–6].It can be used to determine if an existing therapy is conferring the intended benefit or used for a trial of a new therapy.In an N-of-1 trial, the patient and provider are typically blinded to the order that therapy is administered.Following a predetermined treatment period, the patient may crossover to the other therapy.This cycle is repeated in a randomly determined pattern with prospective measurement of disease control until the patient response is determined.The general flow of designing and implementing an N-of-1 trial is displayed in Fig. 1.
Case studies can generate hypothesis based on unique clinical patient encounters and provide guidance among populations with limited numbers of patients. However, case studies are not blinded and are susceptible to a variety of factors that can influence study outcomes. One potential solution to minimize this bias is to use an N-of-1 trial. N-of-1 trials are a double-blinded randomized crossover trial within a limited number of patients, often as small as a single patient. These trials borrow many concepts from randomized controlled trials (RCTs), which in turn increases the validity of findings compared with a case report. Situations best suited for an N-of-1 trial include chronic disease states and therapies with quick onset and offset, such as in patients with seizures. There are many opportunities to use N-of-1 trials among patients with epilepsy, and providers are encouraged to explore and employ these methods. The purpose of this article was to describe N-of-1 trials along with considerations for conducting, publishing, and evaluating N-of-1 trials. While traditional randomized controlled trials (RCTs) provide valuable safety and efficacy data for new drug approvals, they are not without limitations, especially in trials evaluating antiseizure drugs.Typically, phase II–III studies of antiseizure drugs involve patients that present with very high monthly seizure frequency.New agents are studied initially as “add-on” therapy, meaning they are given as adjunctive to a patient already receiving other anti-seizure drugs.In addition, patients with other complex medical or psychiatric conditions may not meet typical RCT eligibility criteria.Given these constraints, data from phase III studies may be difficult to extrapolate to other patient populations.An N-of-1 trial can be used in these situations for patients not meeting RCT inclusion criteria if or different comparators are used [3,5,7].
Case studies can generate hypothesis based on unique clinical patient encounters and provide guidance among populations with limited numbers of patients. However, case studies are not blinded and are susceptible to a variety of factors that can influence study outcomes. One potential solution to minimize this bias is to use an N-of-1 trial. N-of-1 trials are a double-blinded randomized crossover trial within a limited number of patients, often as small as a single patient. These trials borrow many concepts from randomized controlled trials (RCTs), which in turn increases the validity of findings compared with a case report. Situations best suited for an N-of-1 trial include chronic disease states and therapies with quick onset and offset, such as in patients with seizures. There are many opportunities to use N-of-1 trials among patients with epilepsy, and providers are encouraged to explore and employ these methods. The purpose of this article was to describe N-of-1 trials along with considerations for conducting, publishing, and evaluating N-of-1 trials. Often an N-of-1 trial will follow a RCT; the RCT demonstrates that a therapy works, on average, for a population.However, N-of-1 trials can determine who the therapy works for and may be considered phase IV research [3].Despite N-of-1 trials taking place within a small sample, they can still produce a high level of evidence.The Oxford Centre for Evidence-Based Medicine 2011 levels of evidence lists N-of-1 trials as a high level-1 evidence for intervention-based research, especially when considering a clinical question within an individual patient [8].This is a similar level of evidence to an RCT.Case studies, however, are considered a lower level of evidence (level 4) given the potential for bias that influences the outcome.For reference, an RCT can be level 1 or 2 evidence and a nonrandomized observational study is a level 3.Additionally, N-of-1 trials are useful for patients already receiving a therapy when either the patient or clinician is unsure of the therapy's effectiveness [5,9].An example where an N-of-1 trial may be suited is when patients with epilepsy syndromes (e.g., juvenile myoclonic epilepsy) and multiple seizure types are using approved antiseizure drugs but there is a lack of class 1 evidence for the population.
Studies investigating different food processing techniques have shed light on the dietary habits and subsistence strategies adopted by prehistoric populations. They have shown that grinding cereals into flour has taken place since the Palaeolithic period, yet the grinding method employed has often not been investigated. The analysis presented here identified different types of use-wear traces associated with the dry-grinding and wet-grinding of cereals, which can be used to infer prehistoric grinding techniques. Applying this reference baseline to Jiahu, an early Neolithic site known for the earliest findings of domesticated rice in the central plain of China, reveals that dry-grinding rather than wet-grinding was employed for cereal (including rice) processing 9000 years ago. This grinding method could have been inherited from the earlier hunter-gatherers, but could also result from a broad-spectrum subsistence strategy adopted at Jiahu. By comparing the properties and ethnographic uses of different plant species, it is also suggested that cereals such as rice were a more sensible choice for the dry-grinding process. The ability to process food using various techniques is, along with mastery of fire and cooking, one of the key improvements that differentiated early humans from their antecedents and other animals (Wrangham, 2009; Wollstonecroft, 2011; Zink and Lieberman, 2016).Food processing facilitates the removal of undesirable substances in the raw material (Stahl et al., 1984; Johns, 1999; Stahl, 2014), helps to enhance the flavours, and extends the preservation period of foods (Caplice and Fitzgerald, 1999).
Studies investigating different food processing techniques have shed light on the dietary habits and subsistence strategies adopted by prehistoric populations. They have shown that grinding cereals into flour has taken place since the Palaeolithic period, yet the grinding method employed has often not been investigated. The analysis presented here identified different types of use-wear traces associated with the dry-grinding and wet-grinding of cereals, which can be used to infer prehistoric grinding techniques. Applying this reference baseline to Jiahu, an early Neolithic site known for the earliest findings of domesticated rice in the central plain of China, reveals that dry-grinding rather than wet-grinding was employed for cereal (including rice) processing 9000 years ago. This grinding method could have been inherited from the earlier hunter-gatherers, but could also result from a broad-spectrum subsistence strategy adopted at Jiahu. By comparing the properties and ethnographic uses of different plant species, it is also suggested that cereals such as rice were a more sensible choice for the dry-grinding process. Grinding is one of the most basic forms of food processing and has been passed down from the earliest humans (Stahl et al., 1984; Wollstonecroft, 2011).In the past two decades, extensive research has been carried out on grinding implements (e.g. van Gijn and Houkes, 2006; Tsoraki, 2007; Liu et al., 2014; Dubreuil and Nadel, 2015; Yang et al., 2015; Yang et al., 2016b; Fullagar et al., 2017).As revealed by research worldwide, a large proportion of grinding tools were employed for plant food processing (e.g. Piperno et al., 2004; Verbaas and Van Gijn, 2007; Hamon, 2008; Yang et al., 2009; Liu et al., 2016; Fullagar et al., 2017; García-Granero et al., 2017).
Studies investigating different food processing techniques have shed light on the dietary habits and subsistence strategies adopted by prehistoric populations. They have shown that grinding cereals into flour has taken place since the Palaeolithic period, yet the grinding method employed has often not been investigated. The analysis presented here identified different types of use-wear traces associated with the dry-grinding and wet-grinding of cereals, which can be used to infer prehistoric grinding techniques. Applying this reference baseline to Jiahu, an early Neolithic site known for the earliest findings of domesticated rice in the central plain of China, reveals that dry-grinding rather than wet-grinding was employed for cereal (including rice) processing 9000 years ago. This grinding method could have been inherited from the earlier hunter-gatherers, but could also result from a broad-spectrum subsistence strategy adopted at Jiahu. By comparing the properties and ethnographic uses of different plant species, it is also suggested that cereals such as rice were a more sensible choice for the dry-grinding process. Depending on the plants' properties and the dietary habits preferred by different human groups, some plant species may require additional treatment before the grinding process.The study of different techniques involved in food processing practices is important as they often reflect the culinary traditions and subsistence strategies adopted by prehistoric populations (Wright, 2004; Capparelli et al., 2011).For example, enlargement and other damage features recorded on starch grains of maize recovered from ancient human dental calculus in the Caribbean (Mickleburgh and Pagán-Jiménez, 2012), demonstrated that intense grinding of hard endosperm maize kernels in their mature state was carried out as part of selection and processing behaviours associated with the consumption of bread-like foods.In another case, the state of starch grains preserved on a grinding tool discovered at an Upper Palaeolithic site in Southern Italy indicates that thermal treatment of oats was performed before grinding (Mariotti Lippi et al., 2015).This additional stage possibly was applied in order to accelerate drying of the freshly cut cereal grains to make the subsequent processes easier and faster.
Studies investigating different food processing techniques have shed light on the dietary habits and subsistence strategies adopted by prehistoric populations. They have shown that grinding cereals into flour has taken place since the Palaeolithic period, yet the grinding method employed has often not been investigated. The analysis presented here identified different types of use-wear traces associated with the dry-grinding and wet-grinding of cereals, which can be used to infer prehistoric grinding techniques. Applying this reference baseline to Jiahu, an early Neolithic site known for the earliest findings of domesticated rice in the central plain of China, reveals that dry-grinding rather than wet-grinding was employed for cereal (including rice) processing 9000 years ago. This grinding method could have been inherited from the earlier hunter-gatherers, but could also result from a broad-spectrum subsistence strategy adopted at Jiahu. By comparing the properties and ethnographic uses of different plant species, it is also suggested that cereals such as rice were a more sensible choice for the dry-grinding process. The soaking of plant organs such as cereal grains is another common procedure used before grinding (e.g. Stock et al., 2000; Kethireddipalli et al., 2002; Wronkowska, 2016).Pre-soaking of cereals can neutralize compounds (e.g. phytic acid and enzyme inhibitors) that interfere with the absorption of nutrients in plants (Graf et al., 1987; Lopez et al., 2002; Soetan et al., 2010).Soaking can also stimulate the fermentation of sugar compounds in seeds, which is essential when making fermented products such as steamed cakes from rice (Rhee et al., 2011).According to the inscriptions from Bencaogangmu (an ancient Chinese book written in the Ming dynasty), wet-grinding (with a pre-soaking stage) was applied to plant processing at least 2000 years ago in China.However, whether this kind of grinding technique was employed in earlier periods has not been systematically investigated to date.
Studies investigating different food processing techniques have shed light on the dietary habits and subsistence strategies adopted by prehistoric populations. They have shown that grinding cereals into flour has taken place since the Palaeolithic period, yet the grinding method employed has often not been investigated. The analysis presented here identified different types of use-wear traces associated with the dry-grinding and wet-grinding of cereals, which can be used to infer prehistoric grinding techniques. Applying this reference baseline to Jiahu, an early Neolithic site known for the earliest findings of domesticated rice in the central plain of China, reveals that dry-grinding rather than wet-grinding was employed for cereal (including rice) processing 9000 years ago. This grinding method could have been inherited from the earlier hunter-gatherers, but could also result from a broad-spectrum subsistence strategy adopted at Jiahu. By comparing the properties and ethnographic uses of different plant species, it is also suggested that cereals such as rice were a more sensible choice for the dry-grinding process. Use-wear (or “microwear”) analysis has been applied to the study of grinding tools in recent decades (e.g. Adams, 1988; Gibaja Bao and Ferreira Bicho, 2015; Li et al., 2018; Liu et al., 2018).It shows great potential in inferring the worked material on tools, including bone, shell, antler, flax seeds, cereals, acorns (Quercus spp.) and wood (e.g. Van Gijn and Verbaas, 2009; Fullagar et al., 2012; Hayes, 2015; Hayes et al., 2017).Moreover, additives, such as water, may cause particular types of use-wear on stone tools (Grace, 1996; van Gijn and Little, 2016), as water acts as a lubricant during the grinding process, while simultaneously softening hard plant organs such as cereal grains.Hence, use-wear analysis has the potential to give new insights into the techniques and behaviours involved in the processing of plant organs such as the grinding of soaked or dry grains.
Studies investigating different food processing techniques have shed light on the dietary habits and subsistence strategies adopted by prehistoric populations. They have shown that grinding cereals into flour has taken place since the Palaeolithic period, yet the grinding method employed has often not been investigated. The analysis presented here identified different types of use-wear traces associated with the dry-grinding and wet-grinding of cereals, which can be used to infer prehistoric grinding techniques. Applying this reference baseline to Jiahu, an early Neolithic site known for the earliest findings of domesticated rice in the central plain of China, reveals that dry-grinding rather than wet-grinding was employed for cereal (including rice) processing 9000 years ago. This grinding method could have been inherited from the earlier hunter-gatherers, but could also result from a broad-spectrum subsistence strategy adopted at Jiahu. By comparing the properties and ethnographic uses of different plant species, it is also suggested that cereals such as rice were a more sensible choice for the dry-grinding process. In this study, different cereals were chosen for dry-grinding (without prior soaking) and wet-grinding processing, experimental grinding tools were selected, prepared and used to grind the cereals, and microwear analysis was conducted to measure the effect each technique had on the tool surfaces.The resulting reference baseline was then applied to the grinding tools obtained from Jiahu, an early Neolithic site in the central plain of China (Zhang, 1999, 2015) to evaluate the potential of use-wear analysis in the study of different grinding techniques and explore ancient grinding methods utilized in early Neolithic central China.
Studies investigating different food processing techniques have shed light on the dietary habits and subsistence strategies adopted by prehistoric populations. They have shown that grinding cereals into flour has taken place since the Palaeolithic period, yet the grinding method employed has often not been investigated. The analysis presented here identified different types of use-wear traces associated with the dry-grinding and wet-grinding of cereals, which can be used to infer prehistoric grinding techniques. Applying this reference baseline to Jiahu, an early Neolithic site known for the earliest findings of domesticated rice in the central plain of China, reveals that dry-grinding rather than wet-grinding was employed for cereal (including rice) processing 9000 years ago. This grinding method could have been inherited from the earlier hunter-gatherers, but could also result from a broad-spectrum subsistence strategy adopted at Jiahu. By comparing the properties and ethnographic uses of different plant species, it is also suggested that cereals such as rice were a more sensible choice for the dry-grinding process. Archaeological background of Jiahu
Studies investigating different food processing techniques have shed light on the dietary habits and subsistence strategies adopted by prehistoric populations. They have shown that grinding cereals into flour has taken place since the Palaeolithic period, yet the grinding method employed has often not been investigated. The analysis presented here identified different types of use-wear traces associated with the dry-grinding and wet-grinding of cereals, which can be used to infer prehistoric grinding techniques. Applying this reference baseline to Jiahu, an early Neolithic site known for the earliest findings of domesticated rice in the central plain of China, reveals that dry-grinding rather than wet-grinding was employed for cereal (including rice) processing 9000 years ago. This grinding method could have been inherited from the earlier hunter-gatherers, but could also result from a broad-spectrum subsistence strategy adopted at Jiahu. By comparing the properties and ethnographic uses of different plant species, it is also suggested that cereals such as rice were a more sensible choice for the dry-grinding process. The site of Jiahu is located in the upper catchment area of the Huai River valley (Fig. 1A).It is the earliest archaeological site to date that clearly exhibits the characteristics of rice agriculture (Zhao, 2010).Remains from Jiahu have been radiocarbon-dated and dendro-calibrated to three sub-phases (Zhang, 1999, 2015): Phase 1 (ca. 7000–6500 BCE), Phase 2 (ca. 6500–6000 BCE) and Phase 3 (ca. 6000–5500 BCE).
Studies investigating different food processing techniques have shed light on the dietary habits and subsistence strategies adopted by prehistoric populations. They have shown that grinding cereals into flour has taken place since the Palaeolithic period, yet the grinding method employed has often not been investigated. The analysis presented here identified different types of use-wear traces associated with the dry-grinding and wet-grinding of cereals, which can be used to infer prehistoric grinding techniques. Applying this reference baseline to Jiahu, an early Neolithic site known for the earliest findings of domesticated rice in the central plain of China, reveals that dry-grinding rather than wet-grinding was employed for cereal (including rice) processing 9000 years ago. This grinding method could have been inherited from the earlier hunter-gatherers, but could also result from a broad-spectrum subsistence strategy adopted at Jiahu. By comparing the properties and ethnographic uses of different plant species, it is also suggested that cereals such as rice were a more sensible choice for the dry-grinding process. Rice remains (Oryza sativa) (e.g. Fig. 1F) has been recovered from all three phases (Zhang, 1999; Yang et al., 2017; Zhang et al., 2018), and were identified as domesticated rice on the basis of their morphological features (Zhang and Wang, 1998; Liu et al., 2007a).Isotope analysis of human skeletons from Jiahu indicates that C3-based foods, including rice, dominated their diets throughout the site occupation (Hu et al., 2006).Rice was proven to be used as an ingredient for fermented beverage production 9000 years ago at Jiahu (McGovern et al., 2004).
Studies investigating different food processing techniques have shed light on the dietary habits and subsistence strategies adopted by prehistoric populations. They have shown that grinding cereals into flour has taken place since the Palaeolithic period, yet the grinding method employed has often not been investigated. The analysis presented here identified different types of use-wear traces associated with the dry-grinding and wet-grinding of cereals, which can be used to infer prehistoric grinding techniques. Applying this reference baseline to Jiahu, an early Neolithic site known for the earliest findings of domesticated rice in the central plain of China, reveals that dry-grinding rather than wet-grinding was employed for cereal (including rice) processing 9000 years ago. This grinding method could have been inherited from the earlier hunter-gatherers, but could also result from a broad-spectrum subsistence strategy adopted at Jiahu. By comparing the properties and ethnographic uses of different plant species, it is also suggested that cereals such as rice were a more sensible choice for the dry-grinding process. Even though these findings revealed that rice was already part of the diet of the early inhabitants, rice agriculture might have not played a major role in subsistence practices at Jiahu (Zhao, 2010).According to the analysis of 125 soil samples taken from different contexts at Jiahu from all three phases, rice has a lower recurrence rate than other plant species, including wild soybean (Glycine max subsp. soja), wild grape (Vitis spp.), water caltrops (Trapa spp.) and tubers such as lotus root (Nelumbo nucifera) (Zhao and Zhang, 2009).Quantitative analysis of artefacts associated with farming, hunting and fishing reveals that agricultural tools (e.g. Fig. 1B) account for 26% of such activity at Jiahu, slightly above fishing tools (24.8%, e.g. Fig. 1C) but less than hunting tools (49.2%, e.g. Fig. 1D) (Lai et al., 2009).Furthermore, fish bones and shells were abundant at the site, which were discovered from almost all trash pits (Zhang, 1999).
Humans have evolved a distinctive relationship with “objects” (tools and technology), which has strongly influenced their anatomical and cognitive capacities. The human hand is functionally specialized for manipulation and, in terms of cognition, tools are generally integrated into the body scheme when handled. Stone tools can supply information on the evolution of this cognitive reciprocal relationship. Despite the many studies on stone tool morphology, information on hand-tool system is scanty. In this preliminary survey, we measure hand-tool distances in three lithic instruments of different size (cleaver, handaxe, convergent sidescraper), in order to investigate basic patterns associated with their handling patterns. Tool size does influence the distance from the wrist and the aperture of the hand. The associated grasping differences depend more on the tool length than on the hand morphology or dimension. Nonetheless, hand-tool metrics covariation patterns are different according to the different tool types, suggesting specific factors associated with their respective haptic experience. Females display, on average, more variability than males when handling the sidescraper, but not for the power-gripped cleaver and handaxe. We propose a new method to analyze hand-tool metrics according to the haptic interaction. These kinds of studies provide basic mandatory information which can be used to develop proper ergonomic and cognitive perspectives in tool extension and cognitive archaeology. Primates have a particular ability in object manipulation when compared with other mammals.Although there is evidence of object-assisted behaviors in many primate species (Goodall, 1964; Van Lawick-Goodall, 1971; Luncz et al., 2016), only humans evolved a specialized tool-dependant culture (Plummer, 2004).The importance of the human-tool relationship is the main reason why technology is studied in many different fields ranging from biomechanics to rehabilitation medicine or paleoanthropology.
Humans have evolved a distinctive relationship with “objects” (tools and technology), which has strongly influenced their anatomical and cognitive capacities. The human hand is functionally specialized for manipulation and, in terms of cognition, tools are generally integrated into the body scheme when handled. Stone tools can supply information on the evolution of this cognitive reciprocal relationship. Despite the many studies on stone tool morphology, information on hand-tool system is scanty. In this preliminary survey, we measure hand-tool distances in three lithic instruments of different size (cleaver, handaxe, convergent sidescraper), in order to investigate basic patterns associated with their handling patterns. Tool size does influence the distance from the wrist and the aperture of the hand. The associated grasping differences depend more on the tool length than on the hand morphology or dimension. Nonetheless, hand-tool metrics covariation patterns are different according to the different tool types, suggesting specific factors associated with their respective haptic experience. Females display, on average, more variability than males when handling the sidescraper, but not for the power-gripped cleaver and handaxe. We propose a new method to analyze hand-tool metrics according to the haptic interaction. These kinds of studies provide basic mandatory information which can be used to develop proper ergonomic and cognitive perspectives in tool extension and cognitive archaeology. Tools that we use in everyday life have a design that conforms to their specific use.These functional features are perceived by our brain and channel our behavioral response so as to interact with the object.This process is strictly related with the concept of affordance.This term was defined by James J. Gibson in 1979, who stated that “affordances of the environment are what it offers the animal, what it provides or furnishes, either for good or ill.The verb to afford is found in the dictionary, but the noun affordance is not.I have made it up.I mean by it something that refers to both the environment and the animal in a way that no existing term does.It implies the complementarity of the animal and the environment” (Gibson, 1979).
Humans have evolved a distinctive relationship with “objects” (tools and technology), which has strongly influenced their anatomical and cognitive capacities. The human hand is functionally specialized for manipulation and, in terms of cognition, tools are generally integrated into the body scheme when handled. Stone tools can supply information on the evolution of this cognitive reciprocal relationship. Despite the many studies on stone tool morphology, information on hand-tool system is scanty. In this preliminary survey, we measure hand-tool distances in three lithic instruments of different size (cleaver, handaxe, convergent sidescraper), in order to investigate basic patterns associated with their handling patterns. Tool size does influence the distance from the wrist and the aperture of the hand. The associated grasping differences depend more on the tool length than on the hand morphology or dimension. Nonetheless, hand-tool metrics covariation patterns are different according to the different tool types, suggesting specific factors associated with their respective haptic experience. Females display, on average, more variability than males when handling the sidescraper, but not for the power-gripped cleaver and handaxe. We propose a new method to analyze hand-tool metrics according to the haptic interaction. These kinds of studies provide basic mandatory information which can be used to develop proper ergonomic and cognitive perspectives in tool extension and cognitive archaeology. The notion of affordance has acquired a multitude of meanings since then.Turvey and Carello (2011) define affordance as the behavioral possibilities of an object, while for others, affordances are those properties of an object that indicate how to interact with it (Borghi, 2007).The interaction with an object is based on previous personal experience.Other authors (Osiurak et al., 2017) conclude that “an affordance is an animal-relative, biomechanical property specifying an action possibility within a body/hand-centered frame of reference.Affordances correspond to a description of this possibility at a physical, but not at a neurocognitive level”.
Humans have evolved a distinctive relationship with “objects” (tools and technology), which has strongly influenced their anatomical and cognitive capacities. The human hand is functionally specialized for manipulation and, in terms of cognition, tools are generally integrated into the body scheme when handled. Stone tools can supply information on the evolution of this cognitive reciprocal relationship. Despite the many studies on stone tool morphology, information on hand-tool system is scanty. In this preliminary survey, we measure hand-tool distances in three lithic instruments of different size (cleaver, handaxe, convergent sidescraper), in order to investigate basic patterns associated with their handling patterns. Tool size does influence the distance from the wrist and the aperture of the hand. The associated grasping differences depend more on the tool length than on the hand morphology or dimension. Nonetheless, hand-tool metrics covariation patterns are different according to the different tool types, suggesting specific factors associated with their respective haptic experience. Females display, on average, more variability than males when handling the sidescraper, but not for the power-gripped cleaver and handaxe. We propose a new method to analyze hand-tool metrics according to the haptic interaction. These kinds of studies provide basic mandatory information which can be used to develop proper ergonomic and cognitive perspectives in tool extension and cognitive archaeology. The concept of tool is something associated to its effectiveness.Its use influences (and generally enhances) the behavioral capacity of the subject (Wagman and Carello, 2001).Following the principles of the Techno-Functional approach, tools are structurally linked to the user and the physical properties of tools condition how these tools are used (Boëda, 2013: 46).A tool can be seen as a prolongation of the limbs, directly connected to the hand and to the body, modifying the capacity of the individual for perceiving and acting.This tight connection effectively changes the boundary between the organism and the environment and extends effectiveness to accomplish a given target or functional requirement (Wagman and Carello, 2001).If there is no physical contact between the tool and the body, the tool is perceived as a part of the environment, and its specific affordances are limited to the visual information.However, when a tool is handled, it is integrated in the neural scheme of the body as a terminal part of the limbs (Maravita and Iriki, 2004; Quallo et al., 2009).Following this equivalence between body and tool, the tool is perceived as an extension of the body, or the body is perceived as a tool (Iriki and Sakura, 2008).From the archaeological point of view, Leroi-Gourhan has already conceived the tool as a phenomenon of externalization of the body (Leroi-Gourhan, 1965: 41–42).Therefore, it is no longer an external object, and it becomes a functional extension of the user, modifying the user's sensorimotor capacities (Osiurak et al., 2017).Tool use particularly influences the structure and functions of the parietal lobes and tool training can be effective in this sense even in non-human primates (Quallo et al., 2009).
Humans have evolved a distinctive relationship with “objects” (tools and technology), which has strongly influenced their anatomical and cognitive capacities. The human hand is functionally specialized for manipulation and, in terms of cognition, tools are generally integrated into the body scheme when handled. Stone tools can supply information on the evolution of this cognitive reciprocal relationship. Despite the many studies on stone tool morphology, information on hand-tool system is scanty. In this preliminary survey, we measure hand-tool distances in three lithic instruments of different size (cleaver, handaxe, convergent sidescraper), in order to investigate basic patterns associated with their handling patterns. Tool size does influence the distance from the wrist and the aperture of the hand. The associated grasping differences depend more on the tool length than on the hand morphology or dimension. Nonetheless, hand-tool metrics covariation patterns are different according to the different tool types, suggesting specific factors associated with their respective haptic experience. Females display, on average, more variability than males when handling the sidescraper, but not for the power-gripped cleaver and handaxe. We propose a new method to analyze hand-tool metrics according to the haptic interaction. These kinds of studies provide basic mandatory information which can be used to develop proper ergonomic and cognitive perspectives in tool extension and cognitive archaeology. In this process, the hand plays a crucial role.Whether the human hand evolved as an adaptation to tool-use is a recurrent question in anthropology (Marzke and Marzke, 2000), and tool-dependency in our genus suggests a selective pressure on the morphology of the hand.Unfortunately, morphological information on the hands of extinct human species is incredibly scanty (Bruner et al., 2016).Neanderthals had probably more robust hands than modern humans (Niewoehner, 2001; Churchill, 2001; Patiño et al., 2017) but functional interpretations are, in this sense, still speculative.Our hand is functionally adapted for tool manipulation and, vice versa, we produce ergonomic tools with features that optimize hand-tool interaction (Kadefors et al., 1993).Historically, Napier (1960) proposed two basic ways in which our fingers can interact with an object: precision grip and power grip.In the first case, the object is “pinched between the flexor aspects of the fingers and the opposing thumb”.In the second case, objects are grasped mainly by the fingers, and they are actively stabilized in the palm.Studies in hand biomechanics during simulated stone-tool use reveal a correlation between finger length and the force needed to stabilize the objects during the tool making (Rolian et al., 2011).The relative proportion between thumb and radial fingers is important when generating high finger pressure (Rolian et al., 2011).Besides proportions, there are of course more general aspects influencing the haptic experience.For example, somatosensory perception in human hand depends on hand size, with tactile capacity increasing when fingertip size decreases (Peters et al., 2009).
Humans have evolved a distinctive relationship with “objects” (tools and technology), which has strongly influenced their anatomical and cognitive capacities. The human hand is functionally specialized for manipulation and, in terms of cognition, tools are generally integrated into the body scheme when handled. Stone tools can supply information on the evolution of this cognitive reciprocal relationship. Despite the many studies on stone tool morphology, information on hand-tool system is scanty. In this preliminary survey, we measure hand-tool distances in three lithic instruments of different size (cleaver, handaxe, convergent sidescraper), in order to investigate basic patterns associated with their handling patterns. Tool size does influence the distance from the wrist and the aperture of the hand. The associated grasping differences depend more on the tool length than on the hand morphology or dimension. Nonetheless, hand-tool metrics covariation patterns are different according to the different tool types, suggesting specific factors associated with their respective haptic experience. Females display, on average, more variability than males when handling the sidescraper, but not for the power-gripped cleaver and handaxe. We propose a new method to analyze hand-tool metrics according to the haptic interaction. These kinds of studies provide basic mandatory information which can be used to develop proper ergonomic and cognitive perspectives in tool extension and cognitive archaeology. Most metric analyses in archaeology and experimental archaeology deal with measures of tools.Morphometrics applied to stone tools have been traditionally used to analyze the morphological variation of different tool types in order to produce cultural categories and investigate their morphological evolution (e.g., Serwatka and Riede, 2016; Cardillo, 2010).More recently, metric approaches have been also used to deal with technological and functional issues (Chacón et al., 2016 and references therein).All these procedures have considered tools as independent and isolated elements, without taking in account the hand-tool functional system.However, haptic dynamics and body cognition concern the physical integration of the two elements, the hand and the tool, and a morphological analysis of this functional unit is therefore necessary to investigate variations and discontinuities in the fossil record.
We report a case of neuronal intranuclear inclusion disease (NIID) confirmed by detection of intranuclear inclusions in a skin biopsy specimen. Brain magnetic resonance imaging showed mild cerebral atrophy and linear hyperintensities at the corticomedullary junction on diffusion-weighted images. This patient developed nonconvulsive status epilepticus with generalized periodic discharges on electroencephalography after recurrent symptoms of paroxysmal nausea and slowly progressive cognitive decline. There have been no previous reports of NIID with nonconvulsive status epilepticus to our knowledge. Since adult patients with NIID display a wide variety of clinical manifestations, skin biopsy should be considered in patients who have leukoencephalopathy of unknown origin. Neuronal intranuclear inclusion disease (NIID) is a rare and slowly progressive neurodegenerative disease that has been reported as both sporadic and familial cases.The cardinal symptoms of NIID are slowly progressive dementia associated with parkinsonism, cerebellar ataxia, and peripheral neuropathy [1–3].In addition, the disease is also associated with various types of seizures in adult and pediatric patients [2,3].Immunohistochemical examination of biopsy specimens from the skin or subcutaneous abdominal fat was recently reported to be useful for diagnosis of NIID [4,5].Case reports about patients with this disease have been gradually increasing, demonstrating considerable variation of its symptoms and clinical course [2,3].However, genetic analysis for NIID or diagnostic criteria for this disease has not been established to date.
For centuries, and until a few years ago, it was considered that the distillation process had been brought to the new world by the Spaniards, who in turn learned it from the Arabs. For this reason, it was believed that the only alcoholic beverage of the Mesoamerican societies was pulque - a ferment of maguey. However, recent archaeological investigations revealed that the alcohol distillation was known in Mesoamerica long before the arrival of Europeans, for at least 25 centuries. The direct evidence comes from the ceremonial and administrative center of Xochitécatl-Cacaxtla (state of Tlaxcala) with several ovens where cooked maguey remains were discowered. The corresponding archaeological context was radiometrically dated from 600 to 400 BCE. Here, we report a detailed archaeomagnetic study on burned archaeological artifacts found in these cooking ovens. 35 specimens belonging to two pottery fragments, one burned rock and two burned soil samples were pre-selected for archaeointensity experiments. Pottery samples exhibited essentially reversible behavior during thermomagnetic experiments pointing to Ti-poor titanomagnetite (almost magnetite phase) as main magnetic carrier while two ferromagnetic phase seems to co-exist in burned soils. In contrast, burned rock samples exhibited some instabilities during the heating at high temperatures and indirect evidence of the presence of antiferromagnetic hematite grains. In total, 29 specimens allowed the estimation of absolute geomagnetic intensity recorded during the last use of the furnace. Archaeomagnetic dating yielded two possible time intervals between 878 to 693 BCE and 557 to 487 BCE. These new data reinforce the initial hypothesis and corroborate the temporality of these pre-Hispanic kilns. The pre-Hispanic evidence of distilled beverages still remains highly controversial in Mesoamerica.A fermented drinks from the agave plant known as pulque (agave wine) was systematically consumed in central and northern Mesoamerica before European conquest.Tequila, however, was first produced in the 16th century in the town of Tequila (State of Jalisco, Western Mexico).The distillation procedure of Tequila involves only the blue agave (tequiliana weber), while any type of agave may be used to produce Mezcal.Thus, Tequila is a type of Mezcal.It is produced in almost everywhere in Mexico, being Oaxaca State the major Mezcal producer.
For centuries, and until a few years ago, it was considered that the distillation process had been brought to the new world by the Spaniards, who in turn learned it from the Arabs. For this reason, it was believed that the only alcoholic beverage of the Mesoamerican societies was pulque - a ferment of maguey. However, recent archaeological investigations revealed that the alcohol distillation was known in Mesoamerica long before the arrival of Europeans, for at least 25 centuries. The direct evidence comes from the ceremonial and administrative center of Xochitécatl-Cacaxtla (state of Tlaxcala) with several ovens where cooked maguey remains were discowered. The corresponding archaeological context was radiometrically dated from 600 to 400 BCE. Here, we report a detailed archaeomagnetic study on burned archaeological artifacts found in these cooking ovens. 35 specimens belonging to two pottery fragments, one burned rock and two burned soil samples were pre-selected for archaeointensity experiments. Pottery samples exhibited essentially reversible behavior during thermomagnetic experiments pointing to Ti-poor titanomagnetite (almost magnetite phase) as main magnetic carrier while two ferromagnetic phase seems to co-exist in burned soils. In contrast, burned rock samples exhibited some instabilities during the heating at high temperatures and indirect evidence of the presence of antiferromagnetic hematite grains. In total, 29 specimens allowed the estimation of absolute geomagnetic intensity recorded during the last use of the furnace. Archaeomagnetic dating yielded two possible time intervals between 878 to 693 BCE and 557 to 487 BCE. These new data reinforce the initial hypothesis and corroborate the temporality of these pre-Hispanic kilns. The origin of mezcal is a matter of debate and it is still unclear whether distilled drinks were produced in Mexico before the Spaniards arrival (Zizumbo-Villarreal et al., 2009).The idea about the pre-Hispanic origin became popular (Serra Puche, 1994, 1997a, 1997b; Serra Puche and Lazcano, 1997, 1998a, 1998b; Serra Puche and Palavicini, 1996; Serra Puche et al., 2000) since middle 90th.Serra Puche et al. (2000) and Serra Puche and Lazcano (2002a, 2002b), based on the study of pre-Hispanic conical kilns, argued that around 400 BCE the inhabitants of Xochitecatl-Cacaxtla (state of Tlaxcala) already knew the process of fermentation and distillation of Mezcal as a ritual drink.On the other hand, Colunga et al. (2013) found some vessels that may have been used to distill beverages between 1500 through 1000 BCE in the state of Colima (western Mexico).The great importance of agave as food since approximately 8000 BCE was first underlined by Flannery (1986) and Smith (1986) while other authors like Benz (2002), Benz et al. (2006), Schöndube, 2000, Zizumbo-Villarreal et al., 2009 documented its use as fiber and fermented beverage since the Late Formative period in western Mesoamerica (Colima and Jalisco states mainly).In this context, Kelly (1974, 1980) studied Capacha (Colima State) gourd and trifid vessels belonging to the Early Formative period (approx. 1500 through 1000 BCE) that could be used to produce distilled beverages.
For centuries, and until a few years ago, it was considered that the distillation process had been brought to the new world by the Spaniards, who in turn learned it from the Arabs. For this reason, it was believed that the only alcoholic beverage of the Mesoamerican societies was pulque - a ferment of maguey. However, recent archaeological investigations revealed that the alcohol distillation was known in Mesoamerica long before the arrival of Europeans, for at least 25 centuries. The direct evidence comes from the ceremonial and administrative center of Xochitécatl-Cacaxtla (state of Tlaxcala) with several ovens where cooked maguey remains were discowered. The corresponding archaeological context was radiometrically dated from 600 to 400 BCE. Here, we report a detailed archaeomagnetic study on burned archaeological artifacts found in these cooking ovens. 35 specimens belonging to two pottery fragments, one burned rock and two burned soil samples were pre-selected for archaeointensity experiments. Pottery samples exhibited essentially reversible behavior during thermomagnetic experiments pointing to Ti-poor titanomagnetite (almost magnetite phase) as main magnetic carrier while two ferromagnetic phase seems to co-exist in burned soils. In contrast, burned rock samples exhibited some instabilities during the heating at high temperatures and indirect evidence of the presence of antiferromagnetic hematite grains. In total, 29 specimens allowed the estimation of absolute geomagnetic intensity recorded during the last use of the furnace. Archaeomagnetic dating yielded two possible time intervals between 878 to 693 BCE and 557 to 487 BCE. These new data reinforce the initial hypothesis and corroborate the temporality of these pre-Hispanic kilns. The investigation conducted by Serra Puche and Lazcano (2016) was based on the production of mezcal from archaeological, ethno-historical and ethno-archaeological perspectives.Mezcal is obtained from the maguey through traditional work processes such as: the cooking of the pineapples (through the use of kilns dug in the ground in its most traditional form), crushing of cooked pineapple (with axes and clubs) and the fermentation, originally in wooden basin.Finally, the distillation process is achieved in primitive pots, currently copper and filtering dishes.The objective was to establish analogies which may potentially allow understanding the meaning of the cultural material documented in the housing units of the archaeological site of Xochitecatl-Cacaxtla.For this purpose, different studies were carried out in numerous communities in Mexico, southern United States and Guatemala in order to register the diversity of agaves and instruments used in the production of Mezcal to identify the degree of specialization and the current elements that allow to infer whether the archaeological evidence (furnaces, pots, etc.) found in the housing area of Xochitecatl-Cacaxtla has a relationship with elaboration of distilled beverages during the pre-Hispanic times (Serra Puche and Lazcano, 2016).
The easy accessibility and simplicity of Short Message Services (SMS) have made it attractive to malicious users thereby incurring unnecessary costing on the mobile users and the Network providers’ resources. The aim of this paper is to identify and review existing state of the art methodology for SMS spam based on some certain metrics: AI methods and techniques, approaches and deployed environment and the overall acceptability of existing SMS applications. This study explored eleven databases which include IEEE, Science Direct, Springer, Wiley, ACM, DBLP, Emerald, SU, Sage, Google Scholar, and Taylor and Francis, a total number of 1198 publications were found. Several screening criteria were conducted for relevant papers such as duplicate removal, removal based on irrelevancy, abstract eligibility based on the removal of papers with ambiguity (undefined methodology). Finally, 83 papers were identified for depth analysis and relevance. A quantitative evaluation was conducted on the selected studies using seven search strategies (SS): source, methods/ techniques, AI approach, architecture, status, datasets and SMS spam mobile applications. A Quantitative Analysis (QA) was conducted on the selected studies and the result based on existing methodology for classification shows that machine learning gave the highest result with 49% with algorithms such as Bayesian and support vector machines showing highest usage. Unlike statistical analysis with 39% and evolutionary algorithms gave 12%. However, the QA for feature selection methods shows that more studies utilized document frequency, term frequency and n-grams techniques for effective features selection process. Result based on existing approaches for content-based, non-content and hybrid approaches is 83%, 5%, and 12% respectively. The QA based on architecture shows that 25% of existing solutions are deployed on the client side, 19% on server-side, 6% collaborative and 50% unspecified. This survey was able to identify the status of existing SMS spam research as 35% of existing study was based on proposed new methods using existing algorithms and 29% based on only evaluation of existing algorithms, 20% was based on proposed methods only. This study concludes with very interesting findings which shows that the majority of existing SMS spam filtering solutions are still between the “Proposed” status or “Proposed and Evaluated” status. In addition, the taxonomy of existing state of the art methodologies is developed and it is concluded that 8.23% of Android users actually utilize this existing SMS anti-spam applications. Our study also concludes that there is a need for researchers to exploit all security methods and algorithm to secure SMS thus enhancing further classification in other short message platforms. A new English SMS spam dataset is also generated for future research efforts in Text mining, Tele-marketing for reducing global spam activities. The continuous escalation of mobile devices over the years has given users an unbeatable communication experience which has increased users’ performance efficiently (Tully and Mohanraj, 2017).The positive impact of mobile devices on users has made information literally in the palm of everyone (Davidow, 2011).However; users are yet to move with the changes associated with understanding and managing these devices.The most popular and widely used service of the Global System for Mobile communication (GSM) is the Short Message Services known as SMS.This fast and ever-growing service has reached more than 6 billion users globally with approximately 9.5 trillion SMS sent globally in 2009 (Fernandes et al., 2015; Ezpeleta et al., 2016).Thus, SMS service has shown its uniqueness and proven its reliability as it remains one of the most primary communication tools among mobile users based on its ease of use, cheap, compatible and real-time services.
The easy accessibility and simplicity of Short Message Services (SMS) have made it attractive to malicious users thereby incurring unnecessary costing on the mobile users and the Network providers’ resources. The aim of this paper is to identify and review existing state of the art methodology for SMS spam based on some certain metrics: AI methods and techniques, approaches and deployed environment and the overall acceptability of existing SMS applications. This study explored eleven databases which include IEEE, Science Direct, Springer, Wiley, ACM, DBLP, Emerald, SU, Sage, Google Scholar, and Taylor and Francis, a total number of 1198 publications were found. Several screening criteria were conducted for relevant papers such as duplicate removal, removal based on irrelevancy, abstract eligibility based on the removal of papers with ambiguity (undefined methodology). Finally, 83 papers were identified for depth analysis and relevance. A quantitative evaluation was conducted on the selected studies using seven search strategies (SS): source, methods/ techniques, AI approach, architecture, status, datasets and SMS spam mobile applications. A Quantitative Analysis (QA) was conducted on the selected studies and the result based on existing methodology for classification shows that machine learning gave the highest result with 49% with algorithms such as Bayesian and support vector machines showing highest usage. Unlike statistical analysis with 39% and evolutionary algorithms gave 12%. However, the QA for feature selection methods shows that more studies utilized document frequency, term frequency and n-grams techniques for effective features selection process. Result based on existing approaches for content-based, non-content and hybrid approaches is 83%, 5%, and 12% respectively. The QA based on architecture shows that 25% of existing solutions are deployed on the client side, 19% on server-side, 6% collaborative and 50% unspecified. This survey was able to identify the status of existing SMS spam research as 35% of existing study was based on proposed new methods using existing algorithms and 29% based on only evaluation of existing algorithms, 20% was based on proposed methods only. This study concludes with very interesting findings which shows that the majority of existing SMS spam filtering solutions are still between the “Proposed” status or “Proposed and Evaluated” status. In addition, the taxonomy of existing state of the art methodologies is developed and it is concluded that 8.23% of Android users actually utilize this existing SMS anti-spam applications. Our study also concludes that there is a need for researchers to exploit all security methods and algorithm to secure SMS thus enhancing further classification in other short message platforms. A new English SMS spam dataset is also generated for future research efforts in Text mining, Tele-marketing for reducing global spam activities. The growth of mobile users has increased the use of SMS service which has sustained great profits with global SMS revenue close to 117.2 billion dollars forecasted for 2017 (PortioResearch, 2014).There exist other short message applications such as WhatsApp, Hangout, Viber, WeChat, etc. with other functionalities namely calls, multimedia, etc.However, SMS differs from all the aforementioned messaging application due to its independence on internet services, no constant need for updates, unlike other applications.This tremendous growth in mobile devices has made SMS a very attractive area to malicious organizations for carrying out illegal activities and influencing security risks such as SMS spam, Phishing, License to kill Spyware, Malware, and privacy issues to mobile data.Pew Research Centre reported that over 69% of mobile users have received SMS spam (Boyles and Rainie, 2012).
The easy accessibility and simplicity of Short Message Services (SMS) have made it attractive to malicious users thereby incurring unnecessary costing on the mobile users and the Network providers’ resources. The aim of this paper is to identify and review existing state of the art methodology for SMS spam based on some certain metrics: AI methods and techniques, approaches and deployed environment and the overall acceptability of existing SMS applications. This study explored eleven databases which include IEEE, Science Direct, Springer, Wiley, ACM, DBLP, Emerald, SU, Sage, Google Scholar, and Taylor and Francis, a total number of 1198 publications were found. Several screening criteria were conducted for relevant papers such as duplicate removal, removal based on irrelevancy, abstract eligibility based on the removal of papers with ambiguity (undefined methodology). Finally, 83 papers were identified for depth analysis and relevance. A quantitative evaluation was conducted on the selected studies using seven search strategies (SS): source, methods/ techniques, AI approach, architecture, status, datasets and SMS spam mobile applications. A Quantitative Analysis (QA) was conducted on the selected studies and the result based on existing methodology for classification shows that machine learning gave the highest result with 49% with algorithms such as Bayesian and support vector machines showing highest usage. Unlike statistical analysis with 39% and evolutionary algorithms gave 12%. However, the QA for feature selection methods shows that more studies utilized document frequency, term frequency and n-grams techniques for effective features selection process. Result based on existing approaches for content-based, non-content and hybrid approaches is 83%, 5%, and 12% respectively. The QA based on architecture shows that 25% of existing solutions are deployed on the client side, 19% on server-side, 6% collaborative and 50% unspecified. This survey was able to identify the status of existing SMS spam research as 35% of existing study was based on proposed new methods using existing algorithms and 29% based on only evaluation of existing algorithms, 20% was based on proposed methods only. This study concludes with very interesting findings which shows that the majority of existing SMS spam filtering solutions are still between the “Proposed” status or “Proposed and Evaluated” status. In addition, the taxonomy of existing state of the art methodologies is developed and it is concluded that 8.23% of Android users actually utilize this existing SMS anti-spam applications. Our study also concludes that there is a need for researchers to exploit all security methods and algorithm to secure SMS thus enhancing further classification in other short message platforms. A new English SMS spam dataset is also generated for future research efforts in Text mining, Tele-marketing for reducing global spam activities. Spam can be considered as any unsolicited, commercial, bulk electronic message (Graham, 2002) which may sometimes convey un-demanded adverts, viruses, malware or other annoying contents targeted at consumers, businesses or government organizations.Spam comes in different medium like email, SMS, Instant message (SPIM), Usenet newsgroup, social network, search engines, Internet Telephony, etc. (Abayomi-Alli, 2009).However, differentiating all these spam media is technically cumbersome for a review (Blanzieri and Bryl, 2008).Skudlark (2014) reports SMS spam as not only offensive to users, but also incurs unnecessary cost on both MNOs and customers.The effects of SMS spam on users cut across different area of life, including financial, security/ privacy, education, career, health, social networking, etc.
The easy accessibility and simplicity of Short Message Services (SMS) have made it attractive to malicious users thereby incurring unnecessary costing on the mobile users and the Network providers’ resources. The aim of this paper is to identify and review existing state of the art methodology for SMS spam based on some certain metrics: AI methods and techniques, approaches and deployed environment and the overall acceptability of existing SMS applications. This study explored eleven databases which include IEEE, Science Direct, Springer, Wiley, ACM, DBLP, Emerald, SU, Sage, Google Scholar, and Taylor and Francis, a total number of 1198 publications were found. Several screening criteria were conducted for relevant papers such as duplicate removal, removal based on irrelevancy, abstract eligibility based on the removal of papers with ambiguity (undefined methodology). Finally, 83 papers were identified for depth analysis and relevance. A quantitative evaluation was conducted on the selected studies using seven search strategies (SS): source, methods/ techniques, AI approach, architecture, status, datasets and SMS spam mobile applications. A Quantitative Analysis (QA) was conducted on the selected studies and the result based on existing methodology for classification shows that machine learning gave the highest result with 49% with algorithms such as Bayesian and support vector machines showing highest usage. Unlike statistical analysis with 39% and evolutionary algorithms gave 12%. However, the QA for feature selection methods shows that more studies utilized document frequency, term frequency and n-grams techniques for effective features selection process. Result based on existing approaches for content-based, non-content and hybrid approaches is 83%, 5%, and 12% respectively. The QA based on architecture shows that 25% of existing solutions are deployed on the client side, 19% on server-side, 6% collaborative and 50% unspecified. This survey was able to identify the status of existing SMS spam research as 35% of existing study was based on proposed new methods using existing algorithms and 29% based on only evaluation of existing algorithms, 20% was based on proposed methods only. This study concludes with very interesting findings which shows that the majority of existing SMS spam filtering solutions are still between the “Proposed” status or “Proposed and Evaluated” status. In addition, the taxonomy of existing state of the art methodologies is developed and it is concluded that 8.23% of Android users actually utilize this existing SMS anti-spam applications. Our study also concludes that there is a need for researchers to exploit all security methods and algorithm to secure SMS thus enhancing further classification in other short message platforms. A new English SMS spam dataset is also generated for future research efforts in Text mining, Tele-marketing for reducing global spam activities. The bulk of SMS received by mobile users is very disturbing (Androulidakis, 2016) with trends majorly in promotional offers, fake lottery notifications, new tariffs from Mobile Network Operators (MNOs), bank credit offers, and a bunch of unwanted advertisements (Nagwani and Sharaff, 2017).These SMS spam intrude into users’ mobile devices, consume device memory and making users extremely irritated (Chen et al., 2014) through their countless fraudulent websites for other illegal pursuits such as stealing private information or spreading malicious Apps aimed at causing users financial loss (Jiang et al., 2013).The ease of opening SMS immediately and the convenience of dialing numbers or opening embedded links, definitely makes phishers and fraudsters tasks easier (Eshmawi and Nair, 2014).However, with the advent of more resource enabled smartphones, mobile fraud will be on the rise, hence MNOs must aggressively take steps to shield their customers (Tajuddin et al., 2014) from such attacks.
The easy accessibility and simplicity of Short Message Services (SMS) have made it attractive to malicious users thereby incurring unnecessary costing on the mobile users and the Network providers’ resources. The aim of this paper is to identify and review existing state of the art methodology for SMS spam based on some certain metrics: AI methods and techniques, approaches and deployed environment and the overall acceptability of existing SMS applications. This study explored eleven databases which include IEEE, Science Direct, Springer, Wiley, ACM, DBLP, Emerald, SU, Sage, Google Scholar, and Taylor and Francis, a total number of 1198 publications were found. Several screening criteria were conducted for relevant papers such as duplicate removal, removal based on irrelevancy, abstract eligibility based on the removal of papers with ambiguity (undefined methodology). Finally, 83 papers were identified for depth analysis and relevance. A quantitative evaluation was conducted on the selected studies using seven search strategies (SS): source, methods/ techniques, AI approach, architecture, status, datasets and SMS spam mobile applications. A Quantitative Analysis (QA) was conducted on the selected studies and the result based on existing methodology for classification shows that machine learning gave the highest result with 49% with algorithms such as Bayesian and support vector machines showing highest usage. Unlike statistical analysis with 39% and evolutionary algorithms gave 12%. However, the QA for feature selection methods shows that more studies utilized document frequency, term frequency and n-grams techniques for effective features selection process. Result based on existing approaches for content-based, non-content and hybrid approaches is 83%, 5%, and 12% respectively. The QA based on architecture shows that 25% of existing solutions are deployed on the client side, 19% on server-side, 6% collaborative and 50% unspecified. This survey was able to identify the status of existing SMS spam research as 35% of existing study was based on proposed new methods using existing algorithms and 29% based on only evaluation of existing algorithms, 20% was based on proposed methods only. This study concludes with very interesting findings which shows that the majority of existing SMS spam filtering solutions are still between the “Proposed” status or “Proposed and Evaluated” status. In addition, the taxonomy of existing state of the art methodologies is developed and it is concluded that 8.23% of Android users actually utilize this existing SMS anti-spam applications. Our study also concludes that there is a need for researchers to exploit all security methods and algorithm to secure SMS thus enhancing further classification in other short message platforms. A new English SMS spam dataset is also generated for future research efforts in Text mining, Tele-marketing for reducing global spam activities. Recently, mobile security has become a major concern, as commercial activities are drifting from the traditional PC-based platform to the mobile phone platform (Lau, 2017).Spamming challenges in email has been showing significant growth and improvement over the years, unlike the SMS spam which is still counting its growth (Androulidakis et al., 2013).The application of Artificial Intelligence (AI) in automatic text classification is quite encouraging mostly in the areas of email classification but there is still need to explore more algorithms for effective short text classification especially in the areas of SMS and microblog (Huang et al., 2018) Thus, the motivation for this systematic literature review (SLR) is based on the continuous and annoying growth of short text messages in mobile devices.Additionally, existing survey papers on SMS spam have not been able to justify the different existing measures adopted by researchers in solving the SMS spam challenge.Thus, this study aims at identifying all possible measures (methods/techniques, approaches, architecture) adopted by researchers in SMS spam classification, thus enabling future researchers the opportunity exploits unused or underutilized methodology with the aim of improving accuracy and overall system performance.
Primary debris found in an Iron Age bath-shaped vessel excavated from Zincirli, ancient Sam'al, Turkey, was subjected to organic residue analysis by gas chromatography–mass spectrometry (GC–MS). Bath-shaped vessels are usually interpreted as burial coffins, but recently these vessels have been linked to crafts manufacture, specifically wool processing. To test this hypothesis, we compared the total lipid extract (TLE) of the debris to six reference standards drawn from a study of Old World wool processing practices. Key components that may reflect the remains of date palm kernel oil, a substance known to be used in wool washing, were identified. In this study, gas chromatography coupled with mass spectrometry (GC–MS) was used to analyze the total lipid extract (TLE) of pebble debris found inside an Iron Age bath-shaped vessel (see Section 2.1).Residues in ceramic sherds and other porous structures may be analyzed to deconvolve their use (Evershed, 2008).The recent discovery of primary debris in a bath-shaped vessel that was discovered in-situ in a domestic context at Zincirli, Turkey, provides an opportunity to test the hypothesis that bath-shaped vessels were used for wool-processing (e.g. scouring raw wool and fulling woven woolen textiles).
An accurate short-term traffic forecasting model serves as an integral part to enhance the efficiency of vehicle rerouting and traffic light control strategies. The information exchange (pheromone) behavior of ants has been applied to forecast traffic conditions in existing pheromone models. These models were developed to forecast congestion on roads with signalized intersections by considering only green and red phases. Motivated by this issue, three short-term traffic forecasting models are proposed: (i) Extended Pheromone Model (EPM), (ii) Extended Pheromone Model with epsilon-Support Vector Regression (εSVR-EPM), and (iii) Extended Pheromone Model with Artificial Neural Network and Particle Swarm Optimization (ANNPSO-EPM). It is worth noticing that EPM is an algorithmic model whereas the other two are machine learning models. In all proposed models, a new color pheromone concept is proposed with two significant contributions. First, the color pheromone concept is developed to capture stochastic traffic conditions on the roads with non-signalized intersections. Second, the proposed concept is further extended to include all three color phases (red, yellow and green) to forecast dynamic changing traffic behaviors for roads with signalized intersections. The proposed color pheromone concept in EPM, εSVR-EPM, and ANNPSO-EPM is different from the existing models as it dynamically switches its computation techniques based on traffic light phases. All three proposed models can be realized through a Pheromone-based Multi-Agent System composed of Vehicle Agents and Intersection Agents coordinating locally with one another To promote practicality, Singapore City Hall map is employed in a microscopic simulator of Simulation of Urban Mobility (SUMO), showing that all proposed models outperform the other existing pheromone models. Background
An accurate short-term traffic forecasting model serves as an integral part to enhance the efficiency of vehicle rerouting and traffic light control strategies. The information exchange (pheromone) behavior of ants has been applied to forecast traffic conditions in existing pheromone models. These models were developed to forecast congestion on roads with signalized intersections by considering only green and red phases. Motivated by this issue, three short-term traffic forecasting models are proposed: (i) Extended Pheromone Model (EPM), (ii) Extended Pheromone Model with epsilon-Support Vector Regression (εSVR-EPM), and (iii) Extended Pheromone Model with Artificial Neural Network and Particle Swarm Optimization (ANNPSO-EPM). It is worth noticing that EPM is an algorithmic model whereas the other two are machine learning models. In all proposed models, a new color pheromone concept is proposed with two significant contributions. First, the color pheromone concept is developed to capture stochastic traffic conditions on the roads with non-signalized intersections. Second, the proposed concept is further extended to include all three color phases (red, yellow and green) to forecast dynamic changing traffic behaviors for roads with signalized intersections. The proposed color pheromone concept in EPM, εSVR-EPM, and ANNPSO-EPM is different from the existing models as it dynamically switches its computation techniques based on traffic light phases. All three proposed models can be realized through a Pheromone-based Multi-Agent System composed of Vehicle Agents and Intersection Agents coordinating locally with one another To promote practicality, Singapore City Hall map is employed in a microscopic simulator of Simulation of Urban Mobility (SUMO), showing that all proposed models outperform the other existing pheromone models. Increased traffic volumes in limited road capacities due to recent rapid urbanization has caused undesirable traffic congestion, emerging as a serious issue for engineers in many modern metropolitan cities specifically New York, London and Singapore (Bazzan et al., 2008).According to a recent urban transportation report (Schrank et al., 2012), the estimated total economic losses in the U.S. for both travel time delay and fuel consumption due to congestion was $121 billion in 2011 and is expected to reach $199 billion in 2020.Hence, a robust short-term traffic forecasting model is integral to provide accurate information to Intelligent Transport System (ITS) to address traffic congestion.Short-term traffic forecasting entails the prediction from a few seconds to a few hours through past and current traffic conditions (Vlahogianni et al., 2014).The ITS system benefits from the anticipated congestion in near future to develop strategies to manage stochastic traffic conditions (Goves et al., 2016).These strategies can be categorized into vehicle rerouting and traffic light control (France and Ghorbani, 2003; Bazzan et al., 2008).Short-term prediction provides useful information to vehicles, allowing them to change their route to avoid congestion in near future.Similarly, traffic lights can be coordinated to generate green wave scenarios once the congestion is predicted.
An accurate short-term traffic forecasting model serves as an integral part to enhance the efficiency of vehicle rerouting and traffic light control strategies. The information exchange (pheromone) behavior of ants has been applied to forecast traffic conditions in existing pheromone models. These models were developed to forecast congestion on roads with signalized intersections by considering only green and red phases. Motivated by this issue, three short-term traffic forecasting models are proposed: (i) Extended Pheromone Model (EPM), (ii) Extended Pheromone Model with epsilon-Support Vector Regression (εSVR-EPM), and (iii) Extended Pheromone Model with Artificial Neural Network and Particle Swarm Optimization (ANNPSO-EPM). It is worth noticing that EPM is an algorithmic model whereas the other two are machine learning models. In all proposed models, a new color pheromone concept is proposed with two significant contributions. First, the color pheromone concept is developed to capture stochastic traffic conditions on the roads with non-signalized intersections. Second, the proposed concept is further extended to include all three color phases (red, yellow and green) to forecast dynamic changing traffic behaviors for roads with signalized intersections. The proposed color pheromone concept in EPM, εSVR-EPM, and ANNPSO-EPM is different from the existing models as it dynamically switches its computation techniques based on traffic light phases. All three proposed models can be realized through a Pheromone-based Multi-Agent System composed of Vehicle Agents and Intersection Agents coordinating locally with one another To promote practicality, Singapore City Hall map is employed in a microscopic simulator of Simulation of Urban Mobility (SUMO), showing that all proposed models outperform the other existing pheromone models. In transportation, the pheromone model is an algorithmic model that is inspired by the intelligent collective behavior of ants communicating through a medium called pheromone, when foraging a source of food from their nest.Although each individual ant has only a local searching capability, the collective behavior of information exchange medium (pheromone) leads to successful search of food.Originating from this self-organizing behavior, Ant Colony Optimization (ACO) algorithm (Dorigo et al., 2006) has been developed to solve numerous complex computational problems, including the field of transportation.Inspired by the information exchange (pheromone) behavior, several pheromone-based traffic forecasting models were developed to forecast future traffic behavior.Specifically, each vehicle (Vehicle Agent) deposits pheromone information, which is collected by Intersection Agents to perform traffic forecast.The system descriptions of this information exchange between Vehicle agents and Intersection Agents are detailed in Section 1.2.Nevertheless, the existing pheromone models (Jiang et al., 2014; Kurihara et al., 2009; Cao et al., 2017) suffer from several limitations illustrated below:
An accurate short-term traffic forecasting model serves as an integral part to enhance the efficiency of vehicle rerouting and traffic light control strategies. The information exchange (pheromone) behavior of ants has been applied to forecast traffic conditions in existing pheromone models. These models were developed to forecast congestion on roads with signalized intersections by considering only green and red phases. Motivated by this issue, three short-term traffic forecasting models are proposed: (i) Extended Pheromone Model (EPM), (ii) Extended Pheromone Model with epsilon-Support Vector Regression (εSVR-EPM), and (iii) Extended Pheromone Model with Artificial Neural Network and Particle Swarm Optimization (ANNPSO-EPM). It is worth noticing that EPM is an algorithmic model whereas the other two are machine learning models. In all proposed models, a new color pheromone concept is proposed with two significant contributions. First, the color pheromone concept is developed to capture stochastic traffic conditions on the roads with non-signalized intersections. Second, the proposed concept is further extended to include all three color phases (red, yellow and green) to forecast dynamic changing traffic behaviors for roads with signalized intersections. The proposed color pheromone concept in EPM, εSVR-EPM, and ANNPSO-EPM is different from the existing models as it dynamically switches its computation techniques based on traffic light phases. All three proposed models can be realized through a Pheromone-based Multi-Agent System composed of Vehicle Agents and Intersection Agents coordinating locally with one another To promote practicality, Singapore City Hall map is employed in a microscopic simulator of Simulation of Urban Mobility (SUMO), showing that all proposed models outperform the other existing pheromone models. Motivated by these issues, an algorithmic model of ExtendedPheromone Model (EPM) and two machine learning models of Extended Pheromone Model with epsilon-Support Vector Regression(εSVR-EPM) and Extended Pheromone Model with Artificial Neural Network and Particle Swarm Optimization (ANNPSO-EPM) are proposed with the following contributions.First, a new color pheromone concept is extended to estimate future traffic density for roads with both signalized intersections (all three color phases) and non-signalized intersections.In addition, the current traffic density is computed via the traffic pheromone intensity.Second, a new Propagation–Evaporation Pair is proposed to fuse traffic pheromone and color pheromone to form the EPM model.Third, two machine learning techniques are incorporated in the fusion of these pheromone intensities to further enhance prediction accuracy, leading to two hybrid models of εSVR-EPM and ANNPSO-EPM respectively.
Christianson syndrome (CS) is an X-linked intellectual disorder caused by mutations in the SLC9A6 gene. Clinical features of CS include an inability to speak, truncal ataxia, postnatal microcephaly, hyperkinesis, and epilepsy. Almost all patients with CS develop drug-resistant epilepsy—its most serious complication. We report two cases of CS with drug-resistant epilpesy associated with the Lennox–Gastaut syndrome (LGS). One patient experienced generalized tonic seizures since 9 months of age with cognitive regression, which evolved to include atonic seizures at the age of 7 years. Electroencephalography (EEG) showed generalized slow spike–wave complexes and generalized paroxysmal fast activity. Seizures remained drug-resistant despite multiple anti-seizure drugs. The second patient experienced generalized tonic seizures since the age of 17 months and arrested development. EEG showed generalized slow spike–wave complexes, with frequent atonic seizures since the age of 6 years. Electrical status epilepticus during slow-wave sleep (ESES) developed at the age of 7 years. Our cases illustrate that CS may cause LGS in addition to other developmental and epileptic encephalopathies of the neonatal and infantile period. We suggest that generalized tonic or tonic–clonic seizures and generalized slow spike–wave complexes in interictal EEG be included as potential electroclinical features of epilepsy in CS. Christianson syndrome (CS) is an X-linked condition that was first reported by Christianson et al. in South African families who displayed severe intellectual disability, epilepsy, mild craniofacial dysmorphology, ophthalmoplegia, speech absence, acquired microcephaly, and cerebellar and brainstem atrophy [1].Later, patients were also reported from other regions in Europe [2].SLC9A6 was identified as the causative gene; moreover, clinical features in patients with CS may overlap with those seen in Angelman syndrome [2].Pescosolido et al., based on a study on 12 families note the characteristic symptoms of CS include an inability to speak, moderate to severe intellectual disability, epilepsy in 100% of patients, truncal ataxia, postnatal microcephaly or growth arrest of head circumference, and hyperkinesis [3].Female carriers are known to manifest mild to moderate intellectual disabilities, behavioral issues, and psychiatric illnesses [4].
Christianson syndrome (CS) is an X-linked intellectual disorder caused by mutations in the SLC9A6 gene. Clinical features of CS include an inability to speak, truncal ataxia, postnatal microcephaly, hyperkinesis, and epilepsy. Almost all patients with CS develop drug-resistant epilepsy—its most serious complication. We report two cases of CS with drug-resistant epilpesy associated with the Lennox–Gastaut syndrome (LGS). One patient experienced generalized tonic seizures since 9 months of age with cognitive regression, which evolved to include atonic seizures at the age of 7 years. Electroencephalography (EEG) showed generalized slow spike–wave complexes and generalized paroxysmal fast activity. Seizures remained drug-resistant despite multiple anti-seizure drugs. The second patient experienced generalized tonic seizures since the age of 17 months and arrested development. EEG showed generalized slow spike–wave complexes, with frequent atonic seizures since the age of 6 years. Electrical status epilepticus during slow-wave sleep (ESES) developed at the age of 7 years. Our cases illustrate that CS may cause LGS in addition to other developmental and epileptic encephalopathies of the neonatal and infantile period. We suggest that generalized tonic or tonic–clonic seizures and generalized slow spike–wave complexes in interictal EEG be included as potential electroclinical features of epilepsy in CS. Epilepsy in CS is often drug-resistant and is a leading concern for family members [3].While there have been several reports of electrical status epilepticus during slow-wave sleep (ESES) [5–7], few reports provide detailed description regarding the long-term course of epilepsy in CS.
Constructing effective models for detecting, reducing, and/or preventing adverse events is very important in domains such as aviation safety, healthcare, drug administration, and war theaters. This study presents batch and data streaming models to detecting adverse events using data from a war theater context. In all the previous studies, regression models and several machine learning techniques were used for predicting continuous values in an active theater of war, and the error values reported on the test sets were large. In order to overcome the shortcoming, this study investigates the effectiveness of batch and data streaming classification algorithms in detecting or classifying adverse events given infrastructure development spending data and other variables in an active theater of war in Afghanistan. By the feature selection, the valid input variables are obtained and their indexes show that the input variables are mainly the adverse events (t-1) at the previous month, the population densities and related project investments. From the country level, fewer of the 14 project investments affect the adverse events. From the region level, some projects with higher index values, such as Security in the South Western region, Energy and Emergency Assistance in the North Eastern region, and Education in the Eastern region are mainly affecting factors. Three batch classification methods and three data streaming classification methods were assessed for their ability to detect adverse events given infrastructure development data. The study uses cost-sensitive measures to address the very unbalanced nature of the data and it applies variable reduction techniques to identify significant variables. The three batch classification algorithms are C4.5, k-nearest Neighbor, and Support Vector Machine. The three data streaming algorithms are Naïve Bayes, Hoeffding Tree, and Single Classifier Drift. In general, the performance of the cost-sensitive methods in the batch setting is comparable to those in the data stream setting. However, in the batch setting the cost matrix needs to be adjusted manually. In contrast the data stream setting allows one to adjust the models based on the analysis of the classifiers’ performance over time and changing data distribution. The Kappa values using Naïve Bayes are the highest in the three data stream algorithms in the whole country and its regions. The Naïve Bayes classifier has the best global performance. By the Kappa statistic curve, we can observe the concept drifts. In a region level, many models have a better performance including more investments related to project compared with those in a country level. In addition as data distribution becomes more balanced, the classifiers in the data stream setting outperform in terms of the overall classification rates in comparison to the classifiers in the batch setting. The results thus demonstrate the potential of data streaming algorithms to significantly outperform when the data become less unbalanced, and can be used for detecting adverse events in similar areas. Building reliable models for predicting, detecting, mitigating, and/or preventing adverse events is crucial in the aviation industry (Reveley et al., 2011), healthcare institutions (Rafter et al., 2015; Rochefort et al., 2015), drug administration (Armitage and Knapman, 2003; Casillas et al., 2016), and an active war theater (Çakıt and Karwowski, 2018) to name a few.In the last case, adverse events are caused by terrorist activities that primarily target the civilian population in countries such as Afghanistan.The U.S. Department of Defense (DoD) developed the Human Social Culture Behavior (HSCB) modeling program (Bhattacharjee, 2007; HSCB Modeling Program, 2009) to help the military better understand different cultures while conducting a war in overseas countries, to supervise the human terrain, to shelter the civilians from terrorist activities (Drapeau and Mignone, 2007), and finally to undertake infrastructure development efforts to stabilize the country of Afghanistan, and consequently to counter or reduce terrorist events.However, assessing the effect of these efforts presents challenges as the data used to build models exhibit nonlinear and fuzzy behavior and are often ill defined with respect to their socio-economic-cultural factors.
Constructing effective models for detecting, reducing, and/or preventing adverse events is very important in domains such as aviation safety, healthcare, drug administration, and war theaters. This study presents batch and data streaming models to detecting adverse events using data from a war theater context. In all the previous studies, regression models and several machine learning techniques were used for predicting continuous values in an active theater of war, and the error values reported on the test sets were large. In order to overcome the shortcoming, this study investigates the effectiveness of batch and data streaming classification algorithms in detecting or classifying adverse events given infrastructure development spending data and other variables in an active theater of war in Afghanistan. By the feature selection, the valid input variables are obtained and their indexes show that the input variables are mainly the adverse events (t-1) at the previous month, the population densities and related project investments. From the country level, fewer of the 14 project investments affect the adverse events. From the region level, some projects with higher index values, such as Security in the South Western region, Energy and Emergency Assistance in the North Eastern region, and Education in the Eastern region are mainly affecting factors. Three batch classification methods and three data streaming classification methods were assessed for their ability to detect adverse events given infrastructure development data. The study uses cost-sensitive measures to address the very unbalanced nature of the data and it applies variable reduction techniques to identify significant variables. The three batch classification algorithms are C4.5, k-nearest Neighbor, and Support Vector Machine. The three data streaming algorithms are Naïve Bayes, Hoeffding Tree, and Single Classifier Drift. In general, the performance of the cost-sensitive methods in the batch setting is comparable to those in the data stream setting. However, in the batch setting the cost matrix needs to be adjusted manually. In contrast the data stream setting allows one to adjust the models based on the analysis of the classifiers’ performance over time and changing data distribution. The Kappa values using Naïve Bayes are the highest in the three data stream algorithms in the whole country and its regions. The Naïve Bayes classifier has the best global performance. By the Kappa statistic curve, we can observe the concept drifts. In a region level, many models have a better performance including more investments related to project compared with those in a country level. In addition as data distribution becomes more balanced, the classifiers in the data stream setting outperform in terms of the overall classification rates in comparison to the classifiers in the batch setting. The results thus demonstrate the potential of data streaming algorithms to significantly outperform when the data become less unbalanced, and can be used for detecting adverse events in similar areas. There are research attempts whose aim was to build models to predict, reduce, disrupt, and/or eliminate terrorist actions.Since terrorist attacks are not accidental in time and space, some studies have attempted to interrupt terrorist actions by identifying typical patterns in adverse activity and analyzing the geospatial findings on reported incidents (Numrich and Tolk, 2010; Open Source Center (OSC), 2009; Schmorrow and Nicholson, 2011; Schmorrow et al., 2009; Zacharias et al., 2008).Several other publications call for more spatial pattern analysis of adverse events using Geographic Information Systems (GIS) (Berrebi and Lakdawalla, 2007; Brown et al., 2004; Johnson and Braithwaite, 2009; LaFree et al., 2012; Siebeneck et al., 2009; Webb and Cutter, 2009; Çakıt and Karwowski, 2018).Reed et al. (2011) applied a time-correlation based prediction approach to incident-based data to understand statistical patterns in terrorists’ behaviors.Such models could be utilized for predicting future incidents and assigning more resources to areas that may be attacked.Inyaem et al. (2010) applied fuzzy inference system (FIS) and adaptive neuro-fuzzy inference system (ANFIS) for adverse event classification and found that ANFIS outperformed FIS.Minu et al. (2010) examined the time sequence of various terrorist attacks all over the world measured on a monthly basis from 1968 to 2007.They concluded that wavelet neural networks provide the best performance to analyze the terrorist attacks over existing methods.
Constructing effective models for detecting, reducing, and/or preventing adverse events is very important in domains such as aviation safety, healthcare, drug administration, and war theaters. This study presents batch and data streaming models to detecting adverse events using data from a war theater context. In all the previous studies, regression models and several machine learning techniques were used for predicting continuous values in an active theater of war, and the error values reported on the test sets were large. In order to overcome the shortcoming, this study investigates the effectiveness of batch and data streaming classification algorithms in detecting or classifying adverse events given infrastructure development spending data and other variables in an active theater of war in Afghanistan. By the feature selection, the valid input variables are obtained and their indexes show that the input variables are mainly the adverse events (t-1) at the previous month, the population densities and related project investments. From the country level, fewer of the 14 project investments affect the adverse events. From the region level, some projects with higher index values, such as Security in the South Western region, Energy and Emergency Assistance in the North Eastern region, and Education in the Eastern region are mainly affecting factors. Three batch classification methods and three data streaming classification methods were assessed for their ability to detect adverse events given infrastructure development data. The study uses cost-sensitive measures to address the very unbalanced nature of the data and it applies variable reduction techniques to identify significant variables. The three batch classification algorithms are C4.5, k-nearest Neighbor, and Support Vector Machine. The three data streaming algorithms are Naïve Bayes, Hoeffding Tree, and Single Classifier Drift. In general, the performance of the cost-sensitive methods in the batch setting is comparable to those in the data stream setting. However, in the batch setting the cost matrix needs to be adjusted manually. In contrast the data stream setting allows one to adjust the models based on the analysis of the classifiers’ performance over time and changing data distribution. The Kappa values using Naïve Bayes are the highest in the three data stream algorithms in the whole country and its regions. The Naïve Bayes classifier has the best global performance. By the Kappa statistic curve, we can observe the concept drifts. In a region level, many models have a better performance including more investments related to project compared with those in a country level. In addition as data distribution becomes more balanced, the classifiers in the data stream setting outperform in terms of the overall classification rates in comparison to the classifiers in the batch setting. The results thus demonstrate the potential of data streaming algorithms to significantly outperform when the data become less unbalanced, and can be used for detecting adverse events in similar areas. More recent studies used linear regression, neural networks, FIS, ANFIS, fuzzy overlay models and fuzzy C-means with subtractive clustering to predict four types of events: the number of killed, wounded, hijacked, and events based on infrastructure development spending and other variables in a war theater in Afghanistan (Çakıt et al., 2014; Çakıt and Karwowski, 2015a; Çakıt and Karwowski, 2015b, 2017a, b, 2018).These four categories of events are collectively called “adverse events”, the term that will be used throughout the paper.The data sets for these studies were provided by the HSCB program management of the U.S. DoD.The authors used the average absolute percentage error and the average absolute error to assess the prediction results.The error values reported on the test sets were large and varied among the four predicted variables.In general, the errors were lowest for the fourth category, i.e., the number of events.
Constructing effective models for detecting, reducing, and/or preventing adverse events is very important in domains such as aviation safety, healthcare, drug administration, and war theaters. This study presents batch and data streaming models to detecting adverse events using data from a war theater context. In all the previous studies, regression models and several machine learning techniques were used for predicting continuous values in an active theater of war, and the error values reported on the test sets were large. In order to overcome the shortcoming, this study investigates the effectiveness of batch and data streaming classification algorithms in detecting or classifying adverse events given infrastructure development spending data and other variables in an active theater of war in Afghanistan. By the feature selection, the valid input variables are obtained and their indexes show that the input variables are mainly the adverse events (t-1) at the previous month, the population densities and related project investments. From the country level, fewer of the 14 project investments affect the adverse events. From the region level, some projects with higher index values, such as Security in the South Western region, Energy and Emergency Assistance in the North Eastern region, and Education in the Eastern region are mainly affecting factors. Three batch classification methods and three data streaming classification methods were assessed for their ability to detect adverse events given infrastructure development data. The study uses cost-sensitive measures to address the very unbalanced nature of the data and it applies variable reduction techniques to identify significant variables. The three batch classification algorithms are C4.5, k-nearest Neighbor, and Support Vector Machine. The three data streaming algorithms are Naïve Bayes, Hoeffding Tree, and Single Classifier Drift. In general, the performance of the cost-sensitive methods in the batch setting is comparable to those in the data stream setting. However, in the batch setting the cost matrix needs to be adjusted manually. In contrast the data stream setting allows one to adjust the models based on the analysis of the classifiers’ performance over time and changing data distribution. The Kappa values using Naïve Bayes are the highest in the three data stream algorithms in the whole country and its regions. The Naïve Bayes classifier has the best global performance. By the Kappa statistic curve, we can observe the concept drifts. In a region level, many models have a better performance including more investments related to project compared with those in a country level. In addition as data distribution becomes more balanced, the classifiers in the data stream setting outperform in terms of the overall classification rates in comparison to the classifiers in the batch setting. The results thus demonstrate the potential of data streaming algorithms to significantly outperform when the data become less unbalanced, and can be used for detecting adverse events in similar areas. This research uses the same data sets provided by the HSCB program management of the U.S. DoD.We show that infrastructure development investments are helpful in detecting the occurrence of adverse events.From the region level, some projects such as Security, Education, and Emergency Assistance are influencing factors.Selecting developing projects according to characteristics of different regions would be helpful to utilize capitals farthest to stabilize the country.We also show that data stream models outperform batch techniques in detecting adverse events.The primary contribution of this study is that it proposes a different and novel approach through recoding of the four target/output variables to compare the effectiveness of three batch techniques and three data streaming classification algorithms in detecting/classifying adverse events based on infrastructure development spending and other variables.The batch techniques use traditional static batch learning, whereas the data streaming methods use incremental learning that can take snapshots of performance during constructing a model and allow one to examine the classification rate drift over time (Bifet and Kirkby, 2009).In the batch learning approach, k-fold cross-validation technique is frequently used and the classification accuracy rates are averaged over k test set folds.The main drawback of the batch learning approach is that it does not utilize incoming adverse events data accumulated in time.In the data stream approach, the models built are dynamically updated by processing new and incrementally arriving adverse events from the data stream and detection of adverse events can benefit from this approach.
Autonomy on rovers is desirable in order to extend the traversed distance, and therefore, maximize the number of places visited during the mission. It depends heavily on the information that is available for the traversed surface on other planet. This information may come from the vehicle’s sensors as well as from orbital images. Besides, future exploration missions may consider the use of reconfigurable rovers, which are able to execute multiple locomotion modes to better adapt to different terrains. With these considerations, a path planning algorithm based on the Fast Marching Method is proposed. Environment information coming from different sources is used on a grid formed by two layers. First, the Global Layer with a low resolution, but high extension is used to compute the overall path connecting the rover and the desired goal, using a cost function that takes advantage of the rover locomotion modes. Second, the Local Layer with higher resolution but limited distance is used where the path is dynamically repaired because of obstacle presence. Finally, we show simulation and field test results based on several reconfigurable and non-reconfigurable rover prototypes and a experimental terrain. Autonomy is an essential capability for rovers to explore the surface of other planets.The distances from Earth entail big latencies in communications between the rover and the terrestrial ground station.As an example, there is a radio transmission delay of several minutes between Earth and Mars (Lester and Thronson, 2011; Lester et al., 2017).Therefore, direct teleoperation arises as a difficult task to be carried out remotely from Earth.Besides, communications with rovers at the red planet generally occur only a few times per Martian sol (solar day) due to the availability of Deep Space Network antennas, conforming a limited time-slot for providing commands and retrieving data from the rover (Bajracharya et al., 2008).These facts are contrary to the necessity of increasing the number of scientifically interesting places visited by rovers.Providing higher autonomy would allow them to traverse longer distances.However, new issues arise since rovers tackle a high uncertainty when they are traveling, i.e., they may encounter unexpected situations, mostly due to terrain shape and/or composition, as well as the existence of stones.These issues affect the traversability for the vehicle.The improper evaluation of the terrain could lead to a fatal situation of the vehicle, compromising the mission as a result.This was the case of the Spirit rover, which got stuck in loose sand, making it impossible to continue driving (Ono et al., 2015) and thus bringing the mission to an end.By using traversability information, autonomy can be improved thanks to the use of path planning algorithms, which allow the vehicle to compute onboard a safe path from one location to another.
Autonomy on rovers is desirable in order to extend the traversed distance, and therefore, maximize the number of places visited during the mission. It depends heavily on the information that is available for the traversed surface on other planet. This information may come from the vehicle’s sensors as well as from orbital images. Besides, future exploration missions may consider the use of reconfigurable rovers, which are able to execute multiple locomotion modes to better adapt to different terrains. With these considerations, a path planning algorithm based on the Fast Marching Method is proposed. Environment information coming from different sources is used on a grid formed by two layers. First, the Global Layer with a low resolution, but high extension is used to compute the overall path connecting the rover and the desired goal, using a cost function that takes advantage of the rover locomotion modes. Second, the Local Layer with higher resolution but limited distance is used where the path is dynamically repaired because of obstacle presence. Finally, we show simulation and field test results based on several reconfigurable and non-reconfigurable rover prototypes and a experimental terrain. Path planning has been used in Mars exploration missions along with rovers Spirit, Opportunity and Curiosity.A path planning approach, based on two levels (global and local), was deployed on these rovers (Maimone et al., 2007).The main reason behind it is to use data relative to rover surroundings while also considering information relative to the location of elements, such as obstacles, in other areas.As global planning algorithm, the Field-D* was used, initially introduced by Ferguson and Stentz (2006b).With this algorithm, a potential field is computed on a regular grid starting from the goal position to the rover location.Main particularity of this method is the use of an interpolation technique that assigns the values of this potential field to each node, based on the values of its already visited neighbors.By considering that edges connecting nodes are crossable by the path, it results to be smoother than previous methods like D*, where paths are strictly restricted to go through node locations.This algorithm has also re-planning capability, meaning the path can be updated during rover traverse in case the cost of any node is modified.This can happen any time the rover detects an obstacle on its way using its onboard sensors.Besides, this algorithm has been adapted to the use of multi-resolution grids (Ferguson and Stentz, 2006a) to minimize computational resources, saving computation in areas where the level of detail can be simplified.However, it is not clear neither the computational cost of this algorithm nor the steps to extract a path from it, leaving this operation to another algorithm that only focuses on the local planning.In the case of Mars rovers, the GESTALT local planner has been used (Carsten et al., 2007).It basically generates a series of arcs starting from the vehicle position, which are later evaluated according to the potential field created by the global planner.As result, the arc with the best evaluation is chosen to be followed by the rover.
Autonomy on rovers is desirable in order to extend the traversed distance, and therefore, maximize the number of places visited during the mission. It depends heavily on the information that is available for the traversed surface on other planet. This information may come from the vehicle’s sensors as well as from orbital images. Besides, future exploration missions may consider the use of reconfigurable rovers, which are able to execute multiple locomotion modes to better adapt to different terrains. With these considerations, a path planning algorithm based on the Fast Marching Method is proposed. Environment information coming from different sources is used on a grid formed by two layers. First, the Global Layer with a low resolution, but high extension is used to compute the overall path connecting the rover and the desired goal, using a cost function that takes advantage of the rover locomotion modes. Second, the Local Layer with higher resolution but limited distance is used where the path is dynamically repaired because of obstacle presence. Finally, we show simulation and field test results based on several reconfigurable and non-reconfigurable rover prototypes and a experimental terrain. An alternative path planning solution to Field-D* is the Fast Marching Method (FMM), originally introduced by Sethian (1999).It is a numerical method that solves the so-called eikonal equation, which is an expression defining the behavior of a wave that propagates over a continuous 2D scalar function — also extendable to 3D.Unlike Field-D*, only a quadratic expression can be used along each node of the grid to compute a continuous potential field.Such a potential field represents the arrival time of the wave at each location of the grid.Besides, in contrast to Field-D*, the computational cost of FMM is clearly stated, this being O(ζlogζ), where ζ is the number of nodes the grid is composed of.After computing the potential field, the path is extracted by just making use of a gradient descent method on it (Kimmel and Sethian, 2001; Liu and Bucknall, 2015).However, this path planning solution is not originally meant to include a re-planning capability.Previous research (Philippsen et al., 2008) has focused on the modification of FMM to make it dynamic, obtaining as result an algorithm called E*.Nevertheless, it does not consider the use of maps with multiple resolutions.An example of FMM using a multi-resolution map, but not dynamic re-planning, can be seen in the work of Petres et al. (2005), where an already-known environment is modeled with different levels of detail, obtaining as result sub-optimal but fast-computed paths.
Autonomy on rovers is desirable in order to extend the traversed distance, and therefore, maximize the number of places visited during the mission. It depends heavily on the information that is available for the traversed surface on other planet. This information may come from the vehicle’s sensors as well as from orbital images. Besides, future exploration missions may consider the use of reconfigurable rovers, which are able to execute multiple locomotion modes to better adapt to different terrains. With these considerations, a path planning algorithm based on the Fast Marching Method is proposed. Environment information coming from different sources is used on a grid formed by two layers. First, the Global Layer with a low resolution, but high extension is used to compute the overall path connecting the rover and the desired goal, using a cost function that takes advantage of the rover locomotion modes. Second, the Local Layer with higher resolution but limited distance is used where the path is dynamically repaired because of obstacle presence. Finally, we show simulation and field test results based on several reconfigurable and non-reconfigurable rover prototypes and a experimental terrain. In most cases, path planning algorithms consider traversability data to decide whether a path should go through a certain area or not.It heavily depends on the underlying physics of the terrain–vehicle interaction, i.e. terramechanics.Shape and composition of terrains determine the dynamic behavior of any body in contact with it (e.g. friction and slipping effects).The kinematic configuration of the vehicle, together with the distribution of its masses and inertias, affects the forces exerted on the surface, influencing on traction, as well as on the energy required to move the vehicle.While the terrain features cannot be changed, the rover kinematic configuration could be adapted to it.This is the case for vehicles categorized as reconfigurable, which are able to perform several locomotion modes, each one adapted to a particular terrain.A notable example of this feature is the ExoMars rover, which is being conceived to search for signs of life on Mars in the future ExoMars 2020 mission, lead by the European Space Agency (Vago et al., 2015, 2017).The particularity, with respect to other rovers, is the use of additional joints on top of its legs.Such joints are initially meant to deploy the wheels once the rover has landed on Mars, but later, they can be further used to improve traction on loose soil by the use of a locomotion mode called wheel-walking (Woods et al., 2009; Patel et al., 2010).In this sense, Azkarate et al. (2015) performed some experiments using an ExoMars rover prototype that demonstrated this statement.Other authors analyzed the performance of a similar locomotion mode, called push–pull (Creager et al., 2015), on loose soil as well, getting to the same conclusions.These modes of locomotion may be helpful in situations where otherwise, being only capable of executing the standard roving or normal driving locomotion mode, would result in the rover raising its power consumption or even getting entrapped.Having a good knowledge of the locomotion-terrain relation for a particular rover, a path planning algorithm could take advantage of this information, finding even more optimal paths.However, the design of an algorithm that takes into consideration different locomotion modes is still being investigated.In previous works, path planning algorithms have been developed aimed at the reconfiguration of the vehicle chassis.For example, Brunner et al. (2012, 2014) proposed algorithms to find optimal paths adapting the chassis to overcome obstacles in the form of stairs, while Miró et al. (2010) carried out a research focused on maintaining the stability of the vehicle using FMM.However, none of these works takes into consideration a reconfigurable rover for long-range operations, such as those found in planetary exploration missions.
Hypotheses on why the 1845 Franklin expedition to the Arctic ended in tragedy include suggestions of lead (Pb) poisoning. Hair keratin was sequentially analysed for isotopic ratios and lead concentrations from remains of a Franklin expedition member, tentatively identified as HDS Goodsir, buried on King William Island in the Canadian arctic. We approximated lead concentrations in Goodsir's blood to elucidate a pattern of lead burden in the three months prior to death. Lead isotope ratios in Goodsir's hair were almost identical to that of the bodies discovered on King William and Beechey Islands. The lead concentrations (73.3–84.4 ppm) reflect more immediate exposure and are high by today's standards. Estimated blood lead concentrations (~53.6–61.3 μg/dL), suggest that lead exposure, while high, may not have been sufficient to cause worsening physical and mental symptoms. Lead ingestion likely occurred during the expedition; however, it is probable that multiple factors are responsible for the loss of the expedition, of which lead exposure may have been one. In 1845, British explorer Sir John Franklin departed on a voyage to find the North-West Passage, a sea route that links the Atlantic Ocean to the Pacific.The expedition was to complete its mission within three years and return home, but the two ships, HMS Erebus and HMS Terror, and the 129 men aboard vanished in the Arctic.The last Europeans to see them alive were the crews of two whaling ships on Baffin Bay in July 1845, just before they entered the Arctic Archipelago (Beattie and Geiger, 2014).Several rescue expeditions were unsuccessful in locating Franklin and his men, and in 1859, a note from the expedition was discovered on King William Island.It was dated May 1847 and indicated that all was well, but an addendum, written in April 1848, stated that 24 members, including Sir John Franklin, had died by that date and the remaining 105 left their icebound ships and aimed to reach safety via the Back River on the Canadian mainland.
Hypotheses on why the 1845 Franklin expedition to the Arctic ended in tragedy include suggestions of lead (Pb) poisoning. Hair keratin was sequentially analysed for isotopic ratios and lead concentrations from remains of a Franklin expedition member, tentatively identified as HDS Goodsir, buried on King William Island in the Canadian arctic. We approximated lead concentrations in Goodsir's blood to elucidate a pattern of lead burden in the three months prior to death. Lead isotope ratios in Goodsir's hair were almost identical to that of the bodies discovered on King William and Beechey Islands. The lead concentrations (73.3–84.4 ppm) reflect more immediate exposure and are high by today's standards. Estimated blood lead concentrations (~53.6–61.3 μg/dL), suggest that lead exposure, while high, may not have been sufficient to cause worsening physical and mental symptoms. Lead ingestion likely occurred during the expedition; however, it is probable that multiple factors are responsible for the loss of the expedition, of which lead exposure may have been one. Researchers have debated the reasons why the Franklin expedition ended in tragedy, including the possibility of lead (Pb) poisoning from the pipes on the ships and/or the sealed tins that provided the main source of food (e.g., Battersby, 2008; Kowal et al., 1989).Studies show that lead concentrations in the bones of three expedition members who died in the first winter at Beechey Island in the Canadian High Arctic were high relative to the present day (Beattie, 1985; Kowal et al., 1989), and this has resulted in theories that lead toxicity contributed to the demise of the expedition.It is hypothesized that ingestion of lead from contaminated canned provisions caused acute, exogenous poisoning that weakened the men and impaired their cognitive function (Kowal et al., 1989; Beattie and Geiger, 2014).One difficulty with this theory is that bone remodels approximately every 10–50 years (Raisz, 1999), so lead concentrations in the bones would be heavily influenced by the lead burden acquired prior to boarding the ships bound for the Arctic.
Hypotheses on why the 1845 Franklin expedition to the Arctic ended in tragedy include suggestions of lead (Pb) poisoning. Hair keratin was sequentially analysed for isotopic ratios and lead concentrations from remains of a Franklin expedition member, tentatively identified as HDS Goodsir, buried on King William Island in the Canadian arctic. We approximated lead concentrations in Goodsir's blood to elucidate a pattern of lead burden in the three months prior to death. Lead isotope ratios in Goodsir's hair were almost identical to that of the bodies discovered on King William and Beechey Islands. The lead concentrations (73.3–84.4 ppm) reflect more immediate exposure and are high by today's standards. Estimated blood lead concentrations (~53.6–61.3 μg/dL), suggest that lead exposure, while high, may not have been sufficient to cause worsening physical and mental symptoms. Lead ingestion likely occurred during the expedition; however, it is probable that multiple factors are responsible for the loss of the expedition, of which lead exposure may have been one. Hair is not remodeled after formation; therefore, its composition reflects diet and physiological stress at the time of tissue formation (Williams et al., 2011; D'Ortenzio et al., 2015).Hair grows between 0.35 and 0.44 mm per day and based on the growth rate of 0.35 mm/day, 1 cm of hair corresponds to approximately one month of elapsed time (Saitoh et al., 1967; Williams et al., 2011).Because hair grows incrementally, and once formed is an inert tissue, its elemental composition provides a short-term record of lead exposure for the weeks/months/years (depending on the length of hair) preceding death (Appenzeller et al., 2007).As there is an approximate 20-day delay between concentrations of lead in the first cm of hair and the corresponding monthly blood level (Rabinowitz et al., 1976; Clarkson and Magos, 2006), inferences about lead toxicity based on elemental evidence are necessarily general, but human hair can provide an important supplement to, or confirmation of skeletal evidence.
We address the problem of jellyfish polyp counting in underwater images. Modern methods utilize convolutional neural networks for feature extraction and work in two stages. First, hypothetical regions are proposed at potential locations, the features of the regions are extracted and classified according to the contained object. Such methods typically require a dense grid for region proposals, explicitly test various scales and are prone to failure in densely populated regions. We propose a segmentation-based polyp counter — SegCo. A convolutional neural network is trained to produce locally-circular segmentation masks on the polyps, which are then detected by localizing circularly symmetric areas in the segmented image. Detection stage is efficient and avoids a greedy search over position and scales. SegCo outperforms the current state-of-the-art object detector RetinaNet (Lin et al., 2017) and the recent specialized polyp detection method PoCo (Vodopivec et al., 2018) by 2% and 24% in F-score, respectively, and sets a new state-of-the-art in polyp detection. Jellyfish (Scyphozoa) blooms have attracted significant interest of researchers over the recent years (Kogovšek et al., 2018; Brotz et al., 2012; Condon et al., 2013).During blooming, the jellyfish population rapidly increases in a relatively small geographical area, which importantly affects the local ecosystem.Bloom and jellyfish population dynamics prediction can be established by analyzing the population dynamics of polyps, which are one of the many forms in the jellyfish complex life cycle.
We address the problem of jellyfish polyp counting in underwater images. Modern methods utilize convolutional neural networks for feature extraction and work in two stages. First, hypothetical regions are proposed at potential locations, the features of the regions are extracted and classified according to the contained object. Such methods typically require a dense grid for region proposals, explicitly test various scales and are prone to failure in densely populated regions. We propose a segmentation-based polyp counter — SegCo. A convolutional neural network is trained to produce locally-circular segmentation masks on the polyps, which are then detected by localizing circularly symmetric areas in the segmented image. Detection stage is efficient and avoids a greedy search over position and scales. SegCo outperforms the current state-of-the-art object detector RetinaNet (Lin et al., 2017) and the recent specialized polyp detection method PoCo (Vodopivec et al., 2018) by 2% and 24% in F-score, respectively, and sets a new state-of-the-art in polyp detection. (Widmer et al., 2016) analyzed polyp populations and experimentally established relation between the water temperature and salinity and the bloom magnitude.But their study was restricted to in vitro studies and the polyp density was manually estimated.Only a fraction of works focus on in situ studies.Polyp attachment substrates are scarce in natural environments, thus they usually cluster on small regions, like oyster shells.Using such regions, several studies (Hočevar et al., 2018; Vodopivec et al., 2018) have established important relations between polyp numbers in nature and the resulting jellyfish population dynamics.Most of these studies, however, rely on manual polyp counts.
We address the problem of jellyfish polyp counting in underwater images. Modern methods utilize convolutional neural networks for feature extraction and work in two stages. First, hypothetical regions are proposed at potential locations, the features of the regions are extracted and classified according to the contained object. Such methods typically require a dense grid for region proposals, explicitly test various scales and are prone to failure in densely populated regions. We propose a segmentation-based polyp counter — SegCo. A convolutional neural network is trained to produce locally-circular segmentation masks on the polyps, which are then detected by localizing circularly symmetric areas in the segmented image. Detection stage is efficient and avoids a greedy search over position and scales. SegCo outperforms the current state-of-the-art object detector RetinaNet (Lin et al., 2017) and the recent specialized polyp detection method PoCo (Vodopivec et al., 2018) by 2% and 24% in F-score, respectively, and sets a new state-of-the-art in polyp detection. Manual polyp density estimation requires counting polyps in underwater images.These images might contain over a thousand polyps, which makes manual counting a time-consuming and laborious task.Furthermore, the annotation requires a high degree of expert knowledge to accurately classify ambiguous regions/objects as a polyp or a background structure (see Fig. 1).This calls for automated polyp detection methods.The methods have to be particularly robust to deal with a large appearance variability of polyps and the various conditions under which the photographs are taken.These include lighting, camera focus, motion blur and background color.
We address the problem of jellyfish polyp counting in underwater images. Modern methods utilize convolutional neural networks for feature extraction and work in two stages. First, hypothetical regions are proposed at potential locations, the features of the regions are extracted and classified according to the contained object. Such methods typically require a dense grid for region proposals, explicitly test various scales and are prone to failure in densely populated regions. We propose a segmentation-based polyp counter — SegCo. A convolutional neural network is trained to produce locally-circular segmentation masks on the polyps, which are then detected by localizing circularly symmetric areas in the segmented image. Detection stage is efficient and avoids a greedy search over position and scales. SegCo outperforms the current state-of-the-art object detector RetinaNet (Lin et al., 2017) and the recent specialized polyp detection method PoCo (Vodopivec et al., 2018) by 2% and 24% in F-score, respectively, and sets a new state-of-the-art in polyp detection. Recently, an automated polyp counter was proposed by Vodopivec et al. (2018).The approach is based on the classical object detection architecture: a weak fast detector proposes potential polyp boxes and then computation-intensive features are extracted and classified.The approach suffers from several drawbacks.Polyps vary in size significantly within a single image, which requires a greedy search over several scales.This increases the computational cost as well as the potential for false positives.Application of non-maxima suppression to the detections that passed the final classifier results in missed detections in densely populated regions.The detection pipeline is composed of algorithmically non-homogeneous modules.Thus the opportunity to improve robustness by end-to-end training is lost.For practical applications, a streamlined end-to-end trainable detector is required that would address scale variability, retain a high detection rate, and allow re-training on alternative datasets obtained by different hardware.
We address the problem of jellyfish polyp counting in underwater images. Modern methods utilize convolutional neural networks for feature extraction and work in two stages. First, hypothetical regions are proposed at potential locations, the features of the regions are extracted and classified according to the contained object. Such methods typically require a dense grid for region proposals, explicitly test various scales and are prone to failure in densely populated regions. We propose a segmentation-based polyp counter — SegCo. A convolutional neural network is trained to produce locally-circular segmentation masks on the polyps, which are then detected by localizing circularly symmetric areas in the segmented image. Detection stage is efficient and avoids a greedy search over position and scales. SegCo outperforms the current state-of-the-art object detector RetinaNet (Lin et al., 2017) and the recent specialized polyp detection method PoCo (Vodopivec et al., 2018) by 2% and 24% in F-score, respectively, and sets a new state-of-the-art in polyp detection. Our main contribution is a new streamlined automated polyp counter.To deal with clutter and high appearance and scale diversity, we cast polyp detection as a semantic instance segmentation problem that capitalizes on approximate circular symmetry of the polyps (Fig. 2).A convolutional neural network is designed to segment all pixels in the image into polyp and background pixels.The network is trained such to enhance the circular symmetry in the segmentation mask.A detector is then designed to extract approximately circular structures from the mask, using an approach based on distance transform.The detector simultaneously localizes polyps at several scales without explicit enumeration of the scales and naturally handles clustered polyps that partially overlap in the image.The approach is easily trainable, robust and fast.
We address the problem of jellyfish polyp counting in underwater images. Modern methods utilize convolutional neural networks for feature extraction and work in two stages. First, hypothetical regions are proposed at potential locations, the features of the regions are extracted and classified according to the contained object. Such methods typically require a dense grid for region proposals, explicitly test various scales and are prone to failure in densely populated regions. We propose a segmentation-based polyp counter — SegCo. A convolutional neural network is trained to produce locally-circular segmentation masks on the polyps, which are then detected by localizing circularly symmetric areas in the segmented image. Detection stage is efficient and avoids a greedy search over position and scales. SegCo outperforms the current state-of-the-art object detector RetinaNet (Lin et al., 2017) and the recent specialized polyp detection method PoCo (Vodopivec et al., 2018) by 2% and 24% in F-score, respectively, and sets a new state-of-the-art in polyp detection. The proposed segmentation-based polyp counter, SegCo, is evaluated on the recent challenging dataset of polyps in the wild proposed by Vodopivec et al. (2018).Results show that the approach is highly robust, it outperforms a slow but general state-of-the-art object detector RetinaNet (Lin et al., 2017), by 2% in F-1 measure and outperforms the currently best method for polyp detection, PoCo (Vodopivec et al., 2018), by 24% in F-1 measure.A license-free Python implementation of SegCo will be made publicly available.We believe this will substantially contribute to the marine biology community as a trainable toolbox for polyp counting and similar applications, allowing to accurately process large datasets.
Microscopic algae segmentation, specifically of diatoms, is an essential procedure for water quality assessment. The segmentation of these microalgae is still a challenge for computer vision. This paper addresses for the first time this problem using deep learning approaches to predict exactly those pixels that belong to each class, i.e., diatom and non diatom. A comparison between semantic segmentation and instance segmentation is carried out, and the performance of these methods is evaluated in the presence of different types of noise. The trained models are then evaluated with the same raw images used for manual diatom identification. A total of 126 images of the entire field of view at 60x magnification, with a size of 2592x1944 pixels, are analyzed. The images contain 10 different taxa plus debris and fragments. The best results were obtained with instance segmentation achieving an average precision of 85% with 86% sensitivity and 91% specificity (up to 92% precision with 98%, both sensitivity and specificity for some taxa). Semantic segmentation was able to improve the average sensitivity up to 95% but decreasing the specificity down to 60% and precision to 57%. Instance segmentation was also able to properly separate diatoms when overlap occurs, which helps estimate the number of diatoms, a key requirement for water quality grading. The automatic identification of diatoms in water samples is a challenging problem that has a high impact on water quality assessment.Diatoms are a type of plankton called phytoplankton, a type of microscopic algae that live in water areas like oceans, rivers or lakes and which are used as a bioindicator of its quality (Blanco and Bécares, 2010).Diatom identification and quantification in water samples are currently done manually, which is a time consuming task.
Microscopic algae segmentation, specifically of diatoms, is an essential procedure for water quality assessment. The segmentation of these microalgae is still a challenge for computer vision. This paper addresses for the first time this problem using deep learning approaches to predict exactly those pixels that belong to each class, i.e., diatom and non diatom. A comparison between semantic segmentation and instance segmentation is carried out, and the performance of these methods is evaluated in the presence of different types of noise. The trained models are then evaluated with the same raw images used for manual diatom identification. A total of 126 images of the entire field of view at 60x magnification, with a size of 2592x1944 pixels, are analyzed. The images contain 10 different taxa plus debris and fragments. The best results were obtained with instance segmentation achieving an average precision of 85% with 86% sensitivity and 91% specificity (up to 92% precision with 98%, both sensitivity and specificity for some taxa). Semantic segmentation was able to improve the average sensitivity up to 95% but decreasing the specificity down to 60% and precision to 57%. Instance segmentation was also able to properly separate diatoms when overlap occurs, which helps estimate the number of diatoms, a key requirement for water quality grading. In order to assess the quality of a water sample, as per the standard workflow, diatoms on 40 field of views (FoV) must be quantified.The implementation of automatic tools based on computer vision and machine learning techniques to perform this task is needed.A number of recent works have dealt with automatic diatom classification, that is, from an image sample containing a single diatom the model tries to predict the correct taxon name.Some classifiers, based on general handcrafted features, have provided good results, around 95% (Schulze et al., 2013) and 98% of accuracy (Bueno et al., 2017).However, approaches based on convolutional neural networks (CNN) obtain better results, above 99% accuracy (Pedraza et al., 2017).
Microscopic algae segmentation, specifically of diatoms, is an essential procedure for water quality assessment. The segmentation of these microalgae is still a challenge for computer vision. This paper addresses for the first time this problem using deep learning approaches to predict exactly those pixels that belong to each class, i.e., diatom and non diatom. A comparison between semantic segmentation and instance segmentation is carried out, and the performance of these methods is evaluated in the presence of different types of noise. The trained models are then evaluated with the same raw images used for manual diatom identification. A total of 126 images of the entire field of view at 60x magnification, with a size of 2592x1944 pixels, are analyzed. The images contain 10 different taxa plus debris and fragments. The best results were obtained with instance segmentation achieving an average precision of 85% with 86% sensitivity and 91% specificity (up to 92% precision with 98%, both sensitivity and specificity for some taxa). Semantic segmentation was able to improve the average sensitivity up to 95% but decreasing the specificity down to 60% and precision to 57%. Instance segmentation was also able to properly separate diatoms when overlap occurs, which helps estimate the number of diatoms, a key requirement for water quality grading. Although automatic classification results are very promising, in practice the taxonomist will handle full size microscopic images containing several taxon shells from different taxa in the same FoV.Thus, it is common that in a single FoV, several diatoms of different species, sizes and shapes appear, along with debris, fragments and dust specks, as shown in Fig. 1a).
Microscopic algae segmentation, specifically of diatoms, is an essential procedure for water quality assessment. The segmentation of these microalgae is still a challenge for computer vision. This paper addresses for the first time this problem using deep learning approaches to predict exactly those pixels that belong to each class, i.e., diatom and non diatom. A comparison between semantic segmentation and instance segmentation is carried out, and the performance of these methods is evaluated in the presence of different types of noise. The trained models are then evaluated with the same raw images used for manual diatom identification. A total of 126 images of the entire field of view at 60x magnification, with a size of 2592x1944 pixels, are analyzed. The images contain 10 different taxa plus debris and fragments. The best results were obtained with instance segmentation achieving an average precision of 85% with 86% sensitivity and 91% specificity (up to 92% precision with 98%, both sensitivity and specificity for some taxa). Semantic segmentation was able to improve the average sensitivity up to 95% but decreasing the specificity down to 60% and precision to 57%. Instance segmentation was also able to properly separate diatoms when overlap occurs, which helps estimate the number of diatoms, a key requirement for water quality grading. Therefore, segmentation methods or region of interest (ROI) detection algorithms are needed to locate all the diatoms present in the image.Once the diatom is detected, by generating a bounding box and/or mask for each instance a classification may be performed for all ROI detected.
Microscopic algae segmentation, specifically of diatoms, is an essential procedure for water quality assessment. The segmentation of these microalgae is still a challenge for computer vision. This paper addresses for the first time this problem using deep learning approaches to predict exactly those pixels that belong to each class, i.e., diatom and non diatom. A comparison between semantic segmentation and instance segmentation is carried out, and the performance of these methods is evaluated in the presence of different types of noise. The trained models are then evaluated with the same raw images used for manual diatom identification. A total of 126 images of the entire field of view at 60x magnification, with a size of 2592x1944 pixels, are analyzed. The images contain 10 different taxa plus debris and fragments. The best results were obtained with instance segmentation achieving an average precision of 85% with 86% sensitivity and 91% specificity (up to 92% precision with 98%, both sensitivity and specificity for some taxa). Semantic segmentation was able to improve the average sensitivity up to 95% but decreasing the specificity down to 60% and precision to 57%. Instance segmentation was also able to properly separate diatoms when overlap occurs, which helps estimate the number of diatoms, a key requirement for water quality grading. A recent review of phytoplankton image segmentation methods is presented in Tang et al. (2018).Most of the methods are based on classical methods such region based segmentation (Jalba et al., 2004; Verikas et al., 2012; Zheng et al., 2014, 2017b) and active contours (AC) (Gelzinis et al., 2015).As far as the authors know, there are only two works using deep neural network based segmentation methods (Tang et al., 2018 and Pedraza et al., 2018).
Microscopic algae segmentation, specifically of diatoms, is an essential procedure for water quality assessment. The segmentation of these microalgae is still a challenge for computer vision. This paper addresses for the first time this problem using deep learning approaches to predict exactly those pixels that belong to each class, i.e., diatom and non diatom. A comparison between semantic segmentation and instance segmentation is carried out, and the performance of these methods is evaluated in the presence of different types of noise. The trained models are then evaluated with the same raw images used for manual diatom identification. A total of 126 images of the entire field of view at 60x magnification, with a size of 2592x1944 pixels, are analyzed. The images contain 10 different taxa plus debris and fragments. The best results were obtained with instance segmentation achieving an average precision of 85% with 86% sensitivity and 91% specificity (up to 92% precision with 98%, both sensitivity and specificity for some taxa). Semantic segmentation was able to improve the average sensitivity up to 95% but decreasing the specificity down to 60% and precision to 57%. Instance segmentation was also able to properly separate diatoms when overlap occurs, which helps estimate the number of diatoms, a key requirement for water quality grading. The performance of previous classical methods ranges from 88% to 95%.The main drawbacks are that they are sensitive to noise, like those based on region segmentation, or they need to manually set the initial curve, in the case of AC.Moreover, all of them have been demonstrated only on a single taxon and on images containing a single diatom shell.Only the work of Zheng et al. (2017a) was demonstrated on images with multiple diatom shells but for a single taxon with an average precision of 91% and a sensitivity of 81%.
Microscopic algae segmentation, specifically of diatoms, is an essential procedure for water quality assessment. The segmentation of these microalgae is still a challenge for computer vision. This paper addresses for the first time this problem using deep learning approaches to predict exactly those pixels that belong to each class, i.e., diatom and non diatom. A comparison between semantic segmentation and instance segmentation is carried out, and the performance of these methods is evaluated in the presence of different types of noise. The trained models are then evaluated with the same raw images used for manual diatom identification. A total of 126 images of the entire field of view at 60x magnification, with a size of 2592x1944 pixels, are analyzed. The images contain 10 different taxa plus debris and fragments. The best results were obtained with instance segmentation achieving an average precision of 85% with 86% sensitivity and 91% specificity (up to 92% precision with 98%, both sensitivity and specificity for some taxa). Semantic segmentation was able to improve the average sensitivity up to 95% but decreasing the specificity down to 60% and precision to 57%. Instance segmentation was also able to properly separate diatoms when overlap occurs, which helps estimate the number of diatoms, a key requirement for water quality grading. Segmentation techniques based on deep learning may be divided into two approaches: (i) object detection and (ii) pixel-wise binary classification, i.e., into two classes (ROI or background).In (i) all the instances of the ROI can be located within the image using a bounding box and classified.In (ii) a mask with exactly the pixels that belong to each ROI is inferred.
Microscopic algae segmentation, specifically of diatoms, is an essential procedure for water quality assessment. The segmentation of these microalgae is still a challenge for computer vision. This paper addresses for the first time this problem using deep learning approaches to predict exactly those pixels that belong to each class, i.e., diatom and non diatom. A comparison between semantic segmentation and instance segmentation is carried out, and the performance of these methods is evaluated in the presence of different types of noise. The trained models are then evaluated with the same raw images used for manual diatom identification. A total of 126 images of the entire field of view at 60x magnification, with a size of 2592x1944 pixels, are analyzed. The images contain 10 different taxa plus debris and fragments. The best results were obtained with instance segmentation achieving an average precision of 85% with 86% sensitivity and 91% specificity (up to 92% precision with 98%, both sensitivity and specificity for some taxa). Semantic segmentation was able to improve the average sensitivity up to 95% but decreasing the specificity down to 60% and precision to 57%. Instance segmentation was also able to properly separate diatoms when overlap occurs, which helps estimate the number of diatoms, a key requirement for water quality grading. The object detection algorithms have been tested on diatoms, in previous work by the authors (Pedraza et al., 2018), using a Region-based Convolutional Neural Network (R-CNN) (Uijlings et al., 2013; Girshick, 2015) and a framework called Redmon (2013–2016) with YOLO method (Redmon et al., 2016).In R-CNN the first step is to provide region proposals and based on these proposals a CNN extracts image features to be classified by a Support Vector Machine (SVM).In YOLO, a single neural network is applied to the whole image.The network divides the image into regions and predicts the class and the bounding box probabilities.
Microscopic algae segmentation, specifically of diatoms, is an essential procedure for water quality assessment. The segmentation of these microalgae is still a challenge for computer vision. This paper addresses for the first time this problem using deep learning approaches to predict exactly those pixels that belong to each class, i.e., diatom and non diatom. A comparison between semantic segmentation and instance segmentation is carried out, and the performance of these methods is evaluated in the presence of different types of noise. The trained models are then evaluated with the same raw images used for manual diatom identification. A total of 126 images of the entire field of view at 60x magnification, with a size of 2592x1944 pixels, are analyzed. The images contain 10 different taxa plus debris and fragments. The best results were obtained with instance segmentation achieving an average precision of 85% with 86% sensitivity and 91% specificity (up to 92% precision with 98%, both sensitivity and specificity for some taxa). Semantic segmentation was able to improve the average sensitivity up to 95% but decreasing the specificity down to 60% and precision to 57%. Instance segmentation was also able to properly separate diatoms when overlap occurs, which helps estimate the number of diatoms, a key requirement for water quality grading. YOLO gives better results than R-CNN in the evaluation carried out with 10 taxa in full microscopic images with multiple diatom shells (Pedraza et al., 2018).This is due to the fact that the model has information about the global context since the network is fed with the full image.Thus, an average F1-measure value of 0.37 with 29% precision and 68% sensitivity is obtained by the R-CNN against an average F1-measure value of 0.78 with 75% precision and 84% sensitivity obtained with YOLO.The main problem with these methods is that they do not separate properly the ROIs when overlap occurs.Therefore, the quantification of diatoms is limited.
Microscopic algae segmentation, specifically of diatoms, is an essential procedure for water quality assessment. The segmentation of these microalgae is still a challenge for computer vision. This paper addresses for the first time this problem using deep learning approaches to predict exactly those pixels that belong to each class, i.e., diatom and non diatom. A comparison between semantic segmentation and instance segmentation is carried out, and the performance of these methods is evaluated in the presence of different types of noise. The trained models are then evaluated with the same raw images used for manual diatom identification. A total of 126 images of the entire field of view at 60x magnification, with a size of 2592x1944 pixels, are analyzed. The images contain 10 different taxa plus debris and fragments. The best results were obtained with instance segmentation achieving an average precision of 85% with 86% sensitivity and 91% specificity (up to 92% precision with 98%, both sensitivity and specificity for some taxa). Semantic segmentation was able to improve the average sensitivity up to 95% but decreasing the specificity down to 60% and precision to 57%. Instance segmentation was also able to properly separate diatoms when overlap occurs, which helps estimate the number of diatoms, a key requirement for water quality grading. Segmentation methods based on pixel-wise classification can be roughly divided into two families: (i) semantic segmentation and (ii) instance segmentation.
Microscopic algae segmentation, specifically of diatoms, is an essential procedure for water quality assessment. The segmentation of these microalgae is still a challenge for computer vision. This paper addresses for the first time this problem using deep learning approaches to predict exactly those pixels that belong to each class, i.e., diatom and non diatom. A comparison between semantic segmentation and instance segmentation is carried out, and the performance of these methods is evaluated in the presence of different types of noise. The trained models are then evaluated with the same raw images used for manual diatom identification. A total of 126 images of the entire field of view at 60x magnification, with a size of 2592x1944 pixels, are analyzed. The images contain 10 different taxa plus debris and fragments. The best results were obtained with instance segmentation achieving an average precision of 85% with 86% sensitivity and 91% specificity (up to 92% precision with 98%, both sensitivity and specificity for some taxa). Semantic segmentation was able to improve the average sensitivity up to 95% but decreasing the specificity down to 60% and precision to 57%. Instance segmentation was also able to properly separate diatoms when overlap occurs, which helps estimate the number of diatoms, a key requirement for water quality grading. Semantic segmentation for diatoms is used by Tang et al. (2018) but it is applied to a single taxon on images containing a single diatom shell.Although the authors claim an improvement compared to similar previous studies for the same taxon, with a balanced result between precision and recall, the F1-measure remains low with a value of 0.69.
Microscopic algae segmentation, specifically of diatoms, is an essential procedure for water quality assessment. The segmentation of these microalgae is still a challenge for computer vision. This paper addresses for the first time this problem using deep learning approaches to predict exactly those pixels that belong to each class, i.e., diatom and non diatom. A comparison between semantic segmentation and instance segmentation is carried out, and the performance of these methods is evaluated in the presence of different types of noise. The trained models are then evaluated with the same raw images used for manual diatom identification. A total of 126 images of the entire field of view at 60x magnification, with a size of 2592x1944 pixels, are analyzed. The images contain 10 different taxa plus debris and fragments. The best results were obtained with instance segmentation achieving an average precision of 85% with 86% sensitivity and 91% specificity (up to 92% precision with 98%, both sensitivity and specificity for some taxa). Semantic segmentation was able to improve the average sensitivity up to 95% but decreasing the specificity down to 60% and precision to 57%. Instance segmentation was also able to properly separate diatoms when overlap occurs, which helps estimate the number of diatoms, a key requirement for water quality grading. In this work, we present for the first time the application of instance segmentation applied to diatom segmentation and quantification.Instance segmentation is compared to semantic segmentation.Furthermore, the robustness of the method in noise conditions is analyzed.An average value of 0.85 for F1-measure is obtained with instance segmentation against 0.71 obtained with semantic segmentation applied to images containing multiple diatoms of 10 taxa.All overlapped diatoms were separated and correctly quantified.
Driving Behavior (DB) is a complex concept describing how the driver operates the vehicle in the context of the driving scene and surrounding environment. Recently, DB assessment has become an emerging topic of great importance. However, in view of to the stochastic nature of driving, measuring and modeling, DB continues to be a challenging topic today. As such, this paper argues that to move forward in understanding the individual and organizational mechanisms influencing DB, a conceptual framework is outlined whereby DB is viewed in terms of different dimensions established within the Driver–Vehicle–Environment (DVE) system. Moreover, DB assessment has been approached by various machine learning (ML) models. Still, there has been no attempt to analyze the empirical evidence on ML models in a systematic way, furthermore, ML based DB models often face problems and raise questions that must be resolved. This article presents a systematic literature review (SLR) of the DB investigation concept; In the first phase, a framework for conceptualizing a holistic approach of the different facets in DB analysis is presented, as well as a scheme to guide the future development and implementation of DB assessment strategies. In the second phase, an overview of the literature on ML is designed, revealing a premier and unbiased survey of the existing empirical research of ML techniques that have been applied to DB analysis. The results of this study identify an interpretive framework incorporating multiple dimensions influencing the driver’s conduct, in an attempt to achieve a thorough understanding of the DB concept within the DVE system in which the drivers operate. Additionally, 82 primary studies published during the last decade and eight broadly used ML models were identified. The findings of this review prove the performance capability of the ML techniques for assessing DB. The models using the ML techniques outperform other conventional approaches. However, the application of ML models in DB analysis is still limited and more effort is needed to obtain well-formed and generalizable results. To this end, and based on the outcomes obtained in this work, future guidelines have been provided to practitioners and researchers to grasp the major contributions and challenges in the state-of-the-art research. The phenomenon concerning road traffic crashes has become a major concern worldwide.According to the global status report on road safety conducted by the World Health Organization (WHO) in 2015, 1.25 million traffic-related fatalities occur annually worldwide, with millions more sustaining serious injuries and living with long-term adverse health consequences; road traffic injuries are currently estimated to be the leading cause of death among young people, and the main cause of death among those aged 15–29 years.(“WHO | Global status report on road safety 2015”, 2015).Road safety perception cannot be detached from the analysis of the driver behavior (DB) as the major part of traffic accidents is caused by human factors as it was inferred that they took part in the manifestation of 95% of all accidents (Evans, 1991, 1996).For these purposes, analyzing DB can aid assessing driver performance, enhance traffic safety and, furthermore, endorse the development of intelligent and resilient transportation systems.
Driving Behavior (DB) is a complex concept describing how the driver operates the vehicle in the context of the driving scene and surrounding environment. Recently, DB assessment has become an emerging topic of great importance. However, in view of to the stochastic nature of driving, measuring and modeling, DB continues to be a challenging topic today. As such, this paper argues that to move forward in understanding the individual and organizational mechanisms influencing DB, a conceptual framework is outlined whereby DB is viewed in terms of different dimensions established within the Driver–Vehicle–Environment (DVE) system. Moreover, DB assessment has been approached by various machine learning (ML) models. Still, there has been no attempt to analyze the empirical evidence on ML models in a systematic way, furthermore, ML based DB models often face problems and raise questions that must be resolved. This article presents a systematic literature review (SLR) of the DB investigation concept; In the first phase, a framework for conceptualizing a holistic approach of the different facets in DB analysis is presented, as well as a scheme to guide the future development and implementation of DB assessment strategies. In the second phase, an overview of the literature on ML is designed, revealing a premier and unbiased survey of the existing empirical research of ML techniques that have been applied to DB analysis. The results of this study identify an interpretive framework incorporating multiple dimensions influencing the driver’s conduct, in an attempt to achieve a thorough understanding of the DB concept within the DVE system in which the drivers operate. Additionally, 82 primary studies published during the last decade and eight broadly used ML models were identified. The findings of this review prove the performance capability of the ML techniques for assessing DB. The models using the ML techniques outperform other conventional approaches. However, the application of ML models in DB analysis is still limited and more effort is needed to obtain well-formed and generalizable results. To this end, and based on the outcomes obtained in this work, future guidelines have been provided to practitioners and researchers to grasp the major contributions and challenges in the state-of-the-art research. Human driving behavior is a complex concept that, in general terms, delineate how the driver manipulates the vehicle in the context of the driving scene and surrounding environment (Martinez et al., 2017).In recent years, DB has become a topic of interest among the public and researchers; it is generally considered to be one of the most important factors in crash occurrence, yet due to the stochastic nature of driving, measuring and modeling, driving behavior continues to be a challenging topic today (Sagberg et al., 2015).For instance, many researchers have studied the effects of driver’s physiological and psychological characteristics on DB from different standpoints (Ariën et al., 2013; Faure et al., 2016; Dahlen and White, 2006; Li et al., 2017a; Jacobé de Naurois et al., 2017a).On the other hand, taking driver’s operational maneuvers as the research objective, some scholars explored various behaviors such as lane change, acceleration and turning events (Ghasemzadeh and Ahmed, 2018; Kim et al., 2017; Paefgen, Kehr, Zhai, & Michahelles, 2012); Within this context, significant correlations were found between driving performance and the driver’s profile (e.g. gender, age and experience amongst others).
Driving Behavior (DB) is a complex concept describing how the driver operates the vehicle in the context of the driving scene and surrounding environment. Recently, DB assessment has become an emerging topic of great importance. However, in view of to the stochastic nature of driving, measuring and modeling, DB continues to be a challenging topic today. As such, this paper argues that to move forward in understanding the individual and organizational mechanisms influencing DB, a conceptual framework is outlined whereby DB is viewed in terms of different dimensions established within the Driver–Vehicle–Environment (DVE) system. Moreover, DB assessment has been approached by various machine learning (ML) models. Still, there has been no attempt to analyze the empirical evidence on ML models in a systematic way, furthermore, ML based DB models often face problems and raise questions that must be resolved. This article presents a systematic literature review (SLR) of the DB investigation concept; In the first phase, a framework for conceptualizing a holistic approach of the different facets in DB analysis is presented, as well as a scheme to guide the future development and implementation of DB assessment strategies. In the second phase, an overview of the literature on ML is designed, revealing a premier and unbiased survey of the existing empirical research of ML techniques that have been applied to DB analysis. The results of this study identify an interpretive framework incorporating multiple dimensions influencing the driver’s conduct, in an attempt to achieve a thorough understanding of the DB concept within the DVE system in which the drivers operate. Additionally, 82 primary studies published during the last decade and eight broadly used ML models were identified. The findings of this review prove the performance capability of the ML techniques for assessing DB. The models using the ML techniques outperform other conventional approaches. However, the application of ML models in DB analysis is still limited and more effort is needed to obtain well-formed and generalizable results. To this end, and based on the outcomes obtained in this work, future guidelines have been provided to practitioners and researchers to grasp the major contributions and challenges in the state-of-the-art research. While the number of traffic accidents and their outcomes, mainly human injuries and fatalities emphasize the prominence of investigating the factors which contribute to their occurrence; principal factors in accidents include human, vehicle, and environment factors.From this perspective, DB has been considered a direct outcome of the impulse encountered from the road infrastructure, surrounding environment and atmosphere inside the vehicle (Waard, 1996).As such, in order to conduct a more reliable analysis of the DB, diverse types of parameters have been taken into account; The Driver–Vehicle–Environment (DVE) is based on the concept of the “joint” cognitive system, where the dynamic interactions between driver, vehicle and environment are represented in a harmonized and integrated way (Hollnagel, 2005).We have adopted an approach with the purpose of investigating a set of parameters needed for estimation of the DB, consequently, various DVE characteristics have been considered in an effort to analyze DB to improve driver safety and reduce accident rates.Although it is generally assumed that the characterization of the DB is related to the aforementioned triptych: the DVE philosophy, there has been only limited investigation focusing on the details of this relationship and how the DVE characteristics influence the evaluation of the DB.However, perhaps most importantly, there is still a lack of a common underlying conceptual framework to guide this research and clearly distinguish the relationship between the different dimensions of the DB and the DVE model, as well as a commonly accepted scheme to instruct future implementations of DB assessment strategies.
Driving Behavior (DB) is a complex concept describing how the driver operates the vehicle in the context of the driving scene and surrounding environment. Recently, DB assessment has become an emerging topic of great importance. However, in view of to the stochastic nature of driving, measuring and modeling, DB continues to be a challenging topic today. As such, this paper argues that to move forward in understanding the individual and organizational mechanisms influencing DB, a conceptual framework is outlined whereby DB is viewed in terms of different dimensions established within the Driver–Vehicle–Environment (DVE) system. Moreover, DB assessment has been approached by various machine learning (ML) models. Still, there has been no attempt to analyze the empirical evidence on ML models in a systematic way, furthermore, ML based DB models often face problems and raise questions that must be resolved. This article presents a systematic literature review (SLR) of the DB investigation concept; In the first phase, a framework for conceptualizing a holistic approach of the different facets in DB analysis is presented, as well as a scheme to guide the future development and implementation of DB assessment strategies. In the second phase, an overview of the literature on ML is designed, revealing a premier and unbiased survey of the existing empirical research of ML techniques that have been applied to DB analysis. The results of this study identify an interpretive framework incorporating multiple dimensions influencing the driver’s conduct, in an attempt to achieve a thorough understanding of the DB concept within the DVE system in which the drivers operate. Additionally, 82 primary studies published during the last decade and eight broadly used ML models were identified. The findings of this review prove the performance capability of the ML techniques for assessing DB. The models using the ML techniques outperform other conventional approaches. However, the application of ML models in DB analysis is still limited and more effort is needed to obtain well-formed and generalizable results. To this end, and based on the outcomes obtained in this work, future guidelines have been provided to practitioners and researchers to grasp the major contributions and challenges in the state-of-the-art research. As the DB field evolved, researchers studied the use of algorithms from Machine Learning (ML), an area of artificial intelligence (AI) that has been studied since the late 1950s (Martens, 1959); numerous ML-based models were proposed to recognize or predict the DB in an attempt to improve the safety and comfort of drivers, as well as other road users.ML addresses the question of how to build a computer system that improves automatically through experience (Jordan and Mitchell, 2015).ML techniques have been characterized by (i) their autonomously surmounting major non-linear problems using data sets from multiple sources; and (ii) their ability to easily incorporate newly data in an attempt to improve estimation performance (Begg and Kamruzzaman, 2005; Chlingaryan et al., 2018).They are being used in the context of studying DB to provide users with better recommendations and help constructing powerful ML models.However, the ML discipline does not have a definite classification scheme for its algorithms, mostly due to the number of paradigms and the uncertainties introduced in the literature (Lv and Tang, 2011).Subsequently, it becomes difficult and confusing to choose an ML algorithm that fits one’s need when developing a DB computational model.
Driving Behavior (DB) is a complex concept describing how the driver operates the vehicle in the context of the driving scene and surrounding environment. Recently, DB assessment has become an emerging topic of great importance. However, in view of to the stochastic nature of driving, measuring and modeling, DB continues to be a challenging topic today. As such, this paper argues that to move forward in understanding the individual and organizational mechanisms influencing DB, a conceptual framework is outlined whereby DB is viewed in terms of different dimensions established within the Driver–Vehicle–Environment (DVE) system. Moreover, DB assessment has been approached by various machine learning (ML) models. Still, there has been no attempt to analyze the empirical evidence on ML models in a systematic way, furthermore, ML based DB models often face problems and raise questions that must be resolved. This article presents a systematic literature review (SLR) of the DB investigation concept; In the first phase, a framework for conceptualizing a holistic approach of the different facets in DB analysis is presented, as well as a scheme to guide the future development and implementation of DB assessment strategies. In the second phase, an overview of the literature on ML is designed, revealing a premier and unbiased survey of the existing empirical research of ML techniques that have been applied to DB analysis. The results of this study identify an interpretive framework incorporating multiple dimensions influencing the driver’s conduct, in an attempt to achieve a thorough understanding of the DB concept within the DVE system in which the drivers operate. Additionally, 82 primary studies published during the last decade and eight broadly used ML models were identified. The findings of this review prove the performance capability of the ML techniques for assessing DB. The models using the ML techniques outperform other conventional approaches. However, the application of ML models in DB analysis is still limited and more effort is needed to obtain well-formed and generalizable results. To this end, and based on the outcomes obtained in this work, future guidelines have been provided to practitioners and researchers to grasp the major contributions and challenges in the state-of-the-art research. Since the volume of research in the field of DB is expanding constantly, researchers may find it challenging to track the use and the trends of ML algorithms in analyzing DB.In addition, it is becoming more and more difficult to evaluate critically and to provide an overview of the related empirical studies.One way to assist academics and practitioners in using ML techniques in the assessment of the DB is to lay the foundations for grasping the major contributions and shortcomings in the state-of-the-art research.Existing literature reviews of DB investigation can be divided into two categories: traditional literature reviews (TLR) and systematic literature reviews (SLR).The traditional literature reviews (Halim et al., 2016b; Meiring and Myburgh, 2015); mainly cover the state-of-the-art and contemporary beliefs, whereas more methodologically explicit approaches (Vilac et al., 2017) aim to answer various research questions pertaining to the addressed problem.To the best of authors’ knowledge, there is no existing SLR that focuses on ML models for DB analysis, which motivates our work in this paper.Further to this, little to no research has directly constructed a thorough treatment of the DB concept accounting for the variety of its dimensions within the DVE structure.This article provides a systematic literature review (SLR) to identify the applications of ML techniques in DB assessment domain.In the first phase, A framework for conceptualizing DB is outlined, which illustrates a holistic approach of the different facets in perceiving DB as well as a scheme to instruct future development and implementation of the DB assessment strategies.It is expected that more information can be obtained about the investigation of the DB in the context of the DVE approach, and make better implementation or research decisions.In the second phase, an overview of the literature on ML is designed, revealing a premier and unbiased survey of the existing empirical research of ML techniques that have been applied to DB evaluation.Specifically, we performed an SLR on ML models published during the last decade (2009–2019).We further provide future guidelines to DB analysis practitioners and researchers regarding the application of the ML techniques in the field.
Driving Behavior (DB) is a complex concept describing how the driver operates the vehicle in the context of the driving scene and surrounding environment. Recently, DB assessment has become an emerging topic of great importance. However, in view of to the stochastic nature of driving, measuring and modeling, DB continues to be a challenging topic today. As such, this paper argues that to move forward in understanding the individual and organizational mechanisms influencing DB, a conceptual framework is outlined whereby DB is viewed in terms of different dimensions established within the Driver–Vehicle–Environment (DVE) system. Moreover, DB assessment has been approached by various machine learning (ML) models. Still, there has been no attempt to analyze the empirical evidence on ML models in a systematic way, furthermore, ML based DB models often face problems and raise questions that must be resolved. This article presents a systematic literature review (SLR) of the DB investigation concept; In the first phase, a framework for conceptualizing a holistic approach of the different facets in DB analysis is presented, as well as a scheme to guide the future development and implementation of DB assessment strategies. In the second phase, an overview of the literature on ML is designed, revealing a premier and unbiased survey of the existing empirical research of ML techniques that have been applied to DB analysis. The results of this study identify an interpretive framework incorporating multiple dimensions influencing the driver’s conduct, in an attempt to achieve a thorough understanding of the DB concept within the DVE system in which the drivers operate. Additionally, 82 primary studies published during the last decade and eight broadly used ML models were identified. The findings of this review prove the performance capability of the ML techniques for assessing DB. The models using the ML techniques outperform other conventional approaches. However, the application of ML models in DB analysis is still limited and more effort is needed to obtain well-formed and generalizable results. To this end, and based on the outcomes obtained in this work, future guidelines have been provided to practitioners and researchers to grasp the major contributions and challenges in the state-of-the-art research. The work presented on this paper is an extension of our conference paper originally presented at the International Conference on Advanced Intelligent Systems for Sustainable Development - AI2SD’ 2018 (Elamrani Abou Elassad and Mousannif, 2019) which proposed unconventional taxonomies based on the nature of the conducted study, measurement patterns and supervision motives underlying the assessment models of driving behavior.Additional material has been included in order to create a more in-depth research paper.In comparison to our original work, progress has been made regarding the following aspects:
Barley, Hordeum vulgare L., has been cultivated in Fennoscandia (Denmark, Norway, Sweden, Finland) since the start of the Neolithic around 4000 years BCE. Genetic studies of extant and 19th century barley landraces from the area have previously shown that distinct genetic groups exist with geographic structure according to latitude, suggesting strong local adaptation of cultivated crops. It is, however, not known what time depth these patterns reflect. Here we evaluate different archaeobotanical specimens of barley, extending several centuries in time, for their potential to answer this question by analysis of aDNA. Forty-six charred grains, nineteen waterlogged specimens and nine desiccated grains were evaluated by PCR and KASP genotyping. The charred samples did not contain any detectable endogenous DNA. Some waterlogged samples permitted amplification of endogenous DNA, however not sufficient for subsequent analysis. Desiccated plant materials provided the highest genotyping success rates of the materials analysed here in agreement with previous studies. Five desiccated grains from a grave from 1679 in southern Sweden were genotyped with 100 SNP markers and the data was compared to genotypes of 19th century landraces from Fennoscandia. The results showed that the genetic composition of barley grown in southern Sweden changed very little from late 17th to late 19th century and that farmers stayed true to locally adapted crops in spite of societal and agricultural development. Barley, Hordeum vulgare, is an important cereal crop in the temperate world, used as both food and feed (reviewed in Newton et al., 2011).It is a diploid and self-fertilizing species, first domesticated around 10,000 years ago (Zohary et al., 2012).Studies of geographical distribution of genetic diversity suggest multiple domestication events (Dai et al., 2012; Morrell and Clegg, 2007; Ren et al., 2013; Wang et al., 2016) or an origin from a mosaic ancestry from wild progenitors from a wide geographic region (Poets et al., 2015).
Barley, Hordeum vulgare L., has been cultivated in Fennoscandia (Denmark, Norway, Sweden, Finland) since the start of the Neolithic around 4000 years BCE. Genetic studies of extant and 19th century barley landraces from the area have previously shown that distinct genetic groups exist with geographic structure according to latitude, suggesting strong local adaptation of cultivated crops. It is, however, not known what time depth these patterns reflect. Here we evaluate different archaeobotanical specimens of barley, extending several centuries in time, for their potential to answer this question by analysis of aDNA. Forty-six charred grains, nineteen waterlogged specimens and nine desiccated grains were evaluated by PCR and KASP genotyping. The charred samples did not contain any detectable endogenous DNA. Some waterlogged samples permitted amplification of endogenous DNA, however not sufficient for subsequent analysis. Desiccated plant materials provided the highest genotyping success rates of the materials analysed here in agreement with previous studies. Five desiccated grains from a grave from 1679 in southern Sweden were genotyped with 100 SNP markers and the data was compared to genotypes of 19th century landraces from Fennoscandia. The results showed that the genetic composition of barley grown in southern Sweden changed very little from late 17th to late 19th century and that farmers stayed true to locally adapted crops in spite of societal and agricultural development. Subsequent to domestication, barley spread over large areas of the Old World.Geographic structuring of the genetic diversity of barley has been shown both among and within continents (Jones et al., 2011; Morrell and Clegg, 2007; Muñoz-Amatriaín et al., 2014; Pasam et al., 2014; Poets et al., 2015).This geographic structure results from both the different routes of the spread of barley as well as from climate adaption (Jones et al., 2011; Pasam et al., 2014).There is also differentiation between the gene pools of two- and six-row barley and naked and hulled forms (Jones et al., 2011; Malysheva-Otto et al., 2006).
Barley, Hordeum vulgare L., has been cultivated in Fennoscandia (Denmark, Norway, Sweden, Finland) since the start of the Neolithic around 4000 years BCE. Genetic studies of extant and 19th century barley landraces from the area have previously shown that distinct genetic groups exist with geographic structure according to latitude, suggesting strong local adaptation of cultivated crops. It is, however, not known what time depth these patterns reflect. Here we evaluate different archaeobotanical specimens of barley, extending several centuries in time, for their potential to answer this question by analysis of aDNA. Forty-six charred grains, nineteen waterlogged specimens and nine desiccated grains were evaluated by PCR and KASP genotyping. The charred samples did not contain any detectable endogenous DNA. Some waterlogged samples permitted amplification of endogenous DNA, however not sufficient for subsequent analysis. Desiccated plant materials provided the highest genotyping success rates of the materials analysed here in agreement with previous studies. Five desiccated grains from a grave from 1679 in southern Sweden were genotyped with 100 SNP markers and the data was compared to genotypes of 19th century landraces from Fennoscandia. The results showed that the genetic composition of barley grown in southern Sweden changed very little from late 17th to late 19th century and that farmers stayed true to locally adapted crops in spite of societal and agricultural development. Barley has long been an important crop in northern Europe and Fennoscandia where it was introduced during the early Neolithic in 4000 BCE.Naked barley was introduced first, with hulled forms appearing around 1000 BCE to later dominate the barley cultivation of this region (Grabowski, 2011).Two-rowed barley has been recorded from 17th century but was likely introduced earlier (Hjelmqvist, 1996), and became important in production only as of the end of the 19th century (Myrdal and Morell, 2011).
Barley, Hordeum vulgare L., has been cultivated in Fennoscandia (Denmark, Norway, Sweden, Finland) since the start of the Neolithic around 4000 years BCE. Genetic studies of extant and 19th century barley landraces from the area have previously shown that distinct genetic groups exist with geographic structure according to latitude, suggesting strong local adaptation of cultivated crops. It is, however, not known what time depth these patterns reflect. Here we evaluate different archaeobotanical specimens of barley, extending several centuries in time, for their potential to answer this question by analysis of aDNA. Forty-six charred grains, nineteen waterlogged specimens and nine desiccated grains were evaluated by PCR and KASP genotyping. The charred samples did not contain any detectable endogenous DNA. Some waterlogged samples permitted amplification of endogenous DNA, however not sufficient for subsequent analysis. Desiccated plant materials provided the highest genotyping success rates of the materials analysed here in agreement with previous studies. Five desiccated grains from a grave from 1679 in southern Sweden were genotyped with 100 SNP markers and the data was compared to genotypes of 19th century landraces from Fennoscandia. The results showed that the genetic composition of barley grown in southern Sweden changed very little from late 17th to late 19th century and that farmers stayed true to locally adapted crops in spite of societal and agricultural development. By studying barley grains from 19th century seed collections, the presence of fine scale population structure was shown to exist across Fennoscandia (Denmark, Norway, Sweden and Finland) with geographic structure according to latitude extending across country borders (Forsberg et al., 2015; Leino and Hagenblad, 2010).Although these findings suggest a role of climate in shaping the geographic structure, it is not known whether the observed geographical structure is a recently developed phenomenon or what time depth these patterns reflect.Comparisons of barley from Northern Fennoscandia collected some three decades apart (years 1869–1896) suggest that shifts in the genetic structure over time can occur (Forsberg et al., unpublished data).However, in order to gain deeper knowledge of the temporal distribution of population structure, genetic analysis of archaeological materials is needed.
Barley, Hordeum vulgare L., has been cultivated in Fennoscandia (Denmark, Norway, Sweden, Finland) since the start of the Neolithic around 4000 years BCE. Genetic studies of extant and 19th century barley landraces from the area have previously shown that distinct genetic groups exist with geographic structure according to latitude, suggesting strong local adaptation of cultivated crops. It is, however, not known what time depth these patterns reflect. Here we evaluate different archaeobotanical specimens of barley, extending several centuries in time, for their potential to answer this question by analysis of aDNA. Forty-six charred grains, nineteen waterlogged specimens and nine desiccated grains were evaluated by PCR and KASP genotyping. The charred samples did not contain any detectable endogenous DNA. Some waterlogged samples permitted amplification of endogenous DNA, however not sufficient for subsequent analysis. Desiccated plant materials provided the highest genotyping success rates of the materials analysed here in agreement with previous studies. Five desiccated grains from a grave from 1679 in southern Sweden were genotyped with 100 SNP markers and the data was compared to genotypes of 19th century landraces from Fennoscandia. The results showed that the genetic composition of barley grown in southern Sweden changed very little from late 17th to late 19th century and that farmers stayed true to locally adapted crops in spite of societal and agricultural development. Archaeological remains of cereal grains are mostly found in charred condition (Dennell, 1976).However, analyses of DNA from charred grains have met great difficulties (e.g. Boscato et al., 2008; Oliveira et al., 2012b, reviewed in Brown and Barnes, 2015; Palmer et al., 2012), although occasional positive results have been reported (e.g. Bilgic et al., 2016; Bunning et al., 2012; Castillo et al., 2016; Fernández et al., 2013).Recent high-throughput analysis of 38 charred or partially charred archaeobotanical samples from four species (including barley and rice) indicates that charred cereal grains entirely lack endogenous DNA, raising concerns regarding the authenticity of the data generated in previous studies (Nistelberger et al., 2016).
Barley, Hordeum vulgare L., has been cultivated in Fennoscandia (Denmark, Norway, Sweden, Finland) since the start of the Neolithic around 4000 years BCE. Genetic studies of extant and 19th century barley landraces from the area have previously shown that distinct genetic groups exist with geographic structure according to latitude, suggesting strong local adaptation of cultivated crops. It is, however, not known what time depth these patterns reflect. Here we evaluate different archaeobotanical specimens of barley, extending several centuries in time, for their potential to answer this question by analysis of aDNA. Forty-six charred grains, nineteen waterlogged specimens and nine desiccated grains were evaluated by PCR and KASP genotyping. The charred samples did not contain any detectable endogenous DNA. Some waterlogged samples permitted amplification of endogenous DNA, however not sufficient for subsequent analysis. Desiccated plant materials provided the highest genotyping success rates of the materials analysed here in agreement with previous studies. Five desiccated grains from a grave from 1679 in southern Sweden were genotyped with 100 SNP markers and the data was compared to genotypes of 19th century landraces from Fennoscandia. The results showed that the genetic composition of barley grown in southern Sweden changed very little from late 17th to late 19th century and that farmers stayed true to locally adapted crops in spite of societal and agricultural development. Plant specimens are also sometimes found in waterlogged conditions.Like charring, waterlogging is considered suboptimal for DNA preservation (Schlumbaum et al., 2008) and successful studies of aDNA in waterlogged, archaeobotanical specimens all concern species with strong fruit endocarps such as grape seeds (Cappellini et al., 2010; Manen et al., 2003; Wales et al., 2016), olive fruit stones (Elbaum et al., 2006), Prunus fruit stones (Pollmann et al., 2005), apple seeds (Schlumbaum et al., 2012) and bottle gourd seeds (Schlumbaum and Vandorpe, 2012).Cereal grains have much softer grain testa, and for this reason, they have been considered less well-protected against hydrolytic degradation of DNA.To our knowledge few attempts have been made to analyse waterlogged cereal grains, and no positive results have been reported (Brown et al., 1993; Fernández et al., 2013).
Barley, Hordeum vulgare L., has been cultivated in Fennoscandia (Denmark, Norway, Sweden, Finland) since the start of the Neolithic around 4000 years BCE. Genetic studies of extant and 19th century barley landraces from the area have previously shown that distinct genetic groups exist with geographic structure according to latitude, suggesting strong local adaptation of cultivated crops. It is, however, not known what time depth these patterns reflect. Here we evaluate different archaeobotanical specimens of barley, extending several centuries in time, for their potential to answer this question by analysis of aDNA. Forty-six charred grains, nineteen waterlogged specimens and nine desiccated grains were evaluated by PCR and KASP genotyping. The charred samples did not contain any detectable endogenous DNA. Some waterlogged samples permitted amplification of endogenous DNA, however not sufficient for subsequent analysis. Desiccated plant materials provided the highest genotyping success rates of the materials analysed here in agreement with previous studies. Five desiccated grains from a grave from 1679 in southern Sweden were genotyped with 100 SNP markers and the data was compared to genotypes of 19th century landraces from Fennoscandia. The results showed that the genetic composition of barley grown in southern Sweden changed very little from late 17th to late 19th century and that farmers stayed true to locally adapted crops in spite of societal and agricultural development. With cereals, the most successful studies of aDNA thus far have used desiccated materials (Hagenblad et al., 2017; Li et al., 2016; Li et al., 2011; Mascher et al., 2016; Oliveira et al., 2012b).Both multiple copy and nuclear single copy markers have been used successfully with desiccated plant materials, as well as more high-throughput sequencing methods, with the aim to identify taxonomic origin and compare genetic diversity between old specimens and extant landraces.
Excavations conducted in the context of the Palaepaphos Urban Landscape Project (PULP) have revealed a defensive monument of the Cypro-Classical period (fifth and fourth centuries BCE), which had been preserved under an anthropogenic mound (tumulus) of the 3rd century BCE. Besides stone-work, the construction of the monumental rampart made extensive use of mudbricks. In 2016–2017, PULP introduced a pilot study based on analytical techniques (pXRF, SEM-EDS, granulometric and petrographic analysis) to address issues relating to the manufacture and construction of the earthen architecture of the rampart. The paper presents a description of the geoarchaeological analyses and their results, which have highlighted specific manufacturing practices in relation to the construction of the monument. Given that the rampart constituted a major investment of the royal authorities of ancient Paphos, the results provide new information on the production of earthen building materials and also on environmental choices with respect to raw material selection in the context of a public project carried out by a central authority circa the mid first millennium BCE. Since the 1980s numerous geoarchaeological studies in the Near East have showcased the importance of earthen architectural analysis not only in relation to the built environment but as an expression of social agency (French, 1984; Friesem et al., 2011; Goldberg, 1979; Love, 2013; Morgenstein and Redmount, 1998; Rosen, 1986).In the eastern Mediterranean island of Cyprus mudbricks were, and are, an integral part of its architectural identity.While the earliest have been recovered in the context of Neolithic sites (e.g. Aurenche, 1981; Love, 2012; Philokyprou, 1998), mudbricks are still in use today as a key component of vernacular architecture (Costi De Castrillo et al., 2017; Illampas et al., 2011).
The main focus of the present study was to explore the longitudinal changes in the brain executive control system and default mode network after hemispherotomy. Resting-state functional magnetic resonance imaging and diffusion tensor imaging were collected in two children with drug-resistnt epilepsy underwent hemispherotomy. Two patients with different curative effects showed different trajectories of brain connectivity after surgery. The failed hemispherotomy might be due to the fact that the synchrony of epileptic neurons in both hemispheres is preserved by residual neural pathways. Loss of interhemispheric correlations with increased intrahemispheric correlations can be considered as neural marker for evaluating the success of hemispherotomy. Drug-resistant epilepsy is one of the most common neurological disorders in childhood, and it is viewed as a disease of hypersynchronous neuronal activity in its electrophysiological substrates [34].The normal developmental trajectory of the pediatric brain can be affected by epilepsy [21].Achieving permanent seizure freedom is the ultimate goal in clinical practice.Surgical intervention is the most common treatment of drug-resistant epilepsy.For patients with multilobar or hemispheric drug-resistant epilepsy hemispheric disconnection has been long used in clinical practice.This surgical tool can be classified into two types: anatomic hemispherectomy and functional hemispherectomy [17].Anatomic hemispherectomy was the first procedure of choice [2].This procedure has classically involved removal of the temporal, frontal, parietal, and occipital lobes, sparing the thalamus, basal ganglia and insular cortex.Although seizure outcome was excellent after anatomic hemispherectomy, the technique has been abandoned in many centers because of the high incidence of delayed potentially fatal complications.To reduce the risk of complications, a new technique called functional hemispherectomy (hemispherotomy) was developed.Hemispherotomy is a procedure to maximally disconnect the white matter connecting the diseased hemisphere, while performing minimal cortical resections [5].Previous studies have shown that hemispherotomy is an important treatment for seizure reduction in patients with unilateral drug-resistant epilepsy [12,15,19].As is known, the two hemispheres of the brain are mainly connected with the corpus callosum and subcortical pathways.The pathophysiologic basis for the use of hemispherotomy is based on the hypothesis that the spread of seizure activity between the two hemispheres of the brain is mainly transmitted through the corpus callosum and subcortical pathways [1,12,18].Therefore, severing these major cortico-cortical connections between the two hemispheres should dramatically reduce seizures.Over the years, numerous studies in humans have proved the potential advantageous effects of this procedure [13,24,26,32,39].
The main focus of the present study was to explore the longitudinal changes in the brain executive control system and default mode network after hemispherotomy. Resting-state functional magnetic resonance imaging and diffusion tensor imaging were collected in two children with drug-resistnt epilepsy underwent hemispherotomy. Two patients with different curative effects showed different trajectories of brain connectivity after surgery. The failed hemispherotomy might be due to the fact that the synchrony of epileptic neurons in both hemispheres is preserved by residual neural pathways. Loss of interhemispheric correlations with increased intrahemispheric correlations can be considered as neural marker for evaluating the success of hemispherotomy. However, postoperative observations have shown that long-term rates of postoperative seizure freedom are in the range of 43–90% after hemispheric disconnection surgery [20,24,43].Diverse hemispherotomy based epilepsy surgical outcomes might indicate that the neural mechanism underlying brain reorganization after this surgery is not completely understood.The explanation for the clinical phenomenon after hemispherotomy in these previous studies was mainly based on the anatomic pattern plasticity by severing the interhemispheric fiber bundle, such as the corpus callosum and subcortical pathways [19,20].To understand the neural mechanism underlying this type of epilepsy and the diverse surgery outcomes, multimodal imaging methods would be necessary to give a relatively complete view on this topic.A single modality method may only partially detect the potentially important variation underlying the brain.Multimodal imaging methods can provide unprecedented opportunities to further deepen our understanding of the cross-information of the existing data and reveal the neural mechanism of the brain [6].Such approaches can provide a wealth of information, enabling researchers to more confidently draw conclusions.In present day research, multimodal neuroimaging has become a mainstay of neuroscience in humans [25,41].Furthermore, predictors of hemispherotomy outcomes are still unclear.Therefore, it is valuable to investigate the brain functional and structural network changes before and after this surgery.Brain functional and structural imaging data integration can yield further insights into brain reorganization processes after hemispherotomy.
The main focus of the present study was to explore the longitudinal changes in the brain executive control system and default mode network after hemispherotomy. Resting-state functional magnetic resonance imaging and diffusion tensor imaging were collected in two children with drug-resistnt epilepsy underwent hemispherotomy. Two patients with different curative effects showed different trajectories of brain connectivity after surgery. The failed hemispherotomy might be due to the fact that the synchrony of epileptic neurons in both hemispheres is preserved by residual neural pathways. Loss of interhemispheric correlations with increased intrahemispheric correlations can be considered as neural marker for evaluating the success of hemispherotomy. To explore these issues, we studied brain connectivity patterns by magnetic resonance imaging (MRI) in two children with drug-resistant epilepsy both before and after hemispherotomy.A longitudinal and multimodal neuroimaging method, combining diffusion tensor imaging (DTI) and resting-state functional MRI, were used in the present study to evaluate brain structural and functional changes.DTI is an advanced MRI technique used to map white matter pathways [3,46].Functional connectivity (FC) of resting-state functional MRI is a method to calculate the correlation or synchronization between the time course of activation between two regions.FC analyses have been used to examine brain connectivity changes before and after treatment in a variety of disorders, such as stroke [23,42] and epilepsy [22,31,32,44].Longitudinal changes in the FC of these studies have suggested that the FC method may be useful in examining the neural mechanism underlying brain recovery.
Taphonomic modifications to Neolithic human skeletal remains from six rock-cut tombs in Malta has provided key information about funerary practices and the local environment. Application of microscopic analysis, computed tomography (CT) scanning, and 3D imaging of the modifications has allowed their comparison with similar examples in modern and archaeological skeletal material. The modifications are interpreted as pupal chambers and feeding damage by dermestid beetles. Based on observation of the behaviour and ecology of dermestid beetles, we suggest several scenarios for funerary practices at the Xemxija tombs which nuance our current understanding of collective burial during the late Neolithic in Malta. Modifications to human skeletal remains from archaeological contexts provide a wealth of information regarding individual health, cultural interactions with the remains of the dead, and the effects of the burial environment (White, 1992; Haglund, 1997a, 1997b; Andrews and Fernandez-Jalvo, 2003; Ortner, 2003; Smith, 2006; Duday, 2009; Robb et al., 2015).The latter two categories of taphonomic markers – cultural and natural modifications to human remains following death and deposition – are sometimes juxtaposed, and there is debate as to whether natural modifications can reveal cultural practices (cf. Knüsel and Robb, 2016, 656).As the emerging field of funerary archaeoentomology is beginning to show, however, insect modifications to human skeletal remains allow us not only to reconstruct palaeoenvironments, but also advance our understanding of funerary practices (Huchet and Greenberg, 2010; Huchet et al., 2013; Huchet, 2014a, 2014b; Matu et al., 2017).
Taphonomic modifications to Neolithic human skeletal remains from six rock-cut tombs in Malta has provided key information about funerary practices and the local environment. Application of microscopic analysis, computed tomography (CT) scanning, and 3D imaging of the modifications has allowed their comparison with similar examples in modern and archaeological skeletal material. The modifications are interpreted as pupal chambers and feeding damage by dermestid beetles. Based on observation of the behaviour and ecology of dermestid beetles, we suggest several scenarios for funerary practices at the Xemxija tombs which nuance our current understanding of collective burial during the late Neolithic in Malta. Among the non-anthropogenic modifiers of bone, rodents and carnivores have particularly been noted for their ability to disturb archaeological assemblages through feeding practices (Haglund, 1997a, 1997b).Much less studied, however, are modifications to bone which result from insect activity such as feeding and the excavation of pupation chambers.Within the forensic sciences, insects provide crucial information to estimate the postmortem interval as they occupy specific ecological niches (Rodriguez and Bass, 1983; Haskell et al., 1989; Wells and LaMotte, 2001).Only four orders of insects have been recognised as agents of bone modification, particularly on fossil dinosaur (Paik, 2000; Roberts et al., 2007; Britt et al., 2008; Bader et al., 2009; Xing et al., 2013) and faunal remains (Martin and West, 1995; Kaiser, 2000; Holden et al., 2013).These include Coleoptera (dermestid beetles), Diptera (specifically the larvae of certain sarco-saprophagous flies), Hymenoptera (wasps and burrowing bees) and Isoptera (termites) (Huchet, 2014b).Insect modification to archaeological human remains can aid the interpretation of funerary practices, as each insect species has specific ecological niches and feeding practices and leaves morphologically distinct marks in bone.For example, dermestid beetles only colonise exposed cadavers and thrive best in warm, dry environments with restricted light; thus, they can provide information regarding both the environmental and cultural circumstances of funerary practices (Huchet et al., 2013).
In this article, the results of the archaeological excavations at the Late Mesolithic site of Monte do Carrascal 2 are presented. The site, located inland, southeast from the contemporary Sado shell middens, comprised two hearths with faunal remains and a set of lithic materials that were analysed techno-typologically, as well as in terms of their spatial distribution through GIS tools (K Ripley Function, Kernel Density Estimation and Nearest Neighbour analysis). The study points to a different functionality of this site when compared to most all other Mesolithic sites known in the region, with its uncommon inland location, suggesting that it was possibly a hunting camp. The Late Mesolithic settlement (ca. 6200–5600 cal BC) of western Iberia is best known through the shell middens complexes located in the Tagus (Roche, 1989; Bicho et al., 2017, 2013, 2010; Gonçalves, 2013; Gonçalves et al., 2017) and Sado rivers basins (Arias et al., 2015; Diniz and Arias, 2012) and in the coastal areas surrounding the Cape Saint Vincent (Carvalho and Valente, 2005), as well as some sites located in the Mira river valley and southwest coast (Arnaud, 1994; Lubell et al., 2007).The occupation of these regions during Late Mesolithic, with particular relevance to Sado and Muge valleys, is thought to be related to the coastal and estuarine transformations (Zilhão, 2003; Bicho et al., 2010) resulting from the 8.2 ka cold event (Alley and Agustsdottir, 2005; Alley et al., 1993).
In this article, the results of the archaeological excavations at the Late Mesolithic site of Monte do Carrascal 2 are presented. The site, located inland, southeast from the contemporary Sado shell middens, comprised two hearths with faunal remains and a set of lithic materials that were analysed techno-typologically, as well as in terms of their spatial distribution through GIS tools (K Ripley Function, Kernel Density Estimation and Nearest Neighbour analysis). The study points to a different functionality of this site when compared to most all other Mesolithic sites known in the region, with its uncommon inland location, suggesting that it was possibly a hunting camp. In the Sado valley, settlement consists exclusively of shell middens, in a total of 12 sites (Arias et al., 2015, 2016; Arnaud, 1989; Diniz and Arias, 2012), located mainly around the main river margins.Sado's shell middens are very different from the ones identified in Muge in terms of their visual impact on the landscape since they are not true mounds but rather shell layers interbedded in the local sandy deposits.They do present, however, the same range of archaeological attributes, including (1) habitat features such as pits and hearths; (2) a wide range of lithic materials, with particular emphasis for geometric microliths; (3) a diverse component of fauna, which includes shellfish (Scrobicularia plana and Cerastoderma edule), but also fish remains, and medium/small-sized mammals; (4) botanical remains and charcoal (Arias et al., 2016); (5) an important funerary component, with large numbers of human burials (Cunha and Umbelino, 2001; Peyroteo Stjerna, 2016).
In this article, the results of the archaeological excavations at the Late Mesolithic site of Monte do Carrascal 2 are presented. The site, located inland, southeast from the contemporary Sado shell middens, comprised two hearths with faunal remains and a set of lithic materials that were analysed techno-typologically, as well as in terms of their spatial distribution through GIS tools (K Ripley Function, Kernel Density Estimation and Nearest Neighbour analysis). The study points to a different functionality of this site when compared to most all other Mesolithic sites known in the region, with its uncommon inland location, suggesting that it was possibly a hunting camp. The functional role of the Sado and Muge sites has been frequently debated based on recent data, but there's still a gap in the knowledge about how these communities experienced the inland territory.Since the known sites in southern Portugal are mainly open-air and bear an important shell component, Carrascal 2 becomes a singular case, given its inland location that is close to Sado river shell middens but does not comprise a shell component.The importance of this site is not only related to its landscape location but also because it has a set of relatively well-preserved features and a lithic remains distribution that points to anthropic accumulations.
In this article, the results of the archaeological excavations at the Late Mesolithic site of Monte do Carrascal 2 are presented. The site, located inland, southeast from the contemporary Sado shell middens, comprised two hearths with faunal remains and a set of lithic materials that were analysed techno-typologically, as well as in terms of their spatial distribution through GIS tools (K Ripley Function, Kernel Density Estimation and Nearest Neighbour analysis). The study points to a different functionality of this site when compared to most all other Mesolithic sites known in the region, with its uncommon inland location, suggesting that it was possibly a hunting camp. Data from this site provides new insights, amplifying the known territory exploited by these communities and allowing the debate on territorial exploitation strategies (namely, in what concerns lithic raw materials acquisition), for a phase in which the mesolithic way of life is about to end with the Neolithic advent in the Portuguese territory.
A 52-year-old female with a longstanding history of drug-resistant epilepsy that included focal impaired awareness seizure presented at end of service of her vagus nerve stimulator (VNS) generator. She had undergone a generator replacement in 2010 without complication. However, her latest replacement was accompanied by multiple bouts of asystole. We discuss the case, possible causes of the asystole, and its relevance to the future of VNS generator replacement and epilepsy treatment. Drug-resistant epilepsy remains one of the most common neurological conditions, as well as one of the most challenging.Even as understanding of the etiology improves, epilepsy remains troublesome for both the patient and physician to treat.An estimated 1% of the population suffers from epilepsy, with one-third left uncontrolled by two or more anti-seizure medications or other possible therapies [1].In this drug-resistant population, in which seizure freedom is unlikely, vagus nerve stimulation (VNS) remains an effective adjunct therapy.Multiple studies have demonstrated its effectiveness including Révész et al. who investigated the effects of 130 patients implanted with a VNS between 2000 and 2013 and showed an increased response rate from 22.1 to 43.8% from the first and fifth year of implantation regardless of pharmacological anti-seizure drug (ASD) treatment changes [2].
Los Bifaces site, located in the Upper Tar River Basin, comprises an assemblage of at least 11 bifaces that were recovered on a surface of 2 m2. They show intermediate stages of manufacture and would have been deposited at the same time and in a grouped manner. The use of raw materials available in the region –despite the fact that the bifaces were not made at the site- and the high percentage of intentional fractures should be noted. Technological characteristics and their relationship with regional bifacial technology are described, and different hypotheses related to the artifactual assemblage formation are discussed, taking into account both natural and cultural aspects. Los Bifaces site is understood as a cache formed during the first moments of the settlement of the basin, and fractures are related to a ritual behavior. This broadens the considerations on caching in southern Patagonia and about hunter-gatherer occupations in the glacial basin of lakes Tar and San Martín. The discussion regarding the peopling of Patagonia has been enriched by the gradual incorporation of new studied places.One of these is the basin of lakes Tar and San Martín, located in the southwest of Santa Cruz province, Argentina, at the foot of the Andes mountain range, which occupies the western end of a large glacial valley that operates as a cul-de-sac.First hunter-gatherer settlement of the basin began around 11,200–10,800 cal years BP under arid conditions at the onset of the Holocene.The region seems to have been only occasionally occupied until ca. 5500 cal.years BP.By the Late Holocene and until historical times, the frequency of radiocarbon dates increases coinciding with the stabilization of local environmental conditions (Belardi et al., 2010, 2013).
Los Bifaces site, located in the Upper Tar River Basin, comprises an assemblage of at least 11 bifaces that were recovered on a surface of 2 m2. They show intermediate stages of manufacture and would have been deposited at the same time and in a grouped manner. The use of raw materials available in the region –despite the fact that the bifaces were not made at the site- and the high percentage of intentional fractures should be noted. Technological characteristics and their relationship with regional bifacial technology are described, and different hypotheses related to the artifactual assemblage formation are discussed, taking into account both natural and cultural aspects. Los Bifaces site is understood as a cache formed during the first moments of the settlement of the basin, and fractures are related to a ritual behavior. This broadens the considerations on caching in southern Patagonia and about hunter-gatherer occupations in the glacial basin of lakes Tar and San Martín. Regional archaeological research followed a distributional approach (Foley, 1981) seeking diversity in hunter-gatherer artifactual signal and landscape use.Differences in artifactual densities as well as in site function suggest a main residential use of the eastern steppe compared to a logistical marginal use of the forested western side of the basin (Belardi et al., 2010).By the same token, the valley floor in the steppe has been used on a year-round basis compared to a seasonal incorporation (end of spring and summer) of the surrounding basaltic plateaus.The later landscape has a heavy signature of activities related to guanaco (Lama guanicoe) hunting using blinds, as well as in engraved rock art located along the basaltic walls of several ponds that have been occupied as residential spots.So, a logistical use but with residential components could be proposed for the plateaus (Belardi et al., 2017).Similarities in rock art motifs, raw material circulation (mostly obsidian from the Pampa del Asador and limolite from the Cardiel Lake, both sources located northwards), together with the built landscape of hunting blinds in the plateaus, and projectile point types allowed to relate the Tar and San Martín Basin to those located both to the north and south, placing this one in the broader discussion of the peopling and human dynamics of southwestern Patagonia.It is within this general archaeological frame that Los Bifaces site appears as an outlier given its depositional and technological characteristics.Even so, our study will try to show that the site is just one more piece of the regional archaeological landscape variability.
Los Bifaces site, located in the Upper Tar River Basin, comprises an assemblage of at least 11 bifaces that were recovered on a surface of 2 m2. They show intermediate stages of manufacture and would have been deposited at the same time and in a grouped manner. The use of raw materials available in the region –despite the fact that the bifaces were not made at the site- and the high percentage of intentional fractures should be noted. Technological characteristics and their relationship with regional bifacial technology are described, and different hypotheses related to the artifactual assemblage formation are discussed, taking into account both natural and cultural aspects. Los Bifaces site is understood as a cache formed during the first moments of the settlement of the basin, and fractures are related to a ritual behavior. This broadens the considerations on caching in southern Patagonia and about hunter-gatherer occupations in the glacial basin of lakes Tar and San Martín. Los Bifaces site is analyzed by taking the technological characteristics of the assemblage into account (see a first description in Espinosa and Belardi, 2016).This site is located in the upper course of Tar River (815 m.a.s.l.), near the foot of Cardiel Chico plateau (Fig. 1), in a grassy steppe environment where the climatic type corresponds to cold weather conditions, with average temperatures ranging between 0° and 12 °C.Summers are cool and winters are cold to very cold due to the invasion of polar and subpolar winds (Oliva et al., 2001).
Los Bifaces site, located in the Upper Tar River Basin, comprises an assemblage of at least 11 bifaces that were recovered on a surface of 2 m2. They show intermediate stages of manufacture and would have been deposited at the same time and in a grouped manner. The use of raw materials available in the region –despite the fact that the bifaces were not made at the site- and the high percentage of intentional fractures should be noted. Technological characteristics and their relationship with regional bifacial technology are described, and different hypotheses related to the artifactual assemblage formation are discussed, taking into account both natural and cultural aspects. Los Bifaces site is understood as a cache formed during the first moments of the settlement of the basin, and fractures are related to a ritual behavior. This broadens the considerations on caching in southern Patagonia and about hunter-gatherer occupations in the glacial basin of lakes Tar and San Martín. The site is characterized by the presence of bifaces on the edge of a shallow pond that is currently dry (Fig. 2a).Seventy-eight (78) bifacial fragments distributed over a surface of 2 m2 were registered (Fig. 2b).The refitting of 32 fragments served to identify at least 11 bifaces.Given the elevation of the site, its incorporation into hunter-gatherer mobility circuits would have been feasible towards the end of spring-summer.
Los Bifaces site, located in the Upper Tar River Basin, comprises an assemblage of at least 11 bifaces that were recovered on a surface of 2 m2. They show intermediate stages of manufacture and would have been deposited at the same time and in a grouped manner. The use of raw materials available in the region –despite the fact that the bifaces were not made at the site- and the high percentage of intentional fractures should be noted. Technological characteristics and their relationship with regional bifacial technology are described, and different hypotheses related to the artifactual assemblage formation are discussed, taking into account both natural and cultural aspects. Los Bifaces site is understood as a cache formed during the first moments of the settlement of the basin, and fractures are related to a ritual behavior. This broadens the considerations on caching in southern Patagonia and about hunter-gatherer occupations in the glacial basin of lakes Tar and San Martín. Next, we describe and evaluate the social context of the deposition of the biface assemblage.In the first place, the use of bifacial technology in the Tar and San Martín lakes basin will be considered in order to ponder the importance of the assemblage under study, and secondly, the implemented methodology will be introduced and the technological features of the artifacts analyzed.Based on such information, different hypotheses related to natural causes as well as to caching behavior (Franco et al., 2017; Hurst, 2017), costly signaling (Quinn, 2015) and rituality will be discussed in order to account for the assemblage formation and its integration to the archeology of Tar and San Martín Lakes Basin.
Excavations at the site of Kul Tepe in the Jolfa region in north-western Iran have unearthed various archaeological materials from Late Neolithic/Early Chalcolithic to Achaemenid periods (end of 6th millennium to 3rd century BC). During the Chalcolithic and the Bronze Age most lithic tools used in Kul Tepe were made of obsidian. From the first and second excavation seasons, 53 and 32 obsidian samples were selected and analyzed by pXRF. According to the results, the main source of obsidian for the workshops in Kul Tepe was Syunik, but other sources in the Lake Sevan Basin like Ghegam, Bazenk, Choraphor and Gutansar and the Lake Van region (Nemrut Dağ and Meydan Dağ) were utilized also. The site of Kul Tepe (E 45° 39′ 43″ - N 38° 50′ 19″, 967 m asl), as shown in Fig. 1, is located near the city of Jolfa (Hadishahr).It is a multi-period Tepe and covers an area of c. 6 ha with a preserved height up to c. 19 m.It is one of the larger prehistoric sites of the region, lying in a strategic position in the middle Araxes valley.It is located next to a broad valley at the crossroads of major routes linking the Iranian plateau to Anatolia, the Southern Caucasus and northern Mesopotamia.Based on cultural materials recovered during the first and second excavation seasons and according to radiocarbon dating, the following cultural phases at Kul Tepe are present: the Late Neolithic/Early Chalcolithic (Dalma), Late Chalcolithic (Pisdeli = LC1; LC 2 and 3 = Chaff-faced Ware), Proto-Kura-Araxes and Kura-Araxes I, Early, Middle, Late Bronze Age, Iron III, Urartian and Achaemenid periods (Abedi et al., 2014) (Table 1).
Excavations at the site of Kul Tepe in the Jolfa region in north-western Iran have unearthed various archaeological materials from Late Neolithic/Early Chalcolithic to Achaemenid periods (end of 6th millennium to 3rd century BC). During the Chalcolithic and the Bronze Age most lithic tools used in Kul Tepe were made of obsidian. From the first and second excavation seasons, 53 and 32 obsidian samples were selected and analyzed by pXRF. According to the results, the main source of obsidian for the workshops in Kul Tepe was Syunik, but other sources in the Lake Sevan Basin like Ghegam, Bazenk, Choraphor and Gutansar and the Lake Van region (Nemrut Dağ and Meydan Dağ) were utilized also. Almost all the lithic industry of Kul Tepe utilized obsidian, although there are a few rare flint and chert pieces.Obsidian was brought to Kul Tepe in the form of nodules and pebbles for processing locally, as suggested by numerous waste and core fragments, indicating various steps of the chaîne opératoire.Many different types of tools are present, including flakes, blades, scrapers, borers and points.Twenty sickle blades, displaying gloss on one edge, are present.Utilized flakes and blades as well side-scrapers and sickle blades appear in high frequency as mentioned in Table 2.
The lack of consensus surrounding the macroscopic determination of high-quality black flint discovered at the Aldenhoven Plateau sites (Rhineland, North-Western Germany) from the beginning of the Middle Neolithic has far-reaching consequences for the anthropological understanding of the socio-cultural dynamics involved in the neolithization of North-Western Europe. This flint has been assigned to Western Belgian 'Obourg' flint type and is used as a key indicator of strong links between populations from West Belgium (Mons Basin) and the German Rhineland at the beginning of the 5th millennium BC. Here, we present an integrated study of this flint using geochemical and lithic technological approaches. This work rules out attribution of the analysed flint artefacts to the Upper Cretaceous flint sources of the Mons Basin; however, the exact origin of the black flint used in the Rhineland remains unanswered. Our results do not support the hypothesis of intensive contact between populations from West Belgium and the German Rhineland and highlights the urgent need for further combined petrographic and geochemical analyses in the region, particularly on geological samples, in order to build up an extensive and reliable comparative reference collection. The Neolithic developed in temperate Europe with the Linear Pottery Culture (hereafter, LPC, second half of 6th millennium) and spread from Transdanubia (Hungary) to the Paris Basin (France).In Central-Western Europe, it was characterized by substantial uniformity in architecture, ceramic styles, and funerary practices, to name only a few.At the turn of the sixth and fifth millennium BC, a historical turning point occurred in continental neolithization of Europe as the Linear Pottery culture broke up into a mosaic of cultural entities.The northern half of France and Belgium were occupied by the Blicquy/Villeneuve-Saint-Germain culture (BQY/VSG).The BQY group was initially discovered in Belgium (two areas, Hainaut in western Belgium and Hesbaye in eastern Belgium) while VSG group was recognized in France; however, their chrono-cultural characteristics, strongly inherited from LPC, are quite similar (Constantin, 1985).In west Germany, the first post-LPC occupations are linked to the emergence of the Hinkelstein culture on the middle and upper Rhine regions which was followed by the Grossgartach then Planig-Friedberg and Rössen cultures (see Fig. 1).The Aldenhoven Plateau, in the north-western studied part of Germany, hasn't delivered sites from the beginning of this Middle Neolithic sequence (German chronology).These Middle Neolithic Rhineland cultural entities are partially contemporaneous from the Blicquy/Villeneuve-Saint-Germain culture (e. g. Constantin and Ilett, 1998; Gehlen and Schön, 2007; Nowak, 2013; Zimmermann et al., 2006) and are approximately dated from 4950/4900 to 4750/4650 BCE.
The lack of consensus surrounding the macroscopic determination of high-quality black flint discovered at the Aldenhoven Plateau sites (Rhineland, North-Western Germany) from the beginning of the Middle Neolithic has far-reaching consequences for the anthropological understanding of the socio-cultural dynamics involved in the neolithization of North-Western Europe. This flint has been assigned to Western Belgian 'Obourg' flint type and is used as a key indicator of strong links between populations from West Belgium (Mons Basin) and the German Rhineland at the beginning of the 5th millennium BC. Here, we present an integrated study of this flint using geochemical and lithic technological approaches. This work rules out attribution of the analysed flint artefacts to the Upper Cretaceous flint sources of the Mons Basin; however, the exact origin of the black flint used in the Rhineland remains unanswered. Our results do not support the hypothesis of intensive contact between populations from West Belgium and the German Rhineland and highlights the urgent need for further combined petrographic and geochemical analyses in the region, particularly on geological samples, in order to build up an extensive and reliable comparative reference collection. Here, we focus on the north part of this area (see boxed region in Fig. 1) as three main sources of Cretaceous flint are located in this region (Cahen et al., 1986; De Grooth, 2011; Robaszynski, 1995).The Mons Basin (West Belgium) is located 15–20 km south of the BQY sites of West Belgium and has produced a large amount of diverse raw material associated with different Cretaceous layers.Ghlin flint (unknown layer) is the most preferred raw material exploited by the Early Neolithic populations of West Belgium (Allard, 2005; Cahen et al., 1986; Denis, 2017).Alternative flint outcrops of the region include a variety of flint-bearing Upper Cretaceous Campanian formations including Obourg, Nouvelles and Spiennes flint (Moreau et al., 2016).The Neolithic populations of the Aldenhoven Plateau largely utilized flint sources located in the Limbourg and surrounded regions (Netherlands and its borders with Belgium and Germany), with the two most exploited raw materials being the Rijckholt and the Rullen flints of Upper Cretaceous Maastrichtian age (Gulpen formation, Lanaye Member) (De Grooth, 2011).An additional potential flint procurement area is the Lower Rhineland region around Düsseldorf which corresponds to the southernmost extension of erratic Baltic flint (Floss, 1994).The quality of the different flints can be evaluated from the point of view of its clastic properties: presence and frequency of inclusions, homogeneity and delicacy of the grain.
The lack of consensus surrounding the macroscopic determination of high-quality black flint discovered at the Aldenhoven Plateau sites (Rhineland, North-Western Germany) from the beginning of the Middle Neolithic has far-reaching consequences for the anthropological understanding of the socio-cultural dynamics involved in the neolithization of North-Western Europe. This flint has been assigned to Western Belgian 'Obourg' flint type and is used as a key indicator of strong links between populations from West Belgium (Mons Basin) and the German Rhineland at the beginning of the 5th millennium BC. Here, we present an integrated study of this flint using geochemical and lithic technological approaches. This work rules out attribution of the analysed flint artefacts to the Upper Cretaceous flint sources of the Mons Basin; however, the exact origin of the black flint used in the Rhineland remains unanswered. Our results do not support the hypothesis of intensive contact between populations from West Belgium and the German Rhineland and highlights the urgent need for further combined petrographic and geochemical analyses in the region, particularly on geological samples, in order to build up an extensive and reliable comparative reference collection. In the area concerned here, the Linear Pottery occupation area extended from the Belgian Hesbaye, through Dutch Limburg, to the regions of the German Lower Rhine (Fig. 1).Diffusion networks of lithic raw materials suggest the emergence of a boundary between two regions.In fact, circulation network curves of Campanian flint from Hesbaye and Rijckholt flint show a reciprocal collapse phenomenon as the proportion of fine-grained flint from Hesbaye (Campanian) decreases between 10 and 35 km from the outcrops while Rijkholt flint increases (Allard, 2005; Zimmermann, 1995) (Fig. 2).
The lack of consensus surrounding the macroscopic determination of high-quality black flint discovered at the Aldenhoven Plateau sites (Rhineland, North-Western Germany) from the beginning of the Middle Neolithic has far-reaching consequences for the anthropological understanding of the socio-cultural dynamics involved in the neolithization of North-Western Europe. This flint has been assigned to Western Belgian 'Obourg' flint type and is used as a key indicator of strong links between populations from West Belgium (Mons Basin) and the German Rhineland at the beginning of the 5th millennium BC. Here, we present an integrated study of this flint using geochemical and lithic technological approaches. This work rules out attribution of the analysed flint artefacts to the Upper Cretaceous flint sources of the Mons Basin; however, the exact origin of the black flint used in the Rhineland remains unanswered. Our results do not support the hypothesis of intensive contact between populations from West Belgium and the German Rhineland and highlights the urgent need for further combined petrographic and geochemical analyses in the region, particularly on geological samples, in order to build up an extensive and reliable comparative reference collection. At the end of the Linear Pottery culture, marked by a breakdown of these large diffusion networks (Allard and Denis, 2015), this boundary asserts itself and becomes the geographical and cultural border between BQY/VSG populations in Belgium and Grossgartach/Planig-Friedberg populations of Aldenhoven Plateau (Fig. 1).In addition, the end of the Linear Pottery Culture is accompanied by a drastic demographic decline in the Rhineland region (Balkowski, 2017; Zimmermann et al., 2006; Zimmermann, 2009).The first sites linked to the repopulation of this geographic area are attributed to people representing the middle Grossgartach stage after a probable hiatus in the occupation.The geographic origin of which is located further south, in the regions of the Upper Rhine in southwestern Germany (Fig. 1).This raises the question of a single origin of the first post-Linear Pottery culture populations in the German Rhineland.Indeed, several elements indicate the existence of contacts between populations of Aldenhoven Plateau and Blicquian ones further to the West in the form of Blicquian shards discovered at two sites (Spatz, 1991).Moreover, study of the post-LPC lithic assemblages at these sites highlight the presence of Belgian Campanien Hesbaye flint and several diagnostic BQY/VSG tools (Denis, in press; Gehlen and Schön, 2007, 2009).The alleged presence of Obourg black flint at some post-LPC sites of Aldenhoven area has also been used to support the hypothesis of intensive relationships with populations from Belgium, more precisely from West Belgium (Gehlen and Schön, 2007, 2009).Despite the fact that populations from this area themselves did not use this raw material, apart from two pieces discovered in habitat unit 10 at the site of Irchonwelz “la Bonne Fortune” (Denis, 2017).
In this work we propose a minimax probability extreme learning machine framework (MPME), which combines the benefits of minimax probability machine (MPM) with extreme learning machine (ELM). For binary classification problems, we illustrate that the proposed MPME can be interpreted geometrically by minimizing the maximum of Mahalanobis distances to the two classes. Then two variants of the MPME are presented based on the l2-norm loss and l1-norm loss functions (called LSEMPME and LADMPME) respectively. Without making specific assumption on the data distribution, the proposed methods can provide explicit upper-bounds for the generalization error, moreover the LSEMPME and LADMPME minimize empirical risk simultaneously. The decision hyperplanes of the proposed methods pass through the origin in ELM feature space with few decision variables. By using the multivariate Chebyshev–Cantelli inequality, all the proposed problems can be reformulated as second-order cone programming (SOCP) with global solutions. Furthermore, numerical experiments have been carried out on two databases that are drawn from UCI benchmark database and a practical application database. First, the proposed methods are evaluated for a practical application consisting on the analysis of licorice seeds using near-infrared spectral (NIR) data. Experiments in six different spectral regions illustrate that the proposed methods can improve generalization in most cases. Then the proposed methods are evaluated on benchmark datasets. In comparison with traditional methods including MPM, ELM and support vector machine (SVM), experiments show that the proposed methods achieve comparable results in generalization. With few decision variables, the proposed methods are easy to implement for nonlinear classification and to estimate a lower-bound on the prediction accuracy. Machine learning algorithms have been widely used in various fields such as data mining and pattern recognition, where support vector machine (SVM) (Vapnik, 1998; Frenay and Verleysen, 2010; Shawe-Taylor and Sun, 2011; Liu et al., 2012; Yang and Dong, 2018), neural network algorithms (Li et al., 2015; Kolbaek et al., 2017), ensemble learning (Rojarath et al., 2017), deep learning (Deng and Yu, 2014) and logistic regression (Yang and Qian, 2016) have been successfully used in classification, regression, function approximation, feature selection, feature extraction and so on.
In this work we propose a minimax probability extreme learning machine framework (MPME), which combines the benefits of minimax probability machine (MPM) with extreme learning machine (ELM). For binary classification problems, we illustrate that the proposed MPME can be interpreted geometrically by minimizing the maximum of Mahalanobis distances to the two classes. Then two variants of the MPME are presented based on the l2-norm loss and l1-norm loss functions (called LSEMPME and LADMPME) respectively. Without making specific assumption on the data distribution, the proposed methods can provide explicit upper-bounds for the generalization error, moreover the LSEMPME and LADMPME minimize empirical risk simultaneously. The decision hyperplanes of the proposed methods pass through the origin in ELM feature space with few decision variables. By using the multivariate Chebyshev–Cantelli inequality, all the proposed problems can be reformulated as second-order cone programming (SOCP) with global solutions. Furthermore, numerical experiments have been carried out on two databases that are drawn from UCI benchmark database and a practical application database. First, the proposed methods are evaluated for a practical application consisting on the analysis of licorice seeds using near-infrared spectral (NIR) data. Experiments in six different spectral regions illustrate that the proposed methods can improve generalization in most cases. Then the proposed methods are evaluated on benchmark datasets. In comparison with traditional methods including MPM, ELM and support vector machine (SVM), experiments show that the proposed methods achieve comparable results in generalization. With few decision variables, the proposed methods are easy to implement for nonlinear classification and to estimate a lower-bound on the prediction accuracy. Extreme learning machine (ELM) (Huang et al., 2006, 2010, 2012, 2015; Barreto and Barros, 2016) is an important learning algorithm for single-hidden-layer feedforward neural networks (SLFNs) (Rumelhart et al., 1986).The traditional methods used in SLFNs are based on the back-propagation (BP) algorithm (Rumelhart et al., 1986), which adopts the gradient descent method to optimize the weights in neural networks.However, the gradient based method cannot guarantee the global optima and is often time-consuming in weight tuning.Compared with the traditional neural networks, the main advantages of the ELM are that it has a simple structure and is solved quickly.Its hidden nodes and input weights are randomly initialized and the output weights are determined by minimizing the least squares error (Huynh and Won, 2008).Moreover, the ELM overcomes some drawbacks of traditional feedforward neural networks, such as local minima, imprecise learning rates and slow convergence rates (Huang et al., 2006).In addition, ELM can provide a unified framework for binary classification, multiclass classification and regression problems (Huang et al., 2012).The studies show that ELM can achieve comparable or even better generalization than SVM (Liu et al., 2012).Different from the popular SVM that is difficult to optimize when dealing with nonlinear problems because of the unknown implication mapping and the kernel parameters, ELM has the explicit kernel function form: KELM(xi,xj)=ϕ(xi)Tϕ(xj), and its network parameters are randomly generated without tuning.Basically ELM uses less parameters than SVM because the bias parameter is not necessary.Therefore, ELM is more convenient to use in practical applications (Huang et al., 2010; Chorowski et al., 2014).With these advances of ELM, much effort has been made to improve ELM performance from both theoretical and applicative perspectives, such as the optimization method based ELM (Huang et al., 2010), robust ELM (Barreto and Barros, 2016), semi-supervised ELM (Zhou et al., 2015), weighted least squares ELM (Huynh and Won, 2008) and so on.It is worth noting that the statistic information from samples is relatively few applied in ELM researches currently.
In this work we propose a minimax probability extreme learning machine framework (MPME), which combines the benefits of minimax probability machine (MPM) with extreme learning machine (ELM). For binary classification problems, we illustrate that the proposed MPME can be interpreted geometrically by minimizing the maximum of Mahalanobis distances to the two classes. Then two variants of the MPME are presented based on the l2-norm loss and l1-norm loss functions (called LSEMPME and LADMPME) respectively. Without making specific assumption on the data distribution, the proposed methods can provide explicit upper-bounds for the generalization error, moreover the LSEMPME and LADMPME minimize empirical risk simultaneously. The decision hyperplanes of the proposed methods pass through the origin in ELM feature space with few decision variables. By using the multivariate Chebyshev–Cantelli inequality, all the proposed problems can be reformulated as second-order cone programming (SOCP) with global solutions. Furthermore, numerical experiments have been carried out on two databases that are drawn from UCI benchmark database and a practical application database. First, the proposed methods are evaluated for a practical application consisting on the analysis of licorice seeds using near-infrared spectral (NIR) data. Experiments in six different spectral regions illustrate that the proposed methods can improve generalization in most cases. Then the proposed methods are evaluated on benchmark datasets. In comparison with traditional methods including MPM, ELM and support vector machine (SVM), experiments show that the proposed methods achieve comparable results in generalization. With few decision variables, the proposed methods are easy to implement for nonlinear classification and to estimate a lower-bound on the prediction accuracy. When constructing a classifier, the probability of correct classification of future data should be maximized.Minimax probability machine (MPM) (Lanckriet et al., 2002a, b; Yoshiyama and Sakurai, 2014) is a novel classification algorithm based on the prior knowledge and has been successfully applied in classification and regression problems.It has advantages over other machine learning methods:
In this work we propose a minimax probability extreme learning machine framework (MPME), which combines the benefits of minimax probability machine (MPM) with extreme learning machine (ELM). For binary classification problems, we illustrate that the proposed MPME can be interpreted geometrically by minimizing the maximum of Mahalanobis distances to the two classes. Then two variants of the MPME are presented based on the l2-norm loss and l1-norm loss functions (called LSEMPME and LADMPME) respectively. Without making specific assumption on the data distribution, the proposed methods can provide explicit upper-bounds for the generalization error, moreover the LSEMPME and LADMPME minimize empirical risk simultaneously. The decision hyperplanes of the proposed methods pass through the origin in ELM feature space with few decision variables. By using the multivariate Chebyshev–Cantelli inequality, all the proposed problems can be reformulated as second-order cone programming (SOCP) with global solutions. Furthermore, numerical experiments have been carried out on two databases that are drawn from UCI benchmark database and a practical application database. First, the proposed methods are evaluated for a practical application consisting on the analysis of licorice seeds using near-infrared spectral (NIR) data. Experiments in six different spectral regions illustrate that the proposed methods can improve generalization in most cases. Then the proposed methods are evaluated on benchmark datasets. In comparison with traditional methods including MPM, ELM and support vector machine (SVM), experiments show that the proposed methods achieve comparable results in generalization. With few decision variables, the proposed methods are easy to implement for nonlinear classification and to estimate a lower-bound on the prediction accuracy. (1) MPM is a moment-based constrains algorithm (or called a nonparametric algorithm).It utilizes all the information from the samples, mean and variance, to find a minimax probabilistic decision hyperplane for separating the two-class samples for binary classifications.
In this work we propose a minimax probability extreme learning machine framework (MPME), which combines the benefits of minimax probability machine (MPM) with extreme learning machine (ELM). For binary classification problems, we illustrate that the proposed MPME can be interpreted geometrically by minimizing the maximum of Mahalanobis distances to the two classes. Then two variants of the MPME are presented based on the l2-norm loss and l1-norm loss functions (called LSEMPME and LADMPME) respectively. Without making specific assumption on the data distribution, the proposed methods can provide explicit upper-bounds for the generalization error, moreover the LSEMPME and LADMPME minimize empirical risk simultaneously. The decision hyperplanes of the proposed methods pass through the origin in ELM feature space with few decision variables. By using the multivariate Chebyshev–Cantelli inequality, all the proposed problems can be reformulated as second-order cone programming (SOCP) with global solutions. Furthermore, numerical experiments have been carried out on two databases that are drawn from UCI benchmark database and a practical application database. First, the proposed methods are evaluated for a practical application consisting on the analysis of licorice seeds using near-infrared spectral (NIR) data. Experiments in six different spectral regions illustrate that the proposed methods can improve generalization in most cases. Then the proposed methods are evaluated on benchmark datasets. In comparison with traditional methods including MPM, ELM and support vector machine (SVM), experiments show that the proposed methods achieve comparable results in generalization. With few decision variables, the proposed methods are easy to implement for nonlinear classification and to estimate a lower-bound on the prediction accuracy. (2) Making no assumption on the data distribution, MPM can directly estimate a probabilistic accuracy bound by minimizing the maximum probability of misclassification error.
In this work we propose a minimax probability extreme learning machine framework (MPME), which combines the benefits of minimax probability machine (MPM) with extreme learning machine (ELM). For binary classification problems, we illustrate that the proposed MPME can be interpreted geometrically by minimizing the maximum of Mahalanobis distances to the two classes. Then two variants of the MPME are presented based on the l2-norm loss and l1-norm loss functions (called LSEMPME and LADMPME) respectively. Without making specific assumption on the data distribution, the proposed methods can provide explicit upper-bounds for the generalization error, moreover the LSEMPME and LADMPME minimize empirical risk simultaneously. The decision hyperplanes of the proposed methods pass through the origin in ELM feature space with few decision variables. By using the multivariate Chebyshev–Cantelli inequality, all the proposed problems can be reformulated as second-order cone programming (SOCP) with global solutions. Furthermore, numerical experiments have been carried out on two databases that are drawn from UCI benchmark database and a practical application database. First, the proposed methods are evaluated for a practical application consisting on the analysis of licorice seeds using near-infrared spectral (NIR) data. Experiments in six different spectral regions illustrate that the proposed methods can improve generalization in most cases. Then the proposed methods are evaluated on benchmark datasets. In comparison with traditional methods including MPM, ELM and support vector machine (SVM), experiments show that the proposed methods achieve comparable results in generalization. With few decision variables, the proposed methods are easy to implement for nonlinear classification and to estimate a lower-bound on the prediction accuracy. (3) MPM formulation is reformulated as a second-order cone programming (SOCP) (Lobo et al., 1998; Bhattacharyya, 2004) with global optimal solution.
In this work we propose a minimax probability extreme learning machine framework (MPME), which combines the benefits of minimax probability machine (MPM) with extreme learning machine (ELM). For binary classification problems, we illustrate that the proposed MPME can be interpreted geometrically by minimizing the maximum of Mahalanobis distances to the two classes. Then two variants of the MPME are presented based on the l2-norm loss and l1-norm loss functions (called LSEMPME and LADMPME) respectively. Without making specific assumption on the data distribution, the proposed methods can provide explicit upper-bounds for the generalization error, moreover the LSEMPME and LADMPME minimize empirical risk simultaneously. The decision hyperplanes of the proposed methods pass through the origin in ELM feature space with few decision variables. By using the multivariate Chebyshev–Cantelli inequality, all the proposed problems can be reformulated as second-order cone programming (SOCP) with global solutions. Furthermore, numerical experiments have been carried out on two databases that are drawn from UCI benchmark database and a practical application database. First, the proposed methods are evaluated for a practical application consisting on the analysis of licorice seeds using near-infrared spectral (NIR) data. Experiments in six different spectral regions illustrate that the proposed methods can improve generalization in most cases. Then the proposed methods are evaluated on benchmark datasets. In comparison with traditional methods including MPM, ELM and support vector machine (SVM), experiments show that the proposed methods achieve comparable results in generalization. With few decision variables, the proposed methods are easy to implement for nonlinear classification and to estimate a lower-bound on the prediction accuracy. Numerical experiments show that the performance of MPM is comparable with SVM (Lanckriet et al., 2002a), but MPM can provide an explicit lower-bound on prediction accuracy.At the same time, we note that MPM paradigm does not explicitly control the empirical risk.
In this work we propose a minimax probability extreme learning machine framework (MPME), which combines the benefits of minimax probability machine (MPM) with extreme learning machine (ELM). For binary classification problems, we illustrate that the proposed MPME can be interpreted geometrically by minimizing the maximum of Mahalanobis distances to the two classes. Then two variants of the MPME are presented based on the l2-norm loss and l1-norm loss functions (called LSEMPME and LADMPME) respectively. Without making specific assumption on the data distribution, the proposed methods can provide explicit upper-bounds for the generalization error, moreover the LSEMPME and LADMPME minimize empirical risk simultaneously. The decision hyperplanes of the proposed methods pass through the origin in ELM feature space with few decision variables. By using the multivariate Chebyshev–Cantelli inequality, all the proposed problems can be reformulated as second-order cone programming (SOCP) with global solutions. Furthermore, numerical experiments have been carried out on two databases that are drawn from UCI benchmark database and a practical application database. First, the proposed methods are evaluated for a practical application consisting on the analysis of licorice seeds using near-infrared spectral (NIR) data. Experiments in six different spectral regions illustrate that the proposed methods can improve generalization in most cases. Then the proposed methods are evaluated on benchmark datasets. In comparison with traditional methods including MPM, ELM and support vector machine (SVM), experiments show that the proposed methods achieve comparable results in generalization. With few decision variables, the proposed methods are easy to implement for nonlinear classification and to estimate a lower-bound on the prediction accuracy. The second-order cone programming (SOCP) is a nonlinear convex optimization, which can be generally expressed as minxfTxs.t.‖Aix+bi‖≤ciTx+di,i=1,2,…,M with f∈Rn,Ai∈R(ni−1)×n,bi∈Rni−1,ci∈Rn and di∈R.The norm appearing in the constraints is the standard Euclidean norm.The constraints ‖Aix+bi‖≤ciTx+di (i=1,2⋯,M) are called as second-order cone constraints of dimension ni.The SOCP with global solution has been used successfully in machine learning and pattern recognition.
In this work we propose a minimax probability extreme learning machine framework (MPME), which combines the benefits of minimax probability machine (MPM) with extreme learning machine (ELM). For binary classification problems, we illustrate that the proposed MPME can be interpreted geometrically by minimizing the maximum of Mahalanobis distances to the two classes. Then two variants of the MPME are presented based on the l2-norm loss and l1-norm loss functions (called LSEMPME and LADMPME) respectively. Without making specific assumption on the data distribution, the proposed methods can provide explicit upper-bounds for the generalization error, moreover the LSEMPME and LADMPME minimize empirical risk simultaneously. The decision hyperplanes of the proposed methods pass through the origin in ELM feature space with few decision variables. By using the multivariate Chebyshev–Cantelli inequality, all the proposed problems can be reformulated as second-order cone programming (SOCP) with global solutions. Furthermore, numerical experiments have been carried out on two databases that are drawn from UCI benchmark database and a practical application database. First, the proposed methods are evaluated for a practical application consisting on the analysis of licorice seeds using near-infrared spectral (NIR) data. Experiments in six different spectral regions illustrate that the proposed methods can improve generalization in most cases. Then the proposed methods are evaluated on benchmark datasets. In comparison with traditional methods including MPM, ELM and support vector machine (SVM), experiments show that the proposed methods achieve comparable results in generalization. With few decision variables, the proposed methods are easy to implement for nonlinear classification and to estimate a lower-bound on the prediction accuracy. When constructing a classifier, providing a reliability measure of prediction accuracy is very useful especially for ELM classification since its hidden-layer parameters are randomly generated, which may lead to the instability of ELM outputs.Therefore, it is meaningful and reasonable to construct a decision hyperplane with maximal probability to separate two-class samples in ELM feature space, which can also enable post-processing in ELM applications.However, the classical ELM does not provide such a probability.For a sample x, the output of the ELM is a score f(x), instead of a measure of the misclassification probability.
In this work we propose a minimax probability extreme learning machine framework (MPME), which combines the benefits of minimax probability machine (MPM) with extreme learning machine (ELM). For binary classification problems, we illustrate that the proposed MPME can be interpreted geometrically by minimizing the maximum of Mahalanobis distances to the two classes. Then two variants of the MPME are presented based on the l2-norm loss and l1-norm loss functions (called LSEMPME and LADMPME) respectively. Without making specific assumption on the data distribution, the proposed methods can provide explicit upper-bounds for the generalization error, moreover the LSEMPME and LADMPME minimize empirical risk simultaneously. The decision hyperplanes of the proposed methods pass through the origin in ELM feature space with few decision variables. By using the multivariate Chebyshev–Cantelli inequality, all the proposed problems can be reformulated as second-order cone programming (SOCP) with global solutions. Furthermore, numerical experiments have been carried out on two databases that are drawn from UCI benchmark database and a practical application database. First, the proposed methods are evaluated for a practical application consisting on the analysis of licorice seeds using near-infrared spectral (NIR) data. Experiments in six different spectral regions illustrate that the proposed methods can improve generalization in most cases. Then the proposed methods are evaluated on benchmark datasets. In comparison with traditional methods including MPM, ELM and support vector machine (SVM), experiments show that the proposed methods achieve comparable results in generalization. With few decision variables, the proposed methods are easy to implement for nonlinear classification and to estimate a lower-bound on the prediction accuracy. A loss function is a means by which the performance of a learning algorithm is judged.For x∈Rn, least square loss (LS-loss) (or l2-norm loss, l2(x)=∑i=1nxi2) (Huynh and Won, 2008) and least absolute deviation (LAD) loss (or l1-norm loss, l1(x)=∑i=1n|xi|) (Cao and Liu, 2009; Yang et al., 2011; Xiang et al., 2012) are two popular loss functions.They are all convex function and have been successfully used in classification and regression problems.
In this work we propose a minimax probability extreme learning machine framework (MPME), which combines the benefits of minimax probability machine (MPM) with extreme learning machine (ELM). For binary classification problems, we illustrate that the proposed MPME can be interpreted geometrically by minimizing the maximum of Mahalanobis distances to the two classes. Then two variants of the MPME are presented based on the l2-norm loss and l1-norm loss functions (called LSEMPME and LADMPME) respectively. Without making specific assumption on the data distribution, the proposed methods can provide explicit upper-bounds for the generalization error, moreover the LSEMPME and LADMPME minimize empirical risk simultaneously. The decision hyperplanes of the proposed methods pass through the origin in ELM feature space with few decision variables. By using the multivariate Chebyshev–Cantelli inequality, all the proposed problems can be reformulated as second-order cone programming (SOCP) with global solutions. Furthermore, numerical experiments have been carried out on two databases that are drawn from UCI benchmark database and a practical application database. First, the proposed methods are evaluated for a practical application consisting on the analysis of licorice seeds using near-infrared spectral (NIR) data. Experiments in six different spectral regions illustrate that the proposed methods can improve generalization in most cases. Then the proposed methods are evaluated on benchmark datasets. In comparison with traditional methods including MPM, ELM and support vector machine (SVM), experiments show that the proposed methods achieve comparable results in generalization. With few decision variables, the proposed methods are easy to implement for nonlinear classification and to estimate a lower-bound on the prediction accuracy. Inspired by the above researches, in this investigation, we propose a new classification framework which inherits the advantages of MPM and ELM.The main contributions of this work are summarized as follows:
In this work we propose a minimax probability extreme learning machine framework (MPME), which combines the benefits of minimax probability machine (MPM) with extreme learning machine (ELM). For binary classification problems, we illustrate that the proposed MPME can be interpreted geometrically by minimizing the maximum of Mahalanobis distances to the two classes. Then two variants of the MPME are presented based on the l2-norm loss and l1-norm loss functions (called LSEMPME and LADMPME) respectively. Without making specific assumption on the data distribution, the proposed methods can provide explicit upper-bounds for the generalization error, moreover the LSEMPME and LADMPME minimize empirical risk simultaneously. The decision hyperplanes of the proposed methods pass through the origin in ELM feature space with few decision variables. By using the multivariate Chebyshev–Cantelli inequality, all the proposed problems can be reformulated as second-order cone programming (SOCP) with global solutions. Furthermore, numerical experiments have been carried out on two databases that are drawn from UCI benchmark database and a practical application database. First, the proposed methods are evaluated for a practical application consisting on the analysis of licorice seeds using near-infrared spectral (NIR) data. Experiments in six different spectral regions illustrate that the proposed methods can improve generalization in most cases. Then the proposed methods are evaluated on benchmark datasets. In comparison with traditional methods including MPM, ELM and support vector machine (SVM), experiments show that the proposed methods achieve comparable results in generalization. With few decision variables, the proposed methods are easy to implement for nonlinear classification and to estimate a lower-bound on the prediction accuracy. (1) We propose a minimax probability extreme learning machine (called MPME) that combines the benefits of ELM and MPM.Without making specific assumption on data distribution, the proposed MPME can provide an explicit lower-bound on the probability of correct classification for future data.For binary classification problems, the proposed MPME can be geometrically interpreted by minimizing the maximum of the Mahalanobis distances to the two classes in ELM feature space.
In this work we propose a minimax probability extreme learning machine framework (MPME), which combines the benefits of minimax probability machine (MPM) with extreme learning machine (ELM). For binary classification problems, we illustrate that the proposed MPME can be interpreted geometrically by minimizing the maximum of Mahalanobis distances to the two classes. Then two variants of the MPME are presented based on the l2-norm loss and l1-norm loss functions (called LSEMPME and LADMPME) respectively. Without making specific assumption on the data distribution, the proposed methods can provide explicit upper-bounds for the generalization error, moreover the LSEMPME and LADMPME minimize empirical risk simultaneously. The decision hyperplanes of the proposed methods pass through the origin in ELM feature space with few decision variables. By using the multivariate Chebyshev–Cantelli inequality, all the proposed problems can be reformulated as second-order cone programming (SOCP) with global solutions. Furthermore, numerical experiments have been carried out on two databases that are drawn from UCI benchmark database and a practical application database. First, the proposed methods are evaluated for a practical application consisting on the analysis of licorice seeds using near-infrared spectral (NIR) data. Experiments in six different spectral regions illustrate that the proposed methods can improve generalization in most cases. Then the proposed methods are evaluated on benchmark datasets. In comparison with traditional methods including MPM, ELM and support vector machine (SVM), experiments show that the proposed methods achieve comparable results in generalization. With few decision variables, the proposed methods are easy to implement for nonlinear classification and to estimate a lower-bound on the prediction accuracy. (2) Two variants of the proposed MPME are developed based on the least square loss (LS-loss) and least absolute deviation (LAD) loss functions, called LSEMPME and LADMPME respectively.They can minimize the maximal probability of misclassification and empirical risk simultaneously.With explicit kernel function form, the proposed methods are easy to implement for nonlinear decision.Moreover their separation hyperplanes pass through the origin of the ELM feature space with few decision variables.
In this work we propose a minimax probability extreme learning machine framework (MPME), which combines the benefits of minimax probability machine (MPM) with extreme learning machine (ELM). For binary classification problems, we illustrate that the proposed MPME can be interpreted geometrically by minimizing the maximum of Mahalanobis distances to the two classes. Then two variants of the MPME are presented based on the l2-norm loss and l1-norm loss functions (called LSEMPME and LADMPME) respectively. Without making specific assumption on the data distribution, the proposed methods can provide explicit upper-bounds for the generalization error, moreover the LSEMPME and LADMPME minimize empirical risk simultaneously. The decision hyperplanes of the proposed methods pass through the origin in ELM feature space with few decision variables. By using the multivariate Chebyshev–Cantelli inequality, all the proposed problems can be reformulated as second-order cone programming (SOCP) with global solutions. Furthermore, numerical experiments have been carried out on two databases that are drawn from UCI benchmark database and a practical application database. First, the proposed methods are evaluated for a practical application consisting on the analysis of licorice seeds using near-infrared spectral (NIR) data. Experiments in six different spectral regions illustrate that the proposed methods can improve generalization in most cases. Then the proposed methods are evaluated on benchmark datasets. In comparison with traditional methods including MPM, ELM and support vector machine (SVM), experiments show that the proposed methods achieve comparable results in generalization. With few decision variables, the proposed methods are easy to implement for nonlinear classification and to estimate a lower-bound on the prediction accuracy. By using the multivariate Chebyshev–Cantelli inequality (Marshall and Olkin, 1960), all the proposed models are reformulated as second-order cone programming (SOCP) with global solution, and solved efficiently using the interior point algorithm (Lobo et al., 1998).As a typical and important application, the proposed methods are used to evaluate a practical classification problem consisting on the analysis of the hardness of licorice seeds employing near-infrared (NIR) spectroscopy data (Yang and Sun, 2016; Yang and Dong, 2018).Experimental results in different spectral regions demonstrate that the proposed methods improve generalization in most cases, and it can provide the upper-bound on misclassification error rate and control empirical error simultaneously.
In this work we propose a minimax probability extreme learning machine framework (MPME), which combines the benefits of minimax probability machine (MPM) with extreme learning machine (ELM). For binary classification problems, we illustrate that the proposed MPME can be interpreted geometrically by minimizing the maximum of Mahalanobis distances to the two classes. Then two variants of the MPME are presented based on the l2-norm loss and l1-norm loss functions (called LSEMPME and LADMPME) respectively. Without making specific assumption on the data distribution, the proposed methods can provide explicit upper-bounds for the generalization error, moreover the LSEMPME and LADMPME minimize empirical risk simultaneously. The decision hyperplanes of the proposed methods pass through the origin in ELM feature space with few decision variables. By using the multivariate Chebyshev–Cantelli inequality, all the proposed problems can be reformulated as second-order cone programming (SOCP) with global solutions. Furthermore, numerical experiments have been carried out on two databases that are drawn from UCI benchmark database and a practical application database. First, the proposed methods are evaluated for a practical application consisting on the analysis of licorice seeds using near-infrared spectral (NIR) data. Experiments in six different spectral regions illustrate that the proposed methods can improve generalization in most cases. Then the proposed methods are evaluated on benchmark datasets. In comparison with traditional methods including MPM, ELM and support vector machine (SVM), experiments show that the proposed methods achieve comparable results in generalization. With few decision variables, the proposed methods are easy to implement for nonlinear classification and to estimate a lower-bound on the prediction accuracy. Throughout the paper we adopt the following notations.The scalar product of two vectors x and y in the n-dimensional real space is denoted by xTy or 〈x⋅y〉.For a n-dimension vector x, the l1-norm of x is denoted as ‖x‖1, ‖x‖1=∑i=1n|xi|, where |⋅| denotes absolute value operator.The ‖x‖2 denotes the l2-norm of x, ‖x‖2=xTx.The base of the natural logarithm is denoted by ϵ.A vector of zeros in a real space of arbitrary dimension is denoted by 0.An arbitrary dimension vector of ones is denoted by e.
Temporary sites were a critical component of the prehistoric Near Eastern economy but, because of their ephemeral nature, are less frequently the focus of research than sedentary settlements. The present article presents the results of neutron activation analysis conducted on pottery from the temporary site of Saruq al-Hadid, United Arab Emirates. The results identified both continuity and change in the pottery consumed at the site in the Bronze and Iron Ages, which suggests that the peoples gathering here were integrated into economic practices observed at sedentary sites throughout southeastern Arabia. Pottery is ubiquitous at sites in the ancient Near East by the third millennium BCE.Ceramic vessels are used for the preparation, storage, and consumption of food as well as a variety of less well understood ritual practices.As a result, the study of pottery is an essential tool for illuminating past lifeways (Rice, 1987; Sinopoli, 1991).In particular, the geochemical analysis of pottery has proved to be an effective means of gaining insight into patterns of production and exchange (Speakman and Glascock, 2007).In the archaeology of the Near East, there is a tendency for the compositional analysis of ceramics to focus on pottery recovered from sedentary settlements, best typified by the man-made tells of mudbrick and stone that, following generation after generation of construction, collapse, and rebuilding, have come to loom over the surrounding agricultural plains.However, integral segments of ancient Near Eastern society moved between these sedentary settlements and temporary sites (Bernbeck, 2008; Porter, 2012).Much occurred beyond the sedentary villages, towns, and cities that is often not integrated into reconstructions of social and economic practice because of the difficulties associated with identifying temporary occupations and the allure of the mound.Compositional analysis of pottery from a temporary site has the potential to fill-in this gap and tie the groups that gathered at these locations into the social and economic practices documented at sedentary settlements, creating a more accurate and complete picture of the past.
Temporary sites were a critical component of the prehistoric Near Eastern economy but, because of their ephemeral nature, are less frequently the focus of research than sedentary settlements. The present article presents the results of neutron activation analysis conducted on pottery from the temporary site of Saruq al-Hadid, United Arab Emirates. The results identified both continuity and change in the pottery consumed at the site in the Bronze and Iron Ages, which suggests that the peoples gathering here were integrated into economic practices observed at sedentary sites throughout southeastern Arabia. Saruq al-Hadid (Emirate of Dubai, United Arab Emirates) is such a temporary site located in the northern Rub' al-Khali desert (Fig. 1).Though extensive excavations have unearthed an area of approximately 7000 m2 there is no evidence for permanent architecture, only traces of what were ephemeral buildings (Fig. 2) (Casana et al., 2009; Herrmann et al., 2012; Weeks et al., 2017; Weeks et al., 2018).What is even more remarkable is the fact that such temporary structures appear to have been erected episodically at Saruq al-Hadid for over a thousand years, with radiocarbon dates and stratigraphic evidence pointing to occupation levels from the late third millennium BCE to the early first millennium BCE and then after a hiatus of several centuries later periods of activity in the first and second millennia CE, leading to its designation as a ‘persistent temporary place’ (Weeks et al., 2018).The absence of standing architectural remains contrasts sharply with contemporary settlements both inland and along the coast where mudbrick and stone constructions are common and where occupation is regarded as more continuous (Magee, 2014).
In 2003, the excavation of the ancient site Baochuanchang Shipyard was carried out in Nanjing, China and this shipyard was believed to have been the workshop where the huge vessels of Zheng He's fleet were built and maintained. Several pieces of ancient putty were found here, and a small piece was sampled and analyzed in this paper to study its components and structure. The results of X-ray diffraction (XRD) showed that the putty was mainly composed of calcite (CaCO3), while pyrolysis-gas chromatography–mass spectrometry (Py-GC–MS) analysis indicated that tung oil was used in making the putty. In addition, plant fibers inside the putty were identified as jute by the means of polarizing microscope and scanning electron microscope (SEM) analysis. These findings confirmed the putty as the so-called chu-nam putty, which was a traditional sealing material used in ancient shipbuilding. Moreover, the surface morphology of the putty was obtained by SEM, and pore size distribution was measured by gas adsorption-desorption analysis. The analytical results suggested that the putty was quite compact, which could perform well in sealing huge wooden ships. This research revealed the shipbuilding skills of the Baochuanchang Shipyard. It may provide reference for studying Zheng He's vessels and make further a contribution to the conservation and restoration of ancient wooden ships of the Ming Dynasty. China used to hold the leading position in shipbuilding technology before the industrial revolution (Fang et al., 2013; Li, 2006).The invention of the sealing material with high waterproofing and bonding properties made a great contribution for ancient Chinese craftsmen to build huge ships for oversea voyage.Historical sources describe this material as a mixture of boiled tung oil, lime and oakum (optional) (Fang et al., 2013; Song, 1637).It was called “nianliao” in Chinese while Worcester (1971) called it “chu-nam putty”.
In 2003, the excavation of the ancient site Baochuanchang Shipyard was carried out in Nanjing, China and this shipyard was believed to have been the workshop where the huge vessels of Zheng He's fleet were built and maintained. Several pieces of ancient putty were found here, and a small piece was sampled and analyzed in this paper to study its components and structure. The results of X-ray diffraction (XRD) showed that the putty was mainly composed of calcite (CaCO3), while pyrolysis-gas chromatography–mass spectrometry (Py-GC–MS) analysis indicated that tung oil was used in making the putty. In addition, plant fibers inside the putty were identified as jute by the means of polarizing microscope and scanning electron microscope (SEM) analysis. These findings confirmed the putty as the so-called chu-nam putty, which was a traditional sealing material used in ancient shipbuilding. Moreover, the surface morphology of the putty was obtained by SEM, and pore size distribution was measured by gas adsorption-desorption analysis. The analytical results suggested that the putty was quite compact, which could perform well in sealing huge wooden ships. This research revealed the shipbuilding skills of the Baochuanchang Shipyard. It may provide reference for studying Zheng He's vessels and make further a contribution to the conservation and restoration of ancient wooden ships of the Ming Dynasty. Archaeological research of ancient ships usually focused on the shipbuilding techniques such as general constructions and timber elements (Allevato et al., 2010; Gibbins and Adams, 2001; Howitz, 1977).Just a few scholars (Wright and Churchill, 1965) paid attention to the caulking and sealing materials of ancient wooden ships.Connan and Nissenbaum (2003) identified conifer resin as the protective cover on the ship timbers or the caulking material used on the Ma'agan Mikhael ship (500 BCE).The shipwrecks of the Etruscan and Roman ages found at the ancient harbor of San Rossore, Italy were investigated by Colombini (Colombini et al., 2003).The waterproofing and caulking materials were identified as a pitch obtained from the Pinaceae tree, and the painting materials were revealed as Pinaceae resins together with beeswax.Deforce (Deforce et al., 2014) studied the caulking material from shipwrecks of two 14th century maritime cargo vessels excavated from the port of Antwerp, northern Belgium, which was mainly composed by mosses.However, these caulking materials were far different from the chu-nam putty.Although the ancient lime-based mortars were widely studied in the field of architecture (Fang et al., 2014; Yang et al., 2010; Zeng et al., 2008), the lime putty that excavated from the unearthed shipwrecks were overlooked (Wang, 1983; Yuan, 2006).Very few studies had been reported on the components and characteristics of the lime putty.Fang's team (Fang et al., 2013; Fang et al., 2014) have done a great job on the study of chu-nam putty from a Song Dynasty wooden ship called “Huaguang No.1”.Their research identified the components of the ancient putty as tung oil, lime and jute by the means of FT-IR, XRD, and SEM etc., and further discussed its formula, performance and mechanism.However, there is a lack of direct evidence for proving the use of tung oil due to its rare content.Moreover, the actual formula of the putty of different times or areas might be different, thus, it is worth to be studied comprehensively.
In 2003, the excavation of the ancient site Baochuanchang Shipyard was carried out in Nanjing, China and this shipyard was believed to have been the workshop where the huge vessels of Zheng He's fleet were built and maintained. Several pieces of ancient putty were found here, and a small piece was sampled and analyzed in this paper to study its components and structure. The results of X-ray diffraction (XRD) showed that the putty was mainly composed of calcite (CaCO3), while pyrolysis-gas chromatography–mass spectrometry (Py-GC–MS) analysis indicated that tung oil was used in making the putty. In addition, plant fibers inside the putty were identified as jute by the means of polarizing microscope and scanning electron microscope (SEM) analysis. These findings confirmed the putty as the so-called chu-nam putty, which was a traditional sealing material used in ancient shipbuilding. Moreover, the surface morphology of the putty was obtained by SEM, and pore size distribution was measured by gas adsorption-desorption analysis. The analytical results suggested that the putty was quite compact, which could perform well in sealing huge wooden ships. This research revealed the shipbuilding skills of the Baochuanchang Shipyard. It may provide reference for studying Zheng He's vessels and make further a contribution to the conservation and restoration of ancient wooden ships of the Ming Dynasty. In China, people generally know the chu-nam putty through ancient records (Song, 1637).Besides the famous book “Tian Gong Kai Wu” (named as “Heavenly Creations”, which is the technological encyclopedia of the Ming Dynasty), the detailed records of making and using chu-nam putty could be found in “Nan Chuan Ji” (Records of the Southern Ships) (Shen, 1541) and “Long jiang Chuan Chang Zhi” (Records of the Longjiang Shipyard) (Li, 1553) as well.Unfortunately, due to the policy of forbidding sea voyages in the late Ming Dynasty (1522–1566 CE), almost all the literatures of the shipbuilding techniques of oceangoing vessels were destroyed, including the records of Zheng He's fleet which sailed across the Indian Ocean (Zheng He's expedition 1405–1433 CE).Zheng He (1371–1433 CE) is a well-known navigator and diplomatist of the Ming Dynasty, the flagship of his fleet was believed to be the world's hugest wooden vessel at that time which might have a much heavier load than the famous Santa Maria of Columbus who sailed across the Atlantic Ocean almost a century later (Zhang, 2004).Over the years, much effort has been made to estimate the structure and tonnage of Zheng He's treasure ships in spite of the lack of knowledge on the shipbuilding technology in his time, however, the sealing material used was barely known.
In 2003, the excavation of the ancient site Baochuanchang Shipyard was carried out in Nanjing, China and this shipyard was believed to have been the workshop where the huge vessels of Zheng He's fleet were built and maintained. Several pieces of ancient putty were found here, and a small piece was sampled and analyzed in this paper to study its components and structure. The results of X-ray diffraction (XRD) showed that the putty was mainly composed of calcite (CaCO3), while pyrolysis-gas chromatography–mass spectrometry (Py-GC–MS) analysis indicated that tung oil was used in making the putty. In addition, plant fibers inside the putty were identified as jute by the means of polarizing microscope and scanning electron microscope (SEM) analysis. These findings confirmed the putty as the so-called chu-nam putty, which was a traditional sealing material used in ancient shipbuilding. Moreover, the surface morphology of the putty was obtained by SEM, and pore size distribution was measured by gas adsorption-desorption analysis. The analytical results suggested that the putty was quite compact, which could perform well in sealing huge wooden ships. This research revealed the shipbuilding skills of the Baochuanchang Shipyard. It may provide reference for studying Zheng He's vessels and make further a contribution to the conservation and restoration of ancient wooden ships of the Ming Dynasty. In 2003, the rescuing excavation of the Ming Dynasty site ‘Liuzuotang’ of ‘Baochuanchang’ Shipyard was carried out in Nanjing and executed by Nanjing Municipal Museum.Several wooden components such as large-sized rudderstocks and decks along with some metal shipbuilding tools and ancient putty pieces were found here (Nanjing Municipal Museum, 2006).In Chinese, “Baochuanchang” 宝船厂 means “Shipyard of Treasure Ships” which specifically referred to the ships that Zheng He took to sail across the Indian Ocean, and “liuzuotang” 六作塘 means “the No.6 boatyard”.The shipyard was officially completed and put into operation at the 3rd year of Emperor Yongle (1405 CE) for the manufacture of huge ships for the well-known expeditions to the Western Oceans directed by Zheng He (Nanjing Municipal Museum, 2006).Thus, the putty excavated might be the same as those used on the ships of the famous Zheng He's fleet.
Feature subset selection is an essential machine learning approach aimed at the process of dimensionality reduction of the input space. By removing irrelevant and/or redundant variables, not only it enhances model performance, but also facilitates its improved interpretability. The fuzzy set and the rough set are two different but complementary theories that apply the fuzzy rough dependency as a criterion for performing feature subset selection. However, this concept can only maintain a maximal dependency function. It cannot preferably illustrate the differences in object classification and does not fit a particular dataset well. This problem was handled by using a fitting model for feature selection with fuzzy rough sets. However, intuitionistic fuzzy set theory can deal with uncertainty in a much better way when compared to fuzzy set theory as it considers positive, negative and hesitancy degree of an object simultaneously to belong to a particular set. Therefore, in the current study, a novel intuitionistic fuzzy rough set model is proposed for handling above mentioned problems. This model fits the data well and prevents misclassification. Firstly, intuitionistic fuzzy decision of a sample is introduced using neighborhood concept. Then, intuitionistic fuzzy lower and upper approximations are constructed using intuitionistic fuzzy decision and parameterized intuitionistic fuzzy granule. Furthermore, a new dependency function is established. Moreover, a greedy forward algorithm is given using the proposed concept to calculate reduct set. Finally, this algorithm is applied to the benchmark datasets and a comparative study with the existing algorithm is presented. From the experimental results, it can be observed that the proposed model provides more accurate reduct set than existing model. Millions of data is generated in multiple scenarios, including weather, census, health care, government, social networking, production, business, and scientific research.Such high dimensional data may increase inefficiency of classifiers, as they possess several irrelevant or redundant features.Therefore, it is necessary to preprocess the dataset before applying any classification algorithm.Feature selection is a preprocessing step to remove irrelevant and/or redundant features and offers more concise and explicit descriptions of data.Feature selection has got wide applications in data mining, signal processing, bioinformatics, machine learning, etc. (Iannarilli and Rubin, 2003; Jaeger et al., 2002; Jain et al., 2000; Kohavi and John, 1997; Kwak and Chong-Ho, 2002; Langley, 1994; Webb and Copsey, 2011; Xiong et al., 2001).
Feature subset selection is an essential machine learning approach aimed at the process of dimensionality reduction of the input space. By removing irrelevant and/or redundant variables, not only it enhances model performance, but also facilitates its improved interpretability. The fuzzy set and the rough set are two different but complementary theories that apply the fuzzy rough dependency as a criterion for performing feature subset selection. However, this concept can only maintain a maximal dependency function. It cannot preferably illustrate the differences in object classification and does not fit a particular dataset well. This problem was handled by using a fitting model for feature selection with fuzzy rough sets. However, intuitionistic fuzzy set theory can deal with uncertainty in a much better way when compared to fuzzy set theory as it considers positive, negative and hesitancy degree of an object simultaneously to belong to a particular set. Therefore, in the current study, a novel intuitionistic fuzzy rough set model is proposed for handling above mentioned problems. This model fits the data well and prevents misclassification. Firstly, intuitionistic fuzzy decision of a sample is introduced using neighborhood concept. Then, intuitionistic fuzzy lower and upper approximations are constructed using intuitionistic fuzzy decision and parameterized intuitionistic fuzzy granule. Furthermore, a new dependency function is established. Moreover, a greedy forward algorithm is given using the proposed concept to calculate reduct set. Finally, this algorithm is applied to the benchmark datasets and a comparative study with the existing algorithm is presented. From the experimental results, it can be observed that the proposed model provides more accurate reduct set than existing model. Rough set (as introduced by Pawlak, 1982, 2012; Pawlak et al., 1995) based feature selection technique utilizes information present in the data alone and successfully produces the reduct set without using any additional information.It deals with indiscernibility between attributes.In this model, the dependency between conditional and decision attribute is determined to evaluate the classification ability of attributes.However, data need to be discretized in order to apply rough set based feature selection technique, which frequently leads to information loss.Fuzzy rough set based feature selection overcomes this problem of information loss.
Feature subset selection is an essential machine learning approach aimed at the process of dimensionality reduction of the input space. By removing irrelevant and/or redundant variables, not only it enhances model performance, but also facilitates its improved interpretability. The fuzzy set and the rough set are two different but complementary theories that apply the fuzzy rough dependency as a criterion for performing feature subset selection. However, this concept can only maintain a maximal dependency function. It cannot preferably illustrate the differences in object classification and does not fit a particular dataset well. This problem was handled by using a fitting model for feature selection with fuzzy rough sets. However, intuitionistic fuzzy set theory can deal with uncertainty in a much better way when compared to fuzzy set theory as it considers positive, negative and hesitancy degree of an object simultaneously to belong to a particular set. Therefore, in the current study, a novel intuitionistic fuzzy rough set model is proposed for handling above mentioned problems. This model fits the data well and prevents misclassification. Firstly, intuitionistic fuzzy decision of a sample is introduced using neighborhood concept. Then, intuitionistic fuzzy lower and upper approximations are constructed using intuitionistic fuzzy decision and parameterized intuitionistic fuzzy granule. Furthermore, a new dependency function is established. Moreover, a greedy forward algorithm is given using the proposed concept to calculate reduct set. Finally, this algorithm is applied to the benchmark datasets and a comparative study with the existing algorithm is presented. From the experimental results, it can be observed that the proposed model provides more accurate reduct set than existing model. Fuzzy rough set theory (as proposed by Dubois and Prade, 1990, 1992) deals with the concept of vagueness and indiscernibility by combining the concepts of fuzzy set theory (Klir and Yuan, 1995; Zadeh, 1965) and rough set theory (Pawlak, 1982; Pawlak et al., 1995).In fuzzy rough set theory, a similarity relation is defined between the samples and lower as well as upper approximations are constructed on the basis of this relation.Union of lower approximations gives the positive region of decision.The greater is the membership to positive region; more is the possibility of sample belonging to a particular category.Using dependency function (Chen et al., 2011, 2012a,b; Degang and Suyun, 2010; Hu et al., 2006, 2010; Jensen and Shen, 2004a,b, 2005, 2007, 2008, 2009; Kumar et al., 2011; Suyun et al., 2009; Tsang et al., 2008; Wang et al., 2019a, 2016, 2019b), significance of a subset of features is evaluated.Also, the conditional entropy measure is used in Wang et al. (2017, 2019) to find reduct set for homogeneous and heterogeneous datasets respectively.However, it may lead to misclassification of samples when there is a large degree of overlap between different categories of data (Wang et al., 2017b).Also, it deals only with membership of sample to a set.Hence, there is a need of different kind of model that can both fit data well and at the same time it can handle uncertainty arising due to non-membership as uncertainty is not found only in judgment but also in the identification.
Feature subset selection is an essential machine learning approach aimed at the process of dimensionality reduction of the input space. By removing irrelevant and/or redundant variables, not only it enhances model performance, but also facilitates its improved interpretability. The fuzzy set and the rough set are two different but complementary theories that apply the fuzzy rough dependency as a criterion for performing feature subset selection. However, this concept can only maintain a maximal dependency function. It cannot preferably illustrate the differences in object classification and does not fit a particular dataset well. This problem was handled by using a fitting model for feature selection with fuzzy rough sets. However, intuitionistic fuzzy set theory can deal with uncertainty in a much better way when compared to fuzzy set theory as it considers positive, negative and hesitancy degree of an object simultaneously to belong to a particular set. Therefore, in the current study, a novel intuitionistic fuzzy rough set model is proposed for handling above mentioned problems. This model fits the data well and prevents misclassification. Firstly, intuitionistic fuzzy decision of a sample is introduced using neighborhood concept. Then, intuitionistic fuzzy lower and upper approximations are constructed using intuitionistic fuzzy decision and parameterized intuitionistic fuzzy granule. Furthermore, a new dependency function is established. Moreover, a greedy forward algorithm is given using the proposed concept to calculate reduct set. Finally, this algorithm is applied to the benchmark datasets and a comparative study with the existing algorithm is presented. From the experimental results, it can be observed that the proposed model provides more accurate reduct set than existing model. Intuitionistic fuzzy (IF) set (Atanasov, 1999; Atanassov, 1986, 1989) handles the uncertainty by considering both membership and non-membership of a sample to a set.In spite of the fact that rough and IF sets both capture specific aspects of the same idea-imprecision, the combination of IF set theory and rough set theory are rarely discussed by the researchers.Jena et al. (2002) demonstrated that lower and upper approximations concept of IF rough sets are again IF sets.In the last few years, some of the IF rough set models have been established (Chakrabarty et al., 1998; Cornelis et al., 2003; De et al., 1998; Huang et al., 2013; Jena et al., 2002; Nanda and Majumdar, 1992; Rizvi et al., 2002; Samanta and Mondal, 2001; Zhang et al., 2019, 2012) and applied for various decision making problems.Çoker (1998) discussed relationship between rough set and IF set and revealed the fact that fuzzy rough set is admittedly an intuitionistic L-fuzzy set.Huang et al. (2013) established dominance in intuitionistic fuzzy rough set and presented its various applications.Moreover, some of the recently published research articles have presented intuitionistic fuzzy rough set based feature selection or attribute reduction techniques (Chen and Yang, 2011; Esmail et al., 2013; Huang et al., 2012; Lu et al., 2009; Shreevastava et al., 2018; Tiwari et al., 2018a,b; Zhang, 2016).Lu et al. (2009) established the genetic algorithm for performing attribute reduction of the intuitionistic fuzzy information system (IFIS).An intuitionistic fuzzy rough set model was presented by Huang et al. (2012) by using distance function.Furthermore, they generalized it for attribute reduction.An approach for attribute reduction based on the discernibility matrix concept was given by Zhang (2016).Chen and Yang (2011) presented a novel attribute reduction algorithm by combining intuitionistic fuzzy rough set with information entropy.Esmail et al. (2013) discussed about the structure of the intuitionistic fuzzy rough set model as well as its properties and presented concepts of attribute reduction and rule extraction.Tan et al. (2018) established an intuitionistic fuzzy rough set model and applied it for attribute subset selection.Tiwari et al. (2018a, b) and Shreevastava et al. (2019, 2018) established different intuitionistic fuzzy rough set models and developed various feature subset selection techniques for supervised as well as semi-supervised datasets.Li et al. (2019) proposed a novel intuitionistic fuzzy clustering algorithm using feature selection for tracking multiple objects.In the recent years, various research articles (Boran et al., 2009; Revanasiddappa and Harish, 2018; Singh et al., 2019; Tiwari et al., 2019) have presented IF rough set models, with its application in feature selection.However, none of the above studies fit a given dataset well and can ideally illustrate the differences in sample classification (Wang et al., 2017a).
Feature subset selection is an essential machine learning approach aimed at the process of dimensionality reduction of the input space. By removing irrelevant and/or redundant variables, not only it enhances model performance, but also facilitates its improved interpretability. The fuzzy set and the rough set are two different but complementary theories that apply the fuzzy rough dependency as a criterion for performing feature subset selection. However, this concept can only maintain a maximal dependency function. It cannot preferably illustrate the differences in object classification and does not fit a particular dataset well. This problem was handled by using a fitting model for feature selection with fuzzy rough sets. However, intuitionistic fuzzy set theory can deal with uncertainty in a much better way when compared to fuzzy set theory as it considers positive, negative and hesitancy degree of an object simultaneously to belong to a particular set. Therefore, in the current study, a novel intuitionistic fuzzy rough set model is proposed for handling above mentioned problems. This model fits the data well and prevents misclassification. Firstly, intuitionistic fuzzy decision of a sample is introduced using neighborhood concept. Then, intuitionistic fuzzy lower and upper approximations are constructed using intuitionistic fuzzy decision and parameterized intuitionistic fuzzy granule. Furthermore, a new dependency function is established. Moreover, a greedy forward algorithm is given using the proposed concept to calculate reduct set. Finally, this algorithm is applied to the benchmark datasets and a comparative study with the existing algorithm is presented. From the experimental results, it can be observed that the proposed model provides more accurate reduct set than existing model. In the current work, a new intuitionistic fuzzy rough set model is proposed.It fits data well and prevents misclassification of data.Although a model for feature selection is presented in Sheeja and Kuriakose (2018), that fits data well and prevents misclassification, but fitting model based on intuitionistic fuzzy rough set is not yet considered.Our proposed model can handle uncertainty, vagueness and imprecision by combining intuitionistic fuzzy set and rough set for feature subset selection.Intuitionistic fuzzy decision of a sample is defined using neighborhood concept.Then, we construct intuitionistic fuzzy lower and upper approximations based on intuitionistic fuzzy decision and parameterized intuitionistic fuzzy relation.Furthermore, dependency function is presented to calculate reduct set.Moreover, a greedy forward algorithm based on proposed concept is introduced.Finally, this algorithm of fitting model based intuitionistic fuzzy rough feature selection (FMIFRFS) is applied to the benchmark datasets and the results are compared with the results of existing algorithm.
Feature subset selection is an essential machine learning approach aimed at the process of dimensionality reduction of the input space. By removing irrelevant and/or redundant variables, not only it enhances model performance, but also facilitates its improved interpretability. The fuzzy set and the rough set are two different but complementary theories that apply the fuzzy rough dependency as a criterion for performing feature subset selection. However, this concept can only maintain a maximal dependency function. It cannot preferably illustrate the differences in object classification and does not fit a particular dataset well. This problem was handled by using a fitting model for feature selection with fuzzy rough sets. However, intuitionistic fuzzy set theory can deal with uncertainty in a much better way when compared to fuzzy set theory as it considers positive, negative and hesitancy degree of an object simultaneously to belong to a particular set. Therefore, in the current study, a novel intuitionistic fuzzy rough set model is proposed for handling above mentioned problems. This model fits the data well and prevents misclassification. Firstly, intuitionistic fuzzy decision of a sample is introduced using neighborhood concept. Then, intuitionistic fuzzy lower and upper approximations are constructed using intuitionistic fuzzy decision and parameterized intuitionistic fuzzy granule. Furthermore, a new dependency function is established. Moreover, a greedy forward algorithm is given using the proposed concept to calculate reduct set. Finally, this algorithm is applied to the benchmark datasets and a comparative study with the existing algorithm is presented. From the experimental results, it can be observed that the proposed model provides more accurate reduct set than existing model. This paper is organized as follows.In Section 2, some preliminaries are given to introduce the basic concept of intuitionistic fuzzy rough set theory.In Section 3, a fitting intuitionistic fuzzy rough set model is developed.The algorithm for feature selection is presented in Section 4.Experimental results are shown in Section 5.Finally, Section 6 concludes the entire work.
Ancient mummies are very valuable human remains especially for the study of the evolution of disease. Non-invasive imaging methods such as computed tomography and X-ray are the gold standard to study such precious remains. We report the case of an ancient Egyptian child mummy from the Musée d'art et d'histoire in Geneva, Switzerland with multifocal sclerotic bone lesions affecting the spine and the left hip. The mummy is of unknown provenance, dating to the Roman period with an estimated age of 4–5 years. An infectious origin of the lesions such as tuberculosis seems most likely. Also regarding the time period an infectious etiology is plausible, since there is evidence that tuberculosis was wide spread in ancient Egypt. However, multiple differential diagnoses are discussed, since the evaluation of disease in ancient remains is different to the clinical standards. Medical history and additional invasive investigations are lacking. Also the desiccation and mummification processes lead to alteration of the tissue resulting in anatomico-morphological distortions. Thus our hypothesis can not be proven and multiple differential diagnoses need to be taken into consideration in this rare case. The value of historic human remains for the study of the evolution of human morphology and disease patterns is widely recognized (Bosch, 2000).Preserved tissues are the most direct source, in comparison to secondary sources such as visual or written records.
Ancient mummies are very valuable human remains especially for the study of the evolution of disease. Non-invasive imaging methods such as computed tomography and X-ray are the gold standard to study such precious remains. We report the case of an ancient Egyptian child mummy from the Musée d'art et d'histoire in Geneva, Switzerland with multifocal sclerotic bone lesions affecting the spine and the left hip. The mummy is of unknown provenance, dating to the Roman period with an estimated age of 4–5 years. An infectious origin of the lesions such as tuberculosis seems most likely. Also regarding the time period an infectious etiology is plausible, since there is evidence that tuberculosis was wide spread in ancient Egypt. However, multiple differential diagnoses are discussed, since the evaluation of disease in ancient remains is different to the clinical standards. Medical history and additional invasive investigations are lacking. Also the desiccation and mummification processes lead to alteration of the tissue resulting in anatomico-morphological distortions. Thus our hypothesis can not be proven and multiple differential diagnoses need to be taken into consideration in this rare case. Ancient mummies are an especially rich source for the study of disease in former times.They originate from different cultures and time periods, and in comparison to skeletal remains, mummies are composed of preserved soft tissue (Cockburn and Cockburn, 1980).This allows for the study of a wide spectrum of diseases, including those of infectious, vascular or neoplastic etiologies (Aufderheide, 2003; Lynnerup, 2007).Hereby novel disease concepts, such as the evidence of arteriosclerosis in Peruvian and Egyptian mummies, as recently demonstrated, might change our understanding of modern civilization disease (Thompson et al., 2013).
Ancient mummies are very valuable human remains especially for the study of the evolution of disease. Non-invasive imaging methods such as computed tomography and X-ray are the gold standard to study such precious remains. We report the case of an ancient Egyptian child mummy from the Musée d'art et d'histoire in Geneva, Switzerland with multifocal sclerotic bone lesions affecting the spine and the left hip. The mummy is of unknown provenance, dating to the Roman period with an estimated age of 4–5 years. An infectious origin of the lesions such as tuberculosis seems most likely. Also regarding the time period an infectious etiology is plausible, since there is evidence that tuberculosis was wide spread in ancient Egypt. However, multiple differential diagnoses are discussed, since the evaluation of disease in ancient remains is different to the clinical standards. Medical history and additional invasive investigations are lacking. Also the desiccation and mummification processes lead to alteration of the tissue resulting in anatomico-morphological distortions. Thus our hypothesis can not be proven and multiple differential diagnoses need to be taken into consideration in this rare case. In order to investigate such valuable historic remains, non-invasive diagnostic tools are required.For skeletal, and especially mummified remains radiological analyses are the methods of choice with conventional X-ray and computed tomography (CT) as currently the gold standard.In the case presented herein, a CT was performed to estimate age and sex, as well as to record intra vitam pathologies and post mortem changes.
We present the case of a child with long-standing, super-refractory status epilepticus (SRSE) who manifested prompt and complete resolution of SRSE upon exposure to pure cannabidiol. SRSE emerged in the context of remote suspected encephalitis with previously well-controlled epilepsy. We discuss the extent to which response may be specifically attributed to cannabidiol, with consideration and discussion of multiple potential drug–drug interactions. Based on this case, we propose that adjunctive cannabidiol be considered in the treatment of SRSE. Super-refractory status epilepticus (SRSE) is defined as status epilepticus that continues or recurs at least 24 h after the onset of anesthesia [1].SRSE is an important focus of ongoing research given the exceptionally high morbidity and mortality [2, 3], and the absence of evidence-based therapeutic options.Given the relatively low incidence of SRSE, clinical trials are quite challenging from a feasibility standpoint.Accordingly, although there are no randomized controlled trials supporting any specific treatment modality, there are abundant contemporary case reports and case series suggesting effectiveness of multiple therapies including ketamine [4], ketogenic diet therapy [5, 6], electroconvulsive therapy [7], thalamic deep brain stimulation [8], repetitive transcranial magnetic stimulation [9], surgical resection (even hemispherectomy) [10, 11], immunotherapy (i.e. corticosteroids) [12], and most recently, allopregnanolone [13–15].
Along with the constantly increasing complexity of industrial automation systems, machine learning methods have been widely applied to detecting abnormal states in such systems. Anomaly detection tasks can be treated as one-class classification problems in machine learning. Geometric methods can give an intuitive solution to such problems. In this paper, we propose a new geometric structure, oriented non-convex hulls, to represent decision boundaries used for one-class classification. Based on this geometric structure, a novel boundary based one-class classification algorithm is developed to solve the anomaly detection problem. Compared with traditional boundary-based approaches such as convex hulls based methods and one-class support vector machines, the proposed approach can better reflect the true geometry of target data and needs little effort for parameter tuning. The effectiveness of this approach is evaluated with artificial and real world data sets to solve the anomaly detection problem in Cyber–Physical-Production-Systems (CPPS). The evaluation results also show that the proposed approach has higher generality than the used baseline algorithms. Improving the reliability of Cyber–Physical Production Systems (CPPS) has become a central issue in research and industry to ensure high product quality and low cost (Gao et al., 2015).In safety-critical industrial automation systems, such as chemical production systems, wind power plants and nuclear energy systems, performance degradation, which may lead to terrible accidents, is normally not tolerated.Detecting system anomalies as early as possible to reduce unplanned machine breakdown is one of the most effective way to ensure system reliability.
Along with the constantly increasing complexity of industrial automation systems, machine learning methods have been widely applied to detecting abnormal states in such systems. Anomaly detection tasks can be treated as one-class classification problems in machine learning. Geometric methods can give an intuitive solution to such problems. In this paper, we propose a new geometric structure, oriented non-convex hulls, to represent decision boundaries used for one-class classification. Based on this geometric structure, a novel boundary based one-class classification algorithm is developed to solve the anomaly detection problem. Compared with traditional boundary-based approaches such as convex hulls based methods and one-class support vector machines, the proposed approach can better reflect the true geometry of target data and needs little effort for parameter tuning. The effectiveness of this approach is evaluated with artificial and real world data sets to solve the anomaly detection problem in Cyber–Physical-Production-Systems (CPPS). The evaluation results also show that the proposed approach has higher generality than the used baseline algorithms. CPPS are mostly complex and distributed systems consisting of sensors/actors, advanced automatic control systems, information management softwares, mechanical components and operators.The dynamic nature of such highly automated systems has led to an increasing system complexity: more complex system structure and more configuration overheads; more complex system behavior; more complex interconnections and interdependencies between involved components, machines and labors (Efthymiou et al., 2012).This poses significant challenges for anomaly detection tasks, e.g. detection of faults, suboptimal energy consumption or wear, since manually creating physical models of anomaly detection that explicitly describe the relationship between the system observations is unrealistic in CPPS with high complexity (Niggemann and Lohweg, 2015).Therefore, either for vendors of machines or for manufacturing enterprises, self-diagnosis approaches are desired to facilitate the maintenance tasks and reduce the unplanned downtimes more efficiently (BMWi, 2019).
Along with the constantly increasing complexity of industrial automation systems, machine learning methods have been widely applied to detecting abnormal states in such systems. Anomaly detection tasks can be treated as one-class classification problems in machine learning. Geometric methods can give an intuitive solution to such problems. In this paper, we propose a new geometric structure, oriented non-convex hulls, to represent decision boundaries used for one-class classification. Based on this geometric structure, a novel boundary based one-class classification algorithm is developed to solve the anomaly detection problem. Compared with traditional boundary-based approaches such as convex hulls based methods and one-class support vector machines, the proposed approach can better reflect the true geometry of target data and needs little effort for parameter tuning. The effectiveness of this approach is evaluated with artificial and real world data sets to solve the anomaly detection problem in Cyber–Physical-Production-Systems (CPPS). The evaluation results also show that the proposed approach has higher generality than the used baseline algorithms. Enabled by a large amount of collected process data and increasing computational power, many machine learning techniques have been proposed for anomaly detections in CPPS (Dai and Gao, 2013).The problem of detecting anomalies can be approached as a classification problem, where system states should be classified either as normal behavior or as anomaly.Since not all possible anomalies are known a priori and the negative class (anomaly) is, in most cases, poorly sampled in CPPS, only data related to the target class (normal behavior) are available.Learning with such kind of data is known as the one-class classification in machine learning.
Along with the constantly increasing complexity of industrial automation systems, machine learning methods have been widely applied to detecting abnormal states in such systems. Anomaly detection tasks can be treated as one-class classification problems in machine learning. Geometric methods can give an intuitive solution to such problems. In this paper, we propose a new geometric structure, oriented non-convex hulls, to represent decision boundaries used for one-class classification. Based on this geometric structure, a novel boundary based one-class classification algorithm is developed to solve the anomaly detection problem. Compared with traditional boundary-based approaches such as convex hulls based methods and one-class support vector machines, the proposed approach can better reflect the true geometry of target data and needs little effort for parameter tuning. The effectiveness of this approach is evaluated with artificial and real world data sets to solve the anomaly detection problem in Cyber–Physical-Production-Systems (CPPS). The evaluation results also show that the proposed approach has higher generality than the used baseline algorithms. Such one-class classification methods use the data to identify the shape of the target data.By doing this, they both generalize, i.e. allow the handling of so-far unobserved data points, and also compute a more compact representation of the class, i.e. improving runtime and memory performance.But over the last years, the variety and rising complexity of CPPS lead to more and more complex shapes of this normal class, e.g. because the number of product variants and the number of plant modules has increased.So the overall research question is how to represent and learn the shape of the normal data while allowing for generalization and performance.
Along with the constantly increasing complexity of industrial automation systems, machine learning methods have been widely applied to detecting abnormal states in such systems. Anomaly detection tasks can be treated as one-class classification problems in machine learning. Geometric methods can give an intuitive solution to such problems. In this paper, we propose a new geometric structure, oriented non-convex hulls, to represent decision boundaries used for one-class classification. Based on this geometric structure, a novel boundary based one-class classification algorithm is developed to solve the anomaly detection problem. Compared with traditional boundary-based approaches such as convex hulls based methods and one-class support vector machines, the proposed approach can better reflect the true geometry of target data and needs little effort for parameter tuning. The effectiveness of this approach is evaluated with artificial and real world data sets to solve the anomaly detection problem in Cyber–Physical-Production-Systems (CPPS). The evaluation results also show that the proposed approach has higher generality than the used baseline algorithms. The generality and performance of one-class classification algorithms is dependent, to a great extent, on the selection of data representation used in the algorithms.Two traditional solutions to the representation of data exist in one-class classification methods:
Along with the constantly increasing complexity of industrial automation systems, machine learning methods have been widely applied to detecting abnormal states in such systems. Anomaly detection tasks can be treated as one-class classification problems in machine learning. Geometric methods can give an intuitive solution to such problems. In this paper, we propose a new geometric structure, oriented non-convex hulls, to represent decision boundaries used for one-class classification. Based on this geometric structure, a novel boundary based one-class classification algorithm is developed to solve the anomaly detection problem. Compared with traditional boundary-based approaches such as convex hulls based methods and one-class support vector machines, the proposed approach can better reflect the true geometry of target data and needs little effort for parameter tuning. The effectiveness of this approach is evaluated with artificial and real world data sets to solve the anomaly detection problem in Cyber–Physical-Production-Systems (CPPS). The evaluation results also show that the proposed approach has higher generality than the used baseline algorithms. The first kind of method makes assumptions about either the probability density or the generating process of target data.Due to the lack of a priori knowledge about target data, the selection of the right model to describe the target distribution or the data generating process is mostly challenging, especially for high dimensional real world data acquired from CPPS (Khan and Madden, 2014).Contrarily, boundary identification methods do not focus on modeling the underlying distribution or the generating process of target data, but try to find a closed boundary around them (Pimentel et al., 2014).The data points outside the closed boundary are anomalies.This kind of methods have several advantages: (1) no a-priori knowledge is needed, (2) boundaries are a very compact and efficient representations and (3) the amount of generalization can be adjusted.For these reasons, this paper will introduce a new boundary computation algorithm suited to CPPS.Training OCSVM on large data sets is computationally intensive.Furthermore, the selection of the kernel function parameters is still an important practical question that is not entirely solved (Wang et al., 2013).Convex hulls based methods provide an intuitive solution to boundary based one-class classification and therefore have been considered as an intuitive and powerful tool (Casale et al., 2014).
Along with the constantly increasing complexity of industrial automation systems, machine learning methods have been widely applied to detecting abnormal states in such systems. Anomaly detection tasks can be treated as one-class classification problems in machine learning. Geometric methods can give an intuitive solution to such problems. In this paper, we propose a new geometric structure, oriented non-convex hulls, to represent decision boundaries used for one-class classification. Based on this geometric structure, a novel boundary based one-class classification algorithm is developed to solve the anomaly detection problem. Compared with traditional boundary-based approaches such as convex hulls based methods and one-class support vector machines, the proposed approach can better reflect the true geometry of target data and needs little effort for parameter tuning. The effectiveness of this approach is evaluated with artificial and real world data sets to solve the anomaly detection problem in Cyber–Physical-Production-Systems (CPPS). The evaluation results also show that the proposed approach has higher generality than the used baseline algorithms. CPPS are normally equipped with close-loop control systems in component level and also across all distributed units.Efficient and reliable control strategies are needed to push the controlled variables to the predefined work points according to highly complex and time-dependent workloads.The intricate interdependencies and causalities between components of CPPS caused by the first principles and the complex control strategies lead to the appearance of non-stationary dynamic behavior in CPPS.The high sampling rate (in millisecond, even in microsecond for motion control) enhances the impact of such non-stationary dynamic behavior with intrinsically non-linear characteristics on the collected data sets (Reis and Gins, 2017): the collected process data have complex non-convex shapes in most cases.Therefore, convex hulls based methods fail in analyzing data from complex CPPS.
Along with the constantly increasing complexity of industrial automation systems, machine learning methods have been widely applied to detecting abnormal states in such systems. Anomaly detection tasks can be treated as one-class classification problems in machine learning. Geometric methods can give an intuitive solution to such problems. In this paper, we propose a new geometric structure, oriented non-convex hulls, to represent decision boundaries used for one-class classification. Based on this geometric structure, a novel boundary based one-class classification algorithm is developed to solve the anomaly detection problem. Compared with traditional boundary-based approaches such as convex hulls based methods and one-class support vector machines, the proposed approach can better reflect the true geometry of target data and needs little effort for parameter tuning. The effectiveness of this approach is evaluated with artificial and real world data sets to solve the anomaly detection problem in Cyber–Physical-Production-Systems (CPPS). The evaluation results also show that the proposed approach has higher generality than the used baseline algorithms. Fig. 1 shows one negative case.Convex hulls cannot always represent the true geometry of target data.The point P (red star) will be classified as target class.However, it lies far away from the border points of the given target data.Therefore, using convex hulls leads to poor accuracy, when the target data have a non-convex shape.
Along with the constantly increasing complexity of industrial automation systems, machine learning methods have been widely applied to detecting abnormal states in such systems. Anomaly detection tasks can be treated as one-class classification problems in machine learning. Geometric methods can give an intuitive solution to such problems. In this paper, we propose a new geometric structure, oriented non-convex hulls, to represent decision boundaries used for one-class classification. Based on this geometric structure, a novel boundary based one-class classification algorithm is developed to solve the anomaly detection problem. Compared with traditional boundary-based approaches such as convex hulls based methods and one-class support vector machines, the proposed approach can better reflect the true geometry of target data and needs little effort for parameter tuning. The effectiveness of this approach is evaluated with artificial and real world data sets to solve the anomaly detection problem in Cyber–Physical-Production-Systems (CPPS). The evaluation results also show that the proposed approach has higher generality than the used baseline algorithms. As an alternative option, non-convex hulls are exactly the geometric structure to represent the geometry of non-convex data.In recent years, much work has been carried out on the computation of non-convex hulls in different dimensional spaces (Park and Oh, 2012; Li et al., 2018).However, to the best of our knowledge, several research questions (RQ) remain unsolved: (1) No mathematical definition of non-convex hulls exists.(2) No efficient algorithm for computing such hulls has been proposed, which is suitable for typical CPPS data.(3) No recent work proposes solutions to determine whether a point lies inside or outside a non-convex boundary in high dimensional spaces.To enable the application of non-convex hulls in solving anomaly detection problems in CPPS, an appropriate non-convex hulls based representation and an efficient method for detecting the position of points to a given non-convex hull is needed.
Along with the constantly increasing complexity of industrial automation systems, machine learning methods have been widely applied to detecting abnormal states in such systems. Anomaly detection tasks can be treated as one-class classification problems in machine learning. Geometric methods can give an intuitive solution to such problems. In this paper, we propose a new geometric structure, oriented non-convex hulls, to represent decision boundaries used for one-class classification. Based on this geometric structure, a novel boundary based one-class classification algorithm is developed to solve the anomaly detection problem. Compared with traditional boundary-based approaches such as convex hulls based methods and one-class support vector machines, the proposed approach can better reflect the true geometry of target data and needs little effort for parameter tuning. The effectiveness of this approach is evaluated with artificial and real world data sets to solve the anomaly detection problem in Cyber–Physical-Production-Systems (CPPS). The evaluation results also show that the proposed approach has higher generality than the used baseline algorithms. In this work, the one-class classification problem is addressed with respect to non-convex data acquired from CPPS.Our major contributions are:
Along with the constantly increasing complexity of industrial automation systems, machine learning methods have been widely applied to detecting abnormal states in such systems. Anomaly detection tasks can be treated as one-class classification problems in machine learning. Geometric methods can give an intuitive solution to such problems. In this paper, we propose a new geometric structure, oriented non-convex hulls, to represent decision boundaries used for one-class classification. Based on this geometric structure, a novel boundary based one-class classification algorithm is developed to solve the anomaly detection problem. Compared with traditional boundary-based approaches such as convex hulls based methods and one-class support vector machines, the proposed approach can better reflect the true geometry of target data and needs little effort for parameter tuning. The effectiveness of this approach is evaluated with artificial and real world data sets to solve the anomaly detection problem in Cyber–Physical-Production-Systems (CPPS). The evaluation results also show that the proposed approach has higher generality than the used baseline algorithms. The experimental results show that the proposed approach improves the accuracy of anomaly detection compared with using convex hulls, when data have non-convex shapes.Furthermore, the proposed solution can also be used for anomaly detection of convex data sets.Compared with conventional one-class classification methods, the proposed approach does not make any assumptions on target data and therefore has higher generality.
To our knowledge, there are no reports of status epilepticus (SE) associated with mitochondrial diseases and treated with perampanel (PER). We present three cases of patients with refractory SE associated with MELAS syndrome who responded favorably to PER. All cases were diagnosed as non-convulsive SE (focal without impairment of level of consciousness). After an initial treatment with other anti-seizure drugs, PER was added in all cases (8, 16 and 12 mg) and cessation of SE was observed within the next 4-8 hours. All the cases involved a stroke-like lesion present on brain MRI. In our patients, PER was an effective option in SE associated with MELAS syndrome. Epilepsy is commonly described in mitochondrial diseases.The types of epilepsy reported in both adults and children include myoclonic seizures, focal to bilateral tonic–clonic seizures, epilepsia partialis continua, and generalized epilepsy syndromes [1–3].The prevalence of status epilepticus (SE) in these patients is unknown and it is reported less often than other types of seizures, although it may go unrecognized.
To our knowledge, there are no reports of status epilepticus (SE) associated with mitochondrial diseases and treated with perampanel (PER). We present three cases of patients with refractory SE associated with MELAS syndrome who responded favorably to PER. All cases were diagnosed as non-convulsive SE (focal without impairment of level of consciousness). After an initial treatment with other anti-seizure drugs, PER was added in all cases (8, 16 and 12 mg) and cessation of SE was observed within the next 4-8 hours. All the cases involved a stroke-like lesion present on brain MRI. In our patients, PER was an effective option in SE associated with MELAS syndrome. Within the spectrum of mitochondrial diseases, MELAS (mitochondrial encephalopathy with lactic acidosis and stroke-like episodes) is among those most commonly associated with SE.It was reported in 7.9% of cases in one series, and always in the context of a stroke-like episode [4].
To our knowledge, there are no reports of status epilepticus (SE) associated with mitochondrial diseases and treated with perampanel (PER). We present three cases of patients with refractory SE associated with MELAS syndrome who responded favorably to PER. All cases were diagnosed as non-convulsive SE (focal without impairment of level of consciousness). After an initial treatment with other anti-seizure drugs, PER was added in all cases (8, 16 and 12 mg) and cessation of SE was observed within the next 4-8 hours. All the cases involved a stroke-like lesion present on brain MRI. In our patients, PER was an effective option in SE associated with MELAS syndrome. Most SE episodes in mitochondrial diseases are refractory to treatment [2], and status persists despite administration of anti-seizure drugs (ASDs).In this scenario, the efficacy of GABA-ergic drugs is markedly reduced [5] and glutamate can play a major role in SE persistence by promoting perpetuation of the epileptic activity through AMPA receptors.
This paper proposes a new enhanced harmony search (NEHS) algorithm to solve dynamic economic emission dispatch (DEED) problems. NEHS is a variant of harmony search (HS) algorithm that is inspired by the music phenomenon. NEHS introduces two modifications to increase its competitiveness for the DEED problems. First, three levels of arbitrary distance bandwidths are used to enhance its global search and local search abilities. Second, both the best and the worst harmony vectors are selected to participate in the harmony memory considering operation in the second half of generations, which reaches a good compromise between improving solution quality and overcoming premature convergence. In addition, the improvised harmony vector at each generation is almost infeasible, and a repair technique is devised to drive it to the feasible domain efficiently. The performance of NEHS is evaluated on five DEED instances, and compared with that of the other four state-of-the-art HSs. Experimental results verify that NEHS is preferable in solving the five DEED instances as it can produce better solutions with less computation burden. Dynamic economic emission dispatch (DEED) is a hot issue in power systems, and it is a multi-objective constrained optimization problem.DEED has two objectives including the fuel cost and pollutant emission minimizations.The former serves to realize considerable cost savings, and the latter serves to reduce the effect of pollutants from the point of view of environmental protection.Both objectives are subjected to several constraints, such as the generator capacity constraints, power balance constraints and generating unit ramp-rate constraints.Furthermore, the goal of DEED is to simultaneously minimize its two objective functions while satisfying all the equality and inequality constraints, which can be realized by scheduling the power outputs of generating units according to the predicted customer load demands over a certain number of time intervals.Mathematically, to appropriately characterize the DEED problem, it is necessary to establish a comprehensive and accurate evaluation model from the two aspects of multi-objective decision making and constraints handling.
This paper proposes a new enhanced harmony search (NEHS) algorithm to solve dynamic economic emission dispatch (DEED) problems. NEHS is a variant of harmony search (HS) algorithm that is inspired by the music phenomenon. NEHS introduces two modifications to increase its competitiveness for the DEED problems. First, three levels of arbitrary distance bandwidths are used to enhance its global search and local search abilities. Second, both the best and the worst harmony vectors are selected to participate in the harmony memory considering operation in the second half of generations, which reaches a good compromise between improving solution quality and overcoming premature convergence. In addition, the improvised harmony vector at each generation is almost infeasible, and a repair technique is devised to drive it to the feasible domain efficiently. The performance of NEHS is evaluated on five DEED instances, and compared with that of the other four state-of-the-art HSs. Experimental results verify that NEHS is preferable in solving the five DEED instances as it can produce better solutions with less computation burden. DEED is known as an intractable dynamic programming problem because of its large dimensionality, multiple local optima, high nonlinearity, non-differentiability, non-convexity, competing objectives and strong constraints.Hundreds and even thousands of problem variables will inevitably bring a large amount of calculation, and the inherent nonlinearity and non-convexity of the objective functions and constraints will result in narrow and uneven solution space where a number of local optima may lure the search to diverge from the global optimum.Therefore, there is an urgent need to develop fast and accurate algorithms in order to meet the challenges brought by the large calculation and high complexity of the DEED problem.Traditional numerical optimization methods are heavily reliant on problem gradients and are therefore unable to deal with the DEED problem with the characteristic of non-differentiability.Although these methods can employ approximations to achieve the solvability of DEED, their computation accuracies cannot be guaranteed.Moreover, their high sensitivities to the initial guesses of the solutions will exert significant influence on the quality of the final solutions, which is harmful to their stabilities in solving DEED.
This paper proposes a new enhanced harmony search (NEHS) algorithm to solve dynamic economic emission dispatch (DEED) problems. NEHS is a variant of harmony search (HS) algorithm that is inspired by the music phenomenon. NEHS introduces two modifications to increase its competitiveness for the DEED problems. First, three levels of arbitrary distance bandwidths are used to enhance its global search and local search abilities. Second, both the best and the worst harmony vectors are selected to participate in the harmony memory considering operation in the second half of generations, which reaches a good compromise between improving solution quality and overcoming premature convergence. In addition, the improvised harmony vector at each generation is almost infeasible, and a repair technique is devised to drive it to the feasible domain efficiently. The performance of NEHS is evaluated on five DEED instances, and compared with that of the other four state-of-the-art HSs. Experimental results verify that NEHS is preferable in solving the five DEED instances as it can produce better solutions with less computation burden. Recently there has been a surge of interest in heuristic algorithms deriving from natural phenomena and social behaviors.Because of the simplicity and high efficiency, these algorithms have a broad range of potential applications in many fields (Kadri and Boctor, 2018; Sedghizadeh and Beheshti, 2018; Li et al., 2016; Haddar et al., 2016; Mason et al., 2018; Balaji et al., 2016; Biswas et al., 2018; Ali et al., 2014; Kadhar et al., 2015; Akbari et al., 2017; Basu, 2016; Hadidi, 2017).Many heuristic algorithms such as genetic algorithm (GA) (Holl, 1975), simulated annealing (SA) (Kirpatrick et al., 1983), particle swarm optimization (PSO) (Kennedy and Eberhart, 1995), differential evolution (DE) (Storn and Price, 1995), harmony search (HS) (Geem et al., 2001), bacterial foraging optimization (BFO) (Passino, 2002), group search optimizer (GSO) (He et al., 2009), and chemical reaction optimization (CRO) (Lam and Li, 2010) have been improved to suit the DEED problem without using any approximation of its fuel cost and emission functions.These improved algorithms utilize various probabilistic rules to update their candidate solutions in the search space, and can yield reasonable solutions of several DEED instances within an acceptable period of time.The related work can be described as follows:
This paper proposes a new enhanced harmony search (NEHS) algorithm to solve dynamic economic emission dispatch (DEED) problems. NEHS is a variant of harmony search (HS) algorithm that is inspired by the music phenomenon. NEHS introduces two modifications to increase its competitiveness for the DEED problems. First, three levels of arbitrary distance bandwidths are used to enhance its global search and local search abilities. Second, both the best and the worst harmony vectors are selected to participate in the harmony memory considering operation in the second half of generations, which reaches a good compromise between improving solution quality and overcoming premature convergence. In addition, the improvised harmony vector at each generation is almost infeasible, and a repair technique is devised to drive it to the feasible domain efficiently. The performance of NEHS is evaluated on five DEED instances, and compared with that of the other four state-of-the-art HSs. Experimental results verify that NEHS is preferable in solving the five DEED instances as it can produce better solutions with less computation burden. Basu (2006) handled DEED by combining goal attainment method and PSO.The former converts the multi-objective problem into a single-objective one, and the latter is employed as an optimization tool to minimize the single-objective problem.The method is able to determine optimal noninferior solution in accordance with the decision maker’s requirement, and does not require differentiability and convexity for the generating unit characteristics, hence it provides a more flexible operation on generating units.
This paper proposes a new enhanced harmony search (NEHS) algorithm to solve dynamic economic emission dispatch (DEED) problems. NEHS is a variant of harmony search (HS) algorithm that is inspired by the music phenomenon. NEHS introduces two modifications to increase its competitiveness for the DEED problems. First, three levels of arbitrary distance bandwidths are used to enhance its global search and local search abilities. Second, both the best and the worst harmony vectors are selected to participate in the harmony memory considering operation in the second half of generations, which reaches a good compromise between improving solution quality and overcoming premature convergence. In addition, the improvised harmony vector at each generation is almost infeasible, and a repair technique is devised to drive it to the feasible domain efficiently. The performance of NEHS is evaluated on five DEED instances, and compared with that of the other four state-of-the-art HSs. Experimental results verify that NEHS is preferable in solving the five DEED instances as it can produce better solutions with less computation burden. Elaiwa et al. (2013) proposed two hybrid approaches to handle DEED.The first approach integrates DE and sequential quadratic programming (SQP), and the second one integrates PSO and SQP.DE (or PSO) is employed to perform global search, and the obtained best solution is taken as the initial one of SQP.With the assistance of DE (or PSO), SQP is employed to further perform local search in order to obtain the final optimal solution.By integrating DE (or PSO) and SQP, either hybrid approach can achieve a good compromise between the global search and local search.
This paper proposes a new enhanced harmony search (NEHS) algorithm to solve dynamic economic emission dispatch (DEED) problems. NEHS is a variant of harmony search (HS) algorithm that is inspired by the music phenomenon. NEHS introduces two modifications to increase its competitiveness for the DEED problems. First, three levels of arbitrary distance bandwidths are used to enhance its global search and local search abilities. Second, both the best and the worst harmony vectors are selected to participate in the harmony memory considering operation in the second half of generations, which reaches a good compromise between improving solution quality and overcoming premature convergence. In addition, the improvised harmony vector at each generation is almost infeasible, and a repair technique is devised to drive it to the feasible domain efficiently. The performance of NEHS is evaluated on five DEED instances, and compared with that of the other four state-of-the-art HSs. Experimental results verify that NEHS is preferable in solving the five DEED instances as it can produce better solutions with less computation burden. Zhang et al. (2015) devised a multi-objective hybrid differential evolution with simulated annealing technique (MOHDE-SAT) for DEED.In population initialization, MOHDE-SAT employs the orthogonal initialization method (Wang et al., 2007) to increase the population diversity.In mutation, the scaling parameter is formulated as a monotonically decreasing function to improve MOHDE-SAT’s convergence speed.In crossover, SA is used to increase the acceptable probability of the produced solutions.Additionally, entropy diversity method (Wu and Wang, 2010) is employed to measure the population diversity in real time, which ensures that the Pareto front is evenly distributed.
This paper proposes a new enhanced harmony search (NEHS) algorithm to solve dynamic economic emission dispatch (DEED) problems. NEHS is a variant of harmony search (HS) algorithm that is inspired by the music phenomenon. NEHS introduces two modifications to increase its competitiveness for the DEED problems. First, three levels of arbitrary distance bandwidths are used to enhance its global search and local search abilities. Second, both the best and the worst harmony vectors are selected to participate in the harmony memory considering operation in the second half of generations, which reaches a good compromise between improving solution quality and overcoming premature convergence. In addition, the improvised harmony vector at each generation is almost infeasible, and a repair technique is devised to drive it to the feasible domain efficiently. The performance of NEHS is evaluated on five DEED instances, and compared with that of the other four state-of-the-art HSs. Experimental results verify that NEHS is preferable in solving the five DEED instances as it can produce better solutions with less computation burden. Niu et al. (2014) introduced an efficient harmony search approach based on a new pitch adjustment rule (NPAHS) to solve the DEED problem.NPAHS relies on the perturbation information and the mean value of the harmony memory to establish a new pitch adjustment rule, which contributes to the improvements of solution quality and convergence speed.In addition, a boundary limit handling strategy is used to ensure that the problem variables are within their boundaries, and a power balance constraint handling strategy is used to facilitate the satisfactions of the power balance constraints and generating unit ramp-rate constraints.
This paper proposes a new enhanced harmony search (NEHS) algorithm to solve dynamic economic emission dispatch (DEED) problems. NEHS is a variant of harmony search (HS) algorithm that is inspired by the music phenomenon. NEHS introduces two modifications to increase its competitiveness for the DEED problems. First, three levels of arbitrary distance bandwidths are used to enhance its global search and local search abilities. Second, both the best and the worst harmony vectors are selected to participate in the harmony memory considering operation in the second half of generations, which reaches a good compromise between improving solution quality and overcoming premature convergence. In addition, the improvised harmony vector at each generation is almost infeasible, and a repair technique is devised to drive it to the feasible domain efficiently. The performance of NEHS is evaluated on five DEED instances, and compared with that of the other four state-of-the-art HSs. Experimental results verify that NEHS is preferable in solving the five DEED instances as it can produce better solutions with less computation burden. Basu (2007) combined an interactive fuzzy satisfying method and evolutionary programming to solve the DEED problem where fuel cost and pollutant emission are conflicting objectives.Suppose that the decision maker has a fuzzy and imprecise goal for either objective function, then the goal can be quantified by the corresponding membership function, which contributes to the form of a minimax problem.Moreover, an evolutionary programming method is used to solve the minimax problem.With this configuration, a good compromise solution can be determined in an objective manner for DEED.
This paper proposes a new enhanced harmony search (NEHS) algorithm to solve dynamic economic emission dispatch (DEED) problems. NEHS is a variant of harmony search (HS) algorithm that is inspired by the music phenomenon. NEHS introduces two modifications to increase its competitiveness for the DEED problems. First, three levels of arbitrary distance bandwidths are used to enhance its global search and local search abilities. Second, both the best and the worst harmony vectors are selected to participate in the harmony memory considering operation in the second half of generations, which reaches a good compromise between improving solution quality and overcoming premature convergence. In addition, the improvised harmony vector at each generation is almost infeasible, and a repair technique is devised to drive it to the feasible domain efficiently. The performance of NEHS is evaluated on five DEED instances, and compared with that of the other four state-of-the-art HSs. Experimental results verify that NEHS is preferable in solving the five DEED instances as it can produce better solutions with less computation burden. Alsumait et al. applied a pattern search (PS) optimization (Alsumait et al., 2010; Lewis et al., 2000; Alsumait et al., 2007, 2008) to the dynamic economic dispatch and DEED problems.PS proceeds by establishing a sequence of points called a mesh, around the current point.Before the PS process starts, the initial starting point determined by the user is chosen as the current point.In the following generations, the previous step of PS is utilized to compute the current point.Additionally, the mesh is constructed by adding the current point to a scalar multiple of a series of vectors called a pattern.In case that a point in the mesh is better than the current point, the new point will take the place of the current one for the next generation.
This paper proposes a new enhanced harmony search (NEHS) algorithm to solve dynamic economic emission dispatch (DEED) problems. NEHS is a variant of harmony search (HS) algorithm that is inspired by the music phenomenon. NEHS introduces two modifications to increase its competitiveness for the DEED problems. First, three levels of arbitrary distance bandwidths are used to enhance its global search and local search abilities. Second, both the best and the worst harmony vectors are selected to participate in the harmony memory considering operation in the second half of generations, which reaches a good compromise between improving solution quality and overcoming premature convergence. In addition, the improvised harmony vector at each generation is almost infeasible, and a repair technique is devised to drive it to the feasible domain efficiently. The performance of NEHS is evaluated on five DEED instances, and compared with that of the other four state-of-the-art HSs. Experimental results verify that NEHS is preferable in solving the five DEED instances as it can produce better solutions with less computation burden. Guo et al. (2012) proposed a group search optimization with multiple producers (GSOMP) to solve the DEED problem.Group search optimization (GSO) originates from animal foraging behavior (He et al., 2009), and it is based on the assumption that group members search either for “finding“ (producer) or for “joining” (scrounger) opportunities.Accordingly, the animal foraging mechanisms are adopted to make searching strategies for complex optimization problems.With respect to the constraint handling aspect of DEED, a scheme consisting of the constrained tournament method (Deb et al., 2000) and Lamarckian method (Deb, 2001) is integrated into GSOMP in order to satisfy the inequality and equality constraints.With respect to the multi-objective optimization aspect of DEED, the technique for order preference by similarity to an ideal solution (TOPSIS) (Hwang and Yoon, 1981; Olson, 2004) is employed to determine the final solution from the Pareto-optimal solutions based on a decision maker’s preference.
This paper proposes a new enhanced harmony search (NEHS) algorithm to solve dynamic economic emission dispatch (DEED) problems. NEHS is a variant of harmony search (HS) algorithm that is inspired by the music phenomenon. NEHS introduces two modifications to increase its competitiveness for the DEED problems. First, three levels of arbitrary distance bandwidths are used to enhance its global search and local search abilities. Second, both the best and the worst harmony vectors are selected to participate in the harmony memory considering operation in the second half of generations, which reaches a good compromise between improving solution quality and overcoming premature convergence. In addition, the improvised harmony vector at each generation is almost infeasible, and a repair technique is devised to drive it to the feasible domain efficiently. The performance of NEHS is evaluated on five DEED instances, and compared with that of the other four state-of-the-art HSs. Experimental results verify that NEHS is preferable in solving the five DEED instances as it can produce better solutions with less computation burden. Roy and Bhui (2016) developed a hybrid differential evolution-based chemical reaction optimization (HCRO) algorithm to deal with the DEED problem.CRO (Lam and Li, 2010) mimics the interactions of molecules in a chemical reaction in order to achieve a low energy stable state.The mutation operation of DE is incorporated into CRO to enhance its convergence and heighten the solution quality.Experimental results show that the hybrid algorithm is capable of obtaining desirable DEED solutions in a short period of time.
This paper proposes a new enhanced harmony search (NEHS) algorithm to solve dynamic economic emission dispatch (DEED) problems. NEHS is a variant of harmony search (HS) algorithm that is inspired by the music phenomenon. NEHS introduces two modifications to increase its competitiveness for the DEED problems. First, three levels of arbitrary distance bandwidths are used to enhance its global search and local search abilities. Second, both the best and the worst harmony vectors are selected to participate in the harmony memory considering operation in the second half of generations, which reaches a good compromise between improving solution quality and overcoming premature convergence. In addition, the improvised harmony vector at each generation is almost infeasible, and a repair technique is devised to drive it to the feasible domain efficiently. The performance of NEHS is evaluated on five DEED instances, and compared with that of the other four state-of-the-art HSs. Experimental results verify that NEHS is preferable in solving the five DEED instances as it can produce better solutions with less computation burden. Jiang et al. (2013) devised a modified adaptive multiobjective differential evolution algorithm (MAMODE) to solve the DEED problem.Expanded double selection and adaptive random restart operators are incorporated into DE to overcome premature convergence.Furthermore, a dynamic heuristic constraint handling (DHCH) strategy with moderate approximation is specifically proposed to handle the equality constraints.In addition, the fuzzy-based mechanism (Dhillon et al., 1993; Abido, 2003) is employed to extract the best compromise solution from the final external archive (Ziztler et al., 2002).
This paper proposes a new enhanced harmony search (NEHS) algorithm to solve dynamic economic emission dispatch (DEED) problems. NEHS is a variant of harmony search (HS) algorithm that is inspired by the music phenomenon. NEHS introduces two modifications to increase its competitiveness for the DEED problems. First, three levels of arbitrary distance bandwidths are used to enhance its global search and local search abilities. Second, both the best and the worst harmony vectors are selected to participate in the harmony memory considering operation in the second half of generations, which reaches a good compromise between improving solution quality and overcoming premature convergence. In addition, the improvised harmony vector at each generation is almost infeasible, and a repair technique is devised to drive it to the feasible domain efficiently. The performance of NEHS is evaluated on five DEED instances, and compared with that of the other four state-of-the-art HSs. Experimental results verify that NEHS is preferable in solving the five DEED instances as it can produce better solutions with less computation burden. Bacterial foraging optimization (BFO) (Passino, 2002) is enlightened by the theory of natural selection which is inclined to exclude the animals using ineffective foraging strategies and retain the ones using successful strategies.To further improve it performance, an improved bacterial foraging algorithm (IBFA) (Pandit et al., 2012) was proposed in 2012.IBFA utilizes a small number of bacteria to search solution space, adopts crossover operation to avoid saturation of the swarm, and employs a dynamic chemotactic step size to promote exploration and exploitation.
This paper proposes a new enhanced harmony search (NEHS) algorithm to solve dynamic economic emission dispatch (DEED) problems. NEHS is a variant of harmony search (HS) algorithm that is inspired by the music phenomenon. NEHS introduces two modifications to increase its competitiveness for the DEED problems. First, three levels of arbitrary distance bandwidths are used to enhance its global search and local search abilities. Second, both the best and the worst harmony vectors are selected to participate in the harmony memory considering operation in the second half of generations, which reaches a good compromise between improving solution quality and overcoming premature convergence. In addition, the improvised harmony vector at each generation is almost infeasible, and a repair technique is devised to drive it to the feasible domain efficiently. The performance of NEHS is evaluated on five DEED instances, and compared with that of the other four state-of-the-art HSs. Experimental results verify that NEHS is preferable in solving the five DEED instances as it can produce better solutions with less computation burden. Basu (2008b) used a nondominated sorting genetic algorithm-II (NSGA-II) to solve the DEED problem, and it is implemented by using real-coded genetic algorithm (RCGA) (Deb and Agrawal, 1995; Herrera et al., 1998; Deb, 2000).This algorithm involves fast nondominated sorting procedure, fast crowded distance estimation procedure, simple crowded-comparison operator, tournament selection, simulated binary crossover and polynomial mutation (Deb and Agrawal, 1995).In addition, NSGA-II handles DEED as a multi-objective optimization with conflicting objectives, and it is able to obtain a diverse set of well-distributed pareto-optimal solutions for DEED.
This paper proposes a new enhanced harmony search (NEHS) algorithm to solve dynamic economic emission dispatch (DEED) problems. NEHS is a variant of harmony search (HS) algorithm that is inspired by the music phenomenon. NEHS introduces two modifications to increase its competitiveness for the DEED problems. First, three levels of arbitrary distance bandwidths are used to enhance its global search and local search abilities. Second, both the best and the worst harmony vectors are selected to participate in the harmony memory considering operation in the second half of generations, which reaches a good compromise between improving solution quality and overcoming premature convergence. In addition, the improvised harmony vector at each generation is almost infeasible, and a repair technique is devised to drive it to the feasible domain efficiently. The performance of NEHS is evaluated on five DEED instances, and compared with that of the other four state-of-the-art HSs. Experimental results verify that NEHS is preferable in solving the five DEED instances as it can produce better solutions with less computation burden. Zhu et al. (2016) developed a modified non-dominated sorting genetic algorithm-II (MNSGA-II) to solve the DEED problem.This algorithm adopts the selection based on dynamic crowding distance and new controlled elitism, which facilitates the diversity maintenance of population under the premise of elitism of population.Furthermore, a modified heuristic operation based on forward search operator is introduced to handle constraints, and a new membership function is used to evaluate the superiority–inferiority of the candidate solutions in the Pareto set, from which the best compromise solution can be determined.
This paper proposes a new enhanced harmony search (NEHS) algorithm to solve dynamic economic emission dispatch (DEED) problems. NEHS is a variant of harmony search (HS) algorithm that is inspired by the music phenomenon. NEHS introduces two modifications to increase its competitiveness for the DEED problems. First, three levels of arbitrary distance bandwidths are used to enhance its global search and local search abilities. Second, both the best and the worst harmony vectors are selected to participate in the harmony memory considering operation in the second half of generations, which reaches a good compromise between improving solution quality and overcoming premature convergence. In addition, the improvised harmony vector at each generation is almost infeasible, and a repair technique is devised to drive it to the feasible domain efficiently. The performance of NEHS is evaluated on five DEED instances, and compared with that of the other four state-of-the-art HSs. Experimental results verify that NEHS is preferable in solving the five DEED instances as it can produce better solutions with less computation burden. Researchers have also studied the other economic dispatch issues using various metaheuristics, such as particle swarm approach (Coelho and Mariani, 2008), orthogonal learning competitive swarm optimizer (Xiong and Shi, 2018), hybrid grey wolf optimizer (Jayabarathi et al., 2016), cuckoo search method (Nazari-Heris et al., 2018), bee colony optimization (Nazari-Heris et al., 2018), and harmony search algorithm (Karthigeyan et al., 2015; Al-Betar et al., 2016a; Coelho and Mariani, 2009; Ma et al., 2017).These metaheuristics can obtain good solutions for the economic dispatch problems with different demands such as combined heat and power dispatch, plug-in electric vehicles charging etc.
This paper proposes a new enhanced harmony search (NEHS) algorithm to solve dynamic economic emission dispatch (DEED) problems. NEHS is a variant of harmony search (HS) algorithm that is inspired by the music phenomenon. NEHS introduces two modifications to increase its competitiveness for the DEED problems. First, three levels of arbitrary distance bandwidths are used to enhance its global search and local search abilities. Second, both the best and the worst harmony vectors are selected to participate in the harmony memory considering operation in the second half of generations, which reaches a good compromise between improving solution quality and overcoming premature convergence. In addition, the improvised harmony vector at each generation is almost infeasible, and a repair technique is devised to drive it to the feasible domain efficiently. The performance of NEHS is evaluated on five DEED instances, and compared with that of the other four state-of-the-art HSs. Experimental results verify that NEHS is preferable in solving the five DEED instances as it can produce better solutions with less computation burden. The DEED problem is more intractable than the EED and DED problems (Zou et al., 2017, 2018), since it adds both the generating unit ramp-rate constraints and the emission function to the original economic load dispatch problem.Moreover, all the power outputs should be suitably determined so as to achieve the best compromise solution while keeping all the constraints satisfied.To obtain high-quality DEED solutions, both the performance improvement of heuristic algorithm and the design of constraint handling strategy are involved in this paper.In summary, the main contribution of our work can be stated as follows: First, a new enhanced harmony search (NEHS) algorithm is proposed to improve the optimization performance of the HS algorithm, which is beneficial for conducting global exploration and searching potential DEED solutions in the solution space.Second, a repair technique is designed to further refine the solutions of NEHS, which is helpful to the feasibility maintenances and quality improvements of the obtained solutions.
In this work, twenty-three pieces of high-fired glazed samples from the archaeological excavations of the Dong Xia sites in Russia's Primorye region were studied. In addition to visual observation, energy dispersive X-ray fluorescence (EDXRF) and field emission scanning electron microscopy (FESEM) techniques were jointly applied to the analysis of the chemical composition and microstructure of the samples. The major emphasis was placed on the provenance identification of the white, opalescent glazed, and celadon samples. Seven excavated fine whiteware samples were confirmed as the products from the Ding Kiln in the Jin Dynasty, and nine samples as the Jun-series wares. The most important conclusion out of the study is that two celadon samples with opaque glaze were proved to be the Ru Kiln wares. This finding implies that the Ru ware is identified for the first time in the Dong Xia sites of Russia's Primorye region. This work is a successful case of interdisciplinary cooperation in the medieval archaeometry of the Far East. This work is devoted to the high-fired glazed wares excavated from the Late Jin Dynasty (Dong Xia State) sites in Russia's Primorye Region.The Jin Dynasty lasted for 120 years (1115–1234 CE) and was first founded by Jurchen people.During its peak evolution, the Jin Empire covered an extensive area geographically, including the north of Huaihe River, the northeast of Qinling Mountains in China, and Russia's Far East of today.The economy of the Jin Dynasty was inherited mainly from the Northern Song Dynasty (960–1127 CE), and the ceramics and iron-smelting industries were prosperous.Russia's Primorye region of today was once under the jurisdiction of “Xu Pin Lu”, an administrative division of the Jin Dynasty.From 1215 to 1233, this region was controlled by the Dong Xia State, a short-lived kingdom established in Northeast China by Jurchen warlord Puxian Wannu in 1215 during the Mongol conquest of the Jin Dynasty.It was eventually conquered by the Mongolians and was later put under the Liaoyang province by the Yuan Dynasty (1271–1368 CE).The locations of the capital of “Xu Pin Lu” of the Jin Dynasty and the capital Kai Yuan City of the Dong Xia State were tracked down to today's Krasny Yar of Ussuriisk in the downstream of the Suifen River.Over 30 mountain-walled towns, such as Krasny Yar, Shaiga, and Anan'evka etc., make up the main body of the relic sites of Russia's Primorye region (Artemieva and Usuki, 2010; Khorev, 2012).Most of the mountain-walled towns discovered in Russia's Primorye region were dated to the period of the Dong Xia.The high-fired glazed wares excavated from these sites shed new light on understanding people's lives in “Xu Pin Lu” and the Dong Xia State, and their commercial and cultural exchanges with the inland regions of China (Peng, 2016).
In this work, twenty-three pieces of high-fired glazed samples from the archaeological excavations of the Dong Xia sites in Russia's Primorye region were studied. In addition to visual observation, energy dispersive X-ray fluorescence (EDXRF) and field emission scanning electron microscopy (FESEM) techniques were jointly applied to the analysis of the chemical composition and microstructure of the samples. The major emphasis was placed on the provenance identification of the white, opalescent glazed, and celadon samples. Seven excavated fine whiteware samples were confirmed as the products from the Ding Kiln in the Jin Dynasty, and nine samples as the Jun-series wares. The most important conclusion out of the study is that two celadon samples with opaque glaze were proved to be the Ru Kiln wares. This finding implies that the Ru ware is identified for the first time in the Dong Xia sites of Russia's Primorye region. This work is a successful case of interdisciplinary cooperation in the medieval archaeometry of the Far East. The archaeology of Russia's Far East, which is closely associated with the cultural relics studies of the XII-XIII century AD, had experienced a qualitative breakthrough in 1963 when the Russian scholar E.V. Shavkunov for the first time excavated the unique Shaiga site in the Primorye region.This relic site became the benchmark for the Medieval Archaeology in Russia's Far East.Many of these sites were discarded after usage for about 20 years.Fortress sites and fortification system used for defending the invasion of the Mongolians were discovered.Jurchen people created their unique traditions and culture, which were passed down even after the downfall of its political regime (Jilin Provincial Institute of Cultural Relics and Archaeology, Institute of History, Archaeology and Ethnography of the Peoples of the Far East, 2013).
In this work, twenty-three pieces of high-fired glazed samples from the archaeological excavations of the Dong Xia sites in Russia's Primorye region were studied. In addition to visual observation, energy dispersive X-ray fluorescence (EDXRF) and field emission scanning electron microscopy (FESEM) techniques were jointly applied to the analysis of the chemical composition and microstructure of the samples. The major emphasis was placed on the provenance identification of the white, opalescent glazed, and celadon samples. Seven excavated fine whiteware samples were confirmed as the products from the Ding Kiln in the Jin Dynasty, and nine samples as the Jun-series wares. The most important conclusion out of the study is that two celadon samples with opaque glaze were proved to be the Ru Kiln wares. This finding implies that the Ru ware is identified for the first time in the Dong Xia sites of Russia's Primorye region. This work is a successful case of interdisciplinary cooperation in the medieval archaeometry of the Far East. At the Dong Xia sites in Russia's Primorye region of today, high-fired glazed wares were the common excavated relics.E.I. Gelman, a Russian scholar, provided valuable details regarding their type, number, style, chronological sequence, and the possible kilns that produced the excavated high-fired glazed wares (Gelman, 2005, 1999).According to the observation of Chinese archaeologist Peng Shanguo (Peng, 2013), the excavated greenish-whitewares came from the Hutian kiln in Jingdezhen of the Southern Song Dynasty (1127–1279 CE), and the wares with a glaze similar to that of the Jun wares were considered to have been produced by kilns in Henan, China.The coarse whiteware with engobe, the black-and-whiteware (underglaze ware with black patterns painted on the white engobe), and the black-glazed ware were attributed to the Jiang Guan Tun Kiln in Liaoyang, Liaoning Province of China.The excavated high-fired glazed wares included a large share of fine whiteware produced by the Ding Kiln in Quyang, Hebei Province of China (Peng, 2007).
In this work, twenty-three pieces of high-fired glazed samples from the archaeological excavations of the Dong Xia sites in Russia's Primorye region were studied. In addition to visual observation, energy dispersive X-ray fluorescence (EDXRF) and field emission scanning electron microscopy (FESEM) techniques were jointly applied to the analysis of the chemical composition and microstructure of the samples. The major emphasis was placed on the provenance identification of the white, opalescent glazed, and celadon samples. Seven excavated fine whiteware samples were confirmed as the products from the Ding Kiln in the Jin Dynasty, and nine samples as the Jun-series wares. The most important conclusion out of the study is that two celadon samples with opaque glaze were proved to be the Ru Kiln wares. This finding implies that the Ru ware is identified for the first time in the Dong Xia sites of Russia's Primorye region. This work is a successful case of interdisciplinary cooperation in the medieval archaeometry of the Far East. Previous conjectures about the kilns that produced the wares excavated from the Dong Xia sites in Russia's Primorye region were mostly based on the visual comparative observation with no in-depth scientific investigations.However, the cutting-edge development trend of archaeology is the scientific and technical archaeology, which, in particular, allows one to determine the provenance using the state-of-the-art technologies.
Pyrometamorphic rocks produced by natural coal combustion appear at archaeological sites across North America but have received little archaeological attention regarding provenance studies. Tertiary Hills Clinker is a distinct pyrometamorphic rock from Subarctic Canada utilized by hunter-gatherers from 10,000 years ago to European contact. We employ X-ray diffraction, thin section analyses, and electron probe microanalyses to characterise Tertiary Hills Clinker and inform archaeometric studies of rock produced by combustion metamorphism. We geochemically compare pyrometamorphic rocks used by pre-contact people across North America to demonstrate that Tertiary Hills Clinker can be sourced using portable X-ray fluorescence. Results indicate that Late Pleistocene/Early Holocene exchange networks in North America were larger than previously thought. A later change in the distribution of Tertiary Hills Clinker may relate to a Late Holocene volcanic eruption (White River Ash east) that fragmented modes of lithic exchange and associated social networks with potential stimulus for a subsequent large-scale migration of northern hunter-gatherers across the continent. Provenance studies of pyrometamorphic artifacts offer untapped opportunities to study social networks in coal-bearing regions across the world. Pyrometamorphism (also called combustion metamorphism) generally occurs when coal, oil, or gas burn with sufficient energy to bake or fuse neighbouring rock (Allen, 1874; Bentor et al., 1981; Cosca et al., 1989; Grapes, 2011:21; Stracher et al., 2010).Beds of fused rock were targeted for stone tool production because of the raw materials' internal uniformity (Cinq-Mars, 1973; Clark, 1986; Curran et al., 2001; Fredlund, 1976).Pyrometamorphic rock has received geological attention in North and South America (Hefern and Coates, 2004; Henao et al., 2010), Europe (Žáček et al., 2015), Russia (Sokol et al., 1998), China (Song and Kuenzer, 2017), and Africa (Pone et al., 2017).Despite the global distribution of rock produced by coal combustion and the use of it by people, comparatively few efforts have been made by archaeologists to formally identify pyrometamorphic rock in archaeological assemblages (Estes et al., 2010; Hughes and Peterson, 2009; Le Blanc, 1997; Vapnik et al., 2015).
Based on the normalized Minkowski distance in Hausdorff metrics, we study the sensitivity of interval-valued Schweizer-Sklar t-norms and their corresponding residual implications. Moreover, we investigate the robustness of interval-valued fuzzy reasoning triple I algorithms based on Schweizer–Sklar operators and illustrate the feasibility of the algorithms by a numerical example. Finally, the interval-valued fuzzy reasoning triple I algorithms are applied to medical diagnosis. As the most fundamental forms of fuzzy reasoning, fuzzy modus ponens (FMP) and fuzzy modus tollens (FMT) can be respectively expressed as follows (Dubois and Prade, 1991; Gottwald, 2001; Wang, 2000):
Based on the normalized Minkowski distance in Hausdorff metrics, we study the sensitivity of interval-valued Schweizer-Sklar t-norms and their corresponding residual implications. Moreover, we investigate the robustness of interval-valued fuzzy reasoning triple I algorithms based on Schweizer–Sklar operators and illustrate the feasibility of the algorithms by a numerical example. Finally, the interval-valued fuzzy reasoning triple I algorithms are applied to medical diagnosis. FMP(A,B,A∗): For given rule A→B and premise A∗, calculate fuzzy consequence B∗.
Based on the normalized Minkowski distance in Hausdorff metrics, we study the sensitivity of interval-valued Schweizer-Sklar t-norms and their corresponding residual implications. Moreover, we investigate the robustness of interval-valued fuzzy reasoning triple I algorithms based on Schweizer–Sklar operators and illustrate the feasibility of the algorithms by a numerical example. Finally, the interval-valued fuzzy reasoning triple I algorithms are applied to medical diagnosis. FMT(A,B,B∗): For given rule A→B and premise B∗, calculate fuzzy consequence A∗.
Based on the normalized Minkowski distance in Hausdorff metrics, we study the sensitivity of interval-valued Schweizer-Sklar t-norms and their corresponding residual implications. Moreover, we investigate the robustness of interval-valued fuzzy reasoning triple I algorithms based on Schweizer–Sklar operators and illustrate the feasibility of the algorithms by a numerical example. Finally, the interval-valued fuzzy reasoning triple I algorithms are applied to medical diagnosis. In the above models, A,A∗∈F(X) and B,B∗∈F(Y), where F(X) and F(Y) stand for the sets of all fuzzy subsets of the universes X and Y respectively.
Based on the normalized Minkowski distance in Hausdorff metrics, we study the sensitivity of interval-valued Schweizer-Sklar t-norms and their corresponding residual implications. Moreover, we investigate the robustness of interval-valued fuzzy reasoning triple I algorithms based on Schweizer–Sklar operators and illustrate the feasibility of the algorithms by a numerical example. Finally, the interval-valued fuzzy reasoning triple I algorithms are applied to medical diagnosis. Compositional rule of inference (CRI) is proposed to deal with FMP and FMT in Zadeh (1973, 1975a, b, c).As pointed by Wang in Wang (1999b, a), the composition operation is short of clear logic meaning, then Wang proposed a new method, the full implication triple I method, or simply the triple I method, for fuzzy reasoning in Wang (1999a).This method brings fuzzy reasoning within the framework of logical semantic implication.Consequently, some studies on the triple I method have been reported in recent years.Luo and Yao proposed the Schweizer–Sklar fuzzy reasoning triple I methods in Luo and Yao (2013).
Based on the normalized Minkowski distance in Hausdorff metrics, we study the sensitivity of interval-valued Schweizer-Sklar t-norms and their corresponding residual implications. Moreover, we investigate the robustness of interval-valued fuzzy reasoning triple I algorithms based on Schweizer–Sklar operators and illustrate the feasibility of the algorithms by a numerical example. Finally, the interval-valued fuzzy reasoning triple I algorithms are applied to medical diagnosis. Since Zadeh proposed the concept of fuzzy sets in Zadeh (1965), fuzzy theories and fuzzy methods have been successfully applied to many domains (Dubois and Prade, 1991; Cai, 1995, 2001; Li et al., 2014).As a generation of fuzzy set, interval-valued fuzzy set can effectively reduce the loss of fuzzy information and reflect the fuzziness and uncertainty of the results obtained in information processing.In Deschrijver and Kerre (2003), it is shown that intuitionistic fuzzy set is equivalent to interval-valued fuzzy set.Due to the advantages of interval-valued fuzzy set, it has been favored by many scholars.Some scholars have used interval-valued fuzzy set to solve the problems of decision-making, medical diagnosis, neural networks and control systems (De et al., 2001a; Luo and Zhao, 2018; Liu et al., 2018; Wu et al., 2019; Dong et al., 2013; Castro et al., 2009; Hsiao et al., 2008).Some scholars have studied fuzzy reasoning algorithms based on interval-valued fuzzy set.In 2011, Li et al. first extended compositional rule of inference to the case of interval-valued fuzzy set in Li et al. (2011).Later, Luo et al. extended the fuzzy reasoning triple I method to the case of interval-valued fuzzy sets in Luo and Zhang (2015), Luo and Zhou (2015), Luo et al. (2016), Luo and Cheng (2016) and Luo and Liu (2017).
Based on the normalized Minkowski distance in Hausdorff metrics, we study the sensitivity of interval-valued Schweizer-Sklar t-norms and their corresponding residual implications. Moreover, we investigate the robustness of interval-valued fuzzy reasoning triple I algorithms based on Schweizer–Sklar operators and illustrate the feasibility of the algorithms by a numerical example. Finally, the interval-valued fuzzy reasoning triple I algorithms are applied to medical diagnosis. The perturbation and robustness of fuzzy reasoning are highly related to one other.For a fuzzy reasoning method, if a small perturbation of input causes a small variance of the output, then we say this method has a good behavior of robustness.Distance measures play an important role in robustness of fuzzy inference.Szmidt and Kacprzyk proposed the normalized Minkowski distance between intuitionistic fuzzy sets in Szmidt and Kacprzyk (1997, 2000), which are generalizations of the well known normalized Hamming distance, normalized Euclidean distance and Moore distance.Normalized Minkowski distance based on the Hausdorff metrics for interval-valued fuzzy sets was proposed in Zhang (2006).Li et al. (2011) and Luo and Zhang (2015), Luo and Zhou (2015), Luo et al. (2016) and Luo and Cheng (2016) discussed the robustness of interval-valued fuzzy reasoning with the Moore distance.Luo et al. discussed the robustness of interval-valued fuzzy reasoning triple I method with normalized Minkowski distance based on Hausdorff metrics in Luo and Liu (2017).
Based on the normalized Minkowski distance in Hausdorff metrics, we study the sensitivity of interval-valued Schweizer-Sklar t-norms and their corresponding residual implications. Moreover, we investigate the robustness of interval-valued fuzzy reasoning triple I algorithms based on Schweizer–Sklar operators and illustrate the feasibility of the algorithms by a numerical example. Finally, the interval-valued fuzzy reasoning triple I algorithms are applied to medical diagnosis. Although great achievements have been made in interval-valued fuzzy reasoning triple I methods, its application is still blank.Therefore, in this paper, we investigate the robustness of interval-valued fuzzy reasoning triple I methods based on Schweizer–Sklar operators by using the normalized Minkowski distance based on Hausdorff metrics and use interval-valued fuzzy reasoning triple I methods to solve the problem of medical diagnosis.
Depressive disorders in epilepsy often present characteristic clinical manifestations atypical in primary, endogenous depression. Here, we report a case of a 64-year-old woman with right mesial temporal lobe epilepsy, who complained of bizarre, antipsychotic-refractory cenesthetic hallucinations in her interictal phase, and was hospitalized after a suicide attempt. Detailed clinical observations revealed mood symptoms, which led to the diagnosis of interictal dysphoric disorder comorbid with interictal psychosis. Sertraline with low-dose aripiprazole markedly alleviated both depressive and psychotic symptoms. This case suggested that the two diagnostic entities may overlap and that depressive symptoms tend to be concurrent when concurring with psychosis, which hampers the appropriate choice of a treatment option. Patients with epilepsy have a higher incidence of psychiatric comorbidities than does the general population [1].Psychiatric disorders associated with epilepsy are classified into those with a temporal relationship to seizures (periictal) and those without (interictal).Periictal psychiatric disorders can further be classified into those occurring during (ictal), preceding (preictal), and following (postictal) seizures.A large-scale study involving the general population revealed that depression was comorbid in 18% of adults with epilepsy, and psychosis in 9% [1].Additionally, a meta-analysis that included studies of four populations found the prevalence of lifetime depression in patients with epilepsy to be 13.0% (95% CI: 5.1–33.1) [2].A community-based study comprising patients with epilepsy identified a higher incidence of depression in patients with recurrent seizures than in those in remission [3].
Depressive disorders in epilepsy often present characteristic clinical manifestations atypical in primary, endogenous depression. Here, we report a case of a 64-year-old woman with right mesial temporal lobe epilepsy, who complained of bizarre, antipsychotic-refractory cenesthetic hallucinations in her interictal phase, and was hospitalized after a suicide attempt. Detailed clinical observations revealed mood symptoms, which led to the diagnosis of interictal dysphoric disorder comorbid with interictal psychosis. Sertraline with low-dose aripiprazole markedly alleviated both depressive and psychotic symptoms. This case suggested that the two diagnostic entities may overlap and that depressive symptoms tend to be concurrent when concurring with psychosis, which hampers the appropriate choice of a treatment option. The clinical association between epilepsy and depression had been recognized as early as 400 BC, when Hippocrates highlighted their comorbidity (reviewed in [4]).Kraepelin noted that interictal mood disorders in epilepsy often present atypical manifestations, such as irritability intermixed with euphoric moods, fear, anxiety, anergia, pain, and insomnia (reviewed in [4,5]).These patients often fail to meet conventional criteria such as the International Classification of Diseases or Diagnostic and Statistical Manual of Mental Disorders [6].Blumer coined the term interictal dysphoric disorder to re-define this atypical mood disorder, stressing eight symptoms in three categories for operational diagnosis [5].In recent years, the importance of diagnosing and treating mood disorders in epilepsy has been recognized, following the discovery that depression, and not seizure frequency, is the major predictor of quality of life in treatment-refractory epilepsy [7].Even milder depressive episodes increase the risk of suicide, cause an adverse effect on the quality of life and seizure control, and increase the healthcare cost, irrespective of seizure severity or duration [8].For this reason, self-report inventories, such as the Neurological Disorders Depression Inventory for Epilepsy (NDDI-E) for the screening of depression in epilepsy [9] and Interictal Dysphoric Disorder Inventory (IDDI) for standardized diagnosis of IDD [10], were developed.
Depressive disorders in epilepsy often present characteristic clinical manifestations atypical in primary, endogenous depression. Here, we report a case of a 64-year-old woman with right mesial temporal lobe epilepsy, who complained of bizarre, antipsychotic-refractory cenesthetic hallucinations in her interictal phase, and was hospitalized after a suicide attempt. Detailed clinical observations revealed mood symptoms, which led to the diagnosis of interictal dysphoric disorder comorbid with interictal psychosis. Sertraline with low-dose aripiprazole markedly alleviated both depressive and psychotic symptoms. This case suggested that the two diagnostic entities may overlap and that depressive symptoms tend to be concurrent when concurring with psychosis, which hampers the appropriate choice of a treatment option. Controversy exists on the reliability of the IDDI as a diagnostic tool.Additionally, whether IDD can be considered an independent, epilepsy-specific diagnostic entity is debated [11].In fact, IDD is highly comorbid with major depression and is as prevalent in migraine as it is in epilepsy [10].This suggests that characteristic symptoms of IDD are shared among different types of depression with different etiologies.
Depressive disorders in epilepsy often present characteristic clinical manifestations atypical in primary, endogenous depression. Here, we report a case of a 64-year-old woman with right mesial temporal lobe epilepsy, who complained of bizarre, antipsychotic-refractory cenesthetic hallucinations in her interictal phase, and was hospitalized after a suicide attempt. Detailed clinical observations revealed mood symptoms, which led to the diagnosis of interictal dysphoric disorder comorbid with interictal psychosis. Sertraline with low-dose aripiprazole markedly alleviated both depressive and psychotic symptoms. This case suggested that the two diagnostic entities may overlap and that depressive symptoms tend to be concurrent when concurring with psychosis, which hampers the appropriate choice of a treatment option. In addition to the effect of psychosocial disadvantage, underlying common etiologies have been suggested for epilepsy and mood disorders such as changes in cortical and subcortical anatomical structures, neurotransmitter activities (reviewed in [12,13]), and sclerosis in the limbic system [14],although much remains to be understood.For example, the effects of lateralization of epileptic foci on depression severity are still unclear (reviewed in [14]).
Depressive disorders in epilepsy often present characteristic clinical manifestations atypical in primary, endogenous depression. Here, we report a case of a 64-year-old woman with right mesial temporal lobe epilepsy, who complained of bizarre, antipsychotic-refractory cenesthetic hallucinations in her interictal phase, and was hospitalized after a suicide attempt. Detailed clinical observations revealed mood symptoms, which led to the diagnosis of interictal dysphoric disorder comorbid with interictal psychosis. Sertraline with low-dose aripiprazole markedly alleviated both depressive and psychotic symptoms. This case suggested that the two diagnostic entities may overlap and that depressive symptoms tend to be concurrent when concurring with psychosis, which hampers the appropriate choice of a treatment option. Psychosis in epilepsy has been known since the times of ancient Greece.However, it was not until the mid-20th century that “schizophrenia-like psychosis” was formally reported with characteristics such as the absence of cognitive deterioration [15].The current definition of IIP was formalized as psychosis in clear consciousness in patients who have been diagnosed with epilepsy, where the psychosis occurs not exclusively during or immediately following a seizure (reviewed in [16,17]).The diagnosis of IIP relies on psychiatric assessment by clinicians, and although IIP is relatively infrequent compared with mood disorders, the symptoms of IIP markedly affect the daily lives of patients and families.There are other forms of psychosis in epilepsy that can be distinguished from IIP based on their temporal relationship with seizures.Ictal psychosis is not common, and is usually brief, stereotyped, and often associated with subtle automatisms [18].In contrast, postictal psychosis typically occurs within the 72 h following a seizure and is characterized by a “lucid interval” between seizure cessation and psychosis onset [19].
Depressive disorders in epilepsy often present characteristic clinical manifestations atypical in primary, endogenous depression. Here, we report a case of a 64-year-old woman with right mesial temporal lobe epilepsy, who complained of bizarre, antipsychotic-refractory cenesthetic hallucinations in her interictal phase, and was hospitalized after a suicide attempt. Detailed clinical observations revealed mood symptoms, which led to the diagnosis of interictal dysphoric disorder comorbid with interictal psychosis. Sertraline with low-dose aripiprazole markedly alleviated both depressive and psychotic symptoms. This case suggested that the two diagnostic entities may overlap and that depressive symptoms tend to be concurrent when concurring with psychosis, which hampers the appropriate choice of a treatment option. Similar to depression in epilepsy, the pathophysiological basis of psychosis in epilepsy is largely unknown.Candidate mechanisms include aberrant neurotransmitter systems, hippocampal circuits, and autoimmune etiologies including those related to N-methyl-d-aspartate (NMDA) receptors (reviewed in [13]).
Primary Angiitis of the central nervous system is a rare and poorly understood variant of vasculitis. We narrate a case of a 46-year-old male who presented with new onset refractory status epilepticus mimicking autoimmune encephalitis. In this case we are reporting clues that could be useful for diagnosis and extensive literature review on the topic. New onset refractory status epilepticus (NORSE) is a complex disorder, characterized by status epilepticus that is refractory to treatment with no identifiable infectious, inflammatory or brain structural abnormalities [1].NORSE poses considerable distress to physicians due to its heterogeneous etiology and devastating outcome [2].An identifiable cause in patients with NORSE is discovered in the majority of cases, however in some the cause remains unknown despite extensive investigations [3].Primary Angiitis of the central nervous system (PACNS) presenting with seizures was reported in the range of 7–29% [4,5].
Primary Angiitis of the central nervous system is a rare and poorly understood variant of vasculitis. We narrate a case of a 46-year-old male who presented with new onset refractory status epilepticus mimicking autoimmune encephalitis. In this case we are reporting clues that could be useful for diagnosis and extensive literature review on the topic. PACNS is described as an entity under the “umbrella” of central nervous system (CNS) vasculitis that is confined to the central nervous system [6].It is an extremely rare disease with an annual incidence of 2.4 cases per 1,000,000 [7].PACNS has been reported with the greatest frequency in North America [5,8], Europe [9,10,11] and Australia [12].Literature is lacking publications about PACNS in Saudi Arabia.The only case in the literature is of a patient with CNS vasculitis complicating a primary immunodeficiency disorder [13].
Silver and gold artefacts from a 12–13th c. CE tumulus in Senegal were recently analysed by means of XRF and LA-ICP-MS. The identification of major, minor and trace elements allowed gaining a rare insight into the composition of precious metals circulating in that part of Africa in medieval times. The results show that all objects were made of polymetallic alloys. Comparisons with analyses from other West African as well as North African and European silver and gold artefacts suggest that the metal from the studied objects most probably originate from outside West Africa. At least in the case of the gold artefacts, this is quite surprising, as we expected to come across pure West African gold. The results hint at a hitherto little known facet of the medieval African gold trade. It is concluded that, in addition to the acknowledged mass export of West African raw gold toward North Africa, there was also a thus far widely ignored traffic of alloyed gold (and silver) into West Africa. Gold is among the most legendary export commodities of the African continent.Probably first evoked by Herodotus' 6th century BCE account of ‘silent’ bartering for gold by Carthaginians somewhere along the Atlantic coast, the myths of abundance and easy acquisition of this metal from inner Africa never ceased to exist.There is still much scholarly debate about the true antiquity, scale and impact of that trade (e.g., Devisse, 1972; Cahen, 1979; Garrard, 1982; Devisse, 1993; Nixon et al., 2011).Archaeology has had thus far relatively little to contribute to the debate.On the one hand, this is a result of the still comparatively low number and range of archaeological studies specifically devoted to investigate the exploitation, processing and trade of precious metals in the continent.On the other, gold and silver surprisingly appear to be late additions to the material canon of African, but especially West African, cultural expressions.In contrast to Eurasia, where gold was already in use by the mid- to late 5th millennium BCE (e.g., at the Bulgarian sites of Durankulak and Varna, see Krauß et al., 2014; Leusch et al., 2015), none of the archaeological gold or silver finds yet known from West Africa stems from a context older than the 8th century CE, and the majority is indeed considerably younger (Table 1).
Silver and gold artefacts from a 12–13th c. CE tumulus in Senegal were recently analysed by means of XRF and LA-ICP-MS. The identification of major, minor and trace elements allowed gaining a rare insight into the composition of precious metals circulating in that part of Africa in medieval times. The results show that all objects were made of polymetallic alloys. Comparisons with analyses from other West African as well as North African and European silver and gold artefacts suggest that the metal from the studied objects most probably originate from outside West Africa. At least in the case of the gold artefacts, this is quite surprising, as we expected to come across pure West African gold. The results hint at a hitherto little known facet of the medieval African gold trade. It is concluded that, in addition to the acknowledged mass export of West African raw gold toward North Africa, there was also a thus far widely ignored traffic of alloyed gold (and silver) into West Africa. To date, most of the West African archaeological evidence of gold and silver come from some few burial mounds in Senegal (Rao-Nguigelah and Massar: Joire, 1955; Ndalane: Thilmans and Descamps, 1972) and Nigeria (Durbi Takusheyi: Gronenborn et al., 2012), or from urban sites linked with the early Arab-Islamic trans-Saharan trade in Mauretania and Mali: Tegdaoust, the putative archaeological site of Awdaghost (Robert and Devisse, 1970), Djenné Jeno (McIntosh, 1995), and Essouk-Tadmekka (Nixon, 2017) (Fig. 1).It is not surprising that around the same time when gold was demonstrably in use at the latter places, early Islamic scholars first mention this part of Africa as particularly rich in gold – such as Al-Fazari, writing in the 8th century CE, and Al-Yaqubi in the late 9th c. CE (Levtzion and Hopkins, 1981).At their time and in the subsequent centuries, the kingdom of Ghana and the nearby mythical island of Wangara became famous for the sheer abundance and alleged high purity of the gold found there.Gold, at latest from then on, became one of the main factors of the thriving trans-Saharan trade, given its vital importance in the economy of the Islamized regions of the continent.Gold ore deposits are indeed found in West Africa; particularly the gold-rich regions of Buré and Bambuk at the upper Senegal/Falemmé and Niger rivers, the greater Volta region, as well as the Sirba and other tributaries to the Niger River at the Eastern Niger Bend (Fig. 1) have been seen as likely sources of the gold found at the above mentioned sites and traded to North Africa in medieval times.Those regions are part of the West African craton with its extended Birrimian formation and other greenstone belts, bearing most of the gold found in West Africa.Gold from those deposits, particularly if panned from rivers and streams, is rather free of impurities and can reach Au-contents of >97 wt% (Gondonneau et al., 2001; Magnavita and Magnavita, 2018).This certainly reminds us of the purity of West African gold as praised by medieval medieval Arab authors such as Al-Bakri (Levtzion and Hopkins, 1981: 69), who stated in the 11th c. CE that “[t]he gold of Awdaghust is better and purer than that of any other people on earth”.The only other metal commonly found as an impurity in West African placer gold is silver, though usually in quantities of only a few percent.Given the rarity of native silver or silver-rich ores in West Africa, it is indeed highly likely that silver found in the West African archaeological record was produced elsewhere (Joire, 1955; Herbert, 1984; Benhsain and Devisse, 2000).
In this article we draw on suites of new information to reinterpret the date of the Java Sea Shipwreck. The ship was a Southeast Asian trading vessel carrying a large cargo of Chinese ceramics and iron as well as luxury items from outside of China, such as elephant tusks and resin. Initially the wreck, which was recovered in Indonesia, was placed temporally in the mid- to late 13th century based on a single radiocarbon sample and ceramic styles. We employ new data, including multiple radiocarbon dates and inscriptions found on some of the ceramics, to suggest that an earlier chronological placement be considered. The Java Sea Shipwreck was recovered in Indonesia in 1996 (Fig. 1).The ship was a trading vessel thought to have been sailing from Quanzhou in southern China to Indonesia, possibly Tuban on the island of Java (Flecker, 2003).The timing of this voyage has been debated; an issue that we address here.In this article, we use several new lines of evidence to argue that the Java Sea Shipwreck vessel may have sailed almost a century before previously thought (Flecker, 2003), possibly as early as 1162 CE.The diverse suite of data we consider includes comparative ceramic finds from land and underwater wreck sites, new radiocarbon dates, and inscriptions on ceramic pieces.
In this article we draw on suites of new information to reinterpret the date of the Java Sea Shipwreck. The ship was a Southeast Asian trading vessel carrying a large cargo of Chinese ceramics and iron as well as luxury items from outside of China, such as elephant tusks and resin. Initially the wreck, which was recovered in Indonesia, was placed temporally in the mid- to late 13th century based on a single radiocarbon sample and ceramic styles. We employ new data, including multiple radiocarbon dates and inscriptions found on some of the ceramics, to suggest that an earlier chronological placement be considered. The vessel that would become the Java Sea Shipwreck was part of a long and dynamic tradition of maritime trade in Southeast Asian waters (see Christie, 1998; Hall, 2011; Manguin, 2004; Wheatley, 1961).The main cargo of the vessel consisted of Chinese ceramics and cast iron.The ship also carried a number of fine-paste-ware kendis thought to be from southern Thailand, resin, elephant tusks, and an assortment of artifacts in lesser quantities that probably belonged to the crew or passengers on board.Based on ceramic styles, Brown (1997) estimated the date of the wreck to be the mid- to late 13th century.In 1997, as part of the initial research program by Pacific Sea Resources, the company that undertook the final recovery of the wreck, a single sample of resin from the wreck site was sent for standard radiocarbon dating (Flecker, 1997a; Mathers and Flecker, 1997, Appendix H).It was concluded that the dates from the radiometric analysis supported a mid- to late-13th-century assignment, which would correspond to the beginning of the Yuan dynasty (c. 1271 CE) in China and, on Java, the fall of the Singasari kingdom and the rise of the Majapahit Empire.
Based on Cuckoo Search (CS) and Differential Evolution (DE), a novel hybrid optimization algorithm, called CSDE, is proposed in this paper to solve constrained engineering problems. CS has strong ability on global search and less control parameters, but easy to suffer from premature convergence and lower the density of population. DE specializes in local search and good robustness, however, its convergence rate is too late to find the satisfied solution. Furthermore, these two algorithms are both proved to be especially suitable for engineering problems. This work divides population into two subgroups and adopts CS and DE for these two subgroups independently. By division, these two subgroups can exchange useful information and these two algorithms can utilize each other’s advantages to complement their shortcoming, thus avoid premature convergence, balance the quality of solution and the computation consumption, and find satisfactory global optima. Due to the tremendous design variables and constrained conditions of engineering problems, single optimizer failed to meet the requirement of precision, so hybrid optimization algorithms (such like CSDE) is the most promising mean to complete this job. Simulation results reveal that CSDE has more ability to find promising results than other 12 algorithms (including traditional algorithms and state-of-the-art algorithm) on 30 unconstrained benchmark functions, 10 constrained benchmark functions and 6 constrained engineering problems. In real life, there are many real-world engineering optimization problems that need to be solved.The actual engineering problem has some practical constraints and one or more objective functions (Dhiman and Kumar, 2018b).The purpose is to find a set of parameter values to minimize the value of the objective function, where the constraint can be either an equality constraint or an inequality constraint.Due to the large number of these problems and their practical application value, it has become one of the research hotspots in recent years.
Based on Cuckoo Search (CS) and Differential Evolution (DE), a novel hybrid optimization algorithm, called CSDE, is proposed in this paper to solve constrained engineering problems. CS has strong ability on global search and less control parameters, but easy to suffer from premature convergence and lower the density of population. DE specializes in local search and good robustness, however, its convergence rate is too late to find the satisfied solution. Furthermore, these two algorithms are both proved to be especially suitable for engineering problems. This work divides population into two subgroups and adopts CS and DE for these two subgroups independently. By division, these two subgroups can exchange useful information and these two algorithms can utilize each other’s advantages to complement their shortcoming, thus avoid premature convergence, balance the quality of solution and the computation consumption, and find satisfactory global optima. Due to the tremendous design variables and constrained conditions of engineering problems, single optimizer failed to meet the requirement of precision, so hybrid optimization algorithms (such like CSDE) is the most promising mean to complete this job. Simulation results reveal that CSDE has more ability to find promising results than other 12 algorithms (including traditional algorithms and state-of-the-art algorithm) on 30 unconstrained benchmark functions, 10 constrained benchmark functions and 6 constrained engineering problems. With the rapid development of modern social science and technology, the requirements of the engineering field are getting more complex, and the limitations of the problem to be solved are increasing.The characteristics of these problems are mainly large-scale and high-difficult.When the traditional optimization algorithm solves such a large-scale problem, it cannot meet the standard in terms of calculation speed, convergence speed and optimization accuracy.
Based on Cuckoo Search (CS) and Differential Evolution (DE), a novel hybrid optimization algorithm, called CSDE, is proposed in this paper to solve constrained engineering problems. CS has strong ability on global search and less control parameters, but easy to suffer from premature convergence and lower the density of population. DE specializes in local search and good robustness, however, its convergence rate is too late to find the satisfied solution. Furthermore, these two algorithms are both proved to be especially suitable for engineering problems. This work divides population into two subgroups and adopts CS and DE for these two subgroups independently. By division, these two subgroups can exchange useful information and these two algorithms can utilize each other’s advantages to complement their shortcoming, thus avoid premature convergence, balance the quality of solution and the computation consumption, and find satisfactory global optima. Due to the tremendous design variables and constrained conditions of engineering problems, single optimizer failed to meet the requirement of precision, so hybrid optimization algorithms (such like CSDE) is the most promising mean to complete this job. Simulation results reveal that CSDE has more ability to find promising results than other 12 algorithms (including traditional algorithms and state-of-the-art algorithm) on 30 unconstrained benchmark functions, 10 constrained benchmark functions and 6 constrained engineering problems. Recently, inspired by natural laws, engineering scholars have proposed a series of meta-heuristic optimization algorithms or search group algorithm (Noorbin and Alfi, 2018), such as Genetic Algorithm (GA) (Bianco et al., 2017), Simulated Annealing (SA) (Geng et al., 2015), Ant Colony Optimization (ACO) (Engin and Güçlü, 2018), Firefly Algorithm (FFA) (Mousavi and Alfi, 2018), Tabu Search (TS) (Abdel-Basset et al., 2018), Particle Swarm Optimization (PSO) (Shahri et al., 2019), Differential Evolution (DE) (Tey et al., 2018), Harmony Search (HS) (Talarposhti and Jamei, 2016), Artificial Bee Colony (ABC) (Karaboga et al., 2014), Artificial Fish Optimization (AFO) (Rocha et al., 2014), Gravity Search Algorithm (GSA) (Khatibinia and Khosravi, 2014), Bacterial Foraging Algorithm (BFA) (Verma and Parihar, 2017), Cuckoo Search (CS) (Mareli and Twala, 2018), Krill Herd (KH) (Gandomi and Alavi, 2012), Immune Algorithm (IA) (Hong, 2012; Hong et al., 2011), Salp Swarm Algorithm (SSA) (Mirjalili et al., 2017), Grasshopper Optimization Algorithm (GOA) (Saremi et al., 2017), Dragonfly Algorithm (DA) (Mirjalili, 2016), Electro-Search algorithm (ESA) (Tabari and Ahmad, 2017), Pathfinder Algorithm (PFA) (Yapici and Cetinkaya, 2019), Cheetah based Optimization Algorithm (CBA) (Klein et al., 2018), Meerkats-inspired Algorithm (MEA) (Klein and dos Santos Coelho, 2018), Emperor Penguin Optimizer (EPO) (Dhiman and Kumar, 2018a), Falcon Optimization Algorithm (FOA) (Segundo et al., 2019), Seeker Optimization Algorithm (SOA) (Dai et al., 2009), Particle Collision Algorithm (PCA) (Sacco and De Oliveira, 2005), Atom Search Optimization (ASO) (Zhao et al., 2019), Selfish Herd Optimizer (SHO) (Fausto et al., 2017), Mouth Brooding Fish algorithm (MBF) (Jahani and Chizari, 2018), Saplings Growing up Algorithm (SGA) (Karci and Alatas, 2006), Bee Collecting Pollen Algorithm (BCPA) (Lu and Zhou, 2008), Yellow Saddle Goatfish Algorithm (YSGA) (Zaldivar et al., 2018), Galactic Swarm Optimization (GSO) (Muthiah-Nakarajan and Noel, 2016), etc.The characteristic of this kind of algorithm is that people can change their optimization strategies based on similar algorithms, so as to improve the efficiency of the solution and quickly obtain a satisfactory optimal solution to effectively solve the complex problems in large projects.Therefore, people start to employ meta-heuristic optimization algorithms to solve constrained engineering problems (Kohli and Arora, 2018; Tam et al., 2018).
Based on Cuckoo Search (CS) and Differential Evolution (DE), a novel hybrid optimization algorithm, called CSDE, is proposed in this paper to solve constrained engineering problems. CS has strong ability on global search and less control parameters, but easy to suffer from premature convergence and lower the density of population. DE specializes in local search and good robustness, however, its convergence rate is too late to find the satisfied solution. Furthermore, these two algorithms are both proved to be especially suitable for engineering problems. This work divides population into two subgroups and adopts CS and DE for these two subgroups independently. By division, these two subgroups can exchange useful information and these two algorithms can utilize each other’s advantages to complement their shortcoming, thus avoid premature convergence, balance the quality of solution and the computation consumption, and find satisfactory global optima. Due to the tremendous design variables and constrained conditions of engineering problems, single optimizer failed to meet the requirement of precision, so hybrid optimization algorithms (such like CSDE) is the most promising mean to complete this job. Simulation results reveal that CSDE has more ability to find promising results than other 12 algorithms (including traditional algorithms and state-of-the-art algorithm) on 30 unconstrained benchmark functions, 10 constrained benchmark functions and 6 constrained engineering problems. However, single meta-heuristic optimization algorithm also exposes some shortcomings.The most common one is that it relies too much on some mathematical formulas, is prone to premature phenomenon, and reduces the accuracy of understanding.Based on this situation, many scholars propose to hybrid different meta-heuristic optimization algorithms (Mousavi and Alfi, 2015; Arab and Alfi, 2015).The hybrid algorithm not only avoids the original shortcomings, but also increases the individual information exchange and population diversity within the population, thus enhancing the ability to solve complex engineering models.
Based on Cuckoo Search (CS) and Differential Evolution (DE), a novel hybrid optimization algorithm, called CSDE, is proposed in this paper to solve constrained engineering problems. CS has strong ability on global search and less control parameters, but easy to suffer from premature convergence and lower the density of population. DE specializes in local search and good robustness, however, its convergence rate is too late to find the satisfied solution. Furthermore, these two algorithms are both proved to be especially suitable for engineering problems. This work divides population into two subgroups and adopts CS and DE for these two subgroups independently. By division, these two subgroups can exchange useful information and these two algorithms can utilize each other’s advantages to complement their shortcoming, thus avoid premature convergence, balance the quality of solution and the computation consumption, and find satisfactory global optima. Due to the tremendous design variables and constrained conditions of engineering problems, single optimizer failed to meet the requirement of precision, so hybrid optimization algorithms (such like CSDE) is the most promising mean to complete this job. Simulation results reveal that CSDE has more ability to find promising results than other 12 algorithms (including traditional algorithms and state-of-the-art algorithm) on 30 unconstrained benchmark functions, 10 constrained benchmark functions and 6 constrained engineering problems. In general, CS has strongly ability on global search and less number of parameters.Existed work has justified that CS superior to many traditional algorithms such as GA and PSO (Yang et al., 2018).In addition, the exploration and exploitation ability of CS are easy to be controlled by dynamically adjust its discovery probability pa and step size ∝ (Yang et al., 2018).It follows that CS is an effective and simple algorithm.However, sometimes CS may easy to trap into local optima and lower the density of population, thus affect its robustness (Yang et al., 2018).DE has a good performance on searching the local optima and good robustness (Mallipeddi et al., 2011; Hongfeng, 2018).Similarly, DE is also has few parameters, and easy to realize in program.The mutation and crossover of DE are polytropic, so its refresh strategies are flexible.Due to these characteristic of DE, it has already excellently applied to many engineering problems.But the convergence of DE is too slow to find satisfied result in given iterations.
Based on Cuckoo Search (CS) and Differential Evolution (DE), a novel hybrid optimization algorithm, called CSDE, is proposed in this paper to solve constrained engineering problems. CS has strong ability on global search and less control parameters, but easy to suffer from premature convergence and lower the density of population. DE specializes in local search and good robustness, however, its convergence rate is too late to find the satisfied solution. Furthermore, these two algorithms are both proved to be especially suitable for engineering problems. This work divides population into two subgroups and adopts CS and DE for these two subgroups independently. By division, these two subgroups can exchange useful information and these two algorithms can utilize each other’s advantages to complement their shortcoming, thus avoid premature convergence, balance the quality of solution and the computation consumption, and find satisfactory global optima. Due to the tremendous design variables and constrained conditions of engineering problems, single optimizer failed to meet the requirement of precision, so hybrid optimization algorithms (such like CSDE) is the most promising mean to complete this job. Simulation results reveal that CSDE has more ability to find promising results than other 12 algorithms (including traditional algorithms and state-of-the-art algorithm) on 30 unconstrained benchmark functions, 10 constrained benchmark functions and 6 constrained engineering problems. Based on the capability of these two algorithms, a novel effectively hybrid method based on CS and DE, called CSDE, is proposed to speed up the convergence rate and enhance the optimization performance of the hybrid algorithm, aimed for obtaining an optimizer with better performance.The original intention of CSDE is that, CS and DE must be treated as a same status; to maximum utilize the advantages of these two algorithms.At the same time, the disadvantages of CS and DE can be remedied by each other, eventually enhance the quality of solution.Compared with single optimizer, CSDE possesses reasonable construct, more ability to handle with high dimension and non-linear problems and has fewer shortcomings to trap into local optima, thus to find lower cost design in complex constrained engineering problems.
Based on Cuckoo Search (CS) and Differential Evolution (DE), a novel hybrid optimization algorithm, called CSDE, is proposed in this paper to solve constrained engineering problems. CS has strong ability on global search and less control parameters, but easy to suffer from premature convergence and lower the density of population. DE specializes in local search and good robustness, however, its convergence rate is too late to find the satisfied solution. Furthermore, these two algorithms are both proved to be especially suitable for engineering problems. This work divides population into two subgroups and adopts CS and DE for these two subgroups independently. By division, these two subgroups can exchange useful information and these two algorithms can utilize each other’s advantages to complement their shortcoming, thus avoid premature convergence, balance the quality of solution and the computation consumption, and find satisfactory global optima. Due to the tremendous design variables and constrained conditions of engineering problems, single optimizer failed to meet the requirement of precision, so hybrid optimization algorithms (such like CSDE) is the most promising mean to complete this job. Simulation results reveal that CSDE has more ability to find promising results than other 12 algorithms (including traditional algorithms and state-of-the-art algorithm) on 30 unconstrained benchmark functions, 10 constrained benchmark functions and 6 constrained engineering problems. Abundant experiments on 30 unconstrained benchmark functions, 10 constrained benchmark functions and 6 constrained engineering problems are carried out to demonstrate the superiority of CSDE.The experimental results show that the proposed CSDE algorithm can obtain more persuasive optima results than other 12 optimization algorithms on unconstrained/constrained benchmark functions and achieve less cost engineering design results than CS, DE and PSO on constrained engineering problems.
Hospitals play an important role towards ensuring proper health treatment to human beings. One of the major challenges faced in this context refers to the increasingly overcrowded patients queues, which contribute to a potential deterioration of patients health conditions. One of the reasons of such an inefficiency is a poor allocation of health professionals. In particular, such allocation process is usually unable to properly adapt to unexpected changes in the patients demand. As a consequence, it is frequently the case where underused rooms have idle professionals whilst overused rooms have less professionals than necessary. Previous works addressed this issue by analyzing the evolution of supply (doctors) and demand (patients) so as to better adjust one to the other, though none of them focused on proposing effective counter-measures to mitigate poor allocations. In this paper, we build upon the concept of smart hospitals and introduce elastic allocation of human resources in healthcare environments (ElHealth), an IoT-focused model able to monitor patients usage of hospital rooms and to adapt the allocation of health professionals to these rooms so as to meet patients needs. ElHealth employs data prediction techniques to anticipate when the demand of a given room will exceeds its capacity, and to propose actions to allocate health professionals accordingly. We also introduce the concept of multi-level predictive elasticity of human resources (which is an extension of the concept of resource elasticity, from cloud computing) to manage the use of human resources at different levels of a healthcare environment. Furthermore, we devise the concept of proactive human resources elastic speedup (which is an extension of the speedup concept, from parallel computing) to properly measure the gain of healthcare time with dynamic parallel use of human resources within hospital environments. ElHealth was thoroughly evaluated based on simulations of a hospital environment using data from a Brazilian polyclinic, and obtained promising results, decreasing the waiting time by up to 96.71%. Internet of Things (IoT) is a concept where physical, digital, and virtual objects (i.e., things) are connected through a network structure and are part of the Internet activities in order to exchange information about themselves and about objects and things around them (Singh and Kapoor, 2017).IoT enables devices to interact not only with each other but also with services and people on a global scale (Akeju et al., 2018).The development of this paradigm is in constant growth due to the continuous efforts of the research community and due to its usefulness to a wide range of domains, such as airports, military, and healthcare (Singh and Kapoor, 2017; Sarhan, 2018).
Hospitals play an important role towards ensuring proper health treatment to human beings. One of the major challenges faced in this context refers to the increasingly overcrowded patients queues, which contribute to a potential deterioration of patients health conditions. One of the reasons of such an inefficiency is a poor allocation of health professionals. In particular, such allocation process is usually unable to properly adapt to unexpected changes in the patients demand. As a consequence, it is frequently the case where underused rooms have idle professionals whilst overused rooms have less professionals than necessary. Previous works addressed this issue by analyzing the evolution of supply (doctors) and demand (patients) so as to better adjust one to the other, though none of them focused on proposing effective counter-measures to mitigate poor allocations. In this paper, we build upon the concept of smart hospitals and introduce elastic allocation of human resources in healthcare environments (ElHealth), an IoT-focused model able to monitor patients usage of hospital rooms and to adapt the allocation of health professionals to these rooms so as to meet patients needs. ElHealth employs data prediction techniques to anticipate when the demand of a given room will exceeds its capacity, and to propose actions to allocate health professionals accordingly. We also introduce the concept of multi-level predictive elasticity of human resources (which is an extension of the concept of resource elasticity, from cloud computing) to manage the use of human resources at different levels of a healthcare environment. Furthermore, we devise the concept of proactive human resources elastic speedup (which is an extension of the speedup concept, from parallel computing) to properly measure the gain of healthcare time with dynamic parallel use of human resources within hospital environments. ElHealth was thoroughly evaluated based on simulations of a hospital environment using data from a Brazilian polyclinic, and obtained promising results, decreasing the waiting time by up to 96.71%. A particularly relevant scenario for IoT is healthcare (da Costa et al., 2018).According to Pinto et al. (2017), IoT promises to revolutionize healthcare applications by promoting more personalized, preventive, and collaborative ways of caring for patients.In particular, IoT-assisted patients can be supervised uninterruptedly using wearable devices, thus allowing risky situations to be detected and appropriately treated right away (Darshan and Anandakumar, 2015; Srinivas et al., 2018).Moreover, IoT provides a means for health systems to extract and analyze data, which can then be combined with machine learning techniques to early detect health disorders (Singh, 2018; Moreira et al., 2019).
Hospitals play an important role towards ensuring proper health treatment to human beings. One of the major challenges faced in this context refers to the increasingly overcrowded patients queues, which contribute to a potential deterioration of patients health conditions. One of the reasons of such an inefficiency is a poor allocation of health professionals. In particular, such allocation process is usually unable to properly adapt to unexpected changes in the patients demand. As a consequence, it is frequently the case where underused rooms have idle professionals whilst overused rooms have less professionals than necessary. Previous works addressed this issue by analyzing the evolution of supply (doctors) and demand (patients) so as to better adjust one to the other, though none of them focused on proposing effective counter-measures to mitigate poor allocations. In this paper, we build upon the concept of smart hospitals and introduce elastic allocation of human resources in healthcare environments (ElHealth), an IoT-focused model able to monitor patients usage of hospital rooms and to adapt the allocation of health professionals to these rooms so as to meet patients needs. ElHealth employs data prediction techniques to anticipate when the demand of a given room will exceeds its capacity, and to propose actions to allocate health professionals accordingly. We also introduce the concept of multi-level predictive elasticity of human resources (which is an extension of the concept of resource elasticity, from cloud computing) to manage the use of human resources at different levels of a healthcare environment. Furthermore, we devise the concept of proactive human resources elastic speedup (which is an extension of the speedup concept, from parallel computing) to properly measure the gain of healthcare time with dynamic parallel use of human resources within hospital environments. ElHealth was thoroughly evaluated based on simulations of a hospital environment using data from a Brazilian polyclinic, and obtained promising results, decreasing the waiting time by up to 96.71%. Hospitals are among the most important service points capable of ensuring appropriate treatment to the population.Considering the importance of such environments, enhancing the efficiency with which a hospital’s resources and processes are controlled becomes a central concern.Such a concern is particularly relevant in the context of underdeveloped countries, where the high number of patients associated with the lack of resources leads to overly high waiting times (Graham et al., 2018).In this sense, should be possible to identify when health centers would be overloaded, it would allow to establish contingency plans to minimize (or perhaps even eliminate) these bottlenecks.
Hospitals play an important role towards ensuring proper health treatment to human beings. One of the major challenges faced in this context refers to the increasingly overcrowded patients queues, which contribute to a potential deterioration of patients health conditions. One of the reasons of such an inefficiency is a poor allocation of health professionals. In particular, such allocation process is usually unable to properly adapt to unexpected changes in the patients demand. As a consequence, it is frequently the case where underused rooms have idle professionals whilst overused rooms have less professionals than necessary. Previous works addressed this issue by analyzing the evolution of supply (doctors) and demand (patients) so as to better adjust one to the other, though none of them focused on proposing effective counter-measures to mitigate poor allocations. In this paper, we build upon the concept of smart hospitals and introduce elastic allocation of human resources in healthcare environments (ElHealth), an IoT-focused model able to monitor patients usage of hospital rooms and to adapt the allocation of health professionals to these rooms so as to meet patients needs. ElHealth employs data prediction techniques to anticipate when the demand of a given room will exceeds its capacity, and to propose actions to allocate health professionals accordingly. We also introduce the concept of multi-level predictive elasticity of human resources (which is an extension of the concept of resource elasticity, from cloud computing) to manage the use of human resources at different levels of a healthcare environment. Furthermore, we devise the concept of proactive human resources elastic speedup (which is an extension of the speedup concept, from parallel computing) to properly measure the gain of healthcare time with dynamic parallel use of human resources within hospital environments. ElHealth was thoroughly evaluated based on simulations of a hospital environment using data from a Brazilian polyclinic, and obtained promising results, decreasing the waiting time by up to 96.71%. As stated in Butean et al. (2015), no matter how easy or complicated a situation is, if the medical staff does not react in time, everything regarding patients’ health becomes doubtful and unsafe.Hence, health professionals play a major role towards patients’ well-being (Nierop-van Baalen et al., 2019).These professionals range from nurses (who carry out triage procedures and small treatments) to doctors (who attend the most diverse medical specialties).In this kind of scenario, a static allocation of health professionals to health sectors may be inefficient, since some professionals may be misallocated to low demanding sectors, while leading to a lack of professionals in high demanding sectors.Such a problem is illustrated in Fig. 1, where the set of available attendants are statically assigned to two service sectors, one for exams and another for medication.In the example, there are more attendants examining than medicating patients, even though the number of patients waiting for exams is considerably smaller than those waiting to receive some medication.In this context, the idle attendants could be moved from the low demanding room to the high demanding one.In fact, the allocation of attendants should always adapt to the current conditions of the health sectors.
Hospitals play an important role towards ensuring proper health treatment to human beings. One of the major challenges faced in this context refers to the increasingly overcrowded patients queues, which contribute to a potential deterioration of patients health conditions. One of the reasons of such an inefficiency is a poor allocation of health professionals. In particular, such allocation process is usually unable to properly adapt to unexpected changes in the patients demand. As a consequence, it is frequently the case where underused rooms have idle professionals whilst overused rooms have less professionals than necessary. Previous works addressed this issue by analyzing the evolution of supply (doctors) and demand (patients) so as to better adjust one to the other, though none of them focused on proposing effective counter-measures to mitigate poor allocations. In this paper, we build upon the concept of smart hospitals and introduce elastic allocation of human resources in healthcare environments (ElHealth), an IoT-focused model able to monitor patients usage of hospital rooms and to adapt the allocation of health professionals to these rooms so as to meet patients needs. ElHealth employs data prediction techniques to anticipate when the demand of a given room will exceeds its capacity, and to propose actions to allocate health professionals accordingly. We also introduce the concept of multi-level predictive elasticity of human resources (which is an extension of the concept of resource elasticity, from cloud computing) to manage the use of human resources at different levels of a healthcare environment. Furthermore, we devise the concept of proactive human resources elastic speedup (which is an extension of the speedup concept, from parallel computing) to properly measure the gain of healthcare time with dynamic parallel use of human resources within hospital environments. ElHealth was thoroughly evaluated based on simulations of a hospital environment using data from a Brazilian polyclinic, and obtained promising results, decreasing the waiting time by up to 96.71%. Data prediction techniques play a role in this kind of scenario.In particular, such techniques make it possible to anticipate patients demand and to prepare the allocation of medical staff accordingly.Several works in the literature have proposed the use of data prediction techniques for optimizing resources usage: to minimize bottlenecks in patients flow (Vieira and Hollmén, 2016), to predict patients arrival at emergency (Graham et al., 2018), and to plan training sessions for doctors based on patients demand (Ishikawa et al., 2017).Although these works do predict demand and the use of health resources, none of them is able to provide counter-measures to mitigate the problems of poor resources allocation.Therefore, to the best of our knowledge, this is the first work to propose an efficient allocation of health professionals based on predictions made out of patients demand as obtained by IoT sensors.We say that our approach is efficient in a sense that it is able to properly meet patients demand using as least health professionals as possible.
Hospitals play an important role towards ensuring proper health treatment to human beings. One of the major challenges faced in this context refers to the increasingly overcrowded patients queues, which contribute to a potential deterioration of patients health conditions. One of the reasons of such an inefficiency is a poor allocation of health professionals. In particular, such allocation process is usually unable to properly adapt to unexpected changes in the patients demand. As a consequence, it is frequently the case where underused rooms have idle professionals whilst overused rooms have less professionals than necessary. Previous works addressed this issue by analyzing the evolution of supply (doctors) and demand (patients) so as to better adjust one to the other, though none of them focused on proposing effective counter-measures to mitigate poor allocations. In this paper, we build upon the concept of smart hospitals and introduce elastic allocation of human resources in healthcare environments (ElHealth), an IoT-focused model able to monitor patients usage of hospital rooms and to adapt the allocation of health professionals to these rooms so as to meet patients needs. ElHealth employs data prediction techniques to anticipate when the demand of a given room will exceeds its capacity, and to propose actions to allocate health professionals accordingly. We also introduce the concept of multi-level predictive elasticity of human resources (which is an extension of the concept of resource elasticity, from cloud computing) to manage the use of human resources at different levels of a healthcare environment. Furthermore, we devise the concept of proactive human resources elastic speedup (which is an extension of the speedup concept, from parallel computing) to properly measure the gain of healthcare time with dynamic parallel use of human resources within hospital environments. ElHealth was thoroughly evaluated based on simulations of a hospital environment using data from a Brazilian polyclinic, and obtained promising results, decreasing the waiting time by up to 96.71%. This work introduces a model of elastic allocation of human resources in healthcare environments (ElHealth, for short) as an alternative to the traditional static allocation of medical staff.ElHealth works by adjusting the medical staff allocation of smart hospitals (equipped with IoT sensors) based on predictions of patients demand.In particular, ElHealth uses IoT sensors to keep track of patients demand, which is modeled as a time series and is used to predict future demands.Such predictions allow to identify situations where the staff availability is unlikely to meet the demand.Building upon such predictions, ElHealth proposes an efficient allocation of the medical staff by moving such professionals to the most demanding areas while taking their time constraints into account.The idea is to always offer a reasonable waiting time for patients, regardless of the workload (number of patients in the hospital room).Thus, the main scientific contributions of this paper are twofold:
The Late Bronze Age (ca. 1700/1600–1050 BCE) in the Aegean started with strong connections between societies in the region and beyond, and was accompanied by the collapse of palatial polities around 1200 BCE. The collapse led to unrest and migration in the East Mediterranean. In the present study, we focus on settlement contexts dating to the transition between the Mycenaean palatial and post-palatial periods (ca. 1250–1050 BCE) in Greece, which saw the destruction of the Mycenaean palaces (ca. 1200 BCE). We aim to shed light on trade connections and mobility in the region during this substantial period through ancient DNA of livestock. We sequenced pig and cattle mitochondrial DNA (mtDNA) from Tiryns, Greece - a key Bronze Age site in the Aegean region. We discovered an Italian pig haplotype in palatial Tiryns. This is the first time that this particular haplotype is found outside Italy. By contrast, a genetic haplotype of Near Eastern descent (Y1) that was present in the Mycenaean palatial period cannot be ascertained in the post-palatial period. Whether comparable changes in the composition of livestock are also to be found in cattle, we are not able to say, because only the palatial period samples yielded ancient DNA. The results of this study corroborate the published data on mtDNA of pigs from the Mediterranean Basin from the Bronze and Iron Ages. They suggest that in the Mediterranean, pigs were translocated through various patterns of mobility; by Italian migrants to Mycenaean Greece as well as by other mobile groups (“Sea Peoples”) to the Levant. Ancient DNA is a powerful tool to reveal ancient translocations of species, and pigs serve a good proxy for tracing patterns of human mobility and interconnections. The Late Bronze Age (ca. 1700/1600 BCE–1050 BCE), was a period of an upsurge in interconnections between societies around the Mediterranean basin.These patterns of connectivity were driven by trade in precious raw materials and finished objects of metal, ivory, glass/faience, wood, pottery, etc. (Burns, 2010; Cline, 1994, 2007).Human mobility during this period ranging from travels of individuals to migration of entire groups of people also enhanced the dissemination of knowledge, materials and objects (Burns, 2010; Cline, 2007).
The Late Bronze Age (ca. 1700/1600–1050 BCE) in the Aegean started with strong connections between societies in the region and beyond, and was accompanied by the collapse of palatial polities around 1200 BCE. The collapse led to unrest and migration in the East Mediterranean. In the present study, we focus on settlement contexts dating to the transition between the Mycenaean palatial and post-palatial periods (ca. 1250–1050 BCE) in Greece, which saw the destruction of the Mycenaean palaces (ca. 1200 BCE). We aim to shed light on trade connections and mobility in the region during this substantial period through ancient DNA of livestock. We sequenced pig and cattle mitochondrial DNA (mtDNA) from Tiryns, Greece - a key Bronze Age site in the Aegean region. We discovered an Italian pig haplotype in palatial Tiryns. This is the first time that this particular haplotype is found outside Italy. By contrast, a genetic haplotype of Near Eastern descent (Y1) that was present in the Mycenaean palatial period cannot be ascertained in the post-palatial period. Whether comparable changes in the composition of livestock are also to be found in cattle, we are not able to say, because only the palatial period samples yielded ancient DNA. The results of this study corroborate the published data on mtDNA of pigs from the Mediterranean Basin from the Bronze and Iron Ages. They suggest that in the Mediterranean, pigs were translocated through various patterns of mobility; by Italian migrants to Mycenaean Greece as well as by other mobile groups (“Sea Peoples”) to the Levant. Ancient DNA is a powerful tool to reveal ancient translocations of species, and pigs serve a good proxy for tracing patterns of human mobility and interconnections. The site of Tiryns, situated in the Argolid, a fertile plain in the Northeast of the Peloponnese, Greece, was one of the political centers of Mycenaean Greece during the palatial period (ca. 1400–1200 BCE) and post-palatial period (ca. 1200–1050 BCE).In this study, we focus on settlement contexts dating to the period between 1250 and 1050 BCE which comprises the transition between the late palatial period (Late Helladic [LH] IIIB) (ca. 1300–1200 BCE) and the post-palatial period (LH IIIC) (ca. 1200–1050 BCE).This span of roughly 200 years saw the destruction of the Mycenaean palaces (ca. 1200 BCE) and the resurgence of Tiryns as one of the most important sites of the Northeastern Peloponnese in the 12th century BCE (Maran, 2009; Maran, 2015).The special position of post-palatial Tiryns is reflected in processes of political restoration and architectural expansion that are unprecedented in other regions of Greece (Maran, 2010; Maran, 2015; Maran, 2016).Tiryns is also one of the key sites for understanding patterns of inter-societal connectivity in the Late Bronze Age Eastern Mediterranean.It served as an important harbor for supra-regional exchange during the palatial period and was able to reestablish this position during the post-palatial period (Maran, 2012; Maran, 2004).Long-term excavations at Tiryns have provided rich evidence for relations with Crete, the Levant, Cyprus and Italy during the 13th and 12th cents BCE.
Goal recognition is the task of inferring an agent’s goals given some or all of the agent’s observed actions. However, few research focuses on how to improve the usage effectiveness of knowledge produced by a goal recognition system. In this work, we propose a probabilistic goal recognition approach tailored to a dynamic shortest-path network interdiction problem. Apart from inferring a probabilistic distribution over the possible goals of an agent, our work has another four key novelties: (i) a dynamic shortest-path local network interdiction model that allocates resources locally per step using goal recognition information; (ii) two behavior modeling approaches, including a data-driven learning method based on Inverse Reinforcement Learning as well as a heuristic method taking advantage of the network information, to help solve both the data-intensive and no available data situations; (iii) a heuristic named Subjective Confidence that uses variance in particle system for flexible resource allocation adjustment. The empirical test results show the effectiveness of our goal recognition method, and also verify the practical implications of these methods in solving scalable multi-terminus network interdiction problem. Goal recognition, the ability to recognize the plans and goals of other agents, enables humans, AI agents or command and control systems to reason about what the others are doing, why they are doing it, and what they will do next (Sukthankar et al., 2014).Until now, goal recognition system works well in many applications like human–robot interaction (Hofmann and Williams, 2007), intelligent tutoring (Min et al., 2014), system intrusion detection (Geib and Goldman, 2001) and security applications (Jarvis et al., 2005).However, the inability to take full use of the goal recognition results poses another challenge for domains like Game AI (Synnaeve and Bessiere, 2012) and Command and Control system (Xu et al., 2017).
Goal recognition is the task of inferring an agent’s goals given some or all of the agent’s observed actions. However, few research focuses on how to improve the usage effectiveness of knowledge produced by a goal recognition system. In this work, we propose a probabilistic goal recognition approach tailored to a dynamic shortest-path network interdiction problem. Apart from inferring a probabilistic distribution over the possible goals of an agent, our work has another four key novelties: (i) a dynamic shortest-path local network interdiction model that allocates resources locally per step using goal recognition information; (ii) two behavior modeling approaches, including a data-driven learning method based on Inverse Reinforcement Learning as well as a heuristic method taking advantage of the network information, to help solve both the data-intensive and no available data situations; (iii) a heuristic named Subjective Confidence that uses variance in particle system for flexible resource allocation adjustment. The empirical test results show the effectiveness of our goal recognition method, and also verify the practical implications of these methods in solving scalable multi-terminus network interdiction problem. In this work, we select the Shortest-Path Network Interdiction (SPNI) as the problem upon which the goal recognition would be used to provide real-time situation awareness knowledge.As one of classic decision-making problems in domains like public transportation (Laporte et al., 2010) and public security (Cappanera and Scaparra, 2011), network interdiction is most often modeled in the form of a static two-player, two-stage, master–slave game with perfect information (i.e., a Stackelberg game) (Lunday and Sherali, 2010).However, these assumptions are not valid in real-life scenarios where the evader’s possible termini are neither single nor static.This work seeks to orient the knowledge generated by goal recognition system into the decision-making process of the interdictor, and thus allow a dynamic shortest-path local network interdiction.
Goal recognition is the task of inferring an agent’s goals given some or all of the agent’s observed actions. However, few research focuses on how to improve the usage effectiveness of knowledge produced by a goal recognition system. In this work, we propose a probabilistic goal recognition approach tailored to a dynamic shortest-path network interdiction problem. Apart from inferring a probabilistic distribution over the possible goals of an agent, our work has another four key novelties: (i) a dynamic shortest-path local network interdiction model that allocates resources locally per step using goal recognition information; (ii) two behavior modeling approaches, including a data-driven learning method based on Inverse Reinforcement Learning as well as a heuristic method taking advantage of the network information, to help solve both the data-intensive and no available data situations; (iii) a heuristic named Subjective Confidence that uses variance in particle system for flexible resource allocation adjustment. The empirical test results show the effectiveness of our goal recognition method, and also verify the practical implications of these methods in solving scalable multi-terminus network interdiction problem. We first introduce a novel Dynamic Shortest-Path Local Network Interdiction (DSPLNI) model so as to incorporate useful real-time knowledge acquired from goal recognition system.As the problem is a typical bilevel mixed-integer program (BLMIP), a BLMIP solvable dual form is then proposed as the DSPLNI’s reformulation.
Goal recognition is the task of inferring an agent’s goals given some or all of the agent’s observed actions. However, few research focuses on how to improve the usage effectiveness of knowledge produced by a goal recognition system. In this work, we propose a probabilistic goal recognition approach tailored to a dynamic shortest-path network interdiction problem. Apart from inferring a probabilistic distribution over the possible goals of an agent, our work has another four key novelties: (i) a dynamic shortest-path local network interdiction model that allocates resources locally per step using goal recognition information; (ii) two behavior modeling approaches, including a data-driven learning method based on Inverse Reinforcement Learning as well as a heuristic method taking advantage of the network information, to help solve both the data-intensive and no available data situations; (iii) a heuristic named Subjective Confidence that uses variance in particle system for flexible resource allocation adjustment. The empirical test results show the effectiveness of our goal recognition method, and also verify the practical implications of these methods in solving scalable multi-terminus network interdiction problem. Then we introduce a Markov Decision Process-based goal recognition model, its dynamic Bayesian network representation and the applied goal inference method.As an extension to the original work (Xu et al., 2017), and in order to retrieve the agent behavior model which would be used during the goal inference, we propose two behavior modeling approaches, i.e., a data-driven learning method based on Inverse Reinforcement Learning (IRL) as well as a heuristic method taking advantage of the network information, to help solve both the data-intensive and no available data situations.As for the heuristic method, the paper designs a scalable technique in maintaining action utility map for fast goal inference.This is mainly designed to get and update action utilities more efficiently under the dynamic changing network environment.
Goal recognition is the task of inferring an agent’s goals given some or all of the agent’s observed actions. However, few research focuses on how to improve the usage effectiveness of knowledge produced by a goal recognition system. In this work, we propose a probabilistic goal recognition approach tailored to a dynamic shortest-path network interdiction problem. Apart from inferring a probabilistic distribution over the possible goals of an agent, our work has another four key novelties: (i) a dynamic shortest-path local network interdiction model that allocates resources locally per step using goal recognition information; (ii) two behavior modeling approaches, including a data-driven learning method based on Inverse Reinforcement Learning as well as a heuristic method taking advantage of the network information, to help solve both the data-intensive and no available data situations; (iii) a heuristic named Subjective Confidence that uses variance in particle system for flexible resource allocation adjustment. The empirical test results show the effectiveness of our goal recognition method, and also verify the practical implications of these methods in solving scalable multi-terminus network interdiction problem. Lastly, we introduce a heuristic named Subjective Confidence to design a resource assignment mechanism for DSPLNI, where the interdiction resource could be more flexibly and effectively allocated at each confrontation stage.
Goal recognition is the task of inferring an agent’s goals given some or all of the agent’s observed actions. However, few research focuses on how to improve the usage effectiveness of knowledge produced by a goal recognition system. In this work, we propose a probabilistic goal recognition approach tailored to a dynamic shortest-path network interdiction problem. Apart from inferring a probabilistic distribution over the possible goals of an agent, our work has another four key novelties: (i) a dynamic shortest-path local network interdiction model that allocates resources locally per step using goal recognition information; (ii) two behavior modeling approaches, including a data-driven learning method based on Inverse Reinforcement Learning as well as a heuristic method taking advantage of the network information, to help solve both the data-intensive and no available data situations; (iii) a heuristic named Subjective Confidence that uses variance in particle system for flexible resource allocation adjustment. The empirical test results show the effectiveness of our goal recognition method, and also verify the practical implications of these methods in solving scalable multi-terminus network interdiction problem. To illustrate the general idea of dynamic shortest-path local network interdiction using online probabilistic goal recognition, see the framework shown in Fig. 1.The circle consisting of four modules formulates a cognition loop.Specifically, the sensors first feed the observation information into probabilistic goal recognition, inside which the evader’s goal probabilities would be evaluated using state estimation techniques, e.g. particle filters.Then this information would be further fed into the DSPLNI module for the interdictor making real-time decisions.Finally, the decisions would be sent to actuators where the course of actions are implemented.
Goal recognition is the task of inferring an agent’s goals given some or all of the agent’s observed actions. However, few research focuses on how to improve the usage effectiveness of knowledge produced by a goal recognition system. In this work, we propose a probabilistic goal recognition approach tailored to a dynamic shortest-path network interdiction problem. Apart from inferring a probabilistic distribution over the possible goals of an agent, our work has another four key novelties: (i) a dynamic shortest-path local network interdiction model that allocates resources locally per step using goal recognition information; (ii) two behavior modeling approaches, including a data-driven learning method based on Inverse Reinforcement Learning as well as a heuristic method taking advantage of the network information, to help solve both the data-intensive and no available data situations; (iii) a heuristic named Subjective Confidence that uses variance in particle system for flexible resource allocation adjustment. The empirical test results show the effectiveness of our goal recognition method, and also verify the practical implications of these methods in solving scalable multi-terminus network interdiction problem. As for the evader’s behavior modeling, we provide two approaches, covering two situations, as shown in the framework.Both methods consisting of two modules, one for off-line pre-computations and the other designed to accommodate the dynamic changing situations.For no data situation, we use either heuristic or planning method to generate the evader behaviors, and propose the action utility map maintenance method to speed up online goal inference.While for the data-intensive situation, the IRL method is first used to learn the weights of the behavior features from the opponent behavior database, followed by the reinforcement learning (RL) method to learn the evader’s action policy from its reward function.When confronting with the network interdictions and the changing features, we once again use the RL to update the policy.
Archaeobotanical studies carried out in the southern Sierras Pampeanas of Argentina (San Luis Province) are reported, which add new information about the presence of cultigens in the area during the late Holocene and the variety of wild species used in this period. The presence of starch grains of Zea mays (corn), Cucurbita sp. (squash), undifferentiated tubers, and Phaseolus sp. (beans), as well as phytoliths of Panicoideae, Chloridoideae, Arundinoideae, Bambuseae, Cyperaceae, Asteraceae, Arecaceae and woody dicotiledons are documented from analyses on knapped tools. The obtained data allow discussing the diversity of the resources utilized and the importance of cultigens in prehispanic times in a context that is currently considered the southern limit of prehispanic food production economies. Ever since the archaeological research of González (1952, 1960), the Sierras Pampeanas of Argentina have been considered an archaeological region with its own characteristics, distinguishable from the northwest of Argentina and from other areas.In this context, even today it is proposed that the historical trajectories of local groups are related (Berberián et al., 2013; Laguens and Bonnín, 2009).However, there is a lack of balance regarding archaeological data between the two main sectors of the Sierras Pampeanas, namely Sierras de Córdoba and Sierras de San Luis (Cattaneo et al., 2013; Heider and Curtoni, 2016).
Archaeobotanical studies carried out in the southern Sierras Pampeanas of Argentina (San Luis Province) are reported, which add new information about the presence of cultigens in the area during the late Holocene and the variety of wild species used in this period. The presence of starch grains of Zea mays (corn), Cucurbita sp. (squash), undifferentiated tubers, and Phaseolus sp. (beans), as well as phytoliths of Panicoideae, Chloridoideae, Arundinoideae, Bambuseae, Cyperaceae, Asteraceae, Arecaceae and woody dicotiledons are documented from analyses on knapped tools. The obtained data allow discussing the diversity of the resources utilized and the importance of cultigens in prehispanic times in a context that is currently considered the southern limit of prehispanic food production economies. Sierras de Cordoba has been the focus of a profuse research on domestic and wild vegetables, their importance on human diet and intensification processes, among other issues (Berberián and Roldán, 2001; López and Recalde, 2016; Medina and Pastor, 2006; Medina et al., 2016, 2017; Pastor, 2007).The lines of evidence used have been varied: colonial documents from the 16th century that report an agricultural system based on maize, beans, quinoa, squash and peanuts (Berberián, 1987; Medina et al., 2009; Piana de Cuestas, 1992; among others), complemented with the collection of locust bean (Prosopis spp.) and chañar (Geoffroea decorticans) (i.e. Castro Olañeta, 2006; Pastor, 2007).Additionally, bioarchaeological studies with stable isotopes and osteo-dental paleopathological research have indicated the existence of a diet rich in carbohydrates and C4 plants (i.e. Bordach et al., 1991; Fabra et al., 2006).These developments, together with the sharp increase in regional work, the development of micro-botanical analyses and new radiocarbon dates, have produced significant advances.Lately, open-air residential-agricultural archaeological sites (traditionally considered sedentary) have been rethought in terms of their dynamics of occupation, chronology and articulation with productive spaces, as well as the role they played in the context of socio-political processes in the region (Medina et al., 2016).
Archaeobotanical studies carried out in the southern Sierras Pampeanas of Argentina (San Luis Province) are reported, which add new information about the presence of cultigens in the area during the late Holocene and the variety of wild species used in this period. The presence of starch grains of Zea mays (corn), Cucurbita sp. (squash), undifferentiated tubers, and Phaseolus sp. (beans), as well as phytoliths of Panicoideae, Chloridoideae, Arundinoideae, Bambuseae, Cyperaceae, Asteraceae, Arecaceae and woody dicotiledons are documented from analyses on knapped tools. The obtained data allow discussing the diversity of the resources utilized and the importance of cultigens in prehispanic times in a context that is currently considered the southern limit of prehispanic food production economies. In the mountain portion of San Luis, however, the evidence for pre-hispanic use of vegetable resources during the late Holocene is almost exclusively indirect, and was obtained in the late twentieth century (Heider and Curtoni, 2016).Wild resources are hardly mentioned in the literature and agriculture has traditionally been considered as a possibility based on the identification of residential bases in potentially cultivable land, the presence of milling instruments (mortars and conanas), lithic axes and ceramic pots, and the identification of small open-air archaeological sites that have been interpreted as crop fields (Gambier, 1998).
Archaeobotanical studies carried out in the southern Sierras Pampeanas of Argentina (San Luis Province) are reported, which add new information about the presence of cultigens in the area during the late Holocene and the variety of wild species used in this period. The presence of starch grains of Zea mays (corn), Cucurbita sp. (squash), undifferentiated tubers, and Phaseolus sp. (beans), as well as phytoliths of Panicoideae, Chloridoideae, Arundinoideae, Bambuseae, Cyperaceae, Asteraceae, Arecaceae and woody dicotiledons are documented from analyses on knapped tools. The obtained data allow discussing the diversity of the resources utilized and the importance of cultigens in prehispanic times in a context that is currently considered the southern limit of prehispanic food production economies. The earliest evidence for the processing and consumption of maize in the Sierras Pampeanas of Argentina dates back to the final Holocene, approximately 2500 years BP (Pastor et al., 2012).In terms of agricultural practices, archaeological discoveries of cultivated plants, including pumpkin (Cucurbita sp.), quinoa (Chenopodium quinoa), beans (Phaseolus vulgaris and Phaseolus lunatus) and maize, became more abundant only after 1100 years BP.However, maize and crops in general were not a main subsistence resource (Pastor and López, 2011).Recent research has shown the high incidence of wild plants in the subsistence of late societies, with cultigens occupying a secondary role in the diet (López, 2015; Medina et al., 2011; Medina et al., 2016; among others).
Silcrete was often used to make stone tools and the ubiquity of this material in the archaeological record has sparked considerable interest in developing techniques that can be used to trace its geographic origin. However, the highly variable physical and chemical properties of silcrete means that artefacts made from this raw material have proved difficult to provenance. This paper describes the use of Pb isotope analysis to characterize and differentiate silcrete sources in the Willandra Lakes region, a UNESCO World Heritage listed site in southeastern Australia. The sample collection strategy employed in the field has allowed Pb isotope variation both within and between the silcrete sources to be described. Pb isotope variation within each silcrete source does not exhibit spatial patterning, but Pb isotope signatures differ between silcrete sources in the Willandra Lakes region, and clear separation between more distant sources, is demonstrated. This represents a first step in being able to use isotope analysis to investigate how silcrete from different sources was used and how it was moved around the landscape. Archaeologists have long been interested in identifying the rock outcrops that provided the raw materials used to make stone tools.The distribution and characteristics of raw material sources are fundamental to understanding the strategies people employed to ensure that they had raw materials and/or tools available where and when they needed them (Andrefsky, 1994).The types of stone artefact assemblages found in different parts of the landscape can provide information about the extent of people's home range, the form in which raw material was carried around the landscape and the stone working activities that were undertaken in different locations (Isaac, 1986).Stone artefacts discarded at greater distances from raw material sources tend to be used more intensively, and are therefore smaller, than stone artefacts discarded near their source (Newman, 1994).Deviations from this pattern provide the opportunity to investigate the influence of other social and ecological factors, such as access to other resources and the presence of trade and exchange networks (e.g. Blumenschine et al., 2008; McBryde, 1984).The ability to determine the origin of stone raw materials provides the basis for interpreting patterns of stone artefact discard in these terms.
Silcrete was often used to make stone tools and the ubiquity of this material in the archaeological record has sparked considerable interest in developing techniques that can be used to trace its geographic origin. However, the highly variable physical and chemical properties of silcrete means that artefacts made from this raw material have proved difficult to provenance. This paper describes the use of Pb isotope analysis to characterize and differentiate silcrete sources in the Willandra Lakes region, a UNESCO World Heritage listed site in southeastern Australia. The sample collection strategy employed in the field has allowed Pb isotope variation both within and between the silcrete sources to be described. Pb isotope variation within each silcrete source does not exhibit spatial patterning, but Pb isotope signatures differ between silcrete sources in the Willandra Lakes region, and clear separation between more distant sources, is demonstrated. This represents a first step in being able to use isotope analysis to investigate how silcrete from different sources was used and how it was moved around the landscape. However, some types of rock are more amenable to sourcing than are others.Silcrete, which is a sedimentary rock with a high silica content and low fracture toughness, was used often as a raw material for making stone tools and is particularly common in Australia, southern Africa and western Europe (Nash and Ullyott 2007: 95).Silcrete forms via the silicification of regolith (Thiry and Milnes, 2017) and as a result, retains many of the sedimentological and geochemical characteristics of its parent material (e.g. Webb and Golding, 1998).Regolith tends to be highly variable within and between regions, which explains the high degree of textural and geochemical variation that characterizes silcrete.Individual hand specimens sometimes exhibit substantial variation in both color and texture, making it difficult to distinguish silcrete from different sources on the basis of visual characteristics alone.
Silcrete was often used to make stone tools and the ubiquity of this material in the archaeological record has sparked considerable interest in developing techniques that can be used to trace its geographic origin. However, the highly variable physical and chemical properties of silcrete means that artefacts made from this raw material have proved difficult to provenance. This paper describes the use of Pb isotope analysis to characterize and differentiate silcrete sources in the Willandra Lakes region, a UNESCO World Heritage listed site in southeastern Australia. The sample collection strategy employed in the field has allowed Pb isotope variation both within and between the silcrete sources to be described. Pb isotope variation within each silcrete source does not exhibit spatial patterning, but Pb isotope signatures differ between silcrete sources in the Willandra Lakes region, and clear separation between more distant sources, is demonstrated. This represents a first step in being able to use isotope analysis to investigate how silcrete from different sources was used and how it was moved around the landscape. Studies attempting to distinguish material originating from different sources of silcrete have relied primarily on variations in trace element compositions.Nash and others (Nash et al., 2013a, 2013b; Nash et al., 2016) explored the potential for using patterns in major and trace element abundance to source silcrete in sub-Saharan Africa, focusing initially on a study of silcrete outcrops in the South African Cape region (Nash et al., 2013a).They showed that variation in major and trace elements is sometimes linked to the bedrock on which the silcrete formed.Although artefacts were not included in the initial study, the results suggested that regions where silcrete has formed in association with a variety of different bedrock lithologies are likely to be better candidates for sourcing studies than regions where the bedrock is homogenous.Subsequently, Nash et al. (2013b, 2016) investigated silcrete sources in the Tsodilo Hills region, which encompasses adjacent regions in Botswana and Namibia, and showed that discriminant function analysis of bulk-sample major and trace element abundances can distinguish outcrops from one another.However, of the 82 artefacts they studied, only 35.4% were assigned to one of the documented sources (although a conservative confidence level of 99% was used).Another 24.4% of the artefacts plot just beyond specific source clusters, leading the authors to suggest that sampling may have been insufficient to capture intra-outcrop variation and that these artefacts were likely to have been procured from the sources to which they plot most closely.They suggest that the remaining 40.2% of artefacts, which are clear outliers on the discriminant function plots, were likely to have been procured from sources that lie beyond the study area.Given that the efficacy of using major and trace element abundance data to source silcrete artefacts has not yet been demonstrated in other studies, this explanation requires empirical confirmation.So, although Nash et al.'s (2013a, 2013b, 2016) research has demonstrated significant potential for using major and trace element abundance data to source artefacts made from silcrete, the relatively high proportion of artefacts that do not fit within source clusters does need further investigation.
Silcrete was often used to make stone tools and the ubiquity of this material in the archaeological record has sparked considerable interest in developing techniques that can be used to trace its geographic origin. However, the highly variable physical and chemical properties of silcrete means that artefacts made from this raw material have proved difficult to provenance. This paper describes the use of Pb isotope analysis to characterize and differentiate silcrete sources in the Willandra Lakes region, a UNESCO World Heritage listed site in southeastern Australia. The sample collection strategy employed in the field has allowed Pb isotope variation both within and between the silcrete sources to be described. Pb isotope variation within each silcrete source does not exhibit spatial patterning, but Pb isotope signatures differ between silcrete sources in the Willandra Lakes region, and clear separation between more distant sources, is demonstrated. This represents a first step in being able to use isotope analysis to investigate how silcrete from different sources was used and how it was moved around the landscape. In Australia, silcrete commonly occurs in one of two contexts: in association with basalt (e.g. Webb et al., 2013; Webb and Golding, 1998) or in association with other substrates in arid, inland settings (e.g. Doelman, 2005, 2008; Doelman et al., 2001; Thiry and Milnes, 1991).When silcrete forms in association with basalt, the silica is derived from the weathering of the basalt (Webb and Golding, 1998), but in arid, inland contexts like the Willandra Lakes region, the silica is either dissolved out of components of the regolith itself or introduced by fluvial processes from further afield (Thiry and Milnes, 2017).These formation processes are likely to impact on the potential for identifying the source of the material used to make artefacts.Basalt-associated silcretes tend to form in localized settings, deriving silica from basalt deposits that often have distinct genetic histories, and thus distinct geochemical characteristics, which may create silcretes with distinct geochemical characteristics.However, arid, inland silcrete outcrops often represent ground surface exposures of expansive deposits that have a shared genetic history (i.e. they may be part of the same sand-sheet, some or all of which has become silicified by broad-scale processes such as ground water activity).As a result, different arid, inland outcrops may be more difficult to differentiate from one another than different basalt-associated silcrete outcrops.
Silcrete was often used to make stone tools and the ubiquity of this material in the archaeological record has sparked considerable interest in developing techniques that can be used to trace its geographic origin. However, the highly variable physical and chemical properties of silcrete means that artefacts made from this raw material have proved difficult to provenance. This paper describes the use of Pb isotope analysis to characterize and differentiate silcrete sources in the Willandra Lakes region, a UNESCO World Heritage listed site in southeastern Australia. The sample collection strategy employed in the field has allowed Pb isotope variation both within and between the silcrete sources to be described. Pb isotope variation within each silcrete source does not exhibit spatial patterning, but Pb isotope signatures differ between silcrete sources in the Willandra Lakes region, and clear separation between more distant sources, is demonstrated. This represents a first step in being able to use isotope analysis to investigate how silcrete from different sources was used and how it was moved around the landscape. Cochrane et al. (2017) investigated the potential for sourcing artefacts made from basalt-associated silcrete from the Arcadia Valley in the central highlands of Queensland, Australia.Portable XRF analysis was applied to 86 silcrete cobbles collected from three different sources, and to 100 artefacts from nearby surface accumulations.The samples from the three sources plotted in clusters on the basis of their median iron (Fe) and zirconium (Zr) concentrations.Although these clusters are not discrete, they are sufficient to demonstrate patterned variation, which provides the rationale for employing Fe and Zr element concentration data as a basis for sourcing the artefacts.The Fe and Zr concentrations of 75% of the artefacts plot within the 95% confidence ellipses identified for two of the three sources.None of the artefacts plot within the 95% confidence ellipse identified for the third source, indicating that this source was probably not exploited despite being the same geographical distance from the surface artefact accumulations as one of the other sources.With respect to the two utilised sources, it is worth noting that the artefacts plot in a continuous distribution, rather than producing clusters that correlate with the geological sample data for these two sources.If the sources are distinct from one another, and each artefact originated from one of these two sources, it follows that they should cluster with the geological sample data, even if the clusters are quite loose.The continuous distribution indicates that further investigation is needed.Nevertheless, the data highlight the possibility of identifying the sources of artefacts made from basalt-associated silcrete.Furthermore, by ruling out one of the potential sources, Cochrane et al. (2017) were able to determine that geographical proximity was not the most important factor governing raw material source preference in the Arcadia Valley.
Silcrete was often used to make stone tools and the ubiquity of this material in the archaeological record has sparked considerable interest in developing techniques that can be used to trace its geographic origin. However, the highly variable physical and chemical properties of silcrete means that artefacts made from this raw material have proved difficult to provenance. This paper describes the use of Pb isotope analysis to characterize and differentiate silcrete sources in the Willandra Lakes region, a UNESCO World Heritage listed site in southeastern Australia. The sample collection strategy employed in the field has allowed Pb isotope variation both within and between the silcrete sources to be described. Pb isotope variation within each silcrete source does not exhibit spatial patterning, but Pb isotope signatures differ between silcrete sources in the Willandra Lakes region, and clear separation between more distant sources, is demonstrated. This represents a first step in being able to use isotope analysis to investigate how silcrete from different sources was used and how it was moved around the landscape. Silcrete was the raw material most often used to make stone tools in the Willandra Lakes region (Stern et al., 2013), a relict overflow system located in southwest New South Wales, Australia (Fig. 1).The sedimentary sequence in this area preserves archaeological sites with an antiquity of at least 45 ka as well as the world's oldest-known ritual ochre burial and the oldest-known cremation (Bowler et al., 2003).In 1981, these dry lakes and their immediate surroundings were placed on the United Nations Educational Scientific and Cultural Organization (UNESCO) World Heritage List because of their outstanding natural, and exceptional cultural, features.The archaeological research conducted in this region has focused primarily on the archaeological traces incorporated into the Mungo lunette, a large, crescentic dune that built up on the leeward side of Lake Mungo, a large, terminal lake within the overflow system (e.g. Bowler, 1998; Bowler et al., 1970; Shawcross, 1998; Spry, 2014; Stern, 2014, 2015; Stern et al., 2013; Tumney, 2011).During survey work conducted as part of a larger research program, numerous outcrops of silcrete were identified in the area, several of which exhibit signs of having been used as sources of raw material for making stone tools (Kurpiel, 2017).An understanding of the geographic origin of the silcrete used to make tools has the potential to provide crucial insights into the way that people moved around this landscape.For example, it could provide detailed insight into the ways mobility strategies changed in response to local environmental and hydrological conditions.Visually, there is a high degree of variation both within and between sources.Geochemical techniques, such as Pb isotope analysis, offer a more rigorous approach to the problem of provenancing silcrete than do physical characteristics alone.
Silcrete was often used to make stone tools and the ubiquity of this material in the archaeological record has sparked considerable interest in developing techniques that can be used to trace its geographic origin. However, the highly variable physical and chemical properties of silcrete means that artefacts made from this raw material have proved difficult to provenance. This paper describes the use of Pb isotope analysis to characterize and differentiate silcrete sources in the Willandra Lakes region, a UNESCO World Heritage listed site in southeastern Australia. The sample collection strategy employed in the field has allowed Pb isotope variation both within and between the silcrete sources to be described. Pb isotope variation within each silcrete source does not exhibit spatial patterning, but Pb isotope signatures differ between silcrete sources in the Willandra Lakes region, and clear separation between more distant sources, is demonstrated. This represents a first step in being able to use isotope analysis to investigate how silcrete from different sources was used and how it was moved around the landscape. Pb isotope studies have a long history in the earth sciences and are used routinely to characterize magmatic rocks and base metal ore deposits (e.g. Gulson, 1986; Sun, 1980) and to provenance sediments (McLennan et al., 1993).Variations in Pb isotope ratios of rocks, minerals and fluids result from the long-term accumulation of radiogenic 206Pb, 207Pb and 208Pb from radioactive decay of 238U, 235U and 232Th, respectively, with half-lives of 0.7, 4.4 and 14.1 billion years.Pb isotopic variations exist on all scales and depend on the age, U-Th-Pb concentrations and origin of a given Pb-bearing material (e.g. Faure and Mensing, 2005), providing an excellent isotopic tracer for metals and other Pb-bearing materials.In archaeology, Pb isotopic variations are used to provenance metal artefacts (e.g. Baron et al., 2011; Ling et al., 2013; Shortland, 2006; Stos-Gale et al., 1997; Ponting et al., 2003), pottery (e.g. Renson et al., 2011; Wolf et al., 2003) and glass (e.g. Henderson et al., 2005).
Silcrete was often used to make stone tools and the ubiquity of this material in the archaeological record has sparked considerable interest in developing techniques that can be used to trace its geographic origin. However, the highly variable physical and chemical properties of silcrete means that artefacts made from this raw material have proved difficult to provenance. This paper describes the use of Pb isotope analysis to characterize and differentiate silcrete sources in the Willandra Lakes region, a UNESCO World Heritage listed site in southeastern Australia. The sample collection strategy employed in the field has allowed Pb isotope variation both within and between the silcrete sources to be described. Pb isotope variation within each silcrete source does not exhibit spatial patterning, but Pb isotope signatures differ between silcrete sources in the Willandra Lakes region, and clear separation between more distant sources, is demonstrated. This represents a first step in being able to use isotope analysis to investigate how silcrete from different sources was used and how it was moved around the landscape. Pb isotope studies of rocks and minerals used in tool-making are much less common.Weisler and Woodhead (1995) compared Pb isotope compositions of six basalt artefacts from Henderson Island, a limestone island in the Pitcairn group, with 18 basalt samples collected from deposits scattered across southeastern Polynesia.This comparison placed the origin of five of the Henderson Island artefacts at nearby Pitcairn Island, while the sixth could be traced to the Gambiers, some 400 km to the west.This provided evidence that the Pitcairn islands were colonized by people from the Gambiers.Differences in Pb isotope composition were also noted between ‘shield’ and ‘post-shield’ basalt deposits from the same island, illustrating the power of Pb isotopes as a provenancing tool.In another study of this type, Collerson and Weisler (2007) combined isotopic data with major and trace element data to compare Polynesian adzes with possible source locations.Three of the nineteen adzes could be traced precisely, and others could be assigned to likely source regions.The results were consistent with Hawaiian oral histories that describe prehistoric voyages from Hawai'i to Tahiti and back via the Tuamotus.
Silcrete was often used to make stone tools and the ubiquity of this material in the archaeological record has sparked considerable interest in developing techniques that can be used to trace its geographic origin. However, the highly variable physical and chemical properties of silcrete means that artefacts made from this raw material have proved difficult to provenance. This paper describes the use of Pb isotope analysis to characterize and differentiate silcrete sources in the Willandra Lakes region, a UNESCO World Heritage listed site in southeastern Australia. The sample collection strategy employed in the field has allowed Pb isotope variation both within and between the silcrete sources to be described. Pb isotope variation within each silcrete source does not exhibit spatial patterning, but Pb isotope signatures differ between silcrete sources in the Willandra Lakes region, and clear separation between more distant sources, is demonstrated. This represents a first step in being able to use isotope analysis to investigate how silcrete from different sources was used and how it was moved around the landscape. More recently, ten Bruggencate and others (ten Bruggencate et al., 2013, 2014) carried out a provenance study of quartz in the Churchill River basin, in central Canada.In this area, quartz was used to make tools during the pre-contact period and was procured from several different sources.In order to explore the geochemical variability of these sources, ten Bruggencate et al. (2013) measured Pb isotopes and a small selection of trace elements from seven sources of quartz within a 25 km2 area.A combination of color, Pb isotope compositions and Ti-Ge concentrations were found to distinguish most of the quartz source locations from one another.
This provenance study of yellow-firing clays in north central New Mexico examines whether clays recovered in the vicinity of Tunque Pueblo (LA 240) may have been used as slip clays at contemporaneous San Marcos Pueblo (LA 98). A sample of 72 ceramic sherds, bricks, and clays were analyzed through chemical characterization using laser-ablation inductively coupled plasma mass spectrometry (LA-ICP-MS). We argue that Tunque potters were using a subset of clays available at their village to produce pottery. Although San Marcos potters appear to have possibly been using clay from Tunque Pueblo to slip their vessels, these clays were not the same as those used by Tunque potters. Given San Marcos potters' apparent reliance on this slip clay over time, we argue our findings demonstrate that extremely stable social networks were developed and sustained among Rio Grande Pueblo households and communities across north central New Mexico during the late prehispanic and early colonial periods (1400–1680 CE). Among ethnohistoric potters, the raw materials used to make pottery often come from a variety of locations.Some resources such as clay or temper are widely available and are typically obtained close to the area of production (Arnold, 1985).Other resources, such as slip clays or pigments, are more limited in their availability and potters may travel longer distances to obtain these materials (Dillingham, 1992; Najohai and Phelps, 1998; Parsons, 1932).Acquisition of these more specialized materials are embedded within social and economic networks of varying scales; this may have also been the case in the past (Herhahn, 2006; Huntley et al., 2012; Nelson and Habicht-Mauche, 2006).In this study, we are specifically examining the provenance of yellow-firing clays used in the production of Ancestral Pueblo glaze-painted pottery at two villages in north central New Mexico.
This provenance study of yellow-firing clays in north central New Mexico examines whether clays recovered in the vicinity of Tunque Pueblo (LA 240) may have been used as slip clays at contemporaneous San Marcos Pueblo (LA 98). A sample of 72 ceramic sherds, bricks, and clays were analyzed through chemical characterization using laser-ablation inductively coupled plasma mass spectrometry (LA-ICP-MS). We argue that Tunque potters were using a subset of clays available at their village to produce pottery. Although San Marcos potters appear to have possibly been using clay from Tunque Pueblo to slip their vessels, these clays were not the same as those used by Tunque potters. Given San Marcos potters' apparent reliance on this slip clay over time, we argue our findings demonstrate that extremely stable social networks were developed and sustained among Rio Grande Pueblo households and communities across north central New Mexico during the late prehispanic and early colonial periods (1400–1680 CE). The majority of ceramic provenance studies in the Southwest United States have focused either on mineralogical analyses (Capone, 2006; Eckert, 2008; Habicht-Mauche, 1993, 2002; Shepard, 1942) or bulk chemical compositional techniques (Crown, 1994; Glowacki et al., 1998; Huntley, 2008).However, the development of numerous chemical compositional techniques has provided multiple avenues for archaeologists to explore the provenance of specific clays, tempers, slips, and paints (e.g., Duwe and Neff, 2007; Habicht-Mauche et al., 2000).This research focuses on understanding the provenance of the yellow-firing clay used as slip on glaze-decorated pottery produced during the 14th through 17th centuries in north central New Mexico (Fig. 1).Specifically, we collected chemical signatures derived from laser ablation-inductively coupled plasma-mass spectrometry (LA-ICP-MS) of historic bricks, clay samples, and ceramic sherds from Tunque Pueblo (LA 240) and San Marcos Pueblo (LA 98).We interpret these data to explore the distribution of raw materials required to make pottery and to discuss social networks that may have facilitated the movement of these materials.
The present study explores four different regression formulae for the histological assessment of age at death in ribs and their performance in a prehispanic Classic Maya population sample consisting of 57 individuals from Xcambó, Yucatan, Mexico. Regressions employed include the original age regression formula by Stout and Paine (1992), Cho et al.'s (2002) two formulae for samples of indeterminate ethnicity, as well as a formula published by Valencia et al. (2010) adjusted specifically for populations of Maya descent. In addition to applying these methods, we report histomorphometric variables (total cortical area (TA), cortical area (CA), relative cortical area (CA/TA%), osteon population density (OPD), and osteon cross-sectional area (On.Ar.)) from the 6th rib and compare these variables across groups within our sample defined by macroscopic age and sex, as well as with results reported for modern reference samples used by Cho et al. and Valencia and colleagues. Our study shows mean CA and CA/TA% are relatively high across all ages in the prehispanic Maya sample, especially in females, indicating a high degree of robusticity. OPD is high when compared with samples used by Cho et al., but similar to the modern Maya reference sample. Comparison of histological age at death estimates reveals interesting patterns of deviation; specifically Cho et al.'s formulae both deviate strongly from all other age estimates. Calculated mean net difference between Cho et al.'s and macroscopic age estimates, for example, is nearly 16 years. Both, Stout and Paine, and Valencia et al., result in age reconstructions more similar to macroscopic estimates (mean net difference around 8 years). Since Cho et al.'s formulae are unique in employing On.Ar., CA, and TA, in addition to OPD, OPD-based regression formulae may perform better in archaeological samples. However, some of the deviation observed could result from differences in histomorphometric variables between modern reference and archaeological samples, the outcome of complex biocultural processes. Continued analyses of histomorphological variation between differing reference and archaeological samples will be necessary to improve histological assessments of age at death in archaeological contexts. The estimation of age at death in human remains is one of the most fundamental steps undertaken in archaeological and forensic casework.In archaeological contexts, macroscopic methods are often preferred over microscopic ones, for many reasons.First, invasive techniques are often avoided or not permitted due to ethnical concerns, and/or to preserve sample integrity.Second, labs are often not equipped with microscopes and stations for the preparation and analysis of thin sections.Third, histological assessment takes more time, and requires additional experience.Fourth, histological approaches often focus on skeletal regions indicating pathological changes (Schultz, 2001), while standardized healthy bone sections are necessary for age estimations.Finally, and most importantly within the context of the present study, current literature focusses on the refinement of regression formulae, with little published about their applicability in archaeological contexts (see Pfeiffer and Pinto, 2012).
The present study explores four different regression formulae for the histological assessment of age at death in ribs and their performance in a prehispanic Classic Maya population sample consisting of 57 individuals from Xcambó, Yucatan, Mexico. Regressions employed include the original age regression formula by Stout and Paine (1992), Cho et al.'s (2002) two formulae for samples of indeterminate ethnicity, as well as a formula published by Valencia et al. (2010) adjusted specifically for populations of Maya descent. In addition to applying these methods, we report histomorphometric variables (total cortical area (TA), cortical area (CA), relative cortical area (CA/TA%), osteon population density (OPD), and osteon cross-sectional area (On.Ar.)) from the 6th rib and compare these variables across groups within our sample defined by macroscopic age and sex, as well as with results reported for modern reference samples used by Cho et al. and Valencia and colleagues. Our study shows mean CA and CA/TA% are relatively high across all ages in the prehispanic Maya sample, especially in females, indicating a high degree of robusticity. OPD is high when compared with samples used by Cho et al., but similar to the modern Maya reference sample. Comparison of histological age at death estimates reveals interesting patterns of deviation; specifically Cho et al.'s formulae both deviate strongly from all other age estimates. Calculated mean net difference between Cho et al.'s and macroscopic age estimates, for example, is nearly 16 years. Both, Stout and Paine, and Valencia et al., result in age reconstructions more similar to macroscopic estimates (mean net difference around 8 years). Since Cho et al.'s formulae are unique in employing On.Ar., CA, and TA, in addition to OPD, OPD-based regression formulae may perform better in archaeological samples. However, some of the deviation observed could result from differences in histomorphometric variables between modern reference and archaeological samples, the outcome of complex biocultural processes. Continued analyses of histomorphological variation between differing reference and archaeological samples will be necessary to improve histological assessments of age at death in archaeological contexts. Consequently, the application of histological regressions for the estimation of age at death in archaeological samples is uncommon.In some circumstances and in specific geographic regions, however, microscopic age at death assessment is advantageous.This is especially true when remains are often fragmented due to weathering and climate.Under poor preservation, bone regions typically used for macroscopic analysis, i.e. pubic symphysis (Brooks and Suchey, 1990), mid thoracic sternal rib end (Iscan, 1993; Iscan et al., 1984) or the auricular surface (Lovejoy et al., 1985), are often eroded, destroyed and/or missing.Regions of dense long bone diaphyses, however, are often found better preserved.In addition, during the analysis of important dignitaries, histological assessment of age can add necessary detail.An example for such a case is Janaab'Pakal from Palenque, Chiapas.Epigraphic inscriptions indicated Janaab'Pakal was 80 years old when he passed away, however, macroscopic methods suggested an estimate of 40–50 years.To investigate this discrepancy, Stout and Streeter (2004, 2006) microscopically assessed Janaab'Pakal's age and found that histologically his remains proposed a minimum age at death of 52 years old and possibly much older, supporting the biographic information derived from inscriptions.More recently, Suzuki and Tiesler (2016) report a combined macro- and microscopic age at death analysis of three Maya dignitaries from the Mexican sites Palenque, Dzibanché, and Chiapa de Corzo.Similarly, one of the individuals showed a relatively high deviation between macroscopically and microscopically assessed age at death with the latter suggesting a more advanced age, again aiding demographic reconstructions.In a larger prehispanic Maya sample, Tiesler et al. (2008) examined 18 individuals employing three different age regression formulae derived from Stout and Paine (1992), Cho et al. (2002), and Valencia et al. (2010), the same ones used in the present approach.Cho et al.'s (2002) equations resulted in higher estimates than the two others did.Since Cho et al.'s (2002) reconstructions also diverged strongest from macroscopic ages, Tiesler et al. (2008) concluded Cho et al.'s (2002) formulae may result in less reliable age at death estimates for prehispanic Maya populations (see also Hurtado Cen et al., 2006).
Glioma-associated epilepsy is associated with excessive glutamate signaling. We hypothesized that perampanel, an amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA)-type glutamate receptor antagonist, would treat glioma-related epilepsy. We conducted a single-arm study of adjunctive perampanel for patients with focal-onset glioma-associated seizures. The most common related adverse events were fatigue and dizziness. Three out of 8 participants had self-reported seizure reduction and an additional 3 reported improved control. Of these 6, 5 had isocitrate dehydrogenase 1 mutant gliomas. We conclude that perampanel is safe for patients with glioma-related focal-onset epilepsy. Further study into the association between AMPA signaling, IDH1 status and seizures is warranted. Nearly one-third of patients with primary brain tumors experience a seizure as the presenting symptom and another 30–50% develop seizures during their disease course [1].Moreover, many of these patients have epilepsy that adversely affects quality of life.In fact, seizure control is the most important predictor of quality of life in recurrent low-grade glioma patients [2].A challenge of treating brain tumor-related epilepsy is that anti-seizure drug (ASD) options are limited due to interactions with chemotherapy and side effects including cognitive dysfunction in an already susceptible patient population.Therefore, there is a real need to find effective ASDs that are without cytochrome P450 enzyme induction properties that are well tolerated.
Glioma-associated epilepsy is associated with excessive glutamate signaling. We hypothesized that perampanel, an amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA)-type glutamate receptor antagonist, would treat glioma-related epilepsy. We conducted a single-arm study of adjunctive perampanel for patients with focal-onset glioma-associated seizures. The most common related adverse events were fatigue and dizziness. Three out of 8 participants had self-reported seizure reduction and an additional 3 reported improved control. Of these 6, 5 had isocitrate dehydrogenase 1 mutant gliomas. We conclude that perampanel is safe for patients with glioma-related focal-onset epilepsy. Further study into the association between AMPA signaling, IDH1 status and seizures is warranted. Glutamate is the chief excitatory neurotransmitter in the central nervous system and is pro-epileptogenic.In murine models, glutamate release was found to be responsible for epileptic activity [3].Aberrant glutamate regulation is associated with primary brain tumor-associated epilepsy.Specifically, glioma peritumoral tissue contains increased extracellular glutamate concentrations due to increased glioma cell glutamate secretion via cystine–glutamate transporters and impaired glutamate reuptake by surrounding reactive astrocytes [4,5].
We report a teenager with childhood onset focal seizures associated with the chapeau de gendarme sign or ictal pouting of anterior insular lobe origin. The chapeau de gendarme sign has been associated with frontal lobe seizures in patients with focal epilepsy. However, in this case, stereo-electroencephalography (SEEG) localized seizures to the anterior insular cortex prior to her typical clinical manifestations. Surgical resection of the insular and frontal-lobe network resulted in seizure freedom. We propose that the anterior insular cortex should be a site of investigation during pre-surgical phase 2 evaluation in patients exhibiting the chapeau de gendarme sign during focal seizures. Frontal lobe seizures constitute nearly 30% of focal neocortical seizures [1] but remain difficult to characterize due to their complex semiology, large surface area and electrophysiological patterns [2].One localizing sign that has been purported for frontal lobe epilepsy (FLE) is a characteristic down turning of the mouth into a deep frown, known as ictal pouting or ‘chapeau de gendarme’ (CdG) sign.While the CdG sign is not frequently seen, researchers and clinicians have used it to localize the seizure onset zone (SOZ) to the frontal lobe [3–5].Due to the complexity of connections with the frontal lobe, however, other regions may initiate epileptogenic networks with complex propagations to the frontal lobe thereby resulting in false localizations for this unique semiological finding.
The composition, mineralogy, and textures preserved in scoria from ancient fires provide constraints on the firing temperature, the source and nature of the fire, and its potential social and cultural implications. Analyses of four scoria fragments preserved in a posthole of an Iron Age longhouse at Store Tovstrup, West Denmark, by scanning electron microscopy, electron microprobe, and laser ablation ICP-MS show these to consist of rounded quartz and orthoclase grains, gas vesicles, and carbonaceous material bonded together by a silicate- and potassium-rich (SiO2 67–69 wt% and K2O 11–14 wt%) melt (now glass). Given the presence of vesicles and carbonaceous material, the fire is indicated to have occurred under restricted air-flow and to have involved decomposition of biomass and soil. The initiation of melting occurred during what was presumably an event of short duration. Simplified ternary phase equilibria point to localized melting initiated around 700–800 °C and continuing to about 1000 °C. The main structure succumbed to char at lower temperatures. Calculations suggest that a mixture of 50% sandy soil, 41% barley straw, and 9% oak branches best explains the low Al2O3, Fe2O3, and Na2O concentrations in the melt phase. The scoria at Store Tovstrup most likely originated from a short duration burning with restricted air-flow resulting in the collapse and charring of daub walls. The fire was intentional and set after the house had been cleared of household goods. The Store Tovstrup Iron Age site is located on the Jutland peninsula in Western Denmark on the bank of the minor meandering Savstrup Å (creek), approximately 1.2 km east of the point where it meets the larger river Storå (literally translated ‘big river,’ the second longest river in Denmark) (Fig. 1A).The site was excavated in 2014–16 by Holstebro Museum in preparation for major road constructions in the area planned to be completed in 2018.The location is only 1.5 km south of the maximum extend of the Weichselian ice sheet (Fig. 1A), marked by a large outcrop of clayey till, on a glacial alluvial outwash plain made of broad sandy river valleys (known as ‘Karup hedeslette’ or ‘outwashed sandy plain’ on Fig. 1A; Milthers, 1948; Houmark-Nielsen, 1999; Christensen, 2016).The surface deposit of the immediate area consists of quarts-rich sand, gravel, and, occasionally, clayey sediments.Immediately south of the site is an eroded riverbank and a river valley, made-up of fresh water, lacustrine deposits.With the final retreat of the Weichselian ice, both sea level and land surface rose to reach a maximum during late early pre-Roman Iron Age (400–300 BCE), resulting in the local formation of lakes and the stabilization of waterways around the Tovstrup site (Behre, 2007; Odgaard and Dalsgaard, 2014).
The composition, mineralogy, and textures preserved in scoria from ancient fires provide constraints on the firing temperature, the source and nature of the fire, and its potential social and cultural implications. Analyses of four scoria fragments preserved in a posthole of an Iron Age longhouse at Store Tovstrup, West Denmark, by scanning electron microscopy, electron microprobe, and laser ablation ICP-MS show these to consist of rounded quartz and orthoclase grains, gas vesicles, and carbonaceous material bonded together by a silicate- and potassium-rich (SiO2 67–69 wt% and K2O 11–14 wt%) melt (now glass). Given the presence of vesicles and carbonaceous material, the fire is indicated to have occurred under restricted air-flow and to have involved decomposition of biomass and soil. The initiation of melting occurred during what was presumably an event of short duration. Simplified ternary phase equilibria point to localized melting initiated around 700–800 °C and continuing to about 1000 °C. The main structure succumbed to char at lower temperatures. Calculations suggest that a mixture of 50% sandy soil, 41% barley straw, and 9% oak branches best explains the low Al2O3, Fe2O3, and Na2O concentrations in the melt phase. The scoria at Store Tovstrup most likely originated from a short duration burning with restricted air-flow resulting in the collapse and charring of daub walls. The fire was intentional and set after the house had been cleared of household goods. The site contains numerous houses and other settlement structures from several prehistoric periods (e.g., Fig. 1B) as well as traces of a medieval farmstead, likely a predecessor of the still existing Store Tovstrup farm (Hansen, 2018a).Like the vast majority of prehistoric sites in the region (Hvass, 1985; Webley, 2008; Hansen, 2018b), the Tovstrup site does not contain above the ground remains or ruins; only pits, postholes, and other subsoil-level remains are preserved.
The composition, mineralogy, and textures preserved in scoria from ancient fires provide constraints on the firing temperature, the source and nature of the fire, and its potential social and cultural implications. Analyses of four scoria fragments preserved in a posthole of an Iron Age longhouse at Store Tovstrup, West Denmark, by scanning electron microscopy, electron microprobe, and laser ablation ICP-MS show these to consist of rounded quartz and orthoclase grains, gas vesicles, and carbonaceous material bonded together by a silicate- and potassium-rich (SiO2 67–69 wt% and K2O 11–14 wt%) melt (now glass). Given the presence of vesicles and carbonaceous material, the fire is indicated to have occurred under restricted air-flow and to have involved decomposition of biomass and soil. The initiation of melting occurred during what was presumably an event of short duration. Simplified ternary phase equilibria point to localized melting initiated around 700–800 °C and continuing to about 1000 °C. The main structure succumbed to char at lower temperatures. Calculations suggest that a mixture of 50% sandy soil, 41% barley straw, and 9% oak branches best explains the low Al2O3, Fe2O3, and Na2O concentrations in the melt phase. The scoria at Store Tovstrup most likely originated from a short duration burning with restricted air-flow resulting in the collapse and charring of daub walls. The fire was intentional and set after the house had been cleared of household goods. The pre-Roman house is of the type known throughout northwestern Europe during the time, with two rows of central roof bearing posts and two centrally placed doors, in the centers of the northern and southern walls.The dwelling area is located to the west and the byre to the east of the entrances (Webley, 2008, p. 73–74).Our knowledge of the building materials, construction and condition of the house at the time of the fire is limited.We know that it had an earthen or clay floor, wattle walls, plastered with daub or clay, and an inner construction consisting of two rows of wooden posts, holding up the roof.Houses of the type excavated at the Tovstrup site had two centrally placed doors, in the center of the northern and southern walls.To the east of the entrance was the byre, sometimes divided into stalls, and to the west the dwelling area.It is not clear, if a roof vent was placed directly above the open floor fire or if one was placed in the gable on either end of the house.All of these factors are important, as they would impact the airflow at the time of the fire.
The composition, mineralogy, and textures preserved in scoria from ancient fires provide constraints on the firing temperature, the source and nature of the fire, and its potential social and cultural implications. Analyses of four scoria fragments preserved in a posthole of an Iron Age longhouse at Store Tovstrup, West Denmark, by scanning electron microscopy, electron microprobe, and laser ablation ICP-MS show these to consist of rounded quartz and orthoclase grains, gas vesicles, and carbonaceous material bonded together by a silicate- and potassium-rich (SiO2 67–69 wt% and K2O 11–14 wt%) melt (now glass). Given the presence of vesicles and carbonaceous material, the fire is indicated to have occurred under restricted air-flow and to have involved decomposition of biomass and soil. The initiation of melting occurred during what was presumably an event of short duration. Simplified ternary phase equilibria point to localized melting initiated around 700–800 °C and continuing to about 1000 °C. The main structure succumbed to char at lower temperatures. Calculations suggest that a mixture of 50% sandy soil, 41% barley straw, and 9% oak branches best explains the low Al2O3, Fe2O3, and Na2O concentrations in the melt phase. The scoria at Store Tovstrup most likely originated from a short duration burning with restricted air-flow resulting in the collapse and charring of daub walls. The fire was intentional and set after the house had been cleared of household goods. It is generally presumed that houses from this period had either reed thatched or turf roofs (Webley, 2008).In this case, a turf roof might be indicated by the presence of a mottled layer of light and dark sand, surrounding the burned Tovstrup house.This layer looks consistent with the original sandy topsoil in the area, before intensive farming added to the content of organic material in the topsoil.
The composition, mineralogy, and textures preserved in scoria from ancient fires provide constraints on the firing temperature, the source and nature of the fire, and its potential social and cultural implications. Analyses of four scoria fragments preserved in a posthole of an Iron Age longhouse at Store Tovstrup, West Denmark, by scanning electron microscopy, electron microprobe, and laser ablation ICP-MS show these to consist of rounded quartz and orthoclase grains, gas vesicles, and carbonaceous material bonded together by a silicate- and potassium-rich (SiO2 67–69 wt% and K2O 11–14 wt%) melt (now glass). Given the presence of vesicles and carbonaceous material, the fire is indicated to have occurred under restricted air-flow and to have involved decomposition of biomass and soil. The initiation of melting occurred during what was presumably an event of short duration. Simplified ternary phase equilibria point to localized melting initiated around 700–800 °C and continuing to about 1000 °C. The main structure succumbed to char at lower temperatures. Calculations suggest that a mixture of 50% sandy soil, 41% barley straw, and 9% oak branches best explains the low Al2O3, Fe2O3, and Na2O concentrations in the melt phase. The scoria at Store Tovstrup most likely originated from a short duration burning with restricted air-flow resulting in the collapse and charring of daub walls. The fire was intentional and set after the house had been cleared of household goods. The dating of the houses is typologically estimated, as no 14C are dates are available.Typological criteria include the distance between individual postholes, the presence/absence of foundations for walls and inner structures as well as the overall layout of the houses (Rindel, 1997; Ethelberg et al., 2003; Hansen, 2018b).Typological dating is oftentimes more accurate than carbon dates within the pre-Roman Iron Age time frame, because of the so-called Hallstatt plateau (800–400 BCE) on the calibration curve (Haidas, 2008; Hamilton et al., 2015).On this background, the tentative date for the pre-Roman house is 500–100 BCE and the Roman house roughly the first century AD.
The composition, mineralogy, and textures preserved in scoria from ancient fires provide constraints on the firing temperature, the source and nature of the fire, and its potential social and cultural implications. Analyses of four scoria fragments preserved in a posthole of an Iron Age longhouse at Store Tovstrup, West Denmark, by scanning electron microscopy, electron microprobe, and laser ablation ICP-MS show these to consist of rounded quartz and orthoclase grains, gas vesicles, and carbonaceous material bonded together by a silicate- and potassium-rich (SiO2 67–69 wt% and K2O 11–14 wt%) melt (now glass). Given the presence of vesicles and carbonaceous material, the fire is indicated to have occurred under restricted air-flow and to have involved decomposition of biomass and soil. The initiation of melting occurred during what was presumably an event of short duration. Simplified ternary phase equilibria point to localized melting initiated around 700–800 °C and continuing to about 1000 °C. The main structure succumbed to char at lower temperatures. Calculations suggest that a mixture of 50% sandy soil, 41% barley straw, and 9% oak branches best explains the low Al2O3, Fe2O3, and Na2O concentrations in the melt phase. The scoria at Store Tovstrup most likely originated from a short duration burning with restricted air-flow resulting in the collapse and charring of daub walls. The fire was intentional and set after the house had been cleared of household goods. The scoria particles containing preserved glass were from soil sampled from mostly postholes in a Roman period house or from postholes of uncertain origin.The scoria examined in details in this study originates from one posthole marked X8582 in Fig. 1B.This Roman house was constructed on a partially preserved burned debris field of an older pre-Roman house.The debris is believed to have covered a larger area, but is preserved in this spot because of a small depression in the ground, preserving it from modern ploughing.The scoria fragments were thus secondarily deposited and pre-dates the Roman house.The holes for the Roman roof bearing posts were dug through the remains of the older, pre-Roman house and the scoria fragments deposited and preserved as the result of the construction of the later Roman house.
Compared with North America or northern Africa, prehistoric basketry has rarely been studied in East Asia. Even in Japan where plant remains excavated from lowland sites have been studied extensively in the past thirty years, the study of basketry lagged behind that of other wooden artifacts, and materials of less than 1000 baskets of the Jomon period have been identified versus more than 21,300 wooden artifacts and 32,500 natural woods of the same period so far identified. The Higashimyou site in Saga prefecture of western Japan yielded over 700 baskets including basket fragments of the initial Jomon period from 8000 to 7300 cal BP and provided a good material to examine the material selection and basketry technique of the incipient basketry manufacture in Japan. The identification of basket materials revealed that large baskets were made mostly with splints of two arboreal species, Sapindus mukorossi and Ficus erecta, and that small ones were made mostly with stems of two vine species, Sinomenium acutum and Trachelospermum. The basketry techniques used at this site showed that the bodies of the baskets were made by various types of twill, weave, and twining, and that the bodies of large ones were usually made of two kinds of technique across belts in the middle of the bodies. The employment of various basketry techniques at this site showed that most of the basketry techniques used in later Jomon and following periods were already established in the initial Jomon period. Occurrences of remains of the material plants around the site, the archaeological record of these plants in Japan, and floristic studies of present Japan indicated that appropriate materials used for the baskets were not easily available around the site and that some kind of resources management for basketry materials must have existed around the settlement. Among archaeological wooden objects, basketry is regarded as a kind of portable objects that can have important information for understanding subtle prehistoric cultural and social differences (Sands, 2013).Prehistoric basketry has so far been studied extensively in North America (Adovasio, 2010; Carriere and Croes, 2018) and northern Africa (Wendrich, 2000; El Hadidi and Hamdy, 2011; Di Lernia et al., 2012; Wendrich and Holdaway, 2018), and their studies showed that prehistoric baskets can provide information on regional differences in communities and their historic changes.The basket materials in these areas clearly showed local differences in material selection, such as roots, boughs, splints, and bark of Thuja plicata D. Don, Picea sitchensis (Bong.)Carrière, Acer circinatum Pursh, and Prunus emarginata (Douglas) Walp.?and leaves and stems of monocots in North America (Croes and Hawes, 2013; Carriere and Croes, 2018), Phoenix dactylifera L., Hyphaene thebaica Mart., Cyprus papyrus L., and others taxa in Egypt (Wendrich, 2000; El Hadidi and Hamdy, 2011), and monocot leaves or stems and animal fibers in Libya (Di Lernia et al., 2012).By using cluster and cladistic analyses of basketry attributes and types, Croes (2001, 2015), Croes et al. (2009), and Carriere and Croes (2018) further proposed a hypothetical history of stylistic and ethnic continuity from basketry remains found in the Northwest Coast of North America.In northern Africa, use of baskets is discussed in relation to the availably and storage of wild cereals and resource exploitation (Di Lernia et al., 2012) or to mobility and sedentism versus long-term caching by basketry lined storage pits (Wendrich and Holdaway, 2018).Compared with the studies in these areas, few studies have been carried out on prehistoric basketry in East Asia including Japan, and regional or cultural differences and their historical changes have not been clarified (Matsunaga, 2015).
Compared with North America or northern Africa, prehistoric basketry has rarely been studied in East Asia. Even in Japan where plant remains excavated from lowland sites have been studied extensively in the past thirty years, the study of basketry lagged behind that of other wooden artifacts, and materials of less than 1000 baskets of the Jomon period have been identified versus more than 21,300 wooden artifacts and 32,500 natural woods of the same period so far identified. The Higashimyou site in Saga prefecture of western Japan yielded over 700 baskets including basket fragments of the initial Jomon period from 8000 to 7300 cal BP and provided a good material to examine the material selection and basketry technique of the incipient basketry manufacture in Japan. The identification of basket materials revealed that large baskets were made mostly with splints of two arboreal species, Sapindus mukorossi and Ficus erecta, and that small ones were made mostly with stems of two vine species, Sinomenium acutum and Trachelospermum. The basketry techniques used at this site showed that the bodies of the baskets were made by various types of twill, weave, and twining, and that the bodies of large ones were usually made of two kinds of technique across belts in the middle of the bodies. The employment of various basketry techniques at this site showed that most of the basketry techniques used in later Jomon and following periods were already established in the initial Jomon period. Occurrences of remains of the material plants around the site, the archaeological record of these plants in Japan, and floristic studies of present Japan indicated that appropriate materials used for the baskets were not easily available around the site and that some kind of resources management for basketry materials must have existed around the settlement. In Japan, the study of basketry of the Jomon period, a Neolithic period in Japan from 16,000 to 2500 cal BP, characterized by manufacture of pottery, formation of pit dwellings, and lack of agriculture, lagged behind that of other wooden artifacts.This is probably because only bamboos were used for basketry in the Kanto area, central Japan, where important lowland sites of the Jomon period were first excavated intensively, leading to the loss of interest among wood anatomists studying archaeological plant remains (Noshiro and Sasaki, 2014b).According to a review of archaeological wooden remains excavated from Japan up to 2005, over 21,300 wooden artifacts and 32,500 natural woods of the Jomon period were reported and identified, whereas only 80 basket materials of the Jomon period were reported and identified (Itoh and Yamada, 2012).With the discovery of a large number of baskets using various materials from splints to vine stems and the development of a plant anatomical technique for treating fragile basket materials since the early 2000s, a renewed interest is shown to the basketry of the Jomon period (Horikawa, 2011; Matsunaga, 2015).At the Shimoyakebe site, Tokyo, in the Kanto area, Sasaki et al. (2014) showed that the people of the middle to final Jomon periods made baskets with various weaving and twilling techniques by using bamboo splints, probably of Pleioblastus chino (Franch.et Sav.)Makino, that were 0.26 to 0.64 mm thick, made by scraping off two thirds to seven eighth of the inner tissue of split culms.A recent review of basketry remains found since 1980 in Japan lists 1146 baskets from 123 archaeological sites ranging from the Jomon period to the Muromachi period of the middle age, including 845 baskets from 46 site of the initial to final Jomon periods ranging from ca. 11,000 to ca. 2500 cal BP (Horikawa, 2011).Horikawa (2011) pointed out the existence of regional differences in material selection and basketry technique through the studied periods, but the number of identified basket materials for the Jomon period was too few, only 140, to discuss regional differences or cultural background during this period.
Compared with North America or northern Africa, prehistoric basketry has rarely been studied in East Asia. Even in Japan where plant remains excavated from lowland sites have been studied extensively in the past thirty years, the study of basketry lagged behind that of other wooden artifacts, and materials of less than 1000 baskets of the Jomon period have been identified versus more than 21,300 wooden artifacts and 32,500 natural woods of the same period so far identified. The Higashimyou site in Saga prefecture of western Japan yielded over 700 baskets including basket fragments of the initial Jomon period from 8000 to 7300 cal BP and provided a good material to examine the material selection and basketry technique of the incipient basketry manufacture in Japan. The identification of basket materials revealed that large baskets were made mostly with splints of two arboreal species, Sapindus mukorossi and Ficus erecta, and that small ones were made mostly with stems of two vine species, Sinomenium acutum and Trachelospermum. The basketry techniques used at this site showed that the bodies of the baskets were made by various types of twill, weave, and twining, and that the bodies of large ones were usually made of two kinds of technique across belts in the middle of the bodies. The employment of various basketry techniques at this site showed that most of the basketry techniques used in later Jomon and following periods were already established in the initial Jomon period. Occurrences of remains of the material plants around the site, the archaeological record of these plants in Japan, and floristic studies of present Japan indicated that appropriate materials used for the baskets were not easily available around the site and that some kind of resources management for basketry materials must have existed around the settlement. The Higashimyou site in Saga prefecture of western Japan yielded over 700 baskets including basket fragments of the initial Jomon period and provided a good material to examine the material selection and basketry technique of the incipient basketry manufacture and use in Japan.The archaeological reports of the Higashimyou site that was excavated between 2004 and 2007 were published in 2009, and the selection of basket materials and the basketry techniques were reported in Noshiro et al. (2009) and Saga City Board of Education (2009, 2016c), respectively.However, the material selection and basketry techniques at this site have not been discussed in the context of the use of plant resources in prehistoric Japan.In Japan the existence of a prehistoric management of plant resources has been recognized since the early Jomon period starting at ca. 7000 cal BP, and this recognition of a prehistoric management system of plant resources has shown that the Jomon people had a middle ground subsistence system between hunter-gatherers and farmers (Crawford, 2008; Noshiro and Sasaki, 2014a, 2014b).By citing occurrences of various cultigens and putative ones at Jomon sites, Crawford (2011) pointed out the possible production of not only food, but also various other resources by the Jomon people, and the basketry materials can possibly be included in this production of resources around settlements.Moreover, a later analysis of basketry remains pointed out an existence of regional differences in basketry materials during the Jomon period (Noshiro and Sasaki, 2014b).
We propose SECUR-AMA, an Active Malware Analysis (AMA) framework for Android. (AMA) is a technique that aims at acquiring knowledge about target applications by executing actions on the system that trigger responses from the targets. The main strength of this approach is the capability of extracting behaviors that would otherwise remain invisible. A key difference from other analysis techniques is that the triggering actions are not selected randomly or sequentially, but following strategies that aim at maximizing the information acquired about the behavior of the target application. Specifically, we design SECUR-AMA as a framework implementing a stochastic game between two agents: an analyzer and a target application. The strategy of the analyzer consists in a reinforcement learning algorithm based on Monte Carlo Tree Search (MCTS) to efficiently search the state and action spaces taking into account previous interactions in order to obtain more information on the target. The target model instead is created online while playing the game, using the information acquired so far by the analyzer and using it to guide the remainder of the analysis in an iterative process. We conduct an extensive evaluation of SECUR-AMA analyzing about 1200 real Android malware divided into 24 families (classes) from a publicly available dataset, and we compare our approach with multiple state-of-the-art techniques of different types, including passive and active approaches. Results show that SECUR-AMA creates more informative models that allow to reach better classification results for most of the malware families in our dataset. In recent years the increasing reliance on computer systems and the increasing use of Internet, wireless networks, autonomous systems, e.g., cars, boats, as well as the growth of smart and tiny devices as part of the Internet of Things (IoT) resulted in a corresponding increase in the number of cyber-security flaws.In particular, Android is one of the most diffused operating systems employed in smartphones and IoT devices, making it the preferred target for cyber-criminals due to its huge market share (Cheung, 2018).Android malware are then one of the biggest threats in IT security nowadays, with millions of malicious applications released every year.Analyzing such amount of threats have become almost impossible for human security experts, and consequently, tools based on machine learning are fundamental to automate and speed up the process.In this work we aim to extend an analysis technique we proposed in Sartea and Farinelli (2017), by creating a fully fledged automated framework for Android malware analysis that substitutes the manual analysis of an unknown application, i.e., by performing automated test interactions and adapting to what is observed.Broadly, the concept of executing specific actions to perform a better analysis can be linked to the general framework of active learning, and recently there has been a specific interest in applying active learning techniques to malware analysis (Nissim et al., 2014).In that work, authors propose the use of an SVM classifier to select which samples (already analyzed) should be fed to the classifier, so to refine the classification bounds.In this work, our aim is to generate malware models that can be studied by a human security expert or processed by automated techniques (clustering, classification) for comparison.Hence, we focus on the decision making side of the analysis by devising an intelligent strategy for the analyzer action selection.For this reason, SECUR-AMA differs from active learning approaches where the methodology is usually tied to the specific choice of classifier in order to improve the classification bounds, e.g., k-NN and naïve Bayes (Wei et al., 2015), logistic regression (Guo and Schuurmans, 2007), linear regression (Yu et al., 2006), SVM (Tong and Koller, 2002).
We propose SECUR-AMA, an Active Malware Analysis (AMA) framework for Android. (AMA) is a technique that aims at acquiring knowledge about target applications by executing actions on the system that trigger responses from the targets. The main strength of this approach is the capability of extracting behaviors that would otherwise remain invisible. A key difference from other analysis techniques is that the triggering actions are not selected randomly or sequentially, but following strategies that aim at maximizing the information acquired about the behavior of the target application. Specifically, we design SECUR-AMA as a framework implementing a stochastic game between two agents: an analyzer and a target application. The strategy of the analyzer consists in a reinforcement learning algorithm based on Monte Carlo Tree Search (MCTS) to efficiently search the state and action spaces taking into account previous interactions in order to obtain more information on the target. The target model instead is created online while playing the game, using the information acquired so far by the analyzer and using it to guide the remainder of the analysis in an iterative process. We conduct an extensive evaluation of SECUR-AMA analyzing about 1200 real Android malware divided into 24 families (classes) from a publicly available dataset, and we compare our approach with multiple state-of-the-art techniques of different types, including passive and active approaches. Results show that SECUR-AMA creates more informative models that allow to reach better classification results for most of the malware families in our dataset. Current malware analysis methodologies can be broadly divided into static, where program’s code is examined without actually executing it (Sharif et al., 2008; Lakhotia et al., 2013; Yang et al., 2014; Suarez-Tangil et al., 2014b), and dynamic, which refers to methods that execute programs in a safe environment in order to observe their behaviors (Meng et al., 2016; Gascon et al., 2013; Zhang et al., 2014; Shin et al., 2011; Rieck et al., 2011).All the mentioned techniques however suffer from an important limitation: they are passive, meaning that no interaction happens between the analyzer and the target program (in contrast to what a human security expert would usually do).Unfortunately, both static and passive analysis present weaknesses that an adversary can exploit in order to harden the malware code, and consequently thwart the analysis process.For instance, a malware designer could implement obfuscation or encryption mechanisms to make static analysis inconclusive.Although dynamic analysis is almost immune to such misleading adversary measures, it suffers from implicit code coverage issues and can be countered by anti-emulation techniques.A combination of static and dynamic analysis features may help mitigating the limitations of each other, and the recent work of Martín et al. (2019a) aims at doing this.Feature selection is indeed important as attackers may exploit the knowledge about how a detection tool uses such features to mislead the process.A case study of this problem is provided in Calleja et al. (2018), in which authors analyze multiple defense models and also propose how to strengthen them in order to be more resilient to feature thwarting attacks.In particular, different anti-virus software focus on different features, resulting in malware signatures that can be used to identify different families (classes) of malicious applications, e.g., adware, spyware, ransomware, as in the work of Martín et al. (2019b).In Mariconti et al. (2017), the call graph of the applications is used to distinguish a malware from a harmless software, without considering the possible categories.The same goal is pursued in Zhang et al. (2019), where authors aim at using non standard static features and apply the widely used n-gram model to eXtensible Markup Language (XML) strings of the application executable.In our approach we tackle the problem of identification of malware families, a subsequent but still important step, that is performed after an application has been flagged as a possible malicious sample in order to acquire further information on the behaviors and then select the proper countermeasures.Nonetheless, it has been assessed that in many cases interaction is fundamental to extract behaviors that are only exhibited when triggered since malware often try to mask their real intentions (Moser et al., 2007).For this reason, recent works have introduced an active type of dynamic analysis that performs actions on the system to stimulate a target program.In Suarez-Tangil et al. (2014a), authors build an analyzer that aims at reproducing specific activation conditions to trigger malicious payloads relying on stochastic models extracted by past samples of user behaviors.The active procedure proposed in Martín et al. (2018) instead is very effective in assessing malware families, however the triggering policy targets the Graphical User Interface (GUI) with random actions, without employing an intelligent strategy.Another interesting active approach based on random triggering actions is (Bhandari et al., 2018), a runtime semantic-aware malware detector resilient to code injection and capable of deciding whether an application is malicious or not.All the mentioned active techniques present no rational target-oriented strategy to stimulate the malware under analysis, therefore more research is required to devise an approach that can select the triggering actions so to maximize the acquired information.
We propose SECUR-AMA, an Active Malware Analysis (AMA) framework for Android. (AMA) is a technique that aims at acquiring knowledge about target applications by executing actions on the system that trigger responses from the targets. The main strength of this approach is the capability of extracting behaviors that would otherwise remain invisible. A key difference from other analysis techniques is that the triggering actions are not selected randomly or sequentially, but following strategies that aim at maximizing the information acquired about the behavior of the target application. Specifically, we design SECUR-AMA as a framework implementing a stochastic game between two agents: an analyzer and a target application. The strategy of the analyzer consists in a reinforcement learning algorithm based on Monte Carlo Tree Search (MCTS) to efficiently search the state and action spaces taking into account previous interactions in order to obtain more information on the target. The target model instead is created online while playing the game, using the information acquired so far by the analyzer and using it to guide the remainder of the analysis in an iterative process. We conduct an extensive evaluation of SECUR-AMA analyzing about 1200 real Android malware divided into 24 families (classes) from a publicly available dataset, and we compare our approach with multiple state-of-the-art techniques of different types, including passive and active approaches. Results show that SECUR-AMA creates more informative models that allow to reach better classification results for most of the malware families in our dataset. The approach drawn in this paper presents a new analysis framework based on an adversarial, two-agent environment, where two players, i.e., analyzer and malware, have opposite objectives: the analyzer aims at discovering the highest amount of information about the malware, while the malware aims at hiding its malicious policy to the analyzer.This analysis design is justified by works where practical problems of interest are represented as multi-agent systems in which intelligent and autonomous entities interact within a complex dynamic environment learning information and adapting their behavior accordingly.In particular, the work presented in Williamson et al. (2012) builds upon multi-agent system techniques by formalizing Active Malware Analysis (AMA) using game-theoretic approaches, and specifically stochastic games.In more detail, the analysis game involves two players: an analyzer and a malicious program, where the former tries to trigger the latter into showing behaviors that would otherwise remain undetected.The analyzer policy uses past observations of the malware agent behavior in response to the triggering actions in order to select the next estimated best triggering action.A key point of Williamson et al. (2012) lies in the necessary availability of a predefined model that is manually designed to represent the specific parts of the system that the analysis has to monitor.A first step in the direction of removing the prerequisite of a model is presented in Sartea et al. (2016), where the perspective of the analysis has been changed by formalizing the behaviors as execution traces (sequences of API calls) of the observed program.We developed an automated algorithm to generate models representing multiple known malware families to be given as input to the analysis process in order to identify if an unknown application belongs to one of the families embedded in the model.Further progress has been achieved in Sartea and Farinelli (2017) where we present a preliminary version of the proposed method: we completely remove the requirement of a pre-specified model for the analysis by devising a reinforcement learning algorithm based on Monte Carlo Tree Search (MCTS) to generate the model of each target application at runtime and by using it to guide the process.The result is a specific model for each analyzed program that is independent from the underlining system and is represented with multiple Markov chains embedding the observed execution traces.Every model encodes the policy of a target application and they can be compared and grouped using standard machine learning techniques, e.g., classification, clustering, etc.
We propose SECUR-AMA, an Active Malware Analysis (AMA) framework for Android. (AMA) is a technique that aims at acquiring knowledge about target applications by executing actions on the system that trigger responses from the targets. The main strength of this approach is the capability of extracting behaviors that would otherwise remain invisible. A key difference from other analysis techniques is that the triggering actions are not selected randomly or sequentially, but following strategies that aim at maximizing the information acquired about the behavior of the target application. Specifically, we design SECUR-AMA as a framework implementing a stochastic game between two agents: an analyzer and a target application. The strategy of the analyzer consists in a reinforcement learning algorithm based on Monte Carlo Tree Search (MCTS) to efficiently search the state and action spaces taking into account previous interactions in order to obtain more information on the target. The target model instead is created online while playing the game, using the information acquired so far by the analyzer and using it to guide the remainder of the analysis in an iterative process. We conduct an extensive evaluation of SECUR-AMA analyzing about 1200 real Android malware divided into 24 families (classes) from a publicly available dataset, and we compare our approach with multiple state-of-the-art techniques of different types, including passive and active approaches. Results show that SECUR-AMA creates more informative models that allow to reach better classification results for most of the malware families in our dataset. In this work we extend such approach and propose SECUR-AMA, a refined AMA framework implementing the technique introduced in Sartea and Farinelli (2017).In particular, we provide a formal definition of the analysis game, a more detailed explanation of the learning algorithm and a throughout empirical evaluation on a large real world dataset of Android malware.Moreover, we compare with different state-of-the-art malware analysis methodologies both static and dynamic, i.e., CANDYMAN (Martín et al., 2018) and DENDROID (Suarez-Tangil et al., 2014b).In more detail, experiments are conducted on a publicly available dataset of real malware composed of about 1200 samples divided into 24 families (classes).Results show that SECUR-AMA favorably compares with existing techniques in analyzing complex trigger-based malware while being competitive against classical malware.We also provide a running example detailing all the different steps of the methodology and implementation details.In particular, this work expands the previous in the following:
To describe seizures occurring in 3 healthy adults with influenza infection. Seizures associated to influenza infection are rare in adults without encephalitis. Clinical observations of 3 healthy adult patients with influenza A and B infection and seizures. We present here 3 healthy adult patients with seizures related to influenza A or B infection without evidence encephalitis, encephalopathy or any other cause for seizures. Prognosis was excellent. Seizures can occur in healthy adults with influenza infection without evidence of encephalitis, a possibility to be borne in mind to avoid potentially harmful therapeutic and diagnostic procedures. Influenza is an acute infectious respiratory disease with great public health impact because of its worldwide distribution and morbidity.Although most influenza complications are pulmonary, extrapulmonary complications can occur, including neurological disturbances.Encephalopathy, followed by seizures or status epilepticus are the main neurological complications, occurring more frequently in children during natural influenza infection [1].Other, less common complications include acute disseminated encephalomyelitis, Reye's syndrome, Guillain–Barre syndrome, post-viral parkinsonism, cerebellitis, and acute necrotizing encephalitis [1].They can occur during infection, where most fatal and severe cases are observed, or after vaccination, which usually conveys a more favorable prognosis.
The analysis of human behavior is a popular topic of research since it allows obtaining specific information about individuals, their motivations, and the problems and difficulties they can encounter. Human behavior can be grouped to elaborate profiles that would enable the classification of individuals. Nevertheless, the elaboration of profiles related to human behaviors presents some difficulties associated with the volume of data and the number of parameters typically considered. Thus, the development of software able to automatize the manipulation of data through graphical assistants to produce understandable visualizations of the human behaviors is crucial. In this paper, the VISUVER framework is presented. It uses finite state machines to represent and visualize the dynamic human behavior automatically. This behavior could be provided by real data collected by specific sensors or simulated data. The state machines are built in sequential steps in order to illustrate the dynamic evolution of the behavior over time. VISUVER also includes similarity metrics based on text mining techniques to establish possible profiles among the analyzed behaviors. The Intelligent Transportation Systems (ITS) domain has been considered in order to validate the proposal. Human behavior has traditionally been one of the most interesting fields of study for the scientific community (Skinner, 1953) be addressed by several domains of application.The main idea behind these approaches is to obtain knowledge from different tasks achieved by individuals.For instance, this knowledge can indicate which are the human needs (i.e., motivations) (Deci and Ryan, 2000), how they establish relationships with the surrounding environment (e.g., other individuals) (Zastrow and Kirst-Ashman, 2006), which are the most common problems detected (Bonanno, 2004), or how to influence over them in order to modify undesirable behaviors (i.e., persuade individuals) (Cialdini and Cialdini, 2007).
The analysis of human behavior is a popular topic of research since it allows obtaining specific information about individuals, their motivations, and the problems and difficulties they can encounter. Human behavior can be grouped to elaborate profiles that would enable the classification of individuals. Nevertheless, the elaboration of profiles related to human behaviors presents some difficulties associated with the volume of data and the number of parameters typically considered. Thus, the development of software able to automatize the manipulation of data through graphical assistants to produce understandable visualizations of the human behaviors is crucial. In this paper, the VISUVER framework is presented. It uses finite state machines to represent and visualize the dynamic human behavior automatically. This behavior could be provided by real data collected by specific sensors or simulated data. The state machines are built in sequential steps in order to illustrate the dynamic evolution of the behavior over time. VISUVER also includes similarity metrics based on text mining techniques to establish possible profiles among the analyzed behaviors. The Intelligent Transportation Systems (ITS) domain has been considered in order to validate the proposal. In order to analyze human behavior, two different perspectives can be differentiated.The first perspective is focused on gathering data from humans through sensors in order to analyze different parameters.Thus, specific patterns could be detected during the analyzed task and used to predict future behavior.The second perspective is focused on developing models that encompass several previously identified traits in human behavior.These models could also be used to elaborate profiles according to the values of these traits, or they can be included in simulations to check their viability for real situations.
The analysis of human behavior is a popular topic of research since it allows obtaining specific information about individuals, their motivations, and the problems and difficulties they can encounter. Human behavior can be grouped to elaborate profiles that would enable the classification of individuals. Nevertheless, the elaboration of profiles related to human behaviors presents some difficulties associated with the volume of data and the number of parameters typically considered. Thus, the development of software able to automatize the manipulation of data through graphical assistants to produce understandable visualizations of the human behaviors is crucial. In this paper, the VISUVER framework is presented. It uses finite state machines to represent and visualize the dynamic human behavior automatically. This behavior could be provided by real data collected by specific sensors or simulated data. The state machines are built in sequential steps in order to illustrate the dynamic evolution of the behavior over time. VISUVER also includes similarity metrics based on text mining techniques to establish possible profiles among the analyzed behaviors. The Intelligent Transportation Systems (ITS) domain has been considered in order to validate the proposal. These approaches present similar problems related to the amount of data and their understandability (Cooper et al., 2007).Moreover, the human behavior changes over time, being irregular and presenting fluctuations (Baldwin and Baird, 2001).The elaboration of models and the analysis of data become a very demanding task (Strauss, 2017).The visualization of the gathered information (Marcengo and Rapp, 2016) and the use of techniques to simplify data (e.g., Fuzzy Logic Eciolaza et al., 2011) try to mitigate this situation.It leads to the development of systems that address these issues including assistants to guide users and process a significant amount of data making dynamic visualizations of human behavior.
The analysis of human behavior is a popular topic of research since it allows obtaining specific information about individuals, their motivations, and the problems and difficulties they can encounter. Human behavior can be grouped to elaborate profiles that would enable the classification of individuals. Nevertheless, the elaboration of profiles related to human behaviors presents some difficulties associated with the volume of data and the number of parameters typically considered. Thus, the development of software able to automatize the manipulation of data through graphical assistants to produce understandable visualizations of the human behaviors is crucial. In this paper, the VISUVER framework is presented. It uses finite state machines to represent and visualize the dynamic human behavior automatically. This behavior could be provided by real data collected by specific sensors or simulated data. The state machines are built in sequential steps in order to illustrate the dynamic evolution of the behavior over time. VISUVER also includes similarity metrics based on text mining techniques to establish possible profiles among the analyzed behaviors. The Intelligent Transportation Systems (ITS) domain has been considered in order to validate the proposal. This work presents the VISUal VERifying (VISUVER) framework proposed to ease the analysis of human behavior and its evolution over time.It uses finite state machines to represent human behavior, making visual the decisions and tasks achieved by an individual during a certain period of time.It includes a specific module for profiling purposes.This module is able to summarize visual state machines in order to obtain the most important information provided by them.VISUVER can use different similarity metrics to detect behavior patterns.It can also obtain the widest portion of time where human behaviors look similar or correspond to repetitive patterns.
The analysis of human behavior is a popular topic of research since it allows obtaining specific information about individuals, their motivations, and the problems and difficulties they can encounter. Human behavior can be grouped to elaborate profiles that would enable the classification of individuals. Nevertheless, the elaboration of profiles related to human behaviors presents some difficulties associated with the volume of data and the number of parameters typically considered. Thus, the development of software able to automatize the manipulation of data through graphical assistants to produce understandable visualizations of the human behaviors is crucial. In this paper, the VISUVER framework is presented. It uses finite state machines to represent and visualize the dynamic human behavior automatically. This behavior could be provided by real data collected by specific sensors or simulated data. The state machines are built in sequential steps in order to illustrate the dynamic evolution of the behavior over time. VISUVER also includes similarity metrics based on text mining techniques to establish possible profiles among the analyzed behaviors. The Intelligent Transportation Systems (ITS) domain has been considered in order to validate the proposal. In order to validate the proposed approach, the Intelligent Transportation Systems (ITS) domain (Dimitrakopoulos and Demestichas, 2010) is considered.In this area, the drivers’ behavior when performing the driving tasks is a common issue to be studied (Chhabra et al., 2017).As part of the CABINTEC (Intelligent Cabin Truck for Road Transportation) project (Brazalez et al., 2006), a database from a set of driving sessions performed by six professional drivers on a very realistic truck simulator was collected.This project is focused on the study of human factor relevant for the improvement of traffic safety.The driving sessions include different environments: interurban, mountain and urban.
The analysis of human behavior is a popular topic of research since it allows obtaining specific information about individuals, their motivations, and the problems and difficulties they can encounter. Human behavior can be grouped to elaborate profiles that would enable the classification of individuals. Nevertheless, the elaboration of profiles related to human behaviors presents some difficulties associated with the volume of data and the number of parameters typically considered. Thus, the development of software able to automatize the manipulation of data through graphical assistants to produce understandable visualizations of the human behaviors is crucial. In this paper, the VISUVER framework is presented. It uses finite state machines to represent and visualize the dynamic human behavior automatically. This behavior could be provided by real data collected by specific sensors or simulated data. The state machines are built in sequential steps in order to illustrate the dynamic evolution of the behavior over time. VISUVER also includes similarity metrics based on text mining techniques to establish possible profiles among the analyzed behaviors. The Intelligent Transportation Systems (ITS) domain has been considered in order to validate the proposal. Several experiments have been considered to demonstrate the potential of VISUVER.The first one is a test experiment that evidences the functionalities of the framework.The second compares using the similarity metrics provided by VISUVER standard (i.e., non aggressive behavior respecting traffic signals) and unusual behaviors of drivers (i.e., aggressive behavior, possible accidents or risky situations).The third experiment illustrates how the summarization and simplification tasks produce understandable visual representations with low loss of information.This experiment also includes a detailed analysis to make detections related to standard and distractive behaviors.
Hemispherotomy is a surgical treatment indicated in patients with drug-resistant epilepsy due to unilateral hemispheric pathology. Hemispherotomy is less invasive compared with hemispherectomy. We reviewed our experience performing 24 hemispherotomy and report the results of 16 patients with prolonged follow-up of this relatively uncommon procedure in two centers in Indonesia. This is a retrospective observational study conducted from 1999 to July 2019 in two epilepsy neurosurgical centers in Semarang, Indonesia. Surgical techniques included vertical parasagittal hemispherotomy (VPH), peri-insular hemispherotomy (PIH), and modified PIH called the Shimizu approach (SA). The postoperative assessment was carried out using the Engel classification system of seizure outcome. Seizure freedom (Engel class I) outcome was achieved in 10 patients (62.5%), class II in 3 patients (18.7%), class III in 2 patients (12.5%), and class IV in 1 patient (6.3%) with follow-up duration spanning from 24 to 160 months. To the best of our knowledge, this series is the most extensive documentation of hemispherotomy in an Indonesian population. Hemispherotomy is a potential surgical treatment indicated for patients with drug-resistant epilepsy due to unilateral hemispheric pathology [1,2].The underlying etiology for unilateral hemispheric pathology may include conditions such as Rasmussen syndrome, Sturge–Weber syndrome, porencephaly, perinatal stroke and disturbances in neuronal migration (e.g., hemimegalencephaly, cortical dysplasia, and hemiconvulsion–hemiplegia–epilepsy syndrome) [3,4].
Hemispherotomy is a surgical treatment indicated in patients with drug-resistant epilepsy due to unilateral hemispheric pathology. Hemispherotomy is less invasive compared with hemispherectomy. We reviewed our experience performing 24 hemispherotomy and report the results of 16 patients with prolonged follow-up of this relatively uncommon procedure in two centers in Indonesia. This is a retrospective observational study conducted from 1999 to July 2019 in two epilepsy neurosurgical centers in Semarang, Indonesia. Surgical techniques included vertical parasagittal hemispherotomy (VPH), peri-insular hemispherotomy (PIH), and modified PIH called the Shimizu approach (SA). The postoperative assessment was carried out using the Engel classification system of seizure outcome. Seizure freedom (Engel class I) outcome was achieved in 10 patients (62.5%), class II in 3 patients (18.7%), class III in 2 patients (12.5%), and class IV in 1 patient (6.3%) with follow-up duration spanning from 24 to 160 months. To the best of our knowledge, this series is the most extensive documentation of hemispherotomy in an Indonesian population. As it does not involve cerebral artery ligation and hemisphere removal, hemispherotomy has relatively lower complications compared with more invasive hemispherectomy which may result in higher rates of hemorrhage, hydrocephalus, subdural fluid collections, and cerebral hemosiderosis [5,6].In functional hemispherotomy, neural pathways between hemispheres are disconnected without compromising the vasculature in either hemisphere.Since its first description, there have been two major techniques for hemispherotomy: the vertical parasagittal hemispherotomy (VPH), initially described by Delalande [7,8] and the peri-insular hemispherotomy (PIH) detailed by Villemure [9,10].Other authors have described modifications of either approach, including PIH modification by Shimizu and Maehara [11] who also reported satisfactory results [12].
Several micro-archaeological methods are suggested in this study in order to identify cess deposits. These methods were deployed at a Near Eastern mound (Megiddo, Israel), yet are applicable to any archaeological site anywhere in the world. The study presented here, was performed on a 2–3 mm thick yellowish fibrous material, ca. 40 × 15 cm in size, which was discovered in Area H at Tel Megiddo in relation to a well-built structure dating to the Late Bronze Age IIA (mid-14th century BCE). Area H is located near the remains of a large Late Bronze Age palace, which had been excavated in the early 20th century. In order to reveal the nature of the yellowish fibrous material we carried out infrared spectroscopy, petrographic microscopy and lipid analyses. The results led us to suggest that this substance is related to fecal matter. We therefore analyzed it for pollen and gastrointestinal parasite remains. While the latter were for the most part absent, the palynological investigation provided information about dietary components that are usually under-represented in the reconstruction of vegetative diets, especially beverages and possible use of medicinal plants, consumed by the Megiddo residents, who may had some link to the palace. The paper demonstrates how diverse micro-archaeological analyses complement each other, and when applied in concert yield novel information about the past. On the final day of excavations of the 2012 season at Tel Megiddo (northern Israel, Fig. 1) a yellowish fibrous feature was detected.This 2–3 mm thick feature, ca. 40 × 15 cm in size (Fig. 2), was associated with a well-built structure in Level H-14, which dates, based on ceramic finds, to the Late Bronze Age IIA in the 14th century BCE (for radiocarbon dating, see Boaretto, forthcoming).Area H is located in the northwestern sector of the site (Fig. 3), near the Late Bronze Age palace unearthed in the course of the University of Chicago excavations in the 1930s (Area AA, Fig. 4; Loud, 1948), and in proximity to the gate of the city (Fig. 3).Level H-14 is contemporaneous with the Amarna archives – the diplomatic correspondence of Pharaohs Amenophis III and Amenophis IV (Akhenaten) with Near Eastern kingdoms and Canaanite petty-kings, written in Akkadian cuneiform and found in Egypt in the late 19th century CE.Among the ca. 370 tablets, there are six that were sent by Biridiya, the ruler of Megiddo, presumably one of the inhabitants of the Megiddo palace.
Several micro-archaeological methods are suggested in this study in order to identify cess deposits. These methods were deployed at a Near Eastern mound (Megiddo, Israel), yet are applicable to any archaeological site anywhere in the world. The study presented here, was performed on a 2–3 mm thick yellowish fibrous material, ca. 40 × 15 cm in size, which was discovered in Area H at Tel Megiddo in relation to a well-built structure dating to the Late Bronze Age IIA (mid-14th century BCE). Area H is located near the remains of a large Late Bronze Age palace, which had been excavated in the early 20th century. In order to reveal the nature of the yellowish fibrous material we carried out infrared spectroscopy, petrographic microscopy and lipid analyses. The results led us to suggest that this substance is related to fecal matter. We therefore analyzed it for pollen and gastrointestinal parasite remains. While the latter were for the most part absent, the palynological investigation provided information about dietary components that are usually under-represented in the reconstruction of vegetative diets, especially beverages and possible use of medicinal plants, consumed by the Megiddo residents, who may had some link to the palace. The paper demonstrates how diverse micro-archaeological analyses complement each other, and when applied in concert yield novel information about the past. The fragile-looking yellowish fibrous feature was removed from Level H-14 for further analysis.It was extracted as a block of sediment, ca. 30 cm thick, covered by plaster of Paris (gypsum) to keep it intact.Attempts to reveal the nature of the find with the naked eye failed.We therefore turned to several micro-archaeological investigations: infrared spectroscopy, petrographic microscopy and lipid analyses.The results led us to suspect that it was related to cess deposits.Based on this hypothesis, we further analyzed the yellowish fibrous material for pollen and gastrointestinal parasite remains.
Automated modeling aims at the induction of mathematical models, both their structure and parameter values, from time-series measurements of observed system variables. In this paper, we address the task of model structure selection, i.e., selecting an optimal structure from a user-specified finite set of alternative model structures, using various approaches to combinatorial search. We propose a mapping of the set of candidate model structures to a fixed-length, vector representation allowing the use of an arbitrary search algorithm as a solver of the structure selection task. We perform a comparative analysis of the performance of thirteen variants of several search algorithms, ranging from ones with high intensification, i.e., focus on neighborhood of the best candidate solutions, to ones with high diversification, i.e., focus on covering the entire search space. The empirical analysis involves eight tasks of reconstructing known models of dynamical systems from synthetic and measured data. The results of the analysis show that search algorithms involving moderate diversification methods have superior performance on the structure selection task. The empirical analysis also reveals that this finding is related to specific properties of the search space of candidate model structures. Computational scientific discovery is a major research topic since the early days of artificial intelligence.Scientific discovery has been approached with general heuristic methods for problem solving (Langley et al., 1987) or with combinations of knowledge representation formalisms and reasoning methods (Lindsay et al., 1993).Recent technological advances in measurement, i.e., data collecting equipment, on one hand, and data storage and processing equipment on the other hand, reestablish the importance of computational scientific discovery.Kitano (2016), following the paradigm of discovery informatics (Gil et al., 2014), establishes scientific discovery as a grand challenge for artificial intelligence, calling for development of “an AI system that can make major scientific discoveries”.
Automated modeling aims at the induction of mathematical models, both their structure and parameter values, from time-series measurements of observed system variables. In this paper, we address the task of model structure selection, i.e., selecting an optimal structure from a user-specified finite set of alternative model structures, using various approaches to combinatorial search. We propose a mapping of the set of candidate model structures to a fixed-length, vector representation allowing the use of an arbitrary search algorithm as a solver of the structure selection task. We perform a comparative analysis of the performance of thirteen variants of several search algorithms, ranging from ones with high intensification, i.e., focus on neighborhood of the best candidate solutions, to ones with high diversification, i.e., focus on covering the entire search space. The empirical analysis involves eight tasks of reconstructing known models of dynamical systems from synthetic and measured data. The results of the analysis show that search algorithms involving moderate diversification methods have superior performance on the structure selection task. The empirical analysis also reveals that this finding is related to specific properties of the search space of candidate model structures. The task of scientific discovery that we address is the one of mathematical modeling of dynamical systems from observational, measurement data.Most commonly, dynamical systems are modeled with ordinary differential equations, which express the time derivatives (i.e., rates of dynamical change) of system variables as functions of the current values of the system variables and constant model parameters, specifying the system behavior.Thus, the modeling of dynamical systems can be approached by using equation discovery (Todorovski, 2010), also referred to as symbolic regression (Schmidt and Lipson, 2009).Equation discovery aims at automatic induction of mathematical models, both their structure and parameter values, from time-series measurements of observed system variables.This is in contrast with standard regression in machine learning (Hastie et al., 2009), where the focus is on estimating the parameter values of a model with a fixed structure (i.e., linear regression) or a family of models with shared structure (i.e., decision trees).
Automated modeling aims at the induction of mathematical models, both their structure and parameter values, from time-series measurements of observed system variables. In this paper, we address the task of model structure selection, i.e., selecting an optimal structure from a user-specified finite set of alternative model structures, using various approaches to combinatorial search. We propose a mapping of the set of candidate model structures to a fixed-length, vector representation allowing the use of an arbitrary search algorithm as a solver of the structure selection task. We perform a comparative analysis of the performance of thirteen variants of several search algorithms, ranging from ones with high intensification, i.e., focus on neighborhood of the best candidate solutions, to ones with high diversification, i.e., focus on covering the entire search space. The empirical analysis involves eight tasks of reconstructing known models of dynamical systems from synthetic and measured data. The results of the analysis show that search algorithms involving moderate diversification methods have superior performance on the structure selection task. The empirical analysis also reveals that this finding is related to specific properties of the search space of candidate model structures. Both equation discovery and symbolic regression methods rely upon decomposition of the task at hand into two interleaved subtasks of structure identification and parameter estimation.The first is to identify a proper structure (expression) of the model equations.The second is to fit the values of the constant model parameters to the measurements.Both tasks aim at minimizing the discrepancy between the simulated behavior of the model and the measured behavior of the observed system.While the standard approach to parameter estimation is to reformulate it into a numerical optimization task, the task of structure identification has often been approached as a combinatorial optimization problem.Symbolic regression methods (Schmidt and Lipson, 2009; de França, 2018) are often purely data-driven and employ evolutionary algorithms to explore the unconstrained space of all possible arithmetical expressions for building mathematical models.To address the obvious issue of overfitting the observed data with unnecessarily complex mathematical models, these methods often use regularization techniques that introduce bias towards simpler models (Brunton et al., 2016).
Automated modeling aims at the induction of mathematical models, both their structure and parameter values, from time-series measurements of observed system variables. In this paper, we address the task of model structure selection, i.e., selecting an optimal structure from a user-specified finite set of alternative model structures, using various approaches to combinatorial search. We propose a mapping of the set of candidate model structures to a fixed-length, vector representation allowing the use of an arbitrary search algorithm as a solver of the structure selection task. We perform a comparative analysis of the performance of thirteen variants of several search algorithms, ranging from ones with high intensification, i.e., focus on neighborhood of the best candidate solutions, to ones with high diversification, i.e., focus on covering the entire search space. The empirical analysis involves eight tasks of reconstructing known models of dynamical systems from synthetic and measured data. The results of the analysis show that search algorithms involving moderate diversification methods have superior performance on the structure selection task. The empirical analysis also reveals that this finding is related to specific properties of the search space of candidate model structures. On the other hand, knowledge-driven equation discovery methods rely on expert knowledge to introduce bias towards model fragments used by scientists and engineers in the domain of interest (Todorovski, 2010; Bradley et al., 2001).This is in analogy with the work of scientists and engineers that derive the structure of the model equations following established modeling principles from the domain of the observed system.The process-based modeling approach (Bridewell et al., 2008; Tanevski et al., 2017), in particular, follows this paradigm by allowing the user to encode a domain-specific knowledge into templates for modeling entities and processes of interactions among entities.In turn, the entities and processes are used as components for building plausible model structures.Process-based modeling methods can then enumerate all plausible model structures and select one that fits the measured behavior of the observed system well.
Automated modeling aims at the induction of mathematical models, both their structure and parameter values, from time-series measurements of observed system variables. In this paper, we address the task of model structure selection, i.e., selecting an optimal structure from a user-specified finite set of alternative model structures, using various approaches to combinatorial search. We propose a mapping of the set of candidate model structures to a fixed-length, vector representation allowing the use of an arbitrary search algorithm as a solver of the structure selection task. We perform a comparative analysis of the performance of thirteen variants of several search algorithms, ranging from ones with high intensification, i.e., focus on neighborhood of the best candidate solutions, to ones with high diversification, i.e., focus on covering the entire search space. The empirical analysis involves eight tasks of reconstructing known models of dynamical systems from synthetic and measured data. The results of the analysis show that search algorithms involving moderate diversification methods have superior performance on the structure selection task. The empirical analysis also reveals that this finding is related to specific properties of the search space of candidate model structures. The result of the knowledge-driven approach is a readily understandable and interpretable model.The structure of the model can be used to explain the observed system behavior in terms of interactions among system entities.This is in contrast with data-driven approaches to symbolic regression that focus primarily on the fit of the models to the observations, sacrificing the interpretability of the model by considering a space of feasible solutions that is composed of general algebraic expressions.While these models, in principle, might provide interpretations and explanations, they require additional effort and inference.Note however, that this advantage comes at a price, since the knowledge-driven approach can only select one of the plausible model structures that stem from the user-provided knowledge specification.A data-driven approach can, on the other hand, discover new accurate models for a given system that have not been considered before.
Automated modeling aims at the induction of mathematical models, both their structure and parameter values, from time-series measurements of observed system variables. In this paper, we address the task of model structure selection, i.e., selecting an optimal structure from a user-specified finite set of alternative model structures, using various approaches to combinatorial search. We propose a mapping of the set of candidate model structures to a fixed-length, vector representation allowing the use of an arbitrary search algorithm as a solver of the structure selection task. We perform a comparative analysis of the performance of thirteen variants of several search algorithms, ranging from ones with high intensification, i.e., focus on neighborhood of the best candidate solutions, to ones with high diversification, i.e., focus on covering the entire search space. The empirical analysis involves eight tasks of reconstructing known models of dynamical systems from synthetic and measured data. The results of the analysis show that search algorithms involving moderate diversification methods have superior performance on the structure selection task. The empirical analysis also reveals that this finding is related to specific properties of the search space of candidate model structures. Knowledge-driven equation discovery follows the general model-solver paradigm in artificial intelligence (Geffner, 2014) that aims at developing solvers for well-defined mathematical models.1In this paradigm, the model provides a convenient high-level modeling language for specifying problems (model instances), the solutions of which are automatically computed by the general solver that can address any model instance.The burden of the user, therefore, is to provide a declarative problem specification (instead of a procedural one) on how the solution should be computed.In our case of modeling dynamical systems the declarative specification includes high-level knowledge about modeling dynamical systems in the particular domain of interest, as well as a specific modeling scenario and a set of assumptions.The solver, then, transforms the high-level declarative specification of the modeling task into a generator of candidate solutions, i.e., candidate structures of the dynamical system model equations to be explored by combinatorial optimization.Most knowledge-driven equation discovery methods rely upon exhaustive enumeration of candidate model structures (Tanevski et al., 2017).While the exhaustive strategy holds promise for finding the optimal model, its applicability to real-life modeling tasks is limited.A more scalable solution is to replace the exhaustive enumeration strategy with an incomplete search strategy.
Automated modeling aims at the induction of mathematical models, both their structure and parameter values, from time-series measurements of observed system variables. In this paper, we address the task of model structure selection, i.e., selecting an optimal structure from a user-specified finite set of alternative model structures, using various approaches to combinatorial search. We propose a mapping of the set of candidate model structures to a fixed-length, vector representation allowing the use of an arbitrary search algorithm as a solver of the structure selection task. We perform a comparative analysis of the performance of thirteen variants of several search algorithms, ranging from ones with high intensification, i.e., focus on neighborhood of the best candidate solutions, to ones with high diversification, i.e., focus on covering the entire search space. The empirical analysis involves eight tasks of reconstructing known models of dynamical systems from synthetic and measured data. The results of the analysis show that search algorithms involving moderate diversification methods have superior performance on the structure selection task. The empirical analysis also reveals that this finding is related to specific properties of the search space of candidate model structures. In this paper, we aim at identifying the optimal search strategy to be employed by knowledge-driven equation discovery methods for selecting a proper model structure.The search strategies considered in the paper are at different points of the trade-off between diversification, that is exploration of the search space, and intensification, i.e., exploitation of the promising regions of the search space determined by the best solutions found at a given search iteration (Talbi, 2009).More specifically, we consider five strategies ranging from random search, a strategy of extreme diversification, through genetic algorithms that have high diversification and moderate intensification, tabu search and particle swarm optimization (high intensification, moderate diversification) to greedy search (extreme intensification).
Automated modeling aims at the induction of mathematical models, both their structure and parameter values, from time-series measurements of observed system variables. In this paper, we address the task of model structure selection, i.e., selecting an optimal structure from a user-specified finite set of alternative model structures, using various approaches to combinatorial search. We propose a mapping of the set of candidate model structures to a fixed-length, vector representation allowing the use of an arbitrary search algorithm as a solver of the structure selection task. We perform a comparative analysis of the performance of thirteen variants of several search algorithms, ranging from ones with high intensification, i.e., focus on neighborhood of the best candidate solutions, to ones with high diversification, i.e., focus on covering the entire search space. The empirical analysis involves eight tasks of reconstructing known models of dynamical systems from synthetic and measured data. The results of the analysis show that search algorithms involving moderate diversification methods have superior performance on the structure selection task. The empirical analysis also reveals that this finding is related to specific properties of the search space of candidate model structures. We conjecture that the optimal, best-performing search strategy is problem specific, i.e., dependent on the structure and properties of the solution search space.To test this conjecture, we perform an empirical analysis along two dimensions.On one hand, we analyze the structure of the search space by computing a number of its properties.On the other hand, we empirically evaluate the performance of different search strategies on the given modeling tasks and analyze the relation between the performance of the strategies and the properties of the search space corresponding to the modeling task.The empirical analysis includes eight synthetic benchmark tasks of reconstructing known models of dynamical systems from simulated data in the domains of population dynamics and systems biology.
The ability to determine body mass from skulls is valuable for understanding various ecological, physiological, and evolutionary factors. In the Canidae, numerous methods to reconstruct body mass from measurements of the skull have been proposed, however there is no one-size fits all approach that can be applied across all species and subspecies. Added to this, current methods of reconstructing body mass are often complex, and have relatively high error rates. We aimed to produce a multivariate regression equation to estimate body mass of the Australian dingo (Canis dingo) from simple measurements of the skull, whilst ensuring that it could also be used in studies of encephalisation. To do this, we focussed on palate length (PL), palate width (PW) and the length of the first upper molar (M1). A total of 128 adult dingo (64 male; 64 female) crania from one region of Australia with known body mass were measured. Overall, the combination of PL and PW was the best predictor of body mass, with M1 having poor predictability. The model, mass (kg) = 0.246 ∗ (PL) + 0.320 ∗ (PW) − 24.757 produced a prediction error of 8.05%. Thus, these two measures of the palate provide simple and accurate predictors of body mass for the dingo. This will be useful for modern dingo specimens, as well as those found at archaeological sites and in museum collections that often consist of incomplete cranial material. The reconstruction of dingo body size is useful for evaluating variation in body mass through time, and across the Australian continent, particularly in the context of human activity. Body mass and size have long been linked to various ecological, physiological and evolutionary factors (Gittleman, 1985), with the aim to understand an animal or species' life history, general appearance, metabolic needs, and bite force (Damuth and MacFadden, 1990).In the Canidae specifically, knowledge of body mass can reveal information about geographic variation and adaptation, as well as the life histories and roles of canids in past human societies, particularly during domestication (Losey et al., 2015; Losey et al., 2017).Changes in body size across time and space may offer an insight into such aspects as geographic variation, changes to climate, and resource availability, such as the size of prey.
The ability to determine body mass from skulls is valuable for understanding various ecological, physiological, and evolutionary factors. In the Canidae, numerous methods to reconstruct body mass from measurements of the skull have been proposed, however there is no one-size fits all approach that can be applied across all species and subspecies. Added to this, current methods of reconstructing body mass are often complex, and have relatively high error rates. We aimed to produce a multivariate regression equation to estimate body mass of the Australian dingo (Canis dingo) from simple measurements of the skull, whilst ensuring that it could also be used in studies of encephalisation. To do this, we focussed on palate length (PL), palate width (PW) and the length of the first upper molar (M1). A total of 128 adult dingo (64 male; 64 female) crania from one region of Australia with known body mass were measured. Overall, the combination of PL and PW was the best predictor of body mass, with M1 having poor predictability. The model, mass (kg) = 0.246 ∗ (PL) + 0.320 ∗ (PW) − 24.757 produced a prediction error of 8.05%. Thus, these two measures of the palate provide simple and accurate predictors of body mass for the dingo. This will be useful for modern dingo specimens, as well as those found at archaeological sites and in museum collections that often consist of incomplete cranial material. The reconstruction of dingo body size is useful for evaluating variation in body mass through time, and across the Australian continent, particularly in the context of human activity. Various methods have explored the relationship between skeletal dimensions and body mass in vertebrates (Churchill et al., 2014; Legendre and Roth, 1988; Pagels and Blem, 1984; Wroe et al., 2003).In the Canidae, there is a large body of literature on body mass predictions involving cranial and postcranial characters (Andersson, 2004; Anyonge, 1993; Creighton, 1980; Losey et al., 2015; Losey et al., 2017; Legendre and Roth, 1988; Palmqvist et al., 2002; van Valkenburgh, 1990), but the majority are largely based on the cranial and mandibular.Postcranial measurements are more accurate than those of the cranium for reconstruction of body mass because of their weight bearing abilities such as in the femur or humerus.That is, the forces acting on the limbs are directly proportional to the weight the limbs support (Alexander, 1989, 1991; Janis, 1990; Losey et al., 2017).As a result, postcranial remains are strongly correlated with body mass.However, it is often the case that postcranial remains cannot be found in archaeological excavations or they cannot be matched to the skull in cases where there is more than one burial (Janis, 1990).
The ability to determine body mass from skulls is valuable for understanding various ecological, physiological, and evolutionary factors. In the Canidae, numerous methods to reconstruct body mass from measurements of the skull have been proposed, however there is no one-size fits all approach that can be applied across all species and subspecies. Added to this, current methods of reconstructing body mass are often complex, and have relatively high error rates. We aimed to produce a multivariate regression equation to estimate body mass of the Australian dingo (Canis dingo) from simple measurements of the skull, whilst ensuring that it could also be used in studies of encephalisation. To do this, we focussed on palate length (PL), palate width (PW) and the length of the first upper molar (M1). A total of 128 adult dingo (64 male; 64 female) crania from one region of Australia with known body mass were measured. Overall, the combination of PL and PW was the best predictor of body mass, with M1 having poor predictability. The model, mass (kg) = 0.246 ∗ (PL) + 0.320 ∗ (PW) − 24.757 produced a prediction error of 8.05%. Thus, these two measures of the palate provide simple and accurate predictors of body mass for the dingo. This will be useful for modern dingo specimens, as well as those found at archaeological sites and in museum collections that often consist of incomplete cranial material. The reconstruction of dingo body size is useful for evaluating variation in body mass through time, and across the Australian continent, particularly in the context of human activity. In general, older museum collections comprise mainly skulls of larger mammals, while postcranial skeletons were discarded as being considered of less scientific importance.For these reasons, numerous studies propose methods to reconstruct body weight from measurements of the skull (Churchill et al., 2014; Legendre and Roth, 1988; Pagels and Blem, 1984; Wroe et al., 2003).Losey et al. (2015) reconstructed body mass from the skeletal remains of Canidae, specifically, dogs and wolves.They presented simple linear regressions of body mass for each of the 20 cranial (neurocranium and splanchnocranium) and 20 mandibular (including dentition) dimensions taken from the work by von den Driesch (1976) and Morey (1992).Included in these 40 dimensions were the length of lower molar (M1), and the length of the palate.This produced a useful method, however, did not include multivariate regressions.The inclusion of multivariate regressions could potentially lower errors, and thus increase accuracy of weight estimation because as each correlated variable is added to a multivariate regression, it decreases or stabilises the error of prediction.The higher the correlations, the lower the error.In another study, van Valkenburgh (1990) proposed a method of mass estimation based on only four dimensions.These included the total skull length, occiput to the anterior premaxilla, head to body length and the length of the lower molar (M1).van Valkenburgh's study was based on 72 species divided into Families, one of which was Canidae.However, the Australian dingo was excluded from this sample.One minor issue with the regressions used by both van Valkenburgh (1990) and Losey et al. (2015) is that they were determined on logged values, which may create unnecessary mathematical steps, compared to methods using unlogged values.
The ability to determine body mass from skulls is valuable for understanding various ecological, physiological, and evolutionary factors. In the Canidae, numerous methods to reconstruct body mass from measurements of the skull have been proposed, however there is no one-size fits all approach that can be applied across all species and subspecies. Added to this, current methods of reconstructing body mass are often complex, and have relatively high error rates. We aimed to produce a multivariate regression equation to estimate body mass of the Australian dingo (Canis dingo) from simple measurements of the skull, whilst ensuring that it could also be used in studies of encephalisation. To do this, we focussed on palate length (PL), palate width (PW) and the length of the first upper molar (M1). A total of 128 adult dingo (64 male; 64 female) crania from one region of Australia with known body mass were measured. Overall, the combination of PL and PW was the best predictor of body mass, with M1 having poor predictability. The model, mass (kg) = 0.246 ∗ (PL) + 0.320 ∗ (PW) − 24.757 produced a prediction error of 8.05%. Thus, these two measures of the palate provide simple and accurate predictors of body mass for the dingo. This will be useful for modern dingo specimens, as well as those found at archaeological sites and in museum collections that often consist of incomplete cranial material. The reconstruction of dingo body size is useful for evaluating variation in body mass through time, and across the Australian continent, particularly in the context of human activity. Typically, the size of the neurocranium is a direct representation of brain size, and thus its measure can be used for determining brain size in collections where the brain cannot be measured (Manjunath, 2002).However, measurements of the neurocranium to reconstruct body mass cannot be used in studies of encephalisation (the relationship between brain and body size) when the same measurements of the skull are used as a representative of brain size.This is because the use of neurocranium measurements to reconstruct body mass as well as to estimate brain size produces circular reasoning, heavily biasing results.As the postcranial skeleton and mandible are rarely found in archaeological excavations and/or museum collections, methods which estimate body mass should concentrate on the crania and maxillary dentition.However, due to the vast number of studies which investigate encephalisation and/or brain size as well as body size (Smith et al., 2018; Damasceno et al., 2013; Finarelli, 2008; Gittleman, 1986; Röhrs, 1986; Bronson, 1979), methods which use the neurocranium are deemed inappropriate.Thus, methods which use the splanchnocranium and maxillary dentition alone should be investigated.
The ability to determine body mass from skulls is valuable for understanding various ecological, physiological, and evolutionary factors. In the Canidae, numerous methods to reconstruct body mass from measurements of the skull have been proposed, however there is no one-size fits all approach that can be applied across all species and subspecies. Added to this, current methods of reconstructing body mass are often complex, and have relatively high error rates. We aimed to produce a multivariate regression equation to estimate body mass of the Australian dingo (Canis dingo) from simple measurements of the skull, whilst ensuring that it could also be used in studies of encephalisation. To do this, we focussed on palate length (PL), palate width (PW) and the length of the first upper molar (M1). A total of 128 adult dingo (64 male; 64 female) crania from one region of Australia with known body mass were measured. Overall, the combination of PL and PW was the best predictor of body mass, with M1 having poor predictability. The model, mass (kg) = 0.246 ∗ (PL) + 0.320 ∗ (PW) − 24.757 produced a prediction error of 8.05%. Thus, these two measures of the palate provide simple and accurate predictors of body mass for the dingo. This will be useful for modern dingo specimens, as well as those found at archaeological sites and in museum collections that often consist of incomplete cranial material. The reconstruction of dingo body size is useful for evaluating variation in body mass through time, and across the Australian continent, particularly in the context of human activity. The aim of the current paper was to estimate the body mass of the Australian dingo (Canis dingo Meyer 1793; Order Carnivora: Family Canidae) from splanchnocranial dimensions and maxillary dentition using both simple and multivariate regression equations.The usefulness of the upper molar and palate width as a predictor of body mass has also not been investigated in previous methods for Canidae (Losey et al., 2015; van Valkenburgh, 1990).The dingo is Australia's largest extant, terrestrial carnivore, and is reasonably abundant over much of mainland Australia.It was the only native representative of its genus, and the largest non-human terrestrial eutherian found in Australia from its arrival circa 5–10,000 years before present until European arrival in the late 18th Century (Macintosh, 1964; Savolainen et al., 2004; Oskarsson et al., 2011; Sacks et al., 2013; Smith and Savolainen, 2015; Cairns and Wilton, 2016; Fillios and Taçon, 2016; Cairns et al., 2017).The dingo has been geographically isolated from all other species of Canis for millenia, and admixture has only been possible because of recent human activity.This makes the dingo unique among canids globally, where there has been a long history of introgression between wild and domestic canids (Fan et al., 2016).
The ability to determine body mass from skulls is valuable for understanding various ecological, physiological, and evolutionary factors. In the Canidae, numerous methods to reconstruct body mass from measurements of the skull have been proposed, however there is no one-size fits all approach that can be applied across all species and subspecies. Added to this, current methods of reconstructing body mass are often complex, and have relatively high error rates. We aimed to produce a multivariate regression equation to estimate body mass of the Australian dingo (Canis dingo) from simple measurements of the skull, whilst ensuring that it could also be used in studies of encephalisation. To do this, we focussed on palate length (PL), palate width (PW) and the length of the first upper molar (M1). A total of 128 adult dingo (64 male; 64 female) crania from one region of Australia with known body mass were measured. Overall, the combination of PL and PW was the best predictor of body mass, with M1 having poor predictability. The model, mass (kg) = 0.246 ∗ (PL) + 0.320 ∗ (PW) − 24.757 produced a prediction error of 8.05%. Thus, these two measures of the palate provide simple and accurate predictors of body mass for the dingo. This will be useful for modern dingo specimens, as well as those found at archaeological sites and in museum collections that often consist of incomplete cranial material. The reconstruction of dingo body size is useful for evaluating variation in body mass through time, and across the Australian continent, particularly in the context of human activity. Dingoes have survived as a free ranging canid in Australia for millennia, subject to the rigours of natural selection, thriving in all terrestrial habitats, and largely in the absence of human intervention or aid.It does not display the same ranges and abilities of tolerance in all relevant ecological variables, and thus occupies a niche that cannot be readily occupied by other closely related populations.The dingo can be distinguished from the domestic dog in relation to genetics (Wilton, 2001; Stephens et al., 2015), phenotype (Gollan, 1982; Crowther et al., 2014; Parr et al., 2016), behaviour (see Smith, 2015 and references therein), and reproductive biology and behaviour (Lord et al., 2013).Thus, given the dingoes' uniqueness, and important ecological and historical role in Australia, it is important that a method to reconstruct dingo body weight be developed.No other study has provided a method to specifically reconstruct body weight of the Australian dingo.Damasceno et al. (2013) required the estimation of dingo body mass during investigations of bite force, however utilised a generic method which may introduce a high degree of error.
Identifying the range of plants and/or animals processed by pounding and/or grinding stones has been a rapidly developing research area in world prehistory. In Australia, grinding and pounding stones are ubiquitous across the semi-arid and arid zones and the associated tasks have been mostly informed by ethnographic case studies. More recently, plant microfossil studies have provided important insights to the breadth of plants being exploited in a range of contexts and over long time periods. The preservation of starch and/or phytoliths on the used surfaces of these artefacts is well documented, though the factors determining the survival or destruction of use-related starch residues are still largely unknown. Some of these artefacts have also been used for grinding up small animals and these tasks can be identified by specific staining methods for organic remains such as collagen. In this study, 25 grinding and pounding stones identified during an archaeological project in arid South Australia, were examined for starch and collagen residues. The artefacts were from 3 locations in central South Australia, all located in exposed settings. Of these localities, Site 11 in the Western Valley near Woomera is an important Aboriginal landscape specifically associated with male ceremonial practice in the recent past. The remaining two sites, one in the adjacent Nurrungar Valley and the other near Andamooka 100 km distant, have unrestricted access and potentially a different suite of residues. The Kokatha Mula Nations, the Traditional Owners of Woomera, requested that this study be undertaken to explore the range of plants that may have been processed here. It provided an opportunity to investigate the preservation potential of starch and collagen on grinding stones; explore the range of taphonomic factors involved in the persistence of residues in extreme environmental conditions; and test the methodological developments in identifying specific plant origin of starch residues. Of the 25 grinding/pounding stones tested, 7 yielded starch grains. Geometric morphometric analysis identified 3 economic grass species, Crinum flaccidum (Andamooka Lily) and Typha domingensis (Bulrush/Cumbungi). Folded collagen was identified on one artefact. Oral histories recount the movement between Andamooka and Nurrungar/Western Valley for men's ceremonies, and documented in the movement of stone resources, e.g. oolytic chert. The survival of residues in this environment and the identification of economic plant taxa complement the current knowledge of ceremonial activities and the movement of people and resources across significant distances in arid South Australia. Grinding technology emerged in the Late Pleistocene across the globe, and the specific tasks associated with these implements varied considerably through time and space (e.g. Piperno et al., 2000; Fullagar, 2006; Fullagar et al., 2008, 2015; Liu et al., 2014; Field et al., 2016; Louderback and Pavlik, 2017; Barton et al., 2018).The distribution varies with environment, and in Australia, while grinding stones can be found in most environmental zones, they are ubiquitous across the arid and semi-arid regions.Numerous shapes and forms have been described (e.g. Smith, 1999; Smith et al., 2015), and the uses and tasks performed (involving both plants and animals) have been described in the ethnographic literature (e.g. Hayden, 1979; Gould, 1980), or identified in functional studies (e.g. Fullagar et al., 2008).
Identifying the range of plants and/or animals processed by pounding and/or grinding stones has been a rapidly developing research area in world prehistory. In Australia, grinding and pounding stones are ubiquitous across the semi-arid and arid zones and the associated tasks have been mostly informed by ethnographic case studies. More recently, plant microfossil studies have provided important insights to the breadth of plants being exploited in a range of contexts and over long time periods. The preservation of starch and/or phytoliths on the used surfaces of these artefacts is well documented, though the factors determining the survival or destruction of use-related starch residues are still largely unknown. Some of these artefacts have also been used for grinding up small animals and these tasks can be identified by specific staining methods for organic remains such as collagen. In this study, 25 grinding and pounding stones identified during an archaeological project in arid South Australia, were examined for starch and collagen residues. The artefacts were from 3 locations in central South Australia, all located in exposed settings. Of these localities, Site 11 in the Western Valley near Woomera is an important Aboriginal landscape specifically associated with male ceremonial practice in the recent past. The remaining two sites, one in the adjacent Nurrungar Valley and the other near Andamooka 100 km distant, have unrestricted access and potentially a different suite of residues. The Kokatha Mula Nations, the Traditional Owners of Woomera, requested that this study be undertaken to explore the range of plants that may have been processed here. It provided an opportunity to investigate the preservation potential of starch and collagen on grinding stones; explore the range of taphonomic factors involved in the persistence of residues in extreme environmental conditions; and test the methodological developments in identifying specific plant origin of starch residues. Of the 25 grinding/pounding stones tested, 7 yielded starch grains. Geometric morphometric analysis identified 3 economic grass species, Crinum flaccidum (Andamooka Lily) and Typha domingensis (Bulrush/Cumbungi). Folded collagen was identified on one artefact. Oral histories recount the movement between Andamooka and Nurrungar/Western Valley for men's ceremonies, and documented in the movement of stone resources, e.g. oolytic chert. The survival of residues in this environment and the identification of economic plant taxa complement the current knowledge of ceremonial activities and the movement of people and resources across significant distances in arid South Australia. In arid regions, starchy foods prepared from seeds, yams and tubers have always been primary sources of food for Aboriginal people (Clarke, 2013; Pascoe, 2014; Pate and Owen, 2015).Many were prepared by grinding to a paste, followed by roasting or cooking before eating (e.g. Gould, 1980; Clarke, 1978:80).Ethnographic studies have shown that grinding/pounding stones were also used for processing insects, lizards, cats and some small marsupials (Gould, 1980).Non-food materials such as ochre and medicinal plants, e.g. Acacia inaequilatera (Kanji Bush) and Dubosia hopwoodii (Pituri) have also been recorded as processed in this way (Brokensha, 1978).Preparation with or without water effectively rendered food edible and in a form that could be consumed by elderly or the very young.Toxic starchy plants, like Nardoo (Marsilea spp.) and Cycads were prepared by complex processing: baking, pounding/grinding and leaching with water to remove toxins (e.g. Cribb and Cribb, 1974:72 Pedley, 1993; Asmussen, 2011).
Identifying the range of plants and/or animals processed by pounding and/or grinding stones has been a rapidly developing research area in world prehistory. In Australia, grinding and pounding stones are ubiquitous across the semi-arid and arid zones and the associated tasks have been mostly informed by ethnographic case studies. More recently, plant microfossil studies have provided important insights to the breadth of plants being exploited in a range of contexts and over long time periods. The preservation of starch and/or phytoliths on the used surfaces of these artefacts is well documented, though the factors determining the survival or destruction of use-related starch residues are still largely unknown. Some of these artefacts have also been used for grinding up small animals and these tasks can be identified by specific staining methods for organic remains such as collagen. In this study, 25 grinding and pounding stones identified during an archaeological project in arid South Australia, were examined for starch and collagen residues. The artefacts were from 3 locations in central South Australia, all located in exposed settings. Of these localities, Site 11 in the Western Valley near Woomera is an important Aboriginal landscape specifically associated with male ceremonial practice in the recent past. The remaining two sites, one in the adjacent Nurrungar Valley and the other near Andamooka 100 km distant, have unrestricted access and potentially a different suite of residues. The Kokatha Mula Nations, the Traditional Owners of Woomera, requested that this study be undertaken to explore the range of plants that may have been processed here. It provided an opportunity to investigate the preservation potential of starch and collagen on grinding stones; explore the range of taphonomic factors involved in the persistence of residues in extreme environmental conditions; and test the methodological developments in identifying specific plant origin of starch residues. Of the 25 grinding/pounding stones tested, 7 yielded starch grains. Geometric morphometric analysis identified 3 economic grass species, Crinum flaccidum (Andamooka Lily) and Typha domingensis (Bulrush/Cumbungi). Folded collagen was identified on one artefact. Oral histories recount the movement between Andamooka and Nurrungar/Western Valley for men's ceremonies, and documented in the movement of stone resources, e.g. oolytic chert. The survival of residues in this environment and the identification of economic plant taxa complement the current knowledge of ceremonial activities and the movement of people and resources across significant distances in arid South Australia. Numerous studies in Australia and New Guinea have established that starch survives for many millennia as use-related residues on the surface of stone artefacts (Denham et al., 2003; Fullagar, 2006; Fullagar et al., 2008; Field et al., 2009, 2016).While the specific factors controlling the survival of starch over these time periods have yet to be well defined, it is clear that these plant microfossils can be a resilient record of plant use from the distant past.Preservation of starch is not restricted to particular environments, with starch grains recovered from grinding stones in the rainforests of Far North Queensland, the semi-arid and arid regions of Australia and from flaked and ground stone artefacts in the New Guinea highlands (Field et al., 2016; Fullagar, 2006; Fullagar et al., 2015; Summerhayes et al., 2010).The size and morphology of starch grains can be diagnostic to a particular plant species and are governed by their genetics and physical environments (Field, 2006a).Furthermore, it has also been shown that the local environment can have a significant effect on grain characteristics (Lance et al., 2005), though this appears to usually be only in size.As grinding stones can also be multifunctional in some instances, examining them for evidence of processing small animals is also important.These activities can be inferred by the presence of collagen, a residue that can also be preserved for long periods (Stephenson, 2011, 2015).
A study about pre–Hispanic ceramics (pastes and pigments) coming from archaeological sites of the Valley of Metzontla, Mexico (Iglesia Vieja San Sebastian, Coronilla Hill, Agua Socoya Hill and Metzontla Hill), from sites of the vicinity (Cutha, Teteles de Santo Nombre, and Tehuacan Viejo), and from one more distant site (Champayan) is presented. Raw materials such as clays, tempers and pigments were studied as well. Nuclear activation analyses, energy dispersion spectroscopy, and X–ray diffraction were applied. According to the chemical composition of the pastes six groups of ceramics were identified: one of them includes pre–Hispanic Popoloca Orange ceramic, one other group is similar to pre–Hispanic Brown ceramic and present–day Los Reyes Metzontla's samples, and several other exemplars were quite different to these groups. The brown ethnographic pottery of Los Reyes Metzontla town is chemically identical to those pre–Hispanic pottery; apparently raw materials have been the same for a long period of time, whereas Popoloca Orange ceramic is no longer manufactured in the region. Similarities and differences were found among the ceramics of the Metzontla Valley and those of the sites located within the Popoloca area and beyond. The term Popoloca became confused because of the connotation given by the Aztecs to many groups that were not Nahuas (Jäcklein, 1991).At present this denomination (ib.)applies to indigenous groups of Puebla (Mexico) that linguistically belong to the Otomangue family.The term “Historical Popoloca” used by Jäcklein refers to the group that existed during the Early Classic period (ca. 200 CE), and he considers (ib.)to the proto–Otomangues to be the predecessors, who in ca. 7000 BCE began the domestication of several plants.The Popoloca people lived in the southern and central regions of the State of Puebla, the northern zone of Oaxaca, and perhaps in the eastern zone of Guerrero and the south of Tlaxcala.However, so far, insufficient studies have been conducted regarding about this ethnic group.
A study about pre–Hispanic ceramics (pastes and pigments) coming from archaeological sites of the Valley of Metzontla, Mexico (Iglesia Vieja San Sebastian, Coronilla Hill, Agua Socoya Hill and Metzontla Hill), from sites of the vicinity (Cutha, Teteles de Santo Nombre, and Tehuacan Viejo), and from one more distant site (Champayan) is presented. Raw materials such as clays, tempers and pigments were studied as well. Nuclear activation analyses, energy dispersion spectroscopy, and X–ray diffraction were applied. According to the chemical composition of the pastes six groups of ceramics were identified: one of them includes pre–Hispanic Popoloca Orange ceramic, one other group is similar to pre–Hispanic Brown ceramic and present–day Los Reyes Metzontla's samples, and several other exemplars were quite different to these groups. The brown ethnographic pottery of Los Reyes Metzontla town is chemically identical to those pre–Hispanic pottery; apparently raw materials have been the same for a long period of time, whereas Popoloca Orange ceramic is no longer manufactured in the region. Similarities and differences were found among the ceramics of the Metzontla Valley and those of the sites located within the Popoloca area and beyond. The earliest ethnographic studies in the Metzontla Valley were done at the beginning of the 20th century.More recently (2003−2011), ethnographic studies on the pottery process at Los Reyes Metzontla town have been carried out by several researchers (de la Vega Doria et al., 2005; Moreno Hernandez and Galvez Rosales, 2006; de la Vega Doria, 2006, 2007, 2010; Hernandez Zarza, 2008).The information collected allowed the researchers to conclude that the inhabitants of Los Reyes Metzontla are of true Popoloca origin (Moreno Hernandez and Galvez Rosales, 2006).
A study about pre–Hispanic ceramics (pastes and pigments) coming from archaeological sites of the Valley of Metzontla, Mexico (Iglesia Vieja San Sebastian, Coronilla Hill, Agua Socoya Hill and Metzontla Hill), from sites of the vicinity (Cutha, Teteles de Santo Nombre, and Tehuacan Viejo), and from one more distant site (Champayan) is presented. Raw materials such as clays, tempers and pigments were studied as well. Nuclear activation analyses, energy dispersion spectroscopy, and X–ray diffraction were applied. According to the chemical composition of the pastes six groups of ceramics were identified: one of them includes pre–Hispanic Popoloca Orange ceramic, one other group is similar to pre–Hispanic Brown ceramic and present–day Los Reyes Metzontla's samples, and several other exemplars were quite different to these groups. The brown ethnographic pottery of Los Reyes Metzontla town is chemically identical to those pre–Hispanic pottery; apparently raw materials have been the same for a long period of time, whereas Popoloca Orange ceramic is no longer manufactured in the region. Similarities and differences were found among the ceramics of the Metzontla Valley and those of the sites located within the Popoloca area and beyond. The Archaeological Project “Popoloca Settlements at Los Reyes Metzontla Valley” began in 2008, under the direction of the archaeologist Socorro de la Vega Doria.In that first season two test pits were dug at the sites of Coronilla Hill (or Buena Vista Hill) and Metzontla Hill, respectively.During the First Season of Archaeological Prospecting (2009), the topography of some sectors of the Coronilla Hill site was obtained.The Second Season of Archaeological Prospecting and the First Excavation in the sites of Iglesia Vieja, San Sebastian and Metzontla Hill were carried out in 2011.In 2014, the source of red and orange pigment known as Agua la Mina Hill was visited for the first time and material was collected from there.An important discovery during the Third Season of Archaeological Prospecting (2015) was a cave with rock paintings.
A study about pre–Hispanic ceramics (pastes and pigments) coming from archaeological sites of the Valley of Metzontla, Mexico (Iglesia Vieja San Sebastian, Coronilla Hill, Agua Socoya Hill and Metzontla Hill), from sites of the vicinity (Cutha, Teteles de Santo Nombre, and Tehuacan Viejo), and from one more distant site (Champayan) is presented. Raw materials such as clays, tempers and pigments were studied as well. Nuclear activation analyses, energy dispersion spectroscopy, and X–ray diffraction were applied. According to the chemical composition of the pastes six groups of ceramics were identified: one of them includes pre–Hispanic Popoloca Orange ceramic, one other group is similar to pre–Hispanic Brown ceramic and present–day Los Reyes Metzontla's samples, and several other exemplars were quite different to these groups. The brown ethnographic pottery of Los Reyes Metzontla town is chemically identical to those pre–Hispanic pottery; apparently raw materials have been the same for a long period of time, whereas Popoloca Orange ceramic is no longer manufactured in the region. Similarities and differences were found among the ceramics of the Metzontla Valley and those of the sites located within the Popoloca area and beyond. Los Reyes Metzontla Valley is part of the physiographic province of the Sierra Madre del Sur, sub–province of the Sierras Centrales of Oaxaca, within the Mixteca Alta Region and the floristic province of the Tehuacan–Cuicatlan Valley (Sanchez Perez, 2006).It is formed by igneous, sedimentary, and metamorphic rocks (Mendoza Rosales and Silva Romo, 2006), the natural environment is wild, typical of a dry climate, the average temperature is 18.6 °C and the average annual rainfall is 479 to 412.4 mm (de la Vega Doria, 2007).The geological formation and the geographical location of the area have some unique species of flora growth, particularly with respect to an abundant variety of endemic cacti.
A study about pre–Hispanic ceramics (pastes and pigments) coming from archaeological sites of the Valley of Metzontla, Mexico (Iglesia Vieja San Sebastian, Coronilla Hill, Agua Socoya Hill and Metzontla Hill), from sites of the vicinity (Cutha, Teteles de Santo Nombre, and Tehuacan Viejo), and from one more distant site (Champayan) is presented. Raw materials such as clays, tempers and pigments were studied as well. Nuclear activation analyses, energy dispersion spectroscopy, and X–ray diffraction were applied. According to the chemical composition of the pastes six groups of ceramics were identified: one of them includes pre–Hispanic Popoloca Orange ceramic, one other group is similar to pre–Hispanic Brown ceramic and present–day Los Reyes Metzontla's samples, and several other exemplars were quite different to these groups. The brown ethnographic pottery of Los Reyes Metzontla town is chemically identical to those pre–Hispanic pottery; apparently raw materials have been the same for a long period of time, whereas Popoloca Orange ceramic is no longer manufactured in the region. Similarities and differences were found among the ceramics of the Metzontla Valley and those of the sites located within the Popoloca area and beyond. Los Reyes Metzontla town (Municipality of Zapotitlan Salinas, Puebla, Mexico) (Fig. 1) is located in a small valley, surrounded by several hills, emphasizing Coronilla Hill and Metzontla Hill.The geographic coordinates are 1991327N and 288239E, with an altitude of 1800 to 2600 mamsl (Sanchez Perez, 2006; de la Vega Doria, 2007).
A study about pre–Hispanic ceramics (pastes and pigments) coming from archaeological sites of the Valley of Metzontla, Mexico (Iglesia Vieja San Sebastian, Coronilla Hill, Agua Socoya Hill and Metzontla Hill), from sites of the vicinity (Cutha, Teteles de Santo Nombre, and Tehuacan Viejo), and from one more distant site (Champayan) is presented. Raw materials such as clays, tempers and pigments were studied as well. Nuclear activation analyses, energy dispersion spectroscopy, and X–ray diffraction were applied. According to the chemical composition of the pastes six groups of ceramics were identified: one of them includes pre–Hispanic Popoloca Orange ceramic, one other group is similar to pre–Hispanic Brown ceramic and present–day Los Reyes Metzontla's samples, and several other exemplars were quite different to these groups. The brown ethnographic pottery of Los Reyes Metzontla town is chemically identical to those pre–Hispanic pottery; apparently raw materials have been the same for a long period of time, whereas Popoloca Orange ceramic is no longer manufactured in the region. Similarities and differences were found among the ceramics of the Metzontla Valley and those of the sites located within the Popoloca area and beyond. The archaeological sites of Iglesia Vieja San Sebastian, Agua Socoya Hill, Metzontla Hill, and Coronilla Hill (see Fig. 1) were described in the Technical Report on Exploration and Excavation, Second Season, of Archaeological Project Popoloca Settlements at Los Reyes Metzontla Valley (de la Vega Doria, 2014).
A study about pre–Hispanic ceramics (pastes and pigments) coming from archaeological sites of the Valley of Metzontla, Mexico (Iglesia Vieja San Sebastian, Coronilla Hill, Agua Socoya Hill and Metzontla Hill), from sites of the vicinity (Cutha, Teteles de Santo Nombre, and Tehuacan Viejo), and from one more distant site (Champayan) is presented. Raw materials such as clays, tempers and pigments were studied as well. Nuclear activation analyses, energy dispersion spectroscopy, and X–ray diffraction were applied. According to the chemical composition of the pastes six groups of ceramics were identified: one of them includes pre–Hispanic Popoloca Orange ceramic, one other group is similar to pre–Hispanic Brown ceramic and present–day Los Reyes Metzontla's samples, and several other exemplars were quite different to these groups. The brown ethnographic pottery of Los Reyes Metzontla town is chemically identical to those pre–Hispanic pottery; apparently raw materials have been the same for a long period of time, whereas Popoloca Orange ceramic is no longer manufactured in the region. Similarities and differences were found among the ceramics of the Metzontla Valley and those of the sites located within the Popoloca area and beyond. The sites of the present study are briefly described in Table 1 and information about raw materials and their sources is included in Table 2.
Infants in a skeletal population are important proxies of an ancient society's adaptation and well-being. This study uses microscopic dental enamel defects (Accentuated Retzius Lines, ALs) to provide a close-to-longitudinal and detailed estimate of the non-fatal stress prevalence in the first years of life in the community of Portus Romae (necropolis of Isola Sacra, 2nd to 4th century CE, Italy). Eighty-four teeth, 17 deciduous and 67 permanent, from 18 individuals were selected for histological thin sectioning. We scored and assessed the individual chronology of ALs across dental sets, by matching homologous intervals between ALs on several teeth in the same individual. After a steep increase following the 3rd month, AL prevalence distribution shows a maximum between the 9th and 11th month of life. Following the prevalence maximum, the distribution declines steadily until the 25th month, after which it remains almost stable. The ALs frequency by tooth type shows that the bulk of affected enamel is in the center of the tooth crown, with the apical and cervical portions less susceptible to recording stress. Our results illustrate how to derive a longitudinal profile of health status in the childhood segment of an ancient population through histomorphometry. Comparison of the ALs profile with the previously published δ15N and δ13C of Portus Romae and the application of a Bayesian statistical model allowed us to relate the prevalence maximum to the beginning of the weaning process. A multifactorial approach to the palaeobiology of a skeletal series is therefore rewarding and allows correlation with the biological life history of children in this ancient Roman Imperial community. The mortality profile and the health status of the children of any human population represent a measure of the biocultural adaptability of the community (Goodman and Armelagos, 1989; Lewis, 2018).As highlighted by several epidemiological studies on modern populations (for a review see Humphrey and King, 2000), the conditions during the first years of life - from intrauterine life until childhood - are often reflected in ill-health and earlier mortality in adult age (Armelagos et al., 2009; Barker, 2004).However, the reconstruction of life conditions and health in a human skeletal population is an extremely difficult and complex task.The Osteological Paradox (DeWitte and Stojanowski, 2015; Wood et al., 1992) proposes that skeletal series are biased proxies of the corresponding populations' life conditions.These authors contend that heterogeneous frailty and selective mortality are disruptive factors, often impossible to quantify in a skeletal series.However, the individual biological history of the first years of life can be read in the mineralized tissues that form during this period and, particularly, in teeth.Indeed, dental enamel during its formation is a biological archive, recording at a microscopic level important rhythmical growth markers as well as physiological stress episodes (Antoine et al., 2009; Dean, 2006; Hillson, 2014; Smith and Tafforeau, 2008).
Infants in a skeletal population are important proxies of an ancient society's adaptation and well-being. This study uses microscopic dental enamel defects (Accentuated Retzius Lines, ALs) to provide a close-to-longitudinal and detailed estimate of the non-fatal stress prevalence in the first years of life in the community of Portus Romae (necropolis of Isola Sacra, 2nd to 4th century CE, Italy). Eighty-four teeth, 17 deciduous and 67 permanent, from 18 individuals were selected for histological thin sectioning. We scored and assessed the individual chronology of ALs across dental sets, by matching homologous intervals between ALs on several teeth in the same individual. After a steep increase following the 3rd month, AL prevalence distribution shows a maximum between the 9th and 11th month of life. Following the prevalence maximum, the distribution declines steadily until the 25th month, after which it remains almost stable. The ALs frequency by tooth type shows that the bulk of affected enamel is in the center of the tooth crown, with the apical and cervical portions less susceptible to recording stress. Our results illustrate how to derive a longitudinal profile of health status in the childhood segment of an ancient population through histomorphometry. Comparison of the ALs profile with the previously published δ15N and δ13C of Portus Romae and the application of a Bayesian statistical model allowed us to relate the prevalence maximum to the beginning of the weaning process. A multifactorial approach to the palaeobiology of a skeletal series is therefore rewarding and allows correlation with the biological life history of children in this ancient Roman Imperial community. The prevalence in a population of both microscopic (Accentuated Retzius Lines, ALs) and macroscopic (linear enamel hypoplasia) enamel defects is often interpreted as a measure of general health during childhood (Larsen, 2015; Simpson, 1999; Żądzińska et al., 2015).Histomorphometric studies of tooth enamel depict the daily biological history of an individual from the intra-uterine protected period to the end of tooth formation.They approximate a kind of longitudinal record, partly overcoming the limitations of using mortality data.Given that tooth crowns of a single dentition overlap in time during their formation, ALs can be matched among enamel portions that form simultaneously in the separate teeth and can be used to register the combined tooth chronologies of an individual's life.Using different individuals in a sample can lead to more specific details of the early health status of a skeletal series.
Infants in a skeletal population are important proxies of an ancient society's adaptation and well-being. This study uses microscopic dental enamel defects (Accentuated Retzius Lines, ALs) to provide a close-to-longitudinal and detailed estimate of the non-fatal stress prevalence in the first years of life in the community of Portus Romae (necropolis of Isola Sacra, 2nd to 4th century CE, Italy). Eighty-four teeth, 17 deciduous and 67 permanent, from 18 individuals were selected for histological thin sectioning. We scored and assessed the individual chronology of ALs across dental sets, by matching homologous intervals between ALs on several teeth in the same individual. After a steep increase following the 3rd month, AL prevalence distribution shows a maximum between the 9th and 11th month of life. Following the prevalence maximum, the distribution declines steadily until the 25th month, after which it remains almost stable. The ALs frequency by tooth type shows that the bulk of affected enamel is in the center of the tooth crown, with the apical and cervical portions less susceptible to recording stress. Our results illustrate how to derive a longitudinal profile of health status in the childhood segment of an ancient population through histomorphometry. Comparison of the ALs profile with the previously published δ15N and δ13C of Portus Romae and the application of a Bayesian statistical model allowed us to relate the prevalence maximum to the beginning of the weaning process. A multifactorial approach to the palaeobiology of a skeletal series is therefore rewarding and allows correlation with the biological life history of children in this ancient Roman Imperial community. FitzGerald et al. (2006) documented the prevalence of ALs in a large sample of 274 deciduous teeth from the infants of Isola Sacra.For the purposes of this research, the importance of their study rests not only in the large sample size, but in the fact that it takes into consideration the same population of the present study, thus providing elements for a comparative analysis between the deciduous dentition and the permanent one used in the present analysis.
This study compared temporal lobe epilepsy (TLE) patients with amygdala lesion (AL) without hippocampal sclerosis (HS) (TLE-AL) with patients with TLE and HS without AL (TLE-HS). Both subtypes of TLE arose from the right hemisphere. The TLE-AL group exhibited a lower Working Memory Index (WMI) on the Wechsler Adult Intelligence Scale, third edition (WAIS-III), indicating that the amygdala in the right hemisphere is involved in memory-related function. [ 18F]fluorodeoxyglucose positron emission topography (FDG-PET) showed glucose hypometabolism limited to the right uncus for the TLE-AL group. The results suggest the importance of considering cognitive functions in the non-dominant hemisphere to prevent impairment after surgery. The most common cause of drug-resistant mesial temporal lobe epilepsy (TLE) is hippocampal sclerosis (HS) [1–3].Epileptogenesus originating from the pathological hippocampus, however, often involve the amygdala that is anatomically adjacent to the hippocampus [4].For example, Graebenitz and his colleague reported that epileptiform network of TLE with HS involved the amygdala [5].In some cases, amygdala sclerosis coexists with HS in patients with TLE [6].Wieser, furthermore, reported a patient with amygdala epilepsy without HS [7].Recent reports have indicated that enlargement of the amygdala without HS can be epileptogenic [8–10].
This study compared temporal lobe epilepsy (TLE) patients with amygdala lesion (AL) without hippocampal sclerosis (HS) (TLE-AL) with patients with TLE and HS without AL (TLE-HS). Both subtypes of TLE arose from the right hemisphere. The TLE-AL group exhibited a lower Working Memory Index (WMI) on the Wechsler Adult Intelligence Scale, third edition (WAIS-III), indicating that the amygdala in the right hemisphere is involved in memory-related function. [ 18F]fluorodeoxyglucose positron emission topography (FDG-PET) showed glucose hypometabolism limited to the right uncus for the TLE-AL group. The results suggest the importance of considering cognitive functions in the non-dominant hemisphere to prevent impairment after surgery. As for the characteristics of seizures, both similarities and differences between the TLE with amygdala lesion (AL) without HS (TLE-AL) and TLE with HS without AL (TLE-HS) are more or less understood [8, 11].Reports regarding interictal conditions also exist.Tebartz van Elst and his colleagues reported an association between bilateral amygdala enlargement with affective disorders in some patients with TLE [12].Some researchers reported memory impairment associated with HS in TLE-HS [13, 14].To our knowledge, however, neither systematic investigation on the effects of unilateral amygdala lesion nor report that deals with effects of AL on cognitive functions in TLE-AL exists.Hence, functional influence of both TLE-AL and TLE-HS are still to be delineated, particularly in the interictal state.
This study compared temporal lobe epilepsy (TLE) patients with amygdala lesion (AL) without hippocampal sclerosis (HS) (TLE-AL) with patients with TLE and HS without AL (TLE-HS). Both subtypes of TLE arose from the right hemisphere. The TLE-AL group exhibited a lower Working Memory Index (WMI) on the Wechsler Adult Intelligence Scale, third edition (WAIS-III), indicating that the amygdala in the right hemisphere is involved in memory-related function. [ 18F]fluorodeoxyglucose positron emission topography (FDG-PET) showed glucose hypometabolism limited to the right uncus for the TLE-AL group. The results suggest the importance of considering cognitive functions in the non-dominant hemisphere to prevent impairment after surgery. The main function of hippocampus is memory formation.The main function of the amygdala, by contrast, is considered to be related to emotion and drives, and not memory itself.The amygdala plays a role in memory enhancement in emotional conditions.The amygdala is also related to olfaction and autonomic control mediated by connection with the olfactory bulb, hypothalamic and brainstem centers [15–19].A lesion in the hippocampus, therefore, is considered to be associated with memory impairment.As for lesions in the amygdala, however, no report so far demonstrated a clear implication of memory impairment.
Mechatronics design is complex by nature as it involves a large number of couplings and interdependencies between subsystems and components alongside a variety of sometimes contradicting objectives and design constraints. Mechatronics design activity requires cross-disciplinary and multi-objective thinking. In this paper, a fuzzy-based approach for the modeling of a unified performance evaluation index in the detailed design phase is presented. This index acts as a multidisciplinary objective function aggregating all the design criteria and requirements from various disciplines and subsystems while taking into account the interactions and correlations among the objectives. Then this function is optimized using a particle swarm optimization algorithm alongside all the constraints facing each subsystem. As an application, the mechatronics design of a vision-guided quadrotor unmanned aerial vehicle is carried out to demonstrate the effectiveness of the proposed method. Thus, a thorough modeling of system dynamics, structure, aerodynamics, flight control and visual servoing system is carried out to provide the designer with all necessary design variables and requirements. The final results and related computer simulations show the effectiveness of the proposed method in finding solutions for an optimal mechatronic design. Mechatronic systems are multidisciplinary products, that incorporate an interactive and synergistic application of various domains such as mechanics, electronics, controls, and computer engineering.Due to the large number of couplings and dynamic interdependencies between subsystems and components, the design of mechatronic systems is considered to be a challenging and complex task, which requires a cross disciplinary design thought process (Torry-Smith et al., 2013; van Amerongen, 2003).This calls for a more systematic and multi-objective design approach to mechatronics (Mohebbi et al., 2014d).More precisely, a concurrent and integrated design method is needed to obtain more efficient, reliable and flexible products in less complex ways and at a lower cost (Mohebbi et al., 2014a).A number of research efforts have demonstrated that designing the structure and control concurrently, improves the system’s performance and efficiency (Cruz-Villar et al., 2009; de Silva and Behbahani, 2013; Li et al., 2001; Van Brussel, 1996; Zhang et al., 1999).Although, in most of these efforts, the mechanical structures of the system were usually determined in advance without considering the future aspects of the controller design.Therefore, a perfect control action may be far from practical concerns, due to limitations imposed by the poorly designed mechanical structure.
Mechatronics design is complex by nature as it involves a large number of couplings and interdependencies between subsystems and components alongside a variety of sometimes contradicting objectives and design constraints. Mechatronics design activity requires cross-disciplinary and multi-objective thinking. In this paper, a fuzzy-based approach for the modeling of a unified performance evaluation index in the detailed design phase is presented. This index acts as a multidisciplinary objective function aggregating all the design criteria and requirements from various disciplines and subsystems while taking into account the interactions and correlations among the objectives. Then this function is optimized using a particle swarm optimization algorithm alongside all the constraints facing each subsystem. As an application, the mechatronics design of a vision-guided quadrotor unmanned aerial vehicle is carried out to demonstrate the effectiveness of the proposed method. Thus, a thorough modeling of system dynamics, structure, aerodynamics, flight control and visual servoing system is carried out to provide the designer with all necessary design variables and requirements. The final results and related computer simulations show the effectiveness of the proposed method in finding solutions for an optimal mechatronic design. Multidisciplinary system design can be treated as an optimization problem by using a proper evaluation function.Due to their non-convex nature, solving optimization problems involving various structural and control parameters can be present many difficulties.Thus, despite the advances in optimal control system design, optimal integrated mechatronic system design is still an area that is currently being researched.Most of the existing multidisciplinary design frameworks utilize gradient-based solvers as an optimization driver for all the disciplines.Although, due to the extensive improvement, in the recent years, of the computational capacity of modern computers as well as the ability of using large numbers of parallel processors, the use of non-gradient based and probabilistic search algorithms has attracted much interest.This class of algorithms typically requires many more function evaluations than comparable gradient-based algorithms, but in return, it provides designers with several attractive characteristics.In addition, these algorithms are usually easy to implement, do not require continuity of response functions, and are better suited to finding global or near-global solutions.(Affi et al., 2007) presented a genetic algorithm-based method for the design and optimization of the geometry and dynamic behaviors of a four-bar mechatronic system.(Hammadi et al., 2014) proposed a new methodology for optimizing mechatronic systems based on a multi-agent approach.They decomposed the design process to three design agents and coordinated the local optimizations towards these agents.
Mechatronics design is complex by nature as it involves a large number of couplings and interdependencies between subsystems and components alongside a variety of sometimes contradicting objectives and design constraints. Mechatronics design activity requires cross-disciplinary and multi-objective thinking. In this paper, a fuzzy-based approach for the modeling of a unified performance evaluation index in the detailed design phase is presented. This index acts as a multidisciplinary objective function aggregating all the design criteria and requirements from various disciplines and subsystems while taking into account the interactions and correlations among the objectives. Then this function is optimized using a particle swarm optimization algorithm alongside all the constraints facing each subsystem. As an application, the mechatronics design of a vision-guided quadrotor unmanned aerial vehicle is carried out to demonstrate the effectiveness of the proposed method. Thus, a thorough modeling of system dynamics, structure, aerodynamics, flight control and visual servoing system is carried out to provide the designer with all necessary design variables and requirements. The final results and related computer simulations show the effectiveness of the proposed method in finding solutions for an optimal mechatronic design. Particle Swarm Optimization (PSO) is a recent addition to non-gradient-based stochastic and population-based optimization algorithms that was first introduced by (Eberhart and Kennedy, 1995).This method is based on a simplified social model that is closely tied to swarming theory and is inspired by the social and cognitive behaviors of birds in a flock seeking food.Each bird makes use of its own memory, as well as knowledge gained by the flock as a whole, to efficiently adapt to its environment and find food (Eberhart and Kennedy, 1995; Kennedy et al., 2001).PSO is able to solve discontinuous, multi-modal, non-convex problems, and thus it is a suitable tool to support engineering design problems (Campana et al., 2006; Schutte et al., 2004; Van Den Bergh and Engelbrecht, 2006).According to the literature, while the PSO algorithm has been applied to many engineering and design problems, only a few multidisciplinary applications are known (Venter and Sobieszczanski-Sobieski, 2002).In this paper, a multiobjective PSO is used and tailored to a constrained multi-disciplinary design problem.
Mechatronics design is complex by nature as it involves a large number of couplings and interdependencies between subsystems and components alongside a variety of sometimes contradicting objectives and design constraints. Mechatronics design activity requires cross-disciplinary and multi-objective thinking. In this paper, a fuzzy-based approach for the modeling of a unified performance evaluation index in the detailed design phase is presented. This index acts as a multidisciplinary objective function aggregating all the design criteria and requirements from various disciplines and subsystems while taking into account the interactions and correlations among the objectives. Then this function is optimized using a particle swarm optimization algorithm alongside all the constraints facing each subsystem. As an application, the mechatronics design of a vision-guided quadrotor unmanned aerial vehicle is carried out to demonstrate the effectiveness of the proposed method. Thus, a thorough modeling of system dynamics, structure, aerodynamics, flight control and visual servoing system is carried out to provide the designer with all necessary design variables and requirements. The final results and related computer simulations show the effectiveness of the proposed method in finding solutions for an optimal mechatronic design. In mechatronics design, modeling all subsystems and their interconnections, while simultaneously modeling interactions between the design criteria over several engineering domains, is not a trivial task.(Mohebbi et al., 2014a) presented a new approach based on their multi-criteria mechatronic profile (MMP) for the conceptual design stage.In order to facilitate the design process and supporting decision-making, in the presence of interacting criteria, they used fuzzy integrals (Mohebbi et al., 2014b, c) which are proven to be precise and reliable in a multi-criteria problem with interaction between the objectives.The Choquet integral is one of the most expressive preference models used in decision theory.It enables the expression of positive and negative interactions while covering an important range of possible decision behaviors, that are generally ignored in other MCDM methods (Grabisch, 1996, 1997).
Mechatronics design is complex by nature as it involves a large number of couplings and interdependencies between subsystems and components alongside a variety of sometimes contradicting objectives and design constraints. Mechatronics design activity requires cross-disciplinary and multi-objective thinking. In this paper, a fuzzy-based approach for the modeling of a unified performance evaluation index in the detailed design phase is presented. This index acts as a multidisciplinary objective function aggregating all the design criteria and requirements from various disciplines and subsystems while taking into account the interactions and correlations among the objectives. Then this function is optimized using a particle swarm optimization algorithm alongside all the constraints facing each subsystem. As an application, the mechatronics design of a vision-guided quadrotor unmanned aerial vehicle is carried out to demonstrate the effectiveness of the proposed method. Thus, a thorough modeling of system dynamics, structure, aerodynamics, flight control and visual servoing system is carried out to provide the designer with all necessary design variables and requirements. The final results and related computer simulations show the effectiveness of the proposed method in finding solutions for an optimal mechatronic design. In this work, we propose a novel Cascade Fuzzy-based multidisciplinary objective function which aggregates all the design criteria and requirements from various subsystems involved.This objective function is then optimized using a particle swarm optimization (PSO) algorithm for the purpose of detailed design of a mechatronic system.Using the proposed method, we can simultaneously consider the relative importance of each sub-objective or design criteria and also model the interdependencies and interactions amongst them.Subsequently, the proposed method can replace the commonly used sequential design approach which deals with the different subsystems and domains (i.e. mechanical, electrical, software, fluid, thermal, etc.) separately and enables design engineers to simultaneously design all subsystems by taking into account their co-influences.We validate our method to design a vision-guided quadrotor UAV as a complex mechatronic system.In terms of system dynamics, a quadrotor, an underactuated system with six degrees of freedom and four inputs, is inherently unstable and difficult to control.Thus, the design and control of this nonlinear system is a challenge from a practical and a theoretical point of view (Bouabdallah et al., 2004a; Bouabdallah and Siegwart, 2007; Mian and Wang, 2008; Salih et al., 2010).Thus, this is an excellent case for the formulation of a multidisciplinary design problem.However, integrating the sensors, actuators and intelligence into a lightweight vertically flying system with a suitable operation time is a challenging task since numerous interdependent design parameters originating from various engineering disciplines need to be considered.Moreover, in order to enable the system with autonomous capabilities, a visual feedback control strategy, i.e. visual servoing, is needed.This increases the number of requirements and objectives that need to be fulfilled, thus increasing the overall complexity of the design task.
Device-free sensing of human gestures has gained tremendous research attention with the recent advancements in wireless technologies. Channel State Information (CSI), a metric of Wi-Fi devices adopted for device-free sensing achieves better recognition performance. This survey classifies the state of the art recognition task into device-based and device-free sensing methods and highlights advancements with Wi-Fi CSI. This paper also comprehensively summarizes the recognition performance of device-free sensing using CSI under two approaches: model-based and learning based approaches. Machine Learning and Deep Learning algorithms are discussed under the learning based approaches with its corresponding recognition accuracy. Various signal pre-processing, feature extraction, selection, and classification techniques that are widely adopted for gesture recognition along with the environmental factors that influence the recognition accuracy are also discussed. This survey presents the conclusion spotting the challenges and opportunities that could be explored in the device free gesture recognition using the CSI metric of Wi-Fi devices. Digital advancements in Internet of Things (IoT) arena make the lives of humans better than ever before.Sensing and tracking of human activities have become an inevitable part in various fields like surveillance, entertainment, healthcare, etc.Thus, human gesture or activity recognition gains a lot of research interest, especially in areas that require human–machine interaction in some form.Several IoT protocols are implemented for various applications like sensing soil moisture (Boada et al., 2018), monitoring and controlling smart building (Vo et al., 2018), detecting human (Shukri et al., 2016) and stuffs (Nickels et al., 2013), human activity (Razzaq et al., 2018; Bhat et al., 2018; Hossain et al., 2018; Wang et al., 2015) and gesture (Abdelnasser et al., 2015) recognition, locating objects (Nezhadasl and Howard, 2019), finger printing localization (Janssen et al., 2018), crowd sensing (Alvear et al., 2018), smoke alarm (Wu et al., 2018), healthcare (Malik et al., 2018) and location tracking (Hong et al., 2018).IoT protocols like ZigBee, Z-wave, Bluetooth, Long Range (LoRa), and Wi-Fi are the widely used protocols for human activity and gesture recognition applications.
Device-free sensing of human gestures has gained tremendous research attention with the recent advancements in wireless technologies. Channel State Information (CSI), a metric of Wi-Fi devices adopted for device-free sensing achieves better recognition performance. This survey classifies the state of the art recognition task into device-based and device-free sensing methods and highlights advancements with Wi-Fi CSI. This paper also comprehensively summarizes the recognition performance of device-free sensing using CSI under two approaches: model-based and learning based approaches. Machine Learning and Deep Learning algorithms are discussed under the learning based approaches with its corresponding recognition accuracy. Various signal pre-processing, feature extraction, selection, and classification techniques that are widely adopted for gesture recognition along with the environmental factors that influence the recognition accuracy are also discussed. This survey presents the conclusion spotting the challenges and opportunities that could be explored in the device free gesture recognition using the CSI metric of Wi-Fi devices. Table 1 comprehensively discusses the pros and cons of various IoT protocols and analyzes the research advancements adopting COTS Wi-Fi devices in a device free gesture recognition paradigm.Summary of the observations from Table 1 are listed below.
Device-free sensing of human gestures has gained tremendous research attention with the recent advancements in wireless technologies. Channel State Information (CSI), a metric of Wi-Fi devices adopted for device-free sensing achieves better recognition performance. This survey classifies the state of the art recognition task into device-based and device-free sensing methods and highlights advancements with Wi-Fi CSI. This paper also comprehensively summarizes the recognition performance of device-free sensing using CSI under two approaches: model-based and learning based approaches. Machine Learning and Deep Learning algorithms are discussed under the learning based approaches with its corresponding recognition accuracy. Various signal pre-processing, feature extraction, selection, and classification techniques that are widely adopted for gesture recognition along with the environmental factors that influence the recognition accuracy are also discussed. This survey presents the conclusion spotting the challenges and opportunities that could be explored in the device free gesture recognition using the CSI metric of Wi-Fi devices. Gesture recognition automates the recognition task of human activities in a device-based or device-free sensing environment.The recognition task utilizes the advancements in the wireless technologies for sensing and recognizing the human targets in an indoor or outdoor environment depending on the spectral range of the wireless communication protocol adopted.State of the art device-free sensing utilizes radar-based or Commercial Off The Shelf (COTS) products that operate within the electromagnetic spectrum.
Device-free sensing of human gestures has gained tremendous research attention with the recent advancements in wireless technologies. Channel State Information (CSI), a metric of Wi-Fi devices adopted for device-free sensing achieves better recognition performance. This survey classifies the state of the art recognition task into device-based and device-free sensing methods and highlights advancements with Wi-Fi CSI. This paper also comprehensively summarizes the recognition performance of device-free sensing using CSI under two approaches: model-based and learning based approaches. Machine Learning and Deep Learning algorithms are discussed under the learning based approaches with its corresponding recognition accuracy. Various signal pre-processing, feature extraction, selection, and classification techniques that are widely adopted for gesture recognition along with the environmental factors that influence the recognition accuracy are also discussed. This survey presents the conclusion spotting the challenges and opportunities that could be explored in the device free gesture recognition using the CSI metric of Wi-Fi devices. The performance of the recognition model confines with the presence of sensing targets in the environment and the hardware specifications.Automatic recognition of human activities has a wide range of applications in the field of healthcare (Rodriguez et al., 2017; Wang et al., 2016d; Zeng et al., 2015; Shang and Wu, 2016), surveillance (Gavrilova et al., 2017; Ding et al., 2018), vehicular technology (Duan et al., 2018), and in almost all areas that require human–machine interaction (Saha et al., 2018).Gesture recognition systems perform the recognition task by implementing such sensing methods:
Device-free sensing of human gestures has gained tremendous research attention with the recent advancements in wireless technologies. Channel State Information (CSI), a metric of Wi-Fi devices adopted for device-free sensing achieves better recognition performance. This survey classifies the state of the art recognition task into device-based and device-free sensing methods and highlights advancements with Wi-Fi CSI. This paper also comprehensively summarizes the recognition performance of device-free sensing using CSI under two approaches: model-based and learning based approaches. Machine Learning and Deep Learning algorithms are discussed under the learning based approaches with its corresponding recognition accuracy. Various signal pre-processing, feature extraction, selection, and classification techniques that are widely adopted for gesture recognition along with the environmental factors that influence the recognition accuracy are also discussed. This survey presents the conclusion spotting the challenges and opportunities that could be explored in the device free gesture recognition using the CSI metric of Wi-Fi devices. Device-based sensing methods adopt wearable sensors or body contact devices for achieving the recognition task.Monitoring cardiac patients using wearable bio harness (Rodriguez et al., 2017), detecting elderly fall with acceleration sensors (Khawandi et al., 2012) and activity recognition implementing Bluetooth protocol using Texas Instrument-CC265 device (Bhat et al., 2018) are some work adopting device based sensing methods.Similarly, wearable sensing methods have a widespread application in the ambient assisted living environment, though it poses some limitations as these devices are perceived to be obtrusive by the users.Camera-based and sensor-based applications perform well in recognizing activities in complex scenarios, yet privacy and intrusive characteristics remain a challenging task.
Device-free sensing of human gestures has gained tremendous research attention with the recent advancements in wireless technologies. Channel State Information (CSI), a metric of Wi-Fi devices adopted for device-free sensing achieves better recognition performance. This survey classifies the state of the art recognition task into device-based and device-free sensing methods and highlights advancements with Wi-Fi CSI. This paper also comprehensively summarizes the recognition performance of device-free sensing using CSI under two approaches: model-based and learning based approaches. Machine Learning and Deep Learning algorithms are discussed under the learning based approaches with its corresponding recognition accuracy. Various signal pre-processing, feature extraction, selection, and classification techniques that are widely adopted for gesture recognition along with the environmental factors that influence the recognition accuracy are also discussed. This survey presents the conclusion spotting the challenges and opportunities that could be explored in the device free gesture recognition using the CSI metric of Wi-Fi devices. Device-free sensing methods provide alternate solutions as they adopt optical sensors or RGB Depth (RGBD) cameras like Microsoft Kinect (Gavrilova et al., 2017) and video cameras (Ding et al., 2018) and performs recognition in a contactless manner.Besides being device free, even optical sensors are considered intrusive and obtrusive as it captures images of the subject under surveillance.Furthermore, camera-based methods are sensitive to lighting conditions and occlusions.In such circumstances, device-free sensing adopting RF signals will be a better choice as they work only with the wireless signals.Device-free sensing methods adopting RF signals implements various IoT communication protocol and address the limitations mentioned above by establishing a contactless recognition paradigm.Radio frequency sensors using wireless signals of COTS devices perform activity recognition in a non-intrusive and non-obtrusive manner, operating in varying frequency range enabling the recognition task, depending on its coverage range and its corresponding spectral efficiency.Indoor sensing applications prefer Wi-Fi among other protocols as inferred from Table 1, as it is economical and does not demand any special infrastructure.Also, Wi-Fi is available readily with the deployment of commercial Wi-Fi devices in almost all indoor environments.Hence, Wi-Fi based recognition ensures a non-intrusive and privacy-preserving way of sensing by capturing only the signal reflections caused due to human movements.
The process of dating ancient obsidian artifacts converts the quantity of surface diffused molecular water to a calendar age using an experimentally derived diffusion coefficient predicted from glass composition. The internal structural water content of rhyolitic obsidian has been identified as a highly influential variable that controls the rate of water diffusion at ambient temperature. We demonstrate through the use of infrared spectroscopy and specific gravity (density) measurements on samples from 34 obsidian sources that total structural water (H2Ot) concentrations between sources can range from 0.07% to 1.66%. Structural water concentration within individual sources may also vary significantly and impact the accuracy of estimated ages for artifact manufacture if not properly monitored. A calibration for the water determination on individual samples by density measurement is developed here and the impact of structural water variation on obsidian chronometric dates is discussed. Geological flows of rhyolitic glass, or obsidian, were routine sources of tool raw material for humans in the Lower and Middle Paleolithic (Adler et al., 2014; Merrick and Brown, 1984) and afterward (Shackley, 1998).Easily accessible from primary surface outcroppings, or within secondarily deposited erosional contexts (Doleman et al., 2012), obsidian was knapped into functional tools, symbols of cultural interconnectedness (Torrence, 2011), or markers of social inequality (Pierce, 2015).The abundance of obsidian in the archaeological record, created by its use within many types of cultural contexts, provides an opportunity for obsidian hydration age determinations to play a significant role in archaeological interpretation.High resolution temporal reconstructions of past events benefit from chronometric assays where the standard errors associated with dates are small.Rogers (2008b, 2010) discusses four major error sources that can impact the final age determination: hydration rim measurement precision, estimation of temperature history (effective hydration temperature), variation in the experimentally determined source-specific diffusion coefficient, and intra-source variability in intrinsic water content; all of which contribute to the final age uncertainty.The fourth variable, the variability in obsidian structural water content, and its influence on the dating outcome, is the factor explored here.
The process of dating ancient obsidian artifacts converts the quantity of surface diffused molecular water to a calendar age using an experimentally derived diffusion coefficient predicted from glass composition. The internal structural water content of rhyolitic obsidian has been identified as a highly influential variable that controls the rate of water diffusion at ambient temperature. We demonstrate through the use of infrared spectroscopy and specific gravity (density) measurements on samples from 34 obsidian sources that total structural water (H2Ot) concentrations between sources can range from 0.07% to 1.66%. Structural water concentration within individual sources may also vary significantly and impact the accuracy of estimated ages for artifact manufacture if not properly monitored. A calibration for the water determination on individual samples by density measurement is developed here and the impact of structural water variation on obsidian chronometric dates is discussed. Once a geological nodule of obsidian is cleaved, molecular water is immediately adsorbed onto the new surface and begins to diffuse into the glass to form an optically visible hydration layer at high magnification (800×).Over time the layer will increase in thickness if the glass surface is not dissolved by alkaline ground waters (Morgenstein et al., 1999) or subject to alteration through burning or abrasion (Steffen, 2005).At any point in time the age of artifact manufacture may be estimated by measuring the amount of diffused water and converting that quantity to a calendar date with a compositionally dependent water diffusion coefficient adjusted for the temperature and humidity of the archaeological context (Rogers, 2007a, 2012; Stevenson et al., 1998).
The process of dating ancient obsidian artifacts converts the quantity of surface diffused molecular water to a calendar age using an experimentally derived diffusion coefficient predicted from glass composition. The internal structural water content of rhyolitic obsidian has been identified as a highly influential variable that controls the rate of water diffusion at ambient temperature. We demonstrate through the use of infrared spectroscopy and specific gravity (density) measurements on samples from 34 obsidian sources that total structural water (H2Ot) concentrations between sources can range from 0.07% to 1.66%. Structural water concentration within individual sources may also vary significantly and impact the accuracy of estimated ages for artifact manufacture if not properly monitored. A calibration for the water determination on individual samples by density measurement is developed here and the impact of structural water variation on obsidian chronometric dates is discussed. The development of ambient hydration rates is based upon extrapolations from diffusion coefficients experimentally determined at elevated temperatures (90–250 °C) on glasses of known geological provenance (Friedman and Long, 1976; Michels et al., 1983; Rogers and Duke, 2011; Rogers and Stevenson, 2017; Stevenson et al., 1998; Stevenson and Novak, 2011).The application of the rate to archaeological specimens relies upon the assumption of compositional uniformity between the experimental source glass and the cultural artifact found away from the geological deposit.Supportive evidence for this uniformity is based upon a similarity in trace element concentrations between the obsidian and its source as determined by commonly applied methods such as X-ray fluorescence (Shackley, 2005; Frahm, 2014; Fruend et al., 2015), ICP-MS (Tykot, 1998), or neutron activation analysis (Glascock, 2011; Perreault et al., 2016).These data on the elemental composition are however are not sufficient in themselves and must be complemented by a knowledge of a source structural water content and its variability.Only then can the laboratory diffusion coefficients be appropriately applied to equivalent materials.
The process of dating ancient obsidian artifacts converts the quantity of surface diffused molecular water to a calendar age using an experimentally derived diffusion coefficient predicted from glass composition. The internal structural water content of rhyolitic obsidian has been identified as a highly influential variable that controls the rate of water diffusion at ambient temperature. We demonstrate through the use of infrared spectroscopy and specific gravity (density) measurements on samples from 34 obsidian sources that total structural water (H2Ot) concentrations between sources can range from 0.07% to 1.66%. Structural water concentration within individual sources may also vary significantly and impact the accuracy of estimated ages for artifact manufacture if not properly monitored. A calibration for the water determination on individual samples by density measurement is developed here and the impact of structural water variation on obsidian chronometric dates is discussed. Accelerated hydration rate experiments on a variety of obsidians from around the globe have demonstrated that obsidian structural water in the form of bonded OH (hydroxyl) and/or total water (H2Ot) [H2Ot = OH + H2O] correlates strongly with experimental rates of diffusion to the extent that prediction equations for the Arrhenius rate constants have been developed (Stevenson et al., 1998; Rogers, 2015).The influence of the anhydrous component on water diffusion has received only minor attention (Friedman and Long, 1976) and will require additional study to detail its role.Consequently, our coverage of all the potential variables structuring the hydration rate will be incomplete, but we will focus on the impacts of structural water in the obsidian hydration dating (OHD) process.
The Hepu Han Tombs, located on the south coast of Guangxi Zhuang Autonomous Region, in southern China, bordering the Beibu Gulf, can date back to the Han dynasty (206 BCE–220 CE), when Hepu was an important international trade port of the maritime Silk Road, spreading strong and persistent cultural influence to neighboring countries from ancient China, and also connecting China with African, European and other Asian countries in national migration, religious communication, political, trade, technical and cultural exchanges, and so on. The research of the Hepu Han Tombs can offer opportunities to understand the burial customs and the Han culture in the Chinese Han Dynasty. With the aim of detecting the location, depth and geometry of two burial mounds at Jinjiling Site of Hepu Han Tombs, an electrical resistivity tomography (ERT) survey was performed in cooperation with archaeological team to calibrate the results with detailed information from the limited drillings in the area. Besides, series of 2-D ERT sections, acquired above the larger mound, were combined into a “Pseudo 3-D” volume, and iso-resistivity surfaces were further calculated to emphasize the location and lateral variations within the data volume, expected towards a more detailed and quantitative interpretation. Both the 2-D and 3-D resistivity imaging gave a clear evidence of structural details of the burial mounds. The results have shown a general consistency between the geophysical work and traditional archaeological drilling explorations, improving our knowledge on the un-excavated mounds, and allowing detailed pre-excavation plan at the Hepu Han Tombs. Hepu County is located on the south coast of Guangxi Zhuang Autonomous Region, in southern China, bordering the Beibu Gulf (Fig. 1).The name of “Hepu” (合浦) means intersection of rivers and sea in Chinese, based on the local geographical features, as the North-South Nanliu River flows into the Beibu Gulf in the region.In the year 111 BCE, the Hepu district was conquered by the Emperor Che Liu of Han Empire, and soon after became a big city, within the area ten times larger than today.This region also became an important international trade port and a starting point of Maritime Silk Road, spreading strong and persistent cultural influence to neighboring countries from ancient China, and connecting China with African, European and other Asian countries in national migration, religious communication, political, trade, technical and cultural exchanges, and so on in the historical period.
The Hepu Han Tombs, located on the south coast of Guangxi Zhuang Autonomous Region, in southern China, bordering the Beibu Gulf, can date back to the Han dynasty (206 BCE–220 CE), when Hepu was an important international trade port of the maritime Silk Road, spreading strong and persistent cultural influence to neighboring countries from ancient China, and also connecting China with African, European and other Asian countries in national migration, religious communication, political, trade, technical and cultural exchanges, and so on. The research of the Hepu Han Tombs can offer opportunities to understand the burial customs and the Han culture in the Chinese Han Dynasty. With the aim of detecting the location, depth and geometry of two burial mounds at Jinjiling Site of Hepu Han Tombs, an electrical resistivity tomography (ERT) survey was performed in cooperation with archaeological team to calibrate the results with detailed information from the limited drillings in the area. Besides, series of 2-D ERT sections, acquired above the larger mound, were combined into a “Pseudo 3-D” volume, and iso-resistivity surfaces were further calculated to emphasize the location and lateral variations within the data volume, expected towards a more detailed and quantitative interpretation. Both the 2-D and 3-D resistivity imaging gave a clear evidence of structural details of the burial mounds. The results have shown a general consistency between the geophysical work and traditional archaeological drilling explorations, improving our knowledge on the un-excavated mounds, and allowing detailed pre-excavation plan at the Hepu Han Tombs. Hepu Han Tombs, the distribution boundary of which is marked with blue line in Fig. 1 surrounding Hepu County, are the historic outcome of the implementation of centralized rule and new administrative system, the commandery-district system carried out away from the central government, by the Han Empire (Xiong, 2014).Furthermore, 1056 mounds are scattered over a total area of 68 km2 in the region, and can easily be identified on the surface (Xiong, 2015).In the past 60 years, a series of archaeological surveys were performed without any geophysical work.Over 1200 tombs have been excavated and it is estimated that almost 10,000 ancient tombs are still waiting to be discovered.In addition, most of the excavated tombs can date back to the Han dynasty (202 BCE–220 CE), while others are from subsequent historical periods, e.g. Three Kingdoms, Jin Dynasty and Southern Dynasties periods (220–589 CE).
The Hepu Han Tombs, located on the south coast of Guangxi Zhuang Autonomous Region, in southern China, bordering the Beibu Gulf, can date back to the Han dynasty (206 BCE–220 CE), when Hepu was an important international trade port of the maritime Silk Road, spreading strong and persistent cultural influence to neighboring countries from ancient China, and also connecting China with African, European and other Asian countries in national migration, religious communication, political, trade, technical and cultural exchanges, and so on. The research of the Hepu Han Tombs can offer opportunities to understand the burial customs and the Han culture in the Chinese Han Dynasty. With the aim of detecting the location, depth and geometry of two burial mounds at Jinjiling Site of Hepu Han Tombs, an electrical resistivity tomography (ERT) survey was performed in cooperation with archaeological team to calibrate the results with detailed information from the limited drillings in the area. Besides, series of 2-D ERT sections, acquired above the larger mound, were combined into a “Pseudo 3-D” volume, and iso-resistivity surfaces were further calculated to emphasize the location and lateral variations within the data volume, expected towards a more detailed and quantitative interpretation. Both the 2-D and 3-D resistivity imaging gave a clear evidence of structural details of the burial mounds. The results have shown a general consistency between the geophysical work and traditional archaeological drilling explorations, improving our knowledge on the un-excavated mounds, and allowing detailed pre-excavation plan at the Hepu Han Tombs. The extensive tombs at Hepu represent one of the best-preserved ancient tomb complexes of the Han dynasty in China.According to the detailed information from previous archaeological excavation surveys, the scales, orientations, and burial architectures of the ancient tombs are not exactly same: the scales range from 2 × 1 × 1 m to 15 × 10 × 3 m, while the lengths of tomb passages range from 6 m to 40 m, which mainly depends on the social status of the tomb owner and universal architectural style during the building period.Fig. 2 displays four typical planar structures of burial mounds with a historical evolution in the Han Dynasty: the structure of wooden-chambered tombs in the upper left figure was replaced by the two structures in the lower left since Mid-term of Western Dynasty (about 87 BCE), and with the emergence of brick-chambered tombs, until the structure of brick-chambered dominated the construction style in Eastern Han Dynasty.An example photo from an excavated well-preserved brick-chambered tomb found in 1987, with a tomb passage, a front chamber, a back chamber, and two symmetrical side rooms, is also provided in Fig. 2.
The Hepu Han Tombs, located on the south coast of Guangxi Zhuang Autonomous Region, in southern China, bordering the Beibu Gulf, can date back to the Han dynasty (206 BCE–220 CE), when Hepu was an important international trade port of the maritime Silk Road, spreading strong and persistent cultural influence to neighboring countries from ancient China, and also connecting China with African, European and other Asian countries in national migration, religious communication, political, trade, technical and cultural exchanges, and so on. The research of the Hepu Han Tombs can offer opportunities to understand the burial customs and the Han culture in the Chinese Han Dynasty. With the aim of detecting the location, depth and geometry of two burial mounds at Jinjiling Site of Hepu Han Tombs, an electrical resistivity tomography (ERT) survey was performed in cooperation with archaeological team to calibrate the results with detailed information from the limited drillings in the area. Besides, series of 2-D ERT sections, acquired above the larger mound, were combined into a “Pseudo 3-D” volume, and iso-resistivity surfaces were further calculated to emphasize the location and lateral variations within the data volume, expected towards a more detailed and quantitative interpretation. Both the 2-D and 3-D resistivity imaging gave a clear evidence of structural details of the burial mounds. The results have shown a general consistency between the geophysical work and traditional archaeological drilling explorations, improving our knowledge on the un-excavated mounds, and allowing detailed pre-excavation plan at the Hepu Han Tombs. Following the Han burial customs, a tomb should be ideally decorated as a home for the dead person, sharing the same case when he lived, so that he could continue the next life in another world.Buried funerary objects are always supposed to imply the tomb-owner's daily life and to reveal his/her wealth and status when alive.Besides common pottery, bronze, iron, lacquer, gold and silver wares, and local pearls were unearthed, there were other lots of beautiful exotic cultural relics, such as glass from Mediterranean area, garnet from India and Sri Lanka, amber from Roman Empire or Myanmar, turquoise from Persia, agate and perfume from Southeast Asia, and so on (e.g. Liu et al., 2013; Dong et al., 2014; Xiong, 2015).Obviously, abundant unearthed funerary objects reveal many multi-cultural elements, which highlight the region wasn't strongly influenced only from Central Plains of China during the period, but also communicated with other parts of Asia and Mediterranean area (Hao and Lu, 1731).
In recent years, several optimization algorithms are proposed, one of them is Multi-Verse Optimizer (MVO). In this paper, a modified version of MVO is proposed, called CMVHHO, which uses the chaos theory and the Harris Hawks Optimization (HHO). The main aim of using the chaotic maps in the proposed method is to determine the optimal value for the parameters of the basic MVO. Besides, the HHO is used as a local search to improve the ability of the MVO to exploit the search space. The performance of the CMVHHO is conducted using a set of chaotic maps to determine the most suitable map, as well as, the different experiments are performed to determine which parameter has the largest effect on the effectiveness of the MVO. Moreover, the performance of the CMVHHO is compared with a set of state-of-the-art algorithms to find the best solution for global optimization problems. Furthermore, the proposed CMVHHO with the best map is applied to solve four well-known engineering problems. The experimental results illustrate that the chaotic Circle map is the best map among all maps because it improved the performance of the CMVHHO, as well as the HHO, affected positively in the behavior of the proposed algorithm. The CMVHHO showed the best results than other algorithms in terms of the performance measures as well as in engineering problems and it outperformed the state-of-the-art algorithms in all problems. Recently, the real-world optimization problems (OPs) has got more attention in many fields includes function optimization, information science, operational research, engineering design, and their applications.In general, there are two categories of methods used to solve the OPs, the first one is traditional methods and the second is the meta-heuristic methods.The traditional methods such as gradient descent and Newton which are easy to implement, however, these methods are time-consuming, also, in each run there exists only one solution and their efficiency depending on the type of the given problem (such as constraints), variables (integer, continuous, binary), fitness function (linear, non-linear), the search space (convex, non-convex) and the number of the variables (Baykasoglu, 2012).In order to overcome these drawbacks, the meta-heuristic (MH) optimization methods, that considered as a branch of artificial intelligence, were proposed as global optimization approach (Ewees et al., 2018).The properties of MH methods are low complexities, highly robust, and high efficiency.It also overcomes the high complexity and the weakness problem in the searching process of the traditional methods (El Aziz et al., 2018a, 2017).
In recent years, several optimization algorithms are proposed, one of them is Multi-Verse Optimizer (MVO). In this paper, a modified version of MVO is proposed, called CMVHHO, which uses the chaos theory and the Harris Hawks Optimization (HHO). The main aim of using the chaotic maps in the proposed method is to determine the optimal value for the parameters of the basic MVO. Besides, the HHO is used as a local search to improve the ability of the MVO to exploit the search space. The performance of the CMVHHO is conducted using a set of chaotic maps to determine the most suitable map, as well as, the different experiments are performed to determine which parameter has the largest effect on the effectiveness of the MVO. Moreover, the performance of the CMVHHO is compared with a set of state-of-the-art algorithms to find the best solution for global optimization problems. Furthermore, the proposed CMVHHO with the best map is applied to solve four well-known engineering problems. The experimental results illustrate that the chaotic Circle map is the best map among all maps because it improved the performance of the CMVHHO, as well as the HHO, affected positively in the behavior of the proposed algorithm. The CMVHHO showed the best results than other algorithms in terms of the performance measures as well as in engineering problems and it outperformed the state-of-the-art algorithms in all problems. There are several types of MH methods includes evolutionary algorithms (EA) and swarm intelligent (SI) that have received much attention.The EA methods have been inspired by natural evolutionary mechanisms (Yang, 2008).For example, evolutionary strategy (ES) (Beyer and Schwefel, 2002), genetic algorithms (GA) (Han et al., 2016), and differential evolution (DE) (Sarker et al., 2014).However, the parameters of the EA operations (i.e., crossover, mutation, and selection) may lead to getting stuck in a local point since they require to select the optimal value before starting their optimization task.The SI based algorithms inspired by emulating the behavior of insects or animals which live in groups (Yang, 2008).The individuals of these groups provide their experience to service the group in finding their foods in a short time and a fast way.This relation between the individuals is simulated to solve the complex problems.There are several methods of SI include ant colony optimization (ACO) (Kaveh and Talatahari, 2010), bat algorithm (BAT) (Yang, 2010), moth-flame optimization (MFO) (Mirjalili, 2015b), and artificial bee colony (ABC) algorithm (Karaboga, 2005).
In recent years, several optimization algorithms are proposed, one of them is Multi-Verse Optimizer (MVO). In this paper, a modified version of MVO is proposed, called CMVHHO, which uses the chaos theory and the Harris Hawks Optimization (HHO). The main aim of using the chaotic maps in the proposed method is to determine the optimal value for the parameters of the basic MVO. Besides, the HHO is used as a local search to improve the ability of the MVO to exploit the search space. The performance of the CMVHHO is conducted using a set of chaotic maps to determine the most suitable map, as well as, the different experiments are performed to determine which parameter has the largest effect on the effectiveness of the MVO. Moreover, the performance of the CMVHHO is compared with a set of state-of-the-art algorithms to find the best solution for global optimization problems. Furthermore, the proposed CMVHHO with the best map is applied to solve four well-known engineering problems. The experimental results illustrate that the chaotic Circle map is the best map among all maps because it improved the performance of the CMVHHO, as well as the HHO, affected positively in the behavior of the proposed algorithm. The CMVHHO showed the best results than other algorithms in terms of the performance measures as well as in engineering problems and it outperformed the state-of-the-art algorithms in all problems. In general, the meta-heuristic algorithms are divided the search process into exploration and exploitation phases.When the meta-heuristic algorithm tries to explore the best areas of search space, the exploration phase is performed.Unlike the exploration, the exploitation phase points to the convergence of the algorithm towards the global solution.However, both of these concepts conflict with each other and may make the MH algorithms get stuck in a local point (Elaziz et al., 2017; El Aziz et al., 2018b).To try to maintain this issue, chaos is used to balance exploration and exploitation phases.
In recent years, several optimization algorithms are proposed, one of them is Multi-Verse Optimizer (MVO). In this paper, a modified version of MVO is proposed, called CMVHHO, which uses the chaos theory and the Harris Hawks Optimization (HHO). The main aim of using the chaotic maps in the proposed method is to determine the optimal value for the parameters of the basic MVO. Besides, the HHO is used as a local search to improve the ability of the MVO to exploit the search space. The performance of the CMVHHO is conducted using a set of chaotic maps to determine the most suitable map, as well as, the different experiments are performed to determine which parameter has the largest effect on the effectiveness of the MVO. Moreover, the performance of the CMVHHO is compared with a set of state-of-the-art algorithms to find the best solution for global optimization problems. Furthermore, the proposed CMVHHO with the best map is applied to solve four well-known engineering problems. The experimental results illustrate that the chaotic Circle map is the best map among all maps because it improved the performance of the CMVHHO, as well as the HHO, affected positively in the behavior of the proposed algorithm. The CMVHHO showed the best results than other algorithms in terms of the performance measures as well as in engineering problems and it outperformed the state-of-the-art algorithms in all problems. The chaos (Feldman, 2012) is a stochastic phenomenon generated in a nonlinear dynamic system with the properties of regularity, randomness, sensitive to the initial condition and ergodicity.These properties of chaos, make the algorithms perform at higher speeds than standard stochastic search with standard probability distributions and avoid getting stuck in local optimal point (Ewees et al., 2017).
In recent years, several optimization algorithms are proposed, one of them is Multi-Verse Optimizer (MVO). In this paper, a modified version of MVO is proposed, called CMVHHO, which uses the chaos theory and the Harris Hawks Optimization (HHO). The main aim of using the chaotic maps in the proposed method is to determine the optimal value for the parameters of the basic MVO. Besides, the HHO is used as a local search to improve the ability of the MVO to exploit the search space. The performance of the CMVHHO is conducted using a set of chaotic maps to determine the most suitable map, as well as, the different experiments are performed to determine which parameter has the largest effect on the effectiveness of the MVO. Moreover, the performance of the CMVHHO is compared with a set of state-of-the-art algorithms to find the best solution for global optimization problems. Furthermore, the proposed CMVHHO with the best map is applied to solve four well-known engineering problems. The experimental results illustrate that the chaotic Circle map is the best map among all maps because it improved the performance of the CMVHHO, as well as the HHO, affected positively in the behavior of the proposed algorithm. The CMVHHO showed the best results than other algorithms in terms of the performance measures as well as in engineering problems and it outperformed the state-of-the-art algorithms in all problems. Based on the advantages of chaos theory and MH methods, they are combined together to increase the MH algorithms’ performance for solving optimization problems in literature, for example, particle swarm optimization (PSO) (Ren and Zhong, 2011), krill heard (Wang et al., 2014), fruit fly optimization algorithm (FOA) (Mitić et al., 2015), bat algorithm (Gandomi and Yang, 2014), ABC (Oliva et al., 2017), and Runner-Root Algorithm (Ibrahim et al., 2017).All of the previous hybridization algorithms illustrate the importance of chaos theory to improve the performance of MH methods.
In recent years, several optimization algorithms are proposed, one of them is Multi-Verse Optimizer (MVO). In this paper, a modified version of MVO is proposed, called CMVHHO, which uses the chaos theory and the Harris Hawks Optimization (HHO). The main aim of using the chaotic maps in the proposed method is to determine the optimal value for the parameters of the basic MVO. Besides, the HHO is used as a local search to improve the ability of the MVO to exploit the search space. The performance of the CMVHHO is conducted using a set of chaotic maps to determine the most suitable map, as well as, the different experiments are performed to determine which parameter has the largest effect on the effectiveness of the MVO. Moreover, the performance of the CMVHHO is compared with a set of state-of-the-art algorithms to find the best solution for global optimization problems. Furthermore, the proposed CMVHHO with the best map is applied to solve four well-known engineering problems. The experimental results illustrate that the chaotic Circle map is the best map among all maps because it improved the performance of the CMVHHO, as well as the HHO, affected positively in the behavior of the proposed algorithm. The CMVHHO showed the best results than other algorithms in terms of the performance measures as well as in engineering problems and it outperformed the state-of-the-art algorithms in all problems. In the same context, the Harris Hawks Optimization (HHO) is recently developed as a meta-heuristic technique (Heidari et al., 2019).The HHO simulates the strategies used by Harris Hawks to catch their preys (i.e., rabbits).According to these strategies, the HHO is applied to solve global optimization (Heidari et al., 2019) and it established its performance over all the comparative algorithms.This high-performance result can be due to the HHO contains six stages which used for exploration and exploitation.These stages provide HHO with high ability to escape a local point and improve its convergence rate.However, its ability to exploitation is better than its exploration.
In recent years, several optimization algorithms are proposed, one of them is Multi-Verse Optimizer (MVO). In this paper, a modified version of MVO is proposed, called CMVHHO, which uses the chaos theory and the Harris Hawks Optimization (HHO). The main aim of using the chaotic maps in the proposed method is to determine the optimal value for the parameters of the basic MVO. Besides, the HHO is used as a local search to improve the ability of the MVO to exploit the search space. The performance of the CMVHHO is conducted using a set of chaotic maps to determine the most suitable map, as well as, the different experiments are performed to determine which parameter has the largest effect on the effectiveness of the MVO. Moreover, the performance of the CMVHHO is compared with a set of state-of-the-art algorithms to find the best solution for global optimization problems. Furthermore, the proposed CMVHHO with the best map is applied to solve four well-known engineering problems. The experimental results illustrate that the chaotic Circle map is the best map among all maps because it improved the performance of the CMVHHO, as well as the HHO, affected positively in the behavior of the proposed algorithm. The CMVHHO showed the best results than other algorithms in terms of the performance measures as well as in engineering problems and it outperformed the state-of-the-art algorithms in all problems. Therefore, the aim of this paper is to apply chaos theory and HHO to improved Multi-Verse Optimizer (MVO) and evaluate it in solving global optimization and four well-known engineering problems.
In recent years, several optimization algorithms are proposed, one of them is Multi-Verse Optimizer (MVO). In this paper, a modified version of MVO is proposed, called CMVHHO, which uses the chaos theory and the Harris Hawks Optimization (HHO). The main aim of using the chaotic maps in the proposed method is to determine the optimal value for the parameters of the basic MVO. Besides, the HHO is used as a local search to improve the ability of the MVO to exploit the search space. The performance of the CMVHHO is conducted using a set of chaotic maps to determine the most suitable map, as well as, the different experiments are performed to determine which parameter has the largest effect on the effectiveness of the MVO. Moreover, the performance of the CMVHHO is compared with a set of state-of-the-art algorithms to find the best solution for global optimization problems. Furthermore, the proposed CMVHHO with the best map is applied to solve four well-known engineering problems. The experimental results illustrate that the chaotic Circle map is the best map among all maps because it improved the performance of the CMVHHO, as well as the HHO, affected positively in the behavior of the proposed algorithm. The CMVHHO showed the best results than other algorithms in terms of the performance measures as well as in engineering problems and it outperformed the state-of-the-art algorithms in all problems. In general, the search for global solutions depends on randomness; therefore, it may fail to find the best solution and traps in some local optima; therefore, the selection of the optimization algorithm is an important task.In this context, the MVO showed superior results in previous studies includes Mirjalili et al. (2016) and Benmessahel et al. (2017) as well as proved a good behavior in exploitation phase (Elaziz et al., 2019).Nevertheless, its behavior varies due to the standard probability distributions, like any meta-heuristic algorithms.Hence to improve the performance of the basic MVO and make it converges fast to the global optimal solution, it is modified using different chaotic maps which utilized to change the random behavior of the basic MVO parameters as well as the HHO is applied as local search operators.The proposed method is called CMVHHO and its behavior and efficiency are tested in solving global optimization problems and different types of engendering problems.Therefore, this paper presents a new optimization algorithm, called CMVHHO, that benefits from the characteristics of the chaotic theory and HHO algorithm.
In recent years, several optimization algorithms are proposed, one of them is Multi-Verse Optimizer (MVO). In this paper, a modified version of MVO is proposed, called CMVHHO, which uses the chaos theory and the Harris Hawks Optimization (HHO). The main aim of using the chaotic maps in the proposed method is to determine the optimal value for the parameters of the basic MVO. Besides, the HHO is used as a local search to improve the ability of the MVO to exploit the search space. The performance of the CMVHHO is conducted using a set of chaotic maps to determine the most suitable map, as well as, the different experiments are performed to determine which parameter has the largest effect on the effectiveness of the MVO. Moreover, the performance of the CMVHHO is compared with a set of state-of-the-art algorithms to find the best solution for global optimization problems. Furthermore, the proposed CMVHHO with the best map is applied to solve four well-known engineering problems. The experimental results illustrate that the chaotic Circle map is the best map among all maps because it improved the performance of the CMVHHO, as well as the HHO, affected positively in the behavior of the proposed algorithm. The CMVHHO showed the best results than other algorithms in terms of the performance measures as well as in engineering problems and it outperformed the state-of-the-art algorithms in all problems. The main contributions of this paper are as follows:
This study aimed to determine the rate, cause and management of seizures in the context of potential ART–ASD interactions in a cohort of HIV + individuals. Records of 604 HIV + patients were reviewed and those reporting epilepsy/seizure diagnosis were further evaluated. This cohort exhibited a seizure rate of 2.4%. HIV + patients treated for epilepsy displayed low serum ASD levels and failed to achieve seizure control. They were more likely to disengage from Neurology follow-up. For HIV + patients presenting with seizures/epilepsy the ASD prescription and the provision of supplementary support services needs to be carefully considered. In Ireland newly diagnosed cases of HIV have been reported at an annual rate that ranges from 7.0 to 7.5 per 100,000 [1].Despite the introduction of highly active anti-retroviral therapy (HAART), 40–60% of HIV-infected individuals develop neurological complications [2–4].The frequency of new seizures in the HIV positive (+) population is estimated to be between 4 and 11% in the populations studied [5].To date the literature on the epidemiology of seizures and epilepsy in HIV has not generated reliable per patient year incidence estimates.Also no prevalence rates have been determined that can easily separate recurrent provoked seizures from recurrent unprovoked attacks (epilepsy).The data we have so far suggest a prevalence of all seizures of about 6% in a reasonably large HIV + cohort with approximately half of these identified as being unprovoked attacks [3].
This study aimed to determine the rate, cause and management of seizures in the context of potential ART–ASD interactions in a cohort of HIV + individuals. Records of 604 HIV + patients were reviewed and those reporting epilepsy/seizure diagnosis were further evaluated. This cohort exhibited a seizure rate of 2.4%. HIV + patients treated for epilepsy displayed low serum ASD levels and failed to achieve seizure control. They were more likely to disengage from Neurology follow-up. For HIV + patients presenting with seizures/epilepsy the ASD prescription and the provision of supplementary support services needs to be carefully considered. These data compare with a point prevalence of 0.8% for epilepsy in Ireland in the general population [6], although there is an expected 10% life time risk of a seizure of any type in the general population [7].Taking into account the wide variations in prevalence estimates for epilepsy due to differences in definitions and methodological approaches, a reasonable estimate of the prevalence of seizure disorders in people with HIV in Ireland is at least 3 times that of the general population.
Perampanel, a selective, non-competitive α-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA) receptor antagonist, is approved for adjunctive treatment of focal seizures, with or without secondarily generalized seizures, and for primary generalized tonic–clonic seizures in patients with epilepsy aged ≥ 12 years. Perampanel was recently approved for monotherapy use for focal seizures in the U.S.A. Anti-seizure drug monotherapy may be preferable to polytherapy, which is generally associated with increased toxicity, non-compliance, and cost. Here, we report cases where patients had converted to perampanel monotherapy during open-label extension (OLEx) portions of 9 Phase II and III studies. Of 2245 patients who enrolled in the OLEx studies, we identified 7 patients with drug-resistant focal seizures who discontinued all non-perampanel anti-seizure drugs and were maintained on perampanel monotherapy for ≥ 91 days until the end of data cut-off. Patients received perampanel monotherapy for up to 1099 days (157 weeks), most at a modal dose of 12 mg. Seizure data were available for 6 patients, of whom 5 had a ≥ 90% reduction in overall seizure frequency between baseline and their last 13-week period of monotherapy (3 were seizure-free). Perampanel monotherapy was generally well tolerated and the safety profile during perampanel monotherapy was consistent with clinical and post-marketing experience in the adjunctive setting. This analysis included a small proportion of patients with highly drug-resistant focal seizures who converted to monotherapy during OLEx studies. While these limited data are encouraging in suggesting that perampanel might be useful as a monotherapy, further studies are required to explore outcomes in a less drug-resistant population, where a larger proportion of patients might benefit from monotherapy. Perampanel, a selective, non-competitive α-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA) receptor antagonist, is approved for adjunctive treatment of focal seizures, with or without secondarily generalized (SG) seizures, and for primary generalized tonic–clonic seizures in patients with epilepsy aged ≥ 12 years [1,2].Perampanel was recently approved for monotherapy use for focal seizures in the U.S.A.
Perampanel, a selective, non-competitive α-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA) receptor antagonist, is approved for adjunctive treatment of focal seizures, with or without secondarily generalized seizures, and for primary generalized tonic–clonic seizures in patients with epilepsy aged ≥ 12 years. Perampanel was recently approved for monotherapy use for focal seizures in the U.S.A. Anti-seizure drug monotherapy may be preferable to polytherapy, which is generally associated with increased toxicity, non-compliance, and cost. Here, we report cases where patients had converted to perampanel monotherapy during open-label extension (OLEx) portions of 9 Phase II and III studies. Of 2245 patients who enrolled in the OLEx studies, we identified 7 patients with drug-resistant focal seizures who discontinued all non-perampanel anti-seizure drugs and were maintained on perampanel monotherapy for ≥ 91 days until the end of data cut-off. Patients received perampanel monotherapy for up to 1099 days (157 weeks), most at a modal dose of 12 mg. Seizure data were available for 6 patients, of whom 5 had a ≥ 90% reduction in overall seizure frequency between baseline and their last 13-week period of monotherapy (3 were seizure-free). Perampanel monotherapy was generally well tolerated and the safety profile during perampanel monotherapy was consistent with clinical and post-marketing experience in the adjunctive setting. This analysis included a small proportion of patients with highly drug-resistant focal seizures who converted to monotherapy during OLEx studies. While these limited data are encouraging in suggesting that perampanel might be useful as a monotherapy, further studies are required to explore outcomes in a less drug-resistant population, where a larger proportion of patients might benefit from monotherapy. It has been a regulatory standard for anti-seizure drugs to be initially evaluated for adjunctive use, given ethical concerns around the use of placebo-controlled trials for anti-seizure drug monotherapy [3].However, since anti-seizure drug polytherapy is often associated with increased toxicity, non-adherence, and cost, monotherapy may be preferable in some clinical practice settings [4].
Perampanel, a selective, non-competitive α-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA) receptor antagonist, is approved for adjunctive treatment of focal seizures, with or without secondarily generalized seizures, and for primary generalized tonic–clonic seizures in patients with epilepsy aged ≥ 12 years. Perampanel was recently approved for monotherapy use for focal seizures in the U.S.A. Anti-seizure drug monotherapy may be preferable to polytherapy, which is generally associated with increased toxicity, non-compliance, and cost. Here, we report cases where patients had converted to perampanel monotherapy during open-label extension (OLEx) portions of 9 Phase II and III studies. Of 2245 patients who enrolled in the OLEx studies, we identified 7 patients with drug-resistant focal seizures who discontinued all non-perampanel anti-seizure drugs and were maintained on perampanel monotherapy for ≥ 91 days until the end of data cut-off. Patients received perampanel monotherapy for up to 1099 days (157 weeks), most at a modal dose of 12 mg. Seizure data were available for 6 patients, of whom 5 had a ≥ 90% reduction in overall seizure frequency between baseline and their last 13-week period of monotherapy (3 were seizure-free). Perampanel monotherapy was generally well tolerated and the safety profile during perampanel monotherapy was consistent with clinical and post-marketing experience in the adjunctive setting. This analysis included a small proportion of patients with highly drug-resistant focal seizures who converted to monotherapy during OLEx studies. While these limited data are encouraging in suggesting that perampanel might be useful as a monotherapy, further studies are required to explore outcomes in a less drug-resistant population, where a larger proportion of patients might benefit from monotherapy. Despite challenges in trial design, many anti-seizure drugs have demonstrated efficacy as monotherapies [5].In the U.S.A., several anti-seizure drugs have had their original indications expanded to include use in monotherapy settings, including lacosamide, lamotrigine extended release, and topiramate.Furthermore, the arguments of a white paper has recently advocated a unified indication for anti-seizure drugs, irrespective of concomitant anti-seizure drug use [3], and as a consequence, the Food and Drug Administration (FDA) has determined that it is acceptable to extrapolate data from adjunctive trials to the monotherapy setting.
Archaeological research has highlighted the importance of population movement and interaction in promoting cultural change and interaction in past societies. Strontium isotope ratios (87Sr/86Sr) are used worldwide to track prehistoric human population movement, and recent studies have provided new insight into the role of population diversity in the pre-Columbian American midcontinent. To track such movement, we have analyzed enamel from 222 small nonmigratory terrestrial and semiaquatic fauna from a series of midcontinental geographic locations to provide initial baseline regional 87Sr/86Sr information. Results of this study reveal considerable overlap in the strontium isotope ranges within the midcontinent, but also identify important isotopic differences between regions. We conclude that sufficient Sr variation exists within the midcontinent to identify the movement of individuals, however, the lack of regional specificity in Sr currently limits our ability to identify specific place(s) of origin for these individuals using Sr alone. Continued isotopic research offers the potential to produce a more detailed midcontinental isoscape, which combined with other geochemical, biological, and archaeological data, allows us to refine our understanding of the movement of people in pre-Columbian America. In light of this new information, we revisited our earlier case study of Cahokian immigration, reassessing new samples (558 teeth representing 338 individuals), and confirmed that the Cahokian population included a large number of nonlocal residents. Interpretations that employ population movement as a factor in cultural change have a long and uneven history in archaeology from a common use as an explanation of cultural transitions and distribution (via diffusion or migration) in the nineteenth century to their rapid abandonment in the early twentieth century (Adams et al., 1978:525–526).This decline became especially steep with the ascendancy of processual archaeology and its widespread application of environmental and neoevolutionary models of change (Chapman and Hamerow, 1997).Only recently has population movement become academically acceptable as an explanative for cultural transitions.In the United States, for example, consideration of migration has become common in the Southwest (e.g., Clark, 2001:1–2).Despite this resurgence, the antimigration mantra continues among postmodernists.For example, Storey and Jones (2011:19) cite Peter Ucko's (1995:12) statement that culture history is “at its most insidious (when coupled with theories of migration) [that have] deprive[d] whole peoples of any legitimate past.”In this strange twist, recognizing migration, in the eyes of some proponents, becomes potentially demeaning to modern descendants' self-identities (citing Arnold, 2007).Consequently, for both social and political reasons, interpretive models postulating migration remain underevaluated.
Archaeological research has highlighted the importance of population movement and interaction in promoting cultural change and interaction in past societies. Strontium isotope ratios (87Sr/86Sr) are used worldwide to track prehistoric human population movement, and recent studies have provided new insight into the role of population diversity in the pre-Columbian American midcontinent. To track such movement, we have analyzed enamel from 222 small nonmigratory terrestrial and semiaquatic fauna from a series of midcontinental geographic locations to provide initial baseline regional 87Sr/86Sr information. Results of this study reveal considerable overlap in the strontium isotope ranges within the midcontinent, but also identify important isotopic differences between regions. We conclude that sufficient Sr variation exists within the midcontinent to identify the movement of individuals, however, the lack of regional specificity in Sr currently limits our ability to identify specific place(s) of origin for these individuals using Sr alone. Continued isotopic research offers the potential to produce a more detailed midcontinental isoscape, which combined with other geochemical, biological, and archaeological data, allows us to refine our understanding of the movement of people in pre-Columbian America. In light of this new information, we revisited our earlier case study of Cahokian immigration, reassessing new samples (558 teeth representing 338 individuals), and confirmed that the Cahokian population included a large number of nonlocal residents. However, the advent of new bioarchaeological techniques allows researchers to bypass such debates with archaeometric measures such as aDNA and stable isotope analyses to identify past human relationships and spatial movement (Bentley, 2006; Orlando et al., 2015).Numerous studies (see references cited in Section 2) have demonstrated that processual models of ancient monotypic ethnic populations geographically anchored are generally simplistic and that many ancient populations have significant genetic diversity as the result of mingling and interbreeding with surrounding groups, often generated by population movements across a fluid landscape, that is, by migrations.
This paper presents the microstratigraphic analysis of the protohistoric site El Calvari del Molar, dated to the 8th century and the first quarter of 6th century BCE. It focuses specifically on Room 8, located in the northern part. The sedimentary record covers different stratigraphic units, including several floors with their layers of preparation and domestic structures. The study of these floors, mainly through micromorphology, allows us to determine the composition, the processing and the technical treatment as well as the possible origin of the lithological materials used in their manufacture. We highlight that the floors are composed of earth construction, made of local carbonated materials, mainly clayey fine sands. Mixing this material with water would cause a precipitation of CaCO3 favoured by the impermeability of their preparation layers, mostly silty clay aggregates, leading to the semi-cementation of the floor, which appears to have been the aim of the builders of El Calvari. This type of earth floor brings together a series of characteristics at the construction level such as cohesion among particles and tenacity, similar features to lime mortars. In short, we have the opinion that the micromorphological analysis is an essential tool for the study and interpretation of domestic architectural elements. In this case, it has allowed us to identify and characterize the techniques and construction strategies of this type of floor by these first protohistoric builders of the NE Iberia. The geoarchaeological study of occupation surfaces and the search for floors at archaeological sites is becoming an increasingly common practice, especially in settlements, and there are different methodological approaches to help improve their interpretation.One of them is micromorphology, of which examples from as early as the 1990s provide us with detailed information on site formation processes (e.g. how floors were made and used).In the different handbooks and syntheses of Geoarchaeology and Soil Micromorphology we find sections referring to building materials, in which reference is made to the floor and usage levels of the sites or to certain indications of floors, like Goldberg and Macphail (2006), Macphail and Goldberg (2010, 2018), Milek (2014), Friesem et al. (2017) or Rentzel et al. (2017).
This paper presents the microstratigraphic analysis of the protohistoric site El Calvari del Molar, dated to the 8th century and the first quarter of 6th century BCE. It focuses specifically on Room 8, located in the northern part. The sedimentary record covers different stratigraphic units, including several floors with their layers of preparation and domestic structures. The study of these floors, mainly through micromorphology, allows us to determine the composition, the processing and the technical treatment as well as the possible origin of the lithological materials used in their manufacture. We highlight that the floors are composed of earth construction, made of local carbonated materials, mainly clayey fine sands. Mixing this material with water would cause a precipitation of CaCO3 favoured by the impermeability of their preparation layers, mostly silty clay aggregates, leading to the semi-cementation of the floor, which appears to have been the aim of the builders of El Calvari. This type of earth floor brings together a series of characteristics at the construction level such as cohesion among particles and tenacity, similar features to lime mortars. In short, we have the opinion that the micromorphological analysis is an essential tool for the study and interpretation of domestic architectural elements. In this case, it has allowed us to identify and characterize the techniques and construction strategies of this type of floor by these first protohistoric builders of the NE Iberia. Courty et al. (1989) publicized the technique of applying micromorphology to the study of the architectural elements of a site made of earth-based materials, such as floors and occupation surfaces.Along the same lines, there are microstratigraphic studies related to the analysis of the different techniques and construction procedures with earth-based materials, especially in floors and occupational deposits: e.g. Khirokitia (Cyprus) (Hourani, 2003), different sites of the later Neolithic in southern France (Wattez, 2003, 2009), the Bronze Age site of Mitrou (Greece) (Karkanas and Van de Moortel, 2014), Lattes (France) (Cammas, 1994, 1999, 2003) or Tell es-Safi/Gath (Israel) (Namdar et al., 2011).Also worthy of mention are several contributions that applied micromorphology to the study of occupation surfaces, floors and the use of soil at different sites (anthropic activities carried out on them) (Gé et al., 1993; Matthews, 1995, 2012; Matthews et al., 1994, 1997, 2006; Macphail et al., 1997; Shahack-Gross et al., 2005).There have also been noteworthy ethnoarchaeological and experimental studies carried out on the main floor formation processes and the use of these (e.g. Milek, 2012 or Banerjea et al., 2015).Within this framework we also find projects aimed at identifying and determining the use of plaster to make the floors, as in the case of the studies by Macphail and Crowter (2007), Karkanas (2007), Karkanas and Efstratiou (2009), Regev et al. (2010), Mentzer and Quade (2013) or Stoops et al. (2017).
This paper presents the microstratigraphic analysis of the protohistoric site El Calvari del Molar, dated to the 8th century and the first quarter of 6th century BCE. It focuses specifically on Room 8, located in the northern part. The sedimentary record covers different stratigraphic units, including several floors with their layers of preparation and domestic structures. The study of these floors, mainly through micromorphology, allows us to determine the composition, the processing and the technical treatment as well as the possible origin of the lithological materials used in their manufacture. We highlight that the floors are composed of earth construction, made of local carbonated materials, mainly clayey fine sands. Mixing this material with water would cause a precipitation of CaCO3 favoured by the impermeability of their preparation layers, mostly silty clay aggregates, leading to the semi-cementation of the floor, which appears to have been the aim of the builders of El Calvari. This type of earth floor brings together a series of characteristics at the construction level such as cohesion among particles and tenacity, similar features to lime mortars. In short, we have the opinion that the micromorphological analysis is an essential tool for the study and interpretation of domestic architectural elements. In this case, it has allowed us to identify and characterize the techniques and construction strategies of this type of floor by these first protohistoric builders of the NE Iberia. At this point we want to specify that, according to Gé et al. (1993) and Macphail and Goldberg (2010 and 2018), we will use the term “floor” to refer to soil made with earthen material (the constructed feature), and the term “occupation surface” to refer to the living surface developed on unconstructed or constructed floor that results from the use of the floor.
This paper presents the microstratigraphic analysis of the protohistoric site El Calvari del Molar, dated to the 8th century and the first quarter of 6th century BCE. It focuses specifically on Room 8, located in the northern part. The sedimentary record covers different stratigraphic units, including several floors with their layers of preparation and domestic structures. The study of these floors, mainly through micromorphology, allows us to determine the composition, the processing and the technical treatment as well as the possible origin of the lithological materials used in their manufacture. We highlight that the floors are composed of earth construction, made of local carbonated materials, mainly clayey fine sands. Mixing this material with water would cause a precipitation of CaCO3 favoured by the impermeability of their preparation layers, mostly silty clay aggregates, leading to the semi-cementation of the floor, which appears to have been the aim of the builders of El Calvari. This type of earth floor brings together a series of characteristics at the construction level such as cohesion among particles and tenacity, similar features to lime mortars. In short, we have the opinion that the micromorphological analysis is an essential tool for the study and interpretation of domestic architectural elements. In this case, it has allowed us to identify and characterize the techniques and construction strategies of this type of floor by these first protohistoric builders of the NE Iberia. Studies of floors and occupation surfaces in protohistoric sites in the Iberian Peninsula, and specifically in the northeast, are very rare and unevenly distributed.In general, the few studies carried out have been developed in field surveys (macro).These studies usually only revealed the use of earth, commonly stacked, as an occupation surfaces (Belarte, 2002; Mateu, 2016).Another type of floor has been documented only in some sites, such as in San Cristóbal (Mazaleón, Teruel) where a floor made with adobe (Fatás and Catalán, 2005) has been located.
The quality of meat products is traditionally assessed by chemical or sensorial analysis, which are time consuming, need specialized technicians and destroy the products. The development of new technologies to monitor meat pieces using non-destructive methods in order to establish their quality is earning importance in the last years. An increasing number of studies have been carried out on meat pieces combining Magnetic Resonance Imaging (MRI), texture descriptors and regression techniques to predict several physico-chemical or sensorial attributes of the meat, mainly different types of pig ham and loins. In spite of the importance of the problem, the conclusions of these works are still preliminary because they only use the most classical texture descriptors and regressors instead of stronger methods, and because the methodology used to measure the performance is optimistic. In this work, we test a wide range of texture analysis techniques and regression methods using a realistic methodology to predict several physico-chemical and sensorial attributes of different meat pieces of Iberian pigs. The texture descriptors include statistical techniques, like Haralick descriptors, local binary patterns, fractal features and frequential descriptors, like Gabor or wavelet features. The regression techniques include linear regressors, neural networks, deep learning, support vector machines, regression trees, ensembles, boosting machines and random forests, among others. We developed experiments using 15 texture feature vectors, 28 regressors over 4 datasets of Iberian pig meat pieces to predict 39 physico-chemical and sensorial attributes, summarizing16,380 experiments. There is not any combination of texture vector and regressor which provides the best result for all attributes tested. Nevertheless, all these experiments provided the following conclusions: (1) the regressor performance, measured using the squared correlation (R2), is from good to excellent (above 0.5625) for 29 out of 39 attributes tested; (2) the WAPE (Weighted Absolute Percent Error) is lower than 2% for 32 out of 37 attributes; (3) the dispersion in computer predictions around the true attributes is lower or similar than the dispersion in the labeling expert’s for the majority of attributes (85%); and (4) differences between predicted and true values are not statistically significant for 29 out of 37 attributes using the Wilcoxon ranksum statistical test. We can conclude that these results provide a high reliability for an automatic system to predict the quality of meat pieces, which may operate on-line in the meat industries in the future. Hams and loins from Iberian pig, which is an autochthonous porcine breed developed traditionally in the SouthWest of Spain, are one of the most valuable meat products in this country.This is mainly ascribed to their exceptional sensorial attributes that depend on characteristics of raw material and processing conditions.Thus, not only characteristics of fresh pieces but also their modifications during the processing are important parameters to control the technological process of dry-cured hams and loins (Pérez-Palacios et al., 2011b).Temperature and relative humidity conditions during the processing lead to dehydration and, hence, to weight loss and a water activity decrease.Meat from Iberian pigs should contain plenty of intramuscular fat, which is an important characteristic, due to its positive influence on quality parameters on the final product, such as marbling, juiciness, odor, and aroma (Ruiz et al., 2002).The determination of salt content is important from a microbiological point of view, but it also influences on the texture and flavor of the final product (Toldrá et al., 1997).Color is also one of the most interesting characteristics of meat products (Resurrección, 2003), and for dry cured meat products it is the most relevant appearance property (Gandemer, 2002).It is also important to study the final sensory quality of Iberian meat products, considering their most distinguished sensorial attributes, such as appearance, odor, taste and flavor (García and Carrapiso, 2001).Scientific studies on these meat products have carried out the sensory analysis objectively, with trained panellists and following standardized conditions.
The quality of meat products is traditionally assessed by chemical or sensorial analysis, which are time consuming, need specialized technicians and destroy the products. The development of new technologies to monitor meat pieces using non-destructive methods in order to establish their quality is earning importance in the last years. An increasing number of studies have been carried out on meat pieces combining Magnetic Resonance Imaging (MRI), texture descriptors and regression techniques to predict several physico-chemical or sensorial attributes of the meat, mainly different types of pig ham and loins. In spite of the importance of the problem, the conclusions of these works are still preliminary because they only use the most classical texture descriptors and regressors instead of stronger methods, and because the methodology used to measure the performance is optimistic. In this work, we test a wide range of texture analysis techniques and regression methods using a realistic methodology to predict several physico-chemical and sensorial attributes of different meat pieces of Iberian pigs. The texture descriptors include statistical techniques, like Haralick descriptors, local binary patterns, fractal features and frequential descriptors, like Gabor or wavelet features. The regression techniques include linear regressors, neural networks, deep learning, support vector machines, regression trees, ensembles, boosting machines and random forests, among others. We developed experiments using 15 texture feature vectors, 28 regressors over 4 datasets of Iberian pig meat pieces to predict 39 physico-chemical and sensorial attributes, summarizing16,380 experiments. There is not any combination of texture vector and regressor which provides the best result for all attributes tested. Nevertheless, all these experiments provided the following conclusions: (1) the regressor performance, measured using the squared correlation (R2), is from good to excellent (above 0.5625) for 29 out of 39 attributes tested; (2) the WAPE (Weighted Absolute Percent Error) is lower than 2% for 32 out of 37 attributes; (3) the dispersion in computer predictions around the true attributes is lower or similar than the dispersion in the labeling expert’s for the majority of attributes (85%); and (4) differences between predicted and true values are not statistically significant for 29 out of 37 attributes using the Wilcoxon ranksum statistical test. We can conclude that these results provide a high reliability for an automatic system to predict the quality of meat pieces, which may operate on-line in the meat industries in the future. Techniques usually carried out for determining these parameters related to the quality of meat products are laborious and time consuming, they require the destruction of the pieces and a trained staff panel in the case of the sensory analysis.In this sense, MRI and computer vision techniques have been proposed as an alternative, since they are non-destructive, non-invasive, non-intrusive, non-ionizing and innocuous.The majority of works, that used MRI to determine quality characteristics of dry-cured products, are focused on hams, allowing to monitor the ripening process of Iberian (Antequera et al., 2007), Parma (Fantazzini et al., 2009), and S. Daniele hams (Manzocco et al., 2013) and to study Iberian hams as a function of pig feeding background (Pérez-Palacios et al., 2010, 2011a).To our knowledge, only Cernadas et al. (2005) and Caballero et al. (2017a) analyzed loins by MRI, allowing: (1) an adequate product classification as a function of pig breeding; and (2) the prediction of sensory traits.
The quality of meat products is traditionally assessed by chemical or sensorial analysis, which are time consuming, need specialized technicians and destroy the products. The development of new technologies to monitor meat pieces using non-destructive methods in order to establish their quality is earning importance in the last years. An increasing number of studies have been carried out on meat pieces combining Magnetic Resonance Imaging (MRI), texture descriptors and regression techniques to predict several physico-chemical or sensorial attributes of the meat, mainly different types of pig ham and loins. In spite of the importance of the problem, the conclusions of these works are still preliminary because they only use the most classical texture descriptors and regressors instead of stronger methods, and because the methodology used to measure the performance is optimistic. In this work, we test a wide range of texture analysis techniques and regression methods using a realistic methodology to predict several physico-chemical and sensorial attributes of different meat pieces of Iberian pigs. The texture descriptors include statistical techniques, like Haralick descriptors, local binary patterns, fractal features and frequential descriptors, like Gabor or wavelet features. The regression techniques include linear regressors, neural networks, deep learning, support vector machines, regression trees, ensembles, boosting machines and random forests, among others. We developed experiments using 15 texture feature vectors, 28 regressors over 4 datasets of Iberian pig meat pieces to predict 39 physico-chemical and sensorial attributes, summarizing16,380 experiments. There is not any combination of texture vector and regressor which provides the best result for all attributes tested. Nevertheless, all these experiments provided the following conclusions: (1) the regressor performance, measured using the squared correlation (R2), is from good to excellent (above 0.5625) for 29 out of 39 attributes tested; (2) the WAPE (Weighted Absolute Percent Error) is lower than 2% for 32 out of 37 attributes; (3) the dispersion in computer predictions around the true attributes is lower or similar than the dispersion in the labeling expert’s for the majority of attributes (85%); and (4) differences between predicted and true values are not statistically significant for 29 out of 37 attributes using the Wilcoxon ranksum statistical test. We can conclude that these results provide a high reliability for an automatic system to predict the quality of meat pieces, which may operate on-line in the meat industries in the future. The general procedure in most of these studies takes three main steps: image acquisition, image analysis and data analysis.For image acquisition, high field MRI scanners have been used, which provide high quality images but they are expensive.A cheaper alternative are low field scanners, whose capability to acquire images of loin with predictive aims in food analysis, despite its low signal to noise ratio, has been demonstrated in recent works (Caballero et al., 2017a, b).
The quality of meat products is traditionally assessed by chemical or sensorial analysis, which are time consuming, need specialized technicians and destroy the products. The development of new technologies to monitor meat pieces using non-destructive methods in order to establish their quality is earning importance in the last years. An increasing number of studies have been carried out on meat pieces combining Magnetic Resonance Imaging (MRI), texture descriptors and regression techniques to predict several physico-chemical or sensorial attributes of the meat, mainly different types of pig ham and loins. In spite of the importance of the problem, the conclusions of these works are still preliminary because they only use the most classical texture descriptors and regressors instead of stronger methods, and because the methodology used to measure the performance is optimistic. In this work, we test a wide range of texture analysis techniques and regression methods using a realistic methodology to predict several physico-chemical and sensorial attributes of different meat pieces of Iberian pigs. The texture descriptors include statistical techniques, like Haralick descriptors, local binary patterns, fractal features and frequential descriptors, like Gabor or wavelet features. The regression techniques include linear regressors, neural networks, deep learning, support vector machines, regression trees, ensembles, boosting machines and random forests, among others. We developed experiments using 15 texture feature vectors, 28 regressors over 4 datasets of Iberian pig meat pieces to predict 39 physico-chemical and sensorial attributes, summarizing16,380 experiments. There is not any combination of texture vector and regressor which provides the best result for all attributes tested. Nevertheless, all these experiments provided the following conclusions: (1) the regressor performance, measured using the squared correlation (R2), is from good to excellent (above 0.5625) for 29 out of 39 attributes tested; (2) the WAPE (Weighted Absolute Percent Error) is lower than 2% for 32 out of 37 attributes; (3) the dispersion in computer predictions around the true attributes is lower or similar than the dispersion in the labeling expert’s for the majority of attributes (85%); and (4) differences between predicted and true values are not statistically significant for 29 out of 37 attributes using the Wilcoxon ranksum statistical test. We can conclude that these results provide a high reliability for an automatic system to predict the quality of meat pieces, which may operate on-line in the meat industries in the future. Regarding the image analysis, most works (Caballero et al., 2016, 2017a; Pérez-Palacios et al., 2017; Bajd et al., 2017) have only used classical second order statistics like gray-level cooccurrence, gray-level run-length and neighboring gray-level dependence descriptors to extract texture information from the MRI slices.In relation to data analysis usual statistical tools, such as Pearson correlation coefficients or principal components analysis, have been used to process data from MRI and to determine quality parameters of products (Fantazzini et al., 2009; Antequera et al., 2007; Manzocco et al., 2013).Pérez-Palacios et al. (2014), Caballero et al. (2016) and Caballero et al. (2017a) tested common regression techniques such as linear and isotonic regression to predict quality characteristics.However, these studies evaluated the regression methods following the well-known cross-validation methodology (Kohavi, 1995), including slices belonging to the same meat piece both in the training and testing sets.This methodology may provide optimistic results because the training set includes data from the same meat piece whose quality characteristics must be predicted.
The quality of meat products is traditionally assessed by chemical or sensorial analysis, which are time consuming, need specialized technicians and destroy the products. The development of new technologies to monitor meat pieces using non-destructive methods in order to establish their quality is earning importance in the last years. An increasing number of studies have been carried out on meat pieces combining Magnetic Resonance Imaging (MRI), texture descriptors and regression techniques to predict several physico-chemical or sensorial attributes of the meat, mainly different types of pig ham and loins. In spite of the importance of the problem, the conclusions of these works are still preliminary because they only use the most classical texture descriptors and regressors instead of stronger methods, and because the methodology used to measure the performance is optimistic. In this work, we test a wide range of texture analysis techniques and regression methods using a realistic methodology to predict several physico-chemical and sensorial attributes of different meat pieces of Iberian pigs. The texture descriptors include statistical techniques, like Haralick descriptors, local binary patterns, fractal features and frequential descriptors, like Gabor or wavelet features. The regression techniques include linear regressors, neural networks, deep learning, support vector machines, regression trees, ensembles, boosting machines and random forests, among others. We developed experiments using 15 texture feature vectors, 28 regressors over 4 datasets of Iberian pig meat pieces to predict 39 physico-chemical and sensorial attributes, summarizing16,380 experiments. There is not any combination of texture vector and regressor which provides the best result for all attributes tested. Nevertheless, all these experiments provided the following conclusions: (1) the regressor performance, measured using the squared correlation (R2), is from good to excellent (above 0.5625) for 29 out of 39 attributes tested; (2) the WAPE (Weighted Absolute Percent Error) is lower than 2% for 32 out of 37 attributes; (3) the dispersion in computer predictions around the true attributes is lower or similar than the dispersion in the labeling expert’s for the majority of attributes (85%); and (4) differences between predicted and true values are not statistically significant for 29 out of 37 attributes using the Wilcoxon ranksum statistical test. We can conclude that these results provide a high reliability for an automatic system to predict the quality of meat pieces, which may operate on-line in the meat industries in the future. Taking in mind these considerations, the current work is aimed to test: (i) a wide variety of texture extraction techniques to analyze MRI; (ii) a large amount of regression methods using a realistic evaluation methodology, with the final purpose of predicting different physico-chemical characteristics and sensorial attributes of meat products with high accuracy in a non-destructive way; and (iii) a study focused on minimizing the needed images for a good prediction in order to improve the acquisition time.
Locating the subtle and uneven deposition of human activities across the landscape continues to challenge archaeologists. Existing tools (e.g. excavation, shovel testing, pedestrian survey, and terrestrial geophysics) have proven effective at locating many types of archaeological features but remain time-consuming and difficult to undertake on densely vegetated or topographically complex terrain. As a result of these limitations, key aspects of past communities remain largely outside of archaeological detection and interpretation. This flattening of past lifeways not only affects broader understandings of these communities, but can also negatively impact the preservation of archaeological sites. This paper presents the detection of archaeological features through an analysis of drone-acquired thermal, multispectral, and visible light imagery, alongside historical aerial photography, in the area surrounding Middle Grant Creek (11WI2739), a late prehistoric village located at Midewin National Tallgrass Prairie in Will County, IL. Our investigations discovered a probable housing area and a ritual enclosure, increasing the area of the site from 3.4 ha to 20 ha. The proposed housing and ritual areas of this village also help contextualize finds from the ongoing archaeological excavations at Middle Grant Creek. More broadly, results demonstrate the valuable contributions that these relatively new archaeological survey methods have in shaping our understandings of the archaeological landscape and highlight the importance of integrating them into the archaeological toolkit. Archaeologists agree that narrowly defined sites, areas with high concentrations of artifacts, are small components of the complex and textured ways in which people make use of and interact with broader landscapes (e.g. Basso, 1996; Erickson, 2008; Gordillo, 2014; Ingold, 1993; Lelièvre, 2017; Yaeger and Canuto, 2000).However, documenting the varied traces of past human activities, which are often spread over large areas and produce little readily identifiable surface signatures, remains a perennial challenge.These challenges are heightened in locations where materials used for prehistoric construction (e.g. wood) easily degrade and where industry, housing, and agriculture have heavy impacts.Locating the often fragmented archaeological record in these contexts has previously required time intensive and expensive survey techniques, like pedestrian survey and shovel testing, that are poorly suited to documenting broadly dispersed archaeological landscape features.
The capacity allocation is a practically significant factor to influence the quality of the train timetables in the rail operations, especially under the fluctuation of passenger demands. This paper aims to investigate a detailed description for the structure and characteristics of the capacity allocation problem under random demand in the high-speed rail network. A two-stage stochastic integer programming model is provided to get the capacity allocation solutions to meet random fluctuations of passenger demands in the daily operations, which incorporates demand uncertainty and makes no assumptions for the rail network structure and distribution of passenger demands. Given the inherent complexity for solving this problem, we provide a solution framework including a heuristic algorithm based on tabu search in order to obtain a near-optimal solution and strategies to obtain an efficient timetable and train formation adjustment. Finally, two sets of examples, in which a sample rail network with 5 stations and the Beijing-Shanghai high-speed rail network data are adopted as the experimental environments to illustrate the performance and effectiveness of the proposed methods. With the rapidly increasing requirement for “high-speed and high-density” railway transportation, the railway industry has to pursue the intelligence of railway transportation system.Railway intelligent transportation system is a new generation system that is integrated by advanced intelligent technology, communication technology and information processing technology to fulfill the aims of guaranteeing safety, improving transportation efficiency, enhancing management and high service quality at lower cost (Li et al., 2003; Ning et al., 2006).High-speed railway is a mode of transport with large capacity and high safety, which is mainly used as inter-city routes with heavy passenger transport (Wong et al., 2002).Thus, effectively managing and operating the high-speed railways then becomes an important issue for the railway operators.
The capacity allocation is a practically significant factor to influence the quality of the train timetables in the rail operations, especially under the fluctuation of passenger demands. This paper aims to investigate a detailed description for the structure and characteristics of the capacity allocation problem under random demand in the high-speed rail network. A two-stage stochastic integer programming model is provided to get the capacity allocation solutions to meet random fluctuations of passenger demands in the daily operations, which incorporates demand uncertainty and makes no assumptions for the rail network structure and distribution of passenger demands. Given the inherent complexity for solving this problem, we provide a solution framework including a heuristic algorithm based on tabu search in order to obtain a near-optimal solution and strategies to obtain an efficient timetable and train formation adjustment. Finally, two sets of examples, in which a sample rail network with 5 stations and the Beijing-Shanghai high-speed rail network data are adopted as the experimental environments to illustrate the performance and effectiveness of the proposed methods. In the daily railway operations, train timetables and capacity allocation are essential to the enhancement of profitability, level of service and competitiveness in the market for the high-speed rail network operators.In practice, as a sub-problem of the train operating managements, train timetabling problem determines trains’ arrival and departure time at stations and yards, such that conflicts among trains will not occur and the transportation efficiency is improved.And capacity allocation problem aims to find an optimal passenger allocation of capacity subject to constraints like maximum capacity, flow balance and so on to meet each demand.As tactical plans of complex rail operations, trains timetables are programmed and updated every year or every season because of remarkable variation of passenger demands.In reality, if consider the deterministic passenger demands, it would tend to use resources too tightly and may produce big variations in performance when applied in real-world operations.In the worst situation, where demand fluctuates wildly during daily operations, the pre-specified timetable could be disturbed enough to lose its optimality.It is obviously that if trains do not run under full load, it may cause unnecessary waste of resources.So it is worth studying the capacity supply optimization problem under stochastic passenger fluctuation in operations.In other words, to provide flexible train timetables based on the optimal capacity allocation to meet the fluctuation of daily passenger demands is necessary for the high-speed rail network operators.Flexible train timetables can be implemented by autonomous decentralized train scheduling system in the integrated railway intelligent transportation system.Details for concept of autonomous decentralized system have been described previously (Miyamoto et al., 1984; Mori et al., 1986; Ihara and Mori, 1984).
The capacity allocation is a practically significant factor to influence the quality of the train timetables in the rail operations, especially under the fluctuation of passenger demands. This paper aims to investigate a detailed description for the structure and characteristics of the capacity allocation problem under random demand in the high-speed rail network. A two-stage stochastic integer programming model is provided to get the capacity allocation solutions to meet random fluctuations of passenger demands in the daily operations, which incorporates demand uncertainty and makes no assumptions for the rail network structure and distribution of passenger demands. Given the inherent complexity for solving this problem, we provide a solution framework including a heuristic algorithm based on tabu search in order to obtain a near-optimal solution and strategies to obtain an efficient timetable and train formation adjustment. Finally, two sets of examples, in which a sample rail network with 5 stations and the Beijing-Shanghai high-speed rail network data are adopted as the experimental environments to illustrate the performance and effectiveness of the proposed methods. In the real-world train timetabling problem, many factors have been considered including passenger demands, ticket prices, track capacity constraints, train capacity constraints, energy consumption, as well as maintenance and operating costs, which typically aims to construct a timetable that specifies a physical network route and detailed arrival and departure time for each train at passing stations.In addition, passenger demands vary randomly in actual train timetabling problem.Although determined train timetable under published timetable is easily implemented, but it may produce lower performance.Therefore, to get a robust train timetable, not only trains and related capacity supplies have to be considered, but also stochastic fluctuations of passenger demands in daily operations should be considered.It is of significance in practice, and has attracted tremendous attention from a variety of researchers over the past decades.Many studies have adopted different effective methods to analysis the uncertainty of passenger demands in optimizing train timetable problem.Under the fuzzy environment, the number of passengers getting on/off the train at each station was firstly treated as a fuzzy variable other than a crisp quantity (Yang et al., 2009).It is a common way to treat scare data as a fuzzy variable according to the experience or expert’s evaluation.However, compared with fuzzy variables, the method of stochastic programming is easier to deal with the influence of random parameters.Then based on a stochastic programming with recourse framework, the robust single-track train dispatching problem under stochastic segment running times and capacity loss duration (Meng and Zhou, 2011) were studied, which incorporated different probabilistic scenarios.They presented an example of adapting and extending known stochastic optimization frame work to meet the challenges of building realistic disruption handing tools under dynamic and uncertain data inputs.Both of the above studies focused on the single-line railway, rather than rail network.The research on the rail network is more necessary to accommodate the development of the modern railway.Using the same method, a major Taiwan inter-city bus routing and timetable setting problem were studied with the stochastic variations of daily passenger demands and established a stochastic-demand timetabling model (Yan et al., 2006).In order to improve the service quality and robustness against uncertain through minimizing the total waiting time of passengers, the robust multi-objective stochastic programming models (Hassannayebi et al., 2017) and a two-stage stochastic programming model (Shakibayifar et al., 2017) for train timetabling problem in the rail network were studied.Besides, control strategy with simulation-based method is also used to deal with dynamic uncertain passenger demands in optimizing the train timetabling problem (Feng et al., 2017).But too many assumptions should be considered and loses universality.Further, the passenger arrival flow was assumed to be dependent on a discrete Markovian process to investigate robust train regulation problem (Li et al., 2016) and presented the stochastic stability condition for the train traffic subject to control constraints.The Markovian processes are good models for many stochastic systems, including certain queuing systems, inventory systems and reliable systems.But it is not suitable for systematic medium- and long-term forecasting.In this paper, we not only propose a stochastic programming model to adapt the impact of random passenger demands, but also propose a strategy of optimizing timetable and train formation adjustment, which allows for quick optimization results.
The capacity allocation is a practically significant factor to influence the quality of the train timetables in the rail operations, especially under the fluctuation of passenger demands. This paper aims to investigate a detailed description for the structure and characteristics of the capacity allocation problem under random demand in the high-speed rail network. A two-stage stochastic integer programming model is provided to get the capacity allocation solutions to meet random fluctuations of passenger demands in the daily operations, which incorporates demand uncertainty and makes no assumptions for the rail network structure and distribution of passenger demands. Given the inherent complexity for solving this problem, we provide a solution framework including a heuristic algorithm based on tabu search in order to obtain a near-optimal solution and strategies to obtain an efficient timetable and train formation adjustment. Finally, two sets of examples, in which a sample rail network with 5 stations and the Beijing-Shanghai high-speed rail network data are adopted as the experimental environments to illustrate the performance and effectiveness of the proposed methods. Given the inherent complexity for solving this problem, many algorithms were presented to deal with the uncertain passenger demands and realize real-time train operations.The train routing and timetabling problem was formulated as a mixed integer programming model, and was solved using a fast heuristic algorithm based on the tabu search (Feng et al., 2018).To solve a mixed-integer linear programming model of network design problem, a memetic algorithm (Wang et al., 2018) was developed to obtain high quality solutions.Besides that, many meta-heuristics techniques were presented to provide useful ways of determining the initial solution, move rules, etc. (De et al., 2017a, b).They proposed a hybrid particle swarm optimization with a basic variable neighborhood search algorithm, which has been proved to be successful is solving a variety of combinatorial problems and dealing with large size problems.Besides that, many other superior meta-heuristics (particle optimization algorithm, genetic algorithm, etc.) are proposed to solve the problem in a large-scale realistic environment (De et al., 2019; Maiyar and Thakkar, 2018).A particle swarm optimization-composite particle was employed to calculate ship routing and scheduling problem (De et al., 2016).Further, a chemical reaction optimization algorithm was compared with the results with block-based genetic algorithm, genetic algorithm and particle swarm optimization and performed better than others (De et al., 2018).
The capacity allocation is a practically significant factor to influence the quality of the train timetables in the rail operations, especially under the fluctuation of passenger demands. This paper aims to investigate a detailed description for the structure and characteristics of the capacity allocation problem under random demand in the high-speed rail network. A two-stage stochastic integer programming model is provided to get the capacity allocation solutions to meet random fluctuations of passenger demands in the daily operations, which incorporates demand uncertainty and makes no assumptions for the rail network structure and distribution of passenger demands. Given the inherent complexity for solving this problem, we provide a solution framework including a heuristic algorithm based on tabu search in order to obtain a near-optimal solution and strategies to obtain an efficient timetable and train formation adjustment. Finally, two sets of examples, in which a sample rail network with 5 stations and the Beijing-Shanghai high-speed rail network data are adopted as the experimental environments to illustrate the performance and effectiveness of the proposed methods. To formulate a rail network capacity allocation scheme reasonably, the utilization rate of transport equipment capacity, the robustness and the efficiency of the capacity allocation process should be improved constantly on the basis of investigating the random variation of passenger flow of all lines in the rail network.When establishing the draft timetable, operators formulate the train stop plan and determine the type and number of trains according to the passenger demands, the transportation equipment configuration conditions, physical properties (e.g., track curvature and gradient, maximum allowed speed and so on) and interaction among trains.The draft timetable may be adjusted based on the actual operational situations.In the past decades, there have been few studies on the capacity allocation problem based on the fluctuating passenger demands.In order to deal a capacity allocation problem for the central headquarters that determines the capacity limits, the agent behavior and network flow models were incorporated (Demirag and Swann, 2007).Then developed a mixed integer program and some simple heuristic algorithm to solve the problem.By considering the formulation which maximizes the expected total profit over the possible decision under the uncertainty of demands, a heuristic algorithm for the capacity allocation problem with random demands in the rail container transportation was proposed (Cao et al., 2012).
The capacity allocation is a practically significant factor to influence the quality of the train timetables in the rail operations, especially under the fluctuation of passenger demands. This paper aims to investigate a detailed description for the structure and characteristics of the capacity allocation problem under random demand in the high-speed rail network. A two-stage stochastic integer programming model is provided to get the capacity allocation solutions to meet random fluctuations of passenger demands in the daily operations, which incorporates demand uncertainty and makes no assumptions for the rail network structure and distribution of passenger demands. Given the inherent complexity for solving this problem, we provide a solution framework including a heuristic algorithm based on tabu search in order to obtain a near-optimal solution and strategies to obtain an efficient timetable and train formation adjustment. Finally, two sets of examples, in which a sample rail network with 5 stations and the Beijing-Shanghai high-speed rail network data are adopted as the experimental environments to illustrate the performance and effectiveness of the proposed methods. Despite the breakthroughs of the above studies, to the best of the authors’ knowledge, there has been few research on high-speed passenger capacity allocation problem and train timetabling problem under stochastic demands.Specifically, this study intends to provide the following contributions to the frameworks of the capacity allocation problem based on the optimization methods:
The capacity allocation is a practically significant factor to influence the quality of the train timetables in the rail operations, especially under the fluctuation of passenger demands. This paper aims to investigate a detailed description for the structure and characteristics of the capacity allocation problem under random demand in the high-speed rail network. A two-stage stochastic integer programming model is provided to get the capacity allocation solutions to meet random fluctuations of passenger demands in the daily operations, which incorporates demand uncertainty and makes no assumptions for the rail network structure and distribution of passenger demands. Given the inherent complexity for solving this problem, we provide a solution framework including a heuristic algorithm based on tabu search in order to obtain a near-optimal solution and strategies to obtain an efficient timetable and train formation adjustment. Finally, two sets of examples, in which a sample rail network with 5 stations and the Beijing-Shanghai high-speed rail network data are adopted as the experimental environments to illustrate the performance and effectiveness of the proposed methods. (1) Using the theory of rail network and the train and passenger flow space–time network representation respectively, a detailed description for the structure and characteristics of train capacity allocation problem under stochastic demands in the high-speed rail network is presented.
The capacity allocation is a practically significant factor to influence the quality of the train timetables in the rail operations, especially under the fluctuation of passenger demands. This paper aims to investigate a detailed description for the structure and characteristics of the capacity allocation problem under random demand in the high-speed rail network. A two-stage stochastic integer programming model is provided to get the capacity allocation solutions to meet random fluctuations of passenger demands in the daily operations, which incorporates demand uncertainty and makes no assumptions for the rail network structure and distribution of passenger demands. Given the inherent complexity for solving this problem, we provide a solution framework including a heuristic algorithm based on tabu search in order to obtain a near-optimal solution and strategies to obtain an efficient timetable and train formation adjustment. Finally, two sets of examples, in which a sample rail network with 5 stations and the Beijing-Shanghai high-speed rail network data are adopted as the experimental environments to illustrate the performance and effectiveness of the proposed methods. (2) A two-stage stochastic integer programming model is provided to meet stochastic fluctuations of passenger demands in daily operations.The model incorporates demand uncertainty and makes no assumptions for the rail network structure and distribution of passenger demands.Then the model is rewritten as an integer programming model when assumes that there are finite number scenarios.
The capacity allocation is a practically significant factor to influence the quality of the train timetables in the rail operations, especially under the fluctuation of passenger demands. This paper aims to investigate a detailed description for the structure and characteristics of the capacity allocation problem under random demand in the high-speed rail network. A two-stage stochastic integer programming model is provided to get the capacity allocation solutions to meet random fluctuations of passenger demands in the daily operations, which incorporates demand uncertainty and makes no assumptions for the rail network structure and distribution of passenger demands. Given the inherent complexity for solving this problem, we provide a solution framework including a heuristic algorithm based on tabu search in order to obtain a near-optimal solution and strategies to obtain an efficient timetable and train formation adjustment. Finally, two sets of examples, in which a sample rail network with 5 stations and the Beijing-Shanghai high-speed rail network data are adopted as the experimental environments to illustrate the performance and effectiveness of the proposed methods. (3) Since the complexity of large-scale models make it difficult to use the integer programming solvers to search for the optimal capacity allocations within an acceptable computational time, this research especially designs an efficient heuristic algorithm based on tabu search to obtain approximate optimal solutions in a short time.Besides, some strategies are proposed to obtain an efficient timetable and train formation adjustment based on the near-optimal capacity allocations, whose objectives are uniform departure time of trains and minimizing the total travel time of all the involved trains.
The capacity allocation is a practically significant factor to influence the quality of the train timetables in the rail operations, especially under the fluctuation of passenger demands. This paper aims to investigate a detailed description for the structure and characteristics of the capacity allocation problem under random demand in the high-speed rail network. A two-stage stochastic integer programming model is provided to get the capacity allocation solutions to meet random fluctuations of passenger demands in the daily operations, which incorporates demand uncertainty and makes no assumptions for the rail network structure and distribution of passenger demands. Given the inherent complexity for solving this problem, we provide a solution framework including a heuristic algorithm based on tabu search in order to obtain a near-optimal solution and strategies to obtain an efficient timetable and train formation adjustment. Finally, two sets of examples, in which a sample rail network with 5 stations and the Beijing-Shanghai high-speed rail network data are adopted as the experimental environments to illustrate the performance and effectiveness of the proposed methods. (4) Two sets of numerical experiments, in which a five-station rail network and the Beijing-Shanghai high-speed railway in China are performed to illustrate the applications and to evaluate the performance of the proposed model and solution algorithm.The results demonstrate the correctness of the proposed models.Further, sensitivity analyses are implied to verify the practicality of the proposed methods.
Coastal shell middens are known for their generally excellent preservation and abundant identifiable faunal remains, including delicate fish and bird bones that are often rare or poorly preserved at non-shell midden sites. Thus, when we began our human ecodynamics research project focused on the fauna from Čḯxwicən (45CA523, pronounced ch-WHEET-son), a large ancestral village of the Lower Elwha Klallam Tribe, located on the shore of the Strait of Juan de Fuca, Port Angeles, Washington (USA), we anticipated generally high levels of bone identifiability. We quickly realized that the mammal bones were more fragmented and less identifiable than we had expected, though this was not the case with the bird and fish bone or invertebrate remains. To better understand why this fragmentation occurred at Čḯxwicən, we evaluate numerous hypotheses, including both post-depositional and behavioral explanations. We conclude that multiple factors intersected (to varying degrees) to produce the extreme bone fragmentation and low identifiability of mammal bones at the site, including bone fuel use, marrow extraction, grease rendering, tool production, and post-depositional breakdown. Using a human ecodynamics framework, we further consider how both social factors and external environmental forces may mediate human choices, such as the economic decision to use bone for fuel or render bone grease. We place our findings from Čḯxwicən in a regional context and discuss the potential of the approach for other coastal archaeological sites worldwide. Taphonomy is the study of the myriad behavioral, physical, and chemical factors that collectively influence deposition, survival, and recovery of bones and other animal parts from archaeological sites.Since the 1970s, archaeologists have recognized the importance of taphonomic analysis in isolating the factors that affect faunal remains across their ‘life history’ before drawing meaningful interpretations about past human activities (e.g., Behrensmeyer and Hill, 1980; Blumenschine, 1988; Brain, 1981; Ferraro et al., 2013; Gifford, 1981; Lyman, 1994; Orton, 2012).Taphonomic scholarship has two main goals: “to strip away the taphonomic overprint” (Lawrence, 1979:903; cited in Gifford, 1981) to obtain an accurate understanding of the original biological community or the systemic context (e.g., sensu Schiffer, 1987); and to understand the so-called overprint itself—with the idea that the overprint holds behavioral meaning in its own right (Gifford, 1981; Lyman, 1994).
Coastal shell middens are known for their generally excellent preservation and abundant identifiable faunal remains, including delicate fish and bird bones that are often rare or poorly preserved at non-shell midden sites. Thus, when we began our human ecodynamics research project focused on the fauna from Čḯxwicən (45CA523, pronounced ch-WHEET-son), a large ancestral village of the Lower Elwha Klallam Tribe, located on the shore of the Strait of Juan de Fuca, Port Angeles, Washington (USA), we anticipated generally high levels of bone identifiability. We quickly realized that the mammal bones were more fragmented and less identifiable than we had expected, though this was not the case with the bird and fish bone or invertebrate remains. To better understand why this fragmentation occurred at Čḯxwicən, we evaluate numerous hypotheses, including both post-depositional and behavioral explanations. We conclude that multiple factors intersected (to varying degrees) to produce the extreme bone fragmentation and low identifiability of mammal bones at the site, including bone fuel use, marrow extraction, grease rendering, tool production, and post-depositional breakdown. Using a human ecodynamics framework, we further consider how both social factors and external environmental forces may mediate human choices, such as the economic decision to use bone for fuel or render bone grease. We place our findings from Čḯxwicən in a regional context and discuss the potential of the approach for other coastal archaeological sites worldwide. The different ways scholars have approached bone fragmentation illustrates this important distinction.In some cases researchers have evaluated the extent to which differences in bone fragmentation may affect taxonomic or element identifiability (Grayson, 1991; Lyman and O'Brien, 1987; Nagaoka, 2005).In these cases, fragmentation is viewed as a bias structuring the assemblage, since it can misdirect our view of the target variable of interest—taxonomic or element representation.On the other hand, bone fragmentation itself has become a major focus of scholarship, as it can provide insights into carcass processing and broader questions of resource use.Scholars have studied bone fragmentation for its potential link to extraction of marrow, extraction of grease, and use as fuel (e.g., Binford, 1978; Church and Lyman, 2003; Ellis et al., 2011; Morin, 2010; Noe-Nygaard, 1977; Outram, 2003).Building on this theme, some researchers have sought to identify factors that promote bone processing, for example nutritional stress (Ellis et al., 2011; Outram, 2003).Nagaoka's (2005) study of moas, large flightless birds, at a southern New Zealand site illustrates both of these approaches.She evaluated changing bone fragmentation through time to see if it affected body part representation, and then drew on human behavioral ecology to account for changing patterns in processing moa bones for grease.She found that processing intensity increased through time, which she explains in the larger context of declining foraging efficiency.
Coastal shell middens are known for their generally excellent preservation and abundant identifiable faunal remains, including delicate fish and bird bones that are often rare or poorly preserved at non-shell midden sites. Thus, when we began our human ecodynamics research project focused on the fauna from Čḯxwicən (45CA523, pronounced ch-WHEET-son), a large ancestral village of the Lower Elwha Klallam Tribe, located on the shore of the Strait of Juan de Fuca, Port Angeles, Washington (USA), we anticipated generally high levels of bone identifiability. We quickly realized that the mammal bones were more fragmented and less identifiable than we had expected, though this was not the case with the bird and fish bone or invertebrate remains. To better understand why this fragmentation occurred at Čḯxwicən, we evaluate numerous hypotheses, including both post-depositional and behavioral explanations. We conclude that multiple factors intersected (to varying degrees) to produce the extreme bone fragmentation and low identifiability of mammal bones at the site, including bone fuel use, marrow extraction, grease rendering, tool production, and post-depositional breakdown. Using a human ecodynamics framework, we further consider how both social factors and external environmental forces may mediate human choices, such as the economic decision to use bone for fuel or render bone grease. We place our findings from Čḯxwicən in a regional context and discuss the potential of the approach for other coastal archaeological sites worldwide. The developing field of human ecodynamics offers a useful interdisciplinary framework for the study of the drivers behind patterning in bone fragmentation.Such research incorporates concepts from historical ecology, resilience theory, human behavioral ecology, and systems thinking—with the goal of building a long-term history of human-environmental interactions (Fitzhugh et al., this issue).Human ecodynamics scholarship takes the view that people are integral parts of the environment, not external actors.Human agency and historical contingency are incorporated into explanations of socio-ecological change.Faunal remains have figured prominently in human ecodynamics research, as a proxy for evaluating changes in the overall subsistence economy.Given that faunal remains are also a critical source of raw material across many human societies, for tools, fuel and grease, house construction and landform construction, we suggest that changing dynamics of environmental and social conditions would influence the ways human groups selected and processed animal carcasses through time.
Coastal shell middens are known for their generally excellent preservation and abundant identifiable faunal remains, including delicate fish and bird bones that are often rare or poorly preserved at non-shell midden sites. Thus, when we began our human ecodynamics research project focused on the fauna from Čḯxwicən (45CA523, pronounced ch-WHEET-son), a large ancestral village of the Lower Elwha Klallam Tribe, located on the shore of the Strait of Juan de Fuca, Port Angeles, Washington (USA), we anticipated generally high levels of bone identifiability. We quickly realized that the mammal bones were more fragmented and less identifiable than we had expected, though this was not the case with the bird and fish bone or invertebrate remains. To better understand why this fragmentation occurred at Čḯxwicən, we evaluate numerous hypotheses, including both post-depositional and behavioral explanations. We conclude that multiple factors intersected (to varying degrees) to produce the extreme bone fragmentation and low identifiability of mammal bones at the site, including bone fuel use, marrow extraction, grease rendering, tool production, and post-depositional breakdown. Using a human ecodynamics framework, we further consider how both social factors and external environmental forces may mediate human choices, such as the economic decision to use bone for fuel or render bone grease. We place our findings from Čḯxwicən in a regional context and discuss the potential of the approach for other coastal archaeological sites worldwide. The Čḯxwicən research project provides an important case study for applying human ecodynamics research to bone fragmentation.Čḯxwicən,1 located in northwest Washington State on the shore of the Strait of Juan de Fuca (Fig. 1), is a traditional village of the Lower Elwha Klallam Tribe, and is linked more broadly to Coast Salish groups of the Salish Sea.The site was excavated as part of a large-scale mitigation in 2004, with materials excavated from an area of over 500 m2, representing multiple houses and extramural middens (Butler et al., this issue a).Geoarchaeological study shows that both abrupt and gradual environmental processes (earthquakes, climate change, shoreline development) occurred over the ~2700 year period of site occupation (Campbell et al., this issue; Hutchinson et al., this issue).
Coastal shell middens are known for their generally excellent preservation and abundant identifiable faunal remains, including delicate fish and bird bones that are often rare or poorly preserved at non-shell midden sites. Thus, when we began our human ecodynamics research project focused on the fauna from Čḯxwicən (45CA523, pronounced ch-WHEET-son), a large ancestral village of the Lower Elwha Klallam Tribe, located on the shore of the Strait of Juan de Fuca, Port Angeles, Washington (USA), we anticipated generally high levels of bone identifiability. We quickly realized that the mammal bones were more fragmented and less identifiable than we had expected, though this was not the case with the bird and fish bone or invertebrate remains. To better understand why this fragmentation occurred at Čḯxwicən, we evaluate numerous hypotheses, including both post-depositional and behavioral explanations. We conclude that multiple factors intersected (to varying degrees) to produce the extreme bone fragmentation and low identifiability of mammal bones at the site, including bone fuel use, marrow extraction, grease rendering, tool production, and post-depositional breakdown. Using a human ecodynamics framework, we further consider how both social factors and external environmental forces may mediate human choices, such as the economic decision to use bone for fuel or render bone grease. We place our findings from Čḯxwicən in a regional context and discuss the potential of the approach for other coastal archaeological sites worldwide. Our research project, focusing on a large sample of the fauna representing the last ~2150 years of occupation, identified over one million specimens from at least 39 orders, 68 families and 81 genera of shellfish, fish, birds, and mammals (Butler et al., this issue b).A significant and unexpected finding was that the mammal bones are extremely fragmented, both in comparison to other vertebrate remains at the site, and to other sites in the region.Out of 16,520 specimens recovered in 1/4″ and larger mesh screens identified as mammal from all contexts, only 9% could be identified at least to order, while approximately 50% of both fish and bird were identified.The high degree of mammal fragmentation is distinctive in a regional context (Table 1, and see Section 2).
Coastal shell middens are known for their generally excellent preservation and abundant identifiable faunal remains, including delicate fish and bird bones that are often rare or poorly preserved at non-shell midden sites. Thus, when we began our human ecodynamics research project focused on the fauna from Čḯxwicən (45CA523, pronounced ch-WHEET-son), a large ancestral village of the Lower Elwha Klallam Tribe, located on the shore of the Strait of Juan de Fuca, Port Angeles, Washington (USA), we anticipated generally high levels of bone identifiability. We quickly realized that the mammal bones were more fragmented and less identifiable than we had expected, though this was not the case with the bird and fish bone or invertebrate remains. To better understand why this fragmentation occurred at Čḯxwicən, we evaluate numerous hypotheses, including both post-depositional and behavioral explanations. We conclude that multiple factors intersected (to varying degrees) to produce the extreme bone fragmentation and low identifiability of mammal bones at the site, including bone fuel use, marrow extraction, grease rendering, tool production, and post-depositional breakdown. Using a human ecodynamics framework, we further consider how both social factors and external environmental forces may mediate human choices, such as the economic decision to use bone for fuel or render bone grease. We place our findings from Čḯxwicən in a regional context and discuss the potential of the approach for other coastal archaeological sites worldwide. We initially viewed this high degree of fragmentation as a detraction, as it limited our ability to obtain specific taxonomic identifications, important to the larger project goal of studying the ways animals (and, in turn, people) were affected by a range of socio-ecological changes.On further reflection, we realized that taxon-specific patterns and temporal trends in fragmentation itself could provide insights on changing carcass use under differing socio-ecological conditions.The high level of precision in field sampling and deposit description, good chronological control (with 102 radiocarbon dates), and faunal samples obtained from multiple houses and extramural activity areas would allow us to begin to disentangle the many factors that challenge taphonomic analyses in general.In addition, we could use the human ecodynamics framework, which we were already using to examine changing animal use, to consider the range of factors behind fragmentation.
Online tracking is the key enabling technology of modern online advertising. In the recently established model of real-time bidding (RTB), the web pages tracked by ad platforms are shared with advertising agencies (also called DSPs), which, in an auction-based system, may bid for user ad impressions. Since tracking data are no longer confined to ad platforms, RTB poses serious risks to privacy, especially with regard to user profiling, a practice that can be conducted at a very low cost by any DSP or related agency, as we reveal here. In this work, we illustrate these privacy risks by examining a data set with the real ad-auctions of a DSP, and show that for at least 55% of the users tracked by this agency, it paid nothing for their browsing data. To mitigate this abuse, we propose a system that regulates the distribution of bid requests (containing user tracking data) to potentially interested bidders, depending on their previous behavior. In our approach, an ad platform restricts the sharing of tracking data by limiting the number of DSPs participating in each auction, thereby leaving unchanged the current RTB architecture and protocols. However, doing so may have an evident impact on the ad platform’s revenue. The proposed system is designed accordingly, to ensure the revenue is maximized while the abuse by DSPs is prevented to a large degree. Experimental results seem to suggest that our system is able to correct misbehaving DSPs, and consequently enhance user privacy. The growing access of people to information and communication technologies is contributing to reach the so-called “big data era”, where the pervasiveness of data is a major input for increasingly personalized and automated online services.One of such services is online advertising, which aims at selecting and directing ads to the right potential customers (personalization) at the right time (real-time), built on multiple parameters, while users browse the Web (Smith, 2014a; Real-time bidding protocol, 0000; Yuan et al., 2012).
Online tracking is the key enabling technology of modern online advertising. In the recently established model of real-time bidding (RTB), the web pages tracked by ad platforms are shared with advertising agencies (also called DSPs), which, in an auction-based system, may bid for user ad impressions. Since tracking data are no longer confined to ad platforms, RTB poses serious risks to privacy, especially with regard to user profiling, a practice that can be conducted at a very low cost by any DSP or related agency, as we reveal here. In this work, we illustrate these privacy risks by examining a data set with the real ad-auctions of a DSP, and show that for at least 55% of the users tracked by this agency, it paid nothing for their browsing data. To mitigate this abuse, we propose a system that regulates the distribution of bid requests (containing user tracking data) to potentially interested bidders, depending on their previous behavior. In our approach, an ad platform restricts the sharing of tracking data by limiting the number of DSPs participating in each auction, thereby leaving unchanged the current RTB architecture and protocols. However, doing so may have an evident impact on the ad platform’s revenue. The proposed system is designed accordingly, to ensure the revenue is maximized while the abuse by DSPs is prevented to a large degree. Experimental results seem to suggest that our system is able to correct misbehaving DSPs, and consequently enhance user privacy. This targeted advertising offers crucial benefits to several agents on the Internet.To start, users receive ads tailored to their interests and no longer static ads unrelated to their preferences; consequently, behavioral targeting ensures conversion rates1  that double those of untargeted ads (Beales, 2010).Furthermore, web sites have access to an entire ecosystem to fund their operation through the money paid by demand side platforms (DSPs), which are advertising agencies acting in representation of advertisers.2Also, selling entities are given the opportunity to promote their products over a ubiquitous structure with global reach.The upshot is that most of the content users consume online is supported by ad revenue.
Online tracking is the key enabling technology of modern online advertising. In the recently established model of real-time bidding (RTB), the web pages tracked by ad platforms are shared with advertising agencies (also called DSPs), which, in an auction-based system, may bid for user ad impressions. Since tracking data are no longer confined to ad platforms, RTB poses serious risks to privacy, especially with regard to user profiling, a practice that can be conducted at a very low cost by any DSP or related agency, as we reveal here. In this work, we illustrate these privacy risks by examining a data set with the real ad-auctions of a DSP, and show that for at least 55% of the users tracked by this agency, it paid nothing for their browsing data. To mitigate this abuse, we propose a system that regulates the distribution of bid requests (containing user tracking data) to potentially interested bidders, depending on their previous behavior. In our approach, an ad platform restricts the sharing of tracking data by limiting the number of DSPs participating in each auction, thereby leaving unchanged the current RTB architecture and protocols. However, doing so may have an evident impact on the ad platform’s revenue. The proposed system is designed accordingly, to ensure the revenue is maximized while the abuse by DSPs is prevented to a large degree. Experimental results seem to suggest that our system is able to correct misbehaving DSPs, and consequently enhance user privacy. One of the key enabling technologies that makes online advertising so profitable is real-time bidding (RTB), which enables advertisers to compete in real-time auctions to show their ads (Yuan et al., 2013).It is implemented by a management entity called ad exchange.Accordingly, when a user visits a website, her impression is sold to the advertiser (or corresponding DSP) that bids higher, in a matter of milliseconds.Moreover, DSPs are sent bid request messages containing user information (tracking data) to help them tailor ads to the user’s preferences and decide the bidding strategy.In this way, the RTB aim is twofold: offering users a personalized experience through targeted ads and, thus, maximizing the profits of the whole advertising ecosystem.Whereas the operation of RTB behind the scenes is pretty opaque and complex for users  (Mcdonald et al., 2009), it is quite transparent for the actors of the advertising ecosystem.For example, ad exchanges provide DSPs with powerful management interfaces that offer very detailed information about the market and even enable buyers to set up their advertising strategies (e.g., by defining a targeting market).Certainly, a lot of benefits arise for the advertising ecosystem from the optimization capability offered by RTB in terms of automation, personalization, profit and transparency.
Online tracking is the key enabling technology of modern online advertising. In the recently established model of real-time bidding (RTB), the web pages tracked by ad platforms are shared with advertising agencies (also called DSPs), which, in an auction-based system, may bid for user ad impressions. Since tracking data are no longer confined to ad platforms, RTB poses serious risks to privacy, especially with regard to user profiling, a practice that can be conducted at a very low cost by any DSP or related agency, as we reveal here. In this work, we illustrate these privacy risks by examining a data set with the real ad-auctions of a DSP, and show that for at least 55% of the users tracked by this agency, it paid nothing for their browsing data. To mitigate this abuse, we propose a system that regulates the distribution of bid requests (containing user tracking data) to potentially interested bidders, depending on their previous behavior. In our approach, an ad platform restricts the sharing of tracking data by limiting the number of DSPs participating in each auction, thereby leaving unchanged the current RTB architecture and protocols. However, doing so may have an evident impact on the ad platform’s revenue. The proposed system is designed accordingly, to ensure the revenue is maximized while the abuse by DSPs is prevented to a large degree. Experimental results seem to suggest that our system is able to correct misbehaving DSPs, and consequently enhance user privacy. Yet, despite its proven usefulness, the practices inherent to online advertising and RTB may pose serious privacy risks for users (Estrada-Jiménez et al., 2017).Most of these risks derive from the potential misuse of the user data flowing through the advertising ecosystem.To start, vast user data is mined at very high rates to implement real-time personalization; hence, truly detailed profiles are built about millions of people so fast and uncontrollably (Narayanan and Shmatikov, 2008) that privacy protection is discouraged.Additionally, ad distribution mechanisms based on auctioning user impressions might lead to characterize users as more relevant (or more valuable economically) than others, depending on their profiles (Olejnik et al., 2014); such a differentiation may entail social sorting or discrimination (Speicher et al., 2018), thus an even less private environment.Finally, online advertising builds on interactions among myriads of intermediary ad companies that collect, use and share user data, significantly increasing the risk of data misuse.Ironically, users have no control over how their data is managed in this context.
Online tracking is the key enabling technology of modern online advertising. In the recently established model of real-time bidding (RTB), the web pages tracked by ad platforms are shared with advertising agencies (also called DSPs), which, in an auction-based system, may bid for user ad impressions. Since tracking data are no longer confined to ad platforms, RTB poses serious risks to privacy, especially with regard to user profiling, a practice that can be conducted at a very low cost by any DSP or related agency, as we reveal here. In this work, we illustrate these privacy risks by examining a data set with the real ad-auctions of a DSP, and show that for at least 55% of the users tracked by this agency, it paid nothing for their browsing data. To mitigate this abuse, we propose a system that regulates the distribution of bid requests (containing user tracking data) to potentially interested bidders, depending on their previous behavior. In our approach, an ad platform restricts the sharing of tracking data by limiting the number of DSPs participating in each auction, thereby leaving unchanged the current RTB architecture and protocols. However, doing so may have an evident impact on the ad platform’s revenue. The proposed system is designed accordingly, to ensure the revenue is maximized while the abuse by DSPs is prevented to a large degree. Experimental results seem to suggest that our system is able to correct misbehaving DSPs, and consequently enhance user privacy. RTB builds on sharing user data with DSPs to encourage competition and ad personalization, but the unregulated distribution of such data may give rise to concerns.With the aim of helping DSPs decide whether to bid or not for a user impression, an ad exchange distributes to them personal information of the user whose impression is being auctioned (e.g., the URL being visited, the location of the user, or even a label categorizing the user).Thus, not only does the winner DSP receives this input, but also the rest of participating DSPs.This means that there could be agencies maliciously collecting data without even paying for it.We illustrate this risk in Section 3 where we unveil that a given DSP would have paid nothing for at least 55% of the users it tracked in a period of three months.This uncontrolled distribution of user data prompts a non-negligible privacy concern since an increasing number of advertising agencies are relying on RTB to daily reach billions of potential Web customers (Hoelzel, 2015).Although the distribution of personal data among a group of DSPs cannot be entirely stopped without changing the current advertising business model, we report that the potential abuse of these agencies can be tackled with minimum tuning of said data distribution model.
Online tracking is the key enabling technology of modern online advertising. In the recently established model of real-time bidding (RTB), the web pages tracked by ad platforms are shared with advertising agencies (also called DSPs), which, in an auction-based system, may bid for user ad impressions. Since tracking data are no longer confined to ad platforms, RTB poses serious risks to privacy, especially with regard to user profiling, a practice that can be conducted at a very low cost by any DSP or related agency, as we reveal here. In this work, we illustrate these privacy risks by examining a data set with the real ad-auctions of a DSP, and show that for at least 55% of the users tracked by this agency, it paid nothing for their browsing data. To mitigate this abuse, we propose a system that regulates the distribution of bid requests (containing user tracking data) to potentially interested bidders, depending on their previous behavior. In our approach, an ad platform restricts the sharing of tracking data by limiting the number of DSPs participating in each auction, thereby leaving unchanged the current RTB architecture and protocols. However, doing so may have an evident impact on the ad platform’s revenue. The proposed system is designed accordingly, to ensure the revenue is maximized while the abuse by DSPs is prevented to a large degree. Experimental results seem to suggest that our system is able to correct misbehaving DSPs, and consequently enhance user privacy. Our proposal builds on regulating the distribution of personal data from the ad exchange to DSPs when a user impression is auctioned.Such regulation consists essentially in limiting the number of DSPs invited to bid, that is lowering the entities to which user data is leaked and, consequently, getting a more private environment.Accordingly, DSPs or similar intermediaries showing a dishonest behavior (e.g., never winning auctions) will be banned from participating in future auctions, which may entail correcting such harmful behaviors.At the same time, our approach strives to maximize the revenue of the ad exchange, looking for a balance with a given privacy level.The upshot is that some privacy can be reached without affecting the business model of the online advertising ecosystem, by slightly modifying the distribution of personal data among intermediary entities such as DSPs.The resulting adjusting effect on the behavior of these entities is relevant since privacy concerns in general do not directly derive from the act of sharing data itself, but from the inappropriate sharing of user information (Nissenbaum, 2009).
Minimum Time Search (MTS) algorithms help in search missions proposing search trajectories that minimize the target detection time considering the available information about the search scenario. This work proposes a MTS planner based on ant colony optimization that includes communication and collision avoidance constraints. This ensures that the Unmanned Aerial Vehicles (UAVs) are able to complete the optimized search trajectories without risk of collision or loss of communication with the ground control station. This approach is a great advantage nowadays, where UAVs flight regulation is quite strict, often requiring to monitor the state of the UAVs during the whole mission, impeding UAV deployments without continuous communication to the ground control station. The proposed algorithm is tested with several search scenarios and compared against two state of the art techniques based on Cross Entropy Optimization and Genetic Algorithms, which have been adapted to make them consider collision and communication constraints as well. Nowadays, there is a strong research interest in UAVs mission planning due to their advantages in several applications, such as mission planning (Atencia et al., 2018), target tracking (Pulford, 2005), target monitoring (de Moraes and de Freitas, 2019; Khan et al., 2017) or fire fighting (Bilbao et al., 2015).This work focuses on search under uncertainty or Probabilistic Search (PS), more concretely on Minimum Time Search (MTS) algorithms which propose search trajectories that minimize the target detection time.MTS has several applications that vary from searching for survivors after natural disasters (e.g. a fire or an earthquake) to look for military targets.An example is shown in Fig. 1, where several UAVs search for a life boat while the mission is being monitored from the base station located on a ship.MTS algorithms exploit the available information about the target location, which is typically modeled with a probability map that states the prior target presence probability distribution.Fig. 1 depicts a search mission where the prior knowledge of the target (life boat) position is represented by a discretized probability map.In this map, cells with warmer colors indicates higher probability of target presence.In order to reduce the target detection time, cells with higher probability of target presence should be explored as soon as possible by the UAVs.
Minimum Time Search (MTS) algorithms help in search missions proposing search trajectories that minimize the target detection time considering the available information about the search scenario. This work proposes a MTS planner based on ant colony optimization that includes communication and collision avoidance constraints. This ensures that the Unmanned Aerial Vehicles (UAVs) are able to complete the optimized search trajectories without risk of collision or loss of communication with the ground control station. This approach is a great advantage nowadays, where UAVs flight regulation is quite strict, often requiring to monitor the state of the UAVs during the whole mission, impeding UAV deployments without continuous communication to the ground control station. The proposed algorithm is tested with several search scenarios and compared against two state of the art techniques based on Cross Entropy Optimization and Genetic Algorithms, which have been adapted to make them consider collision and communication constraints as well. Probabilistic search theory has its origins in the research done by the USA Anti-Submarine Warfare Operations Research Group (ASWORG) during World War II, whose results were later summarized in Koopman’s report “Search and Screening” Koopman (1946).In probabilistic search, the uncertainty associated to the problem is tackled from a probabilistic approach, considering probabilistic models of the target position (probability map) and sensor performance (sensor likelihood model).Besides, some works like (Lanillos et al., 2012; Chang-jian et al., 2015; Pérez-Carabaza et al., 2018) or this work include the possibility of incorporating information about the possible target movements (target dynamic model).
Minimum Time Search (MTS) algorithms help in search missions proposing search trajectories that minimize the target detection time considering the available information about the search scenario. This work proposes a MTS planner based on ant colony optimization that includes communication and collision avoidance constraints. This ensures that the Unmanned Aerial Vehicles (UAVs) are able to complete the optimized search trajectories without risk of collision or loss of communication with the ground control station. This approach is a great advantage nowadays, where UAVs flight regulation is quite strict, often requiring to monitor the state of the UAVs during the whole mission, impeding UAV deployments without continuous communication to the ground control station. The proposed algorithm is tested with several search scenarios and compared against two state of the art techniques based on Cross Entropy Optimization and Genetic Algorithms, which have been adapted to make them consider collision and communication constraints as well. Due its NP-hard computational complexity (Trummel and Weisinger, 1986), this problem is usually tackled with a wide range of methods such as greedy search (Carpin et al., 2013), gradient based methods (Lanillos et al., 2014), genetic algorithms (Lin and Goodrich, 2009), cross entropy optimization (Lanillos et al., 2012) or ant colony optimization (Pérez-Carabaza et al., 2017).In this work, the ant colony based algorithm Max–Min Ant System (MMAS) by Stützle and H. Hoos (2000) is used due to its ability to handle the unconstrained MTS problem (Pérez-Carabaza et al., 2018) and to incorporate specific knowledge of the problem through the ant colony heuristic information mechanism.
Minimum Time Search (MTS) algorithms help in search missions proposing search trajectories that minimize the target detection time considering the available information about the search scenario. This work proposes a MTS planner based on ant colony optimization that includes communication and collision avoidance constraints. This ensures that the Unmanned Aerial Vehicles (UAVs) are able to complete the optimized search trajectories without risk of collision or loss of communication with the ground control station. This approach is a great advantage nowadays, where UAVs flight regulation is quite strict, often requiring to monitor the state of the UAVs during the whole mission, impeding UAV deployments without continuous communication to the ground control station. The proposed algorithm is tested with several search scenarios and compared against two state of the art techniques based on Cross Entropy Optimization and Genetic Algorithms, which have been adapted to make them consider collision and communication constraints as well. Many PS works focus on the optimization of a probabilistic search criterion (e.g. probability of detection or entropy) and only a few take into account additional criteria that must be considered to make the results obtained by the algorithm applicable to real-world PS missions.However, the problem complexity and the strict regulations for UAV flights, which often require that the UAV’s states are continuously monitored during the mission, make the use of many known search algorithms to real missions difficult (Carpin et al., 2013).For this reason, the proposed MTS algorithm, ensures continuous communications between the Ground Control Station (GCS) and the UAVs, as well as collision-free trajectories by including a constraint handling technique.
Minimum Time Search (MTS) algorithms help in search missions proposing search trajectories that minimize the target detection time considering the available information about the search scenario. This work proposes a MTS planner based on ant colony optimization that includes communication and collision avoidance constraints. This ensures that the Unmanned Aerial Vehicles (UAVs) are able to complete the optimized search trajectories without risk of collision or loss of communication with the ground control station. This approach is a great advantage nowadays, where UAVs flight regulation is quite strict, often requiring to monitor the state of the UAVs during the whole mission, impeding UAV deployments without continuous communication to the ground control station. The proposed algorithm is tested with several search scenarios and compared against two state of the art techniques based on Cross Entropy Optimization and Genetic Algorithms, which have been adapted to make them consider collision and communication constraints as well. In order to obtain search trajectories without collisions and communication loss, three different penalty-based methods are tested, which allow to transform the constrained problem into an unconstrained one that can be solved with MMAS.Furthermore, the proposed MTS algorithm benefits from the inclusion of three heuristics, especially designed for MTS, communication maintenance and collision avoidance, that allow the algorithm to converge faster to high quality solutions.
Minimum Time Search (MTS) algorithms help in search missions proposing search trajectories that minimize the target detection time considering the available information about the search scenario. This work proposes a MTS planner based on ant colony optimization that includes communication and collision avoidance constraints. This ensures that the Unmanned Aerial Vehicles (UAVs) are able to complete the optimized search trajectories without risk of collision or loss of communication with the ground control station. This approach is a great advantage nowadays, where UAVs flight regulation is quite strict, often requiring to monitor the state of the UAVs during the whole mission, impeding UAV deployments without continuous communication to the ground control station. The proposed algorithm is tested with several search scenarios and compared against two state of the art techniques based on Cross Entropy Optimization and Genetic Algorithms, which have been adapted to make them consider collision and communication constraints as well. In short, the main contributions of this work are:
The consequences of natural or man-made catastrophes can be devastating. To minimize its impact, it is crucial to carry out a rapid analysis of the affected environment in the moments after they occur, especially from the perspective of alert notification or crisis management. In this context, the use of UAVs, understood as the technological basis on which intelligent systems capable of providing support to rescue teams is built, has positively contributed to face this challenge. In this article the design of a multi-agent architecture which enables the deployment of systems made up of intelligent agents that can monitor environments affected by a catastrophe and provide support to human staff in the decision-making process is proposed. These environments, known in advance, are characterized through a set of points of interests that are critical from the point of view of aerial surveillance and monitoring. To conduct an intelligent information analysis, a formal model of normality analysis is employed, which makes possible the definition of surveillance components. These represent the knowledge bases of the agents responsible for monitoring environments. Likewise, the architecture envisages communication and cooperation mechanisms between the different agents, as the basis for fusing information to assess the overall level of risk of the monitored environment. A case study is presented in which the spread of toxic smoke in an industrial complex which has just suffered a hypothetical earthquake is monitored. The capacity to react in the moments immediately after a catastrophe, whether this is natural, such as an earthquake, or man-made, such as that caused by a nuclear reactor meltdown, is critical for minimizing the caused human and material damages.Unfortunately, in situations as extreme as these ones, it is highly complicated to tackle the chaos that usually arises.On occasions, this is due to the lack of information in such moments.Taking an earthquake that has affected an urban area as an example, with different types of buildings, and the difficulty associated with obtaining information about what is tracking place and which parts of the environment are in a critical state.In these situations, solutions based on independent robots which cooperate to obtain information about the environment and to analyze what is happening are especially important.If these are damaged in the rescue process, losses will only be economic ones.In recent years, research work linked to the use of independent robots used as a rescue tool in catastrophes and emergency situations has gained great importance, with one outstanding aspect of this being their independence in relation to human intervention and coordination (Murphy, 2004).Two more illustrative examples are shown by the intervention of independent robots in the collapse of the World Trade Center in the United States (Casper and Murphy, 2003) and in the Fukushima power station meltdown in Japan (Nagatani et al., 2013).
The consequences of natural or man-made catastrophes can be devastating. To minimize its impact, it is crucial to carry out a rapid analysis of the affected environment in the moments after they occur, especially from the perspective of alert notification or crisis management. In this context, the use of UAVs, understood as the technological basis on which intelligent systems capable of providing support to rescue teams is built, has positively contributed to face this challenge. In this article the design of a multi-agent architecture which enables the deployment of systems made up of intelligent agents that can monitor environments affected by a catastrophe and provide support to human staff in the decision-making process is proposed. These environments, known in advance, are characterized through a set of points of interests that are critical from the point of view of aerial surveillance and monitoring. To conduct an intelligent information analysis, a formal model of normality analysis is employed, which makes possible the definition of surveillance components. These represent the knowledge bases of the agents responsible for monitoring environments. Likewise, the architecture envisages communication and cooperation mechanisms between the different agents, as the basis for fusing information to assess the overall level of risk of the monitored environment. A case study is presented in which the spread of toxic smoke in an industrial complex which has just suffered a hypothetical earthquake is monitored. Although traditional robots represent a very good solution for preventing human staff putting themselves in danger when applying rescuing protocols, sometimes they cannot effectively operate due to the nature of the catastrophe.In this context, the use of unmanned aerial vehicles (UAVs), commonly known as drones, is ideal.Take, for example, the series of earthquakes which occurred in 2016 in central Italy with a death toll of 299 and substantial material damage.In similar situations, a mini-fleet of drones could be used as a first line of action to autonomously gather information in an environment with poor accessibility, quickly and accurately, thereby supplementing independent terrestrial robots and the human staff themselves.This approach could be based on the use of coordination and cooperation mechanisms, reducing the time needed to sweep the affected area and minimizing the damage the catastrophe may cause in the moments after it strikes.
The consequences of natural or man-made catastrophes can be devastating. To minimize its impact, it is crucial to carry out a rapid analysis of the affected environment in the moments after they occur, especially from the perspective of alert notification or crisis management. In this context, the use of UAVs, understood as the technological basis on which intelligent systems capable of providing support to rescue teams is built, has positively contributed to face this challenge. In this article the design of a multi-agent architecture which enables the deployment of systems made up of intelligent agents that can monitor environments affected by a catastrophe and provide support to human staff in the decision-making process is proposed. These environments, known in advance, are characterized through a set of points of interests that are critical from the point of view of aerial surveillance and monitoring. To conduct an intelligent information analysis, a formal model of normality analysis is employed, which makes possible the definition of surveillance components. These represent the knowledge bases of the agents responsible for monitoring environments. Likewise, the architecture envisages communication and cooperation mechanisms between the different agents, as the basis for fusing information to assess the overall level of risk of the monitored environment. A case study is presented in which the spread of toxic smoke in an industrial complex which has just suffered a hypothetical earthquake is monitored. Thanks to the flexibility and the rapid response provided by drones, it is no coincidence that they are currently in use in different domains, such as the military field, maps reconstruction, surveillance or goods transport, providing affordable solutions and a very high response time.In fact, a drone can be understood as a autonomous device that can collect information by means of a range of sensors (cameras, temperature sensors, presence detectors, etc.) quickly and in situations which it would be really complicated for other mobile devices.To be precise, designing systems based on independent drones, which can cooperate and coordinate with each other and with other agents, sharing information in order to reach an overall goal and to make decisions poses today a real challenge for the research community.
The consequences of natural or man-made catastrophes can be devastating. To minimize its impact, it is crucial to carry out a rapid analysis of the affected environment in the moments after they occur, especially from the perspective of alert notification or crisis management. In this context, the use of UAVs, understood as the technological basis on which intelligent systems capable of providing support to rescue teams is built, has positively contributed to face this challenge. In this article the design of a multi-agent architecture which enables the deployment of systems made up of intelligent agents that can monitor environments affected by a catastrophe and provide support to human staff in the decision-making process is proposed. These environments, known in advance, are characterized through a set of points of interests that are critical from the point of view of aerial surveillance and monitoring. To conduct an intelligent information analysis, a formal model of normality analysis is employed, which makes possible the definition of surveillance components. These represent the knowledge bases of the agents responsible for monitoring environments. Likewise, the architecture envisages communication and cooperation mechanisms between the different agents, as the basis for fusing information to assess the overall level of risk of the monitored environment. A case study is presented in which the spread of toxic smoke in an industrial complex which has just suffered a hypothetical earthquake is monitored. So, how can we assure that a set of drones work in cooperation with each other to solve the problem of information retrieval and intelligent analysis, quickly and efficiently, in environments affected by natural or man-made catastrophes?Then, as a supplementary question, how can we make the most of all current research in the field of intelligent surveillance to find out what is taking place in an environment?
The consequences of natural or man-made catastrophes can be devastating. To minimize its impact, it is crucial to carry out a rapid analysis of the affected environment in the moments after they occur, especially from the perspective of alert notification or crisis management. In this context, the use of UAVs, understood as the technological basis on which intelligent systems capable of providing support to rescue teams is built, has positively contributed to face this challenge. In this article the design of a multi-agent architecture which enables the deployment of systems made up of intelligent agents that can monitor environments affected by a catastrophe and provide support to human staff in the decision-making process is proposed. These environments, known in advance, are characterized through a set of points of interests that are critical from the point of view of aerial surveillance and monitoring. To conduct an intelligent information analysis, a formal model of normality analysis is employed, which makes possible the definition of surveillance components. These represent the knowledge bases of the agents responsible for monitoring environments. Likewise, the architecture envisages communication and cooperation mechanisms between the different agents, as the basis for fusing information to assess the overall level of risk of the monitored environment. A case study is presented in which the spread of toxic smoke in an industrial complex which has just suffered a hypothetical earthquake is monitored. Regarding information retrieval in the moments right after a catastrophe has occurred, a fleet of drones which suitably cooperate would provide that initial analysis of the affected area.In this way, the human agents could evaluate the situation and act in the most effective way possible.Apart from needing a cooperation layout, the system proposed herein must be scalable and independent from the number of available drones and adaptable to the monitoring needs of each situation.So, for example, a terrestrial robot may request a drone to act when it is not possible to access a certain point deemed critical from an information retrieval perspective.
The consequences of natural or man-made catastrophes can be devastating. To minimize its impact, it is crucial to carry out a rapid analysis of the affected environment in the moments after they occur, especially from the perspective of alert notification or crisis management. In this context, the use of UAVs, understood as the technological basis on which intelligent systems capable of providing support to rescue teams is built, has positively contributed to face this challenge. In this article the design of a multi-agent architecture which enables the deployment of systems made up of intelligent agents that can monitor environments affected by a catastrophe and provide support to human staff in the decision-making process is proposed. These environments, known in advance, are characterized through a set of points of interests that are critical from the point of view of aerial surveillance and monitoring. To conduct an intelligent information analysis, a formal model of normality analysis is employed, which makes possible the definition of surveillance components. These represent the knowledge bases of the agents responsible for monitoring environments. Likewise, the architecture envisages communication and cooperation mechanisms between the different agents, as the basis for fusing information to assess the overall level of risk of the monitored environment. A case study is presented in which the spread of toxic smoke in an industrial complex which has just suffered a hypothetical earthquake is monitored. From a computational point of view, both physical drones and terrestrial robots could be controlled by software agents (Wooldridge and Jennings, 1995) that intelligently cooperate to provide a solution to the previously stated problem.These agents would be capable of acting independently, obtaining and analyzing information about the environment, and communicating with each other, to make up a multi-agent system (Weiss, 1999) that acts as a whole in order to approach the posed challenge.It must be stressed that multi-agent systems adapt very well to problems in which complex situations are tackled where there are a range of heterogeneous information sources (Wooldridge, 2001).In particular, multi-agent systems have been extensively used in recent years to solve problems in which cooperation, planning, negotiation or distributed decision-making are essential to ensure key characteristics such as robustness, scalability and adaptability (Weiss, 1999).
The consequences of natural or man-made catastrophes can be devastating. To minimize its impact, it is crucial to carry out a rapid analysis of the affected environment in the moments after they occur, especially from the perspective of alert notification or crisis management. In this context, the use of UAVs, understood as the technological basis on which intelligent systems capable of providing support to rescue teams is built, has positively contributed to face this challenge. In this article the design of a multi-agent architecture which enables the deployment of systems made up of intelligent agents that can monitor environments affected by a catastrophe and provide support to human staff in the decision-making process is proposed. These environments, known in advance, are characterized through a set of points of interests that are critical from the point of view of aerial surveillance and monitoring. To conduct an intelligent information analysis, a formal model of normality analysis is employed, which makes possible the definition of surveillance components. These represent the knowledge bases of the agents responsible for monitoring environments. Likewise, the architecture envisages communication and cooperation mechanisms between the different agents, as the basis for fusing information to assess the overall level of risk of the monitored environment. A case study is presented in which the spread of toxic smoke in an industrial complex which has just suffered a hypothetical earthquake is monitored. To make possible the deployment of these multi-agent systems, in this paper a multi-agent architecture composed of different software agents is proposed.These agents play a series of defined roles to solve the problem of retrieving, analyzing and fusing information in catastrophic situations, and, ultimately, to provide support in the decision-making process.Outstanding among all the agents put forward are those known as information analysis agents, which are capable of analyzing what is happening in the environment thanks to the expert knowledge defined by means of a formal model of normality analysis (Albusac et al., 2009).This model, satisfactorily used in the field of intelligent surveillance of urban traffic environments (Albusac et al., 2010), is based on designing surveillance components, which combine both the knowledge base and the inference engine used to carry out this analysis.Moreover, the architecture considers the use of a series of communication mechanisms, such as the event channels and the blackboard architecture (Jennings et al., 1998), that provide the agents with flexibility when sharing information.
The consequences of natural or man-made catastrophes can be devastating. To minimize its impact, it is crucial to carry out a rapid analysis of the affected environment in the moments after they occur, especially from the perspective of alert notification or crisis management. In this context, the use of UAVs, understood as the technological basis on which intelligent systems capable of providing support to rescue teams is built, has positively contributed to face this challenge. In this article the design of a multi-agent architecture which enables the deployment of systems made up of intelligent agents that can monitor environments affected by a catastrophe and provide support to human staff in the decision-making process is proposed. These environments, known in advance, are characterized through a set of points of interests that are critical from the point of view of aerial surveillance and monitoring. To conduct an intelligent information analysis, a formal model of normality analysis is employed, which makes possible the definition of surveillance components. These represent the knowledge bases of the agents responsible for monitoring environments. Likewise, the architecture envisages communication and cooperation mechanisms between the different agents, as the basis for fusing information to assess the overall level of risk of the monitored environment. A case study is presented in which the spread of toxic smoke in an industrial complex which has just suffered a hypothetical earthquake is monitored. In the article a case study which contemplates monitoring the potential spread of toxic smoke in a scenario made up of an industrial complex and a nearby city is discussed.This example is used to show how the proposed architecture enables the deployment of a specific multi-agent system that carries out this monitoring task.Moreover, how to define a surveillance component, used to monitor the spread of toxic smoke, is described.
In the recent years, a rapid growth of IoT devices has been observed, which in turn results in a huge amount of data produced from multiple sources towards the most disparate cloud platforms or the Internet in general. In a typical cloud-centric approach, the data produced by these devices is simply transmitted over the Internet, for consumption and/or storage. However, with the exponential growth in data production rates, the available network resources are becoming the actual bottleneck of this huge data flowing. Therefore, several challenges are appearing in the coming years, which are mainly related to data transmission, processing, and storage along the so-called cloud-to-thing continuum. In fact, one of the most critical requirements of several IoT applications is low latency, which often hinders raw data consumption to happen at the opposite endpoint with respect to its production. In the context of IoT data stream analytics, for instance, the detection of anomalies or rare-events is one of the most demanding tasks, as it needs prompt detection to increase its significance. In this respect, Fog and Edge Computing seem to be the correct paradigms to alleviate these stringent demands in terms of latency and bandwidth as, by leveraging on re-configurable IoT gateways and smart devices able to support the distribution of the overall computational task, they envisage to liquefy data processing along the way from the sensing device to a cloud endpoint. In this paper, we will present IRESE, that is a rare-event detection system able to apply unsupervised machine learning techniques on the incoming data, directly on affordable gateways located in the IoT edge. Notwithstanding the proposed approach enjoys the benefits of a fully unsupervised learning approach, such as the ability to learn from unlabeled data, it has been tested against various audio rare-event categories, such as gunshot, glass break, scream, and siren, achieving precision and recall measures above 90% in detecting such events. Improved cost-effectiveness and miniaturization of sensing devices have increased their utility in various domains of daily human life such as healthcare, transport, education, agriculture, and security.In a typical IoT environment, these sensing devices are connected to the Internet and responsible to continuously sense their surroundings and then transmit data to a cloud station for further processing.In the past few years, an exponential growth in IoT devices have been observed in the form of smart products.Based on the context, these devices of various varieties produce a huge amount of data at varying rates.According to an estimate by Cisco Global Cloud Index, the data produced by a variety of data sources will reach to around 500 ZB by 2019, whereas the internet infrastructure will be capable to handle 10.4 ZB by that time (Cisco, 2016).Similarly, according to CISCO Internet Business Solutions Group the number of devices connected to the internet will reach around 50 Billion by 2020 (Evans, 2011).These factors (variety, amount of data, and variable data rate) have raised serious concerns in an IoT environment, which mainly relates to data transportation, data storage, data processing, and security.The first concern, transportation of data, needs a high-speed Internet, which can quickly and efficiently transmit data to the destination.The second concern, storing huge amount of data, needs cloud services and other necessary networking infrastructure.The third concern, data processing, is important to be handled because raw data is not meaningful and it is required to transform raw data into meaningful information (Ackoff, 1989).The fourth concern, security must be addressed for critical applications in which IoT data can be stolen or intruders can attack the system.Cloud-based paradigms are widely used in IoT systems, in which the data is pushed to the cloud and after computations, the outcome is delivered back to the local system.However, due to the proliferation of IoT, an increased amount of data is produced at the edge of the network.The limited network bandwidth is unable to meet the requirements of low-latency transportation of data coming at a high speed.Therefore, one can conclude that Cloud Computing alone is not efficient enough to handle the IoT generated data in the coming years (Antonini et al., 2018).Since data production at the edge of a network is increasing, an adequate choice is to perform the necessary processing on an edge device; near the source.The edge devices are becoming more powerful and resource friendly with optimal utilization of resources such as memory and energy.
In the recent years, a rapid growth of IoT devices has been observed, which in turn results in a huge amount of data produced from multiple sources towards the most disparate cloud platforms or the Internet in general. In a typical cloud-centric approach, the data produced by these devices is simply transmitted over the Internet, for consumption and/or storage. However, with the exponential growth in data production rates, the available network resources are becoming the actual bottleneck of this huge data flowing. Therefore, several challenges are appearing in the coming years, which are mainly related to data transmission, processing, and storage along the so-called cloud-to-thing continuum. In fact, one of the most critical requirements of several IoT applications is low latency, which often hinders raw data consumption to happen at the opposite endpoint with respect to its production. In the context of IoT data stream analytics, for instance, the detection of anomalies or rare-events is one of the most demanding tasks, as it needs prompt detection to increase its significance. In this respect, Fog and Edge Computing seem to be the correct paradigms to alleviate these stringent demands in terms of latency and bandwidth as, by leveraging on re-configurable IoT gateways and smart devices able to support the distribution of the overall computational task, they envisage to liquefy data processing along the way from the sensing device to a cloud endpoint. In this paper, we will present IRESE, that is a rare-event detection system able to apply unsupervised machine learning techniques on the incoming data, directly on affordable gateways located in the IoT edge. Notwithstanding the proposed approach enjoys the benefits of a fully unsupervised learning approach, such as the ability to learn from unlabeled data, it has been tested against various audio rare-event categories, such as gunshot, glass break, scream, and siren, achieving precision and recall measures above 90% in detecting such events. Formally, in Edge Computing nomenclature, an edge device (e.g., a gateway) is used to perform computation over data.In fact, Edge Computing approaches aims at performing data processing as close as possible to its source.Moreover, it is an effort to involve decentralized agents to perform necessary processing, which can reduce the burden on centralized processing units (Shi and Dustdar, 2016).Edge Computing is particularity useful in time-critical applications, in which quick data processing is required with prompt response in a particular situation.For example, Boeing 787 generates around 5 GB of data in each second and it needs a large bandwidth to transmit this data, which is not realistic (Shi et al., 2016).In such scenarios computing at the edge is crucial, as users may need very fast responses from the system.
A multi-proxy palaeoenvironmental study (pollen, non-pollen palynomorphs, charcoal particles, mollusk macrofauna) of coastal marshland in Doñana National Park (southwestern Iberian Peninsula) was undertaken to trace environmental change, human activities related to woodland clearance, and past land-use during the mid-late Holocene (~5000–2800 cal BP). The results of this study are combined with archaeological data from the Copper and Bronze Ages and are subsequently compared with those of similar research carried out at the south-westernmost part of Europe with the aim of discerning regional differences or similarities. Our research has allowed us to recognize climate changes and four extreme wave events in the Guadalquivir paleoestuary, which might have contributed to both the cultural change that is observed in the archaeological record at the end of the Chalcolithic and the subsequent population decline during much of the Bronze Age. During the Upper Pleistocene and the Holocene, the south-western extreme of Europe is a territory of major archaeological and anthropological interest due to its border position at the crossroads between Europe and Africa and between the Atlantic Ocean and the Mediterranean Sea (López-García and López-Sáez, 1994a, 1994b; Pérez-Díaz et al., 2017).In the southwest of the Iberian Peninsula it has been documented the earliest known use of marine resources was by Neanderthals ~150 kyr ago (Cortés-Sánchez et al., 2011) and was a crucial reservoir of biodiversity during the Upper Pleistocene and early Holocene (Carrión et al., 2008; Cortés-Sánchez et al., 2008).Southwest Iberia is also key to understanding the neolithization process in the Iberian Peninsula; evidence of Neolithic settlement is present from at least 7500 cal BP, when an agricultural and food producing economy quickly replaced the coastal economies of the Mesolithic populations after the 8200 cal BP abrupt climatic event (López-Sáez et al., 2011; Cortés-Sánchez et al., 2012).Southwest Iberia is a unique area for the study of the funerary record and demography from the Mesolithic to the Copper Age (Díaz-Zorita et al., 2012), when some highly-elaborate tombs, such as Montelirio (Seville), feature ceremonial and rich burial goods unparalleled in Chalcolithic Europe (Fernández-Flores et al., 2016).Importantly, this region exhibits sharp cultural disruptions at the time of the 4200 cal BP climatic event and is, therefore, crucial for tracking plausible population movements between the SW and the SE of the Iberian Peninsula (Lillios et al., 2016; Blanco-González et al., 2018).Finally, the timing and forms of interaction between local Late Bronze Age and Early Iron Age societies and the expansion of Mycenaean, Phoenician, and Greek influence, as evidenced by artefacts, are the subject of intense debate (López-Sáez et al., 2002b; Celestino-Pérez et al., 2008).
A multi-proxy palaeoenvironmental study (pollen, non-pollen palynomorphs, charcoal particles, mollusk macrofauna) of coastal marshland in Doñana National Park (southwestern Iberian Peninsula) was undertaken to trace environmental change, human activities related to woodland clearance, and past land-use during the mid-late Holocene (~5000–2800 cal BP). The results of this study are combined with archaeological data from the Copper and Bronze Ages and are subsequently compared with those of similar research carried out at the south-westernmost part of Europe with the aim of discerning regional differences or similarities. Our research has allowed us to recognize climate changes and four extreme wave events in the Guadalquivir paleoestuary, which might have contributed to both the cultural change that is observed in the archaeological record at the end of the Chalcolithic and the subsequent population decline during much of the Bronze Age. Within Southwest Iberia (Fig. 1), Doñana National Park (DNP) is one of the largest in Spain, the landscape formed as a result of a long and complex geological evolution (~6 Myr) and anthropogenic history (Celestino-Pérez et al., 2016).The area encompasses a large system of rivers, marshes, dunes, and beaches associated with coastal dynamics at the mouth of the Guadalquivir River.The archaeological literature regarding the formation of this landscape in the course of the Holocene is still under the influence of an uniformitarian model that Schulten (1945) and Gavala (1927, 1936) proposed in the first half of the 20th century, before the scientific impact of Plate Tectonics Theory.According to this model, the maximum transgression of the Atlantic Ocean during the Holocene opened up a wide estuary that extended as far inland as Coria del Río, some 15 km south of Seville.Thereafter, estuarine and riverine sedimentation began to infill the feature, leading to the development of a major delta and the progradation, of coastal barriers.Behind these barriers, a large inland lake developed, the Lacus Ligustinus mentioned by Rufus Festus Avienus in the 4th century AD (Rodríguez-Ramírez et al., 2016).Eventually, the lake was infilled, turning the landscape into a series of marshes and lagoons that anyone visiting the DNP today would recognize.
A multi-proxy palaeoenvironmental study (pollen, non-pollen palynomorphs, charcoal particles, mollusk macrofauna) of coastal marshland in Doñana National Park (southwestern Iberian Peninsula) was undertaken to trace environmental change, human activities related to woodland clearance, and past land-use during the mid-late Holocene (~5000–2800 cal BP). The results of this study are combined with archaeological data from the Copper and Bronze Ages and are subsequently compared with those of similar research carried out at the south-westernmost part of Europe with the aim of discerning regional differences or similarities. Our research has allowed us to recognize climate changes and four extreme wave events in the Guadalquivir paleoestuary, which might have contributed to both the cultural change that is observed in the archaeological record at the end of the Chalcolithic and the subsequent population decline during much of the Bronze Age. Elsewhere with in the DNP, large tracts of the former estuary have been reclaimed by constructing flood levees and undertaking large-scale agricultural projects.Consequently, it was not until this process of extensive infilling reached a certain level, not before the Middle Ages, that the area could have carried a significant human population.Later versions of such an uniformitarian view are, for instance, those of Menanteau (1981), Arteaga and Roos (1995), and Escacena (2001, 2014).
A multi-proxy palaeoenvironmental study (pollen, non-pollen palynomorphs, charcoal particles, mollusk macrofauna) of coastal marshland in Doñana National Park (southwestern Iberian Peninsula) was undertaken to trace environmental change, human activities related to woodland clearance, and past land-use during the mid-late Holocene (~5000–2800 cal BP). The results of this study are combined with archaeological data from the Copper and Bronze Ages and are subsequently compared with those of similar research carried out at the south-westernmost part of Europe with the aim of discerning regional differences or similarities. Our research has allowed us to recognize climate changes and four extreme wave events in the Guadalquivir paleoestuary, which might have contributed to both the cultural change that is observed in the archaeological record at the end of the Chalcolithic and the subsequent population decline during much of the Bronze Age. However, recent geological, archaeological, and paleoenvironmental research (Rodríguez-Ramírez et al., 2014, 2015; Jiménez-Moreno et al., 2015), including the research reported in this paper, point instead to a far more complex evolution.In this new model, geomorphological developments in the Guadalquivir paleoestuary in the Holocene, such as the formation of rivers, marshes, dunes, and beaches have been the result of complex interactions between natural processes (climate fluctuations, fluvial and coastal dynamics, and neotectonic movements) and human activities.By the end of the 5th millennium cal BP, for instance, much of the original paleoestuary had turned into marshland by river sedimentation into the paleoestuary, reducing it to a relatively small brackish lagoon; an extreme wave event (EWE) occurring ~4200 cal BP led to erosion and reversed this process, ushering in a new cycle of infilling in the paleoestuary (Rodríguez-Ramírez et al., 2015).Because of this long, complex history of natural and human evolution, DNP has been characterized as a coupled socio-ecological system covering four major ecosystem types: marshland, dunes, coastal and estuar (Ojeda, 1990; Gómez-Baggethum et al., 2010, 2012; Palomo et al., 2014).This socio-ecosystem depends on freshwater flows to sustain biodiversity and agriculture, making it extremely vulnerable to human or climate-induced changes in freshwater availability (Fernández et al., 2010; Martín-López et al., 2011).DNP is recognized as one of the most emblematic natural wetlands in Europe (Fernández-Delgado, 2006; Fernández et al., 2010).Its cultural and ecological importance is recognized through its declaration as a Ramsar site, a UNESCO World Heritage Site, and as a Biosphere Reserve.Its relevance is also acknowledged through different conservation strategies covering about 1080 km2, including a Natural Park as well as the National Park (García-Novo and Marín-Cabrera, 2006).Given the uniqueness of these environmental conditions within Southwest Iberia, and as a result of the high level of protection afforded to Doñana, Finlayson et al. (2008) consider DNP as one of the few remaining zones in Iberia that can be used as witness to the habitats and landscapes of the last Neanderthal populations.Furthermore, although Southwest Iberia is classified as a low-probability tsunamigenic area, at least sixteen EWEs, tsunamis and/or severe coastal storms have been identified so far between 7000 cal BP and 1900 CE.Five to seven of them are recorded as palaeotsunamis even though the palaeoenvironmental consequences of these events are poorly known (Ruiz et al., 2010; Lario et al., 2010, 2011; Morales et al., 2011; Rodríguez-Ramírez et al., 2015).
A multi-proxy palaeoenvironmental study (pollen, non-pollen palynomorphs, charcoal particles, mollusk macrofauna) of coastal marshland in Doñana National Park (southwestern Iberian Peninsula) was undertaken to trace environmental change, human activities related to woodland clearance, and past land-use during the mid-late Holocene (~5000–2800 cal BP). The results of this study are combined with archaeological data from the Copper and Bronze Ages and are subsequently compared with those of similar research carried out at the south-westernmost part of Europe with the aim of discerning regional differences or similarities. Our research has allowed us to recognize climate changes and four extreme wave events in the Guadalquivir paleoestuary, which might have contributed to both the cultural change that is observed in the archaeological record at the end of the Chalcolithic and the subsequent population decline during much of the Bronze Age. Since 2005, an integrated research programme involving geologists, palaeoenvironmentalists, archaeologists, historians, and anthropologists has been undertaken in DNP, with the aim of reconstructing the long-term landscape development and socio-ecological, cultural history of this coastal environment.Thus far, the programme has focused on understanding the complex, neotectonic - as well as climate-related geomorphological evolution of the Guadalquivir paleoestuary during the Holocene (Rodríguez-Ramírez et al., 2014) and on assessing the impact of EWEs on paleogeographical changes and human settlement in prehistoric, early historical, and Roman times (Rodríguez-Ramírez et al., 2015, 2016).The programme has also modelled the relationships between late Holocene climate variability and vegetation dynamics (Jiménez-Moreno et al., 2015) to provide insights into the geological, archaeological, and anthropological factors that have sustained the preservation of this singular natural space (Celestino-Pérez et al., 2016).
Lenders, such as credit card companies and banks, use credit scores to evaluate the potential risk posed by lending money to consumers and, therefore, mitigating losses due to bad debt. Within the financial technology domain, an ideal approach should be able to operate proactively, without the need of knowing the behavior of non-reliable users. Actually, this does not happen because the most used techniques need to train their models with both reliable and non-reliable data in order to classify new samples. Such a scenario might be affected by the cold-start problem in datasets, where there is a scarcity or total absence of non-reliable examples, which is further worsened by the potential unbalanced distribution of the data that reduces the classification performances. In this paper, we overcome the aforementioned issues by proposing a proactive approach, composed of a combined entropy-based method that is trained considering only reliable cases and the sample under investigation. Experiments done in different real-world datasets show competitive performances with several state-of-art approaches that use the entire dataset of reliable and unreliable cases. The main task of a Credit Scoring system is the evaluation of new loan applications (from now on named instances) in terms of their potential reliability.Its goal is to lead the financial operators toward a decision about accepting or not a new credit, on the basis of a reliability score assigned by the Credit Scoring system (Morrison, 2004).In a nutshell, the Credit Scoring system is a statistical approach able to evaluate the probability that a new instance is considered reliable (non-default) or unreliable (default), by exploiting a model defined on the basis of previous instances (Mester et al., 1997; Henley, 1994).Banks and credit card companies use credit scores to determine who qualifies for a loan, at what interest rate, and at what credit limits.Therefore, Credit Scoring systems reduce losses due to default cases (Henley et al., 1997), and, for this reason, they represent a crucial instrument.Although similar technical issues are shared, Credit Scoring is different from Fraud detection, which consists of a set of activities undertaken to prevent money or property from being obtained through false pretenses.
Lenders, such as credit card companies and banks, use credit scores to evaluate the potential risk posed by lending money to consumers and, therefore, mitigating losses due to bad debt. Within the financial technology domain, an ideal approach should be able to operate proactively, without the need of knowing the behavior of non-reliable users. Actually, this does not happen because the most used techniques need to train their models with both reliable and non-reliable data in order to classify new samples. Such a scenario might be affected by the cold-start problem in datasets, where there is a scarcity or total absence of non-reliable examples, which is further worsened by the potential unbalanced distribution of the data that reduces the classification performances. In this paper, we overcome the aforementioned issues by proposing a proactive approach, composed of a combined entropy-based method that is trained considering only reliable cases and the sample under investigation. Experiments done in different real-world datasets show competitive performances with several state-of-art approaches that use the entire dataset of reliable and unreliable cases. Thanks to their capability to analyze all the components that contribute to determine default cases (Fensterstock, 2005), Credit Scoring techniques can also be considered a powerful instrument for risk assessment and real-time monitoring (Brill, 1998).
Lenders, such as credit card companies and banks, use credit scores to evaluate the potential risk posed by lending money to consumers and, therefore, mitigating losses due to bad debt. Within the financial technology domain, an ideal approach should be able to operate proactively, without the need of knowing the behavior of non-reliable users. Actually, this does not happen because the most used techniques need to train their models with both reliable and non-reliable data in order to classify new samples. Such a scenario might be affected by the cold-start problem in datasets, where there is a scarcity or total absence of non-reliable examples, which is further worsened by the potential unbalanced distribution of the data that reduces the classification performances. In this paper, we overcome the aforementioned issues by proposing a proactive approach, composed of a combined entropy-based method that is trained considering only reliable cases and the sample under investigation. Experiments done in different real-world datasets show competitive performances with several state-of-art approaches that use the entire dataset of reliable and unreliable cases. Moreover, lenders may also use credit scores to determine which customers are likely to bring in the most revenue.However, as usually happens with other similar contexts (e.g., Fraud Detection (Pozzolo et al., 2014)), the main problem that limits the effectiveness of Credit Scoring classification techniques is represented by the unbalanced distribution of data (Batista et al., 2004).This happens because the default cases available for training the evaluation model are fewer than the non-default ones, hampering the performances of machine learning approaches applied to Credit Scoring (Japkowicz and Stephen, 2002).To note that the unbalanced distribution of data is one of the problems that enables the cold start problem.As such, approaches for balancing data mitigate the cold start problem as well.
Lenders, such as credit card companies and banks, use credit scores to evaluate the potential risk posed by lending money to consumers and, therefore, mitigating losses due to bad debt. Within the financial technology domain, an ideal approach should be able to operate proactively, without the need of knowing the behavior of non-reliable users. Actually, this does not happen because the most used techniques need to train their models with both reliable and non-reliable data in order to classify new samples. Such a scenario might be affected by the cold-start problem in datasets, where there is a scarcity or total absence of non-reliable examples, which is further worsened by the potential unbalanced distribution of the data that reduces the classification performances. In this paper, we overcome the aforementioned issues by proposing a proactive approach, composed of a combined entropy-based method that is trained considering only reliable cases and the sample under investigation. Experiments done in different real-world datasets show competitive performances with several state-of-art approaches that use the entire dataset of reliable and unreliable cases. To overcome such an issue, in this paper we evaluate the instances in terms of their features entropy, defining a metric able to measure their level of reliability considering only non-default cases and the instance under investigation.More formally, we evaluate the reliability of a new instance in terms of comparing the Shannon Entropy (from now on referred simply as entropy) measured within a set of previous non-default instances before and after adding the instance under investigation.As the entropy measures the uncertainty of a random variable, a larger entropy in the set including the sample investigated indicates that it contains similar data in its features, which increases the level of equiprobability, and then we tend to classify it as reliable.Otherwise, it contains different data and we consider the instance as unreliable.Such a process allows us operating proactively, overcoming the issue related to the unbalanced distribution of the data and, at the same time, mitigating the cold-start problem (i.e., the scarcity or total absence of default examples).
Lenders, such as credit card companies and banks, use credit scores to evaluate the potential risk posed by lending money to consumers and, therefore, mitigating losses due to bad debt. Within the financial technology domain, an ideal approach should be able to operate proactively, without the need of knowing the behavior of non-reliable users. Actually, this does not happen because the most used techniques need to train their models with both reliable and non-reliable data in order to classify new samples. Such a scenario might be affected by the cold-start problem in datasets, where there is a scarcity or total absence of non-reliable examples, which is further worsened by the potential unbalanced distribution of the data that reduces the classification performances. In this paper, we overcome the aforementioned issues by proposing a proactive approach, composed of a combined entropy-based method that is trained considering only reliable cases and the sample under investigation. Experiments done in different real-world datasets show competitive performances with several state-of-art approaches that use the entire dataset of reliable and unreliable cases. We report comparisons between our approach and Random Forests, which are considered state-of-the-art approaches for credit scoring tasks (Lessmann et al., 2015; Brown and Mues, 2012; Bhattacharyya et al., 2011).For that we used two real-world datasets, characterized by different distribution of data (unbalanced and slightly unbalanced).Experiments results show that, although our approach is trained on reliable cases only, it has similar performances to the Random Forests.
Lenders, such as credit card companies and banks, use credit scores to evaluate the potential risk posed by lending money to consumers and, therefore, mitigating losses due to bad debt. Within the financial technology domain, an ideal approach should be able to operate proactively, without the need of knowing the behavior of non-reliable users. Actually, this does not happen because the most used techniques need to train their models with both reliable and non-reliable data in order to classify new samples. Such a scenario might be affected by the cold-start problem in datasets, where there is a scarcity or total absence of non-reliable examples, which is further worsened by the potential unbalanced distribution of the data that reduces the classification performances. In this paper, we overcome the aforementioned issues by proposing a proactive approach, composed of a combined entropy-based method that is trained considering only reliable cases and the sample under investigation. Experiments done in different real-world datasets show competitive performances with several state-of-art approaches that use the entire dataset of reliable and unreliable cases. Therefore, the main scientific contributions given by this paper are listed below:
Lenders, such as credit card companies and banks, use credit scores to evaluate the potential risk posed by lending money to consumers and, therefore, mitigating losses due to bad debt. Within the financial technology domain, an ideal approach should be able to operate proactively, without the need of knowing the behavior of non-reliable users. Actually, this does not happen because the most used techniques need to train their models with both reliable and non-reliable data in order to classify new samples. Such a scenario might be affected by the cold-start problem in datasets, where there is a scarcity or total absence of non-reliable examples, which is further worsened by the potential unbalanced distribution of the data that reduces the classification performances. In this paper, we overcome the aforementioned issues by proposing a proactive approach, composed of a combined entropy-based method that is trained considering only reliable cases and the sample under investigation. Experiments done in different real-world datasets show competitive performances with several state-of-art approaches that use the entire dataset of reliable and unreliable cases. This paper is based on a previous work (Saia and Carta, 2016), which has been completely revised, rewritten, improved and extended with the following novel contributions:
An important aspect of studying ancient empire formation is the role of local political economies throughout imperial fluctuations. Such insight can help us understand how imperial powers may or may not have exerted control over their subjects, and the broader impacts of imperial change on local populations. This study uses geochemical analysis (INAA) of ceramic samples and raw clays from Angamuco, located in the Lake Pátzcuaro Basin, Michoacán. Angamuco was occupied before and throughout the development of the Purépecha Empire (1350–1530 CE) and is thus an important case study for evaluating the impacts of political change on material production and manufacturing. We identify four compositional groups, two of which match previously identified groups elsewhere in the lake basin. We argue that Angamuco ceramics were largely locally produced and that raw material use remained relatively stable over long periods. The results of this study contribute to our understanding of ceramic production processes at Angamuco and will be compared to archaeometric studies in Western Mesoamerica and elsewhere. An enduring question in studies of empire formation is the role of local and regional political economies throughout broader political fluctuations (Brumfiel and Earle, 1987; Costin and Hagstrum, 1995; Hirth, 2009; Roux, 2003; Sinopoli, 2003).This question is particularly salient in Postclassic (c. 900–1530 CE) central-western Mexico, where the Purépecha Empire is thought to have operated via a top-down centralized political, economic, and social system beginning in the fourteenth century CE (e.g. Pollard, 2008; Smith and Berdan, 2003).There is limited scholarly attention to the Purépecha compared to central and southeastern Mesoamerican societies, and additional work is necessary to evaluate the impacts of the emerging empire on new subjects.Previous research indicates that the idea of a centralized empire may have been more complicated in terms of resource control and distribution (Cohen, 2016; Hirshman, 2008; Hirshman et al., 2010), but that some top-down control over metal and obsidian resources likely occurred (Maldonado, 2008; Pollard, 2016; Rebnegger, 2010; Walton, 2017).Although the study of regional political economies and the role of tribute have been important in Purépecha archaeology, there has been less attention to local political economies at one site.
An important aspect of studying ancient empire formation is the role of local political economies throughout imperial fluctuations. Such insight can help us understand how imperial powers may or may not have exerted control over their subjects, and the broader impacts of imperial change on local populations. This study uses geochemical analysis (INAA) of ceramic samples and raw clays from Angamuco, located in the Lake Pátzcuaro Basin, Michoacán. Angamuco was occupied before and throughout the development of the Purépecha Empire (1350–1530 CE) and is thus an important case study for evaluating the impacts of political change on material production and manufacturing. We identify four compositional groups, two of which match previously identified groups elsewhere in the lake basin. We argue that Angamuco ceramics were largely locally produced and that raw material use remained relatively stable over long periods. The results of this study contribute to our understanding of ceramic production processes at Angamuco and will be compared to archaeometric studies in Western Mesoamerica and elsewhere. Our study uses data derived from compositional analysis (Instrumental Neutron Activation Analysis, or INAA) to evaluate what geochemical differences – if any – are visible in ceramic pastes from one site in the Purépecha imperial heartland.Such a localized approach is important for assessing variation in paste composition, and the data can help determine if there are any differences in paste recipes over time and throughout site areas.Potential changes in paste recipes could reflect reorganization of production, differential access to resources, and preferential use of resources for particular ceramic forms or styles.Our sample includes excavated ceramic fragments and raw clays from the ancient city of Angamuco, located in the southeastern portion of the Lake Pátzcuaro Basin in Michoacán (Fig. 1).Angamuco was occupied before and throughout the development of the Purépecha Empire (1350–1530 CE) and is thus a critical case study for evaluating the impacts of political change on ceramic manufacturing and consumption.Archaeological and geological samples were submitted for INAA and analyzed to evaluate: 1) whether geochemically distinctive clays and paste recipes were used by potters at Angamuco; 2) whether these varied temporally and spatially across the site; 3) how the Angamuco pottery compares with other known datasets.
This paper provides the preliminary results from an integrated study of the Hellenistic and early Roman glass vessel assemblage from the large domestic context at Paphos named the House of Orpheus. The homogeneous appearance of the omnipresent slumped and cast vessels in late Hellenistic and early Roman contexts within the entire Mediterranean makes it fairly complex to define the origin of specific assemblages when only studying the external features. We present here the results of a combined study of in-situ optical analysis by means of absorption spectroscopy and chemical analyses with SEM-EDS and LA-ICP-MS. In a first step 107 fragments were optically analysed to categorize possible glass groups and 54 selected pieces were sampled to characterize their chemical composition. Four optically defined glass types can be distinguished with respect to the use of a decolouring or colouring agent and the impact of the furnace condition. The chemical analysis techniques define four distinct subgroups within the homogeneous cluster of Levantine glass. Since 1983 (with a long break between 1992 and 2009) excavations have been conducted by Demetrios Michaelides in Nea Paphos at the site of the House of Orpheus.These have brought to light a sequence of buildings, the earliest of which dates to the late 4th c. BC, and is more or less contemporary with the foundation of Nea Paphos.The latest of the structures is a habitation dating to the mid-Roman period, which has been named the House of Orpheus, after the most important of its mosaics, which depict the mythical hero charming the beasts with his music (Michaelides 1991).These different structures show that the site remained continuously occupied all through the Hellenistic and the Roman Imperial period up to the 4th c. AD, when Nea Paphos was devastated by destructive earthquakes.After this date the area of the House of Orpheus appears to have been abandoned (Michaelides, 1983–1998, 1986, 1991a, 1991b).
This paper provides the preliminary results from an integrated study of the Hellenistic and early Roman glass vessel assemblage from the large domestic context at Paphos named the House of Orpheus. The homogeneous appearance of the omnipresent slumped and cast vessels in late Hellenistic and early Roman contexts within the entire Mediterranean makes it fairly complex to define the origin of specific assemblages when only studying the external features. We present here the results of a combined study of in-situ optical analysis by means of absorption spectroscopy and chemical analyses with SEM-EDS and LA-ICP-MS. In a first step 107 fragments were optically analysed to categorize possible glass groups and 54 selected pieces were sampled to characterize their chemical composition. Four optically defined glass types can be distinguished with respect to the use of a decolouring or colouring agent and the impact of the furnace condition. The chemical analysis techniques define four distinct subgroups within the homogeneous cluster of Levantine glass. From the over 7500 so far recorded glass fragments recovered during the extended excavation period, we consider here only the Hellenistic to early Roman core-formed, cast or slumped vessel fragments numbering 262 pieces.The Hellenistic and early Roman glass jewellery, utensils and counters are not taken into account, though well represented at the site, nor are the 1st–4th c. AD blown glass vessels.An in-depth discussion of the entire glass assemblage including an intra-site evaluation of the glass material up to the period of abandonment of the house will be considered within a full excavation report of the site.
This paper provides the preliminary results from an integrated study of the Hellenistic and early Roman glass vessel assemblage from the large domestic context at Paphos named the House of Orpheus. The homogeneous appearance of the omnipresent slumped and cast vessels in late Hellenistic and early Roman contexts within the entire Mediterranean makes it fairly complex to define the origin of specific assemblages when only studying the external features. We present here the results of a combined study of in-situ optical analysis by means of absorption spectroscopy and chemical analyses with SEM-EDS and LA-ICP-MS. In a first step 107 fragments were optically analysed to categorize possible glass groups and 54 selected pieces were sampled to characterize their chemical composition. Four optically defined glass types can be distinguished with respect to the use of a decolouring or colouring agent and the impact of the furnace condition. The chemical analysis techniques define four distinct subgroups within the homogeneous cluster of Levantine glass. This paper is to be considered an interim report providing a preliminary examination of the vessel forms covering the 3rd c. BC – 1st c. AD including grooved and linear-cut bowls, plain bowls, fluted bowls and bowls decorated with an abraded vegetal design, ribbed bowls, and cast vessels as well as one core-formed vessel (Fig. 1).The first aim was to examine whether the glass used for making core-formed glass vessels is similar to that of the slumped and cast vessels.Another was to verify whether two chunks of raw glass from the House of Orpheus are related to the late Hellenistic and early Roman vessel assemblage or whether they are of later date, when the glassblowing technique replaced the cast and slumping techniques.Finally, the optical and chemical analyses of this large set of early glass assemblage is expected to characterize potential distinctive provenances of glass tableware supplying households in Nea Paphos.
This paper provides the preliminary results from an integrated study of the Hellenistic and early Roman glass vessel assemblage from the large domestic context at Paphos named the House of Orpheus. The homogeneous appearance of the omnipresent slumped and cast vessels in late Hellenistic and early Roman contexts within the entire Mediterranean makes it fairly complex to define the origin of specific assemblages when only studying the external features. We present here the results of a combined study of in-situ optical analysis by means of absorption spectroscopy and chemical analyses with SEM-EDS and LA-ICP-MS. In a first step 107 fragments were optically analysed to categorize possible glass groups and 54 selected pieces were sampled to characterize their chemical composition. Four optically defined glass types can be distinguished with respect to the use of a decolouring or colouring agent and the impact of the furnace condition. The chemical analysis techniques define four distinct subgroups within the homogeneous cluster of Levantine glass. An assessment of the early glass from the Paphian House of Orpheus is important for a broader survey of the domestic consumption of vessel glass in the Hellenistic and early Roman period within the entire Mediterranean.The debate concerning this subject commonly remains restricted to sites from the two established major production/consumption areas, i.e. the Aegean and the Levant.The interest of including the House of Orpheus in this discourse is that Paphos was a major coastal city in the Hellenistic period (Młynarczyk, 1990; Ballandier, 2016) situated between these two regions.It had strong connections with Egypt and the Levant, the traditional suppliers of raw glass and finished glass products, e.g. Beirut (Jennings, 2000, 2006; Foy, 2005), as well as with the Aegean, generally recognized as a region with secondary glass workshops, e.g. Rhodes (Weinberg, 1971; Triantafyllidis, 2000, 2003).
This paper provides the preliminary results from an integrated study of the Hellenistic and early Roman glass vessel assemblage from the large domestic context at Paphos named the House of Orpheus. The homogeneous appearance of the omnipresent slumped and cast vessels in late Hellenistic and early Roman contexts within the entire Mediterranean makes it fairly complex to define the origin of specific assemblages when only studying the external features. We present here the results of a combined study of in-situ optical analysis by means of absorption spectroscopy and chemical analyses with SEM-EDS and LA-ICP-MS. In a first step 107 fragments were optically analysed to categorize possible glass groups and 54 selected pieces were sampled to characterize their chemical composition. Four optically defined glass types can be distinguished with respect to the use of a decolouring or colouring agent and the impact of the furnace condition. The chemical analysis techniques define four distinct subgroups within the homogeneous cluster of Levantine glass. Most sets of Hellenistic-early Roman glass vessels from Cypriot sites are small and come mainly from sanctuaries, e.g. the sanctuary of Apollo Hylates near Kourion (Grose, 1986), Geronisos opposite Agios Georgios of Peyia, allegedly an island sanctuary dedicated to Apollo (Burdajewicz, 2009) and the sanctuary of Aphrodite at Amathous (Nenna, 2006).There is also some published material from public buildings, e.g. the Fabrika Hill at Paphos (McCall, 2016), and from burials, e.g. Amathous (Oliver, 1992) and Aphendrika (Dray and Plat Taylor, 1937–1939).Recently, information from other Paphian domestic contexts (Mazanek, 2014, 2016a, b; McCall, 2016)1 has been made accessible providing an added value to the comparative study with, on the one hand, the large Hellenistic-early Roman assemblage from the House of Orpheus and, on the other, the early vessel glass consumption in Cyprus and the eastern Mediterranean.
This paper provides the preliminary results from an integrated study of the Hellenistic and early Roman glass vessel assemblage from the large domestic context at Paphos named the House of Orpheus. The homogeneous appearance of the omnipresent slumped and cast vessels in late Hellenistic and early Roman contexts within the entire Mediterranean makes it fairly complex to define the origin of specific assemblages when only studying the external features. We present here the results of a combined study of in-situ optical analysis by means of absorption spectroscopy and chemical analyses with SEM-EDS and LA-ICP-MS. In a first step 107 fragments were optically analysed to categorize possible glass groups and 54 selected pieces were sampled to characterize their chemical composition. Four optically defined glass types can be distinguished with respect to the use of a decolouring or colouring agent and the impact of the furnace condition. The chemical analysis techniques define four distinct subgroups within the homogeneous cluster of Levantine glass. It is generally accepted that the late Hellenistic cast glass vessels were produced in various workshops in the Aegean and the Levant/Syro-Palestinian coast, whereas it has been proposed that one or more Hellenistic glass workshops producing core-formed glass vessels were active in Cyprus (McClellan, 1984; Cosyns and Nys, 2010).One of these glass workshops producing cast glass is thought to have been located in Rhodes (Triantafyllidis, 2000, 2003), but several other workshops were most likely active contemporaneously elsewhere in the Aegean (Jackson-Tal, 2004).To the present, there is no identified workshop (Triantafyllidis, 2006) there, but one might well have existed at Tel Anafa, Israel, considering the huge amount of slumped glass vessels found within such a small settlement (Weinberg, 1970; Grose, 2012; Larson, 2016).
Fermented and alcoholic beverages played a pivotal role in feastings and social events in past agricultural and urban societies across the globe, but the origins of the sophisticated relevant technologies remain elusive. It has long been speculated that the thirst for beer may have been the stimulus behind cereal domestication, which led to a major social-technological change in human history; but this hypothesis has been highly controversial. We report here of the earliest archaeological evidence for cereal-based beer brewing by a semi-sedentary, foraging people. The current project incorporates experimental study, contextual examination, and use-wear and residue analyses of three stone mortars from a Natufian burial site at Raqefet Cave, Israel (13,700–11,700 cal. BP). The results of the analyses indicate that the Natufians exploited at least seven plant taxa, including wheat or barley, oat, legumes and bast fibers (including flax). They packed plant-foods, including malted wheat/barley, in fiber-made containers and stored them in boulder mortars. They used bedrock mortars for pounding and cooking plant-foods, including brewing wheat/barley-based beer likely served in ritual feasts ca. 13,000 years ago. These innovations predated the appearance of domesticated cereals by several millennia in the Near East. The consumption of fermented and alcoholic beverages is one of the most prevalent human behaviors, but the time and cultural context of its origins remain unclear.Archaeological evidence for alcohol production and use is usually associated with fermenting domesticated species in agricultural societies, such as ancient Egypt, Mesopotamia, China, and South America (Goldstein, 2001; Jennings, et al., 2005; Katz and Voigt, 1986; McGovern, et al., 2004; Samuel, 1996; Wang, et al., 2016).It has long been speculated that humans' thirst for beer may have been the stimulus behind cereal domestication (Braidwood et al., 1953), and some scholars have attributed this invention to the Natufians in the Near East (ca. 15,000–11,500 Cal BP) (see Hayden, et al., 2013).The Natufians were innovative in many material and social realms, and paved the way to the establishment of the first sedentary Neolithic villages at about 11,500 cal BP (Bar-Yosef, 1998).To test the so far unsubstantiated “Natufian beer hypothesis” we examined three stone mortars from the first chamber at Raqefet Cave (13,700–11,700 cal.BP), a site with a long archaeological sequence (Lengyel, 2007), that also served as a Natufian burial place in Mt. Carmel, Israel (Fig. 1A; Fig. S1A) (Nadel, et al., 2013).
Fermented and alcoholic beverages played a pivotal role in feastings and social events in past agricultural and urban societies across the globe, but the origins of the sophisticated relevant technologies remain elusive. It has long been speculated that the thirst for beer may have been the stimulus behind cereal domestication, which led to a major social-technological change in human history; but this hypothesis has been highly controversial. We report here of the earliest archaeological evidence for cereal-based beer brewing by a semi-sedentary, foraging people. The current project incorporates experimental study, contextual examination, and use-wear and residue analyses of three stone mortars from a Natufian burial site at Raqefet Cave, Israel (13,700–11,700 cal. BP). The results of the analyses indicate that the Natufians exploited at least seven plant taxa, including wheat or barley, oat, legumes and bast fibers (including flax). They packed plant-foods, including malted wheat/barley, in fiber-made containers and stored them in boulder mortars. They used bedrock mortars for pounding and cooking plant-foods, including brewing wheat/barley-based beer likely served in ritual feasts ca. 13,000 years ago. These innovations predated the appearance of domesticated cereals by several millennia in the Near East. The consumption of fermented and alcoholic beverages is one of the most prevalent human behaviors, but the time and cultural context of its origins remain unclear.Archaeological evidence for alcohol production and use is usually associated with fermenting domesticated species in agricultural societies, such as ancient Egypt, Mesopotamia, China, and South America (Goldstein, 2001; Jennings, et al., 2005; Katz and Voigt, 1986; McGovern, et al., 2004; Samuel, 1996; Wang, et al., 2016).It has long been speculated that humans' thirst for beer may have been the stimulus behind cereal domestication (Braidwood, et al., 1953), and some scholars have attributed this invention to the Natufians in the Near East (ca. 15,000–11,500 Cal BP) (see Hayden, et al., 2013).The Natufians were innovative in many material and social realms, and paved the way to the establishment of the first sedentary Neolithic villages at about 11,500 cal BP (Bar-Yosef, 1998).To test the so far unsubstantiated “Natufian beer hypothesis” we examined three stone mortars from the first chamber at Raqefet Cave (13,700–11,700 cal.BP), a site with a long archaeological sequence (Lengyel, 2007), that also served as a Natufian burial place in Mt. Carmel, Israel (Fig. 1A; Fig. S1A) (Nadel, et al., 2013).
Fermented and alcoholic beverages played a pivotal role in feastings and social events in past agricultural and urban societies across the globe, but the origins of the sophisticated relevant technologies remain elusive. It has long been speculated that the thirst for beer may have been the stimulus behind cereal domestication, which led to a major social-technological change in human history; but this hypothesis has been highly controversial. We report here of the earliest archaeological evidence for cereal-based beer brewing by a semi-sedentary, foraging people. The current project incorporates experimental study, contextual examination, and use-wear and residue analyses of three stone mortars from a Natufian burial site at Raqefet Cave, Israel (13,700–11,700 cal. BP). The results of the analyses indicate that the Natufians exploited at least seven plant taxa, including wheat or barley, oat, legumes and bast fibers (including flax). They packed plant-foods, including malted wheat/barley, in fiber-made containers and stored them in boulder mortars. They used bedrock mortars for pounding and cooking plant-foods, including brewing wheat/barley-based beer likely served in ritual feasts ca. 13,000 years ago. These innovations predated the appearance of domesticated cereals by several millennia in the Near East. Excavations at Raqefet Cave between 2004 and 2011 revealed a radio-metrically dated Natufian graveyard with ca. 30 burials (Barzilai, et al., 2017; Lengyel, et al., 2013).Clear indications for burial-associated rituals include repetitive interments, floral grave lining in some of the burials (Nadel, et al., 2013), and animal bones in the graves representing punctuated funerary feasts (Yeshurun, et al., 2013).About 100 bedrock features (e.g. mortars and cupmarks hewn in the cave floor and the terrace) were found (Nadel and Lengyel, 2009), some directly associated with burials.Two deep narrow boulder mortars (BM1 and BM2) were found in situ and juxtaposed to human remains.Thus, they were the focus of our contextual, use-wear and residue analyses.BM1 has a cylindrical shaft ca. 33 cm deep and a hole at the base; it is stored in the Laboratory for Ground Stone Tools Research, Zinman Institute of Archaeology, University of Haifa after the excavation in 2006.BM2, a funnel-shaped shaft ca. 35 cm deep, has remained inside Raqefet cave.A well-preserved bowl-like bedrock mortar (BM3), 18 cm deep and 27 cm in rim diameter, located in a cluster of bedrock features in the middle of the cave floor was included in the current study (Fig. 1B; Fig. S1B,C).
Modeling and analysis of students’ performance is a common task that is aimed at identifying important factors that affect the learning process. Typically, the analysis uses one-dimensional input parameters. However, with the advancement of data collections tools, many of the gathered educational datasets have become high-dimensional. Hence, the use of standard statistical methods may be limited in cases that the initial data unit is a vector. This paper proposes to use vector input units, which consist of student performance trajectories, for identifying statistical differences in college performances for several populations of college students. Two kernel based methods named diffusion maps and the kernel two-sample test are utilized. Diffusion maps generates a low-dimensional representation of the data, in which important characteristic factors are identified. The kernel two-sample test is a statistical test for comparing whether high-dimensional samples are drawn from two different probability distributions. The two methods are combined into a unified framework. Two case studies, which are processed similarly, are presented. The first tests for significant distributional differences between students with or without learning disabilities. Our results show that these groups’ performances is significantly different. The second case-study analyzes whether the SAT score impacts students’ performance throughout their 4-year of studies. It was found that significant distribution differences in performance are only present for groups of students having a very high or a very low SAT score. Thus, the SAT score is only weakly correlated to students’ college performance. Modeling and analysis of students’ performance is a common task that is carried out by researchers and educational institutes (Natek and Zwilling, 2014; Pena-Ayala, 2014; Shahiri et al., 2015; Hoffait and Schyns, 2017; Miguèis et al., 2018).The goal is to identify important factors that affect the learning process.Based on these factors, educational policies can be updated and teachers may be encouraged to adopt new educational approaches that enhance and improve the overall learning process.Common examined factors include the student’s personal background, the acceptance criteria and performance attributes.
Modeling and analysis of students’ performance is a common task that is aimed at identifying important factors that affect the learning process. Typically, the analysis uses one-dimensional input parameters. However, with the advancement of data collections tools, many of the gathered educational datasets have become high-dimensional. Hence, the use of standard statistical methods may be limited in cases that the initial data unit is a vector. This paper proposes to use vector input units, which consist of student performance trajectories, for identifying statistical differences in college performances for several populations of college students. Two kernel based methods named diffusion maps and the kernel two-sample test are utilized. Diffusion maps generates a low-dimensional representation of the data, in which important characteristic factors are identified. The kernel two-sample test is a statistical test for comparing whether high-dimensional samples are drawn from two different probability distributions. The two methods are combined into a unified framework. Two case studies, which are processed similarly, are presented. The first tests for significant distributional differences between students with or without learning disabilities. Our results show that these groups’ performances is significantly different. The second case-study analyzes whether the SAT score impacts students’ performance throughout their 4-year of studies. It was found that significant distribution differences in performance are only present for groups of students having a very high or a very low SAT score. Thus, the SAT score is only weakly correlated to students’ college performance. Many studies examine the influence of the Scholastic Aptitude Test (SAT ®) on college performance (Fishman and Pasanella, 1960; Young, 2001; Kobrin and Michel, 2006; Keller et al., 2009).These studies typically use methods like correlation and regression analysis to model this relationship.It is widely accepted that although the SAT score is generally correlated with college performance, the predictions are not accurate (Rothstein, 2005; Bai et al., 2014) and there are other factors that influence performance (Camara, 2005; Atwood and Pretz, 2016).One of the case-studies that is addressed in this paper quantifies the relationship between SAT scores and college performance by using kernel based method.The proposed methods overcome the limitations of regression techniques.In particular, we show that the reliability of the SAT score as a performance predictor varies in different regions of the data.Therefore, using a fine quantization of the SAT score values may provide a more accurate performance predictor in the scope of college acceptance criteria.
Modeling and analysis of students’ performance is a common task that is aimed at identifying important factors that affect the learning process. Typically, the analysis uses one-dimensional input parameters. However, with the advancement of data collections tools, many of the gathered educational datasets have become high-dimensional. Hence, the use of standard statistical methods may be limited in cases that the initial data unit is a vector. This paper proposes to use vector input units, which consist of student performance trajectories, for identifying statistical differences in college performances for several populations of college students. Two kernel based methods named diffusion maps and the kernel two-sample test are utilized. Diffusion maps generates a low-dimensional representation of the data, in which important characteristic factors are identified. The kernel two-sample test is a statistical test for comparing whether high-dimensional samples are drawn from two different probability distributions. The two methods are combined into a unified framework. Two case studies, which are processed similarly, are presented. The first tests for significant distributional differences between students with or without learning disabilities. Our results show that these groups’ performances is significantly different. The second case-study analyzes whether the SAT score impacts students’ performance throughout their 4-year of studies. It was found that significant distribution differences in performance are only present for groups of students having a very high or a very low SAT score. Thus, the SAT score is only weakly correlated to students’ college performance. The second case-study that is presented in this paper examines college performance differences between students with or without learning disabilities.The number of college students with learning disabilities has increased over the years.Students with learning disabilities are usually entitled to accommodations in an attempt to make learning conditions equal.Time adjustments in exams is one of the common accommodations (Gregg, 2009).Evaluating the effectiveness of the accommodations was addressed in a number of papers (Thompson and Thurlow, 2002; Fuchs et al., 2000; Huesman and Frisbie, 2000), which stated that it is difficult to objectively quantify the influence of the accommodations on performance.In a later paper (Gregg and Nelson, 2012) students with or without learning disabilities were tested with and without the extra time adjustments.It was found that time adjustments improve the performance of all students and in particular the performance of students with learning disabilities.In this study, we model the performance trajectories (learning curve) of undergraduate engineering students with or without learning disabilities by using the kernel based tools.The results show that these two groups are not equally distributed, and we point out to two characteristic factors that distinguish between the groups.The obtained analysis may improve the institution’s supporting systems for the group of students that have a learning disability in various periods during their studies and help them reach their full potential.
Modeling and analysis of students’ performance is a common task that is aimed at identifying important factors that affect the learning process. Typically, the analysis uses one-dimensional input parameters. However, with the advancement of data collections tools, many of the gathered educational datasets have become high-dimensional. Hence, the use of standard statistical methods may be limited in cases that the initial data unit is a vector. This paper proposes to use vector input units, which consist of student performance trajectories, for identifying statistical differences in college performances for several populations of college students. Two kernel based methods named diffusion maps and the kernel two-sample test are utilized. Diffusion maps generates a low-dimensional representation of the data, in which important characteristic factors are identified. The kernel two-sample test is a statistical test for comparing whether high-dimensional samples are drawn from two different probability distributions. The two methods are combined into a unified framework. Two case studies, which are processed similarly, are presented. The first tests for significant distributional differences between students with or without learning disabilities. Our results show that these groups’ performances is significantly different. The second case-study analyzes whether the SAT score impacts students’ performance throughout their 4-year of studies. It was found that significant distribution differences in performance are only present for groups of students having a very high or a very low SAT score. Thus, the SAT score is only weakly correlated to students’ college performance. Typically, the above mentioned case-studies relay on one-dimensional input parameters for constructing statistical analysis models.However, with the advancement of data collections tools, many of the gathered datasets have become high-dimensional.Hence, the use of standard statistical methods may be limited in cases that the initial data unit is a vector.Here, we explore various kernel based methods that are suited for processing high-dimensional input data.In particular, the input data in this work consists of a vector that holds the student’s average grades over his/her 4 years of undergraduate studies.This vector is denoted by performance trajectory.
Modeling and analysis of students’ performance is a common task that is aimed at identifying important factors that affect the learning process. Typically, the analysis uses one-dimensional input parameters. However, with the advancement of data collections tools, many of the gathered educational datasets have become high-dimensional. Hence, the use of standard statistical methods may be limited in cases that the initial data unit is a vector. This paper proposes to use vector input units, which consist of student performance trajectories, for identifying statistical differences in college performances for several populations of college students. Two kernel based methods named diffusion maps and the kernel two-sample test are utilized. Diffusion maps generates a low-dimensional representation of the data, in which important characteristic factors are identified. The kernel two-sample test is a statistical test for comparing whether high-dimensional samples are drawn from two different probability distributions. The two methods are combined into a unified framework. Two case studies, which are processed similarly, are presented. The first tests for significant distributional differences between students with or without learning disabilities. Our results show that these groups’ performances is significantly different. The second case-study analyzes whether the SAT score impacts students’ performance throughout their 4-year of studies. It was found that significant distribution differences in performance are only present for groups of students having a very high or a very low SAT score. Thus, the SAT score is only weakly correlated to students’ college performance. Kernel methods occupy a central role machine learning and recently also in statistical analysis.The so called kernel trick allows to analyze complex datasets based on a reliable pairwise distance that is defined by the kernel.When the data is not linearly separable, its mapping to a higher-dimensional space, as defined by the kernel, may simplify the analysis.In addition, kernels are suited to model datasets of higher-dimensions with possible non-linear connections between the data parameters.Spectral decomposition of the kernel results in a compact representation for visualizing and analyzing the data, while staying faithful to its original geometric structure and local connections.These kernel methods are also known as manifold learning techniques.
Modeling and analysis of students’ performance is a common task that is aimed at identifying important factors that affect the learning process. Typically, the analysis uses one-dimensional input parameters. However, with the advancement of data collections tools, many of the gathered educational datasets have become high-dimensional. Hence, the use of standard statistical methods may be limited in cases that the initial data unit is a vector. This paper proposes to use vector input units, which consist of student performance trajectories, for identifying statistical differences in college performances for several populations of college students. Two kernel based methods named diffusion maps and the kernel two-sample test are utilized. Diffusion maps generates a low-dimensional representation of the data, in which important characteristic factors are identified. The kernel two-sample test is a statistical test for comparing whether high-dimensional samples are drawn from two different probability distributions. The two methods are combined into a unified framework. Two case studies, which are processed similarly, are presented. The first tests for significant distributional differences between students with or without learning disabilities. Our results show that these groups’ performances is significantly different. The second case-study analyzes whether the SAT score impacts students’ performance throughout their 4-year of studies. It was found that significant distribution differences in performance are only present for groups of students having a very high or a very low SAT score. Thus, the SAT score is only weakly correlated to students’ college performance. Manifold learning techniques such as Local Linear Embedding (Roweis and Saul, 2000), Laplacian Eigenmaps (Belkin and Niyogi, 2003, 2004), Local Tangent Space Alignment (Zhang and Zha, 2002) and Diffusion Maps (Coifman and Lafon, 2006) assume that patterns in a high-dimensional ambient space lie in fact on a lower-dimensional manifold.Moreover, in diffusion maps, which is the manifold learning technique applied in this work, the intrinsic manifold is described by an appropriate diffusion distance (Coifman and Lafon, 2006) that serves as a metric for this data.When patterns are embedded into the diffusion maps coordinates, the diffusion distance metric becomes the Euclidean metric on the embedded space.This property makes diffusion methods very attractive when looking for structures in the data that can be characterized by intrinsic geometry.In practice this means that the distances in the new low-dimensional representation stay loyal to the original structure of the data in the ambient space.Linear dimensionality reduction methods like Principal Component Analysis (Pearson, 1901) do not have this property.
Modeling and analysis of students’ performance is a common task that is aimed at identifying important factors that affect the learning process. Typically, the analysis uses one-dimensional input parameters. However, with the advancement of data collections tools, many of the gathered educational datasets have become high-dimensional. Hence, the use of standard statistical methods may be limited in cases that the initial data unit is a vector. This paper proposes to use vector input units, which consist of student performance trajectories, for identifying statistical differences in college performances for several populations of college students. Two kernel based methods named diffusion maps and the kernel two-sample test are utilized. Diffusion maps generates a low-dimensional representation of the data, in which important characteristic factors are identified. The kernel two-sample test is a statistical test for comparing whether high-dimensional samples are drawn from two different probability distributions. The two methods are combined into a unified framework. Two case studies, which are processed similarly, are presented. The first tests for significant distributional differences between students with or without learning disabilities. Our results show that these groups’ performances is significantly different. The second case-study analyzes whether the SAT score impacts students’ performance throughout their 4-year of studies. It was found that significant distribution differences in performance are only present for groups of students having a very high or a very low SAT score. Thus, the SAT score is only weakly correlated to students’ college performance. Another view of a manifold learning and kernel based methods is related to graphs.A kernel connects the data-points by edges and results in a network in which the data points are the nodes of a graph and the graph’s edges are weighted based on a defined distance between the nodes.The idea of modeling educational data with graphs recently gained attention in conferences (Lynch et al., 2015) and in journal papers that propose to use directed and undirected graph structures.Modeling the relations among courses with directed graphs and introducing the notion of concept graph learning was proposed in Yang et al. (2015) and Liu et al. (2016).In Grawemeyer et al. (2017) and Poulovassilis et al. (2015) graphs were utilized to identify the differences between students with high and low performance when working in an exploratory learning environment.The PageRank algorithm (Brin and Page, 1998), which is based on a graph representation of a dataset, was applied in London and Nèmeth (2014) for evaluating and ranking of students’ achievements.The PageRank method uses only the first top eigenvector of the Markov matrix, which is the stationary distribution of a random walk on the link structure of the graph.Manifold learning techniques use numerous eigenvectors of the graph’s associated Markov matrix, hence, their description of the graph is more accurate when compared to PageRank.
Modeling and analysis of students’ performance is a common task that is aimed at identifying important factors that affect the learning process. Typically, the analysis uses one-dimensional input parameters. However, with the advancement of data collections tools, many of the gathered educational datasets have become high-dimensional. Hence, the use of standard statistical methods may be limited in cases that the initial data unit is a vector. This paper proposes to use vector input units, which consist of student performance trajectories, for identifying statistical differences in college performances for several populations of college students. Two kernel based methods named diffusion maps and the kernel two-sample test are utilized. Diffusion maps generates a low-dimensional representation of the data, in which important characteristic factors are identified. The kernel two-sample test is a statistical test for comparing whether high-dimensional samples are drawn from two different probability distributions. The two methods are combined into a unified framework. Two case studies, which are processed similarly, are presented. The first tests for significant distributional differences between students with or without learning disabilities. Our results show that these groups’ performances is significantly different. The second case-study analyzes whether the SAT score impacts students’ performance throughout their 4-year of studies. It was found that significant distribution differences in performance are only present for groups of students having a very high or a very low SAT score. Thus, the SAT score is only weakly correlated to students’ college performance. In this paper, two kernel based methods, diffusion maps and the kernel two-sample test are applied to model students’ performance.Diffusion maps is used for generating a low-dimensional representation of the data and for identifying important characteristic factors that distinguish between groups of students.The kernel two-sample test (Gretton et al., 2007a, b, 2009, 2012a, b) is a framework based on statistical testing for comparing whether samples are drawn from two different probability distributions.It has been applied vastly, for example for domain adaptation in Li et al. (2015), Shaham et al. (2017), Tao et al. (2012) and it has recently been extended to work with anisotropic kernels (Cheng et al., 2017).In the framework of educational data mining, we propose to use this test for quantifying the differences between groups of students, where the data that defines a student is a performance trajectory vector, rather than a single number.
Modeling and analysis of students’ performance is a common task that is aimed at identifying important factors that affect the learning process. Typically, the analysis uses one-dimensional input parameters. However, with the advancement of data collections tools, many of the gathered educational datasets have become high-dimensional. Hence, the use of standard statistical methods may be limited in cases that the initial data unit is a vector. This paper proposes to use vector input units, which consist of student performance trajectories, for identifying statistical differences in college performances for several populations of college students. Two kernel based methods named diffusion maps and the kernel two-sample test are utilized. Diffusion maps generates a low-dimensional representation of the data, in which important characteristic factors are identified. The kernel two-sample test is a statistical test for comparing whether high-dimensional samples are drawn from two different probability distributions. The two methods are combined into a unified framework. Two case studies, which are processed similarly, are presented. The first tests for significant distributional differences between students with or without learning disabilities. Our results show that these groups’ performances is significantly different. The second case-study analyzes whether the SAT score impacts students’ performance throughout their 4-year of studies. It was found that significant distribution differences in performance are only present for groups of students having a very high or a very low SAT score. Thus, the SAT score is only weakly correlated to students’ college performance. The goal of this work is two-fold.The first goal is to identify several representative types of student performance trajectories based on the diffusion maps embedding.The second goal is to use the identified types as a basis for answering different research questions by using a statistical framework.Two-case studies, which are modeled and analyzed in a similar manner, are presented in this paper.The first case-study tests whether the distribution of students that receive accommodations significantly differs from those who do not have accommodations.In the second case-study, we test for significant differences in the distributions between groups of students that have different SAT scores.
Modeling and analysis of students’ performance is a common task that is aimed at identifying important factors that affect the learning process. Typically, the analysis uses one-dimensional input parameters. However, with the advancement of data collections tools, many of the gathered educational datasets have become high-dimensional. Hence, the use of standard statistical methods may be limited in cases that the initial data unit is a vector. This paper proposes to use vector input units, which consist of student performance trajectories, for identifying statistical differences in college performances for several populations of college students. Two kernel based methods named diffusion maps and the kernel two-sample test are utilized. Diffusion maps generates a low-dimensional representation of the data, in which important characteristic factors are identified. The kernel two-sample test is a statistical test for comparing whether high-dimensional samples are drawn from two different probability distributions. The two methods are combined into a unified framework. Two case studies, which are processed similarly, are presented. The first tests for significant distributional differences between students with or without learning disabilities. Our results show that these groups’ performances is significantly different. The second case-study analyzes whether the SAT score impacts students’ performance throughout their 4-year of studies. It was found that significant distribution differences in performance are only present for groups of students having a very high or a very low SAT score. Thus, the SAT score is only weakly correlated to students’ college performance. The contribution lies in combining the diffusion maps and the kernel two-sample test into a unified framework and applying them to a dataset that holds educational data, which is typically processed by standard statistical tools.The proposed approach is demonstrated on a dataset that holds over 2000 student performance trajectories of Engineering students.Our findings show that there are significant distribution differences between students with or without accommodations and between students with different SAT scores.
In absence of organic remains suitable for radiocarbon dating, archaeomagnetic dating can contribute significantly to the better understanding and rescue of our past and cultural heritage. Nowadays, in the archaeological research is fundamental to obtain as much information as possible, including precise dates of the archaeological sites; indeed, the rapid expansion of urban areas implies the destruction of archaeological structures, causing the loss of knowledge about our past. In this effort, we report an archaeomagnetic and rock-magnetic study on burnt archaeological samples from two kilns discovered at the monastery of San Pelayo del Cerrato (Cevico Navero, Palencia, Spain). This was a rescue excavation where arose the possibility of studying archaeomagnetically two ovens for the manufacture of tiles and bells before their destruction due to the restoration works of the monastery. Given their suitable conditions and the lack of independent absolute dating, the aim of the study is to archaeomagnetically date their last use. The main magnetic carrier is pseudo-single domain (PSD) Ti-poor titanomagnetite, indicating that the magnetic signal is stable. The comparison of mean full-vector values (mean direction and absolute archaeointensity determinations) with the SCHA.DIF.14k model determined their last use between 1301–1391 CE (bells´ kiln) and 1295–1404 CE (tiles´ kiln) at 95% confidence level, respectively. These dates agree well with the archaeological evidence indicating that the last use of both kilns occurred almost simultaneously or closely confined in time (14th century AD). The dating resolution obtained is comparable to the radiocarbon with the benefit that archaeomagnetism dates the last use (abandonment) of the structures and no material associated to it which might be slightly younger. During the last years, the archaeomagnetic research has experienced a great development.The improvement of archaeomagnetism as a dating method as well as our knowledge of the Earth's magnetic field variations in direction and intensity for the last millennia mainly depends on our ability to obtain high-quality data from a wide suite of geological or archaeological records.With regard to archaeological remains, burnt materials such as kilns, baked clays or hearths are particularly suitable to this aim.They usually have independent dating information, are generally well preserved and reached high temperatures (>600 °C) in the past, potentially carrying a thermoremanence or TRM.Consequently, multiple secular variation (SV) curves have been published in different countries or regions worldwide.It is worth mentioning, for instance, the American southwest (e.g.: Goguitchaichvili et al., 2018; Hagstrum and Blinman, 2010; Lengyel, 2010), the Near East (e.g.: Stillinger et al., 2015) and mostly Europe covering the last 2–3 millennia (e.g.: Gómez-Paccard et al., 2006; Schnepp and Lanos, 2005; Tema et al., 2006; Gallet et al., 2002; Hervè et al., 2013; Batt et al., 2017), and occasionally even older chronologies (e.g.: Palencia-Ortas et al., 2017; Kapper et al., 2014; Carrancho et al., 2013; Tema and Kondopoulou, 2011).
In absence of organic remains suitable for radiocarbon dating, archaeomagnetic dating can contribute significantly to the better understanding and rescue of our past and cultural heritage. Nowadays, in the archaeological research is fundamental to obtain as much information as possible, including precise dates of the archaeological sites; indeed, the rapid expansion of urban areas implies the destruction of archaeological structures, causing the loss of knowledge about our past. In this effort, we report an archaeomagnetic and rock-magnetic study on burnt archaeological samples from two kilns discovered at the monastery of San Pelayo del Cerrato (Cevico Navero, Palencia, Spain). This was a rescue excavation where arose the possibility of studying archaeomagnetically two ovens for the manufacture of tiles and bells before their destruction due to the restoration works of the monastery. Given their suitable conditions and the lack of independent absolute dating, the aim of the study is to archaeomagnetically date their last use. The main magnetic carrier is pseudo-single domain (PSD) Ti-poor titanomagnetite, indicating that the magnetic signal is stable. The comparison of mean full-vector values (mean direction and absolute archaeointensity determinations) with the SCHA.DIF.14k model determined their last use between 1301–1391 CE (bells´ kiln) and 1295–1404 CE (tiles´ kiln) at 95% confidence level, respectively. These dates agree well with the archaeological evidence indicating that the last use of both kilns occurred almost simultaneously or closely confined in time (14th century AD). The dating resolution obtained is comparable to the radiocarbon with the benefit that archaeomagnetism dates the last use (abandonment) of the structures and no material associated to it which might be slightly younger. Along with these regional SV curves, the design of high-quality geomagnetic field models (e.g.: Korte et al., 2009; Pavón-Carrasco et al., 2009, 2014) represents a step forward in the development of the archaeomagnetic dating method.By “high-quality” we mean models composed exclusively of TRM data, which is by far the most efficient magnetization mechanism in nature.Provided that the material remains physically in situ, it may preserve a very stable and snapshot signal of the Earth's magnetic field direction at the time of last cooling.The intensity, however, does not necessary requires the material to be in situ.If the age of these data is known by other techniques, they can be used to construct regional PSV curves or implement geomagnetic field models, which are already being used as dating tools of fired materials in archaeology (e.g.: Carrancho et al., 2017; Casas et al., 2014; Tema et al., 2013).For that reason the latest archaeomagnetic studies are aiming to improve, update and temporarily extend these records thus expanding the potential of archaeomagnetism as a dating method (e.g.: Batt et al., 2017; Palencia-Ortas et al., 2017; Osete et al., 2016).
In this paper, a Variable-Order Fractional Single-layer Neural Network (VOFSNN) and a Variable-Order Fractional Multi-layer Neural Network (VOFMNN) are proposed to identify nonlinear systems assuming all the system states are measurable. Fractional Lyapunov-like approach and Gronwall–Bellman integral inequality are employed to prove stability and asymptotic stability conditions of the identification error dynamics. A set of novel stable learning rules for the fractional order, the hidden layer weights and the output layer weights are derived to update the proposed VOFSNN and VOFMNN parameters. The proposed methods capabilities are evaluated and confirmed by the practical data gathered from a wind turbine under operation in a wind farm. Fractional calculus, formulated 300 years ago, is basically a generalization of the well-known traditional calculus that deals with non-integer order derivatives.Although studies of the fractional calculus topic were initiated in the 17th century, it was not until the last few years when it was used for modeling of many physical phenomena with engineering applications.Fractional order derivative operator in fractional calculus acts as an effective tool for describing memory and hereditary properties of the system.In fact, fractional order-based models, in comparison with the integer order-based models, have the potential to model the behavior of complicated systems more accurately.This property is mainly due to more degrees of freedom and infinite memory which is intrinsic in fractional order calculus.Because of the mentioned properties, the fractional order models are highly successful in describing many phenomena in the various branches of science such as, health monitoring system (Leyden and Goodwine, 2018), aging material (Beltempo et al., 2018), a hydro-turbine system (Xu et al., 2017), biological tissues (Magin, 2010), diffusion phenomena (Pintelon et al., 2005), Viscoelasticity (Soczkiewicz, 2002) and numerous of other examples in various fields.
In this paper, a Variable-Order Fractional Single-layer Neural Network (VOFSNN) and a Variable-Order Fractional Multi-layer Neural Network (VOFMNN) are proposed to identify nonlinear systems assuming all the system states are measurable. Fractional Lyapunov-like approach and Gronwall–Bellman integral inequality are employed to prove stability and asymptotic stability conditions of the identification error dynamics. A set of novel stable learning rules for the fractional order, the hidden layer weights and the output layer weights are derived to update the proposed VOFSNN and VOFMNN parameters. The proposed methods capabilities are evaluated and confirmed by the practical data gathered from a wind turbine under operation in a wind farm. Due to universal approximation capability, neural network-based method turns out to be a powerful instrument to identify a broad diversity of complex nonlinear systems by training, where we have no important a priori knowledge about their structures and parameters.Neural networks are usually classified into two types, namely, static neural networks and dynamic neural networks.In cases, where there is a high dependency between the data value at the present time and its past values (either input or output past values), a larger static neural structure must be used in order to map the input data to the output data, which is a drawback from the computational point of view and risk of getting trapped into the local minima.In these cases, dynamic neural networks can successfully overcome many problems associated with the static neural network due to using feedback in its structure (Yazdizadeh and Khorasani, 2002).Moreover, representation capability is essential in every application especially while dealing with dynamic systems.Neural networks may possess a dynamic input–output representation in three main categories.The first category uses a tapped delay line along with a static neural network to create dynamics in the structure (Narendra and Parthasarathy, 1990).The second category is recurrent neural networks.In this category, a dynamic input–output representation is obtained using a recurrent structure (Nejadmorad Moghanloo et al., 2015).The third category of the networks benefits from using Dynamic Neurons Units (DNUs) (Yazdizadeh et al., 2000).The DNUs not only receive external inputs but also receive state feedback signals from themselves and other neurons as well.The famous Hopfield neural network introduced in 1984 may be considered both as a neural network with DNU (Gupta et al., 2004) and as a recurrent dynamic neural network due to its single-layer structure (Narendra and Parthasarathy, 1990).Some dynamic neural networks have no hidden layer and this can lead to a limitation in approximation capabilities of such networks (Haykin, 1994).In (Albertini and Sontag, 1994; Deng, 2013; Lu and Cao, 2007; Poznyak et al., 2001; Rovithakis and Christodoulou, 1994; Yu et al., 2007) an Integer Order Neural Network (IONN) with both single layer and multilayer has been proposed to identify and control various systems.
In this paper, a Variable-Order Fractional Single-layer Neural Network (VOFSNN) and a Variable-Order Fractional Multi-layer Neural Network (VOFMNN) are proposed to identify nonlinear systems assuming all the system states are measurable. Fractional Lyapunov-like approach and Gronwall–Bellman integral inequality are employed to prove stability and asymptotic stability conditions of the identification error dynamics. A set of novel stable learning rules for the fractional order, the hidden layer weights and the output layer weights are derived to update the proposed VOFSNN and VOFMNN parameters. The proposed methods capabilities are evaluated and confirmed by the practical data gathered from a wind turbine under operation in a wind farm. By employing the fractional calculus in a dynamic neural network, the Fractional Order Neural Network (FONN) is created that gives us a prominent ground for studying complicated systems both from the theoretical and practical point of views.Thus, the FONN has just attracted increasing interest of many researchers.Among FONNs, we may refer to the fractional order Hopfield dynamic neural network that can be interpreted by replacing the common capacitor with reactance (Boroomand and Menhaj, 2009a).The formulation and the numerical simulations of the FONN have been carried out by many authors (Arena et al., 2000; Boroomand and Menhaj, 2009a; Matsuzaki and Nakagawa, 2003; Petras, 2006).The study of dynamic behaviors of the FONN such as bifurcation (Huang et al., 2017, 2018), stability (Liu et al., 2017), stabilization (Wu and Zeng, 2017), synchronization (Zhang et al., 2018), etc., are important topics which have been under study in the recent years in many papers and the references cited therein.
In this paper, a Variable-Order Fractional Single-layer Neural Network (VOFSNN) and a Variable-Order Fractional Multi-layer Neural Network (VOFMNN) are proposed to identify nonlinear systems assuming all the system states are measurable. Fractional Lyapunov-like approach and Gronwall–Bellman integral inequality are employed to prove stability and asymptotic stability conditions of the identification error dynamics. A set of novel stable learning rules for the fractional order, the hidden layer weights and the output layer weights are derived to update the proposed VOFSNN and VOFMNN parameters. The proposed methods capabilities are evaluated and confirmed by the practical data gathered from a wind turbine under operation in a wind farm. Few researchers have focused on identifying the various plants with inherent fractional order behavior as the fractional order model (Aslipour and Yazdizadeh, 2018).They have employed two different neural network structures namely the static neural network (Benoit-Marand et al., 2006; Rahmani and Farrokhi, 2017, 2018; Sierociuk et al., 2011), and the FONN (Aslipour and Yazdizadeh, 2018; Boroomand and Menhaj, 2009b, c).In Benoit-Marand et al. (2006), the authors identified a fractional order nonlinear system via a static neural network.The static physical part of the systems is modeled by using a static neural network and its dynamic part is identified by employing a fractional order integral block.In (Rahmani and Farrokhi, 2017, 2018), identification of a nonlinear dynamic system using neuro-fractional Hammerstein model are presented.The authors of this paper have considered a model that consists of a static neural network as the nonlinear subsystem and the fractional order system as the linear subsystem.We may also refer to Sierociuk et al. (2011) in which a neural network has been introduced for modeling and identification of a fractional order nonlinear system.This identifier is a combination of a static neural network and a linear fractional state-space model.Levenberg Marquardt and Backpropagation algorithm have been used by the authors to train the proposed neural network.Boroomand and Menhaj (2009b, c) attempted to solve the parameter identification problem via the FONN with the known fractional order.The authors in Aslipour and Yazdizadeh (2018) identified a nonlinear system by using the FONN where its fractional order is also updated via a Particle Swarm Optimization (PSO) algorithm.
In this paper, a Variable-Order Fractional Single-layer Neural Network (VOFSNN) and a Variable-Order Fractional Multi-layer Neural Network (VOFMNN) are proposed to identify nonlinear systems assuming all the system states are measurable. Fractional Lyapunov-like approach and Gronwall–Bellman integral inequality are employed to prove stability and asymptotic stability conditions of the identification error dynamics. A set of novel stable learning rules for the fractional order, the hidden layer weights and the output layer weights are derived to update the proposed VOFSNN and VOFMNN parameters. The proposed methods capabilities are evaluated and confirmed by the practical data gathered from a wind turbine under operation in a wind farm. Motivated by the above literature, this paper presents an identification method based on the FONN to model the behavior of the complicated systems that are inherently nonlinear, dynamic and fractional order systems.Different from the above-studied structures, the new proposed FONN is a general case of the FONN that leads to more precise identification of a big enough class of nonlinear dynamic systems.Similar to many other published papers in the field, it is assumed that all system states are measurable and the number of the states of the FONN is selected to be equal to the dimension of the system (Poznyak et al., 2001).We propose two structures, namely, the Variable-Order Fractional Single-layer Neural Network (VOFSNN) and the Variable-Order Fractional Multi-layer Neural Network (VOFMNN).All the associated parameters including the weights and the fractional order are updated based on the newly derived learning rules.To derive the learning rules and in order to analyze the error stability conditions, we use the fractional Lyapunov-like approach and some useful lemmas.Although in Aslipour and Yazdizadeh (2018), as the only published paper in this field to the best of our knowledge, the fractional order is adjusted via PSO, it is worth mentioning that the proposed method in Aslipour and Yazdizadeh (2018) is not a Lyapunov-like method and in this regard the proposed structure and theory in this paper is unique.
In this paper, a Variable-Order Fractional Single-layer Neural Network (VOFSNN) and a Variable-Order Fractional Multi-layer Neural Network (VOFMNN) are proposed to identify nonlinear systems assuming all the system states are measurable. Fractional Lyapunov-like approach and Gronwall–Bellman integral inequality are employed to prove stability and asymptotic stability conditions of the identification error dynamics. A set of novel stable learning rules for the fractional order, the hidden layer weights and the output layer weights are derived to update the proposed VOFSNN and VOFMNN parameters. The proposed methods capabilities are evaluated and confirmed by the practical data gathered from a wind turbine under operation in a wind farm. In order to show the merit of the proposed method, after applying to many numerical examples, it was applied to the practical data which was gathered from a wind farm.The capability of the proposed method for solving identification problems in real physical complicated plants due to their changing characteristics which are coming from the profile of the wind speed and many other complexities including flexibility of the structure of the tower and the blades is shown.
In this paper, a Variable-Order Fractional Single-layer Neural Network (VOFSNN) and a Variable-Order Fractional Multi-layer Neural Network (VOFMNN) are proposed to identify nonlinear systems assuming all the system states are measurable. Fractional Lyapunov-like approach and Gronwall–Bellman integral inequality are employed to prove stability and asymptotic stability conditions of the identification error dynamics. A set of novel stable learning rules for the fractional order, the hidden layer weights and the output layer weights are derived to update the proposed VOFSNN and VOFMNN parameters. The proposed methods capabilities are evaluated and confirmed by the practical data gathered from a wind turbine under operation in a wind farm. The main advantages and contributions of this paper may be summarized as follows:
The Neolithic painted pottery of northwest China has long been admired for its high level of craftsmanship. Yet, little is known about the technological processes and potting communities behind these objects. At the same time, the wide variety of supposedly less “beautiful” Bronze Age wares is often disregarded and simply ascribed to the emergence of multiple new cultures. In both cases, the relationship between object appearance, technology, and cultural expectations is unexplored. The present paper presents the first results of a pilot study using a combination of scientific techniques to learn about traditions of ceramic production and their transformation over time and space in prehistoric northwest China. The basis of this study is finds excavated in the 1920s and held in the Museum of Far Eastern Antiquities in Stockholm. This collection has long lain dormant and their potential remains largely unexplored. This paper draws attention to the collection and at the same time shows the usefulness of combining thin-section petrography and portable X-ray fluorescence for this specific set of material and research questions. This analysis of a small sample already provides important insights. For instance, it shows continuity in criteria of raw material selection during the Neolithic but a radical break in tempering behavior at the transition from early to late Bronze Age. The study also identifies technical challenges as well as possibilities posed by the quality of the local raw material in conjunction with long-standing traditions of high-level local craftsmanship. All of these phenomena, so the paper shows, are best investigated with a combination of petrographic and chemical analyses on archaeological and geological samples viewed in a comparative perspective. Ceramics are one of the main types of evidence that archaeologists work with on a daily basis.Due to their ubiquity, durability, and chronological sensitivity, even in the age of scientific dating methods ceramics still form the basis of many chronological frameworks, especially in Chinese archaeology (Hein, 2016).This tendency to define archaeological cultures based on ceramic types is highly problematic, for the relationship between ceramics and past identity groups is by no means straightforward.Ceramics form a complex entity shaped by the nature of the raw material, the available technology, the abilities and preferences of the potter as well as their ideas of what a specific vessel should look like.These ideas, in turn, are shaped by the users and observers who engage with these wares, all of them acting within various overlapping communities of practice (Wenger, 1998).Research on early ceramics aims to understand these communities and their interplay – both locally and across space and time.Such insights are made possible by the nature of ceramics; they preserve traces of the various production stages, thus allowing for the reconstruction of the chaînes opératoires that created them.Furthermore, individual vessels can be compared to each other and fitted within larger assemblages to reconstruct whole craft traditions (Stilborg, 2017: 658–9).
The Neolithic painted pottery of northwest China has long been admired for its high level of craftsmanship. Yet, little is known about the technological processes and potting communities behind these objects. At the same time, the wide variety of supposedly less “beautiful” Bronze Age wares is often disregarded and simply ascribed to the emergence of multiple new cultures. In both cases, the relationship between object appearance, technology, and cultural expectations is unexplored. The present paper presents the first results of a pilot study using a combination of scientific techniques to learn about traditions of ceramic production and their transformation over time and space in prehistoric northwest China. The basis of this study is finds excavated in the 1920s and held in the Museum of Far Eastern Antiquities in Stockholm. This collection has long lain dormant and their potential remains largely unexplored. This paper draws attention to the collection and at the same time shows the usefulness of combining thin-section petrography and portable X-ray fluorescence for this specific set of material and research questions. This analysis of a small sample already provides important insights. For instance, it shows continuity in criteria of raw material selection during the Neolithic but a radical break in tempering behavior at the transition from early to late Bronze Age. The study also identifies technical challenges as well as possibilities posed by the quality of the local raw material in conjunction with long-standing traditions of high-level local craftsmanship. All of these phenomena, so the paper shows, are best investigated with a combination of petrographic and chemical analyses on archaeological and geological samples viewed in a comparative perspective. In the research area of the present study, northwest China, the traditional identification of ceramics with cultures still goes largely unchallenged, something that the authors address in this paper by focusing on communities of practice, more specifically potting communities and craft traditions without assuming that these can be equated with larger cultural groups.The area is thought of as an exchange corridor between China and Central Asia, active from the Neolithic onwards and has therefore recently become a focus of scientific research on human movement and dissemination of new technologies and subsistence systems (Atahan et al., 2011; Flad et al., 2010).Northwest China is thus of interest as a case study for issues of culture contact and long-distance interaction.However, in such big-picture debates the role of local developments is often overlooked.The present study focuses on local changes in ceramic production as an indicator for both local social changes and larger regional and super-regional trends.These patterns are reflected in the appearance of painted pottery over large parts of northern China during the Neolithic period as well as its subsequent disappearance.
The Neolithic painted pottery of northwest China has long been admired for its high level of craftsmanship. Yet, little is known about the technological processes and potting communities behind these objects. At the same time, the wide variety of supposedly less “beautiful” Bronze Age wares is often disregarded and simply ascribed to the emergence of multiple new cultures. In both cases, the relationship between object appearance, technology, and cultural expectations is unexplored. The present paper presents the first results of a pilot study using a combination of scientific techniques to learn about traditions of ceramic production and their transformation over time and space in prehistoric northwest China. The basis of this study is finds excavated in the 1920s and held in the Museum of Far Eastern Antiquities in Stockholm. This collection has long lain dormant and their potential remains largely unexplored. This paper draws attention to the collection and at the same time shows the usefulness of combining thin-section petrography and portable X-ray fluorescence for this specific set of material and research questions. This analysis of a small sample already provides important insights. For instance, it shows continuity in criteria of raw material selection during the Neolithic but a radical break in tempering behavior at the transition from early to late Bronze Age. The study also identifies technical challenges as well as possibilities posed by the quality of the local raw material in conjunction with long-standing traditions of high-level local craftsmanship. All of these phenomena, so the paper shows, are best investigated with a combination of petrographic and chemical analyses on archaeological and geological samples viewed in a comparative perspective. Neolithic painted pottery from northwest China associated with the Majiayao “culture” or period (c. 3200–2000 BCE) has long been admired for its high level of craftsmanship and aesthetic appeal, but little is known about the complex interplay of human actions leading to their production and dissemination.Based on similarity in object forms, decoration motives, and smooth reddish surfaces, painted vessels are usually seen as part of the same ceramic tradition, but it is by no means certain if the techniques used in their production are unchanged over time and space.Similarly, the subsequent Qijia period (c. 2300–1500 BCE) is characterized by the occurrence of similar ceramic forms and decorative motives over a wide area usually ascribed to a shared cultural identity (Fitzgerald-Huber, 2003).Conversely, the diversification of ceramic forms from the early 2nd mill.BC onward is interpreted as the sudden appearance of many new “cultures”, most importantly Xindian (c. 1600–700 BCE), Siwa (c. 1600–700 BCE), and Kayue (c. 1600–400 BCE) (Shui, 2001); however, the nature of the human actions behind these ceramic types is unclear and the associated dates are highly contested (Fiskesjö and Chen, 2004).
The Neolithic painted pottery of northwest China has long been admired for its high level of craftsmanship. Yet, little is known about the technological processes and potting communities behind these objects. At the same time, the wide variety of supposedly less “beautiful” Bronze Age wares is often disregarded and simply ascribed to the emergence of multiple new cultures. In both cases, the relationship between object appearance, technology, and cultural expectations is unexplored. The present paper presents the first results of a pilot study using a combination of scientific techniques to learn about traditions of ceramic production and their transformation over time and space in prehistoric northwest China. The basis of this study is finds excavated in the 1920s and held in the Museum of Far Eastern Antiquities in Stockholm. This collection has long lain dormant and their potential remains largely unexplored. This paper draws attention to the collection and at the same time shows the usefulness of combining thin-section petrography and portable X-ray fluorescence for this specific set of material and research questions. This analysis of a small sample already provides important insights. For instance, it shows continuity in criteria of raw material selection during the Neolithic but a radical break in tempering behavior at the transition from early to late Bronze Age. The study also identifies technical challenges as well as possibilities posed by the quality of the local raw material in conjunction with long-standing traditions of high-level local craftsmanship. All of these phenomena, so the paper shows, are best investigated with a combination of petrographic and chemical analyses on archaeological and geological samples viewed in a comparative perspective. For all three periods, the relationship between object appearance, technology, cultural expectations, and social context of production and consumption are largely unexplored.For the Majiayao-style painted wares, based on their high quality and occurrence over wide regions, it has been suggested that they may have been prestige goods sometimes exchanged between elites over long distances (Hong et al., 2012).Evidence for such long-distance connections, however, is limited and contested (Ren et al., 2013), and so far, there is no evidence for the presence of production centers that may have distributed such wares.Mortuary remains furthermore suggest that access to these vessels was by no means restricted by social rank (Hung, 2011: 237–8), suggesting that the communities who used these wares may not have been strongly stratified (Allard, 2001).
The Neolithic painted pottery of northwest China has long been admired for its high level of craftsmanship. Yet, little is known about the technological processes and potting communities behind these objects. At the same time, the wide variety of supposedly less “beautiful” Bronze Age wares is often disregarded and simply ascribed to the emergence of multiple new cultures. In both cases, the relationship between object appearance, technology, and cultural expectations is unexplored. The present paper presents the first results of a pilot study using a combination of scientific techniques to learn about traditions of ceramic production and their transformation over time and space in prehistoric northwest China. The basis of this study is finds excavated in the 1920s and held in the Museum of Far Eastern Antiquities in Stockholm. This collection has long lain dormant and their potential remains largely unexplored. This paper draws attention to the collection and at the same time shows the usefulness of combining thin-section petrography and portable X-ray fluorescence for this specific set of material and research questions. This analysis of a small sample already provides important insights. For instance, it shows continuity in criteria of raw material selection during the Neolithic but a radical break in tempering behavior at the transition from early to late Bronze Age. The study also identifies technical challenges as well as possibilities posed by the quality of the local raw material in conjunction with long-standing traditions of high-level local craftsmanship. All of these phenomena, so the paper shows, are best investigated with a combination of petrographic and chemical analyses on archaeological and geological samples viewed in a comparative perspective. In contrast, for the early Bronze Age sites where Qijia-style wares were found, there is no clear evidence for short- or long-distance exchange of ceramic wares, but some graves contain a limited number of metal and jade objects pointing to long-distance exchange networks among a small subsection of the societies in question (Womack, 2017: 237).The ceramics in both graves and settlements are comparatively plain, and yet, similar forms appear over a relatively large area, though not as large as for the distribution of Majiayao-style painted wares.For both periods, there is robust evidence for a sedentary lifestyle based on millet-dominated agriculture and domesticated animals (esp.pig) (Jia et al., 2013).
The Neolithic painted pottery of northwest China has long been admired for its high level of craftsmanship. Yet, little is known about the technological processes and potting communities behind these objects. At the same time, the wide variety of supposedly less “beautiful” Bronze Age wares is often disregarded and simply ascribed to the emergence of multiple new cultures. In both cases, the relationship between object appearance, technology, and cultural expectations is unexplored. The present paper presents the first results of a pilot study using a combination of scientific techniques to learn about traditions of ceramic production and their transformation over time and space in prehistoric northwest China. The basis of this study is finds excavated in the 1920s and held in the Museum of Far Eastern Antiquities in Stockholm. This collection has long lain dormant and their potential remains largely unexplored. This paper draws attention to the collection and at the same time shows the usefulness of combining thin-section petrography and portable X-ray fluorescence for this specific set of material and research questions. This analysis of a small sample already provides important insights. For instance, it shows continuity in criteria of raw material selection during the Neolithic but a radical break in tempering behavior at the transition from early to late Bronze Age. The study also identifies technical challenges as well as possibilities posed by the quality of the local raw material in conjunction with long-standing traditions of high-level local craftsmanship. All of these phenomena, so the paper shows, are best investigated with a combination of petrographic and chemical analyses on archaeological and geological samples viewed in a comparative perspective. For the subsequent period, the middle Bronze Age, hardly anything is known about social structure or subsistence practices of the various communities distinguished by widely differing ceramic traditions.As some graves with Siwa- and Kayue-style ceramics have furnished considerable amounts of bronze items while others held none, it has been suggested that distribution of wealth and status was uneven, while no similar evidence can be quoted for graves with Xindian-style wares (Mao et al., 2014; Pak, 1995; Shui, 2001).Considering the small number of sites that these claims are based on, however, these results can at best be seen as preliminary.The same applies to suggestions for a shift to a pastoral economy for some or all of these groups due to climate change (An et al., 2005; Wu and Liu, 2004).These sweeping claims based on environmental modelling are contradicted by the – admittedly rather sparse – zooarchaeological and palaeobotanical evidence.Some sites with Xindian-type ceramics show evidence for settled farming, others for hunting activities, others again for a strong reliance on sheep rearing (Shui, 2001).For sites associated with Kayue- or Siwa-style wares, a combination of animal herding and millet-based agriculture seems to have been common, but again with a varying importance of the different food sources by site (Pak, 1995: 292–317; Shui, 2001).While for the Hexi corridor and various parts of northern China, recent stable isotope research on human bones has helped immensely in reconstructing ancient diets (e.g., Atahan et al., 2011), no similar material is available for sites containing the types of ceramics discussed here.Likewise, very little faunal or floral evidence is available, simply because many of the sites in question have been excavated decades before environmental sampling became international standard.Thus, further excavation is necessary before more definite suggestions on the social structure and subsistence systems of these communities in question can be made.
The Neolithic painted pottery of northwest China has long been admired for its high level of craftsmanship. Yet, little is known about the technological processes and potting communities behind these objects. At the same time, the wide variety of supposedly less “beautiful” Bronze Age wares is often disregarded and simply ascribed to the emergence of multiple new cultures. In both cases, the relationship between object appearance, technology, and cultural expectations is unexplored. The present paper presents the first results of a pilot study using a combination of scientific techniques to learn about traditions of ceramic production and their transformation over time and space in prehistoric northwest China. The basis of this study is finds excavated in the 1920s and held in the Museum of Far Eastern Antiquities in Stockholm. This collection has long lain dormant and their potential remains largely unexplored. This paper draws attention to the collection and at the same time shows the usefulness of combining thin-section petrography and portable X-ray fluorescence for this specific set of material and research questions. This analysis of a small sample already provides important insights. For instance, it shows continuity in criteria of raw material selection during the Neolithic but a radical break in tempering behavior at the transition from early to late Bronze Age. The study also identifies technical challenges as well as possibilities posed by the quality of the local raw material in conjunction with long-standing traditions of high-level local craftsmanship. All of these phenomena, so the paper shows, are best investigated with a combination of petrographic and chemical analyses on archaeological and geological samples viewed in a comparative perspective. As fieldwork, especially on the scale needed here, takes considerable resources, it will take time before we gain deeper insights into the life of the groups in question through that avenue.What is already abundantly available, however, is ceramic material, so this is our chosen dataset for this paper.Most research to date has focused on typochronologies and explanatory models for the connections between the consumers of wares of the same style, while very little attention has been paid to the producers and their craft.Ceramics make up a particular kind of objects whose form, material composition, and traces of craftsmanship combined with their distribution in time and space can throw light on the organization of production and handicraft traditions, as well as their transmission over time and space.When viewed in context, research on ceramic production traditions furthermore provides insights into the embeddedness of these processes in the local environment as well as in social structures.
The Neolithic painted pottery of northwest China has long been admired for its high level of craftsmanship. Yet, little is known about the technological processes and potting communities behind these objects. At the same time, the wide variety of supposedly less “beautiful” Bronze Age wares is often disregarded and simply ascribed to the emergence of multiple new cultures. In both cases, the relationship between object appearance, technology, and cultural expectations is unexplored. The present paper presents the first results of a pilot study using a combination of scientific techniques to learn about traditions of ceramic production and their transformation over time and space in prehistoric northwest China. The basis of this study is finds excavated in the 1920s and held in the Museum of Far Eastern Antiquities in Stockholm. This collection has long lain dormant and their potential remains largely unexplored. This paper draws attention to the collection and at the same time shows the usefulness of combining thin-section petrography and portable X-ray fluorescence for this specific set of material and research questions. This analysis of a small sample already provides important insights. For instance, it shows continuity in criteria of raw material selection during the Neolithic but a radical break in tempering behavior at the transition from early to late Bronze Age. The study also identifies technical challenges as well as possibilities posed by the quality of the local raw material in conjunction with long-standing traditions of high-level local craftsmanship. All of these phenomena, so the paper shows, are best investigated with a combination of petrographic and chemical analyses on archaeological and geological samples viewed in a comparative perspective. The present project therefore provides a novel perspective on the cultural developments in prehistoric northwest China.Rather than a focus on burial customs, subsistence systems, or form typology alone, we study the craft-technological relationships in the prehistoric pottery from the region as an embedded expression of past human communities, their actions and interactions both with each other and the environment.To achieve this we use a combination of scientific techniques.
The Neolithic painted pottery of northwest China has long been admired for its high level of craftsmanship. Yet, little is known about the technological processes and potting communities behind these objects. At the same time, the wide variety of supposedly less “beautiful” Bronze Age wares is often disregarded and simply ascribed to the emergence of multiple new cultures. In both cases, the relationship between object appearance, technology, and cultural expectations is unexplored. The present paper presents the first results of a pilot study using a combination of scientific techniques to learn about traditions of ceramic production and their transformation over time and space in prehistoric northwest China. The basis of this study is finds excavated in the 1920s and held in the Museum of Far Eastern Antiquities in Stockholm. This collection has long lain dormant and their potential remains largely unexplored. This paper draws attention to the collection and at the same time shows the usefulness of combining thin-section petrography and portable X-ray fluorescence for this specific set of material and research questions. This analysis of a small sample already provides important insights. For instance, it shows continuity in criteria of raw material selection during the Neolithic but a radical break in tempering behavior at the transition from early to late Bronze Age. The study also identifies technical challenges as well as possibilities posed by the quality of the local raw material in conjunction with long-standing traditions of high-level local craftsmanship. All of these phenomena, so the paper shows, are best investigated with a combination of petrographic and chemical analyses on archaeological and geological samples viewed in a comparative perspective. A question of this magnitude applied to an area as extensive as northwest China and a period stretching from c. 3300 to 1000 BCE can only be addressed through long-term international collaboration, an undertaking which is under way between the University of Oxford, Stockholm University, the Museum of Far Eastern Antiquities (MFEA) in Stockholm, the Gansu Provincial Institute of Archaeology, and Beijing University.As a pilot project, we began with an under-studied collection, the early Chinese ceramics excavated by Johan Gunnar Andersson in the 1920 in Gansu, part of which are now held in the MFEA.
The Neolithic painted pottery of northwest China has long been admired for its high level of craftsmanship. Yet, little is known about the technological processes and potting communities behind these objects. At the same time, the wide variety of supposedly less “beautiful” Bronze Age wares is often disregarded and simply ascribed to the emergence of multiple new cultures. In both cases, the relationship between object appearance, technology, and cultural expectations is unexplored. The present paper presents the first results of a pilot study using a combination of scientific techniques to learn about traditions of ceramic production and their transformation over time and space in prehistoric northwest China. The basis of this study is finds excavated in the 1920s and held in the Museum of Far Eastern Antiquities in Stockholm. This collection has long lain dormant and their potential remains largely unexplored. This paper draws attention to the collection and at the same time shows the usefulness of combining thin-section petrography and portable X-ray fluorescence for this specific set of material and research questions. This analysis of a small sample already provides important insights. For instance, it shows continuity in criteria of raw material selection during the Neolithic but a radical break in tempering behavior at the transition from early to late Bronze Age. The study also identifies technical challenges as well as possibilities posed by the quality of the local raw material in conjunction with long-standing traditions of high-level local craftsmanship. All of these phenomena, so the paper shows, are best investigated with a combination of petrographic and chemical analyses on archaeological and geological samples viewed in a comparative perspective. As described above, so far ceramic analyses of this material are few, and scientific analysis of ceramics is not well established anywhere in China.While portable X-ray fluorescence (P-ED-XRF) analysis has been employed on occasion, thin-section analysis is not commonly applied to material from anywhere in China, let alone prehistoric ceramics from the loess plateau, so this pilot study aims to ascertain whether this is a viable approach to this specific body of material.
Finding a person across a camera network plays an important role in video surveillance. For a real-world person re-identification application, in order to guarantee an optimal time response, it is crucial to find the balance between accuracy and speed. We analyse this trade-off, comparing a classical method, that comprises hand-crafted feature description and metric learning, in particular, LOMO and XQDA, to deep learning based techniques, using image classification networks, ResNet and MobileNets. Additionally, we propose and analyse network distillation as a learning strategy to reduce the computational cost of the deep learning approach at test time. We evaluate both methods on the Market-1501 and DukeMTMC-reID large-scale datasets, showing that distillation helps reducing the computational cost at inference time while even increasing the accuracy performance. Person re-identification refers to the problem of identifying a person of interest across a camera network (Zheng et al., 2017a; Panda et al., 2017).This task is specially important in surveillance applications, since nowadays the security systems in public areas such as airports, train stations or crowded city areas, are continuously improving to ensure the population’s welfare.In big cities, there are extensive networks of cameras in the most sensitive locations.Identifying an individual requires finding it among all the instances that are present on the collection of images captured by the cameras.These images show usually complex crowded scenes, thus increasing even more the computational complexity of the problem.Therefore, the automation of this task that involves large-scale data becomes essential, as otherwise it would be a laborious task to be performed by humans.
Finding a person across a camera network plays an important role in video surveillance. For a real-world person re-identification application, in order to guarantee an optimal time response, it is crucial to find the balance between accuracy and speed. We analyse this trade-off, comparing a classical method, that comprises hand-crafted feature description and metric learning, in particular, LOMO and XQDA, to deep learning based techniques, using image classification networks, ResNet and MobileNets. Additionally, we propose and analyse network distillation as a learning strategy to reduce the computational cost of the deep learning approach at test time. We evaluate both methods on the Market-1501 and DukeMTMC-reID large-scale datasets, showing that distillation helps reducing the computational cost at inference time while even increasing the accuracy performance. The aim of person re-identification is to find a person of interest, also referred as query, across a gallery of images.The difficulty of this problem lies in the fact that the images are subject to variations in the point of view, person pose, light conditions and occlusions.Fig. 4 shows examples of gallery images for identities with such kind of variability.Fig. 1 shows the full person re-identification system, including the previous person detection stage.In the person re-identification module, a query image of a person of interest is compared against the gallery, retrieving the images that correspond to the same identity.To compare them, the system first extracts a feature representation that describes every image, either by using a hand-crafted descriptor or a deep neural network.Usually the features of the gallery are previously computed offline and stored, so that at test time we only have to extract the features for the query image.Once the features are extracted, they can be compared with the features of the gallery by computing a similarity measure.Finally, all the gallery images are ordered by the degree of similarity, obtaining a ranked list of the most similar images in the gallery to the person of interest (Zhong et al., 2017).
Finding a person across a camera network plays an important role in video surveillance. For a real-world person re-identification application, in order to guarantee an optimal time response, it is crucial to find the balance between accuracy and speed. We analyse this trade-off, comparing a classical method, that comprises hand-crafted feature description and metric learning, in particular, LOMO and XQDA, to deep learning based techniques, using image classification networks, ResNet and MobileNets. Additionally, we propose and analyse network distillation as a learning strategy to reduce the computational cost of the deep learning approach at test time. We evaluate both methods on the Market-1501 and DukeMTMC-reID large-scale datasets, showing that distillation helps reducing the computational cost at inference time while even increasing the accuracy performance. In real scenarios, in order to have a feasible application that is able to work with large-scale datasets in an efficient and effective way, we have to address the problem of optimizing the computational cost of the system at test time, without decreasing drastically its accuracy.For that purpose, we consider both classical and deep learning based person re-identification methods.Although deep learning based techniques outperform significantly hand-crafted methods in terms of accuracy, their drawback is that they require dedicated hardware, i.e. GPUs, and big amounts of data for training, which takes usually long periods of time, i.e. weeks, in order to be effective.
In this study, we evaluate various Convolutional Neural Networks based Super-Resolution (SR) models to improve facial areas detection in thermal images. In particular, we analyze the influence of selected spatiotemporal properties of thermal image sequences on detection accuracy. For this purpose, a thermal face database was acquired for 40 volunteers. Contrary to most of existing thermal databases of faces, we publish our dataset in a raw, original format (14-bit depth) to preserve all important details. In our experiments, we utilize two metrics usually used for image enhancement evaluation: Peak-Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Metric (SSIM). In addition, we present how to design a SR network with a widened receptive field to mitigate the problem of contextual information being spread over larger image regions due to the heat flow in thermal images. Finally, we determine whether there is a relation between achieved PSNR and accuracy of facial areas detection that can be analyzed for vital signs extraction (e.g. nostril region). The performed evaluation showed that PSNR can be improved even by 60% if full bit depth resolution data is used instead of 8 bits. Also, we showed that the application of image enhancement solution is necessary for low resolution images to achieve a satisfactory accuracy of object detection. High resolution (HR) image restoration from corresponding low resolution (LR) data is known as image super resolution (SR).Specifically, if a single image is used for the enhancement, the approach is called single image super resolution (SISR).Inherently, this problem is ill-posed since it is possible to recover various HR outputs for a single LR input.Such inverse problem is usually solved by utilizing the prior knowledge.The prior knowledge can be learned by predicting a pixel value with interpolation methods, e.g. bicubic interpolation (Keys, 1981), edge-guided interpolation (Zhang and Wu, 2006), or adaptive non local sparsity-based modeling (Romano et al., 2014).Another ways to acquire the knowledge is to exploit the internal structure of pixels within the same LR image (Glasner et al., 2009; Freedman and Fattal, 2011; Cui et al., 2014), or learn it from corresponding pairs of LR and HR examples, i.e. example-based algorithms (Chang et al., 2004; Kim and Kwon, 2010; Bevilacqua et al., 2012; Jia et al., 2013; Dong et al., 2016; Kim et al., 2016a, b; Tai et al., 2017; Liu et al., 2017).The first group, known as interpolation-based SISR (Li and Orchard, 2001), often intend to mitigate a down-sampling process only.Also, the interpolation techniques are based on generic smoothness priors and therefore are indiscriminate, as they smooth both edges and object parts, what leads to the blurring effect (Chang et al., 2004).
In this study, we evaluate various Convolutional Neural Networks based Super-Resolution (SR) models to improve facial areas detection in thermal images. In particular, we analyze the influence of selected spatiotemporal properties of thermal image sequences on detection accuracy. For this purpose, a thermal face database was acquired for 40 volunteers. Contrary to most of existing thermal databases of faces, we publish our dataset in a raw, original format (14-bit depth) to preserve all important details. In our experiments, we utilize two metrics usually used for image enhancement evaluation: Peak-Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Metric (SSIM). In addition, we present how to design a SR network with a widened receptive field to mitigate the problem of contextual information being spread over larger image regions due to the heat flow in thermal images. Finally, we determine whether there is a relation between achieved PSNR and accuracy of facial areas detection that can be analyzed for vital signs extraction (e.g. nostril region). The performed evaluation showed that PSNR can be improved even by 60% if full bit depth resolution data is used instead of 8 bits. Also, we showed that the application of image enhancement solution is necessary for low resolution images to achieve a satisfactory accuracy of object detection. Hence, due to limited applicability of interpolation approaches, the learning-based methods are becoming more popular and are being further investigated, e.g. by combining learning based gradient regularization with reconstruction approach that aims at preserving consistency between HR and LR images, while satisfying the prior knowledge (Chen et al., 2017).In particular, deep learning based SR algorithms, which allow to establish the mapping between LR and HR patches of the image using a stack of convolutional operations have recently become state-of-the-art solutions.Majority of the conducted work considered visible light images only.The pioneer research of applying deep learning to SISR problem for RGB data, conducted by Jain and Seung (2009), aimed at image denoising.In later studies, stacked collaborative local auto-encoders were proved to be successful for a low resolution RGB images up-scaling (Cui et al., 2014).At each stacked layer, high frequency components are enhanced using similarity search applied to the input LR image, that are then fed to auto-encoders in order to suppress the noise and take into account the correspondence of the overlapping reconstructed patches.To overcome the disadvantage of independent optimization of the similarity search and the auto-encoder for each layer, as well as the absence of steps other than the learning part in the framework pipeline, Dong et al. proposed to formulate both mapping and feature extraction as convolutional operations (Dong et al., 2016).As a result, proposed SR pipeline, called SR Convolutional Neural Network (SRCNN), can be fully obtained through end to end mapping.Additional modifications introduced to the SRCNN allowed to further improve the Peak Signal-to-Noise Ratio (PSNR) index, a metric usually used for quantitative evaluation of super-resolution algorithms.Kim et al. at first introduced Very Deep Super Resolution (VDSR) model with residual connections that correlate LR input with HR output, outperforming SRCNN by 0.87 dB PSNR (Kim et al., 2016a) on RGB dataset called Set5 (Bevilacqua et al., 2012) downscaled by a factor of 2.Later same authors proposed Deeply Recursive Convolutional Network (DRCN) (Kim et al., 2016b), which incorporates recursive supervision to (1) increase the depth while keeping number of parameters constant; (2) eliminate the vanishing gradient problem.Afterwards, Tai et al. (2017) further enhanced the efficiency with Deep Recursive Residual Network (DRRN) that adopts weight-shared residual connections both in global and local manner to increase the network depth, achieving PSNR 1.08 dB higher than SRCNN on RGB data.Generative Adversarial Network (GAN) based solutions have been also already applied to visible light image enhancement, allowing for successful restoration of high frequency details (e.g. SRGAN Ledig et al., 2017 or EnhanceNet Sajjadi et al., 2017).
In this study, we evaluate various Convolutional Neural Networks based Super-Resolution (SR) models to improve facial areas detection in thermal images. In particular, we analyze the influence of selected spatiotemporal properties of thermal image sequences on detection accuracy. For this purpose, a thermal face database was acquired for 40 volunteers. Contrary to most of existing thermal databases of faces, we publish our dataset in a raw, original format (14-bit depth) to preserve all important details. In our experiments, we utilize two metrics usually used for image enhancement evaluation: Peak-Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Metric (SSIM). In addition, we present how to design a SR network with a widened receptive field to mitigate the problem of contextual information being spread over larger image regions due to the heat flow in thermal images. Finally, we determine whether there is a relation between achieved PSNR and accuracy of facial areas detection that can be analyzed for vital signs extraction (e.g. nostril region). The performed evaluation showed that PSNR can be improved even by 60% if full bit depth resolution data is used instead of 8 bits. Also, we showed that the application of image enhancement solution is necessary for low resolution images to achieve a satisfactory accuracy of object detection. HR data is especially desired for medical applications in order to make proper diagnosis, as super-resolved image can usually offer more diagnostically important details  (Park et al., 2003; Villanueva et al., 2010).Health care sectors that use imaging techniques can utilize resolution enhancement, e.g. for computer-aided diagnosis (CAD) of breast tumors to improve accuracy of malignancy classification (Abdel-Nasser et al., 2017) or reconstruct computed tomography images to provide clinicians with important details to make correct decisions (Gao et al., 2017).Undoubtedly, providing more detailed HR samples can ease the analysis of medical imaging.The interesting research question is whether it can also help with improving accuracy of remote medical diagnostics.Due to global aging, the medicine is expected to deliver novel solutions that allow for performing basic diagnostic and monitoring tests at home (Kwaśniewska et al., 2017).Some studies have already proved that basic vital signs can be estimated from a single camera stream in a non-contact way, e.g. heart rate from visible light sequences (Lewandowska et al., 2011) or a breathing rate from thermal data (Ruminski and Kwasniewska, 2017).The starting point of image processing-based vital signs estimation is accurate detection of proper facial regions.Despite the majority of object detection research focuses on visible light spectrum, some attempts to localize facial features from low resolution thermal imagery have been done (Hanmandlu et al., 2014; Kwaśniewska and Rumiński, 2016), recently also using Deep Learning (DL) techniques (Kwaśniewska et al., 2018).The achieved results, though, were not satisfactory, giving accuracy of 0.53 ± 0.15 for eyes area and 0.60 ± 0.18 for nostrils, expressed as Intersection over Union (IoU) metric.Localizing a proper region is crucial for the robustness of signal processing algorithms aimed at estimating vital signs (Ruminski and Kwasniewska, 2017).It has already been shown that motion magnification can improve performance of heart rate estimation at distances above 6 meters (Szankin et al., 2018b).However, to the best of our knowledge, it has not been evaluated yet whether image enhancement with DL-based SR has a positive effect on the accuracy of object detectors, especially in thermal imaging.Simultaneously, analysis of other than visible light image domains is crucial, as representation of features may differ across them, e.g. thermal images are characterized by blurring and lower contrast between adjacent regions due to the heat flow, hence networks designed for extracting high frequency components (e.g. edges, lines) from visible light spectrum images may not be sufficient in the thermal domain.
In this study, we evaluate various Convolutional Neural Networks based Super-Resolution (SR) models to improve facial areas detection in thermal images. In particular, we analyze the influence of selected spatiotemporal properties of thermal image sequences on detection accuracy. For this purpose, a thermal face database was acquired for 40 volunteers. Contrary to most of existing thermal databases of faces, we publish our dataset in a raw, original format (14-bit depth) to preserve all important details. In our experiments, we utilize two metrics usually used for image enhancement evaluation: Peak-Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Metric (SSIM). In addition, we present how to design a SR network with a widened receptive field to mitigate the problem of contextual information being spread over larger image regions due to the heat flow in thermal images. Finally, we determine whether there is a relation between achieved PSNR and accuracy of facial areas detection that can be analyzed for vital signs extraction (e.g. nostril region). The performed evaluation showed that PSNR can be improved even by 60% if full bit depth resolution data is used instead of 8 bits. Also, we showed that the application of image enhancement solution is necessary for low resolution images to achieve a satisfactory accuracy of object detection. The contribution of our work is threefold: (1) First, we collect and publish a dataset of raw thermal sequences of a face in the original full 14-bit precision.(2) Second, we applied deep learning based super resolution algorithms to thermal image sequences.We experimentally compared the state of the art algorithms and our algorithm analyzing selected spatiotemporal properties of thermal sequences including temporal frames averaging and the influence of various bit depths.(3) Finally, we evaluate the effect of enhancing image resolution (and related PSNR/SSIM measures) on the robustness of areas detection in the auditing thermal domain.
In this study, we evaluate various Convolutional Neural Networks based Super-Resolution (SR) models to improve facial areas detection in thermal images. In particular, we analyze the influence of selected spatiotemporal properties of thermal image sequences on detection accuracy. For this purpose, a thermal face database was acquired for 40 volunteers. Contrary to most of existing thermal databases of faces, we publish our dataset in a raw, original format (14-bit depth) to preserve all important details. In our experiments, we utilize two metrics usually used for image enhancement evaluation: Peak-Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Metric (SSIM). In addition, we present how to design a SR network with a widened receptive field to mitigate the problem of contextual information being spread over larger image regions due to the heat flow in thermal images. Finally, we determine whether there is a relation between achieved PSNR and accuracy of facial areas detection that can be analyzed for vital signs extraction (e.g. nostril region). The performed evaluation showed that PSNR can be improved even by 60% if full bit depth resolution data is used instead of 8 bits. Also, we showed that the application of image enhancement solution is necessary for low resolution images to achieve a satisfactory accuracy of object detection. Since in our research we focus on facial features detection, we analyze PSNR changes for both extracted facial areas and images as a whole.In this way, we aim at determining whether PSNR is sensitive to the change of pixel values caused be the presence of breathing patterns in the extracted areas (e.g. nostrils).In addition, we evaluate how PSNR changes by (a) using averaging operation of subsequent frames in a sequence, what allows for reducing random noise; (b) utilizing 16-bit resolution data in order to preserve important image components that may be invisible after conversion to lower bit resolution (e.g. 8-bit).To the best of our knowledge, the majority of existing publicly available face thermal datasets contain images that were converted to lower precision image format with loss of some of the information available in the higher precision.
In this study, we evaluate various Convolutional Neural Networks based Super-Resolution (SR) models to improve facial areas detection in thermal images. In particular, we analyze the influence of selected spatiotemporal properties of thermal image sequences on detection accuracy. For this purpose, a thermal face database was acquired for 40 volunteers. Contrary to most of existing thermal databases of faces, we publish our dataset in a raw, original format (14-bit depth) to preserve all important details. In our experiments, we utilize two metrics usually used for image enhancement evaluation: Peak-Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Metric (SSIM). In addition, we present how to design a SR network with a widened receptive field to mitigate the problem of contextual information being spread over larger image regions due to the heat flow in thermal images. Finally, we determine whether there is a relation between achieved PSNR and accuracy of facial areas detection that can be analyzed for vital signs extraction (e.g. nostril region). The performed evaluation showed that PSNR can be improved even by 60% if full bit depth resolution data is used instead of 8 bits. Also, we showed that the application of image enhancement solution is necessary for low resolution images to achieve a satisfactory accuracy of object detection. Thus, the data collected by us possesses advantages over them, as it is shared in the raw format that contains unprocessed pixels, what is potentially useful for extracting intensity changes, caused e.g. by breathing.Collected dataset1  and supplementary materials2  are publicly available.
Cova Bonica has yielded one of the few assemblages of Cardial Neolithic records of directly dated human remains (c. 5470 and 5220 years cal. BC – unmodelled) in the Iberian Peninsula and has provided the first complete genome of an Iberian farmer. A minimum of seven individuals and six age clusters have been ascribed on the basis of the disarticulated human bones. A large number of archaeological artifacts have likewise been identified in the same layer, preserved in a small number of remnants in different areas of the cave. This study presents the results of a multi-proxy archaeological analysis of the spatial distribution, human remains, small and large mammals, palaeobotanical remains, lithics, ceramics and radiocarbon dating, with the aim of reconstructing the cave's history and the context of the layer containing the human remains. The results suggest the cave was used for at least two distinct purposes: one related to its use for funerary practices, as documented by a small group of artifacts (ornamental objects, ceramics, tools), charcoal and small mammals; the other related to its use as a sheep pen as indicated by reworked fumier, the results of a zooarchaeological study and an ovicaprine palaeodemographic profile. The paper concludes that the funerary and ritualistic practices of the Cardial Neolithic in SW Europe are difficult to reconstruct because human remains are often scattered in archaeological layers where other human activities may also have been conducted. For this reason, artifacts associated with human remains do not constitute a solid foundation on which to reconstruct funerary practices. Indeed, only a multi-proxy analysis of the archaeological material is capable of evaluating different geological and/or archaeological processes and their associated activities. The transition from hunter-gatherer to farmer populations meant radical economic and social changes from the ways of nomadism to more sedentary ways of life (Guilaine, 2013).According to palaeogenetic data (Gamba et al., 2014; Olalde et al., 2015; Rivollat et al., 2015), farming appears to have been introduced into central and western Europe around 8000 years ago by new migrant populations, while the archaeological evidence suggests at least two distinct, but well-defined, migration routes: up the Danube valley and along the Mediterranean shoreline (Cruz Berrocal, 2012; Guilaine, 2013; Zilhão, 2001).The Neolithic period in the associated archaeological sites shares close links with the Linearbandkeramik (LBK) culture in central Europe and with the Impressa and Cardial traditions in southern Europe, characterized by the domestication of plants and animals, and the use of a new lithic technology and raw materials, among other features.Scholars have employed a range of approaches in their study of Neolithic phenomena in Europe, including, the determination of the rhythms of this process, radiocarbon dating, stratigraphy, and analyses of the social structure, economy, funerary practices, etc. (Bickle and Whittle, 2013; Binder, 2000; Bogaard, 2004; Cruz Berrocal, 2012; Davison et al., 2006; Gronenborn, 1999; Oross and Bánffy, 2009).
Cova Bonica has yielded one of the few assemblages of Cardial Neolithic records of directly dated human remains (c. 5470 and 5220 years cal. BC – unmodelled) in the Iberian Peninsula and has provided the first complete genome of an Iberian farmer. A minimum of seven individuals and six age clusters have been ascribed on the basis of the disarticulated human bones. A large number of archaeological artifacts have likewise been identified in the same layer, preserved in a small number of remnants in different areas of the cave. This study presents the results of a multi-proxy archaeological analysis of the spatial distribution, human remains, small and large mammals, palaeobotanical remains, lithics, ceramics and radiocarbon dating, with the aim of reconstructing the cave's history and the context of the layer containing the human remains. The results suggest the cave was used for at least two distinct purposes: one related to its use for funerary practices, as documented by a small group of artifacts (ornamental objects, ceramics, tools), charcoal and small mammals; the other related to its use as a sheep pen as indicated by reworked fumier, the results of a zooarchaeological study and an ovicaprine palaeodemographic profile. The paper concludes that the funerary and ritualistic practices of the Cardial Neolithic in SW Europe are difficult to reconstruct because human remains are often scattered in archaeological layers where other human activities may also have been conducted. For this reason, artifacts associated with human remains do not constitute a solid foundation on which to reconstruct funerary practices. Indeed, only a multi-proxy analysis of the archaeological material is capable of evaluating different geological and/or archaeological processes and their associated activities. Identifying the funerary practices of these cultures is of particular value as they are a good reflection of associated social behavior, providing us with insights into these past societies.The study of the early Neolithic in Europe should enable us to establish, and distinguish between, the funerary patterns of each specific area (LBK, Cardial, Impressa, etc.) and between the latter and those of the Near East.Archaeological evidence of the treatment of the dead in central Europe is well defined and suggests there was much uniformity throughout the LBK world.At least three funerary patterns have been recorded in the LBK/Rubané groups (Jeunesse, 1996): a) isolated burials in pits associated with living areas; b) small groups of burials associated with living areas; and, c) isolated necropolises.Skeletons were often buried individually, with corpses in the left decubitus position, while the most common grave goods were bracelets and Spondylus shell-beads, polished adze blades and beads made from a range of different raw materials (Jeunesse, 2003).A differentiation between male and female burials has been suggested based on the grave goods recorded in the sepultures.Additionally, many corpses were sprinkled in ochre and data from the Herxheim site (Palatine, Germany) (Boulestin et al., 2009; Zeeb-Lanz et al., 2009) and other synchronous sites demonstrate that violence and cannibalism were not rare (Jeunesse et al., 2014; Meyer et al., 2015).
Cova Bonica has yielded one of the few assemblages of Cardial Neolithic records of directly dated human remains (c. 5470 and 5220 years cal. BC – unmodelled) in the Iberian Peninsula and has provided the first complete genome of an Iberian farmer. A minimum of seven individuals and six age clusters have been ascribed on the basis of the disarticulated human bones. A large number of archaeological artifacts have likewise been identified in the same layer, preserved in a small number of remnants in different areas of the cave. This study presents the results of a multi-proxy archaeological analysis of the spatial distribution, human remains, small and large mammals, palaeobotanical remains, lithics, ceramics and radiocarbon dating, with the aim of reconstructing the cave's history and the context of the layer containing the human remains. The results suggest the cave was used for at least two distinct purposes: one related to its use for funerary practices, as documented by a small group of artifacts (ornamental objects, ceramics, tools), charcoal and small mammals; the other related to its use as a sheep pen as indicated by reworked fumier, the results of a zooarchaeological study and an ovicaprine palaeodemographic profile. The paper concludes that the funerary and ritualistic practices of the Cardial Neolithic in SW Europe are difficult to reconstruct because human remains are often scattered in archaeological layers where other human activities may also have been conducted. For this reason, artifacts associated with human remains do not constitute a solid foundation on which to reconstruct funerary practices. Indeed, only a multi-proxy analysis of the archaeological material is capable of evaluating different geological and/or archaeological processes and their associated activities. These well-defined funerary patterns differ from those of the early Neolithic in the western Mediterranean area.Although some funerary structures (pits, graves and individualized sepultures) have been documented, inferences concerning mortuary rituals or practices from Impressa and Cardial horizons are not easy because human bones usually occur non-articulated and scattered in the archaeological layers of caves and rock-shelters together with artifacts from other activities.Indeed, this archaeological record may be related to practices in which corpses were not actually buried but were left to rot in caves or rock-shelters, with an absence of negative structures, that is, the creation of an artificial place to serve as a grave and the covering of the corpse with the removed sediment, or alternatively corpses may have been interred by disturbing and moving previously buried skeletons.As a result, human remains are commonly found commingled with archaeological artifacts (animal bones, lithics, charcoal, etc.), making it difficult to distinguish between grave goods and functional or domestic artifacts.In fact, at many sites – including Abri de Jean Cros (Guilaine, 1979), Cova de Can Sadurní (Blasco et al., 2012) and Cova de l'Or (García Borja et al., 2011) – a large number of Cardial human remains are associated with domestic waste.Less frequently, human remains are covered by blocks, as occurs at Grotte d'Unang (Paccard, 1987) and l'Abri de Pendimoun (Binder et al., 1993), with primary inhumations being rare (Utrilla et al., 2008; Zemour et al., 2017).
Cova Bonica has yielded one of the few assemblages of Cardial Neolithic records of directly dated human remains (c. 5470 and 5220 years cal. BC – unmodelled) in the Iberian Peninsula and has provided the first complete genome of an Iberian farmer. A minimum of seven individuals and six age clusters have been ascribed on the basis of the disarticulated human bones. A large number of archaeological artifacts have likewise been identified in the same layer, preserved in a small number of remnants in different areas of the cave. This study presents the results of a multi-proxy archaeological analysis of the spatial distribution, human remains, small and large mammals, palaeobotanical remains, lithics, ceramics and radiocarbon dating, with the aim of reconstructing the cave's history and the context of the layer containing the human remains. The results suggest the cave was used for at least two distinct purposes: one related to its use for funerary practices, as documented by a small group of artifacts (ornamental objects, ceramics, tools), charcoal and small mammals; the other related to its use as a sheep pen as indicated by reworked fumier, the results of a zooarchaeological study and an ovicaprine palaeodemographic profile. The paper concludes that the funerary and ritualistic practices of the Cardial Neolithic in SW Europe are difficult to reconstruct because human remains are often scattered in archaeological layers where other human activities may also have been conducted. For this reason, artifacts associated with human remains do not constitute a solid foundation on which to reconstruct funerary practices. Indeed, only a multi-proxy analysis of the archaeological material is capable of evaluating different geological and/or archaeological processes and their associated activities. The Cova Bonica site, located in the NE of the Iberian Peninsula, has yielded a rich assemblage of Cardial Neolithic human bones dated between c.5470 and 5220 years cal.BC.The site is a rare example of Cardial Neolithic funerary contexts in the Iberian Peninsula and one of very few in the western Mediterranean area in which Cardial human remains have been directly dated and associated with archaeological artifacts excavated in the last 20 years.Moreover, the recent publication of the complete genome of a Cova Bonica human has renewed interest in the cave.This genetic material constitutes the first complete genome of an Iberian farmer and also the first ancient genome to be reconstructed in the entire Mediterranean area (Olalde et al., 2015).
One persistent archaeological challenge is the generation of systematic documentation for the extant archaeological record at the scale of landscapes. Often our information for landscapes is the result of haphazard and patchy surveys that stem from opportunistic and historic efforts. Consequently, overall knowledge of some regions is the product of ad hoc survey area delineation, degree of accessibility, effective ground visibility, and the fraction of areas that have survived destruction from development. These factors subsequently contribute unknown biases to our understanding of chronology, settlements patterns, interaction, and exchange. Aerial remote sensing offers one potential solution for improving our knowledge of landscapes. With sensors that include LiDAR, remote sensing can identify archaeological features that are otherwise obscured by vegetation. Object-based image analyses (OBIA) of remote sensing data hold particular promise to facilitate regional analyses thorough the automation of archaeological feature recognition. Here, we explore four OBIA algorithms for artificial mound feature detection using LiDAR from Beaufort County, South Carolina: multiresolution segmentation, inverse depression analysis, template matching, and a newly designed algorithm that combines elements of segmentation and template matching. While no single algorithm proved to be consistently superior to the others, a combination of methods is shown to be the most effective for detecting archaeological features. At the time of European arrival into Eastern North America, the archaeological record included thousands of intact earth and shell mound structures (Anderson, 2012; Howe, 2014; Thomas, 1894).Beginning in the 19th century, these deposits became the focus of archaeological research due to their ability to produce artifacts that shed light on cultural affinity and chronology (Lyman et al., 1997; e.g., Claflin, 1931; Fairbanks, 1942; Ford and Willey, 1941; Jones et al., 1933; Moore, 1894a, 1894b, 1899; Moorehead, 1891; Putnam, 1875; Squire and Davis, 1848; Swallow, 1858; Wauchope, 1948; Willey, 1939).Over time, archaeological interest in mounds has grown to include studies of pre-contact technology, diet, social behavior, trade, exchange, interaction, and settlement (e.g., Anderson, 2004; Caldwell, 1952; Calmes, 1967; Claassen, 1986, 1991, 2010; Crusoe and DePratter, 1976; Marquardt, 2010; Matteson, 1960; Russo, 2004, 2006; Thompson et al., 2011; Trinkley, 1985).
One persistent archaeological challenge is the generation of systematic documentation for the extant archaeological record at the scale of landscapes. Often our information for landscapes is the result of haphazard and patchy surveys that stem from opportunistic and historic efforts. Consequently, overall knowledge of some regions is the product of ad hoc survey area delineation, degree of accessibility, effective ground visibility, and the fraction of areas that have survived destruction from development. These factors subsequently contribute unknown biases to our understanding of chronology, settlements patterns, interaction, and exchange. Aerial remote sensing offers one potential solution for improving our knowledge of landscapes. With sensors that include LiDAR, remote sensing can identify archaeological features that are otherwise obscured by vegetation. Object-based image analyses (OBIA) of remote sensing data hold particular promise to facilitate regional analyses thorough the automation of archaeological feature recognition. Here, we explore four OBIA algorithms for artificial mound feature detection using LiDAR from Beaufort County, South Carolina: multiresolution segmentation, inverse depression analysis, template matching, and a newly designed algorithm that combines elements of segmentation and template matching. While no single algorithm proved to be consistently superior to the others, a combination of methods is shown to be the most effective for detecting archaeological features. Our knowledge of the distribution of mound features, however, tends to be biased towards some areas more than others.These areas may come from regions that have seen a greater number of field studies (e.g., Michie's (1980) survey of the coastal plains of the Port Royal Sound) but also include those that are easier to survey due to a lack of substantial ground cover such as in areas of beaches and shallow intertidal zones (South, 1960) as well as piedmonts and coastal plains (House and Ballinger, 1976).Specifically, environments that are dominated by heavy vegetation (e.g., woodlands, bayous, and coastal marshes) are often missing from our knowledge of the record as they are difficult to evaluate using systematic pedestrian tactics.The most recent example of this lapse in knowledge is the discovery of thousands of monumental complexes in the dense forests of Guatemala (Canuto et al., 2018).Prior to the use of LiDAR survey, these archaeological features were unknown, and their discovery may rewrite the history of this area.
One persistent archaeological challenge is the generation of systematic documentation for the extant archaeological record at the scale of landscapes. Often our information for landscapes is the result of haphazard and patchy surveys that stem from opportunistic and historic efforts. Consequently, overall knowledge of some regions is the product of ad hoc survey area delineation, degree of accessibility, effective ground visibility, and the fraction of areas that have survived destruction from development. These factors subsequently contribute unknown biases to our understanding of chronology, settlements patterns, interaction, and exchange. Aerial remote sensing offers one potential solution for improving our knowledge of landscapes. With sensors that include LiDAR, remote sensing can identify archaeological features that are otherwise obscured by vegetation. Object-based image analyses (OBIA) of remote sensing data hold particular promise to facilitate regional analyses thorough the automation of archaeological feature recognition. Here, we explore four OBIA algorithms for artificial mound feature detection using LiDAR from Beaufort County, South Carolina: multiresolution segmentation, inverse depression analysis, template matching, and a newly designed algorithm that combines elements of segmentation and template matching. While no single algorithm proved to be consistently superior to the others, a combination of methods is shown to be the most effective for detecting archaeological features. This aspect of past archaeological surveys raises the possibility that our knowledge of the record is biased towards features that appear in the best cleared and most visible landscapes (Banning et al., 2017; Bintliff, 2000; Bintliff et al., 1999; Hirth, 1978; Nance, 1979; Stark and Garraty, 2008).The potential for increasing our understanding of the archaeological record is likely greatest in the exploration of areas that have seen little systematic observation.Given that unknown deposits are often least visited and impacted, those that remain hidden in vegetation potentially hold some of the most promising opportunities for new archaeological discovery.To address the challenges of large-scale documentation presented by heavily-vegetated landscapes, and to aid in the study of these poorly studied regions, new kinds of techniques are required.
One persistent archaeological challenge is the generation of systematic documentation for the extant archaeological record at the scale of landscapes. Often our information for landscapes is the result of haphazard and patchy surveys that stem from opportunistic and historic efforts. Consequently, overall knowledge of some regions is the product of ad hoc survey area delineation, degree of accessibility, effective ground visibility, and the fraction of areas that have survived destruction from development. These factors subsequently contribute unknown biases to our understanding of chronology, settlements patterns, interaction, and exchange. Aerial remote sensing offers one potential solution for improving our knowledge of landscapes. With sensors that include LiDAR, remote sensing can identify archaeological features that are otherwise obscured by vegetation. Object-based image analyses (OBIA) of remote sensing data hold particular promise to facilitate regional analyses thorough the automation of archaeological feature recognition. Here, we explore four OBIA algorithms for artificial mound feature detection using LiDAR from Beaufort County, South Carolina: multiresolution segmentation, inverse depression analysis, template matching, and a newly designed algorithm that combines elements of segmentation and template matching. While no single algorithm proved to be consistently superior to the others, a combination of methods is shown to be the most effective for detecting archaeological features. Remote sensing using computational algorithms for automatic feature detection offers one promising solution.High-resolution aerial imagery provides detailed information about the structure of landscapes.Multispectral imagery expands the wavelengths that can be used for sensing to include bands that are sensitive to vegetation and sediment composition (Jensen, 2007).Active sensors, such as light detecting and ranging (LiDAR), provide a mapping technique that permits direct measurements of surface topography that is faster, more systematic, and more accurate than other forms of manual mapping (Chase et al., 2017; Evans et al., 2013).New computational methods greatly facilitate the use of these many classes of data as they can be configured to automatically identify features of interest (Freeland et al., 2016; Magnini et al., 2016; Sevara et al., 2016; Trier et al., 2015).Object-based image analysis (OBIA) covers a broad array of promising algorithms for archaeological prospection (Sevara et al., 2016).These compositional techniques include shape templates (Kvamme, 2013; Trier et al., 2008), machine learning algorithms (Wu et al., 2015, 2016), and image segmentation (Witharana et al., 2018).
Fermentation is seldom considered in paleodietary analyses of Arctic and Subarctic peoples despite its ubiquitous traditional use as a cooking technique in high latitudes. Further, chemists have yet to assess the potential isotopic effects of fermentation on animal tissues, though isotopic research has documented measurable isotopic effects associated with other cooking techniques such as stewing and boiling. To measure the effects of fermentation on stable carbon and nitrogen isotopes, muscle tissues from 11 central Alaskan Chinook salmon (Oncorhynchus tshawytscha) were fermented following ethnographic methods. The results of this experiment demonstrate statistically significant isotopic differences in both carbon (−1.5‰) and nitrogen (+1.3‰) values taken from fermented muscle tissues compared to raw muscle tissues. Additionally, this study found a significant physiological fractionation effect between bone collagen and raw muscle (+3.3‰) consistent with previous research on Pacific salmon. In contrast to previous research, however, these tissues' nitrogen values show no significant fractionation. These findings have implications for central Alaskan dietary reconstructions, residue analyses, and our understanding of past subsistence practices in the Arctic and Subarctic. Dietary reconstructions are fundamental to anthropological assessments of past human behavior.During the last half-century, archaeologists have increasingly based dietary reconstructions on isotopic data.In contrast to faunal or paleobotanical analyses, an isotopic analysis can be directly applied to human remains to evaluate the types of food that individuals consumed in the past.However, accurate isotopic dietary reconstruction requires precise isotopic baseline values for food items in the trophic web (Schwarcz, 1991).Further, recent isotopic research has shown that certain food processing or cooking techniques have a significant isotopic effect, and therefore, dietary reconstructions on past human populations must also account for the effects of food processing techniques (Warinner and Tuross, 2009; Brettell et al., 2012).Fermentation is a culturally and historically significant fish processing technique, and it may yield a recognizable isotopic effect that anthropologists have yet to incorporate into isotopic dietary mixing models (Speth, 2017; Tanasupawat and Visessanguan, 2014).
Chronic cannabis use impacts memory functioning, even while users are not acutely intoxicated. The impact of cannabis use on Wada or intracarotid amobarbital testing (IAT) has not previously been described. We reviewed cannabis consumption in epilepsy patients undergoing IAT during pre-surgical work-up. Of 58 patients reviewed, 16 patients (28%) indicated regular use. During IAT, five regular cannabis users with suspected temporal lobe epilepsy exhibited poor memory while testing their presumptively healthy temporal lobe (i.e., the side opposite that targeted for epilepsy surgery), indicating the potential for an amnestic syndrome post-operatively. It was suspected that the pattern of IAT results for these patients was attributable to the deleterious impact of cannabis use on cognition. Thus, three of the five underwent repeat IAT after a period of enforced abstinence. On repeat IAT, each of the three patients exhibited improved memory performance while testing their healthy temporal lobe, suggesting that the healthy temporal lobe of each mediated sufficient memory ability to allow for epilepsy surgery. These findings raised concerns that frequent cannabis use may alter IAT results, leading to incorrect assessments regarding potential post-operative cognitive deficits, and led to a mandate at our institution that patients must stop cannabis use before IAT. The Wada test or intracarotid amobarbital test (IAT) is used for language lateralization and to assess memory function in patients with drug-resistant epilepsy who are considering anterior temporal lobectomy with hippocampal resection as treatment [1].During the test, amobarbital is injected into the internal carotid artery and inactivates brain function in the dependent vascular territory, typically including hippocampal function.It is intended to mimic the effect of removing the epileptogenic temporal lobe and its medial structures, and assesses whether the remaining, contralateral temporal lobe can provide sufficient memory function to compensate for the loss of the ipsilateral hippocampus.If a patient exhibits poor memory performance while testing the presumptively healthy side, an anterior temporal resection with removal of mesial structures cannot be recommended as the results indicate that the patient may have inadequate memory function post-operatively.Conceivably, systemic factors that impair memory, such as medications or drugs like cannabis, can interfere with IAT performance and cause misleading results.
Chronic cannabis use impacts memory functioning, even while users are not acutely intoxicated. The impact of cannabis use on Wada or intracarotid amobarbital testing (IAT) has not previously been described. We reviewed cannabis consumption in epilepsy patients undergoing IAT during pre-surgical work-up. Of 58 patients reviewed, 16 patients (28%) indicated regular use. During IAT, five regular cannabis users with suspected temporal lobe epilepsy exhibited poor memory while testing their presumptively healthy temporal lobe (i.e., the side opposite that targeted for epilepsy surgery), indicating the potential for an amnestic syndrome post-operatively. It was suspected that the pattern of IAT results for these patients was attributable to the deleterious impact of cannabis use on cognition. Thus, three of the five underwent repeat IAT after a period of enforced abstinence. On repeat IAT, each of the three patients exhibited improved memory performance while testing their healthy temporal lobe, suggesting that the healthy temporal lobe of each mediated sufficient memory ability to allow for epilepsy surgery. These findings raised concerns that frequent cannabis use may alter IAT results, leading to incorrect assessments regarding potential post-operative cognitive deficits, and led to a mandate at our institution that patients must stop cannabis use before IAT. Colorado legalized “medical marijuana” after voter approval in November 2000 and the use of cannabis for personal, recreational consumption after elections in November 2012.Epilepsy was considered by legislators, not physicians, as one of the statewide-approved medical indications for “medical marijuana.”Despite the lack of rigorous scientific data, some patients, particularly those with drug-resistant epilepsy, use it frequently.Indeed, a survey of epilepsy patients in Denver conducted immediately prior to legalization of recreational cannabis found that 33% of patients consumed plant-derived cannabis [2].
Chronic cannabis use impacts memory functioning, even while users are not acutely intoxicated. The impact of cannabis use on Wada or intracarotid amobarbital testing (IAT) has not previously been described. We reviewed cannabis consumption in epilepsy patients undergoing IAT during pre-surgical work-up. Of 58 patients reviewed, 16 patients (28%) indicated regular use. During IAT, five regular cannabis users with suspected temporal lobe epilepsy exhibited poor memory while testing their presumptively healthy temporal lobe (i.e., the side opposite that targeted for epilepsy surgery), indicating the potential for an amnestic syndrome post-operatively. It was suspected that the pattern of IAT results for these patients was attributable to the deleterious impact of cannabis use on cognition. Thus, three of the five underwent repeat IAT after a period of enforced abstinence. On repeat IAT, each of the three patients exhibited improved memory performance while testing their healthy temporal lobe, suggesting that the healthy temporal lobe of each mediated sufficient memory ability to allow for epilepsy surgery. These findings raised concerns that frequent cannabis use may alter IAT results, leading to incorrect assessments regarding potential post-operative cognitive deficits, and led to a mandate at our institution that patients must stop cannabis use before IAT. Chronic cannabis users with no known neurological disease exhibit mild deficits in learning and memory, as well as other cognitive domains, even while not acutely intoxicated [3].However, no differences on cognitive testing are detected between cannabis users and non-users after an abstinence period of about four weeks, though individuals who began using in adolescence may exhibit more persistent cognitive deficits [4,5].The mild cognitive deficits observed among chronic users early on during a period of abstinence are thought to possibly represent the effects of persistent low levels of tetrahydrocannabinol (THC), abstinence phenomena, or both [3].The use of cannabis products with higher ratios of cannabidiol (CBD) to THC may attenuate cognitive effects, but findings in this regard are limited and mixed [6,7].
Cannabis use is associated with changes in brain structure and function; its neurotoxic effects are largely attributed to Δ9-tetrahydrocannabidiol. Whether such effects are present in patients with epilepsy exposed to a highly-purified cannabidiol isolate (CBD; Epidiolex®; Greenwich Biosciences, Inc.) has not been investigated to date. This preliminary study examines whether daily CBD dose of 15–25 mg/kg produces cerebral macrostructure changes and, if present, how they relate to changes in seizure frequency. Twenty-seven patients with treatment-resistant epilepsy were recruited from the University of Alabama at Birmingham CBD Program. Participants provided seizure frequency diaries (SF), completed the Chalfont Seizure Severity Scale (CSSS) and Adverse Events Profile (AEP), and underwent MRI before CBD (baseline) and after achieving a stable CBD dosage (on-CBD). We examined T1-weighted structural images for gray matter volume (GMV) and cortical thickness changes from baseline to on-CBD in 18 participants. Repeated measures t-tests confirmed decreases in SF [t(17) = 3.08, p = 0.0069], CSSS [t(17) = 5.77, p < 0.001], and AEP [t(17) = 3.04, p = 0.0074] between the two time-points. Voxel-level paired samples t-tests did not identify significant changes in GMV or cortical thickness between these two time-points. In conclusion, short-term exposure to highly purified CBD may not affect cortical macrostructure. Of the 1.2% of the population that suffers from epilepsy, one-third has treatment-resistant epilepsy (TRE) in which anti-seizure drug (ASD) mono- or poly-therapy does not control seizures [1].In TRE, the primary tissue insult from chronic, uncontrolled seizures, combined with the secondary effects of failed ASDs, results in on-going insult to brain structure and function [2,3].Patients with TRE are thus at increased risk for epilepsy-related mortality (e.g., sudden unexpected death in epilepsy; SUDEP), as well as more severe cognitive and neuropsychological impairments [2,3].While ASDs treat seizures, they do not interrupt or reverse the underlying epileptogenesis [1] which underscores the necessity of finding treatments that interrupt or reverse the pathophysiology that underlies epilepsy.Recent evidence points to chronic neuroinflammation as one of the potential drivers of epileptogenesis [4–8].Perpetual activation of the neuroinflammatory cascade can lower seizure threshold, resulting in dysfunction of the blood–brain-barrier and chronic neuronal hyperexcitability [4–8].This notion highlights an under-exploited therapeutic target: the development of treatments that interrupt the neuroinflammatory cascade to provide seizure freedom to patients with TRE.
Cannabis use is associated with changes in brain structure and function; its neurotoxic effects are largely attributed to Δ9-tetrahydrocannabidiol. Whether such effects are present in patients with epilepsy exposed to a highly-purified cannabidiol isolate (CBD; Epidiolex®; Greenwich Biosciences, Inc.) has not been investigated to date. This preliminary study examines whether daily CBD dose of 15–25 mg/kg produces cerebral macrostructure changes and, if present, how they relate to changes in seizure frequency. Twenty-seven patients with treatment-resistant epilepsy were recruited from the University of Alabama at Birmingham CBD Program. Participants provided seizure frequency diaries (SF), completed the Chalfont Seizure Severity Scale (CSSS) and Adverse Events Profile (AEP), and underwent MRI before CBD (baseline) and after achieving a stable CBD dosage (on-CBD). We examined T1-weighted structural images for gray matter volume (GMV) and cortical thickness changes from baseline to on-CBD in 18 participants. Repeated measures t-tests confirmed decreases in SF [t(17) = 3.08, p = 0.0069], CSSS [t(17) = 5.77, p < 0.001], and AEP [t(17) = 3.04, p = 0.0074] between the two time-points. Voxel-level paired samples t-tests did not identify significant changes in GMV or cortical thickness between these two time-points. In conclusion, short-term exposure to highly purified CBD may not affect cortical macrostructure. Cannabis has been used as complementary medicine for a variety of conditions, including epilepsy [9].In the 1970s–80s, the two most abundant phytocannabinoids, Δ9-tetrahydrocannabidiol (Δ9-THC) and cannabidiol (CBD), were chemically identified, isolated, and synthetically manufactured [10].This allowed empirical investigation of the reported phytocannabinoids' anti-seizure properties.Since then, several animal models have confirmed the anti-seizure properties of Δ9-THC and CBD, thus renewing interest in their therapeutic potential for humans [11–14].The most well-understood cannabis actions are attributable to Δ9-THC, which attenuates seizure frequency in many models; however, its psychotropic and cognitive effects render it an undesirable therapeutic option [11,15].With the shift in legality surrounding phytocannabinoids, there is an increased interest in cannabidiol (CBD), a potentially superior treatment option to Δ9-THC due to its non-euphoric, neuroprotective, and anti-neuroinflammatory effects [16].
Cannabis use is associated with changes in brain structure and function; its neurotoxic effects are largely attributed to Δ9-tetrahydrocannabidiol. Whether such effects are present in patients with epilepsy exposed to a highly-purified cannabidiol isolate (CBD; Epidiolex®; Greenwich Biosciences, Inc.) has not been investigated to date. This preliminary study examines whether daily CBD dose of 15–25 mg/kg produces cerebral macrostructure changes and, if present, how they relate to changes in seizure frequency. Twenty-seven patients with treatment-resistant epilepsy were recruited from the University of Alabama at Birmingham CBD Program. Participants provided seizure frequency diaries (SF), completed the Chalfont Seizure Severity Scale (CSSS) and Adverse Events Profile (AEP), and underwent MRI before CBD (baseline) and after achieving a stable CBD dosage (on-CBD). We examined T1-weighted structural images for gray matter volume (GMV) and cortical thickness changes from baseline to on-CBD in 18 participants. Repeated measures t-tests confirmed decreases in SF [t(17) = 3.08, p = 0.0069], CSSS [t(17) = 5.77, p < 0.001], and AEP [t(17) = 3.04, p = 0.0074] between the two time-points. Voxel-level paired samples t-tests did not identify significant changes in GMV or cortical thickness between these two time-points. In conclusion, short-term exposure to highly purified CBD may not affect cortical macrostructure. Endocannabinoids' actions at the intersection of the endocannabinoid system (ECS) and immune system drive our interest in CBD's pharmacologic effects.In general, cannabinoid 1 and 2 receptors (CB1R and CB2R) are found throughout the central nervous system (CB1R > CB2R) and on immune cells such as microglia (CB1R < CB2R) [17,18].The ECS homeostatically balances excitatory and inhibitory synaptic transmission via CB1Rs and CB2Rs—a balance that is not conserved in epilepsy [19].Due to their expression on immune cells, these receptors—notably CB2Rs—also play an important role in neuroinflammation [18].Studies have revealed the dual-nature of endocannabinoids with concentration and context driving either their pro- or anti-inflammatory effects [1,17,20,21].This duality is evident in their ability to activate and recruit microglia, affect apoptosis, inhibit cell proliferation, and induce regulatory T cells [4,17,19–21].Endocannabinoids' immunosuppressive effects are best demonstrated by their inhibition of CB2R-mediated release TNF-α, IL-6, and IL-8, major pro-neuroinflammatory cytokines implicated in epileptogenesis [1,18,20].In this framework, the autoimmune nature of epilepsy underscores the importance of harnessing endocannabinoids' immunomodulating effects and their ability to modulate the ECS.CBD's anti-epileptic mechanism is not yet fully elucidated, but evidence suggests it decreases neuronal hyperexcitability by ECS-dependent inhibition of excitatory glutamatergic neurotransmission; CBD's anti-inflammatory effects in central and peripheral tissues have not been clearly delineated [11,12,21,22].
Cannabis use is associated with changes in brain structure and function; its neurotoxic effects are largely attributed to Δ9-tetrahydrocannabidiol. Whether such effects are present in patients with epilepsy exposed to a highly-purified cannabidiol isolate (CBD; Epidiolex®; Greenwich Biosciences, Inc.) has not been investigated to date. This preliminary study examines whether daily CBD dose of 15–25 mg/kg produces cerebral macrostructure changes and, if present, how they relate to changes in seizure frequency. Twenty-seven patients with treatment-resistant epilepsy were recruited from the University of Alabama at Birmingham CBD Program. Participants provided seizure frequency diaries (SF), completed the Chalfont Seizure Severity Scale (CSSS) and Adverse Events Profile (AEP), and underwent MRI before CBD (baseline) and after achieving a stable CBD dosage (on-CBD). We examined T1-weighted structural images for gray matter volume (GMV) and cortical thickness changes from baseline to on-CBD in 18 participants. Repeated measures t-tests confirmed decreases in SF [t(17) = 3.08, p = 0.0069], CSSS [t(17) = 5.77, p < 0.001], and AEP [t(17) = 3.04, p = 0.0074] between the two time-points. Voxel-level paired samples t-tests did not identify significant changes in GMV or cortical thickness between these two time-points. In conclusion, short-term exposure to highly purified CBD may not affect cortical macrostructure. Studies have demonstrated significant decreases in seizure frequency and severity following CBD administration [23–25].It has also been shown that CBD may alter the effects of the co-administered ASDs [26,27].Structural and functional neuroimaging studies are necessary to better understand CBD's impact on the central nervous system, especially in the context of the developing brain [28].Our gaps in knowledge about CBD's mechanism of action, coupled with our understanding of cannabis' negative effects, make it increasingly important to further delineate how a CBD isolate differs from cannabis as a whole.
Cannabis use is associated with changes in brain structure and function; its neurotoxic effects are largely attributed to Δ9-tetrahydrocannabidiol. Whether such effects are present in patients with epilepsy exposed to a highly-purified cannabidiol isolate (CBD; Epidiolex®; Greenwich Biosciences, Inc.) has not been investigated to date. This preliminary study examines whether daily CBD dose of 15–25 mg/kg produces cerebral macrostructure changes and, if present, how they relate to changes in seizure frequency. Twenty-seven patients with treatment-resistant epilepsy were recruited from the University of Alabama at Birmingham CBD Program. Participants provided seizure frequency diaries (SF), completed the Chalfont Seizure Severity Scale (CSSS) and Adverse Events Profile (AEP), and underwent MRI before CBD (baseline) and after achieving a stable CBD dosage (on-CBD). We examined T1-weighted structural images for gray matter volume (GMV) and cortical thickness changes from baseline to on-CBD in 18 participants. Repeated measures t-tests confirmed decreases in SF [t(17) = 3.08, p = 0.0069], CSSS [t(17) = 5.77, p < 0.001], and AEP [t(17) = 3.04, p = 0.0074] between the two time-points. Voxel-level paired samples t-tests did not identify significant changes in GMV or cortical thickness between these two time-points. In conclusion, short-term exposure to highly purified CBD may not affect cortical macrostructure. Numerous studies examined the effects of cannabis on brain morphology and function [29].Despite great variability and inconsistent methodology, structural neuroimaging studies' results converge on abnormalities in CB1R-dense brain regions [29,30].For example, Battistella et al. showed that regular cannabis use reduces gray matter volume (GMV) in the temporal, insular, and orbitofrontal cortices—regions rich in CB1Rs and frequently involved in seizure initiation and generation [31].Further, studies of cannabis users have demonstrated GMV reductions and cortical thinning in the hippocampus, amygdala, and other subcortical structures, with volume reductions increasing as a function of heavier use [32].These structural abnormalities have been linked with corresponding functional deficits—for example, hippocampal GMV decreases are associated with decreased working memory [32].It is not clear whether such effects are attributable to its individual constituents e.g., Δ9-THC or CBD, or to a combination of all cannabis plant constituents [31].There are insufficient neuroimaging data on participants administered purified CBD and imaging has not examined whether neuroanatomical alternations may result from such administration to patients with TRE [28].
Mortality associated with cannabis used for treatment of epilepsy is not well documented. We discuss two fatalities in the setting of epilepsy and self-determined therapy with cannabis (SDTC). One patient had probable sudden unexpected death in epilepsy, the second death was due to seizure-associated drowning. Both directed SDTC over conventional anti-seizure medications. Where recreational cannabis is legal, decisions to use cannabis are often self-directed and independent of physician advice of cannabis risks, in part because physicians may not be aware of the risk of SDTC. Further study of morbidity and mortality of SDTC in patients with epilepsy is needed. In Washington State, marijuana legalization occurred for medicinal purposes in 1996 and for recreational use in 2012 [1].Cannabis based therapies have been suggested for a broad range or problems including anxiety, insomnia, chronic pain, depression and epilepsy [2].Publication of prospective trials supporting adjunctive effective use of cannabidiol oil (CBD) for the reduction of convulsive seizures in patients with Dravet (DS) and Lennox–Gastaut syndromes (LGS) occurred in 2017 [3,4].In our Epilepsy clinic, patient inquiries regarding efficacy of cannabis have increased since 2012 and even more since the landmark trials and further with the USA Food and Drug Administration's rescheduling of CBD in 2018.Specifically, many patients are prepared to discuss cannabis use in the epilepsy clinic and are well versed through reviews available from various online and anecdotal resources, but with limited understanding of available peer-reviewed literature, including side effects demonstrated in the 2017 LGS and DS trials or from studies of artisanal cannabis use in epilepsy [1,3,4].
The territorial characteristics and environmental factors involved in the selection of a specific site for establishing a settlement are key features in the analysis of hunter-gatherers' knowledge of (and dominance over) their surroundings and in the attempts to understand the survival strategies that they deployed. This paper presents a macrospatial analysis using GIS tools, which provides an objective comparison of territorial variables at several Magdalenian archaeological sites located in the south-eastern Pyrenees (NE Iberia). To establish the settlement patterns, we analyse territorial values: orientation, elevation, slope, sites aspect (caves, rock shelters or open air) and distance from rivers. With the data obtained, we create solar radiation models, construct groups of sites according to visibility, and calculate the displacement costs of mobility. The results suggest a series of different settlement patterns during the Magdalenian. The visibility of rivers from the archaeological sites and potential sunlight are characteristic features throughout the period, but the distance between rivers and settlements decreases diachronically. Comparison of the climate models indicates that settlements in the vicinity of the river were more frequent at times with evidence of low rainfall. Likewise, the costs of displacement from the surrounding territory to the archaeological sites increase; access to the Lower Magdalenian sites is easier, and access to the Upper Magdalenian sites much more difficult. The Magdalenian is one of the most documented and well-known Palaeolithic chrono-cultural phases in western Europe, characterized by the diversity of symbolic art, fauna and lithic remains in the archaeological record (Fullola et al., 2012; Vega et al., 2013).These records have allowed the study of changes in strategies for procuring biotic and abiotic resources and in social and symbolic behaviour, and reflect the variability and development of technical systems of lithic knapping and production of bone tools (Benito-Calvo et al., 2009; Calvo et al., 2009, 2008; Esteve, 2009; Fullola, 2001; Fullola et al., 2006; García-Diez and Vaquero, 2015; Langlais, 2010; Maluquer de Motes, 1983–1984; Mangado et al., 2014, 2013, 2010, 2009; Martínez-Moreno et al., 2007; Mithen, 1988; Montes, 2005; Mora et al., 2011; Morales and Verges, 2014; Peña and Cruz, 2014; Roman, 2016; Sánchez de la Torre, 2015; Sánchez de la Torre and Mangado, 2013; Tejero, 2009; Tejero et al., 2013; Utrilla et al., 2013, 2010; Utrilla and Mazo, 2014; Utrilla and Montes, 2009).These studies have used a variety of approaches, but spatial analyses have not been applied to date in the Magdalenian sites of north-eastern Iberia.The spatial analysis of prehistoric sites provides interesting insights into the study of settlement dynamics and mobility strategies.Moreover, the conservation and excavation of the Magdalenian sites in north-eastern Iberia has allowed paleoenvironmental reconstructions of the southern Pyrenees at the end of the Last Glacial period, coinciding with the deglaciation of the Pyrenees around 20 kyr BP (Alcolea, 2017; Allué, 2009; Allué et al., 2012a, 2012b, 2018; Aura et al., 2011; Bergadà, 1991; Burjachs, 2009; Fullola et al., 2012; Fumanal and Ferrer, 2014; Soler et al., 2009).During the Magdalenian, the climatic sequence underwent a transition between the colder episode GS-2a and the GI-1 interstadial.The interstadial GI-1 began with a rapid increase in temperatures, similar to current temperatures.Favourable temperatures were interrupted by two colder oscillations, the sub-stadials GI-1d and GI-1b (sensu Walker et al., 1999).The warm pulsation after GI-1a oscillation gradually decreased until the beginning of stadial GS-1, with very cold temperatures in a large part of Europe until the beginning of the Holocene.
The territorial characteristics and environmental factors involved in the selection of a specific site for establishing a settlement are key features in the analysis of hunter-gatherers' knowledge of (and dominance over) their surroundings and in the attempts to understand the survival strategies that they deployed. This paper presents a macrospatial analysis using GIS tools, which provides an objective comparison of territorial variables at several Magdalenian archaeological sites located in the south-eastern Pyrenees (NE Iberia). To establish the settlement patterns, we analyse territorial values: orientation, elevation, slope, sites aspect (caves, rock shelters or open air) and distance from rivers. With the data obtained, we create solar radiation models, construct groups of sites according to visibility, and calculate the displacement costs of mobility. The results suggest a series of different settlement patterns during the Magdalenian. The visibility of rivers from the archaeological sites and potential sunlight are characteristic features throughout the period, but the distance between rivers and settlements decreases diachronically. Comparison of the climate models indicates that settlements in the vicinity of the river were more frequent at times with evidence of low rainfall. Likewise, the costs of displacement from the surrounding territory to the archaeological sites increase; access to the Lower Magdalenian sites is easier, and access to the Upper Magdalenian sites much more difficult. Models of the differences in the forms of territorial occupation and exploitation can help to define the adaptation and survival strategies used by palaeolithic human groups (Binford, 1980, 1982; Brück and Goodman, 1999; Cleland, 1966; Gamble, 1986; Kelly, 1983).Interpreting the possible interactions between these groups and their immediate environment is essential to an explanation of the influence of the different strategies used for land management and natural resource catchment (Allen et al., 1990; Butzer, 1989; Chatters, 1987; Clark, 1972; Clarke, 1977; Eriksen, 1997).The choice of the site for a settlement depends on the season in which it is to be occupied, the pattern of territorial mobility and the procedures involved in the decisions made by human groups (Binford, 1980; Clark, 1972; Kelly, 1983; Saaty, 1972: 1061).A variety of factors of the landscape must be taken into consideration when deciding on its exploitation, for example its topography (location, riverbeds, vegetation), resource availability (proximity and accessibility), and conditions of liveability (dimensions, visibility and thermal comfort).
The territorial characteristics and environmental factors involved in the selection of a specific site for establishing a settlement are key features in the analysis of hunter-gatherers' knowledge of (and dominance over) their surroundings and in the attempts to understand the survival strategies that they deployed. This paper presents a macrospatial analysis using GIS tools, which provides an objective comparison of territorial variables at several Magdalenian archaeological sites located in the south-eastern Pyrenees (NE Iberia). To establish the settlement patterns, we analyse territorial values: orientation, elevation, slope, sites aspect (caves, rock shelters or open air) and distance from rivers. With the data obtained, we create solar radiation models, construct groups of sites according to visibility, and calculate the displacement costs of mobility. The results suggest a series of different settlement patterns during the Magdalenian. The visibility of rivers from the archaeological sites and potential sunlight are characteristic features throughout the period, but the distance between rivers and settlements decreases diachronically. Comparison of the climate models indicates that settlements in the vicinity of the river were more frequent at times with evidence of low rainfall. Likewise, the costs of displacement from the surrounding territory to the archaeological sites increase; access to the Lower Magdalenian sites is easier, and access to the Upper Magdalenian sites much more difficult. Quantitative and qualitative information on the territorial and environmental characteristics of archaeological sites allows the description of settlement patterns and mobility strategies of hunter-gatherer groups (Grove, 2009; Jochim, 1976).Using Geographical Information Systems (GIS) it is possible to obtain, quantify, and analyse the territorial factors that are observed qualitatively in the immediate environment of an archaeological site (Allen et al., 1990; Conolly and Lake, 2006; Eriksen, 1997; Hodder and Orton, 1990).
This paper examines the pre-contact history of the eulachon fishery on the northern Northwest Coast of North America through multiple lines of evidence: zooarchaeological, ethnographic, and oral historical. The eulachon fishery and eulachon oil production was central to Northern Tsimshian socio-political relations, systems of ownership, and trade during the contact-and-post-contact period in the region. We bring together the results of an analysis of 15 fine-screened faunal assemblages collected from village sites in Prince Rupert Harbour and compare these with published northern coast village and camp fine-screened faunal assemblages. Our results show that eulachon and other smelt taxa are present in these assemblages, suggesting a deep history to the eulachon fishery. We suggest also that the paucity of eulachon remains at some sites could be explained by eulachon oil production and consider what lines of evidence are needed to explore the limitations of zooarchaeological data and the history of eulachon oil production in the future. Archaeologists have long understood the critical role of coastal and anadromous resources in Northwest Coast (NWC) subsistence practices, but recent research is illustrating the historical depth, variability, and complexity of Indigenous fisheries in the region (Brewster and Martindale, 2011; Cannon et al., 2011; McKechnie and Moss, 2016; Moss, 2012, 2016; Moss and Cannon, 2011; Orchard and Szpak, 2015).Evidence for salmon and herring fishing, for example, exceeds 10,000 years at some sites (McKechnie et al., 2014), while coast-wide studies document the enormous diversity that existed in pre-contact-period fisheries (McKechnie and Moss, 2016; Moss and Cannon, 2011).An important contribution of these works is that they challenge conventional progressive-evolutionary models regarding NWC fishing, which have tended to emphasize salmon at the expense of other fish and to couch explanations for broader societal change in terms of intensification (e.g. Ames and Maschner, 1999; Matson and Coupland, 1994).These researchers have shown, for example, that salmon were not necessarily the “prime mover” in the development of social complexity in California, the Coast Salish area, or in Haida Gwaii (Bilton, 2014; Orchard and Clark, 2014; Orchard and Szpak, 2015; Tushingham and Christiansen, 2015); that typical indicators of intensification such as fish weirs, domestic dwellings, and other examples of the built environment have deep historical roots (Grier, 2014; Martindale et al., 2017a; Smethhurst, 2014); and that activities surrounding the capture, processing, storage, and consumption of resources other than salmon also influenced settlement practices, were central to systems of ownership, and structured socio-political relations (Cannon et al., 2011; McKechnie and Moss, 2016; Tushingham and Christiansen, 2015).
This paper examines the pre-contact history of the eulachon fishery on the northern Northwest Coast of North America through multiple lines of evidence: zooarchaeological, ethnographic, and oral historical. The eulachon fishery and eulachon oil production was central to Northern Tsimshian socio-political relations, systems of ownership, and trade during the contact-and-post-contact period in the region. We bring together the results of an analysis of 15 fine-screened faunal assemblages collected from village sites in Prince Rupert Harbour and compare these with published northern coast village and camp fine-screened faunal assemblages. Our results show that eulachon and other smelt taxa are present in these assemblages, suggesting a deep history to the eulachon fishery. We suggest also that the paucity of eulachon remains at some sites could be explained by eulachon oil production and consider what lines of evidence are needed to explore the limitations of zooarchaeological data and the history of eulachon oil production in the future. On the northern NWC and specifically within the Northern Tsimshian homeland, archaeological research has focussed on village sites in the inner harbour around the city of Prince Rupert (Letham et al., 2015) and the lower Skeena River (Fig. 1).These sites have produced large assemblages of vertebrate faunal remains that show a notable emphasis on salmon (Oncorhynchus spp.) (Coupland et al., 2010), and as such, these fish have been at the core of discussions of pre-contact settlement patterns, subsistence practices, and social relations in Northern Tsimshian territory.More work is needed, however, in order to assess the place of other marine resources in structuring these same processes.
This paper examines the pre-contact history of the eulachon fishery on the northern Northwest Coast of North America through multiple lines of evidence: zooarchaeological, ethnographic, and oral historical. The eulachon fishery and eulachon oil production was central to Northern Tsimshian socio-political relations, systems of ownership, and trade during the contact-and-post-contact period in the region. We bring together the results of an analysis of 15 fine-screened faunal assemblages collected from village sites in Prince Rupert Harbour and compare these with published northern coast village and camp fine-screened faunal assemblages. Our results show that eulachon and other smelt taxa are present in these assemblages, suggesting a deep history to the eulachon fishery. We suggest also that the paucity of eulachon remains at some sites could be explained by eulachon oil production and consider what lines of evidence are needed to explore the limitations of zooarchaeological data and the history of eulachon oil production in the future. In this paper we examine the evidence for pre-contact eulachon (Thaleichthys pacificus) fisheries in Northern Tsimshian territory through an examination of faunal remains.We look specifically at fish bone recovered in fine mesh screens during bulk sampling.First, we explore what biases might affect the recovery and interpretation of eulachon and other smelt remains in archaeological contexts in comparison to expectations drawn from written and oral historical sources; second, we consider a separate, and potentially confounding sampling issue resulting from the historical practice of storage, which separates some fish bones from flesh and oil, creating dual trajectories of use and deposition; third, we examine the relative abundance of smelts, including eulachon, in Prince Rupert Harbour (PRH) archaeological sites and present new data from fifteen village sites in this region; fourth, we compare these results with zooarchaeological assemblages from other sites in PRH and the neighbouring Dundas Islands; and fifth, we look at multiple lines of data to consider how the Northern Tsimshian might have harvested, processed, and consumed smelts in the distant past and how activities pertaining to acquiring eulachon might have influenced broader aspects of ancient Northern Tsimshian lifeways, including settlement, property rights, and trade.
A multiobjective great deluge algorithm with a two-stage external memory support and associated search operators exploiting the experience accumulated in memory are introduced. The level based acceptance criterion of the great deluge algorithm is implemented based on the dominance of a new solution against its parent and archive elements. The novel two-stage memory architecture and the use of dominance-based level approach make it possible to exploit promising solutions that both lie on better Pareto fronts in objective space and that are diversely separated in variable space. In this respect, the first stage of the external memory is managed as a short-term archive that is updated frequently when a solution that dominates its parent or some individuals over the current Pareto front is extracted whereas the second stage is organized as a long-term memory that is updated only after a number of first stage insertions. The use of memory-based search supported by effective move operators and dominance-based implementation of level mechanism within the great deluge algorithm resulted in a powerful multiobjective optimization method. The success of the presented approach is illustrated using unconstrained (bound constrained) multiobjective test instances used in the CEC’09 contest of multiobjective optimization algorithms. Using the evaluation framework described in CEC’09 contest and in comparison to published results of well-known modern algorithms, it is observed that the presented approach performs better than majority of its competitors and is a powerful alternative for multiobjective optimization. Many real-life problems can be formulated more realistically within multiobjective optimization (MOO) framework since a set solutions, rather than a single solution, exhibiting different forms of concession among multiple and often conflicting objectives is provided as result of the optimization process (Abraham et al., 2005).Such a set of solutions containing different degrees of trade-off among multiple objectives is commonly known as a Pareto-optimal set in which Pareto-optimality is defined in terms of a dominance relation between two solutions as follows: given two solutions s1 and s2, s1≠s2, s1 is said to dominate s2 if s1 is not worse than s2 in all objectives and s1 is strictly better than s2 for at least one objective.For example, for a minimization problem, minf(x)=(f1(x),f2(x),…,fP(x)), x=(x1,x2,…,xn)∈Rn, solution s1∈Rn is said to dominate solution s2∈Rn, denoted as s1⪰s2, if and only if fi(s1)≤fi(s2) for i=1,2,…,j−1,j+1,…,P and fj(s1)<fj(s2) for at least one 1≤j≤P, where P is the number of objectives.
A multiobjective great deluge algorithm with a two-stage external memory support and associated search operators exploiting the experience accumulated in memory are introduced. The level based acceptance criterion of the great deluge algorithm is implemented based on the dominance of a new solution against its parent and archive elements. The novel two-stage memory architecture and the use of dominance-based level approach make it possible to exploit promising solutions that both lie on better Pareto fronts in objective space and that are diversely separated in variable space. In this respect, the first stage of the external memory is managed as a short-term archive that is updated frequently when a solution that dominates its parent or some individuals over the current Pareto front is extracted whereas the second stage is organized as a long-term memory that is updated only after a number of first stage insertions. The use of memory-based search supported by effective move operators and dominance-based implementation of level mechanism within the great deluge algorithm resulted in a powerful multiobjective optimization method. The success of the presented approach is illustrated using unconstrained (bound constrained) multiobjective test instances used in the CEC’09 contest of multiobjective optimization algorithms. Using the evaluation framework described in CEC’09 contest and in comparison to published results of well-known modern algorithms, it is observed that the presented approach performs better than majority of its competitors and is a powerful alternative for multiobjective optimization. A common difficulty with multi-objective optimization problems is the presence of a number of conflicting objectives and, in general, none of the feasible solutions allow simultaneous optimality for all objectives.Hence, any favourable Pareto optimum provides a solution exhibiting a subjective compromise between the problem objectives.Even though some classical methods transform a multiobjective optimization problem into a single-objective one, through different scalarization and objective combination methods, and aim to obtain one single globally optimal solution, this approach includes serious drawbacks in terms of appropriate representation of the real-world problems and quality of resulting solutions.In order to have better mathematical models for real-world problems and increase the efficiency of search within arbitrarily complex solution spaces, through providing a set of solutions rather than a single solution, some advanced MOO techniques have been proposed.These techniques are generally based on some metaheuristics such as Simulated Annealing (SA)  (Kirkpatrick et al., 1983; Smith, 2006; Bandyopadhyay et al., 2008), Evolutionary Algorithms (EA) (Fonseca and Fleming, 1993; Deb et al., 2002; Zitzler et al., 2002), Particle Swarm Optimization (PSO) (Coello and Lechuga, 2002; Durillo et al., 2009; Tripathi et al., 2011), Artificial Immune Systems (AIS) (Chen and Mahfouf, 2006; Tan et al., 2008; Pierrard and Coello, 2012), Cultural Algorithms (CA) (Best et al., 2010; Srinivasan and Ramakrishnan, 2013), Tabu Search (TS) (Glover, 1986; Hansen and Hansen, 2010; Jaeggi et al., 2005), Ant Colony Optimization (ACO) (Alaya et al., 2007; -Ibanez, 2012; -Vilela et al., 2013) and Artificial Bee Colony Optimization (ABC) (Zeng et al., 2010; Zou et al., 2011; Kumar et al., 2012).Obviously, this is a restricted list of well-known metaheuristics and their associated MO implementations, new proposals are frequently arriving in this hot research field.
A multiobjective great deluge algorithm with a two-stage external memory support and associated search operators exploiting the experience accumulated in memory are introduced. The level based acceptance criterion of the great deluge algorithm is implemented based on the dominance of a new solution against its parent and archive elements. The novel two-stage memory architecture and the use of dominance-based level approach make it possible to exploit promising solutions that both lie on better Pareto fronts in objective space and that are diversely separated in variable space. In this respect, the first stage of the external memory is managed as a short-term archive that is updated frequently when a solution that dominates its parent or some individuals over the current Pareto front is extracted whereas the second stage is organized as a long-term memory that is updated only after a number of first stage insertions. The use of memory-based search supported by effective move operators and dominance-based implementation of level mechanism within the great deluge algorithm resulted in a powerful multiobjective optimization method. The success of the presented approach is illustrated using unconstrained (bound constrained) multiobjective test instances used in the CEC’09 contest of multiobjective optimization algorithms. Using the evaluation framework described in CEC’09 contest and in comparison to published results of well-known modern algorithms, it is observed that the presented approach performs better than majority of its competitors and is a powerful alternative for multiobjective optimization. Great Deluge algorithm (GDA) is a trajectory based optimization algorithm that is similar to simulated annealing except for its dynamically adjusted level-based acceptance mechanism.The algorithm starts with a randomly constructed initial solution and has four fundamental parameters to be set initially.These are the estimated value of fitness for a globally optimal solution, the maximum number of iterations, the initial value of the level and the level decay parameter.Usually, the initial value of the level parameter is set equal to the fitness of the initial solution.Throughout the execution of GDA, the value of the level parameter is decayed linearly or nonlinearly, and the acceptance of new solutions depends on the level parameter.A detailed description of GDA is presented in Section 2.3.
A multiobjective great deluge algorithm with a two-stage external memory support and associated search operators exploiting the experience accumulated in memory are introduced. The level based acceptance criterion of the great deluge algorithm is implemented based on the dominance of a new solution against its parent and archive elements. The novel two-stage memory architecture and the use of dominance-based level approach make it possible to exploit promising solutions that both lie on better Pareto fronts in objective space and that are diversely separated in variable space. In this respect, the first stage of the external memory is managed as a short-term archive that is updated frequently when a solution that dominates its parent or some individuals over the current Pareto front is extracted whereas the second stage is organized as a long-term memory that is updated only after a number of first stage insertions. The use of memory-based search supported by effective move operators and dominance-based implementation of level mechanism within the great deluge algorithm resulted in a powerful multiobjective optimization method. The success of the presented approach is illustrated using unconstrained (bound constrained) multiobjective test instances used in the CEC’09 contest of multiobjective optimization algorithms. Using the evaluation framework described in CEC’09 contest and in comparison to published results of well-known modern algorithms, it is observed that the presented approach performs better than majority of its competitors and is a powerful alternative for multiobjective optimization. Considering that a Pareto front resulting from a MO search algorithm contains a set of nondominated solutions, almost all MO metaheuristics maintain an archive (memory) keeping the latest extracted Pareto front and use this archive also for efficient guidance of the search procedure.Using archive elements within the mechanisms of search operators allow intensification of search around the potentially promising solutions found so far while also diversifying the exploration within the solution space by using more than one reference template within the maintained archive.
A multiobjective great deluge algorithm with a two-stage external memory support and associated search operators exploiting the experience accumulated in memory are introduced. The level based acceptance criterion of the great deluge algorithm is implemented based on the dominance of a new solution against its parent and archive elements. The novel two-stage memory architecture and the use of dominance-based level approach make it possible to exploit promising solutions that both lie on better Pareto fronts in objective space and that are diversely separated in variable space. In this respect, the first stage of the external memory is managed as a short-term archive that is updated frequently when a solution that dominates its parent or some individuals over the current Pareto front is extracted whereas the second stage is organized as a long-term memory that is updated only after a number of first stage insertions. The use of memory-based search supported by effective move operators and dominance-based implementation of level mechanism within the great deluge algorithm resulted in a powerful multiobjective optimization method. The success of the presented approach is illustrated using unconstrained (bound constrained) multiobjective test instances used in the CEC’09 contest of multiobjective optimization algorithms. Using the evaluation framework described in CEC’09 contest and in comparison to published results of well-known modern algorithms, it is observed that the presented approach performs better than majority of its competitors and is a powerful alternative for multiobjective optimization. This paper introduces a two-stage external memory based implementation of GDA for real-valued multiobjective optimization problems.The objective is to use experience-based knowledge stored in a two-stage memory to localize the globally optimal solutions better while GDA’s level-based exploration within the solution space is also exploited.In this proposal, the first stage acts as a short-term memory that is updated frequently and it keeps promising solutions that dominate either their parents or a number of solutions on the current Pareto front.The second stage acts as a long-term memory that stores promising solutions from separate areas of parametric solution space so that further intensification can be performed around these solutions before they are being updated.Accordingly, the second stage memory is updated less frequently, after updating the first stage for a number of times, to allow exploitation of promising solutions within this stage in generation of multiple offspring individuals.
A multiobjective great deluge algorithm with a two-stage external memory support and associated search operators exploiting the experience accumulated in memory are introduced. The level based acceptance criterion of the great deluge algorithm is implemented based on the dominance of a new solution against its parent and archive elements. The novel two-stage memory architecture and the use of dominance-based level approach make it possible to exploit promising solutions that both lie on better Pareto fronts in objective space and that are diversely separated in variable space. In this respect, the first stage of the external memory is managed as a short-term archive that is updated frequently when a solution that dominates its parent or some individuals over the current Pareto front is extracted whereas the second stage is organized as a long-term memory that is updated only after a number of first stage insertions. The use of memory-based search supported by effective move operators and dominance-based implementation of level mechanism within the great deluge algorithm resulted in a powerful multiobjective optimization method. The success of the presented approach is illustrated using unconstrained (bound constrained) multiobjective test instances used in the CEC’09 contest of multiobjective optimization algorithms. Using the evaluation framework described in CEC’09 contest and in comparison to published results of well-known modern algorithms, it is observed that the presented approach performs better than majority of its competitors and is a powerful alternative for multiobjective optimization. The main motivation for implementing the proposed two-stage memory architecture within the GDA framework is the successful implementation of this idea in two recently published articles by  Acan and Unveren (2015a, b).In Acan and Unveren (2015a), a two-stage external memory is maintained within GDA and this approach, together with the associated memory-based search operators, is used for the solution of single-objective hard numerical optimization problems, namely the CEC2005 and CEC2010 competition benchmark problems.Results obtained for these multimodal and large-scale global optimization problems using the approach presented in Acan and Unveren (2015a) are highly competitive to those found by state-of-the-art metaheuristic which attended to both of the associated competitions.In Acan and Unveren (2015b), a similar algorithm is applied for the solution of quadratic assignment problem (QAP).The results presented in this article, for the solution of hard QAP problems, also showed that the proposed two-stage external memory architecture within the GDA framework and with dedicated memory-based combinatorial search operators reached close to optimal results that are as good as those found by well-known state-of-the-art algorithms for QAP.These highly successful studies led us to apply the two-stage external memory architecture and search principles of GDA for the solution of multiobjective optimization problems, the algorithmic description and experimental evaluations of the resulting algorithm are presented in the following sections.
Morphological and morphometric bone variation between archaeological wolves and the oldest domestic dogs commonly are used to define species differences. However, reference data often have been based on small numbers, without robust statistical support. We consulted the literature on these matters in all possible languages and tested many of the proposed species differences by examining wolf and dog skeletons from several collections, accompanied by an extensive synthesis of existing literature. We thus created large reference groups, assessing data distributions and variability. We examined mandible height, width, length, and convexity; contact points of the skull on a horizontal plane; caudal shifting of the border of the hard palate; skull size; carnassials tooth size reduction; micro-anatomical differences in teeth, snout, and skull height; and snout length and width. Our results show that skull length and related size; skull height; snout width; orbital angle; P4 and M1 mesio-distal diameter can help (albeit to a limited extent) to distinguish the oldest archaeological dogs from wolves. Based on our observations, we re-evaluated recent large Pleistocene canids reported as Paleolithic dogs and concluded instead that they fit well within the morphomentric distributions seen with Pleistocene wolves. The research presented here reflects the recent trend to critically re-evaluate axiomatic assumptions about wolf-dog differences, and to rephrase the morphological and morphometric definition of an early archaeological dog in a more suitable manner. These results are important to the international archaeological community because they place historical reports in a newer context, and create a robust (although narrow) framework for further evaluation of archaeological dogs and wolves. Researchers have examined the origins of dogs for over a century (Galton, 1865; Gaudry and Boule, 1892; Huxley, 1880; Nehring, 1888; Rütimeyer, 1861; Studer, 1901; Verworn et al., 1919; Wolfgram, 1894) and have used specific morphometric and morphological criteria to assign specimens as dog or wolf.
Morphological and morphometric bone variation between archaeological wolves and the oldest domestic dogs commonly are used to define species differences. However, reference data often have been based on small numbers, without robust statistical support. We consulted the literature on these matters in all possible languages and tested many of the proposed species differences by examining wolf and dog skeletons from several collections, accompanied by an extensive synthesis of existing literature. We thus created large reference groups, assessing data distributions and variability. We examined mandible height, width, length, and convexity; contact points of the skull on a horizontal plane; caudal shifting of the border of the hard palate; skull size; carnassials tooth size reduction; micro-anatomical differences in teeth, snout, and skull height; and snout length and width. Our results show that skull length and related size; skull height; snout width; orbital angle; P4 and M1 mesio-distal diameter can help (albeit to a limited extent) to distinguish the oldest archaeological dogs from wolves. Based on our observations, we re-evaluated recent large Pleistocene canids reported as Paleolithic dogs and concluded instead that they fit well within the morphomentric distributions seen with Pleistocene wolves. The research presented here reflects the recent trend to critically re-evaluate axiomatic assumptions about wolf-dog differences, and to rephrase the morphological and morphometric definition of an early archaeological dog in a more suitable manner. These results are important to the international archaeological community because they place historical reports in a newer context, and create a robust (although narrow) framework for further evaluation of archaeological dogs and wolves. Some of these assignment criteria have been rejected, including:backward turning of the dorsal part of the vertical mandibular ramus (Olsen and Olsen, 1977), because the feature is present in both dogs and wolves (Janssens et al., 2016a);variations in types and frequencies of oral and dental abnormalities such as tooth agenesis or occlusion-related pathology (Andersone and Ozolins, 2000; Stockhaus, 1965; Vila et al., 1993; Wobeser, 1992) that are similar among wolves and dogs (Janssens et al., 2016b);paedomorphosis (juvenile appearance in dogs) (Morey, 1994; Waller et al., 2013) for which the typical morphological criteria are lacking in dogs (Drake, 2011);tooth crowding in dogs (Benecke, 1994; Wolfgram, 1894) that occurs also in wolves (Ameen et al., 2017);larger tympanic bulla in wolves (Benecke, 1987; Bökönyi, 1975; Zeuner, 1963) that is not species-specific, but is related isometrically to stature (Stockhaus, 1965);differences in the sum of maxillary mesio-distal diameters of M1 + M2, being less than P4 in dogs and greater in wolves (Clutton-Brock, 1963); many possible variations exist in dogs and wolves (Gaudry and Boule, 1892; Wolfgram, 1894);reduced sagittal crest in dogs (Studer, 1901) (bony protuberance in midline sagittal suture line located at the level of the parietal bones) given that dogs are quite variable in this criteria (Lawrence and Bossert, 1967; Rizk, 2012);clear difference in orbital angle (OA) between dogs and wolves (Bockelmann, 1920; Iljin, 1941; Studer, 1901) that was contradicted by a recent study showing there was full overlap between archaeological dogs and modern wolves (Janssens et al., 2016c);convex mandible in dogs versus a straight one in wolves (Germonpré et al., 2015; Lawrence and Reed, 1983), that was agreed on in modern samples but incorrect in archaeological samples (Drake et al., 2017).
Variation in textile production processes from archaeological contexts can distinguish communities of weavers and signal distinct group identities. In this paper, we present an analysis of 141 textiles recovered from a single grave located in the mid-Chincha Valley, Peru that dates from the Late Horizon (1400–1532 CE) to the Colonial Period (1532–1825 CE). This sample represents one of the largest and best-preserved textile assemblages from a clearly defined and radiocarbon-dated archaeological context in the Chincha Valley. For this study, we document techniques used in two distinct phases of textile production: yarn production (spinning and plying) and weaving. We 1) develop a manual hierarchical classification method for identifying groups of textiles featuring consistent associations among techniques used for each production phase and 2) assess how these groups vary in terms of thread count, size, garment type, and design. Our results reveal six groupings of textile production techniques that account for 71% of the assemblage by count. We compare these results to that of an independent cluster analysis that examines the joint co-occurrence of yarn production and weaving techniques and find that they are largely in accordance with each other. We suggest that these multiple textile groups corresponded to distinct communities or households of weavers associated with this grave. Our study provides a methodology for analyzing the variation and consistency of textile production to learn about communities of weavers within and outside the Andes. Craft production broadly entails the transformation of raw materials into finished products.Several anthropological studies on technology describe production as a social process mediated by communities and individuals (Dobres, 2000; Lemonnier, 1986, 1992, 2013).Through this process and the identities of the individuals who made them, products acquire value, meaning, and power (Clark and Parry, 1990; Costin, 1998a, b, 2001, p. 274; Lechtman, 1993).Scholars draw attention to the body as a medium through which techniques are learned, transmitted, and employed, and assert that these activities occur in social environments that integrate people, relationships, and material culture (Budden and Sofaer, 2009; Dobres, 2000, pp. 131, 137; Mauss, 2006).Embodied techniques and motor skills can leave physical traces on finished products, and they can reveal social norms because they are frequently learned through imitation and repetition in social contexts (Minar, 2001; Tiballi, 2010, p. 146).Methodologies derived from the concept of chaîne opératoire, translated as “chain of operations,” consider traces left by embodied techniques to model sequences of actions and decisions that characterize the life history of an artifact (Lemonnier, 1986, 1992, 2013; Leroi-Gourhan, 1943; Mauss, 2006).Different sequences of production can foster variations in the material record, variations that may connect to style (Carr, 1995; Conkey and Hastorf, 1990; Sackett, 1977, 1982, 1990; Wiessner, 1983, 1984; Wobst, 1977), group identity (Brumfiel, 1998; Costin, 1998a, 1998b), individual experimentation and personal preference, and cultural and community-based norms and preferences (Dobres, 2000; Peters, 2014; Tiballi, 2010).
Variation in textile production processes from archaeological contexts can distinguish communities of weavers and signal distinct group identities. In this paper, we present an analysis of 141 textiles recovered from a single grave located in the mid-Chincha Valley, Peru that dates from the Late Horizon (1400–1532 CE) to the Colonial Period (1532–1825 CE). This sample represents one of the largest and best-preserved textile assemblages from a clearly defined and radiocarbon-dated archaeological context in the Chincha Valley. For this study, we document techniques used in two distinct phases of textile production: yarn production (spinning and plying) and weaving. We 1) develop a manual hierarchical classification method for identifying groups of textiles featuring consistent associations among techniques used for each production phase and 2) assess how these groups vary in terms of thread count, size, garment type, and design. Our results reveal six groupings of textile production techniques that account for 71% of the assemblage by count. We compare these results to that of an independent cluster analysis that examines the joint co-occurrence of yarn production and weaving techniques and find that they are largely in accordance with each other. We suggest that these multiple textile groups corresponded to distinct communities or households of weavers associated with this grave. Our study provides a methodology for analyzing the variation and consistency of textile production to learn about communities of weavers within and outside the Andes. Two considerations emerge from these theoretical and methodological frameworks that are of relevance to this paper.First, if techniques and other forms of non-discursive knowledge (Budden and Sofaer, 2009, p. 203) related to the production of an artifact were learned and transmitted in social contexts and potentially across generations, then it is likely that these activities generated technical preferences among those engaged in their manufacture.These preferences may materialize as consistency in technical attributes associated with different steps in the production process (Peters, 2014, p. 113).Second, if materials were used that require multiple phases of manufacturing—necessitating distinct techniques—it is possible that different communities participated in their production.Therefore, we propose that materials with apparent, consistent techniques across multiple phases of production correspond to preferences that marked distinct communities of weavers.We apply this model to an assemblage of well-preserved Andean textiles to gain insights into the communities involved in their production.
Variation in textile production processes from archaeological contexts can distinguish communities of weavers and signal distinct group identities. In this paper, we present an analysis of 141 textiles recovered from a single grave located in the mid-Chincha Valley, Peru that dates from the Late Horizon (1400–1532 CE) to the Colonial Period (1532–1825 CE). This sample represents one of the largest and best-preserved textile assemblages from a clearly defined and radiocarbon-dated archaeological context in the Chincha Valley. For this study, we document techniques used in two distinct phases of textile production: yarn production (spinning and plying) and weaving. We 1) develop a manual hierarchical classification method for identifying groups of textiles featuring consistent associations among techniques used for each production phase and 2) assess how these groups vary in terms of thread count, size, garment type, and design. Our results reveal six groupings of textile production techniques that account for 71% of the assemblage by count. We compare these results to that of an independent cluster analysis that examines the joint co-occurrence of yarn production and weaving techniques and find that they are largely in accordance with each other. We suggest that these multiple textile groups corresponded to distinct communities or households of weavers associated with this grave. Our study provides a methodology for analyzing the variation and consistency of textile production to learn about communities of weavers within and outside the Andes. Textiles are ideal materials for this study.As structurally complex materials, they retain physical traces of a multi-stage production process described by Peters (2014, p. 111) as “agglutinative.”Textile production broadly consists of several phases: 1) harvesting and processing cotton or camelid hair fibers, 2) spinning these fibers and plying them into yarns, 3) dyeing or not dyeing these yarns, and then 4) manipulating yarns through weaving or non-weaving techniques (e.g., looping, braiding, etc.) to produce textiles, which can be further modified through painting, embroidery, or attachment to other textiles.Close examination of various features in a single finished textile such as spin direction, ply, dye, and weave structure, can determine the techniques used by weavers throughout the production process.Given the multiple choices available to weavers during each phase, such as spinning a fiber in either the S direction or the Z direction, textiles from a given archaeological context that display shared techniques demonstrate consistency and preference in the production process.We define such groups of textiles as “style groups,” following Peters (2014), which may reflect communities of weavers.We also follow Sackett's (1982, p. 63) broad definition of style as a “highly specific and characteristic way of doing something which by its very nature is peculiar to a time and space.”Ethnographic and archaeological research suggest that weavers within the Andes learned and transmitted technical knowledge in group environments which in turn, created preferences in textile production practices that distinguished communities and cultures and expressed group identity (Bird, 1979; Brezine, 2013, p. 240; Franquemont, 1986; Hendon, 2006; Medlin, 1986; Rowe, 1998, p. 22; Splitstoser, 2009; Wallace, 1979).
Beads were an important form of personal ornamentation in southern Levantine Pre-Pottery Neolithic societies and hold information on symbolic practices and exchange networks engaged by these communities. Establishing the exact mineralogical composition of stone beads and the precise methods used to manufacture beads are key to documenting shifts in these systems, but so far visual inspection has served as the primary mode of analysis for most assemblages. Here, we apply XRD and 3D digital optical microscopy to investigate raw material selection and bead manufacture at the Pre-Pottery Neolithic (PPN) settlement of el-Hemmeh, Jordan. Analyses reveal that PPNA bead assemblages emphasize the color green but included a wide variety of mineral types. The LPPNB assemblage exhibits a greater diversity in shape and color, including an emphasis on the use of red minerals, but consist of a more restricted range of minerals and lack the rectangular beads found in the PPNA. Color rather than suitability of material properties for bead making appears to have motivated choices for mineral selection. Bidirectional perforation was codified into the technological system and was consistently used to produce beads during the PPNA regardless of their size, thickness, or hardness. Stone beads are ubiquitous objects of material culture present in settlement and mortuary deposits that hold information on symbolic systems and participation in exchange networks.As personal adornment items, beads accrue social and symbolic meaning and are used by individuals to express group affiliation, reify social roles and status, and exchange cultural capital (Dubin, 2009; Jones, 2004; Lankton, 2003).In addition to signaling qualities associated with the use of beads as ornamentation, the series of technological and stylistic choices with bead manufacture are determined by both the physical properties of the raw material as well as cultural traditions, symbolic practices, and social networks (Lechtman, 1977; Lechtman, 1999; Hosler, 1994).
Beads were an important form of personal ornamentation in southern Levantine Pre-Pottery Neolithic societies and hold information on symbolic practices and exchange networks engaged by these communities. Establishing the exact mineralogical composition of stone beads and the precise methods used to manufacture beads are key to documenting shifts in these systems, but so far visual inspection has served as the primary mode of analysis for most assemblages. Here, we apply XRD and 3D digital optical microscopy to investigate raw material selection and bead manufacture at the Pre-Pottery Neolithic (PPN) settlement of el-Hemmeh, Jordan. Analyses reveal that PPNA bead assemblages emphasize the color green but included a wide variety of mineral types. The LPPNB assemblage exhibits a greater diversity in shape and color, including an emphasis on the use of red minerals, but consist of a more restricted range of minerals and lack the rectangular beads found in the PPNA. Color rather than suitability of material properties for bead making appears to have motivated choices for mineral selection. Bidirectional perforation was codified into the technological system and was consistently used to produce beads during the PPNA regardless of their size, thickness, or hardness. In the Near East, stone beads were a form of personal expression from the end of the tenth through the seventh millennium cal.BC during the Pre-Pottery Neolithic when sedentary communities first experimented with food production and new forms of symbolic expression.The increased use of beads, whether made of stone, shell, bone, or wood, during the Pre-Pottery Neolithic may have reflected the emergence of novel symbolic systems and signaling behaviors that developed in order to deal with new and unfamiliar forms of social organization (Verhoeven, 2002; Wright, 2009).Increasingly elaborated concepts of identity or metaphorical thinking (Mithen et al., 2005; Twiss, 2007; Wright, 2009) or the rise of prestige technology (Bains, 2012) may have been crucial for signaling participation in these new social roles.
Beads were an important form of personal ornamentation in southern Levantine Pre-Pottery Neolithic societies and hold information on symbolic practices and exchange networks engaged by these communities. Establishing the exact mineralogical composition of stone beads and the precise methods used to manufacture beads are key to documenting shifts in these systems, but so far visual inspection has served as the primary mode of analysis for most assemblages. Here, we apply XRD and 3D digital optical microscopy to investigate raw material selection and bead manufacture at the Pre-Pottery Neolithic (PPN) settlement of el-Hemmeh, Jordan. Analyses reveal that PPNA bead assemblages emphasize the color green but included a wide variety of mineral types. The LPPNB assemblage exhibits a greater diversity in shape and color, including an emphasis on the use of red minerals, but consist of a more restricted range of minerals and lack the rectangular beads found in the PPNA. Color rather than suitability of material properties for bead making appears to have motivated choices for mineral selection. Bidirectional perforation was codified into the technological system and was consistently used to produce beads during the PPNA regardless of their size, thickness, or hardness. In the southern Levant, stone beads appear to have first come into use during the Late Natufian (11500 to 10750 cal.BC; Bar-Yosef Mayer and Porat, 2008).The Natufians, semi-sedentary hunter-gatherers who lived in well-constructed settlements often containing buildings constructed of stone, fashioned beads out of stone with a preference for green colored raw materials and used these beads, as well as shell beads, as objects of personal ornamentation (Bar-Yosef Mayer, 2008; Bar-Yosef, 1999).Green beads made out of stone continued to rise to prominence during the Pre-Pottery Neolithic A (ca. 9700 to 8500 cal.BC) when semi-sedentary communities began experimentation with plant cultivation (Bar-Yosef Mayer and Porat, 2008; Wright and Garrard, 2003).Green minerals used to make these beads were sourced from a variety of locations some distance from the settlements in which they were used, suggesting that extended social or trade networks were in place at the time.Less is known about bead use later during the Late PPNB (7250 to 6700 cal.BC) when domesticated sheep and goats were husbanded and settlements increased dramatically in size and occupation density (Kuijt and Goring-Morris, 2002; Makarewicz, 2013)
Changes in the importance of agriculture in prehistoric economies are of major interest in a range of contexts worldwide. Measures of site location in relation to agricultural potential are an important tool for identifying relative shifts in the importance of agriculture over time within a given region. Here we examine the application of GIS modeling of agricultural potential based on soil characteristics, topography, and proximity to drainage in the highlands of the Jornada branch of the Mogollon culture of southcentral New Mexico to explore shifts in agricultural reliance over time. We describe the methods, potential limitations, and potential advantages of this approach, as well as preliminary results. Identifying variables that reliably predict agricultural potential is complicated by the limited resolution of available data for the large study area, overlapping legal jurisdictions (state versus federal land), complex topography, as well as by a lack of modern non-mechanized farming data to ground-truth estimates of relative productivity. Nevertheless, the approach allows comparisons for relative changes in settlement placement over time in relation to variables that are likely to impact agricultural productivity. Our results support other evidence of strong agricultural reliance in the pithouse period, substantially greater than in the Archaic; the pueblo period occupation may be slightly more tightly linked to optimal agricultural land, though the latter conclusion is uncertain. The evolution of agricultural life is associated with a group of developments sometimes referred to as “the Neolithic package” in the Old World and reflected in parallel developments in New World Formative societies.Aside from the adoption of agriculture itself, these changes typically include major technological transformations (for instance construction of substantial dwellings and storage facilities and widespread use of pottery and food grinding technologies), and changes in material manifestations of social patterns (particularly aggregation into sedentary village communities of integrated but distinct households).The interaction among these developments is of fundamental interest given their near universality and their role in creating the foundations of subsequent urbanization and complex social formations.However, despite the apparent consistency of such a “package,” the links among its constituents are variable, a factor noted even in early descriptions of Neolithic and Formative patterns (e.g., Childe, 1936; Willey and Phillips, 1958), and the concept of a rigid Neolithic package has been heavily criticized in recent decades (e.g., Jordan and Zvelebil, 2009; Thomas, 1999).
Changes in the importance of agriculture in prehistoric economies are of major interest in a range of contexts worldwide. Measures of site location in relation to agricultural potential are an important tool for identifying relative shifts in the importance of agriculture over time within a given region. Here we examine the application of GIS modeling of agricultural potential based on soil characteristics, topography, and proximity to drainage in the highlands of the Jornada branch of the Mogollon culture of southcentral New Mexico to explore shifts in agricultural reliance over time. We describe the methods, potential limitations, and potential advantages of this approach, as well as preliminary results. Identifying variables that reliably predict agricultural potential is complicated by the limited resolution of available data for the large study area, overlapping legal jurisdictions (state versus federal land), complex topography, as well as by a lack of modern non-mechanized farming data to ground-truth estimates of relative productivity. Nevertheless, the approach allows comparisons for relative changes in settlement placement over time in relation to variables that are likely to impact agricultural productivity. Our results support other evidence of strong agricultural reliance in the pithouse period, substantially greater than in the Archaic; the pueblo period occupation may be slightly more tightly linked to optimal agricultural land, though the latter conclusion is uncertain. Critical areas for research include both expanding the tools for individually evaluating elements of the “package” and examining the pattern of covariation among its constituent parts.Here we consider an approach to assessing agricultural dependence in the context of the settlement changes associated with the origins of village formation (the Formative period) in the United States Southwest, focusing on the highlands of south-central New Mexico.The goals are twofold, methodological and analytical.First, we develop an approach to assessing agricultural dependence using a GIS based model of agricultural potential in relation to site placement; we model potential based on soil characteristics, topography, and proximity to drainage.Then, we apply this model to compare agricultural dependence across time spanning the early period of village formation associated with pithouses, and then, pueblo occupations in this region.
Changes in the importance of agriculture in prehistoric economies are of major interest in a range of contexts worldwide. Measures of site location in relation to agricultural potential are an important tool for identifying relative shifts in the importance of agriculture over time within a given region. Here we examine the application of GIS modeling of agricultural potential based on soil characteristics, topography, and proximity to drainage in the highlands of the Jornada branch of the Mogollon culture of southcentral New Mexico to explore shifts in agricultural reliance over time. We describe the methods, potential limitations, and potential advantages of this approach, as well as preliminary results. Identifying variables that reliably predict agricultural potential is complicated by the limited resolution of available data for the large study area, overlapping legal jurisdictions (state versus federal land), complex topography, as well as by a lack of modern non-mechanized farming data to ground-truth estimates of relative productivity. Nevertheless, the approach allows comparisons for relative changes in settlement placement over time in relation to variables that are likely to impact agricultural productivity. Our results support other evidence of strong agricultural reliance in the pithouse period, substantially greater than in the Archaic; the pueblo period occupation may be slightly more tightly linked to optimal agricultural land, though the latter conclusion is uncertain. There are two basic approaches available for modeling prehistoric agricultural potential across the landscape.Perhaps best are the approaches that use data on known locations of maximum agricultural productivity to train the model to rank other locations for agricultural potential (Healy et al., 2017).As discussed below, the existing data from south-central New Mexico do not currently permit this approach.The alternative approach that we adopt here is to model agricultural potential through first principles of environmental characteristics of soils and location.
Changes in the importance of agriculture in prehistoric economies are of major interest in a range of contexts worldwide. Measures of site location in relation to agricultural potential are an important tool for identifying relative shifts in the importance of agriculture over time within a given region. Here we examine the application of GIS modeling of agricultural potential based on soil characteristics, topography, and proximity to drainage in the highlands of the Jornada branch of the Mogollon culture of southcentral New Mexico to explore shifts in agricultural reliance over time. We describe the methods, potential limitations, and potential advantages of this approach, as well as preliminary results. Identifying variables that reliably predict agricultural potential is complicated by the limited resolution of available data for the large study area, overlapping legal jurisdictions (state versus federal land), complex topography, as well as by a lack of modern non-mechanized farming data to ground-truth estimates of relative productivity. Nevertheless, the approach allows comparisons for relative changes in settlement placement over time in relation to variables that are likely to impact agricultural productivity. Our results support other evidence of strong agricultural reliance in the pithouse period, substantially greater than in the Archaic; the pueblo period occupation may be slightly more tightly linked to optimal agricultural land, though the latter conclusion is uncertain. Our project area lies in the Sierra Blanca and Capitan Mountains and adjoining parts of southern Lincoln County, New Mexico (Fig. 1).The Formative period archaeological materials of this region fall within the Jornada Branch of the Mogollon archaeological culture area (Lehmer, 1948).Our study focuses on a transect from the high elevation portion of this region to its foothills, but does not include the better studied adjoining lowland basins to the south and west which show a different pattern of economic and settlement development (Rocek and Rautman, 2012; Railey and Turnbow, 2018).
Changes in the importance of agriculture in prehistoric economies are of major interest in a range of contexts worldwide. Measures of site location in relation to agricultural potential are an important tool for identifying relative shifts in the importance of agriculture over time within a given region. Here we examine the application of GIS modeling of agricultural potential based on soil characteristics, topography, and proximity to drainage in the highlands of the Jornada branch of the Mogollon culture of southcentral New Mexico to explore shifts in agricultural reliance over time. We describe the methods, potential limitations, and potential advantages of this approach, as well as preliminary results. Identifying variables that reliably predict agricultural potential is complicated by the limited resolution of available data for the large study area, overlapping legal jurisdictions (state versus federal land), complex topography, as well as by a lack of modern non-mechanized farming data to ground-truth estimates of relative productivity. Nevertheless, the approach allows comparisons for relative changes in settlement placement over time in relation to variables that are likely to impact agricultural productivity. Our results support other evidence of strong agricultural reliance in the pithouse period, substantially greater than in the Archaic; the pueblo period occupation may be slightly more tightly linked to optimal agricultural land, though the latter conclusion is uncertain. Traces of maize horticulture in the Jornada Mogollon date back over 3000 calibrated radiocarbon years before present but are not initially accompanied by most of the other indications of the Neolithic package.By around 2000 years ago, this region shows growing evidence of agricultural dependence in the form of abundant finds of maize and large bell-shaped storage pits (Campbell and Railey, 2008; Rocek and Rautman, 2012; Railey and Turnbow, 2018; Wiseman, 1996).In the mid-sixth century A.D. (around 1400 cal BP), clusters of large round pit dwellings and pottery appear in quick succession (Campbell and Railey, 2008; Rocek, 2013).The period ca. 1100–1200 CE suggests perhaps a mix of architectural forms, and by ca. 1200 CE, we see an aggregation into villages of surface pueblo architecture in the northern portion of the area (the Lincoln Phase) or tightly packed rectangular pit structures to the south (the late Glencoe Phase) (Kelley, 1984).These late “pueblo period” sites have all the hallmarks of heavily agriculturally based integrated village communities typical of the full Formative or Neolithic concept.The social and economic pattern characteristic of the earlier “pithouse period,” dominated primarily by round pithouses (usually assigned to the early Glencoe Phase1) is less clear.Rocek (1995) has argued based on the frequent correlation of pithouse architecture and extramural pit storage with seasonal site abandonment that the pithouse period is characterized by significant although heavily tethered seasonal mobility.Analysis of the pattern of structure chronological and spatial overlap among structures also suggests that pithouse period sites in this area may be loose palimpsests of relatively isolated farmsteads rather than integrated village communities (Rocek, 2013).
Changes in the importance of agriculture in prehistoric economies are of major interest in a range of contexts worldwide. Measures of site location in relation to agricultural potential are an important tool for identifying relative shifts in the importance of agriculture over time within a given region. Here we examine the application of GIS modeling of agricultural potential based on soil characteristics, topography, and proximity to drainage in the highlands of the Jornada branch of the Mogollon culture of southcentral New Mexico to explore shifts in agricultural reliance over time. We describe the methods, potential limitations, and potential advantages of this approach, as well as preliminary results. Identifying variables that reliably predict agricultural potential is complicated by the limited resolution of available data for the large study area, overlapping legal jurisdictions (state versus federal land), complex topography, as well as by a lack of modern non-mechanized farming data to ground-truth estimates of relative productivity. Nevertheless, the approach allows comparisons for relative changes in settlement placement over time in relation to variables that are likely to impact agricultural productivity. Our results support other evidence of strong agricultural reliance in the pithouse period, substantially greater than in the Archaic; the pueblo period occupation may be slightly more tightly linked to optimal agricultural land, though the latter conclusion is uncertain. On the other hand, high agricultural dependence in this period is suggested by the abundance of the large pit storage features, high maize ubiquity (and high ubiquity rank relative to that of other seeds) that are indistinguishable from the pueblo period (Rocek, 1995, 2007; see also e.g., Campbell and Railey, 2008; Hard et al., 1996; Railey and Turnbow, 2018).This suggests a disjunction in the Neolithic package, with agricultural dependence significantly preceding settlement and social changes associated with aggregation into settled village communities.The current project aims to develop and apply a model of agricultural potential to examine whether the pithouse period sites are positioned on the landscape to maximize proximity to good farmland, consistent with the argument that they are already fundamentally committed to agriculture, or whether their locations imply a more limited focus on agricultural resources or compromise with other concerns.
This paper addresses sequential hypothesis testing for Markov models of time-series data by using the concepts of symbolic dynamics. These models are inferred by discretizing the measurement space of a dynamical system, where the system dynamics are approximated as a finite-memory Markov chain on the discrete state space. The study is motivated by time-critical detection problems in physical processes, where a temporal model is trained to make fast and reliable decisions with streaming data. Sequential update rules have been constructed for log-posterior ratio statistic of Markov models in the setting of binary hypothesis testing and the stochastic evolution of this statistic is analyzed. The proposed technique allows selection of a lower bound on the performance of the detector and guarantees that the test will terminate in finite time. The underlying algorithms are first illustrated through an example by numerical simulation, and are subsequently validated on time-series data of pressure oscillations from a laboratory-scale swirl-stabilized combustor apparatus to detect the onset of thermo-acoustic instability. The performance of the proposed sequential hypothesis tests for Markov models has been compared with that of a maximum-likelihood classifier with fixed sample size (i.e., sequence length). It is shown that the proposed method yields reliable detection of combustion instabilities with fewer observations in comparison to a fixed-sample-size test. Recently there has been an unprecedented increase in the volume and speed of temporal data being generated by physical systems, due to the improvements in low-cost sensing and high-speed computation & communication.Typical machine learning tools, used for monitoring of physical processes, extract features from a fixed number of consecutive observations and pose a supervised learning problem for detection or classification using those features (Bishop, 2006).Similarly, fixed-sample-size (FSS) tests from the estimation theory literature (Poor, 1994) compute likelihood or posterior probability of an observed sequence of fixed length and select the class which maximizes this statistic.However, in order to perform well on all feasible sequences, the chosen sample length in these approaches is made longer than needed for most of the easily separable cases.For example, in time-critical online monitoring systems such as detection of combustion instabilities in aircraft gas-turbine engines, an early detection can enhance mitigation of structural damage in engines by avoiding the thermo-acoustic resonance and may even prevent accidents due to engine shutdown.A dynamic data-driven approach (Darema, 2004) for sequential detection and classification is needed, which can adapt the observed length of time series without any significant computational burden.Sequential hypothesis tests offer one such efficient and adaptive framework, which would allow online detection of anomalies, faults, and mode transitions in dynamical systems, where high-speed streaming data are generated.This paper presents a sequential hypothesis testing procedure for a class of Markov models of temporal data inferred by using symbolic time-series analysis (STSA) (Daw et al., 2003; Beim Graben, 2001).
This paper addresses sequential hypothesis testing for Markov models of time-series data by using the concepts of symbolic dynamics. These models are inferred by discretizing the measurement space of a dynamical system, where the system dynamics are approximated as a finite-memory Markov chain on the discrete state space. The study is motivated by time-critical detection problems in physical processes, where a temporal model is trained to make fast and reliable decisions with streaming data. Sequential update rules have been constructed for log-posterior ratio statistic of Markov models in the setting of binary hypothesis testing and the stochastic evolution of this statistic is analyzed. The proposed technique allows selection of a lower bound on the performance of the detector and guarantees that the test will terminate in finite time. The underlying algorithms are first illustrated through an example by numerical simulation, and are subsequently validated on time-series data of pressure oscillations from a laboratory-scale swirl-stabilized combustor apparatus to detect the onset of thermo-acoustic instability. The performance of the proposed sequential hypothesis tests for Markov models has been compared with that of a maximum-likelihood classifier with fixed sample size (i.e., sequence length). It is shown that the proposed method yields reliable detection of combustion instabilities with fewer observations in comparison to a fixed-sample-size test. A sequential detector is a statistical decision function that uses a random number of samples depending on the observation sequence to detect the underlying hypothesis.A sequential detector, on the average, would need a much smaller sequence length than FSS tests (Poor, 1994).Sequential probability ratio test (SPRT) (Wald and Wolfowitz, 1948; Shiryaev, 1978) has been traditionally used for binary hypothesis testing and is known to be an optimal detector when the observation sequences are independent and identically distributed (IID) (Burkholder and Wijsman, 1963).However, sequential data from physical systems are typically not independent as causal relations, because of the underlying physics, may lead to statistical dependencies.
This paper addresses sequential hypothesis testing for Markov models of time-series data by using the concepts of symbolic dynamics. These models are inferred by discretizing the measurement space of a dynamical system, where the system dynamics are approximated as a finite-memory Markov chain on the discrete state space. The study is motivated by time-critical detection problems in physical processes, where a temporal model is trained to make fast and reliable decisions with streaming data. Sequential update rules have been constructed for log-posterior ratio statistic of Markov models in the setting of binary hypothesis testing and the stochastic evolution of this statistic is analyzed. The proposed technique allows selection of a lower bound on the performance of the detector and guarantees that the test will terminate in finite time. The underlying algorithms are first illustrated through an example by numerical simulation, and are subsequently validated on time-series data of pressure oscillations from a laboratory-scale swirl-stabilized combustor apparatus to detect the onset of thermo-acoustic instability. The performance of the proposed sequential hypothesis tests for Markov models has been compared with that of a maximum-likelihood classifier with fixed sample size (i.e., sequence length). It is shown that the proposed method yields reliable detection of combustion instabilities with fewer observations in comparison to a fixed-sample-size test. Hidden Markov models (HMMs) (Rabiner, 1989) undergo temporal evolution, which are learned from data using iterative techniques to capture some of the statistical dependencies (Bishop, 2006).In Grossi and Lops (2008), Chen and Willett (2000), Fuh (2003), the concept of SPRT was extended for data generated from a HMM.In contrast, symbolic time-series analysis-based Markov modeling makes use of concepts from symbolic dynamics to represent an observation sequence as a discrete Markov model.This technique consists of two critical steps, namely, discretization (also called partitioning) of the measurement space of a dynamical system (Kennel and Buhl, 2003; Virani et al., 2016) and memory-size estimation (Srivastav, 2014; Jha et al., 2015).The time-series data are then approximated as a D-Markov model, which is a Markov chain, with finite memory (or order which is denoted as D), over the discrete state-space of the system (Ray, 2004; Mukherjee and Ray, 2014); however, the D-Markov model is different from a hidden Markov model (HMM) in the sense that the state-space of a D-Markov model is always observable.The structure of a D-Markov model can be inferred by searching for the optimal discretization and the corresponding order of the Markov chain.Thus, the parameters associated with the model structure are: the size of partitioning set (also called alphabet in symbolic dynamics literature), location of partitioning segments, and the memory of the associated Markov chain.Once the memory of the discrete symbol sequence is estimated, the state-space of the Markov model is represented by the corresponding collection of memory words (or collections of symbols of length equal to the estimated memory for the discrete sequence).This paper presents a technique to sequentially estimate the log-posterior ratio (LPR) for a binary hypothesis test, where each hypothesis is represented by a D-Markov model.
This paper addresses sequential hypothesis testing for Markov models of time-series data by using the concepts of symbolic dynamics. These models are inferred by discretizing the measurement space of a dynamical system, where the system dynamics are approximated as a finite-memory Markov chain on the discrete state space. The study is motivated by time-critical detection problems in physical processes, where a temporal model is trained to make fast and reliable decisions with streaming data. Sequential update rules have been constructed for log-posterior ratio statistic of Markov models in the setting of binary hypothesis testing and the stochastic evolution of this statistic is analyzed. The proposed technique allows selection of a lower bound on the performance of the detector and guarantees that the test will terminate in finite time. The underlying algorithms are first illustrated through an example by numerical simulation, and are subsequently validated on time-series data of pressure oscillations from a laboratory-scale swirl-stabilized combustor apparatus to detect the onset of thermo-acoustic instability. The performance of the proposed sequential hypothesis tests for Markov models has been compared with that of a maximum-likelihood classifier with fixed sample size (i.e., sequence length). It is shown that the proposed method yields reliable detection of combustion instabilities with fewer observations in comparison to a fixed-sample-size test. Contributions: This paper extends the results of classical SPRT for IID observations to sequential hypothesis tests for observations from D-Markov models of time-series data.D-Markov modeling for time-series data via STSA is an existing technique (Ray, 2004) with applications in target detection and classification in surveillance (Virani et al., 2013; Mukherjee et al., 2011), prognostics and health monitoring of physical systems (e.g., nuclear plants (Jin et al., 2011) and electronic products (Kumar and Pecht, 2007)), and others.The novel contribution of this work is to enable the use of D-Markov models for streaming data analysis in detection and classification problems by formulating their real-time sequential estimation and hypothesis testing problem in a Bayesian framework.In this paper, the probability density of the D-Markov model is represented as a product of categorical distributions; and the parameters of this distribution themselves have a Dirichlet distribution.Then, using the fact that the Dirichlet distribution is the conjugate prior of the categorical distribution, a sequential update rule is developed for posterior probability ratios of D-Markov models to test a time-series model against an alternate time-series model for binary classification problems.Expected increment of the log-posterior ratios are explicitly provided under each hypothesis; and it is shown that the sequential tests for the Markov models terminate in finite time with probability one.The efficacy of the proposed method for online monitoring with streaming data is first illustrated with numerical simulation and then validated on experimental data from a lean-premixed swirl-stabilized combustor apparatus (Kim et al., 2010).The performance of the sequential tests is also compared with that of a (maximum likelihood classifier) FSS test to demonstrate that the proposed method is capable of making more accurate decisions using fewer observations.
The village of Orduña is located in the north of the Iberian Peninsula and has played throughout centuries as an important exchange point, connecting the Spanish plateau with the Basque coastline villages. Furthermore, several trade agreements converted the village into an important trade-center for goods from the 13th century CE onwards. Despite the historical importance of this village, little is known about the pottery activity and its diachronic evolution, which would have experimented a drastic change during the 18th century. Recently, first material relicts evidencing pottery production were found in archaeological excavations, where numerous ceramic sherds along with at least one ceramic kiln dated back to the 17th–19th century were unearthed. These findings constituted an unprecedented framework of reference and paradigm for the study of the ceramic materials from other plots of the villa as well as other Basque villages. In the present work, a selection of 100 sherds, which were representative of the different typologies found, have been subjected to a multi-analytical approach by ICP-MS, XRD and SEM-EDS in order to shed light on these ceramic materials and to obtain a better understanding on their historical implications in relation to regional and extra-regional trading activity. The results allowed establishing seven compositional groups that not only respond to the different technotypes, but they also depict the diachronic evolution of the ceramic production, showing compositional differences throughout time. In addition, characterization of the glazes was performed. Thus, the main technological features used by the potters of Orduña were identified. Orduña, located in the central-northern extension of the Iberian Peninsula (see Fig. 1) shows a long economic and commercial history that has had in its custom system and in its vicinity to major trade routes the basis of its existence (Salazar and Llano, 2005).It is located in the bottom of a valley surrounded by mountains reaching 1000 m. Both its strategic location and its topography, have allowed and enhanced its pivotal role connecting the Spanish plateau with the Basque coastline villages (Salazar and Llano, 2005; Angulo, 1995).Moreover, from the first medieval settlements in the 13th century, several trade agreements have ensured the nature of the village as an important exchange point of goods (Salazar Arechalde, 1995).
The village of Orduña is located in the north of the Iberian Peninsula and has played throughout centuries as an important exchange point, connecting the Spanish plateau with the Basque coastline villages. Furthermore, several trade agreements converted the village into an important trade-center for goods from the 13th century CE onwards. Despite the historical importance of this village, little is known about the pottery activity and its diachronic evolution, which would have experimented a drastic change during the 18th century. Recently, first material relicts evidencing pottery production were found in archaeological excavations, where numerous ceramic sherds along with at least one ceramic kiln dated back to the 17th–19th century were unearthed. These findings constituted an unprecedented framework of reference and paradigm for the study of the ceramic materials from other plots of the villa as well as other Basque villages. In the present work, a selection of 100 sherds, which were representative of the different typologies found, have been subjected to a multi-analytical approach by ICP-MS, XRD and SEM-EDS in order to shed light on these ceramic materials and to obtain a better understanding on their historical implications in relation to regional and extra-regional trading activity. The results allowed establishing seven compositional groups that not only respond to the different technotypes, but they also depict the diachronic evolution of the ceramic production, showing compositional differences throughout time. In addition, characterization of the glazes was performed. Thus, the main technological features used by the potters of Orduña were identified. The origin of the pottery activity in the village is still unclear.Nonetheless, it may have played a relevant role in the village's formation, which seems to coincide in time with the process of the craft specialization that in other Basque regions started flourishing by the 9th century (Solaun, 2005).
The village of Orduña is located in the north of the Iberian Peninsula and has played throughout centuries as an important exchange point, connecting the Spanish plateau with the Basque coastline villages. Furthermore, several trade agreements converted the village into an important trade-center for goods from the 13th century CE onwards. Despite the historical importance of this village, little is known about the pottery activity and its diachronic evolution, which would have experimented a drastic change during the 18th century. Recently, first material relicts evidencing pottery production were found in archaeological excavations, where numerous ceramic sherds along with at least one ceramic kiln dated back to the 17th–19th century were unearthed. These findings constituted an unprecedented framework of reference and paradigm for the study of the ceramic materials from other plots of the villa as well as other Basque villages. In the present work, a selection of 100 sherds, which were representative of the different typologies found, have been subjected to a multi-analytical approach by ICP-MS, XRD and SEM-EDS in order to shed light on these ceramic materials and to obtain a better understanding on their historical implications in relation to regional and extra-regional trading activity. The results allowed establishing seven compositional groups that not only respond to the different technotypes, but they also depict the diachronic evolution of the ceramic production, showing compositional differences throughout time. In addition, characterization of the glazes was performed. Thus, the main technological features used by the potters of Orduña were identified. The reception and trading with pottery in Orduña is documented from the 8th century (Solaun, 2005) until the 17th century (Ruiz-Escribano, 2014).However, the first solid production evidence is not acquired until the interventions carried out in Zaharra street (N.2–4 and N.24–30) and Tras-Santiago street in Orduña, where a ceramic kiln and numerous sherds dating back to the 17th to 19th century were unearthed (Cajigas Panera et al., 2004).
The village of Orduña is located in the north of the Iberian Peninsula and has played throughout centuries as an important exchange point, connecting the Spanish plateau with the Basque coastline villages. Furthermore, several trade agreements converted the village into an important trade-center for goods from the 13th century CE onwards. Despite the historical importance of this village, little is known about the pottery activity and its diachronic evolution, which would have experimented a drastic change during the 18th century. Recently, first material relicts evidencing pottery production were found in archaeological excavations, where numerous ceramic sherds along with at least one ceramic kiln dated back to the 17th–19th century were unearthed. These findings constituted an unprecedented framework of reference and paradigm for the study of the ceramic materials from other plots of the villa as well as other Basque villages. In the present work, a selection of 100 sherds, which were representative of the different typologies found, have been subjected to a multi-analytical approach by ICP-MS, XRD and SEM-EDS in order to shed light on these ceramic materials and to obtain a better understanding on their historical implications in relation to regional and extra-regional trading activity. The results allowed establishing seven compositional groups that not only respond to the different technotypes, but they also depict the diachronic evolution of the ceramic production, showing compositional differences throughout time. In addition, characterization of the glazes was performed. Thus, the main technological features used by the potters of Orduña were identified. The pottery activity at regional level would have suffered an important development at the end of the 18th century and all the later 19th century (Escribano-Ruiz, 2013), which in Orduña is specially marked by two important historical facts: the opening of the access road to the Castilian plateau in 1774 (which will result into the deviation of the commercial traffic from Balmaseda towards Orduña); and the immediate construction of a new customs office in 1792, capable of storing and managing the notorious increase of goods that began to pass through the city (Salazar Arechalde, 1995).Both events promoted new economic conditions in the city and new business opportunities, including the pottery activity.
The village of Orduña is located in the north of the Iberian Peninsula and has played throughout centuries as an important exchange point, connecting the Spanish plateau with the Basque coastline villages. Furthermore, several trade agreements converted the village into an important trade-center for goods from the 13th century CE onwards. Despite the historical importance of this village, little is known about the pottery activity and its diachronic evolution, which would have experimented a drastic change during the 18th century. Recently, first material relicts evidencing pottery production were found in archaeological excavations, where numerous ceramic sherds along with at least one ceramic kiln dated back to the 17th–19th century were unearthed. These findings constituted an unprecedented framework of reference and paradigm for the study of the ceramic materials from other plots of the villa as well as other Basque villages. In the present work, a selection of 100 sherds, which were representative of the different typologies found, have been subjected to a multi-analytical approach by ICP-MS, XRD and SEM-EDS in order to shed light on these ceramic materials and to obtain a better understanding on their historical implications in relation to regional and extra-regional trading activity. The results allowed establishing seven compositional groups that not only respond to the different technotypes, but they also depict the diachronic evolution of the ceramic production, showing compositional differences throughout time. In addition, characterization of the glazes was performed. Thus, the main technological features used by the potters of Orduña were identified. In spite of the historic role of pottery, which was developed at the pace of the configuration of the village and that is still evident through the toponymy, the ceramologic studies performed until now have only been approached from the technotypologic perspective.Regarding the documentary sources, the ethnographer Enrike Ibabe reports the local potters' activity during the 17th and 18th centuries (Ortiz, 1995).
The village of Orduña is located in the north of the Iberian Peninsula and has played throughout centuries as an important exchange point, connecting the Spanish plateau with the Basque coastline villages. Furthermore, several trade agreements converted the village into an important trade-center for goods from the 13th century CE onwards. Despite the historical importance of this village, little is known about the pottery activity and its diachronic evolution, which would have experimented a drastic change during the 18th century. Recently, first material relicts evidencing pottery production were found in archaeological excavations, where numerous ceramic sherds along with at least one ceramic kiln dated back to the 17th–19th century were unearthed. These findings constituted an unprecedented framework of reference and paradigm for the study of the ceramic materials from other plots of the villa as well as other Basque villages. In the present work, a selection of 100 sherds, which were representative of the different typologies found, have been subjected to a multi-analytical approach by ICP-MS, XRD and SEM-EDS in order to shed light on these ceramic materials and to obtain a better understanding on their historical implications in relation to regional and extra-regional trading activity. The results allowed establishing seven compositional groups that not only respond to the different technotypes, but they also depict the diachronic evolution of the ceramic production, showing compositional differences throughout time. In addition, characterization of the glazes was performed. Thus, the main technological features used by the potters of Orduña were identified. In addition, the scarce archaeometrical analyses correspond to chronologies previous to the establishment of the so-called Basque Popular Pottery — a ceramic typology manufactured by Basque potters and very extended in the Basque Society before the industrialization, that consist of reddish earthenware (mostly tableware and household pieces) characterized by partial tin-lead glazes.For more information see Ortiz (1995).
The village of Orduña is located in the north of the Iberian Peninsula and has played throughout centuries as an important exchange point, connecting the Spanish plateau with the Basque coastline villages. Furthermore, several trade agreements converted the village into an important trade-center for goods from the 13th century CE onwards. Despite the historical importance of this village, little is known about the pottery activity and its diachronic evolution, which would have experimented a drastic change during the 18th century. Recently, first material relicts evidencing pottery production were found in archaeological excavations, where numerous ceramic sherds along with at least one ceramic kiln dated back to the 17th–19th century were unearthed. These findings constituted an unprecedented framework of reference and paradigm for the study of the ceramic materials from other plots of the villa as well as other Basque villages. In the present work, a selection of 100 sherds, which were representative of the different typologies found, have been subjected to a multi-analytical approach by ICP-MS, XRD and SEM-EDS in order to shed light on these ceramic materials and to obtain a better understanding on their historical implications in relation to regional and extra-regional trading activity. The results allowed establishing seven compositional groups that not only respond to the different technotypes, but they also depict the diachronic evolution of the ceramic production, showing compositional differences throughout time. In addition, characterization of the glazes was performed. Thus, the main technological features used by the potters of Orduña were identified. In the first extensive study of the medieval Basque ceramic record (Solaun, 2005), the reception and potential re-distribution of several ceramic typologies in Orduña is referred, specially after the concession of the privilege of mercado franco to the village in 1228 (i.e. the privilege from the king to establish a market and trade within the city limits).
The village of Orduña is located in the north of the Iberian Peninsula and has played throughout centuries as an important exchange point, connecting the Spanish plateau with the Basque coastline villages. Furthermore, several trade agreements converted the village into an important trade-center for goods from the 13th century CE onwards. Despite the historical importance of this village, little is known about the pottery activity and its diachronic evolution, which would have experimented a drastic change during the 18th century. Recently, first material relicts evidencing pottery production were found in archaeological excavations, where numerous ceramic sherds along with at least one ceramic kiln dated back to the 17th–19th century were unearthed. These findings constituted an unprecedented framework of reference and paradigm for the study of the ceramic materials from other plots of the villa as well as other Basque villages. In the present work, a selection of 100 sherds, which were representative of the different typologies found, have been subjected to a multi-analytical approach by ICP-MS, XRD and SEM-EDS in order to shed light on these ceramic materials and to obtain a better understanding on their historical implications in relation to regional and extra-regional trading activity. The results allowed establishing seven compositional groups that not only respond to the different technotypes, but they also depict the diachronic evolution of the ceramic production, showing compositional differences throughout time. In addition, characterization of the glazes was performed. Thus, the main technological features used by the potters of Orduña were identified. These typologies include cooking pots, wringers, and diverse forms of pitchers and large jars dated back from the 8th to the 13th century.More precisely, in the same work (Solaun, 2005) a ceramic typology existing from the 8th to the 10th century and characterized by coarse ceramic of fine walls (denominated Group II) is suggested having its provenance in Orduña, or at least, being compatible with the macroscopic compositional features of the pastes of the clays from Orduña, i.e. pastes with low content of calcite inclusions and a matrix characterized by autigenic quartz and pyroxenes (for more detailed geological information on the area see Ábalos, 2016).However, no further evidence is provided to support this hypothesis and the possible provenance from other places with outcrops from Triassic periods, such as Maeztu, Murgia, Salinas de Añaña, Salinillas de Buradón or Peñacerrada (all of them in the southern part of the Basque region) is not disregarded (Solaun, 2005).
The village of Orduña is located in the north of the Iberian Peninsula and has played throughout centuries as an important exchange point, connecting the Spanish plateau with the Basque coastline villages. Furthermore, several trade agreements converted the village into an important trade-center for goods from the 13th century CE onwards. Despite the historical importance of this village, little is known about the pottery activity and its diachronic evolution, which would have experimented a drastic change during the 18th century. Recently, first material relicts evidencing pottery production were found in archaeological excavations, where numerous ceramic sherds along with at least one ceramic kiln dated back to the 17th–19th century were unearthed. These findings constituted an unprecedented framework of reference and paradigm for the study of the ceramic materials from other plots of the villa as well as other Basque villages. In the present work, a selection of 100 sherds, which were representative of the different typologies found, have been subjected to a multi-analytical approach by ICP-MS, XRD and SEM-EDS in order to shed light on these ceramic materials and to obtain a better understanding on their historical implications in relation to regional and extra-regional trading activity. The results allowed establishing seven compositional groups that not only respond to the different technotypes, but they also depict the diachronic evolution of the ceramic production, showing compositional differences throughout time. In addition, characterization of the glazes was performed. Thus, the main technological features used by the potters of Orduña were identified. In a recent study of the ceramic record from Basque region of Alava (Escribano-Ruiz, 2014), in the analysis of the regional and extra-regional production and consumption, no evidence of ceramic products is reported for the case of Orduña corresponding to the period of 14th–17th century, where most of the pottery products received in Alava seem to have been predominantly produced in the workshops of in Bilbao, Salinillas de Buradón, Ollerias or Egileta, which covered a large part of the regional production in the Basque Country (all of them are nearby locations from the Basque region).In contrast, pottery produced in those sites would have been traded in Orduña (Escribano-Ruiz, 2014).
The village of Orduña is located in the north of the Iberian Peninsula and has played throughout centuries as an important exchange point, connecting the Spanish plateau with the Basque coastline villages. Furthermore, several trade agreements converted the village into an important trade-center for goods from the 13th century CE onwards. Despite the historical importance of this village, little is known about the pottery activity and its diachronic evolution, which would have experimented a drastic change during the 18th century. Recently, first material relicts evidencing pottery production were found in archaeological excavations, where numerous ceramic sherds along with at least one ceramic kiln dated back to the 17th–19th century were unearthed. These findings constituted an unprecedented framework of reference and paradigm for the study of the ceramic materials from other plots of the villa as well as other Basque villages. In the present work, a selection of 100 sherds, which were representative of the different typologies found, have been subjected to a multi-analytical approach by ICP-MS, XRD and SEM-EDS in order to shed light on these ceramic materials and to obtain a better understanding on their historical implications in relation to regional and extra-regional trading activity. The results allowed establishing seven compositional groups that not only respond to the different technotypes, but they also depict the diachronic evolution of the ceramic production, showing compositional differences throughout time. In addition, characterization of the glazes was performed. Thus, the main technological features used by the potters of Orduña were identified. With regard to the archaeometrical analysis, and to the knowledge of the authors, the only existing ones correspond to 12 sherds unearthed in Orduña and dated back to the 14th century to 16th centuries (Barrachina, 2016).Therefore, these are out of the target chronological range of the present work.
The village of Orduña is located in the north of the Iberian Peninsula and has played throughout centuries as an important exchange point, connecting the Spanish plateau with the Basque coastline villages. Furthermore, several trade agreements converted the village into an important trade-center for goods from the 13th century CE onwards. Despite the historical importance of this village, little is known about the pottery activity and its diachronic evolution, which would have experimented a drastic change during the 18th century. Recently, first material relicts evidencing pottery production were found in archaeological excavations, where numerous ceramic sherds along with at least one ceramic kiln dated back to the 17th–19th century were unearthed. These findings constituted an unprecedented framework of reference and paradigm for the study of the ceramic materials from other plots of the villa as well as other Basque villages. In the present work, a selection of 100 sherds, which were representative of the different typologies found, have been subjected to a multi-analytical approach by ICP-MS, XRD and SEM-EDS in order to shed light on these ceramic materials and to obtain a better understanding on their historical implications in relation to regional and extra-regional trading activity. The results allowed establishing seven compositional groups that not only respond to the different technotypes, but they also depict the diachronic evolution of the ceramic production, showing compositional differences throughout time. In addition, characterization of the glazes was performed. Thus, the main technological features used by the potters of Orduña were identified. These archaeometrical analyses performed on a reduced selection suggest a subdivision into two similar groups (PB10b and PB10a) characterized for being highly calcareous (15–20 wt%), low in Fe2O3 (≈3.5 wt%), medium in MgO (1–5 wt%) and distinguishable between them by Ce concentration.Moreover, these are characterized by a remarkably high Pb content with regard to the rest of the ceramics from the Basque region studied in the corresponding work.The presence of this element detected in the inclusions is tentatively ascribed to the clay paste used for the manufacture (Barrachina, 2016).Nevertheless, the potential contribution of the plumbiferous glaze should not be disregarded.As proposed by Barrachina, these ceramics would have been manufactured in Vitoria and Bilbao respectively.
The village of Orduña is located in the north of the Iberian Peninsula and has played throughout centuries as an important exchange point, connecting the Spanish plateau with the Basque coastline villages. Furthermore, several trade agreements converted the village into an important trade-center for goods from the 13th century CE onwards. Despite the historical importance of this village, little is known about the pottery activity and its diachronic evolution, which would have experimented a drastic change during the 18th century. Recently, first material relicts evidencing pottery production were found in archaeological excavations, where numerous ceramic sherds along with at least one ceramic kiln dated back to the 17th–19th century were unearthed. These findings constituted an unprecedented framework of reference and paradigm for the study of the ceramic materials from other plots of the villa as well as other Basque villages. In the present work, a selection of 100 sherds, which were representative of the different typologies found, have been subjected to a multi-analytical approach by ICP-MS, XRD and SEM-EDS in order to shed light on these ceramic materials and to obtain a better understanding on their historical implications in relation to regional and extra-regional trading activity. The results allowed establishing seven compositional groups that not only respond to the different technotypes, but they also depict the diachronic evolution of the ceramic production, showing compositional differences throughout time. In addition, characterization of the glazes was performed. Thus, the main technological features used by the potters of Orduña were identified. As mentioned before, the pottery manufacturing system experienced a dramatic modification by the 18th.In this way, the ceramics that during the 14th–15th centuries had remained affordable only for a reduced sector of the society, became reachable to everyone (Escribano-Ruiz, 2013, 2014).In the particular case of Orduña, this re-dimension of the pottery market was marked by the implementation of the new customs office and the opening of the access road to the Castilian plateau in 1774.These changes gradually led to the materialization of the Basque Popular pottery, which would continue as long as there were workshops where pottery was produced manually.This popular pottery was severely hit by the Spanish Civil war and lasted until the introduction of the industrialization in the second half of the 20th century (Iñañez et al., 2015).
Microwear analysis of lithic debris offers an aid to field archaeologists in need of focusing excavation activity at the parts of Stone Age sites that are least disturbed by post-depositional processes. In this progress report we describe the ridge-wear approach and report on its application to the submerged Mesolithic settlement of Orehoved, Denmark. Test-pitting had provided worked flints from two distinctive layers within an area of ca. 2.8 ha, located 4–7 m below present sea level. Through analysis of wear on dorsal ridges on flint flakes we demonstrate that the assemblages of artefacts from both strata spanned the whole range from minimally-rounded to extremely-rounded. Spatial and statistical analyses indicate that both layers contain redeposited artefacts. Major parts of higher ground areas were therefore considered so disturbed by sea-level rise that further excavation should be avoided here. Follow-up excavation in a more low lying area located a third and stratigraphically deeper layer with numerous artefacts in stone, bone, and plant material in situ. Lithic microwear analysis is commonly applied when studying wear patterns on the edges of stone tools to infer their prehistoric use.Wear, however, can occur on the surfaces of stone artefacts through a variety of natural as well as cultural processes.Here, we report on the examination of wear on artefacts collected by a test-pit survey in 4–7 m of water in a locality of expected Kongemosian habitation near the port of Orehoved, Denmark (Fig. 1).The results showed that artefacts from the two excavated stratigraphic layers were redeposited most likely from wave action and by erosion from water currents.
Several optimization algorithms have been developed in the past for application in a variety of complex real world problems. Herein, a new hybrid variant of Genetic Algorithm (GA), utilizing the sample space reduction technique of Cohort Intelligence (CI) Algorithm, referred to as Adaptive Range Genetic Algorithm (ARGA) is presented and discussed. The correctness of the proposed algorithm is established by solving 50 standard test functions. Also, the practical applicability of the proposed algorithm is demonstrated by applying it to the design and economic optimization problem of a Shell and Tube Heat Exchanger. Heat exchangers are critical industrial components used in a wide range of industrial applications. Owing to their high manufacturing costs and complicated design criteria, their economic efficiency of operation depends heavily on optimization of their design. A relative assessment, comparing the performance of the proposed method of solution to other existing algorithmic methods, is conducted. The quality and robustness of the proposed solution method highlights its adeptness in solving real world computational problems in core mechanical engineering applications. Metaheuristics are high-level problem independent frameworks that can be used for solution of multiple categories of problems without taking into account the specific mathematical structuring of the problem.In the field of Artificial Intelligence (AI), metaheuristic algorithms are a critical part of modern global optimization algorithms.In the recent past, a large number of AI based optimization metaheuristics were developed and tested for applicability in complex real-life engineering problems.One class of algorithms is Nature inspired Algorithms (NIAs) such as Ant Colony Optimization (ACO) (Dorigo, 1992), Particle Swarm Optimization (PSO) (Eberhart and Kennedy, 1995), Artificial Bee Colony Algorithm (ABC) (Karaboga, 2005), and Firefly Algorithm (FFA) (Yang 2009).The Physics-based methods include Simulated Annealing (Kirkpatrick and Vecchi, 1983), and Gravitational Search Algorithm (Rashedi et al., 2009).Other optimization algorithms include Random Optimization Algorithm (Matyas, 1965), Random Search Algorithm (Rastrigin, 1963), and Backtracking Search Algorithm (Patterson et al., 1990).Genetic Algorithms (GA) (Goldberg and Holland, 1988) and Memetic Algorithms (Moscato, 1989) are two examples of very well established Evolutionary (EA) methods.GA is an optimization algorithm whose basis is the Darwin’s principle of evolution (Goldberg and Holland, 1988).The artificial chromosomes form the population in GA and each chromosome constitutes a possible way out to a problem.The fitness function governs the quality of a solution.The randomly generated population of chromosomes, fitness-based selection followed by recombination is used to populate a next generation of solutions.In this way, GA evolves a near optimal solution even to those problems which were considered computationally hard nuts to crack.One of the characteristics and drawbacks of GA is that there is always a chance that there is a better chromosome in the search space.GA possesses good convergence characteristics; however, global optimum is not guaranteed.
Several optimization algorithms have been developed in the past for application in a variety of complex real world problems. Herein, a new hybrid variant of Genetic Algorithm (GA), utilizing the sample space reduction technique of Cohort Intelligence (CI) Algorithm, referred to as Adaptive Range Genetic Algorithm (ARGA) is presented and discussed. The correctness of the proposed algorithm is established by solving 50 standard test functions. Also, the practical applicability of the proposed algorithm is demonstrated by applying it to the design and economic optimization problem of a Shell and Tube Heat Exchanger. Heat exchangers are critical industrial components used in a wide range of industrial applications. Owing to their high manufacturing costs and complicated design criteria, their economic efficiency of operation depends heavily on optimization of their design. A relative assessment, comparing the performance of the proposed method of solution to other existing algorithmic methods, is conducted. The quality and robustness of the proposed solution method highlights its adeptness in solving real world computational problems in core mechanical engineering applications. Another class of algorithms referred to as Socio inspired algorithms or Cultural Algorithms (Reynolds and Sverdlik, 1994) are based on concept that individuals evolve much faster through cultural evolution than through biological evolution alone.Humans learn from the peers and achieve their goals using teamwork.Several algorithms were formulated based on this tendency of collective intelligence.The Imperialist Competitive algorithm (ICA) (Atashpaz-Gargari and Lucas, 2007) mimics the socio-political competition seen across imperialist nations to conquer the weaker colonies or empires.League Championship algorithm (LCA) (Kashan, 2014) simulates the competition observed in league matches.The Election Campaign Optimization algorithm (Lv et al., 2010) (ECO) algorithm is based on the natural tendency of voters to vote for a candidate with better reputation.Teaching–learning-based optimization (TLBO) simulates the influence of an instructor on the learning outcome of students.Socio Evolution & Learning Optimization Algorithm (SELO) (Kumar et al., 2018) mimics the learning behaviour of humans in families as a societal setup.
Several optimization algorithms have been developed in the past for application in a variety of complex real world problems. Herein, a new hybrid variant of Genetic Algorithm (GA), utilizing the sample space reduction technique of Cohort Intelligence (CI) Algorithm, referred to as Adaptive Range Genetic Algorithm (ARGA) is presented and discussed. The correctness of the proposed algorithm is established by solving 50 standard test functions. Also, the practical applicability of the proposed algorithm is demonstrated by applying it to the design and economic optimization problem of a Shell and Tube Heat Exchanger. Heat exchangers are critical industrial components used in a wide range of industrial applications. Owing to their high manufacturing costs and complicated design criteria, their economic efficiency of operation depends heavily on optimization of their design. A relative assessment, comparing the performance of the proposed method of solution to other existing algorithmic methods, is conducted. The quality and robustness of the proposed solution method highlights its adeptness in solving real world computational problems in core mechanical engineering applications. Cohort Intelligence (CI) is a socio-inspired metaheuristic proposed by Kulkarni et al. (2013, 2017) and is inspired from the self-supervised learning behaviour of the candidates in a cohort.Every candidate iteratively tries to adapt the behaviour of peers and improve its behaviour by following a certain candidate’s behaviour.The algorithm suffers from premature convergence due to tendency of following each other only during exploration phase (Krishnasamy et al., 2014).Patankar and Kulkarni (2018) proposed several variations of this algorithm which were tested on seven multimodal and three uni-modal unconstrained test functions.Sapre et al. (2018, 2019) have applied CI and variations of CI for hexahedral mesh smoothing of cubical and prismatic geometries respectively.CI was also proved to be applicable to binary optimization problems (Aladeemy et al., 2017).Kulkarni and Shabir (2016) applied CI for solution of combinatorial problems such as 0–1 Knapsack problem and the classic Travelling Salesman Problem (TSP).Kulkarni et al. (2016) applied CI for solution of combinatorial healthcare problem to prepare a cyclic surgery schedule for minimization of congestion in the recovery unit.It was also applied to cross border transportation problems.Shastri et al. (2015) discussed solutions to constrained test problems using CI Algorithm.Sarmah and Kulkarni (2017) discussed two steganography techniques using CI with cognitive computing (CC) and Modified Multi-Random Start Local Search (MMRSLS) which use Joint Photographic Expert Group (JPEG) compression on greyscale image to hide secret text.Sarmah and Kulkarni (2018) developed JPEG compression techniques using modified MMRSL and modified CI with CC.Sarmah and Kulkarni (2019) also developed Improved CI algorithm and applied it to JPEG image compression to develop a novel steganography method.Shah et al. (2017) explained the design process of Proportional Integral Derivative (PID) controller using fractional calculus and CI algorithm.Multi CI algorithm developed by Shastri and Kulkarni (2018) focusses on intra-group and inter-group learning mechanisms amongst different cohorts.
Several optimization algorithms have been developed in the past for application in a variety of complex real world problems. Herein, a new hybrid variant of Genetic Algorithm (GA), utilizing the sample space reduction technique of Cohort Intelligence (CI) Algorithm, referred to as Adaptive Range Genetic Algorithm (ARGA) is presented and discussed. The correctness of the proposed algorithm is established by solving 50 standard test functions. Also, the practical applicability of the proposed algorithm is demonstrated by applying it to the design and economic optimization problem of a Shell and Tube Heat Exchanger. Heat exchangers are critical industrial components used in a wide range of industrial applications. Owing to their high manufacturing costs and complicated design criteria, their economic efficiency of operation depends heavily on optimization of their design. A relative assessment, comparing the performance of the proposed method of solution to other existing algorithmic methods, is conducted. The quality and robustness of the proposed solution method highlights its adeptness in solving real world computational problems in core mechanical engineering applications. The sampling interval in CI algorithm shrinks in every learning attempt.This mechanism used in CI was applied to the mutation operator in GA to develop a new variant of GA, referred to as Adaptive Mutation Range Genetic Algorithm (ARGA).Population initialization, evaluation of fitness values of the chromosomes and cross-over processes in ARGA were identical to those in GA.In ARGA, the sampling interval for mutation shrunk in each iteration/generation.ARGA was good at exploring the solution like the parent GA; however, its interval shrinking characteristic helped in better exploitation of the solution.Mutation rate was not adjusted, however the interval from which the genes were chosen was updated for each generation.The updated value of gene to be mutated was taken from the new sampling interval.
Around 2900–2300 cal BCE, mobile stockbreeders introduced the Neolithic Corded Ware culture (CWC) into the Eastern Baltic. Here, a Central or Northern European Neolithic economy and ideology took hold despite differences in burial practices. Although around 90 CWC graves are known in the region their contents have not been intensively studied. Here, we present new AMS radiocarbon (14C) measurements and carbon and nitrogen stable isotope data obtained on human bone collagen, molecular and isotopic data obtained from ceramic beakers, and user-wear data of flint and bone tools from several CWC graves, Benaičiai, Biržai, Krasnasieĺski, Dakudava 5, and Drazdy 12, in Lithuania and Western Belarus. The bone collagen δ13C and δ15N stable isotope data are rather homogenous and demonstrate that the majority of consumed protein was derived from terrestrial resources. Organic residue analysis of two CWC beakers yielded lipids consistent with ruminant carcass and dairy fats, whilst use-wear analyses indicates that bone pins, flint blades and axes were used as grinders, functional tools or had been carefully renewed before deposition respectively. The peoples of the Corded Ware culture (hereafter CWC) are traditionally regarded as mobile stockbreeders who brought animal husbandry into the Eastern Baltic between ca. 2900–2300 cal BCE.The presence of domesticated faunal remains within CWC contexts as well as stable isotope data obtained on human bone collagen has demonstrated that terrestrial derived protein was preferentially consumed when compared with the preceding Subneolithic hunter-gatherers who relied heavily on aquatic resources (Lõugas et al., 2007; Piličiauskas et al., 2017b, 2017d).Throughout Europe the CWC appears abruptly and differs to all preceding cultures in terms of material culture.Thus, nearly 100 years ago it was postulated that the CWC was brought into Europe by a mass migration of pastoralists from the Pontic steppe (Childe, 1926; Gimbutas, 1979); a hypothesis that has been critiqued numerous times (e.g. Lang, 1998; Furholt, 2014; Beckerman, 2015) until recently proved by genetic analyses (Allentoft et al., 2015; Haak et al., 2015; Saag et al., 2017; Mittnik et al., 2018).
Around 2900–2300 cal BCE, mobile stockbreeders introduced the Neolithic Corded Ware culture (CWC) into the Eastern Baltic. Here, a Central or Northern European Neolithic economy and ideology took hold despite differences in burial practices. Although around 90 CWC graves are known in the region their contents have not been intensively studied. Here, we present new AMS radiocarbon (14C) measurements and carbon and nitrogen stable isotope data obtained on human bone collagen, molecular and isotopic data obtained from ceramic beakers, and user-wear data of flint and bone tools from several CWC graves, Benaičiai, Biržai, Krasnasieĺski, Dakudava 5, and Drazdy 12, in Lithuania and Western Belarus. The bone collagen δ13C and δ15N stable isotope data are rather homogenous and demonstrate that the majority of consumed protein was derived from terrestrial resources. Organic residue analysis of two CWC beakers yielded lipids consistent with ruminant carcass and dairy fats, whilst use-wear analyses indicates that bone pins, flint blades and axes were used as grinders, functional tools or had been carefully renewed before deposition respectively. This contribution focuses on several CWC graves in the Eastern Baltic.Whilst CWC human remains are highly sought-after for AMS radiocarbon (14C) dating, stable isotopic and genetic analyses, less than a third are available for scientific analysis.Moreover, human remains are, in general, poorly preserved.This has largely been attributed to the acidic soils in the region, which have affected the organic component of the bones, but also unsolicited excavations and the mishandling of materials in storage (Žukauskaitė, 2004).
Around 2900–2300 cal BCE, mobile stockbreeders introduced the Neolithic Corded Ware culture (CWC) into the Eastern Baltic. Here, a Central or Northern European Neolithic economy and ideology took hold despite differences in burial practices. Although around 90 CWC graves are known in the region their contents have not been intensively studied. Here, we present new AMS radiocarbon (14C) measurements and carbon and nitrogen stable isotope data obtained on human bone collagen, molecular and isotopic data obtained from ceramic beakers, and user-wear data of flint and bone tools from several CWC graves, Benaičiai, Biržai, Krasnasieĺski, Dakudava 5, and Drazdy 12, in Lithuania and Western Belarus. The bone collagen δ13C and δ15N stable isotope data are rather homogenous and demonstrate that the majority of consumed protein was derived from terrestrial resources. Organic residue analysis of two CWC beakers yielded lipids consistent with ruminant carcass and dairy fats, whilst use-wear analyses indicates that bone pins, flint blades and axes were used as grinders, functional tools or had been carefully renewed before deposition respectively. In 2017, one of us (GP) was fortunate to discover and excavate a new CWC inhumation at the site of Benaičiai in North West Lithuania.It was during these investigations when we realised Belarusian CWC burials are also poorly understood.A grave found inside the flint mine at Krasnasieĺski is one such example.Although the remains were well preserved it has not been directly AMS radiocarbon (14C) dated.In order to rectify this imbalance, reconstruct the diets of these two individuals and place them into the wider context, here, we present new osteological, radiocarbon and stable isotope data from the two skeletons.
Around 2900–2300 cal BCE, mobile stockbreeders introduced the Neolithic Corded Ware culture (CWC) into the Eastern Baltic. Here, a Central or Northern European Neolithic economy and ideology took hold despite differences in burial practices. Although around 90 CWC graves are known in the region their contents have not been intensively studied. Here, we present new AMS radiocarbon (14C) measurements and carbon and nitrogen stable isotope data obtained on human bone collagen, molecular and isotopic data obtained from ceramic beakers, and user-wear data of flint and bone tools from several CWC graves, Benaičiai, Biržai, Krasnasieĺski, Dakudava 5, and Drazdy 12, in Lithuania and Western Belarus. The bone collagen δ13C and δ15N stable isotope data are rather homogenous and demonstrate that the majority of consumed protein was derived from terrestrial resources. Organic residue analysis of two CWC beakers yielded lipids consistent with ruminant carcass and dairy fats, whilst use-wear analyses indicates that bone pins, flint blades and axes were used as grinders, functional tools or had been carefully renewed before deposition respectively. To supplement the aforementioned dietary reconstruction we undertook organic residue analysis of two CWC beakers.Prior to this study it was unknown why beakers were placed within inhumations and what their function was.Unfortunately pottery was absent at Benaičiai as well as other Lithuanian CWC graves, whilst a beaker found at Krasnasieĺski was not available for analysis.To overcome this, beakers were sampled from two Belarusian CWC burial-bearing sites, Dakudava 5 and Drazdy 12 located in the Upper Neman (Lakiza, 2008; Asheichyk and Vaitovich, 2016).
Around 2900–2300 cal BCE, mobile stockbreeders introduced the Neolithic Corded Ware culture (CWC) into the Eastern Baltic. Here, a Central or Northern European Neolithic economy and ideology took hold despite differences in burial practices. Although around 90 CWC graves are known in the region their contents have not been intensively studied. Here, we present new AMS radiocarbon (14C) measurements and carbon and nitrogen stable isotope data obtained on human bone collagen, molecular and isotopic data obtained from ceramic beakers, and user-wear data of flint and bone tools from several CWC graves, Benaičiai, Biržai, Krasnasieĺski, Dakudava 5, and Drazdy 12, in Lithuania and Western Belarus. The bone collagen δ13C and δ15N stable isotope data are rather homogenous and demonstrate that the majority of consumed protein was derived from terrestrial resources. Organic residue analysis of two CWC beakers yielded lipids consistent with ruminant carcass and dairy fats, whilst use-wear analyses indicates that bone pins, flint blades and axes were used as grinders, functional tools or had been carefully renewed before deposition respectively. Lastly, we present the use-wear results obtained on a flint ‘knife’ recovered from the Benaičiai burial.Moreover, a flint blade and axe as well as a bone ‘pin’ recovered from another CWC grave in Lithuania, Biržai, were analysed.Since use-wear analyses have not been previously undertaken on CWC grave goods, it is currently unknown whether they were intentionally commissioned for internment with the deceased as has been demonstrated elsewhere in Europe (e.g. Little et al., 2017) or held a utilitarian function prior to deposition.
The Zayukovo (Baksan) source is the only obsidian source known in the Northern Caucasus. We report new data, collected in 2017–2018, about exploitation of the Zayukovo (Baksan) source in the Paleolithic, including results of analysis of 34 new samples from Saradj-Chuko grotto, Mezmaiskaya cave, Sosruko rockshelter and Kasojskaya cave. This is the largest series of obsidian samples ever analyzed for the Northern Caucasus Paleolithic sites. Obsidian transport long distances indicates hominid mobility and cultural contacts within the Northern Caucasus in the Paleolithic. Obsidian is one of the most widely exploited raw materials in the Paleolithic of Eurasia and the Stone Age of Africa (McBrearty and Brooks, 2000; Negash and Shackley, 2006; Negash et al., 2006; Piperno et al., 2009).Long-term studies (e.g., Glascock et al., 1998; Shackley, 2005) indicate that the chemical composition of obsidians is unique for each eruptive event.Obsidian then becomes an especially valuable raw material for understanding human mobility in the Paleolithic.Obsidian was exploited by early hominids since the end of Oldowan as indicated by transportation long distances.In the Middle Paleolithic (MP) obsidian was transported >100 km in Central Europe (Féblot-Augustins, 1997) and >200 km in the Caucasus (Doronicheva and Shackley, 2014).In the Upper Paleolithic (UP), transport distances of obsidian increased (Féblot-Augustins, 1997; Ono, 2014; Kuzmin, 2017), sometimes >700 km (Frahm and Hauck, 2017).
The Zayukovo (Baksan) source is the only obsidian source known in the Northern Caucasus. We report new data, collected in 2017–2018, about exploitation of the Zayukovo (Baksan) source in the Paleolithic, including results of analysis of 34 new samples from Saradj-Chuko grotto, Mezmaiskaya cave, Sosruko rockshelter and Kasojskaya cave. This is the largest series of obsidian samples ever analyzed for the Northern Caucasus Paleolithic sites. Obsidian transport long distances indicates hominid mobility and cultural contacts within the Northern Caucasus in the Paleolithic. In the Caucasus, obsidian represented the most attractive stone raw material for both MP Neanderthals and UP modern humans in both the northern (Doronicheva and Shackley, 2014; Doronicheva et al., 2016) and southern (Badalyan et al., 2004; Le Bourdonnec et al., 2012; Chataigner and Gratuze, 2014; Frahm et al., 2016; Glauberman et al., 2016; Pleurdeau et al., 2016; Tornero et al., 2016; Kandel et al., 2017; Biagi et al., 2017; Biagi and Nisbet, 2018) slopes of the Caucasus.
In this paper, a new configuration of Interval Type-2 (IT2) fuzzy Proportional–Integral (PI) or fuzzy Proportional–Derivative (PD) controller of Takagi–Sugeno (TS) type is presented. An attempt is made to generalize the IT2 fuzzy PI/PD controller structure using multiple fuzzy sets. Fuzzification of the inputs is done with three or more fuzzy sets having triangular/trapezoidal membership functions. The rule base consists of only three rules to reduce the number of tuneable parameters of the controller. Minimum (Min) triangular norm and Bounded Sum (BS) triangular co-norm are used as conjunction and disjunction operators to reduce the number of rules. Karnik–Mendel (KM) type reducer and Weighted Average (WA) defuzzifier are considered to derive the analytical structure of the fuzzy controller. Properties and gain variations of the fuzzy controller are investigated. Simulation study is carried out on nonlinear dynamical systems to verify the applicability of the fuzzy controllers. Over the years fuzzy controllers have gained importance due to their ability to control complex dynamical systems without precise knowledge of the processes.Initially fuzzy control activity started with type-1 fuzzy sets and then has been extended gradually to deal with IT2 fuzzy sets.Both type-1 and IT2 fuzzy controllers are capable of handling complex control situations while the problem of uncertainty is addressed only with IT2 fuzzy sets.IT2 fuzzy sets have a very unique property that allows incorporating the uncertainty in the membership function, which is termed as Footprint of Uncertainty (FoU).IT2 fuzzy controllers (Wu and Tan, 2006, 2007) have been developed by modifying type-1 fuzzy controllers and their performance has been evaluated.The IT2 fuzzy controllers were proved to be superior over type-1 fuzzy controllers but this superiority cannot be guaranteed always.Hence a thorough understanding and rigorous mathematical analysis are required to develop a strong background for IT2 fuzzy controllers.Also a thorough understanding of FoU is required to analyze the design aspects of IT2 fuzzy controllers.
In this paper, a new configuration of Interval Type-2 (IT2) fuzzy Proportional–Integral (PI) or fuzzy Proportional–Derivative (PD) controller of Takagi–Sugeno (TS) type is presented. An attempt is made to generalize the IT2 fuzzy PI/PD controller structure using multiple fuzzy sets. Fuzzification of the inputs is done with three or more fuzzy sets having triangular/trapezoidal membership functions. The rule base consists of only three rules to reduce the number of tuneable parameters of the controller. Minimum (Min) triangular norm and Bounded Sum (BS) triangular co-norm are used as conjunction and disjunction operators to reduce the number of rules. Karnik–Mendel (KM) type reducer and Weighted Average (WA) defuzzifier are considered to derive the analytical structure of the fuzzy controller. Properties and gain variations of the fuzzy controller are investigated. Simulation study is carried out on nonlinear dynamical systems to verify the applicability of the fuzzy controllers. Mathematical modeling is one of the few effective means to obtain precise and comprehensive understanding of fuzzy controllers.It provides us ample information about the behavior of fuzzy controllers.The first few models of Mamdani type fuzzy PI controller were proposed (Ying, 1993) way back in 1993.Since then the area of mathematical modeling of fuzzy controllers has been an active domain of interest.With the introduction of TS fuzzy modeling (Takagi and Sugeno, 1985) in 1985, mathematical modeling of fuzzy controllers was extended to the simplest type-1 fuzzy PI (Ying, 1998a) and fuzzy PD (Ying, 1998b) controllers of TS type.Subsequently, extensive work was done by several researchers in this domain over the years (see for example Mohan and Sinha, 2008; Mohan and Ghosh, 2012; Raj and Mohan, 2018b).Recently, Raj and Mohan (2019) proposed four new models of the simplest type-1 fuzzy PI/PD controllers with a modified rule base consisting of only two rules with four tuneable parameters.This resulted in the reduction of number of tuneable parameters of the simplest type-1 fuzzy controllers.
In this paper, a new configuration of Interval Type-2 (IT2) fuzzy Proportional–Integral (PI) or fuzzy Proportional–Derivative (PD) controller of Takagi–Sugeno (TS) type is presented. An attempt is made to generalize the IT2 fuzzy PI/PD controller structure using multiple fuzzy sets. Fuzzification of the inputs is done with three or more fuzzy sets having triangular/trapezoidal membership functions. The rule base consists of only three rules to reduce the number of tuneable parameters of the controller. Minimum (Min) triangular norm and Bounded Sum (BS) triangular co-norm are used as conjunction and disjunction operators to reduce the number of rules. Karnik–Mendel (KM) type reducer and Weighted Average (WA) defuzzifier are considered to derive the analytical structure of the fuzzy controller. Properties and gain variations of the fuzzy controller are investigated. Simulation study is carried out on nonlinear dynamical systems to verify the applicability of the fuzzy controllers. With the introduction to IT2 fuzzy logic systems, there was a quantum shift towards the development of fuzzy control theory.Researchers came up with several design methodologies for IT2 fuzzy logic controllers.A design method for IT2 fuzzy PI/PD controllers was developed in Aliasghory et al. (2013) based on the theory of linear PI/PD control.Robust design methods for IT2 fuzzy controllers were presented (Kumbasar, 2016) by explicitly deriving the fuzzy mapping of IT2 fuzzy logic controllers.A design methodology for IT2 fuzzy controllers was proposed (Kumbasar et al., 2017) by obtaining the sub-models of IT2 fuzzy models and then finding the inverse of each activated sub-model.Bee colony optimization algorithm was applied (Castillo and Amador-Angulo, 2018) for designing and stabilizing a generalized type-2 fuzzy controller.As IT2 fuzzy controllers were gaining popularity, some comparisons (Wu, 2012; Castillo et al., 2016) were made with type-1 fuzzy controllers.In most cases IT2 fuzzy controllers outperform type-1 fuzzy controllers in handling disturbances and uncertainties.However, the high computational cost hinders the implementation of IT2 fuzzy controllers.Hence, a few methods were proposed (Wu, 2013; Melin et al., 2019) to reduce the high computational cost associated with the implementation of IT2 fuzzy controllers.
In this paper, a new configuration of Interval Type-2 (IT2) fuzzy Proportional–Integral (PI) or fuzzy Proportional–Derivative (PD) controller of Takagi–Sugeno (TS) type is presented. An attempt is made to generalize the IT2 fuzzy PI/PD controller structure using multiple fuzzy sets. Fuzzification of the inputs is done with three or more fuzzy sets having triangular/trapezoidal membership functions. The rule base consists of only three rules to reduce the number of tuneable parameters of the controller. Minimum (Min) triangular norm and Bounded Sum (BS) triangular co-norm are used as conjunction and disjunction operators to reduce the number of rules. Karnik–Mendel (KM) type reducer and Weighted Average (WA) defuzzifier are considered to derive the analytical structure of the fuzzy controller. Properties and gain variations of the fuzzy controller are investigated. Simulation study is carried out on nonlinear dynamical systems to verify the applicability of the fuzzy controllers. Earlier IT2 fuzzy controllers were treated as black-boxes since their mathematical models were unavailable.As a result of this, precise and thorough understanding of IT2 fuzzy controllers became difficult.Nonetheless, some researchers took initiative and came up with novel models of the simplest IT2 fuzzy controllers of Mamdani type (Du and Ying, 2010; Nei and Tan, 2012).Later, Zhou and Ying (2013) proposed a method to obtain the structure of a broad class of IT2 Mamdani fuzzy controllers.Another structure of Mamdani type IT2 fuzzy controllers was also reported in Mendel et al. (2014).All these fuzzy controllers were developed using four rules, minimum triangular norm, two IT2 input fuzzy sets and four output singletons.It is noted that using four rules increases the number of tuneable parameters of fuzzy controllers and thereby the computational complexity.Later a mathematical model of a typical IT2 fuzzy controller of TS type was presented (Zhou and Ying, 2017), and the role of FoU in shaping the structure of the controller was discussed.Very recently, selecting appropriate shapes of FoU for different control requirements has been presented (Yip et al., 2019).Further it has been shown in Zhou et al. (2019) that the analytical structure of IT2 fuzzy controllers becomes a constant or piecewise linear when FoU is maximum.
In this paper, a new configuration of Interval Type-2 (IT2) fuzzy Proportional–Integral (PI) or fuzzy Proportional–Derivative (PD) controller of Takagi–Sugeno (TS) type is presented. An attempt is made to generalize the IT2 fuzzy PI/PD controller structure using multiple fuzzy sets. Fuzzification of the inputs is done with three or more fuzzy sets having triangular/trapezoidal membership functions. The rule base consists of only three rules to reduce the number of tuneable parameters of the controller. Minimum (Min) triangular norm and Bounded Sum (BS) triangular co-norm are used as conjunction and disjunction operators to reduce the number of rules. Karnik–Mendel (KM) type reducer and Weighted Average (WA) defuzzifier are considered to derive the analytical structure of the fuzzy controller. Properties and gain variations of the fuzzy controller are investigated. Simulation study is carried out on nonlinear dynamical systems to verify the applicability of the fuzzy controllers. In general, fuzzy controllers of TS type are computationally more attractive than Mamdani type since TS rule base employs linear functions of inputs whereas Mamdani rule base employs fuzzy sets.Hence inferencing is not required in the case of TS fuzzy systems which makes it computationally attractive.Analysis and design of IT2 fuzzy PI/PD controllers of TS type are essential.In this paper we make an attempt to generalize the analytical structure of IT2 fuzzy PI/PD controllers.A similar kind of study was performed by the authors in Raj and Mohan (2018a) where mathematical models of fuzzy controllers were developed using type-1 fuzzy sets.Here we extend the mathematical modeling of fuzzy controllers using IT2 fuzzy sets which provide an additional degree of freedom in terms of FoU.FoU allows incorporating uncertainty in the membership functions, making the controller structure more robust.We consider multiple IT2 input fuzzy sets with trapezoidal/triangular membership functions.The analytical structure is general in the sense that the number of fuzzy sets on the input variables and the shape of the membership functions can be varied according to requirement.Three rules capture the entire control strategy efficiently and reduce the overall number of tuneable parameters of the fuzzy controller.Min triangular norm and BS triangular co-norm are used for performing logical conjunction and disjunction in the rule base.We use KM type reducer and WA defuzzifier to obtain the general analytical structure of IT2 fuzzy PI/PD controller.An analysis of the control surfaces of type-1 fuzzy (Raj and Mohan, 2018a) and IT2 fuzzy controllers is presented which provides insights into the performance of the fuzzy controllers.A comparative analysis of analytical structures and the associated computational burdens of type-1 and IT2 fuzzy controllers are also discussed.A relation between type-1 and IT2 fuzzy controllers is established.It is shown that the controller model in Raj and Mohan (2018a) is a special case of the controller model proposed in this paper.The simulation study on control of three nonlinear dynamical systems validates the performance of IT2 fuzzy PI/PD controllers.
We develop a novel divide-and-conquer framework for image restoration and enhancement based on their task-driven requirements, which takes advantage of visual importance differences of image contents (i.e., noise versus image, edge-based structures versus smoothing areas, high-frequency versus low-frequency components) and sparse prior differences of image contents for performance improvements. The proposed framework is efficient in implementation of decomposition-processing-integration. An observed image is first decomposed into different subspaces based on considering visual importance of different subspaces and exploiting their prior differences. Different models are separately established for image subspace restoration and enhancement, and existing image restoration and enhancement methods are utilized to deal with them effectively. Then a simple but effective fusion scheme with different weights is used to integrate the post-processed subspaces for the final reconstructed image. Final experimental results demonstrate that the proposed divide-and-conquer framework outperforms several restoration and enhancement algorithms in both subjective results and objective assessments. The performance improvements of image restoration and enhancement can be yielded by using the proposed divide-and-conquer strategy, which greatly benefits in terms of mixed Gaussian and salt-and-pepper noise removal, non-blind deconvolution, and image enhancement. In addition, our divide-and-conquer framework can be simply extensible to other restoration and enhancement algorithms, and can be a new way to promote their performances for image restoration and enhancement. Image restoration and enhancement have been significant topics in image processing and computer vision, and a large number of image restoration and enhancement algorithms are widely applied in the fields of medical image restoration (Eldaly et al., 2018; Zhang et al., 2017b), underwater image or video enhancement (Ancuti et al., 2018) and remote sensing fusion (Wang et al., 2019).The aim of image restoration is to recover an ideal image from a degraded image according to degradation principles, while the target of image enhancement is to enhance an original image by promoting useful characteristics and inhibiting uninteresting information according to specific requirements.The former is an objective process to restore an ideal image from image degradation model, while the latter is a subjective process of improving image quality by referring to human visual perception.However, their common purpose is to improve image visual quality based on respective principles.
We develop a novel divide-and-conquer framework for image restoration and enhancement based on their task-driven requirements, which takes advantage of visual importance differences of image contents (i.e., noise versus image, edge-based structures versus smoothing areas, high-frequency versus low-frequency components) and sparse prior differences of image contents for performance improvements. The proposed framework is efficient in implementation of decomposition-processing-integration. An observed image is first decomposed into different subspaces based on considering visual importance of different subspaces and exploiting their prior differences. Different models are separately established for image subspace restoration and enhancement, and existing image restoration and enhancement methods are utilized to deal with them effectively. Then a simple but effective fusion scheme with different weights is used to integrate the post-processed subspaces for the final reconstructed image. Final experimental results demonstrate that the proposed divide-and-conquer framework outperforms several restoration and enhancement algorithms in both subjective results and objective assessments. The performance improvements of image restoration and enhancement can be yielded by using the proposed divide-and-conquer strategy, which greatly benefits in terms of mixed Gaussian and salt-and-pepper noise removal, non-blind deconvolution, and image enhancement. In addition, our divide-and-conquer framework can be simply extensible to other restoration and enhancement algorithms, and can be a new way to promote their performances for image restoration and enhancement. Significant developments of image restoration and enhancement have been witnessed in recent years.A comprehensive review of image restoration and enhancement algorithms will be briefly presented as follows:
We develop a novel divide-and-conquer framework for image restoration and enhancement based on their task-driven requirements, which takes advantage of visual importance differences of image contents (i.e., noise versus image, edge-based structures versus smoothing areas, high-frequency versus low-frequency components) and sparse prior differences of image contents for performance improvements. The proposed framework is efficient in implementation of decomposition-processing-integration. An observed image is first decomposed into different subspaces based on considering visual importance of different subspaces and exploiting their prior differences. Different models are separately established for image subspace restoration and enhancement, and existing image restoration and enhancement methods are utilized to deal with them effectively. Then a simple but effective fusion scheme with different weights is used to integrate the post-processed subspaces for the final reconstructed image. Final experimental results demonstrate that the proposed divide-and-conquer framework outperforms several restoration and enhancement algorithms in both subjective results and objective assessments. The performance improvements of image restoration and enhancement can be yielded by using the proposed divide-and-conquer strategy, which greatly benefits in terms of mixed Gaussian and salt-and-pepper noise removal, non-blind deconvolution, and image enhancement. In addition, our divide-and-conquer framework can be simply extensible to other restoration and enhancement algorithms, and can be a new way to promote their performances for image restoration and enhancement. Image Restoration Numerous image restoration algorithms have been developed, and existing restoration methods can be classified into four categories.The first class of restoration method is based on spatial domain, including Wiener filtering (Suresh et al., 2018), bilateral filtering (Zhu et al., 2017), guided filtering (GF) (He et al., 2010), nonlocal means (Buades et al., 2005).They employ image local or nonlocal similarity in spatial domain to recover an ideal image.However, limited in spatial domain, complex degradation problems are difficult to be addressed without the utility of frequency or other transform domains.The second class of restoration method is based on transform domain, including wavelet (He et al., 2019), curvelet (Gai, 2018), contourlet transform (Zhang et al., 2017a), and block matching and 3D filtering (BM3D) (Dabov et al., 2007).They obtain better results by taking helpful image properties in transform domain, but large amount of calculation are taken and mathematical theories are complex.The third class of restoration method is based on dictionary learning, which exploits data learning and dictionary optimizing to represent an image in adaptive and sparse ways, and the restoration result is better than that of traditional transform domain methods.The representatives involve K-singular value decomposition (KSVD) (Aharon et al., 2006) and its weighted version (WKSVD) (Liu et al., 2013), beta process factor analysis (BPFA) (Zhou et al., 2012), and centralized sparse representation (CSR) (Dong et al., 2011).However, they have a high computational complexity due to the fact that dictionary training and optimization requires a lot of computing time.The last class of restoration method is based on deep neural network, which uses numerous pairs of ideal and degraded images or patches to train network parameters, and a final output is obtained via a trained network model.Stacked sparse denoising auto-encoders (Zhang et al., 2018b), deconvolutional network (Wan et al., 2018), deep recurrent neural network (Tao et al., 2018; Zhang et al., 2018a) are representatives that have advantages of self-learning, robustness and adaptability.Unfortunately, they require numerous data samples and much training time, and hardware cost of their implementation is high.To overcome their disadvantages, a number of recent works (Liu et al., 2016; Zhang et al., 2017d; Dong et al., 2018) are proposed to combine iterative optimization and learning discriminative image priors, tailored to a specific restoration task.In addition, discriminative transfer learning (Xiao et al., 2018; Badri et al., 2016) and generative adversarial network techniques (Ulyanov et al., 2018) are proposed for effective image restoration, and a convincing tradeoff between restoration quality and computational efficiency is achieved.
We develop a novel divide-and-conquer framework for image restoration and enhancement based on their task-driven requirements, which takes advantage of visual importance differences of image contents (i.e., noise versus image, edge-based structures versus smoothing areas, high-frequency versus low-frequency components) and sparse prior differences of image contents for performance improvements. The proposed framework is efficient in implementation of decomposition-processing-integration. An observed image is first decomposed into different subspaces based on considering visual importance of different subspaces and exploiting their prior differences. Different models are separately established for image subspace restoration and enhancement, and existing image restoration and enhancement methods are utilized to deal with them effectively. Then a simple but effective fusion scheme with different weights is used to integrate the post-processed subspaces for the final reconstructed image. Final experimental results demonstrate that the proposed divide-and-conquer framework outperforms several restoration and enhancement algorithms in both subjective results and objective assessments. The performance improvements of image restoration and enhancement can be yielded by using the proposed divide-and-conquer strategy, which greatly benefits in terms of mixed Gaussian and salt-and-pepper noise removal, non-blind deconvolution, and image enhancement. In addition, our divide-and-conquer framework can be simply extensible to other restoration and enhancement algorithms, and can be a new way to promote their performances for image restoration and enhancement. Image Enhancement A large number of image enhancement algorithms are proposed, and several types of enhancement methods are reviewed subsequently.The first type of enhancement method is based on the Retinex theory, which assumes color sensations strongly correlating with reflectance, and the amount of visible light reaching human eyes depending on the product of reflectance and illumination (Land and McCann, 1971; Bertalmio et al., 2009).It decomposes an image into the illumination and the reflectance, and the two components are computed with different regularization constraints.Single-scale Retinex (Jobson et al., 1997b), multi-scale Retinex (Jobson et al., 1997a), Retinex based on total variation (Ng and Wang, 2011; Yue et al., 2017; Li et al., 2018), and Bayesian Retinex (Wang et al., 2014; Fu et al., 2015) are representatives.But they easily produce halo artifacts in salient edges and poorly perform in unnatural images.The second type of enhancement method based on histogram equalization is widely used for contrast enhancement.Exact histogram specification (Coltuc et al., 2006; Balado), and gradient histogram preservation (Zuo et al., 2014; Tanaka et al., 2019), naturalness preserved enhancement (Wang et al., 2013; Wang and Luo, 2018), and gradient distribution specification (GDS) (Gong and Sbalzarini, 2014, 2016), adjust uneven gray probability density distributions to ideal uniform distributions, and the gray-scale range is stretched to enhance image contrast.However, subjective and objective enhancement consistency cannot be achieved without considering imaging mechanism.The third type of enhancement method based on unsharp masking is effective in sharpness and contrast enhancement.It first employs an edge-preserving filter to decompose an image into base and detail layers, and then the two components are enhanced respectively.The edge-preserving filtering methods, including weighted least square framework (WLS) (Farbman et al., 2008; Kou et al., 2018), domain transform (DT) (Gastal and Oliveira, 2011), guided filtering (GF) (He et al., 2010; Guo et al., 2018), and adaptive manifolds filtering (AM) (Gastal and Oliveira, 2012), determine final enhancement effect.However, they fail in the tradeoff between detail enhancement and naturalness retention.The last type of enhancement method is based on partial differential equations (PDE), which is effective in improving edge-based structures.For nonlinear anisotropic diffusion such as the P-M model (Perona and Malik, 1990), image gradient magnitude determines the diffusion of gray values, and the diffusion is stopped across edges.But it is difficult to determine the stopping time of diffusion to obtain nontrivial results.The coherence nonlinear diffusion model (Weickert, 1998, 1999) is directional in gradient and contour directions, but the brushstroke effects may be generated in non-structure regions due to the errors of local structure estimation (Wang et al., 2006).It is worth mentioning that a bio-inspired PDE model (Alaa and Zirhem, 2018) is presented for restoration and enhancement, where a well-established noise estimator is adopted to provide a stopping criterion for diffusion.Furthermore, the literature (Aarab et al., 2018) provides the existence of a global weak solution to a generic reaction–diffusion system, where the theoretical framework analyzes a class of PDE models for restoration and enhancement.These PDE-based methods have complete theoretical frameworks (Jin et al., 2012; Aarab et al., 2018), and partial differential equations are taken in the calculation, which demands for high mathematical theories.Additionally, recent data-driven approaches (Ignatov et al., 2017; Chen et al., 2018b; Hu et al., 2018) are proposed for expressive enhancement results by learning adjustment in terms of color, contrast and saturation, but they are limited in severely underexposed images.
We develop a novel divide-and-conquer framework for image restoration and enhancement based on their task-driven requirements, which takes advantage of visual importance differences of image contents (i.e., noise versus image, edge-based structures versus smoothing areas, high-frequency versus low-frequency components) and sparse prior differences of image contents for performance improvements. The proposed framework is efficient in implementation of decomposition-processing-integration. An observed image is first decomposed into different subspaces based on considering visual importance of different subspaces and exploiting their prior differences. Different models are separately established for image subspace restoration and enhancement, and existing image restoration and enhancement methods are utilized to deal with them effectively. Then a simple but effective fusion scheme with different weights is used to integrate the post-processed subspaces for the final reconstructed image. Final experimental results demonstrate that the proposed divide-and-conquer framework outperforms several restoration and enhancement algorithms in both subjective results and objective assessments. The performance improvements of image restoration and enhancement can be yielded by using the proposed divide-and-conquer strategy, which greatly benefits in terms of mixed Gaussian and salt-and-pepper noise removal, non-blind deconvolution, and image enhancement. In addition, our divide-and-conquer framework can be simply extensible to other restoration and enhancement algorithms, and can be a new way to promote their performances for image restoration and enhancement. Despite the achievements of aforementioned image restoration and enhancement approaches, the limitation for them can be found below: Task-driven image restoration and enhancement has not been currently taken into account in these restoration and enhancement methods, and an observed image is directly processed through these restoration and enhancement methods which only focus on sparse priors of the whole image.However, these methods ignore sparse prior differences of image contents (noise versus image, edge-based structures versus smoothing areas, high-frequency versus low-frequency components), and even these differences cannot be effectively utilized.Fortunately, the divide-and-conquer scheme (He et al., 2014; Lampert, 2010; Yin and Collins, 2006; Gao et al., 2011; Blanes et al., 2012) is based on task-driven requirements of image restoration and enhancement, and can consider visual importance differences of image contents (noise versus image, edge-based structures versus smoothing areas, high-frequency versus low-frequency components) and exploit their prior differences for performance improvements.To address the above problem, we develop a divide-and-conquer framework based on task-driven requirements of image restoration and enhancement, and the main contributions of the paper are summarized below:
A 54-year-old man was admitted to the intensive care unit with an aneurysmal subarachnoid hemorrhage and subsequently underwent mechanical ventilation and received neuromuscular blocking drugs to control refractory elevated intracranial pressure. During quantitative EEG monitoring, an automated alert was triggered by the train of four peripheral nerve stimulation artifacts. Real-time feedback was made possible due to remote monitoring. This case illustrates how computerized, automated artificial intelligence algorithms can be used beyond typical seizure detection in the intensive care unit for remote monitoring to benefit patient care. Since the introduction of digital electroencephalogram (EEG) and computer-based microprocessing, methods of EEG recording have evolved from paper records to dedicated EEG digital servers that can be accessed remotely, similar to telemedicine in the intensive care unit (ICU) [1,2].Further, raw EEG data can now be processed via computerized software by Fast Fourier transform techniques into condensed quantitative EEG (QEEG) displays with numerous mathematical derivatives for seizure detection and even surrogate cerebral blood flow inferences.The technology for EEG data analysis has advanced rapidly in the last decade, using an array of sophisticated software and artificial intelligence algorithms for seizure detection based on the EEG waveform morphology (i.e., spike detection) combined with spike frequency (> 2–3 Hz), or on a combination of amplitude, morphology, and frequency (seizure detection and artifact rejection).
A 54-year-old man was admitted to the intensive care unit with an aneurysmal subarachnoid hemorrhage and subsequently underwent mechanical ventilation and received neuromuscular blocking drugs to control refractory elevated intracranial pressure. During quantitative EEG monitoring, an automated alert was triggered by the train of four peripheral nerve stimulation artifacts. Real-time feedback was made possible due to remote monitoring. This case illustrates how computerized, automated artificial intelligence algorithms can be used beyond typical seizure detection in the intensive care unit for remote monitoring to benefit patient care. EEG with QEEG trend analysis panels are increasingly used in the ICU setting to remotely monitor for nonconvulsive seizures and status epilepticus, as well as to provide potential prognostic information after brain injury (e.g., cardiac arrest) and monitor vasospasm in subarachnoid hemorrhage [3].Remote monitoring technological advancements can now send encrypted electronic alerts via email to a subspecialist's mobile phone (iPhone or Android platforms) for near-real-time ICU monitoring.Despite these advances in computer technology, they are not immune to numerous ICU artifacts.This is because they sample various frequencies, amplitudes, and sharp/spike morphology, which can generate erroneous “artifacts” in attempts to detect seizure.The ICU also has many types of electrical interference and 60 Hz artifact from mechanical ventilators and enteral feeding machines, which contaminate the EEG recording [4].Therefore, ICU EEG monitoring still requires human review of the raw EEG, despite automated technology alerts, to distinguish clinically significant seizures from other ICU-generated artifacts.
Micro-blog has changed people’s life, study, and work styles. Every day, we want to know what public opinion news happens and how it evolves. Extracting and tracking these topics correctly help us better understand the latest public opinions and pay attention to their evolution. To extract topics from Microblog posts accurately, we adopt five unique features of micro-blogs to drive the joint probability distributions of all words and topics, and improve LDA into our topic extraction model(named MF-LDA). To track evolution trend of the topic, we propose a hot topic life cycle model (named HTLCM). We divide the HTLCM into five stages, namely, birth, growth, maturity, decline, and disappearance. The HTLCM determines whether a topic is the candidate hot topic or not and estimates hot topic evolution stages. On the other hand, we propose a hot topic tracking (shorten for HTT) algorithm which integrates MF-LDA and HTLCM. First, the HTT assigns candidate hot topics, which are labeled by HTLCM, to the corresponding time window according to the release time. Second, to obtain the hot topic in each time window, we input Micro-blog posts of each time window into MF-LDA in order. By analyzing changes in these hot topics, we track the changes in their contents. The experiment results show that MF-LDA has a lower perplexity and higher coverage rate than LDA under the same conditions. We conclude parameters of the Transition regions of our proposed HTLCM model. The MR, FR of our proposed HTLCM model are lower than 18%. The average P, R, F of the HTT algorithm are 85.64%, 84.97%, 85.66%, respectively. A practical application on topicFemale driver beats male driver in chengdu shows an excellent effect and practical significance of HTLCM model and HTT algorithm in extracting and tracking hot topics. With rapid development of communication technologies and popularization of smartphones, more and more people begin to use mobile Internet.On December 2017, the number of Internet users in China reached 731 million, among which 695 million are mobile Internet users.This proportion increases from 90.1% (the end of 2015) to 95.1% (Anon., 2019).The high-speed development of the mobile Internet network rapidly rise development of social network platforms, such as Sina Micro-blog.
Micro-blog has changed people’s life, study, and work styles. Every day, we want to know what public opinion news happens and how it evolves. Extracting and tracking these topics correctly help us better understand the latest public opinions and pay attention to their evolution. To extract topics from Microblog posts accurately, we adopt five unique features of micro-blogs to drive the joint probability distributions of all words and topics, and improve LDA into our topic extraction model(named MF-LDA). To track evolution trend of the topic, we propose a hot topic life cycle model (named HTLCM). We divide the HTLCM into five stages, namely, birth, growth, maturity, decline, and disappearance. The HTLCM determines whether a topic is the candidate hot topic or not and estimates hot topic evolution stages. On the other hand, we propose a hot topic tracking (shorten for HTT) algorithm which integrates MF-LDA and HTLCM. First, the HTT assigns candidate hot topics, which are labeled by HTLCM, to the corresponding time window according to the release time. Second, to obtain the hot topic in each time window, we input Micro-blog posts of each time window into MF-LDA in order. By analyzing changes in these hot topics, we track the changes in their contents. The experiment results show that MF-LDA has a lower perplexity and higher coverage rate than LDA under the same conditions. We conclude parameters of the Transition regions of our proposed HTLCM model. The MR, FR of our proposed HTLCM model are lower than 18%. The average P, R, F of the HTT algorithm are 85.64%, 84.97%, 85.66%, respectively. A practical application on topicFemale driver beats male driver in chengdu shows an excellent effect and practical significance of HTLCM model and HTT algorithm in extracting and tracking hot topics. The registered users in Sina Micro-blog share videos, images, and text messages of 140 words to other users.Micro-blog platforms have hundreds of millions of data flows every day.The data can cover all aspects of human life and contain abundant amounts of valuable information.
The ‘Euphrates Monochrome Painted Ware’ (henceforth EMPW) is a ceramic style attested in the Middle Euphrates region in northern Syria at the beginning of the Early Bronze Age, ca. 2900–2700 BCE. This style is not an isolated phenomenon; rather, it must be understood in the context of a general, albeit short-lived, re-introduction of painted ceramics into local assemblages of Greater Mesopotamia. In the present study, we investigate the technology and provenance of the painted pottery from Tell el-'Abd (North Syria) and its relation to contemporary ceramics retrieved at this site. We apply a combination of macroscopic observations, ceramic petrography, and micro X-ray diffraction (μ-XRD2) in order to reconstruct the manufacturing process and to define the mineralogical and chemical composition of the sherds as well as of the pigments used for the painted decoration. The results of these analyses are then compared to the local geology in order to identify possible raw material sources. Based on the evidence, we provide the first interpretation of the provenance and technology of the Euphrates Monochrome Painted as well as unpainted ceramics of the assemblage. In Greater Mesopotamia, the Early Bronze Age (EBA), roughly corresponding to the time span between 3100 and 2000 BCE, constitutes a period of dramatic social, political, and economic change (Akkermans and Schwartz, 2003: 211–287; Cooper, 2006).This led to the rise and successive consolidation, at least in some areas, of a new urban model which through alternate phases would have characterised the history of the region in the millennia ahead.
The ‘Euphrates Monochrome Painted Ware’ (henceforth EMPW) is a ceramic style attested in the Middle Euphrates region in northern Syria at the beginning of the Early Bronze Age, ca. 2900–2700 BCE. This style is not an isolated phenomenon; rather, it must be understood in the context of a general, albeit short-lived, re-introduction of painted ceramics into local assemblages of Greater Mesopotamia. In the present study, we investigate the technology and provenance of the painted pottery from Tell el-'Abd (North Syria) and its relation to contemporary ceramics retrieved at this site. We apply a combination of macroscopic observations, ceramic petrography, and micro X-ray diffraction (μ-XRD2) in order to reconstruct the manufacturing process and to define the mineralogical and chemical composition of the sherds as well as of the pigments used for the painted decoration. The results of these analyses are then compared to the local geology in order to identify possible raw material sources. Based on the evidence, we provide the first interpretation of the provenance and technology of the Euphrates Monochrome Painted as well as unpainted ceramics of the assemblage. The initial stage of the period (ca. 3100–2700 BCE) follows, and is partly the result of, the collapse of the complex exchange network established in the LC 3-5/Uruk period of the second half of the fourth millennium (Rothman, 2001) and is marked by a trend towards regionalisation.Single areas undertook different trajectories of development, as pointed out by their own local character in settlement pattern (nucleated versus sparse), strong differences in site size and territorial organisation (Wilkinson et al., 2014) and, most importantly, by the emergence of a wide range of pottery styles quite different the one from the other.The latter still lay the foundation for most chronological assessments (Lebeau, 2014) for the proto-historic periods.
The ‘Euphrates Monochrome Painted Ware’ (henceforth EMPW) is a ceramic style attested in the Middle Euphrates region in northern Syria at the beginning of the Early Bronze Age, ca. 2900–2700 BCE. This style is not an isolated phenomenon; rather, it must be understood in the context of a general, albeit short-lived, re-introduction of painted ceramics into local assemblages of Greater Mesopotamia. In the present study, we investigate the technology and provenance of the painted pottery from Tell el-'Abd (North Syria) and its relation to contemporary ceramics retrieved at this site. We apply a combination of macroscopic observations, ceramic petrography, and micro X-ray diffraction (μ-XRD2) in order to reconstruct the manufacturing process and to define the mineralogical and chemical composition of the sherds as well as of the pigments used for the painted decoration. The results of these analyses are then compared to the local geology in order to identify possible raw material sources. Based on the evidence, we provide the first interpretation of the provenance and technology of the Euphrates Monochrome Painted as well as unpainted ceramics of the assemblage. The tendency towards regionalisation is somehow substantiated by a short-term revival of the role of painted pottery as bearer of symbolic value.Over a wide area spanning from the Mediterranean coast (Amuq plain) to the Iranian plateau, and from the Taurus mountains down to the Gulf, roughly at the same time painted decoration re-appeared on different kinds of vessels (Gerber, 2005: 81–103; Rova, 2014).This trend remains apparently unconnected with specific functional needs and has been interpreted as a renewed desire for the communities to express kin-based connections or intentionally mark their cultural identity; it has even been related to the appearance of new ethnic groups (Rova, 2014).
The no-wait job shop where no waiting time is allowed between two successive operations of a job has a strong industrial background, especially in steel-making industry and concrete manufacturing. This study formulates the no-wait job shop problem with a total flow time criterion based on time difference and decomposes the problem into timetabling and sequencing subproblems. Several timetabling methods are designed for the total flow time criterion to generate a sequence timetable. By adopting favourable features of the iterated greedy algorithm, the population-based iterated greedy (PBIG) algorithm for the sequencing subproblem is proposed. The individuals in the population evolve by means of a destruction and construction perturbator and an insertion-based local search. In each iteration, a tournament selection is designed to replace a relatively worse solution. In order to generate starting solutions with a certain quality and diversity, the Nawaz–Enscore–Ham-based heuristics for flow shop scheduling are extended in no-wait job shops. In computational experiments based on well-known benchmark instances, timetabling methods are investigated, and it is shown that the left timetabling is superior to other timetabling methods for the total flow time minimisation. Computational results also show that the proposed algorithm significantly outperforms several state-of-the-art metaheuristics, and it could be applicable to practical production environment. In recent years, shop scheduling has been extensively studied, and enormous progress has been made in complicated flow shop and job shop scheduling (Li et al., 2019, 2014).In the traditional job shop scheduling problem (JSP), it is assumed that when a job completes a prior operation, the posterior operation does not have to be processed promptly because the job is allowed to be stored.However, there are numerous real production environments where a job has to be processed continuously until its last operation once it is started.Hence, no waiting time is allowed between any two consecutive operations of a job.Such no-wait constraints widely exist in the steel-making industry (Pinedo, 2016; Tang et al., 2000), concrete manufacturing (Grabowski and Pempera, 2000; Deng et al., 2015), chemical and pharmaceutical industries (Rajendran, 1994), food industry (Hall and Sriskandarajah, 1996), etc.A steelmaking continuous casting process generally consists of several production stages, including the steel melting process and the steel solidifying process.The heated steel must continuously go through a sequence of operations before it is cooled.Another example of the no-wait constraints is the food industry, where the canning operation has to immediately follow the cooking to ensure freshness.The job shop problem with the no-wait constraint is called the no-wait job shop scheduling problem (NWJSP), which is considerably different from the JSP because of the former’s no-wait characteristics.
The no-wait job shop where no waiting time is allowed between two successive operations of a job has a strong industrial background, especially in steel-making industry and concrete manufacturing. This study formulates the no-wait job shop problem with a total flow time criterion based on time difference and decomposes the problem into timetabling and sequencing subproblems. Several timetabling methods are designed for the total flow time criterion to generate a sequence timetable. By adopting favourable features of the iterated greedy algorithm, the population-based iterated greedy (PBIG) algorithm for the sequencing subproblem is proposed. The individuals in the population evolve by means of a destruction and construction perturbator and an insertion-based local search. In each iteration, a tournament selection is designed to replace a relatively worse solution. In order to generate starting solutions with a certain quality and diversity, the Nawaz–Enscore–Ham-based heuristics for flow shop scheduling are extended in no-wait job shops. In computational experiments based on well-known benchmark instances, timetabling methods are investigated, and it is shown that the left timetabling is superior to other timetabling methods for the total flow time minimisation. Computational results also show that the proposed algorithm significantly outperforms several state-of-the-art metaheuristics, and it could be applicable to practical production environment. The no-wait job shop scheduling problem has gained the increasing attention of researchers in previous decades.With regard to its complexity, the NWJSP is a non-deterministic polynomial time (NP)-hard problem to a considerable degree (Lenstra et al., 1977).Sahni and Cho (1979) proved that it is strongly NP-hard even for two machine cases.In an earlier research report on solving the NWJSP, Kubiak (1989) proposed a pseudo-polynomial time algorithm to minimise the makespan for the problem with two machines.Hall and Sriskandarajah (1996) later provided a remarkable review and emphasised the difficulty of the NWJSP especially for large-size cases.Mascis and Pacciarelli (2002) studied the characteristics of the problem, formulated it as an alternative graph resembling the disjunctive graph of the JSP, and proposed several heuristics and a branch-and-bound method for the problem.Woeginger (2004) investigated the approximability of the problem and showed the complexity of the problem for two and three machines.Bansal et al. (2005) also studied the problem with two machines and demonstrated the use of a polynomial time approximation algorithm. van den Broek (2009)formulated the problem as a mixed-integer programme and presented a branch-and-bound method.
The no-wait job shop where no waiting time is allowed between two successive operations of a job has a strong industrial background, especially in steel-making industry and concrete manufacturing. This study formulates the no-wait job shop problem with a total flow time criterion based on time difference and decomposes the problem into timetabling and sequencing subproblems. Several timetabling methods are designed for the total flow time criterion to generate a sequence timetable. By adopting favourable features of the iterated greedy algorithm, the population-based iterated greedy (PBIG) algorithm for the sequencing subproblem is proposed. The individuals in the population evolve by means of a destruction and construction perturbator and an insertion-based local search. In each iteration, a tournament selection is designed to replace a relatively worse solution. In order to generate starting solutions with a certain quality and diversity, the Nawaz–Enscore–Ham-based heuristics for flow shop scheduling are extended in no-wait job shops. In computational experiments based on well-known benchmark instances, timetabling methods are investigated, and it is shown that the left timetabling is superior to other timetabling methods for the total flow time minimisation. Computational results also show that the proposed algorithm significantly outperforms several state-of-the-art metaheuristics, and it could be applicable to practical production environment. Metaheuristics have been commonly used by researchers as approaches to solve the NWJSP.The pioneer work conducted by Macchiaroli et al. (1999) decomposed the problem into two subproblems and presented a two-phase tabu search algorithm and showed its enhanced results as compared with dispatching rules.Schuster and Framinan (2003) tackled the problem by introducing a variable neighbourhood search (VNS) algorithm, as well as a hybrid simulated annealing/generic algorithm (GASA).A fast tabu search method and complete local search with memory (CLM), presented by Schuster (2006) and Framinan and Schuster (2006), respectively, both attained excellent performance for the problem.Zhu et al. (2009) proposed a complete local search with limited memory (CLLM) and showed its superiority over the VNS, GASA, and CLM algorithms.A modified complete local search with memory (MCLM) was later developed by Zhu and Li (2012), who showed its incomparable superiority over the CLM and CLLM methods.Most recently, in the complete local search with memory and variable neighbourhood structure (CLMMV) developed by Li et al. (2016), the CLLM was strengthened by integrating the variable neighbourhood structures.Sundar et al. (2017) proposed a hybrid artificial bee colony (HABC) algorithm for the problem and stated that the HABC is better than the CLLM and MCLM.Several other approaches, such as the hybrid genetic algorithm (Pan and Huang, 2009), hybrid tabu search (Bozejko and Makuchowski, 2009), genetic algorithm with automatic adjustment (Bozejko and Makuchowski, 2011), neuro-evolutionary variable neighbourhood search (Mokhtari, 2014), branch-and-bound method and particle swarm optimisation (Aitzai et al., 2016) have been proposed.Samarghandi et al. (2013) studied the effect of different combinations of timetabling with sequencing algorithms for the problem.Most recently, Bürgy and Gröflin (2013) proposed an effective approach based on optimal job insertion, and thereafter, extended the approach to other objectives (Bürgy and Gröflin, 2016).Deng et al. (2019) studied the total flow time criterion for the problem and proposed an effective hybrid discrete group search optimiser (HDGSO).
The no-wait job shop where no waiting time is allowed between two successive operations of a job has a strong industrial background, especially in steel-making industry and concrete manufacturing. This study formulates the no-wait job shop problem with a total flow time criterion based on time difference and decomposes the problem into timetabling and sequencing subproblems. Several timetabling methods are designed for the total flow time criterion to generate a sequence timetable. By adopting favourable features of the iterated greedy algorithm, the population-based iterated greedy (PBIG) algorithm for the sequencing subproblem is proposed. The individuals in the population evolve by means of a destruction and construction perturbator and an insertion-based local search. In each iteration, a tournament selection is designed to replace a relatively worse solution. In order to generate starting solutions with a certain quality and diversity, the Nawaz–Enscore–Ham-based heuristics for flow shop scheduling are extended in no-wait job shops. In computational experiments based on well-known benchmark instances, timetabling methods are investigated, and it is shown that the left timetabling is superior to other timetabling methods for the total flow time minimisation. Computational results also show that the proposed algorithm significantly outperforms several state-of-the-art metaheuristics, and it could be applicable to practical production environment. The above research is primarily focused on the minimisation of the makespan.However, effective approaches remain limited, especially for some other criteria, such as total tardiness and total flow time.Therefore, it is significant to develop high-performing algorithms for the problem with other criteria in both theory and engineering application.To the best of our knowledge, practically all existing metaheuristics for the NWJSP are based on the concept of decomposition in which the problem is decomposed into timetabling and sequencing subproblems, as proposed by Macchiaroli et al. (1999).Similarly, to handle the problem with the total flow time criterion, it is formulated based on time difference and decomposed into two subproblems.Considering that the sequencing subproblem is the same as the permutation flow shop problem (PFSP) to a certain degree, metaheuristics used in the PFSP are totally applicable to the NWJSP.As an effective and efficient procedure, the iterated greedy (IG) algorithm originally presented by Ruiz and Stutzle (2007) has gained a host of successful applications in various scheduling environments, such as multi-objective flow shop scheduling problem (Minella et al., 2011), mixed no-idle permutation flow shop scheduling problem (Pan and Ruiz, 2014), single machine total tardiness scheduling (Deng and Gu, 2014), multi-objective unrelated parallel machine scheduling (Lin et al., 2016), blocking job shop scheduling problem (Pranzo and Pacciarelli, 2016), blocking flow shop scheduling problem (Tasgetiren et al., 2017), scheduling unrelated parallel batch machines with non-identical capacities and unequal ready times (Arroyo and Leung, 2017), distributed no-wait flow shop scheduling problem (Shao et al., 2017), job shop scheduling with job families and sequence-dependent set-ups (Kim et al., 2017), identical parallel machine scheduling to minimise total tardiness (Lee, 2018), distributed flow shop scheduling problem (Ruiz et al., 2019), and total tardiness parallel blocking flow shop scheduling problem (Ribas et al., 2019).Pan and Ruiz (2012a) developed a population-based iterated greedy algorithm (PIGA) and showed its effectiveness in solving flow shop scheduling problem with flow time minimisation.
The no-wait job shop where no waiting time is allowed between two successive operations of a job has a strong industrial background, especially in steel-making industry and concrete manufacturing. This study formulates the no-wait job shop problem with a total flow time criterion based on time difference and decomposes the problem into timetabling and sequencing subproblems. Several timetabling methods are designed for the total flow time criterion to generate a sequence timetable. By adopting favourable features of the iterated greedy algorithm, the population-based iterated greedy (PBIG) algorithm for the sequencing subproblem is proposed. The individuals in the population evolve by means of a destruction and construction perturbator and an insertion-based local search. In each iteration, a tournament selection is designed to replace a relatively worse solution. In order to generate starting solutions with a certain quality and diversity, the Nawaz–Enscore–Ham-based heuristics for flow shop scheduling are extended in no-wait job shops. In computational experiments based on well-known benchmark instances, timetabling methods are investigated, and it is shown that the left timetabling is superior to other timetabling methods for the total flow time minimisation. Computational results also show that the proposed algorithm significantly outperforms several state-of-the-art metaheuristics, and it could be applicable to practical production environment. On one hand, like the traditional JSP, the NWJSP can be formulated as a mixed-integer programme and solved by branch-and-bound method.On the other hand, unlike the metaheuristics for the traditional JSP, most of the metaheuristics for the NWJSP adopt the decomposition idea in Macchiaroli et al. (1999).The differences among all those metaheuristics for the NWJSP mainly lie in the timetabling methods used for the timetabling subproblem and especially the searching paradigm of the algorithm used for the sequencing subproblem.
The no-wait job shop where no waiting time is allowed between two successive operations of a job has a strong industrial background, especially in steel-making industry and concrete manufacturing. This study formulates the no-wait job shop problem with a total flow time criterion based on time difference and decomposes the problem into timetabling and sequencing subproblems. Several timetabling methods are designed for the total flow time criterion to generate a sequence timetable. By adopting favourable features of the iterated greedy algorithm, the population-based iterated greedy (PBIG) algorithm for the sequencing subproblem is proposed. The individuals in the population evolve by means of a destruction and construction perturbator and an insertion-based local search. In each iteration, a tournament selection is designed to replace a relatively worse solution. In order to generate starting solutions with a certain quality and diversity, the Nawaz–Enscore–Ham-based heuristics for flow shop scheduling are extended in no-wait job shops. In computational experiments based on well-known benchmark instances, timetabling methods are investigated, and it is shown that the left timetabling is superior to other timetabling methods for the total flow time minimisation. Computational results also show that the proposed algorithm significantly outperforms several state-of-the-art metaheuristics, and it could be applicable to practical production environment. The foregoing motivated us to adapt the IG for the sequencing subproblem with the total flow time criterion.The following key elements are introduced to the iterated greedy algorithm: a destruction and construction perturbator, an insertion-based local search, and an evolving scheme based on population.A population-based iterated greedy (PBIG) algorithm is proposed for the total flow time minimisation in the no-wait job shop.Moreover, the Nawaz–Enscore–Ham (NEH) heuristic (Nawaz et al., 1983), originally presented for the PFSP, is extended in the NWJSP for the first time.
This paper examines the stone tool technology from the site of Kenure, Co. Dublin, on the East coast of Ireland. Collected in the 1940s by the avocational archaeologist Gwendoline C. Stacpoole, Kenure represents an extremely large surface assemblage likely belonging to the later phase of the Irish Late Mesolithic period (~7000–5500 BP). The results of this analysis affirm the likelihood that the Kenure lithic assemblage does, in fact, date to the Late Mesolithic. Core reduction was conducted using hard hammers and was generally expedient, usually involving the splitting of locally occurring glacial till chert cobbles followed by the use of single platform core reduction strategies. Retouched tools were characterized by a range of scraper, notch, denticulate, and borer forms. Moderate frequencies of convergent flakes were also present, with many fitting the typological definition of Bann flakes. In addition, there were noticeable numbers of small blades, though these generally lack the specialized technical features of Mesolithic blades from the Irish Early Mesolithic and Mesolithic sites in other regions of Western Europe. Moderate frequencies of pieces derived from bipolar percussion were also present in the Kenure assemblage and this paper offers an explanation of the use of bipolar percussion in relation to the small size of locally available lithic raw materials. Finally, this paper concludes with a consideration of the implications of the technological features of the Kenure assemblage for the organization of Irish Late Mesolithic foraging technology, as well as the potential for future research on the Kenure collection. For nearly a century, archaeological remains dating to the Irish Later Mesolithic (~7000–5500 BP; hereafter ILM) have been documented along Ireland's coastlines and waterways.Yet, ILM lithic industries remain known mostly from a small number of excavated sites and a scattering of surface assemblages.This paper offers an assessment of a somewhat odd source of information on variability in ILM lithic technology: the surface collections made by Gwendoline C. Stacpoole in the 1940s at locations along the Leinster coast north of Dublin (Liversage, 1961; Stacpoole, 1963; Woodman, 1977a, 1978).These assemblages are obviously not without some serious problems in terms of provenience.Their collection was poorly documented and their chronology is ambiguous relative to systematically excavated sites from this time period.Yet, what the Stacpoole collections lack in documentation quality, they perhaps make up for in quantity.Housed in the National Museum of Ireland in Dublin, these collections likely include hundreds of thousands of lithic artifacts.1While it is clear that not all of these lithics belong to the ILM, most of them probably do.In particular, on the basis of index typology at least, the Stacpoole collection from the site of Kenure, Co.Dublin, appears to be dominated by ILM artifacts.Thus, while far from perfect, the Kenure collection would seem to be a useful and under-explored sources of information about the ILM.
This paper examines the stone tool technology from the site of Kenure, Co. Dublin, on the East coast of Ireland. Collected in the 1940s by the avocational archaeologist Gwendoline C. Stacpoole, Kenure represents an extremely large surface assemblage likely belonging to the later phase of the Irish Late Mesolithic period (~7000–5500 BP). The results of this analysis affirm the likelihood that the Kenure lithic assemblage does, in fact, date to the Late Mesolithic. Core reduction was conducted using hard hammers and was generally expedient, usually involving the splitting of locally occurring glacial till chert cobbles followed by the use of single platform core reduction strategies. Retouched tools were characterized by a range of scraper, notch, denticulate, and borer forms. Moderate frequencies of convergent flakes were also present, with many fitting the typological definition of Bann flakes. In addition, there were noticeable numbers of small blades, though these generally lack the specialized technical features of Mesolithic blades from the Irish Early Mesolithic and Mesolithic sites in other regions of Western Europe. Moderate frequencies of pieces derived from bipolar percussion were also present in the Kenure assemblage and this paper offers an explanation of the use of bipolar percussion in relation to the small size of locally available lithic raw materials. Finally, this paper concludes with a consideration of the implications of the technological features of the Kenure assemblage for the organization of Irish Late Mesolithic foraging technology, as well as the potential for future research on the Kenure collection. In this paper, I present an analysis of a sample of the Stacpoole collection from Kenure.After giving some background concerning the fieldwork responsible for this collection, I first present a typological assessment of the collection with particular interest in the assemblage's chronology.Here, I try to look beyond initial characterizations of the Kenure lithic assemblage as belonging to the “Larnian Industry” based on its “crude” qualities and the general absence of lithic type fossils that might be indicative of earlier or later periods (Liversage, 1961; Stacpoole, 1963; contra Woodman, 1977a, 1978).I also found some affirmative evidence for a ILM age for the Kenure assemblage in the form of type fossils and knapping techniques belonging to this period.Thus, in spite of all of its problems, I conclude that much of the Kenure assemblage can tentatively be viewed as belonging to the later phase of the ILM.
Different hypotheses exist to explain population development and replacement on the East Coast of the Yucatan Peninsula after the so-called Maya collapse, a period of dynamic population movement responding to changing socio-economic and political spheres of influence in the region. Here we investigate this dynamic by combining the evidence from dental morphology, and 87Sr/86Sr and δ18O isotopic analyses of the human skeletal remains from the Late Postclassic coastal trader settlements of El Meco, El Rey, and Tulum (1200–1550 CE). Isotopic results show different scenarios by locality. The sample from El Meco does not show foreign individuals, while El Rey presents 20% of the sample as likely non-local. Tulum has the most varied isotopic values of the three, though only one individual could be non-local. The series from the core settlement of Tulum displays lower dental morphological heterogeneity, but higher diversity in artificial head shapes and a small proportion of non-locals, probably due to the small sample size available. El Rey shows homogeneity in head shape and high morphological variability. El Meco is overall the most homogeneous sample from the East Coast of the Peninsula of Yucatán. In the broader contexts of social organization and regional population dynamics, our results indicate broad trends in population movement and cultural cohesion along the Yucatecan coast. Human mobility and biological affinity among Maya groups have been studied under different bioarchaeological lenses, including head shape (Sierra Sosa et al., 2014; Tiesler, 2014; Tiesler and Cucina, 2012, 2010; Tiesler and Ortega Muñoz, 2013), aDNA (González-Oliver et al., 2001; Matheson et al., 2003; Merriwether et al., 1997), dental morphology (Austin, 1978; Cucina, 2013, 2015, 2016; Cucina and Ortega-Muñoz, 2014; Cucina et al., 2008, 2009, 2018; Tiesler and Cucina, 2012; Wrobler and Graham, 2015), morphometric criteria (Cucina and Tiesler Blos, 2004; Scherer, 2007; Scherer and Wright, 2013, 2015; Serafin et al., 2013, 2014; Wrobler and Graham, 2015), as well as with trace elements (Cucina et al., 2011) and strontium and oxygen isotope ratios (Buikstra et al., 2003; Freiwald, 2011; Freiwald et al., 2014; Olsen et al., 2014; Price et al., 2006a, b, c, 2007, 2008, 2010, 2014, 2015, 2018; Scherer and Wright, 2013, 2015; Sierra Sosa et al., 2014; Wright, 2005, 2012; Wright et al., 2010; Wrobler et al., 2014).
Different hypotheses exist to explain population development and replacement on the East Coast of the Yucatan Peninsula after the so-called Maya collapse, a period of dynamic population movement responding to changing socio-economic and political spheres of influence in the region. Here we investigate this dynamic by combining the evidence from dental morphology, and 87Sr/86Sr and δ18O isotopic analyses of the human skeletal remains from the Late Postclassic coastal trader settlements of El Meco, El Rey, and Tulum (1200–1550 CE). Isotopic results show different scenarios by locality. The sample from El Meco does not show foreign individuals, while El Rey presents 20% of the sample as likely non-local. Tulum has the most varied isotopic values of the three, though only one individual could be non-local. The series from the core settlement of Tulum displays lower dental morphological heterogeneity, but higher diversity in artificial head shapes and a small proportion of non-locals, probably due to the small sample size available. El Rey shows homogeneity in head shape and high morphological variability. El Meco is overall the most homogeneous sample from the East Coast of the Peninsula of Yucatán. In the broader contexts of social organization and regional population dynamics, our results indicate broad trends in population movement and cultural cohesion along the Yucatecan coast. Scholars have pointed out that the homogeneity, low genetic variability and genetic continuity among Lowland Maya populations from Classic (250–1000 CE) to Postclassic (1000–1550 CE) periods is reflected in a continuous and intense interchange of items and genes (Austin, 1978; Cucina, 2015; Cucina and Ortega-Muñoz, 2014; Cucina and Tiesler Blos, 2004; Ibarra-Rivera et al., 2008, Serafin et al., 2013; Scherer, 2007; Scherer and Wright, 2015; Wrobler and Graham, 2015).Connections in the form of corridors and paved roadways between main and secondary cities of the lowland Maya areas and among northern coastal localities of the Yucatan peninsula were established (Cucina, 2015; Cucina and Tiesler Blos, 2004).
Different hypotheses exist to explain population development and replacement on the East Coast of the Yucatan Peninsula after the so-called Maya collapse, a period of dynamic population movement responding to changing socio-economic and political spheres of influence in the region. Here we investigate this dynamic by combining the evidence from dental morphology, and 87Sr/86Sr and δ18O isotopic analyses of the human skeletal remains from the Late Postclassic coastal trader settlements of El Meco, El Rey, and Tulum (1200–1550 CE). Isotopic results show different scenarios by locality. The sample from El Meco does not show foreign individuals, while El Rey presents 20% of the sample as likely non-local. Tulum has the most varied isotopic values of the three, though only one individual could be non-local. The series from the core settlement of Tulum displays lower dental morphological heterogeneity, but higher diversity in artificial head shapes and a small proportion of non-locals, probably due to the small sample size available. El Rey shows homogeneity in head shape and high morphological variability. El Meco is overall the most homogeneous sample from the East Coast of the Peninsula of Yucatán. In the broader contexts of social organization and regional population dynamics, our results indicate broad trends in population movement and cultural cohesion along the Yucatecan coast. The “collapse” at the end of the Classic period and the following changes in the political spheres of influence in the region generated a population dynamic that responded to the mutated socioeconomic and political scenario.Scholars debate the nature and the dynamics of continuous massive migrations.Within a complex scenario that varied from region to region within the Maya realm, two main events are thought to have influenced these dynamics in the northern Maya lowlands.The first one concerns the potential arrival at Chichen Itza of non-Maya, or “nahuatized Maya” groups, like the Chontales or Putunes around the 10th century CE — maritime traders influenced by the culture of Central Mexico (Miller, 1982; Thompson, 2003).The second one refers to the potential migrations described by Folan et al. (2000) of groups from the Peten to the northern territories of the Yucatan peninsula on the eve of Late Postclassic (around 1200 CE).The goal of this paper is to assess the effects that migrations had on the population structure of the East Coast of the Peninsula of Yucatan during the Late Postclassic (1200–1550 CE), through the combined lenses of strontium and oxygen isotopic analyses and dental morphology.
Different hypotheses exist to explain population development and replacement on the East Coast of the Yucatan Peninsula after the so-called Maya collapse, a period of dynamic population movement responding to changing socio-economic and political spheres of influence in the region. Here we investigate this dynamic by combining the evidence from dental morphology, and 87Sr/86Sr and δ18O isotopic analyses of the human skeletal remains from the Late Postclassic coastal trader settlements of El Meco, El Rey, and Tulum (1200–1550 CE). Isotopic results show different scenarios by locality. The sample from El Meco does not show foreign individuals, while El Rey presents 20% of the sample as likely non-local. Tulum has the most varied isotopic values of the three, though only one individual could be non-local. The series from the core settlement of Tulum displays lower dental morphological heterogeneity, but higher diversity in artificial head shapes and a small proportion of non-locals, probably due to the small sample size available. El Rey shows homogeneity in head shape and high morphological variability. El Meco is overall the most homogeneous sample from the East Coast of the Peninsula of Yucatán. In the broader contexts of social organization and regional population dynamics, our results indicate broad trends in population movement and cultural cohesion along the Yucatecan coast. The analyses of strontium and oxygen isotope ratios have been applied to study individual residential mobility in human skeletal remains across past societies (Bethard et al., 2008; Turner and Armelagos, 2012) and have been recently applied to the ancient Maya settlements (Freiwald, 2011; Price et al., 2006a, 2010, 2014, 2015; Scherer and Wright, 2015; Suzuki et al., 2018; Wright, 2012).They allow one to identify individuals who can be considered non-local, i.e., individuals who were not born in the place (site) where they were eventually interred.
The need to protect underwater cultural heritage from biodegradation is paramount, however with many sites needing funding and support, it is hard to prioritise, thus the ability to identify high risk sites is crucial to ensure resources are best placed. In doing so a clear understanding of environmental conditions acting upon a site and abundance and composition of species present is essential to this identification. Therefore, the aim of this study was to assess the rate of biodegradation on four underwater cultural heritage sites in different marine environments by placing a series of wooden test panels in direct contact with the exposed structure on the sites. Upon recovery, test panels were photographed, X-rayed, and wood boring and sessile fouling species were identified and counted. The damage attributed to each species was recorded with CAD software. Results indicated a significant difference between sites, with HMS Invincible having the highest abundance of marine wood borers and the highest rate of surface area and volume degradation; whilst vestigial evidence of marine wood borers was found on the London, it would appear the environmental conditions had significantly impeded their survival. The study indicated further factors such as sediment type and coverage, availability of wood and the proximity of other colonised sites were also determining factors controlling the abundance of marine wood borers and the rate of biodegradation. For centuries, timber has been a focal material for shipbuilding, with its wide spread availability, natural hardiness and workable qualities (Eaton and Hale, 1993); furthermore, numerous timber species are available, dependant on region, with elm, oak and pine most commonly used in European shipbuilding until mid-19th century (Couper, 2000; McGrail, 2001).England has derived a prominent relationship with the sea, building global trade networks and developing a strong naval force (Smith, 2009; Vego, 2016); as such, the English Channel has an extensive history of trade, transport and warfare reflected in the many underwater cultural heritage (UCH) sites along the coastline (Pater, 2007).However, many of these wooden UCH sites are at risk of degradation by marine wood borers within the Amphipoda, Isopoda and Myodia (Borges, 2014).
The need to protect underwater cultural heritage from biodegradation is paramount, however with many sites needing funding and support, it is hard to prioritise, thus the ability to identify high risk sites is crucial to ensure resources are best placed. In doing so a clear understanding of environmental conditions acting upon a site and abundance and composition of species present is essential to this identification. Therefore, the aim of this study was to assess the rate of biodegradation on four underwater cultural heritage sites in different marine environments by placing a series of wooden test panels in direct contact with the exposed structure on the sites. Upon recovery, test panels were photographed, X-rayed, and wood boring and sessile fouling species were identified and counted. The damage attributed to each species was recorded with CAD software. Results indicated a significant difference between sites, with HMS Invincible having the highest abundance of marine wood borers and the highest rate of surface area and volume degradation; whilst vestigial evidence of marine wood borers was found on the London, it would appear the environmental conditions had significantly impeded their survival. The study indicated further factors such as sediment type and coverage, availability of wood and the proximity of other colonised sites were also determining factors controlling the abundance of marine wood borers and the rate of biodegradation. Teredinidae (Mollusca:Bivalvia), consist of 68 species (Voight, 2015), with Teredo navalis and Lyrodus pedicellatus commonly found within English waters.Both are protandrous hermaphrodites; born as males, they later transition into females, and spawn free swimming larvae into the water column (Coe, 1943; Lane, 1959; Appelqvist and Havenhand, 2016).T. navalis remains in the water column for 17–34 days before settlement, and are capable of travelling large distances and inhabiting a range of sites (Appelqvist et al., 2015; Lippert et al., 2017).L. pedicellatus retains their eggs until they are further along in their developmental stage (Wurzinger Mayer et al., 2014); due to this, L. pedicellatus remain in the water column for 24–48 h, making them more likely to recolonise the same site compared to T. navalis (Borges et al., 2010).This method aids in minimising larval mortality and the risk of being carried to unfavourable environments.After settlement, Teredinidae larvae metamorphose, and the development of a shell covered in serrated teeth enables them to bore, and tunnel formation begins (Quayle, 1992; Lippert et al., 2017).Within these tunnels, Teredinidae grow and consume wood for the duration of their lives, however, survival and growth of Teredinidae is linked to the availability of wood, thus rapid settlement and growth are vital to survival (Cragg et al., 2009; Macintosh et al., 2014) but detrimental to the wood they live within.
The need to protect underwater cultural heritage from biodegradation is paramount, however with many sites needing funding and support, it is hard to prioritise, thus the ability to identify high risk sites is crucial to ensure resources are best placed. In doing so a clear understanding of environmental conditions acting upon a site and abundance and composition of species present is essential to this identification. Therefore, the aim of this study was to assess the rate of biodegradation on four underwater cultural heritage sites in different marine environments by placing a series of wooden test panels in direct contact with the exposed structure on the sites. Upon recovery, test panels were photographed, X-rayed, and wood boring and sessile fouling species were identified and counted. The damage attributed to each species was recorded with CAD software. Results indicated a significant difference between sites, with HMS Invincible having the highest abundance of marine wood borers and the highest rate of surface area and volume degradation; whilst vestigial evidence of marine wood borers was found on the London, it would appear the environmental conditions had significantly impeded their survival. The study indicated further factors such as sediment type and coverage, availability of wood and the proximity of other colonised sites were also determining factors controlling the abundance of marine wood borers and the rate of biodegradation. Limnoria (Crustacea: Isopoda) have a worldwide distribution with L.lignorum, L.quadripunctata and L.tripunctata abundant in English waters (Jones, 1963).Limnoria give birth to a small number of live young (Eltringham and Hockley, 1961; Quayle, 1992), providing extended parental care, and enabling a low mortality rate (Mohr, 1959; Thiel, 2003).The young rapidly mature and begin to bore secondary burrows away from the maternal burrow, forming interconnecting burrows under the surface of the wood (Mohr, 1959; Thiel, 2003).Due to their limited swimming capabilities Limnoria can only migrate over short distances, and as a result develop large, rapidly growing aggregations that aggressively attack wood until depletion (Mohr, 1959, Thiel, 2003).The burrows destroy the wood's original surface area, and as weakened wood breaks away, Limnoria burrow deeper, eventually depleting the wood volume.
The need to protect underwater cultural heritage from biodegradation is paramount, however with many sites needing funding and support, it is hard to prioritise, thus the ability to identify high risk sites is crucial to ensure resources are best placed. In doing so a clear understanding of environmental conditions acting upon a site and abundance and composition of species present is essential to this identification. Therefore, the aim of this study was to assess the rate of biodegradation on four underwater cultural heritage sites in different marine environments by placing a series of wooden test panels in direct contact with the exposed structure on the sites. Upon recovery, test panels were photographed, X-rayed, and wood boring and sessile fouling species were identified and counted. The damage attributed to each species was recorded with CAD software. Results indicated a significant difference between sites, with HMS Invincible having the highest abundance of marine wood borers and the highest rate of surface area and volume degradation; whilst vestigial evidence of marine wood borers was found on the London, it would appear the environmental conditions had significantly impeded their survival. The study indicated further factors such as sediment type and coverage, availability of wood and the proximity of other colonised sites were also determining factors controlling the abundance of marine wood borers and the rate of biodegradation. The marine amphipod, Chelura terebans, is found in association with Limnoria and has a preference for pre-softened wood, in which they bore to enlarge Limnoria burrows (Poore et al., 2002; Green Etxabe, 2013).Independently, C.terebans is not a major concern, mainly because they rely on wood for shelter rather than nutrition (Green Etxabe, 2013); however the concerning factor is C. terebans' ability to worsen the damage made by Limnoria.
The need to protect underwater cultural heritage from biodegradation is paramount, however with many sites needing funding and support, it is hard to prioritise, thus the ability to identify high risk sites is crucial to ensure resources are best placed. In doing so a clear understanding of environmental conditions acting upon a site and abundance and composition of species present is essential to this identification. Therefore, the aim of this study was to assess the rate of biodegradation on four underwater cultural heritage sites in different marine environments by placing a series of wooden test panels in direct contact with the exposed structure on the sites. Upon recovery, test panels were photographed, X-rayed, and wood boring and sessile fouling species were identified and counted. The damage attributed to each species was recorded with CAD software. Results indicated a significant difference between sites, with HMS Invincible having the highest abundance of marine wood borers and the highest rate of surface area and volume degradation; whilst vestigial evidence of marine wood borers was found on the London, it would appear the environmental conditions had significantly impeded their survival. The study indicated further factors such as sediment type and coverage, availability of wood and the proximity of other colonised sites were also determining factors controlling the abundance of marine wood borers and the rate of biodegradation. Teredinidae, Limnoria and C. terebans are affected by differences in the marine environment, thus varying environmental conditions can affect the rate of biodegradation (Florian et al., 1977).Temperature and salinity are dominant factors affecting distribution, growth and reproduction of marine wood borers (Eltringham and Hockley, 1961; Nair and Saraswathy, 1971; Appelqvist et al., 2015).T. navalis has a broad tolerance to temperature and salinity, allowing a long spawning season in English waters and the ability to settle in diverse habitats; whilst L. pedicellatus shares similar temperature tolerances, their salinity tolerance limits them to marine sites (Table 1), (Nair and Saraswathy, 1971; Borges et al., 2010; Paalvast and Van der Velde, 2011; Borges, 2014; Borges et al., 2014a; Appelqvist et al., 2015; Appelqvist and Havenhand, 2016; Lippert et al., 2017; Fofonoff et al., 2018a).L.lignorum are suited to salinities in brackish and marine waters, and a preference for cooler temperatures allows them to reproduce throughout most of the year, with a wide distribution across the UK; L.quadripunctata and L.tripunctata are restricted to warmer marine waters, and thus limited to the south of England (Table 1), (Jones, 1963; Borges et al., 2010; Borges et al., 2014b; Fofonoff et al., 2018b).Temperature preferences may also affect depth distribution; Limnoria are abundant in shallow waters, living in the intertidal range to 30 m (Cookson, 1991; Shalaeva, 2012), however, the depth distribution for C. terebans is unknown, but it is active in warmer shallow waters down to at least 12 m (unpublished results).Similarly, T. navalis has a depth range down to 150 m but is more abundant and destructive in shallower waters down to 20 m (Bernard et al., 1993; Elam, 2009).Thus, most shallow sites lying within optimum temperature ranges, such as the majority of wooden protected wreck sites along the south coast of England (Historic England, 2018), are at risk of biodegradation.
The need to protect underwater cultural heritage from biodegradation is paramount, however with many sites needing funding and support, it is hard to prioritise, thus the ability to identify high risk sites is crucial to ensure resources are best placed. In doing so a clear understanding of environmental conditions acting upon a site and abundance and composition of species present is essential to this identification. Therefore, the aim of this study was to assess the rate of biodegradation on four underwater cultural heritage sites in different marine environments by placing a series of wooden test panels in direct contact with the exposed structure on the sites. Upon recovery, test panels were photographed, X-rayed, and wood boring and sessile fouling species were identified and counted. The damage attributed to each species was recorded with CAD software. Results indicated a significant difference between sites, with HMS Invincible having the highest abundance of marine wood borers and the highest rate of surface area and volume degradation; whilst vestigial evidence of marine wood borers was found on the London, it would appear the environmental conditions had significantly impeded their survival. The study indicated further factors such as sediment type and coverage, availability of wood and the proximity of other colonised sites were also determining factors controlling the abundance of marine wood borers and the rate of biodegradation. Dissolved oxygen is a dominant factor controlling the distribution and survival of marine wood borers (Menzies et al., 1963; Anderson and Reish, 1967; Eriksen et al., 2014), and should be considered alongside sediment type and coverage, which can create anoxic conditions and affect larval settlement; hence, studies have successfully trialled various reburial methods of wooden UCH, aiming to produce anoxic conditions for in situ preservation (Gregory, 1998; Palma, 2005; Curci, 2006; Manders, 2006; Björdal and Nilsson, 2008; Palma and Parham, 2009; Eriksen et al., 2014).Other factors for consideration are the availability of wood and competition for space, particularly with sessile fouling species (Weiss, 1948; Quayle, 1992), water movements, which although may aid larval settlement and burrow oxygenation, can also have opposing effects (Doochin and Smith, 1951), and the wood type and direction of cut (Eriksen et al., 2016).Marine wood borers have a preference to non-durable wood (Sivrikaya et al., 2009), but studies have shown Teredinidae prefer wood cut on the radial plane, which is common on clinker built boats such as the Skuldelev ships, Bremen cog and Grace Deiu, where timber was radially split for construction (Crumlin-Pedersen, 1984; Litwin, 1998; Childs, 2009; Eriksen et al., 2016).
The design of a control algorithm is difficult when models are unavailable, the physics are varying in time, or structural uncertainties are involved. One such case is an oil production platform in which reservoir conditions and the composition of the multiphase flow are not precisely known. Today, with streams of data generated from sensors, black-box adaptive control emerged as an alternative to control such systems. In this work, we employed an online adaptive controller based on Echo State Networks (ESNs) in diverse scenarios of controlling an oil production platform. The ESN learns an inverse model of the plant from which a control law is derived to attain set-point tracking of a simulated model. The analysis considers high steady-state gains, potentially unstable conditions, and a multi-variate control structure. All in all, this work contributes to the literature by demonstrating that online-learning control can be effective in highly complex dynamic systems (oil production platforms) devoid of suitable models, and with multiple inputs and outputs. In spite of the recent advances in renewable energies, oil and gas remain the most important energy sources in the world.As oil reservoirs are drained with time, more sophisticated technologies are required for extraction, to a great extent due to the loss of reservoir pressure and the accumulation of impurities, such as wax, gas hydrates, and asphaltene to name a few.These hurdles led to the concept of “flow assurance”, initially coined by Petrobras in the 90’s, which is concerned with the development and applications of technologies that ensure a stable and economically desirable production, under a wide range of operating conditions (Jahanshahi, 2013).
The design of a control algorithm is difficult when models are unavailable, the physics are varying in time, or structural uncertainties are involved. One such case is an oil production platform in which reservoir conditions and the composition of the multiphase flow are not precisely known. Today, with streams of data generated from sensors, black-box adaptive control emerged as an alternative to control such systems. In this work, we employed an online adaptive controller based on Echo State Networks (ESNs) in diverse scenarios of controlling an oil production platform. The ESN learns an inverse model of the plant from which a control law is derived to attain set-point tracking of a simulated model. The analysis considers high steady-state gains, potentially unstable conditions, and a multi-variate control structure. All in all, this work contributes to the literature by demonstrating that online-learning control can be effective in highly complex dynamic systems (oil production platforms) devoid of suitable models, and with multiple inputs and outputs. One of the main flow assurance problems is the slugging flow in wells and risers, a phenomenon associated with the high oscillations that arise from the speed difference between the oil and gas flows.Most solutions found in the literature rely on models to design feedback control strategies, such as Jahanshahi (2013), de Oliveira et al. (2015), Campos et al. (2015), and Stasiak et al. (2012).Usually, the feedback control solutions to anti-slugging problems are either model-based or tuned based on a simplified model, which tends to lead to robustness issues due to either parametric or structural model mismatch (such as assuming the incorrect flow composition) (Jahanshahi, 2013).Rather than model-based approaches, this work turns to black-box data-driven approaches that continuously train online, using Recurrent Neural Networks (RNNs) to control a nonlinear plant.We can view the RNN as embedded in the controller, simultaneously learning the inverse model and controlling the plant.Throughout this paper, we will interchangeably use the terms controller and (recurrent neural) networks with the same meaning: an RNN-based controller.
The design of a control algorithm is difficult when models are unavailable, the physics are varying in time, or structural uncertainties are involved. One such case is an oil production platform in which reservoir conditions and the composition of the multiphase flow are not precisely known. Today, with streams of data generated from sensors, black-box adaptive control emerged as an alternative to control such systems. In this work, we employed an online adaptive controller based on Echo State Networks (ESNs) in diverse scenarios of controlling an oil production platform. The ESN learns an inverse model of the plant from which a control law is derived to attain set-point tracking of a simulated model. The analysis considers high steady-state gains, potentially unstable conditions, and a multi-variate control structure. All in all, this work contributes to the literature by demonstrating that online-learning control can be effective in highly complex dynamic systems (oil production platforms) devoid of suitable models, and with multiple inputs and outputs. As the online learning RNN derives the control law, the controller can learn and adapt its parameters directly from streaming data in an online way, which renders it suitable to deal with unmodeled phenomena, for which limited knowledge of the structure and parameters is available (Nelles, 2001).However, a black-box approach can only generalize the phenomenology and physics in the region within which data were provided to fit the models (i.e., they are poor in extrapolating their predictions to regions outside the training input space), being a drawback when compared to phenomenological approaches.This is an issue specially in offline learning, where periodic retraining of the model would be necessary for learning new operating points and dynamical behaviors.In this case, a phenomenological model (if available) should be used specially if computational power is abundant and time constraints are not tight.On the other hand, online learning allows us to continuously learn from data, updating the model in real time to the most recent samples.The approach taken in this work follows that line of thought, whereby the controller endlessly adapts over time, making it capable of learning and controlling the system at any operating region, with no prior knowledge of the model.
The design of a control algorithm is difficult when models are unavailable, the physics are varying in time, or structural uncertainties are involved. One such case is an oil production platform in which reservoir conditions and the composition of the multiphase flow are not precisely known. Today, with streams of data generated from sensors, black-box adaptive control emerged as an alternative to control such systems. In this work, we employed an online adaptive controller based on Echo State Networks (ESNs) in diverse scenarios of controlling an oil production platform. The ESN learns an inverse model of the plant from which a control law is derived to attain set-point tracking of a simulated model. The analysis considers high steady-state gains, potentially unstable conditions, and a multi-variate control structure. All in all, this work contributes to the literature by demonstrating that online-learning control can be effective in highly complex dynamic systems (oil production platforms) devoid of suitable models, and with multiple inputs and outputs. There are several examples of neural adaptive controllers in the literature.For instance, Lungu and Lungu (2018) adds a feed-forward neural network (NN) to a model-reference adaptive framework for airplane landing control.The Model-reference controller essentially forces the plant to behave as dictated by a reference model.This structure is linear by nature, so Lungu and Lungu (2018) proposed adding a feedback linearization based approach, by canceling out the non-linearities with their inverse model identified by the NN.This methodology is gray-box, because it still uses information from the linearized system to design the controller.Besides, since their work uses a feed-forward static neural network, the dynamics must be realized at the input layer through the use of lagged inputs, increasing input dimensionality by many folds.Also, as training uses backpropagation, problems such as local optima, numerical instabilities, and slow convergence can arise.
The design of a control algorithm is difficult when models are unavailable, the physics are varying in time, or structural uncertainties are involved. One such case is an oil production platform in which reservoir conditions and the composition of the multiphase flow are not precisely known. Today, with streams of data generated from sensors, black-box adaptive control emerged as an alternative to control such systems. In this work, we employed an online adaptive controller based on Echo State Networks (ESNs) in diverse scenarios of controlling an oil production platform. The ESN learns an inverse model of the plant from which a control law is derived to attain set-point tracking of a simulated model. The analysis considers high steady-state gains, potentially unstable conditions, and a multi-variate control structure. All in all, this work contributes to the literature by demonstrating that online-learning control can be effective in highly complex dynamic systems (oil production platforms) devoid of suitable models, and with multiple inputs and outputs. Although recent advances in deep learning have made possible the effective training of special recurrent networks such as the LSTMs (Long Short Term Memory) for large training sets (Graves et al., 2013), in general RNNs are still hard to train, since Backpropagation through time (Werbos, 1990) is susceptible to local minima, slow convergence, bifurcation, and other instability related problems.One example of alternative ways to overcome these learning problems in nonlinear dynamic system identification is by Münker and Nelles (2018), who propose a network of local (linear) Finite Impulse Response (FIR) models where each local model is trained separately with the help of a special regularization matrix.The idea is that the network connection between multiple linear dynamic systems leads to a model capable of capturing nonlinear dynamics.
The design of a control algorithm is difficult when models are unavailable, the physics are varying in time, or structural uncertainties are involved. One such case is an oil production platform in which reservoir conditions and the composition of the multiphase flow are not precisely known. Today, with streams of data generated from sensors, black-box adaptive control emerged as an alternative to control such systems. In this work, we employed an online adaptive controller based on Echo State Networks (ESNs) in diverse scenarios of controlling an oil production platform. The ESN learns an inverse model of the plant from which a control law is derived to attain set-point tracking of a simulated model. The analysis considers high steady-state gains, potentially unstable conditions, and a multi-variate control structure. All in all, this work contributes to the literature by demonstrating that online-learning control can be effective in highly complex dynamic systems (oil production platforms) devoid of suitable models, and with multiple inputs and outputs. In our work, to circumvent the aforementioned learning problems, the RNN used in our controller is trained according to the Reservoir Computing (RC) paradigm (Verstraeten et al., 2007).In particular, we employ Echo State Networks (ESN) (Jaeger and Haas, 2004; Jaeger, 2001), one of the flavors of RC, as a substrate for RNN-based controller learning.Liquid State Machines (LSM), originally from the computational neuroscience field, are another flavor of RC which use spiking neural networks as reservoirs instead of analog reservoirs as in ESNs (Maass et al., 2002) (the latter being easier to implement).RC (or ESN) networks are composed of two main parts: a dynamical non-linear state equation system which represents the recurrent part called reservoir, whose weights are randomly fixed and not trained, thus avoiding non-linear terms and dependence of time in the loss function; and a linear readout output layer, the sole part to be trained.Usually, the readout training in the RC approach is done through linear Least Squares or Ridge Regression, which has global convergence properties in contrast to LSTMs which use gradient-based learning on a nonlinear cost function, requiring orders of magnitude of more time to train than RC networks, for instance.ESNs have been successfully applied to a wide range of problems in the literature, such as learning complex goal-directed robot behaviors (Antonelo and Schrauwen, 2015), grammatical structure processing (Hinaut and Dominey, 2012), short-term stock prediction (technical analysis) (Lin et al., 2009), predictive control (Pan and Wang, 2012; Xiang et al., 2016), and noninvasive fetal detection (Lukoševičius and Marozas, 2014).Moreover, ESNs have shown promising results in identifying the complex dynamics involving a slugging flow riser (Antonelo et al., 2017), which is considered a difficult task in system identification.Chen et al. (2017a) present a tutorial on how to apply several types of neural networks, including Echo State Networks, into applications related to wireless communication, such as: using Unmanned Aerial Vehicles (UAV) as service providers; Virtual Reality with wireless networks; Content request prediction for mobile edge catching and computing; and even Internet of Things applications.Chen et al. (2017b) apply an enhanced version of Echo State Network into the context of cloud radio access networks, as a user behavior predictor.Chen et al. (2018) use ESNs in the context of virtual reality, using wireless networks to predict the quality of service of small base stations (information receivers and transmitters in the virtual reality framework) for the purpose of solving a resource allocation problem.Another successful use for Echo State Networks is the non-linear predictive control of a gas-lifted oil well with no prior model information (Jordanou et al., 2018).
The design of a control algorithm is difficult when models are unavailable, the physics are varying in time, or structural uncertainties are involved. One such case is an oil production platform in which reservoir conditions and the composition of the multiphase flow are not precisely known. Today, with streams of data generated from sensors, black-box adaptive control emerged as an alternative to control such systems. In this work, we employed an online adaptive controller based on Echo State Networks (ESNs) in diverse scenarios of controlling an oil production platform. The ESN learns an inverse model of the plant from which a control law is derived to attain set-point tracking of a simulated model. The analysis considers high steady-state gains, potentially unstable conditions, and a multi-variate control structure. All in all, this work contributes to the literature by demonstrating that online-learning control can be effective in highly complex dynamic systems (oil production platforms) devoid of suitable models, and with multiple inputs and outputs. The ESN-based controller used in this work was first proposed by Waegeman et al. (2012), being referred herein also as the inverse model on-line learning controller.An Echo State Network implements the inverse model of the plant to be controlled, which is trained by the Recursive Least Squares (RLS) algorithm.This controller has been applied in problems such as: the heat tank temperature control assuming variable delay, airplane pitch angle control for steady cruise and balancing of an inverted pendulum system by Waegeman et al. (2012); position control of a real-world hydraulic excavator by Park et al. (2014); and robotic arm control by Waegeman et al. (2013).Bo and Zhang (2018) proposed another on-line learning control strategy that uses Echo State Networks.Based on an actor-critic framework, they derive an optimal control policy from the Bellman optimality equation to regulate the dissolved oxygen concentration in a wastewater treatment.Their controller is composed of four ESNs: two for modeling critics (as an approximation of the cost function to be minimized); one for modeling the plant (the forward model); and one for the actor, obtained analytically by deriving the steepest descent from the gradient of the model and critic networks.The ESNs are trained by a method based on the Recursive Least Squares.On the other hand, the controller employed in our work consists of only two ESNs, one that learns from past observed behavior in an online fashion (with RLS), and another that actually controls the plant.Knowledge is transferred in real time at every instant from the learning ESN to the control ESN.Compared to Bo and Zhang (2018), our framework is simpler and less computationally expensive, which can be readily applied to MIMO systems as shown in the experiments hereafter.The online learning control strategy used in this work is purely regulatory, while Bo and Zhang (2018) minimize an expected cost function to induce an optimal control policy in a reinforcement learning way.
The design of a control algorithm is difficult when models are unavailable, the physics are varying in time, or structural uncertainties are involved. One such case is an oil production platform in which reservoir conditions and the composition of the multiphase flow are not precisely known. Today, with streams of data generated from sensors, black-box adaptive control emerged as an alternative to control such systems. In this work, we employed an online adaptive controller based on Echo State Networks (ESNs) in diverse scenarios of controlling an oil production platform. The ESN learns an inverse model of the plant from which a control law is derived to attain set-point tracking of a simulated model. The analysis considers high steady-state gains, potentially unstable conditions, and a multi-variate control structure. All in all, this work contributes to the literature by demonstrating that online-learning control can be effective in highly complex dynamic systems (oil production platforms) devoid of suitable models, and with multiple inputs and outputs. In oil and gas, our previous work (Jordanou et al., 2017) employed the online learning controller in order to control a gas-lifted oil well model, successfully realizing set-point tracking and disturbance rejection tasks.The current work can be considered as a comprehensive extension of Jordanou et al. (2017), where now:
The design of a control algorithm is difficult when models are unavailable, the physics are varying in time, or structural uncertainties are involved. One such case is an oil production platform in which reservoir conditions and the composition of the multiphase flow are not precisely known. Today, with streams of data generated from sensors, black-box adaptive control emerged as an alternative to control such systems. In this work, we employed an online adaptive controller based on Echo State Networks (ESNs) in diverse scenarios of controlling an oil production platform. The ESN learns an inverse model of the plant from which a control law is derived to attain set-point tracking of a simulated model. The analysis considers high steady-state gains, potentially unstable conditions, and a multi-variate control structure. All in all, this work contributes to the literature by demonstrating that online-learning control can be effective in highly complex dynamic systems (oil production platforms) devoid of suitable models, and with multiple inputs and outputs. Finally, this work demonstrates that black-box strategies can be effective to the control of processes in oil production platforms, which is relevant for flow assurance as discussed above.
The technologies developed so far to help visually impaired people (VIP) navigate meet only some of their everyday needs. This project allows the visually impaired to improve the comprehension of their context by generating a risk map following an analysis of the position, distance, size and motion of the objects present in their environment. This comprehension is refined by data fusion steps applied to the High Level Information Fusion (HLIF) to predict possible impacts in the near future. A risk map is made up of probabilities generated after executing a set of inferences. These inferences allow the evaluation of future collision risks in different directions by detecting static objects, detecting free passage and analyzing paths followed by dynamic objects in a 3D plane. Different datasets were modeled and a comparative analysis was performed to check the percentage of correct answers and the accuracy of the inferences made using different classifiers. Thus, in order to demonstrate the advantages of the HLIF implementation in a dedicated VIP navigation system, the proposed architecture was tested against three other navigation systems that use different approaches. The generation of specific results made it possible to validate and compare these navigation systems. For this comparative analysis, different environments were used with the goal of indicating a direction for the VIP to move in with fewer collision risks. In addition to providing a risk map giving possible collisions, this project system provided greater reliability for navigation, especially when obstacles were very close and moving objects were detected and tracked. The research and consequent development of sensory support and navigation technologies for visually impaired people (VIP) is increasing (Chan et al., 2017; Mekhalfi et al., 2016; Tsirmpas et al., 2015; Mascetti et al., 2016; Bourbakis et al., 2013; Aladren et al., 2014; Xiao et al., 2015).Although it is a specific area of research, solving the problems and difficulties faced by these people requires comprehensive analysis, and should include several different difficulties that a VIP has to deal with where they move.
The technologies developed so far to help visually impaired people (VIP) navigate meet only some of their everyday needs. This project allows the visually impaired to improve the comprehension of their context by generating a risk map following an analysis of the position, distance, size and motion of the objects present in their environment. This comprehension is refined by data fusion steps applied to the High Level Information Fusion (HLIF) to predict possible impacts in the near future. A risk map is made up of probabilities generated after executing a set of inferences. These inferences allow the evaluation of future collision risks in different directions by detecting static objects, detecting free passage and analyzing paths followed by dynamic objects in a 3D plane. Different datasets were modeled and a comparative analysis was performed to check the percentage of correct answers and the accuracy of the inferences made using different classifiers. Thus, in order to demonstrate the advantages of the HLIF implementation in a dedicated VIP navigation system, the proposed architecture was tested against three other navigation systems that use different approaches. The generation of specific results made it possible to validate and compare these navigation systems. For this comparative analysis, different environments were used with the goal of indicating a direction for the VIP to move in with fewer collision risks. In addition to providing a risk map giving possible collisions, this project system provided greater reliability for navigation, especially when obstacles were very close and moving objects were detected and tracked. This Sensory Analysis System For Visually Impaired People (SAS-VIP) introduces a new data fusion architecture based on contexts similar to those established by a person with vision to support VIP decision making.According to Zhu et al. (2014), a context refers to current values and specific data that provide the user with an activity or situation.For Anagnostopoulos and Hadjiefthymiades (2009), Context Awareness (CA) is the ability of the system to perceive, interpret and react to changes that occur in the environment in which the user is present.In an environment there may be a set of contexts.A context can be formed through the relationship that entities have with the goal that a user must achieve.An entity can be an object, a person, or an area (Zhu et al., 2014).If any entity is considered relevant to VIP traffic without collisions, it can be chosen to form a context.
The technologies developed so far to help visually impaired people (VIP) navigate meet only some of their everyday needs. This project allows the visually impaired to improve the comprehension of their context by generating a risk map following an analysis of the position, distance, size and motion of the objects present in their environment. This comprehension is refined by data fusion steps applied to the High Level Information Fusion (HLIF) to predict possible impacts in the near future. A risk map is made up of probabilities generated after executing a set of inferences. These inferences allow the evaluation of future collision risks in different directions by detecting static objects, detecting free passage and analyzing paths followed by dynamic objects in a 3D plane. Different datasets were modeled and a comparative analysis was performed to check the percentage of correct answers and the accuracy of the inferences made using different classifiers. Thus, in order to demonstrate the advantages of the HLIF implementation in a dedicated VIP navigation system, the proposed architecture was tested against three other navigation systems that use different approaches. The generation of specific results made it possible to validate and compare these navigation systems. For this comparative analysis, different environments were used with the goal of indicating a direction for the VIP to move in with fewer collision risks. In addition to providing a risk map giving possible collisions, this project system provided greater reliability for navigation, especially when obstacles were very close and moving objects were detected and tracked. Moving in unfamiliar environments that contain obstacles and moving objects is one of the main difficulties that VIP have to deal with.A variety of applications have been produced (Pham et al., 2016; López-de Ipiña et al., 2011; Mekhalfi et al., 2015; Bourbakis et al., 2013; Tian et al., 2014; Tamjidi et al., 2013; Ando et al., 2011) to provide directions alternatives for VIP.So far, several directions alternatives have been developed for VIP, only in specific areas.Unlike these proposals, this project prioritizes use in any area in order to provide the VIP with a safe, collision-free path, with both dynamic and static objects.Fig. 1 presents an example of the environment used for the experiments performed in this project, in which the VIP moves with the presence of obstacles and other people in movement.
The technologies developed so far to help visually impaired people (VIP) navigate meet only some of their everyday needs. This project allows the visually impaired to improve the comprehension of their context by generating a risk map following an analysis of the position, distance, size and motion of the objects present in their environment. This comprehension is refined by data fusion steps applied to the High Level Information Fusion (HLIF) to predict possible impacts in the near future. A risk map is made up of probabilities generated after executing a set of inferences. These inferences allow the evaluation of future collision risks in different directions by detecting static objects, detecting free passage and analyzing paths followed by dynamic objects in a 3D plane. Different datasets were modeled and a comparative analysis was performed to check the percentage of correct answers and the accuracy of the inferences made using different classifiers. Thus, in order to demonstrate the advantages of the HLIF implementation in a dedicated VIP navigation system, the proposed architecture was tested against three other navigation systems that use different approaches. The generation of specific results made it possible to validate and compare these navigation systems. For this comparative analysis, different environments were used with the goal of indicating a direction for the VIP to move in with fewer collision risks. In addition to providing a risk map giving possible collisions, this project system provided greater reliability for navigation, especially when obstacles were very close and moving objects were detected and tracked. VIP need technologies that go beyond simply indicating the desired destination.They should detect systematic patterns, contextualize the elements of the environment and indicate what action should be taken to ensure their safety.Based on these prerequisites, this project presents a new data fusion architecture designed to provide the VIP with the perception of entities in a given environment for later comprehension of the contexts.Many projects use CA to classify contexts that are highly dynamic (Tang et al., 2013; Zhu et al., 2014; Lee and Lee, 2014; Anagnostopoulos and Hadjiefthymiades, 2009; Cordeiro et al., 2016).In this project, the CA theory is applied to the system to understand the relationship between the static objects, the trajectory followed by the dynamic objects and the direction of the free passage in the environment in which the VIP is present.
The technologies developed so far to help visually impaired people (VIP) navigate meet only some of their everyday needs. This project allows the visually impaired to improve the comprehension of their context by generating a risk map following an analysis of the position, distance, size and motion of the objects present in their environment. This comprehension is refined by data fusion steps applied to the High Level Information Fusion (HLIF) to predict possible impacts in the near future. A risk map is made up of probabilities generated after executing a set of inferences. These inferences allow the evaluation of future collision risks in different directions by detecting static objects, detecting free passage and analyzing paths followed by dynamic objects in a 3D plane. Different datasets were modeled and a comparative analysis was performed to check the percentage of correct answers and the accuracy of the inferences made using different classifiers. Thus, in order to demonstrate the advantages of the HLIF implementation in a dedicated VIP navigation system, the proposed architecture was tested against three other navigation systems that use different approaches. The generation of specific results made it possible to validate and compare these navigation systems. For this comparative analysis, different environments were used with the goal of indicating a direction for the VIP to move in with fewer collision risks. In addition to providing a risk map giving possible collisions, this project system provided greater reliability for navigation, especially when obstacles were very close and moving objects were detected and tracked. The data fusion system developed was based on the concepts of CA and the Salerno Model for high level Information fusion (HLIF).According to Liggins et al. (2008) this model incorporates concepts from the Joint Directors of Laboratories (JDL) model and the Situation Awareness (SAW) model proposed by Endsley et al. (2003).Joseph et al. (2014) suggests that SAW is formed only when the first three levels of a data fusion process are performed, in which the first level is perception, the second comprehension and the third projection.Alkhanifer and Ludi (2014) apply the SAW model because it is a useful approach to any type of decision support system and also because it enables information that is easy to understand from the context, thus providing more confidence in the actions that will be taken.
The technologies developed so far to help visually impaired people (VIP) navigate meet only some of their everyday needs. This project allows the visually impaired to improve the comprehension of their context by generating a risk map following an analysis of the position, distance, size and motion of the objects present in their environment. This comprehension is refined by data fusion steps applied to the High Level Information Fusion (HLIF) to predict possible impacts in the near future. A risk map is made up of probabilities generated after executing a set of inferences. These inferences allow the evaluation of future collision risks in different directions by detecting static objects, detecting free passage and analyzing paths followed by dynamic objects in a 3D plane. Different datasets were modeled and a comparative analysis was performed to check the percentage of correct answers and the accuracy of the inferences made using different classifiers. Thus, in order to demonstrate the advantages of the HLIF implementation in a dedicated VIP navigation system, the proposed architecture was tested against three other navigation systems that use different approaches. The generation of specific results made it possible to validate and compare these navigation systems. For this comparative analysis, different environments were used with the goal of indicating a direction for the VIP to move in with fewer collision risks. In addition to providing a risk map giving possible collisions, this project system provided greater reliability for navigation, especially when obstacles were very close and moving objects were detected and tracked. In SAS-VIP, the perception level can be described as being the information captured from the environment in which the VIP moves, such as, for example, the detection of moving and stationary objects, including their positions and sizes, the detection of free spaces, etc.The comprehension level defines the relationship between the elements mentioned and the meaning of their actions.The projection level allows for predicting consequences in the near future based on the relationships and actions of objects in the environment, giving warning that the person might possibly collide with an object.
The technologies developed so far to help visually impaired people (VIP) navigate meet only some of their everyday needs. This project allows the visually impaired to improve the comprehension of their context by generating a risk map following an analysis of the position, distance, size and motion of the objects present in their environment. This comprehension is refined by data fusion steps applied to the High Level Information Fusion (HLIF) to predict possible impacts in the near future. A risk map is made up of probabilities generated after executing a set of inferences. These inferences allow the evaluation of future collision risks in different directions by detecting static objects, detecting free passage and analyzing paths followed by dynamic objects in a 3D plane. Different datasets were modeled and a comparative analysis was performed to check the percentage of correct answers and the accuracy of the inferences made using different classifiers. Thus, in order to demonstrate the advantages of the HLIF implementation in a dedicated VIP navigation system, the proposed architecture was tested against three other navigation systems that use different approaches. The generation of specific results made it possible to validate and compare these navigation systems. For this comparative analysis, different environments were used with the goal of indicating a direction for the VIP to move in with fewer collision risks. In addition to providing a risk map giving possible collisions, this project system provided greater reliability for navigation, especially when obstacles were very close and moving objects were detected and tracked. Several technological solutions have been presented with the aim of helping a VIP discover their position, which elements are in their path and which is the safest place to move.In most of these methodologies, different types of sensors have been used, such as those that define distance, presence, movement and color (Chan et al., 2017; Mascetti et al., 2016; Jabnoun et al., 2014; Pei and Wang, 2011; Bourbakis et al., 2013; Tian et al., 2014).However, few data fusion systems integrate all this information and generate decision making based on the SAW projection level.Typically, these sensors are used as data sources, however, systems do not have progressions to more refined fusion phases for the purpose of correcting errors, removing redundancies and generating decisions that a human being can trust.
The technologies developed so far to help visually impaired people (VIP) navigate meet only some of their everyday needs. This project allows the visually impaired to improve the comprehension of their context by generating a risk map following an analysis of the position, distance, size and motion of the objects present in their environment. This comprehension is refined by data fusion steps applied to the High Level Information Fusion (HLIF) to predict possible impacts in the near future. A risk map is made up of probabilities generated after executing a set of inferences. These inferences allow the evaluation of future collision risks in different directions by detecting static objects, detecting free passage and analyzing paths followed by dynamic objects in a 3D plane. Different datasets were modeled and a comparative analysis was performed to check the percentage of correct answers and the accuracy of the inferences made using different classifiers. Thus, in order to demonstrate the advantages of the HLIF implementation in a dedicated VIP navigation system, the proposed architecture was tested against three other navigation systems that use different approaches. The generation of specific results made it possible to validate and compare these navigation systems. For this comparative analysis, different environments were used with the goal of indicating a direction for the VIP to move in with fewer collision risks. In addition to providing a risk map giving possible collisions, this project system provided greater reliability for navigation, especially when obstacles were very close and moving objects were detected and tracked. In SAS-VIP, data fusion is applied as a key decision-making technique, which is intended to reach higher fusion levels to predict possible collisions in the near future.The projection of collisions is obtained after the construction of a risk map, made up of inferences.Inferences are performed by means of learning models, built based on the extraction of system characteristics.It is important to note that for the generation of this map, the mapping of the static objects, the free passage and the paths followed by dynamic objects in a 3D plane is carried out.This map allows you to project collisions in different directions.Another feature little explored in this kind of study is the use of feedback to the decision making system.For SAS-VIP, this feedback is very useful for adjusting probabilities in the course of its use and providing a more reliable risk map.
The technologies developed so far to help visually impaired people (VIP) navigate meet only some of their everyday needs. This project allows the visually impaired to improve the comprehension of their context by generating a risk map following an analysis of the position, distance, size and motion of the objects present in their environment. This comprehension is refined by data fusion steps applied to the High Level Information Fusion (HLIF) to predict possible impacts in the near future. A risk map is made up of probabilities generated after executing a set of inferences. These inferences allow the evaluation of future collision risks in different directions by detecting static objects, detecting free passage and analyzing paths followed by dynamic objects in a 3D plane. Different datasets were modeled and a comparative analysis was performed to check the percentage of correct answers and the accuracy of the inferences made using different classifiers. Thus, in order to demonstrate the advantages of the HLIF implementation in a dedicated VIP navigation system, the proposed architecture was tested against three other navigation systems that use different approaches. The generation of specific results made it possible to validate and compare these navigation systems. For this comparative analysis, different environments were used with the goal of indicating a direction for the VIP to move in with fewer collision risks. In addition to providing a risk map giving possible collisions, this project system provided greater reliability for navigation, especially when obstacles were very close and moving objects were detected and tracked. HLIF has been applied in several projects but it not been applied very much in navigation support and sensory analysis systems for VIP.And its use decreases substantially when the fusion reaches the comprehension and projection levels (see Table 1).These levels are intended to explore data for consistent relationships and consequently to project impacts in the near future.In SAS-VIP, the fusion reaches the projection level with the generation of the risk map.
The technologies developed so far to help visually impaired people (VIP) navigate meet only some of their everyday needs. This project allows the visually impaired to improve the comprehension of their context by generating a risk map following an analysis of the position, distance, size and motion of the objects present in their environment. This comprehension is refined by data fusion steps applied to the High Level Information Fusion (HLIF) to predict possible impacts in the near future. A risk map is made up of probabilities generated after executing a set of inferences. These inferences allow the evaluation of future collision risks in different directions by detecting static objects, detecting free passage and analyzing paths followed by dynamic objects in a 3D plane. Different datasets were modeled and a comparative analysis was performed to check the percentage of correct answers and the accuracy of the inferences made using different classifiers. Thus, in order to demonstrate the advantages of the HLIF implementation in a dedicated VIP navigation system, the proposed architecture was tested against three other navigation systems that use different approaches. The generation of specific results made it possible to validate and compare these navigation systems. For this comparative analysis, different environments were used with the goal of indicating a direction for the VIP to move in with fewer collision risks. In addition to providing a risk map giving possible collisions, this project system provided greater reliability for navigation, especially when obstacles were very close and moving objects were detected and tracked. Table 1 presents a comparison of studies using different techniques to solve specific navigation problems.It can be seen that all the studies use sensors worn by the VIP.In five of these studies (Tsirmpas et al., 2015; Mekhalfi et al., 2016; López-de Ipiña et al., 2011; Xiao et al., 2015; Ando et al., 2011), some other equipment such as sensors or tags and code labels (such as RFID or QR-Code) are also installed in the environment.This facility requires prior planning at all locations where the VIP will be traveling.SAS-VIP was designed not to rely on technologies installed in the environment or remotely requested information.
The technologies developed so far to help visually impaired people (VIP) navigate meet only some of their everyday needs. This project allows the visually impaired to improve the comprehension of their context by generating a risk map following an analysis of the position, distance, size and motion of the objects present in their environment. This comprehension is refined by data fusion steps applied to the High Level Information Fusion (HLIF) to predict possible impacts in the near future. A risk map is made up of probabilities generated after executing a set of inferences. These inferences allow the evaluation of future collision risks in different directions by detecting static objects, detecting free passage and analyzing paths followed by dynamic objects in a 3D plane. Different datasets were modeled and a comparative analysis was performed to check the percentage of correct answers and the accuracy of the inferences made using different classifiers. Thus, in order to demonstrate the advantages of the HLIF implementation in a dedicated VIP navigation system, the proposed architecture was tested against three other navigation systems that use different approaches. The generation of specific results made it possible to validate and compare these navigation systems. For this comparative analysis, different environments were used with the goal of indicating a direction for the VIP to move in with fewer collision risks. In addition to providing a risk map giving possible collisions, this project system provided greater reliability for navigation, especially when obstacles were very close and moving objects were detected and tracked. Mapping and pattern recognition are the main features discussed in VIP navigation support systems, presented both in the state of the art and here in Table 1.In this table, there are another two items that are less exploited but which improve the perception and navigation of the VIP.These items are 2D to 3D conversion and dynamic object analysis.In addition to the SAS-VIP, only 4 studies (Tsirmpas et al., 2015; Xiao et al., 2015; Brilhault et al., 2011; Ando et al., 2011) approach 2D to 3D conversion as essential for safe navigation.The 2D plane does not provide actual measurements because of the approximations that occur in converting global coordinates to image coordinates.This may affect the VIP navigation.The other little discussed feature in this type of system is the analysis of dynamic objects.It can alert potential risks of collision with the VIP as well as provide useful information for navigation.It is important to note that a route is presented as a route that can be executed by a VIP with a lower risk of collision.
The technologies developed so far to help visually impaired people (VIP) navigate meet only some of their everyday needs. This project allows the visually impaired to improve the comprehension of their context by generating a risk map following an analysis of the position, distance, size and motion of the objects present in their environment. This comprehension is refined by data fusion steps applied to the High Level Information Fusion (HLIF) to predict possible impacts in the near future. A risk map is made up of probabilities generated after executing a set of inferences. These inferences allow the evaluation of future collision risks in different directions by detecting static objects, detecting free passage and analyzing paths followed by dynamic objects in a 3D plane. Different datasets were modeled and a comparative analysis was performed to check the percentage of correct answers and the accuracy of the inferences made using different classifiers. Thus, in order to demonstrate the advantages of the HLIF implementation in a dedicated VIP navigation system, the proposed architecture was tested against three other navigation systems that use different approaches. The generation of specific results made it possible to validate and compare these navigation systems. For this comparative analysis, different environments were used with the goal of indicating a direction for the VIP to move in with fewer collision risks. In addition to providing a risk map giving possible collisions, this project system provided greater reliability for navigation, especially when obstacles were very close and moving objects were detected and tracked. Systems developed for VIP that use cameras generally rely on pattern recognition techniques to generate some kind of comprehension of the environment.It can be seen from Table 1 that almost all projects use some kind of object recognition technique.This type of system requires prior training, and it is difficult to recognize all the objects of a given class.Thus, the system becomes dependent on extensive training to ensure the classification of the desired objects, because if they are not classified, systems may fail to generate information necessary for decision making.
The technologies developed so far to help visually impaired people (VIP) navigate meet only some of their everyday needs. This project allows the visually impaired to improve the comprehension of their context by generating a risk map following an analysis of the position, distance, size and motion of the objects present in their environment. This comprehension is refined by data fusion steps applied to the High Level Information Fusion (HLIF) to predict possible impacts in the near future. A risk map is made up of probabilities generated after executing a set of inferences. These inferences allow the evaluation of future collision risks in different directions by detecting static objects, detecting free passage and analyzing paths followed by dynamic objects in a 3D plane. Different datasets were modeled and a comparative analysis was performed to check the percentage of correct answers and the accuracy of the inferences made using different classifiers. Thus, in order to demonstrate the advantages of the HLIF implementation in a dedicated VIP navigation system, the proposed architecture was tested against three other navigation systems that use different approaches. The generation of specific results made it possible to validate and compare these navigation systems. For this comparative analysis, different environments were used with the goal of indicating a direction for the VIP to move in with fewer collision risks. In addition to providing a risk map giving possible collisions, this project system provided greater reliability for navigation, especially when obstacles were very close and moving objects were detected and tracked. Tsirmpas et al. (2015) present the architecture of an indoor navigation system that provides locations and suggestions for guiding the visually impaired.This navigation is accomplished by obtaining remote data through devices called Radio Frequency Identification (RFID) tags.RFIDs are distributed in the environment and provide the visually impaired with site-specific information.López-de Ipiña et al. (2011) also apply a similar concept, through RFID and QR Codes for identification of supermarket products.This strategy is interesting to provide the VIP with the appropriate place to move or recognize objects but it is not suitable to inform possible collisions with dynamic objects or obstacles that do not have RFID.
The technologies developed so far to help visually impaired people (VIP) navigate meet only some of their everyday needs. This project allows the visually impaired to improve the comprehension of their context by generating a risk map following an analysis of the position, distance, size and motion of the objects present in their environment. This comprehension is refined by data fusion steps applied to the High Level Information Fusion (HLIF) to predict possible impacts in the near future. A risk map is made up of probabilities generated after executing a set of inferences. These inferences allow the evaluation of future collision risks in different directions by detecting static objects, detecting free passage and analyzing paths followed by dynamic objects in a 3D plane. Different datasets were modeled and a comparative analysis was performed to check the percentage of correct answers and the accuracy of the inferences made using different classifiers. Thus, in order to demonstrate the advantages of the HLIF implementation in a dedicated VIP navigation system, the proposed architecture was tested against three other navigation systems that use different approaches. The generation of specific results made it possible to validate and compare these navigation systems. For this comparative analysis, different environments were used with the goal of indicating a direction for the VIP to move in with fewer collision risks. In addition to providing a risk map giving possible collisions, this project system provided greater reliability for navigation, especially when obstacles were very close and moving objects were detected and tracked. Another important feature that should be emphasized is the dependency of installing multiple RFIDs in the environment.If there is no RFID in a certain place, the system does not have basic information for decision making and the system loses its purpose.Thus, systems using technologies such as RFID, QR Codes or any other type of remote information (Mekhalfi et al., 2016; López-de Ipiña et al., 2011; Tsirmpas et al., 2015) should be integrated with other systems that complement the mobility needs of VIP so that they do not depend only on sources of data implanted a priori.
The technologies developed so far to help visually impaired people (VIP) navigate meet only some of their everyday needs. This project allows the visually impaired to improve the comprehension of their context by generating a risk map following an analysis of the position, distance, size and motion of the objects present in their environment. This comprehension is refined by data fusion steps applied to the High Level Information Fusion (HLIF) to predict possible impacts in the near future. A risk map is made up of probabilities generated after executing a set of inferences. These inferences allow the evaluation of future collision risks in different directions by detecting static objects, detecting free passage and analyzing paths followed by dynamic objects in a 3D plane. Different datasets were modeled and a comparative analysis was performed to check the percentage of correct answers and the accuracy of the inferences made using different classifiers. Thus, in order to demonstrate the advantages of the HLIF implementation in a dedicated VIP navigation system, the proposed architecture was tested against three other navigation systems that use different approaches. The generation of specific results made it possible to validate and compare these navigation systems. For this comparative analysis, different environments were used with the goal of indicating a direction for the VIP to move in with fewer collision risks. In addition to providing a risk map giving possible collisions, this project system provided greater reliability for navigation, especially when obstacles were very close and moving objects were detected and tracked. Ando et al. (2011), present an interesting methodology with the use of data fusion in a VIP navigation support system.Its purpose is to provide continuous communication of the VIP with a network of sensors distributed in the environment to provide collision-free movement.With equipment worn by the VIP, the sensors and the processing center are connected by wireless networks.In this project, a set of physical sensors distributed around the environment is needed.
The technologies developed so far to help visually impaired people (VIP) navigate meet only some of their everyday needs. This project allows the visually impaired to improve the comprehension of their context by generating a risk map following an analysis of the position, distance, size and motion of the objects present in their environment. This comprehension is refined by data fusion steps applied to the High Level Information Fusion (HLIF) to predict possible impacts in the near future. A risk map is made up of probabilities generated after executing a set of inferences. These inferences allow the evaluation of future collision risks in different directions by detecting static objects, detecting free passage and analyzing paths followed by dynamic objects in a 3D plane. Different datasets were modeled and a comparative analysis was performed to check the percentage of correct answers and the accuracy of the inferences made using different classifiers. Thus, in order to demonstrate the advantages of the HLIF implementation in a dedicated VIP navigation system, the proposed architecture was tested against three other navigation systems that use different approaches. The generation of specific results made it possible to validate and compare these navigation systems. For this comparative analysis, different environments were used with the goal of indicating a direction for the VIP to move in with fewer collision risks. In addition to providing a risk map giving possible collisions, this project system provided greater reliability for navigation, especially when obstacles were very close and moving objects were detected and tracked. Xiao et al. (2015), have developed a system that adopts the Kinect motion sensor to obtain a disparity map and then looks at the use of social sensors to provide data for sites and social networks that support the VIP to find out if there is danger when moving in a particular area.With these data sources, it is possible to generate decisions, which allows for producing information with greater reliability.However, this system depends on the power of data provided by other people.Thus, Xiao et al. (2015) and Ando et al. (2011) have a high dependence on external data sources for performing data fusion.
The technologies developed so far to help visually impaired people (VIP) navigate meet only some of their everyday needs. This project allows the visually impaired to improve the comprehension of their context by generating a risk map following an analysis of the position, distance, size and motion of the objects present in their environment. This comprehension is refined by data fusion steps applied to the High Level Information Fusion (HLIF) to predict possible impacts in the near future. A risk map is made up of probabilities generated after executing a set of inferences. These inferences allow the evaluation of future collision risks in different directions by detecting static objects, detecting free passage and analyzing paths followed by dynamic objects in a 3D plane. Different datasets were modeled and a comparative analysis was performed to check the percentage of correct answers and the accuracy of the inferences made using different classifiers. Thus, in order to demonstrate the advantages of the HLIF implementation in a dedicated VIP navigation system, the proposed architecture was tested against three other navigation systems that use different approaches. The generation of specific results made it possible to validate and compare these navigation systems. For this comparative analysis, different environments were used with the goal of indicating a direction for the VIP to move in with fewer collision risks. In addition to providing a risk map giving possible collisions, this project system provided greater reliability for navigation, especially when obstacles were very close and moving objects were detected and tracked. Angin et al. (2011) address the use of the CA model to support VIP navigation.They list several data sources provides information to the VIP.In that study GPS is used to estimate the positioning and a stereo camera to detect the distance to the objects.Their system also performs a pattern recognition operation to make more information available for its fusion system.However, the SAW model that allows comprehension and designing collisions is not applied.
Pathological bacterial translocation (PBT) in cirrhosis is the hallmark of spontaneous bacterial infections, increasing mortality several-fold. Increased intestinal permeability is known to contribute to PBT in cirrhosis, although the role of the mucus layer has not been addressed in detail. A clear route of translocation for luminal intestinal bacteria is yet to be defined, but we hypothesize that the recently described gut-vascular barrier (GVB) is impaired in experimental portal hypertension, leading to increased accessibility of the vascular compartment for translocating bacteria. Cirrhosis was induced in mouse models using bile-duct ligation (BDL) and CCl4. Pre-hepatic portal-hypertension was induced by partial portal vein ligation (PPVL). Intestinal permeability was compared in these mice after GFP-Escherichia coli or different sized FITC-dextrans were injected into the intestine. Healthy and pre-hepatic portal-hypertensive (PPVL) mice lack translocation of FITC-dextran and GFP-E. coli from the small intestine to the liver, whereas BDL and CCl4-induced cirrhotic mice demonstrate pathological translocation, which is not altered by prior thoracic-duct ligation. The mucus layer is reduced in thickness, with loss of goblet cells and Muc2-staining and expression in cirrhotic but not PPVL mice. These changes are associated with bacterial overgrowth in the inner mucus layer and pathological translocation of GFP-E. coli through the ileal epithelium. GVB is profoundly altered in BDL and CCl4-mice with Ileal extravasation of large-sized 150 kDa-FITC-dextran, but only slightly altered in PPVL mice. This pathological endothelial permeability and accessibility in cirrhotic mice is associated with augmented expression of PV1 in intestinal vessels. OCA but not fexaramine stabilizes the GVB, whereas both FXR-agonists ameliorate gut to liver translocation of GFP-E. coli. Cirrhosis, but not portal hypertension per se, grossly impairs the endothelial and muco-epithelial barriers, promoting PBT to the portal-venous circulation. Both barriers appear to be FXR-modulated, with FXR-agonists reducing PBT via the portal-venous route. The gut-liver axis represents the pathophysiological hallmark for initiation and/or perpetuation of multiple liver diseases1 and has been proposed to be fueled by pathological bacterial translocation (PBT) from the gut.2In liver cirrhosis, PBT from the gut into the liver and systemic circulation is one of the causes of bacterial infections and the augmented pro-inflammatory response to gut-derived products.2,3In fact, failure to control invading bacteria and bacterial products in concert with host susceptibility determines remote organ injury in liver cirrhosis.This may include acute-on-chronic liver failure, hepatorenal syndrome and hepatic encephalopathy, which are all associated with worsening prognosis.4PBT in liver cirrhosis has been attributed to small intestinal bacterial overgrowth, increased intestinal permeability and lack of host defense mechanisms.5Herein, we focused on the first and last barrier separating luminal bacteria and the vascular compartment, namely intestinal mucus and the newly defined gut-vascular barrier (GVB),6 neither of which have been addressed so far in liver cirrhosis and PBT.
Pathological bacterial translocation (PBT) in cirrhosis is the hallmark of spontaneous bacterial infections, increasing mortality several-fold. Increased intestinal permeability is known to contribute to PBT in cirrhosis, although the role of the mucus layer has not been addressed in detail. A clear route of translocation for luminal intestinal bacteria is yet to be defined, but we hypothesize that the recently described gut-vascular barrier (GVB) is impaired in experimental portal hypertension, leading to increased accessibility of the vascular compartment for translocating bacteria. Cirrhosis was induced in mouse models using bile-duct ligation (BDL) and CCl4. Pre-hepatic portal-hypertension was induced by partial portal vein ligation (PPVL). Intestinal permeability was compared in these mice after GFP-Escherichia coli or different sized FITC-dextrans were injected into the intestine. Healthy and pre-hepatic portal-hypertensive (PPVL) mice lack translocation of FITC-dextran and GFP-E. coli from the small intestine to the liver, whereas BDL and CCl4-induced cirrhotic mice demonstrate pathological translocation, which is not altered by prior thoracic-duct ligation. The mucus layer is reduced in thickness, with loss of goblet cells and Muc2-staining and expression in cirrhotic but not PPVL mice. These changes are associated with bacterial overgrowth in the inner mucus layer and pathological translocation of GFP-E. coli through the ileal epithelium. GVB is profoundly altered in BDL and CCl4-mice with Ileal extravasation of large-sized 150 kDa-FITC-dextran, but only slightly altered in PPVL mice. This pathological endothelial permeability and accessibility in cirrhotic mice is associated with augmented expression of PV1 in intestinal vessels. OCA but not fexaramine stabilizes the GVB, whereas both FXR-agonists ameliorate gut to liver translocation of GFP-E. coli. Cirrhosis, but not portal hypertension per se, grossly impairs the endothelial and muco-epithelial barriers, promoting PBT to the portal-venous circulation. Both barriers appear to be FXR-modulated, with FXR-agonists reducing PBT via the portal-venous route. Mucus represents the first frontier that commensal microbes in the gut have to cross in order to achieve PBT.The mucus consists of 2 layers with a similar protein composition where mucin-2 (MUC2) is the main component.7On one hand, the inner mucus layer is firmly attached to the epithelium, is densely packed and is devoid of bacteria.8On the other hand, the outer mucus layer is much more mobile, looser and is colonized with a distinct bacterial community.9Goblet cells (GCs) are responsible for the formation of both the inner and outer mucus layer10 but also sense bacteria11 and react accordingly with mucin secretion.12,13After crossing the mucus and epithelial barrier, translocating bacteria reach the lymphatic system, as shown by culture positive mesenteric lymph nodes in experimental cirrhosis in multiple independent studies.14,15In contrast, access to the intestinal microcirculation and portal-venous route has been proposed for PBT,16 but has not been delineated in detail in portal hypertension and liver cirrhosis yet.The splanchnic circulation in portal hypertension presents with multiple vascular abnormalities17 including arterial vasodilation,14 hyporesponsiveness to vasoconstrictors18,19 and increased angiogenesis.20However, accessability of the intestinal microcirculation and thus, portal-venous route has not been investigated in portal hypertension so far.Endothelial barriers are characterized by the presence of junctional complexes which strictly control paracellular trafficking of solutes, fluids and cells.21In healthy conditions, the endothelial vascular barrier discriminates between differently sized particles of the same nature with 4 kDa-dextran freely diffusing through the endothelium, whereas 70 kDa-dextran does not.Plasmalemma vesicle-associated protein (PV)-1 is an endothelial cell-specific protein that forms the stomatal and fenestral diaphragms of blood vessels22 and regulates basal permeability.23
Pathological bacterial translocation (PBT) in cirrhosis is the hallmark of spontaneous bacterial infections, increasing mortality several-fold. Increased intestinal permeability is known to contribute to PBT in cirrhosis, although the role of the mucus layer has not been addressed in detail. A clear route of translocation for luminal intestinal bacteria is yet to be defined, but we hypothesize that the recently described gut-vascular barrier (GVB) is impaired in experimental portal hypertension, leading to increased accessibility of the vascular compartment for translocating bacteria. Cirrhosis was induced in mouse models using bile-duct ligation (BDL) and CCl4. Pre-hepatic portal-hypertension was induced by partial portal vein ligation (PPVL). Intestinal permeability was compared in these mice after GFP-Escherichia coli or different sized FITC-dextrans were injected into the intestine. Healthy and pre-hepatic portal-hypertensive (PPVL) mice lack translocation of FITC-dextran and GFP-E. coli from the small intestine to the liver, whereas BDL and CCl4-induced cirrhotic mice demonstrate pathological translocation, which is not altered by prior thoracic-duct ligation. The mucus layer is reduced in thickness, with loss of goblet cells and Muc2-staining and expression in cirrhotic but not PPVL mice. These changes are associated with bacterial overgrowth in the inner mucus layer and pathological translocation of GFP-E. coli through the ileal epithelium. GVB is profoundly altered in BDL and CCl4-mice with Ileal extravasation of large-sized 150 kDa-FITC-dextran, but only slightly altered in PPVL mice. This pathological endothelial permeability and accessibility in cirrhotic mice is associated with augmented expression of PV1 in intestinal vessels. OCA but not fexaramine stabilizes the GVB, whereas both FXR-agonists ameliorate gut to liver translocation of GFP-E. coli. Cirrhosis, but not portal hypertension per se, grossly impairs the endothelial and muco-epithelial barriers, promoting PBT to the portal-venous circulation. Both barriers appear to be FXR-modulated, with FXR-agonists reducing PBT via the portal-venous route. Liver cirrhosis is characterized by deficient levels of luminal bile acids in the gut.24Bile acids have been long known for their major effects on the microbiome and the intestinal barrier function.They exert their effects via transcription factors among which the farnesoid X receptor (FXR) is known to be one of the most important.FXR activation has been reported to influence epithelial cell proliferation25 and to exert potent anti-inflammatory actions in the intestine, stabilizing epithelial integrity.26–29Moreover, FXR stimulation in the small intestine exerts antibacterial actions via induction of antimicrobial substances30 and FXR-agonists have been shown to ameliorate chemically induced intestinal inflammation, improving symptoms of colitis and inhibiting epithelial permeability.However, the exact role of bile acids and FXR in controlling intestinal muco-epithelial as well as vascular permeability is still unknown.In addition, the microbiome has been proposed to play a key role in mucus synthesis, release and barrier-function31 but information on its impact on GC density and mucus thickness are limited.Finally, although colonization by microbial commensals is known to promote vascular development32 its impact and modulatory role on the GVB-function is not known.
The presence of hepatocellular adenoma (HCA) in pregnant women requires special consideration, as it has been reported to carry the risk of growth and clinically significant haemorrhage. In this prospective study we assessed aspects of growth of HCA <5 cm during pregnancy. This was a multicentre prospective cohort study in pregnant women with suspected HCA <5 cm on imaging. Definitive HCA diagnosis was established by MRI with hepatobiliary contrast agents (LCE-MRI), preferably before pregnancy. If at study inclusion a definitive diagnosis was lacking, LCE-MRI was performed after giving birth. Growth of the adenoma (defined as an increase of >20%) was closely monitored with ultrasound examinations throughout pregnancy. Of the 66 women included, 18 were excluded from analysis because postpartum LCE-MRI did not confirm the diagnosis of HCA and showed the lesion to be focal nodular hyperplasia. The remaining 48 women, with an HCA confirmed by LCE-MRI, were followed during 51 pregnancies. Median age was 30 years (IQR 27–33) and median body mass index 31.9 kg/m2 (IQR 26.3–36.6). Growth of HCA was seen in 13 of the pregnancies (25.5%); the median increase was 14 mm (IQR 8–19). One woman whose HCA grew to >70 mm successfully underwent transarterial embolization at week 26 of pregnancy to prevent further growth. The other 50 pregnancies proceeded without complications. This study suggests that an HCA <5 cm confers minimal risk to a pregnant woman and none to her child. HCA increased in size during a quarter of pregnancies, so we recommend close monitoring with ultrasound examinations, enabling intervention if needed. In light of the large proportion of misdiagnosed HCA, LCE-MRI should be performed to prevent unnecessary anxiety in women with a benign liver lesion. Hepatocellular adenoma (HCA) occurs particularly among reproductive women and is associated with the use of oestrogen-containing oral contraceptives, androgen intake, obesity, and metabolic disorders.1,2The tumour may regress upon cessation of oestrogen-containing oral contraceptives and weight reduction.3,4
The presence of hepatocellular adenoma (HCA) in pregnant women requires special consideration, as it has been reported to carry the risk of growth and clinically significant haemorrhage. In this prospective study we assessed aspects of growth of HCA <5 cm during pregnancy. This was a multicentre prospective cohort study in pregnant women with suspected HCA <5 cm on imaging. Definitive HCA diagnosis was established by MRI with hepatobiliary contrast agents (LCE-MRI), preferably before pregnancy. If at study inclusion a definitive diagnosis was lacking, LCE-MRI was performed after giving birth. Growth of the adenoma (defined as an increase of >20%) was closely monitored with ultrasound examinations throughout pregnancy. Of the 66 women included, 18 were excluded from analysis because postpartum LCE-MRI did not confirm the diagnosis of HCA and showed the lesion to be focal nodular hyperplasia. The remaining 48 women, with an HCA confirmed by LCE-MRI, were followed during 51 pregnancies. Median age was 30 years (IQR 27–33) and median body mass index 31.9 kg/m2 (IQR 26.3–36.6). Growth of HCA was seen in 13 of the pregnancies (25.5%); the median increase was 14 mm (IQR 8–19). One woman whose HCA grew to >70 mm successfully underwent transarterial embolization at week 26 of pregnancy to prevent further growth. The other 50 pregnancies proceeded without complications. This study suggests that an HCA <5 cm confers minimal risk to a pregnant woman and none to her child. HCA increased in size during a quarter of pregnancies, so we recommend close monitoring with ultrasound examinations, enabling intervention if needed. In light of the large proportion of misdiagnosed HCA, LCE-MRI should be performed to prevent unnecessary anxiety in women with a benign liver lesion. Several HCA subtypes can be distinguished radiologically on contrast-enhanced MRI with hepatobiliary contrast agents (LCE-MRI)5,6) or on the basis of immunohistochemical staining or molecular characterization.They include hepatocyte nuclear factor 1α inactivated (H-HCA), inflammatory (I-HCA), β-catenin-activated (β-HCA), β-catenin-activated inflammatory (β-IHCA), and sonic hedgehog (sh-HCA) adenomas.7,8If specific mutations are not found, the HCA is labelled unclassified (U-HCA).Resection of an HCA >5 cm is usually advocated if it does not regress to <5 cm within 12 months, because the risk of complications is thought to be higher in HCA >5 cm.9–12
The presence of hepatocellular adenoma (HCA) in pregnant women requires special consideration, as it has been reported to carry the risk of growth and clinically significant haemorrhage. In this prospective study we assessed aspects of growth of HCA <5 cm during pregnancy. This was a multicentre prospective cohort study in pregnant women with suspected HCA <5 cm on imaging. Definitive HCA diagnosis was established by MRI with hepatobiliary contrast agents (LCE-MRI), preferably before pregnancy. If at study inclusion a definitive diagnosis was lacking, LCE-MRI was performed after giving birth. Growth of the adenoma (defined as an increase of >20%) was closely monitored with ultrasound examinations throughout pregnancy. Of the 66 women included, 18 were excluded from analysis because postpartum LCE-MRI did not confirm the diagnosis of HCA and showed the lesion to be focal nodular hyperplasia. The remaining 48 women, with an HCA confirmed by LCE-MRI, were followed during 51 pregnancies. Median age was 30 years (IQR 27–33) and median body mass index 31.9 kg/m2 (IQR 26.3–36.6). Growth of HCA was seen in 13 of the pregnancies (25.5%); the median increase was 14 mm (IQR 8–19). One woman whose HCA grew to >70 mm successfully underwent transarterial embolization at week 26 of pregnancy to prevent further growth. The other 50 pregnancies proceeded without complications. This study suggests that an HCA <5 cm confers minimal risk to a pregnant woman and none to her child. HCA increased in size during a quarter of pregnancies, so we recommend close monitoring with ultrasound examinations, enabling intervention if needed. In light of the large proportion of misdiagnosed HCA, LCE-MRI should be performed to prevent unnecessary anxiety in women with a benign liver lesion. In pregnant women, HCA requires special attention because of the risk of hormone-induced growth and rupture, which may threaten the life of both mother and child.Cobey and Salem reported that the mortality risk of ruptured HCA >6.5 cm during pregnancy was 44% for the mothers and 38% for the foetuses [13].However, almost all cases included in this review dated from the 1970s and 1980s.In 2011, our research group proposed close monitoring of pregnant women with small HCA instead of intervention, and suggested that women with small HCA should not be discouraged to fall pregnant.This suggestion was based on a study in which we monitored 12 women with documented HCA (5 with HCA >5 cm and 7 with HCA <5 cm) during a total of 17 pregnancies.14,15All pregnancies in this study had an uneventful course without adverse maternal or foetal outcomes.
Microvascular invasion (MVI) impairs surgical outcomes in patients with hepatocellular carcinoma (HCC). As there is no single highly reliable factor to preoperatively predict MVI, we developed a computational approach integrating large-scale clinical and imaging modalities, especially radiomic features from contrast-enhanced CT, to predict MVI and clinical outcomes in patients with HCC. In total, 495 surgically resected patients were retrospectively included. MVI-related radiomic scores (R-scores) were built from 7,260 radiomic features in 6 target volumes. Six R-scores, 15 clinical factors, and 12 radiographic scores were integrated into a predictive model, the radiographic-radiomic (RR) model, with multivariate logistic regression. Radiomics related to tumor size and intratumoral heterogeneity were the top-ranked MVI predicting features. The related R-scores showed significant differences according to MVI status (p <0.001). Regression analysis identified 8 MVI risk factors, including 5 radiographic features and an R-score. The R-score (odds ratio [OR] 2.34) was less important than tumor capsule (OR 5.12), tumor margin (OR 4.20), and peritumoral enhancement (OR 3.03). The RR model using these predictors achieved an area under the curve (AUC) of 0.909 in training/validation and 0.889 in the test set. Progression-free survival (PFS) and overall survival (OS) were significantly different between the RR-predicted MVI-absent and MVI-present groups (median PFS: 49.5 vs. 12.9 months; median OS: 76.3 vs. 47.3 months). RR-computed MVI probability, histologic MVI, tumor size, and Edmondson-Steiner grade were independently associated with disease-specific recurrence and mortality. The computational approach, integrating large-scale clinico-radiologic and radiomic features, demonstrates good performance for predicting MVI and clinical outcomes. However, radiomics with current CT imaging analysis protocols do not provide statistically significant added value to radiographic scores. Hepatocellular carcinoma (HCC) is one of the most common primary hepatic malignant tumors and its incidence is increasing worldwide.1It is the second leading cause of cancer-specific mortality in the Asia-Pacific regions, and especially in China.2Surgical resection and liver transplantation (LT) are potentially curative for patients with HCC,3 but recurrence after surgical treatment is common.Some studies maintained that approximately 70% of patients would suffer from recurrence within 5 years after surgical resection, and 35% after LT.4–8
Microvascular invasion (MVI) impairs surgical outcomes in patients with hepatocellular carcinoma (HCC). As there is no single highly reliable factor to preoperatively predict MVI, we developed a computational approach integrating large-scale clinical and imaging modalities, especially radiomic features from contrast-enhanced CT, to predict MVI and clinical outcomes in patients with HCC. In total, 495 surgically resected patients were retrospectively included. MVI-related radiomic scores (R-scores) were built from 7,260 radiomic features in 6 target volumes. Six R-scores, 15 clinical factors, and 12 radiographic scores were integrated into a predictive model, the radiographic-radiomic (RR) model, with multivariate logistic regression. Radiomics related to tumor size and intratumoral heterogeneity were the top-ranked MVI predicting features. The related R-scores showed significant differences according to MVI status (p <0.001). Regression analysis identified 8 MVI risk factors, including 5 radiographic features and an R-score. The R-score (odds ratio [OR] 2.34) was less important than tumor capsule (OR 5.12), tumor margin (OR 4.20), and peritumoral enhancement (OR 3.03). The RR model using these predictors achieved an area under the curve (AUC) of 0.909 in training/validation and 0.889 in the test set. Progression-free survival (PFS) and overall survival (OS) were significantly different between the RR-predicted MVI-absent and MVI-present groups (median PFS: 49.5 vs. 12.9 months; median OS: 76.3 vs. 47.3 months). RR-computed MVI probability, histologic MVI, tumor size, and Edmondson-Steiner grade were independently associated with disease-specific recurrence and mortality. The computational approach, integrating large-scale clinico-radiologic and radiomic features, demonstrates good performance for predicting MVI and clinical outcomes. However, radiomics with current CT imaging analysis protocols do not provide statistically significant added value to radiographic scores. Microvascular invasion (MVI) is one of the most important prognostic factors for HCC after surgical treatment.9–11Contrary to macrovascular invasion, which can be detected with diagnostic imaging, MVI is a histologic finding that can only be postoperatively diagnosed with a surgical specimen.12Preoperative prediction of MVI is still challenging.A variety of imaging findings have been described, with variable diagnostic utility.Previous studies found that imaging features such as tumor size, multinodular tumor morphology, tumor margins, and peritumoral enhancement were associated with MVI.13–15In addition, Renzulli et al. showed that a 2-trait predictor of venous invasion can be a useful preoperative predictor of MVI.14Banerjee et al. showed that a radio-genomic venous invasion (RVI) predictor, based on the association between imaging features and gene expression, achieves high accuracy in predicting MVI in HCC.8However, these criteria for a preoperative imaging diagnosis of MVI in HCC have not yet been widely recognized.
Microvascular invasion (MVI) impairs surgical outcomes in patients with hepatocellular carcinoma (HCC). As there is no single highly reliable factor to preoperatively predict MVI, we developed a computational approach integrating large-scale clinical and imaging modalities, especially radiomic features from contrast-enhanced CT, to predict MVI and clinical outcomes in patients with HCC. In total, 495 surgically resected patients were retrospectively included. MVI-related radiomic scores (R-scores) were built from 7,260 radiomic features in 6 target volumes. Six R-scores, 15 clinical factors, and 12 radiographic scores were integrated into a predictive model, the radiographic-radiomic (RR) model, with multivariate logistic regression. Radiomics related to tumor size and intratumoral heterogeneity were the top-ranked MVI predicting features. The related R-scores showed significant differences according to MVI status (p <0.001). Regression analysis identified 8 MVI risk factors, including 5 radiographic features and an R-score. The R-score (odds ratio [OR] 2.34) was less important than tumor capsule (OR 5.12), tumor margin (OR 4.20), and peritumoral enhancement (OR 3.03). The RR model using these predictors achieved an area under the curve (AUC) of 0.909 in training/validation and 0.889 in the test set. Progression-free survival (PFS) and overall survival (OS) were significantly different between the RR-predicted MVI-absent and MVI-present groups (median PFS: 49.5 vs. 12.9 months; median OS: 76.3 vs. 47.3 months). RR-computed MVI probability, histologic MVI, tumor size, and Edmondson-Steiner grade were independently associated with disease-specific recurrence and mortality. The computational approach, integrating large-scale clinico-radiologic and radiomic features, demonstrates good performance for predicting MVI and clinical outcomes. However, radiomics with current CT imaging analysis protocols do not provide statistically significant added value to radiographic scores. Radiomics is a newly emerging form of imaging analysis using a series of data-mining algorithms or statistical analysis tools on high-throughput imaging features to obtain predictive or prognostic information.By building appropriate models with refined features, it achieved successful assessment and prediction abilities in various challenging clinical tasks.16–19A landmark study in colorectal cancer revealed clear associations between CT radiomics and lymph node metastases, and a combination of radiomic and clinico-radiologic factors could achieve significant clinical benefits.20However, despite its potential, the use of radiomics as a clinical biomarker still necessitates amelioration and standardization.21Greater integration between radiomics and other sources of data is required for clinicians to fully and confidently accept its role in patient management.To the best of our knowledge, only one study to date has assessed the prognostic aspect of radiomics for MVI in a group of 304 patients with HCC.22However, stronger evidence is needed in support of the implications for tumor progression and the reliability of the methodology.Additionally, MVI concerns tumor edges, while in previous studies radiomic features were only extracted inside the tumor, where by definition there is no microvascular involvement.A more effective evaluation should focus on the radiomic features at the tumor periphery.
Recently the Amsterdam-Oxford model (AOM) was introduced as a prognostic model to assess the risk of death and/or liver transplantation (LT) in primary sclerosing cholangitis (PSC). We aimed to validate and assess the utility of the AOM. Clinical and laboratory data were collected from the time of PSC diagnosis until the last visit or time of LT or death. The AOM was calculated at yearly intervals following PSC diagnosis. Discriminatory performance was assessed by calculation of the C-statistic and prediction accuracy by comparing the predicted survival with the observed survival in Kaplan-Meier estimates. A grid search was performed to identify the most discriminatory AOM threshold. A total of 534 patients with PSC and a mean (SD) age of 39.2 (13.1) years were included. The diagnosis was large duct PSC in 466 (87%), PSC with features of autoimmune hepatitis in 52 (10%) and small-duct PSC in 16 (3%). During the median (IQR) follow-up of 7.8 (4.0–12.6) years, 167 patients underwent LT and 65 died. The median LT-free survival was 13.2 (11.8–14.7) years. The C-statistic of the AOM ranged from 0.67 at baseline to 0.75 at 5 years of follow-up. The difference between the predicted and observed survival ranged from −1.6% at 1 year to + 3.9% at 5 years of follow-up. Patients that developed AOM scores >2.0 were at significant risk of LT or death (time-dependent hazard ratio 4.09; 95% CI 2.99–5.61). In this large cohort of patients with PSC, the AOM showed an adequate discriminative performance and good prediction accuracy at PSC diagnosis and during follow-up. This study further validates the AOM as a valuable risk stratification tool in PSC and extends its utility. Primary sclerosing cholangitis (PSC) is a chronic, variably progressive cholestatic liver disease characterized by inflammation of the intrahepatic and extrahepatic bile ducts, sclerosis and destruction of the biliary tract.1–4This leads to chronic cholestasis, biliary fibrosis and (decompensated) cirrhosis, which may eventually culminate into liver failure requiring liver transplantation; the only potential curative treatment for PSC.2,3Following a PSC diagnosis a median transplant-free survival of 13 years has been reported in studies from tertiary referral centres, although this may be longer in a population-based setting.5
Recently the Amsterdam-Oxford model (AOM) was introduced as a prognostic model to assess the risk of death and/or liver transplantation (LT) in primary sclerosing cholangitis (PSC). We aimed to validate and assess the utility of the AOM. Clinical and laboratory data were collected from the time of PSC diagnosis until the last visit or time of LT or death. The AOM was calculated at yearly intervals following PSC diagnosis. Discriminatory performance was assessed by calculation of the C-statistic and prediction accuracy by comparing the predicted survival with the observed survival in Kaplan-Meier estimates. A grid search was performed to identify the most discriminatory AOM threshold. A total of 534 patients with PSC and a mean (SD) age of 39.2 (13.1) years were included. The diagnosis was large duct PSC in 466 (87%), PSC with features of autoimmune hepatitis in 52 (10%) and small-duct PSC in 16 (3%). During the median (IQR) follow-up of 7.8 (4.0–12.6) years, 167 patients underwent LT and 65 died. The median LT-free survival was 13.2 (11.8–14.7) years. The C-statistic of the AOM ranged from 0.67 at baseline to 0.75 at 5 years of follow-up. The difference between the predicted and observed survival ranged from −1.6% at 1 year to + 3.9% at 5 years of follow-up. Patients that developed AOM scores >2.0 were at significant risk of LT or death (time-dependent hazard ratio 4.09; 95% CI 2.99–5.61). In this large cohort of patients with PSC, the AOM showed an adequate discriminative performance and good prediction accuracy at PSC diagnosis and during follow-up. This study further validates the AOM as a valuable risk stratification tool in PSC and extends its utility. One of the major challenges in the management of PSC is the lack of therapies that halt disease progression.Despite the biochemical improvement reported with ursodeoxycholic acid (UDCA) treatment in PSC, a survival benefit has never been reported.6–11Another challenge concerns reliable estimation of prognosis in PSC, largely because of the heterogeneity in clinical course progression and the variety of outcomes ranging from end-stage liver disease to development of hepatobiliary and colorectal malignancies.12–14In this setting, risk prediction models that quantify the risk of future events for individual patients with PSC are of critical importance for patient counselling, timely diagnostic procedures and subsequent therapeutic interventions for disease-related complications.Also, reliable risk stratification is important for the selection of patients in future drug development trials.
Eight-week glecaprevir/pibrentasvir leads to high rates of sustained virological response at post-treatment week 12 (SVR12) across HCV genotypes (GT) 1–6 in treatment-naïve patients without cirrhosis. We evaluated glecaprevir/pibrentasvir once daily for 8 weeks in treatment-naïve patients with compensated cirrhosis. EXPEDITION-8 was a single-arm, multicenter, phase IIIb trial. The primary and key secondary efficacy analyses were to compare the lower bound of the 95% CI of the SVR12 rate in i) patients with GT1,2,4–6 in the per protocol (PP) population, ii) patients with GT1,2,4–6 in the intention-to-treat (ITT) population, iii) patients with GT1–6 in the PP population, and iv) patients with GT1–6 in the ITT population, to pre-defined efficacy thresholds based on historical SVR12 rates for 12 weeks of glecaprevir/pibrentasvir in the same populations. Safety was also assessed. A total of 343 patients were enrolled. Most patients were male (63%), white (83%), and had GT1 (67%). The SVR12 rate in patients with GT1–6 was 99.7% (n/N = 334/335; 95% CI 98.3–99.9) in the PP population and 97.7% (n/N = 335/343; 95% CI 96.1–99.3) in the ITT population. All primary and key secondary efficacy analyses were achieved. One patient (GT3a) experienced relapse (0.3%) at post-treatment week 4. Common adverse events (≥5%) were fatigue (9%), pruritus (8%), headache (8%), and nausea (6%). Serious adverse events (none related) occurred in 2% of patients. No adverse event led to study drug discontinuation. Clinically significant laboratory abnormalities were infrequent. Eight-week glecaprevir/pibrentasvir was well tolerated and led to a similarly high SVR12 rate as the 12-week regimen in treatment-naïve patients with chronic HCV GT1–6 infection and compensated cirrhosis. Trial registration: ClinicalTrials.gov, NCT03089944. Chronic HCV infection is a major public health threat, with an estimated 71 million individuals affected worldwide.1In 2015, it was estimated that only 20% of these individuals were aware of their HCV infection.1In 2016, approximately 13% of those aware of having chronic HCV infection were being treated.2Approximately 15–30% of patients with chronic HCV infection will develop cirrhosis within 20 years3 and, if left untreated, these patients are at risk of developing hepatic decompensation and hepatocellular carcinoma, ultimately leading to increased liver-related mortality.4Successful treatment of chronic HCV infection can significantly reduce disease progression, as well as rates of HCV transmission.5
Eight-week glecaprevir/pibrentasvir leads to high rates of sustained virological response at post-treatment week 12 (SVR12) across HCV genotypes (GT) 1–6 in treatment-naïve patients without cirrhosis. We evaluated glecaprevir/pibrentasvir once daily for 8 weeks in treatment-naïve patients with compensated cirrhosis. EXPEDITION-8 was a single-arm, multicenter, phase IIIb trial. The primary and key secondary efficacy analyses were to compare the lower bound of the 95% CI of the SVR12 rate in i) patients with GT1,2,4–6 in the per protocol (PP) population, ii) patients with GT1,2,4–6 in the intention-to-treat (ITT) population, iii) patients with GT1–6 in the PP population, and iv) patients with GT1–6 in the ITT population, to pre-defined efficacy thresholds based on historical SVR12 rates for 12 weeks of glecaprevir/pibrentasvir in the same populations. Safety was also assessed. A total of 343 patients were enrolled. Most patients were male (63%), white (83%), and had GT1 (67%). The SVR12 rate in patients with GT1–6 was 99.7% (n/N = 334/335; 95% CI 98.3–99.9) in the PP population and 97.7% (n/N = 335/343; 95% CI 96.1–99.3) in the ITT population. All primary and key secondary efficacy analyses were achieved. One patient (GT3a) experienced relapse (0.3%) at post-treatment week 4. Common adverse events (≥5%) were fatigue (9%), pruritus (8%), headache (8%), and nausea (6%). Serious adverse events (none related) occurred in 2% of patients. No adverse event led to study drug discontinuation. Clinically significant laboratory abnormalities were infrequent. Eight-week glecaprevir/pibrentasvir was well tolerated and led to a similarly high SVR12 rate as the 12-week regimen in treatment-naïve patients with chronic HCV GT1–6 infection and compensated cirrhosis. Trial registration: ClinicalTrials.gov, NCT03089944. The development of combination treatments comprising all-oral, interferon-free, direct-acting antiviral (DAA) drugs has substantially improved the efficacy and safety of HCV therapy.Recommended treatments are associated with high rates of sustained virological response at post-treatment week 12 (SVR12), low risk of HCV drug resistance, and treatment durations as short as 8 weeks for patients without cirrhosis.6However, patients with compensated cirrhosis are still considered more challenging to treat.5,7At the time of designing this study, DAA therapies recommended for the population with cirrhosis required at least 12 weeks of treatment, and no 8-week DAA regimen was approved for treatment-naïve patients with compensated cirrhosis (although 8 weeks of sofosbuvir/velpatasvir/voxilaprevir could be considered in DAA-naïve HCV genotype 3–infected patients with compensated cirrhosis in Europe).8Additional limitations for some approved regimens included the requirement for ribavirin, resistance testing, and monitoring for potential drug–drug interactions.5,7In a compensated cirrhotic population at risk of disease progression, an 8-week pangenotypic DAA regimen without such limitations may be preferable if SVR12 remains high.Furthermore, simplification of HCV care such that all treatment-naïve patients are treated for 8 weeks regardless of cirrhosis status may expand the number of healthcare professionals who can prescribe DAA therapy and increase the number of patients treated.5
Eight-week glecaprevir/pibrentasvir leads to high rates of sustained virological response at post-treatment week 12 (SVR12) across HCV genotypes (GT) 1–6 in treatment-naïve patients without cirrhosis. We evaluated glecaprevir/pibrentasvir once daily for 8 weeks in treatment-naïve patients with compensated cirrhosis. EXPEDITION-8 was a single-arm, multicenter, phase IIIb trial. The primary and key secondary efficacy analyses were to compare the lower bound of the 95% CI of the SVR12 rate in i) patients with GT1,2,4–6 in the per protocol (PP) population, ii) patients with GT1,2,4–6 in the intention-to-treat (ITT) population, iii) patients with GT1–6 in the PP population, and iv) patients with GT1–6 in the ITT population, to pre-defined efficacy thresholds based on historical SVR12 rates for 12 weeks of glecaprevir/pibrentasvir in the same populations. Safety was also assessed. A total of 343 patients were enrolled. Most patients were male (63%), white (83%), and had GT1 (67%). The SVR12 rate in patients with GT1–6 was 99.7% (n/N = 334/335; 95% CI 98.3–99.9) in the PP population and 97.7% (n/N = 335/343; 95% CI 96.1–99.3) in the ITT population. All primary and key secondary efficacy analyses were achieved. One patient (GT3a) experienced relapse (0.3%) at post-treatment week 4. Common adverse events (≥5%) were fatigue (9%), pruritus (8%), headache (8%), and nausea (6%). Serious adverse events (none related) occurred in 2% of patients. No adverse event led to study drug discontinuation. Clinically significant laboratory abnormalities were infrequent. Eight-week glecaprevir/pibrentasvir was well tolerated and led to a similarly high SVR12 rate as the 12-week regimen in treatment-naïve patients with chronic HCV GT1–6 infection and compensated cirrhosis. Trial registration: ClinicalTrials.gov, NCT03089944. Co-formulated glecaprevir (HCV NS3/4A protease inhibitor) and pibrentasvir (HCV NS5A inhibitor) is approved as a fixed-dose, once-daily, all-oral, ribavirin-free, pangenotypic combination therapy (glecaprevir/pibrentasvir) for the treatment of patients with chronic HCV genotype 1–6 infection without cirrhosis or with compensated cirrhosis.9,10Glecaprevir/pibrentasvir has shown potent pangenotypic activity against common amino acid substitutions of HCV that confer resistance to approved NS3/4A and NS5A inhibitors.11–13At the time of designing this study, in HCV treatment-naïve patients without cirrhosis, the approved treatment duration for glecaprevir/pibrentasvir was 8 weeks and was associated with an SVR12 rate of 97.5%.9In HCV treatment-naïve patients with compensated cirrhosis, the approved treatment duration was 12 weeks and was associated with an SVR12 rate of 98.0% overall9 and 97.5% in HCV genotype 3–infected patients.9,14
Cerebral oxidative stress plays an important role in the pathogenesis of hepatic encephalopathy (HE), but the underlying mechanisms are incompletely understood. Herein, we analyzed a role of heme oxygenase (HO)1, iron and NADPH oxidase 4 (Nox4) for the induction of oxidative stress and senescence in HE. Gene and protein expression in human post-mortem brain samples was analyzed by gene array and western blot analysis. Mechanisms and functional consequences of HO1 upregulation were studied in NH4Cl-exposed astrocytes in vitro by western blot, qPCR and super-resolution microscopy. HO1 and the endoplasmic reticulum (ER) stress marker grp78 were upregulated, together with changes in the expression of multiple iron metabolism-related genes, in post-mortem brain samples from patients with liver cirrhosis and HE. NH4Cl elevated HO1 protein and mRNA in cultured astrocytes through glutamine synthetase (GS)-dependent upregulation of glutamine/fructose amidotransferases 1/2 (GFAT1/2), which blocked the transcription of the HO1-targeting miR326-3p in a O-GlcNAcylation dependent manner. Upregulation of HO1 by NH4Cl triggered ER stress and was associated with elevated levels of free ferrous iron and expression changes in iron metabolism-related genes, which were largely abolished after knockdown or inhibition of GS, GFAT1/2, HO1 or iron chelation. NH4Cl, glucosamine (GlcN) and inhibition of miR326-3p upregulated Nox4, while knockdown of Nox4, GS, GFAT1/2, HO1 or iron chelation prevented NH4Cl-induced RNA oxidation and astrocyte senescence. Elevated levels of grp78 and O-GlcNAcylated proteins were also found in brain samples from patients with liver cirrhosis and HE. The present study identified glucosamine synthesis-dependent protein O-GlcNAcylation as a novel mechanism in the pathogenesis of HE that triggers oxidative and ER stress, as well as senescence, through upregulation of HO1 and Nox4. Ammonia is a major toxin in the pathogenesis of hepatic encephalopathy (HE).1In the brain, ammonia is detoxified by glutamine synthetase (GS) in astrocytes and glutamine accumulation in astrocytes in patients with liver cirrhosis and HE triggers the development of a low grade cerebral edema2 associated with oxidative/nitrosative stress.3,4
Cerebral oxidative stress plays an important role in the pathogenesis of hepatic encephalopathy (HE), but the underlying mechanisms are incompletely understood. Herein, we analyzed a role of heme oxygenase (HO)1, iron and NADPH oxidase 4 (Nox4) for the induction of oxidative stress and senescence in HE. Gene and protein expression in human post-mortem brain samples was analyzed by gene array and western blot analysis. Mechanisms and functional consequences of HO1 upregulation were studied in NH4Cl-exposed astrocytes in vitro by western blot, qPCR and super-resolution microscopy. HO1 and the endoplasmic reticulum (ER) stress marker grp78 were upregulated, together with changes in the expression of multiple iron metabolism-related genes, in post-mortem brain samples from patients with liver cirrhosis and HE. NH4Cl elevated HO1 protein and mRNA in cultured astrocytes through glutamine synthetase (GS)-dependent upregulation of glutamine/fructose amidotransferases 1/2 (GFAT1/2), which blocked the transcription of the HO1-targeting miR326-3p in a O-GlcNAcylation dependent manner. Upregulation of HO1 by NH4Cl triggered ER stress and was associated with elevated levels of free ferrous iron and expression changes in iron metabolism-related genes, which were largely abolished after knockdown or inhibition of GS, GFAT1/2, HO1 or iron chelation. NH4Cl, glucosamine (GlcN) and inhibition of miR326-3p upregulated Nox4, while knockdown of Nox4, GS, GFAT1/2, HO1 or iron chelation prevented NH4Cl-induced RNA oxidation and astrocyte senescence. Elevated levels of grp78 and O-GlcNAcylated proteins were also found in brain samples from patients with liver cirrhosis and HE. The present study identified glucosamine synthesis-dependent protein O-GlcNAcylation as a novel mechanism in the pathogenesis of HE that triggers oxidative and ER stress, as well as senescence, through upregulation of HO1 and Nox4. Several in vitro and in vivo studies demonstrated that ammonia induces the formation of reactive oxygen species (ROS) in astrocytes,1 which triggers a variety of functional consequences, such as gene expression changes, protein tyrosine nitration, RNA oxidation and senescence.1,5,6Of note, these functional consequences were also identified in post-mortem brain samples from patients with liver cirrhosis and HE, but not in those without HE.3,4,6Of particular interest, RNA oxidation was suggested to disturb postsynaptic protein synthesis which is required for memory formation in HE.5Astrocyte senescence was proposed to destabilize synaptic connections, thereby explaining the clinical observation that cognitive impairment in patients with liver cirrhosis can persist after resolution of an acute episode of overt HE.6,7In line with this, a recent study showed that astrocyte senescence initiates and promotes cognitive decline in a mouse model of Alzheimer’s disease.8Overall, this evidence strongly supports an important role of oxidative stress in the pathogenesis of HE, but the underlying mechanism remains poorly understood.
Cerebral oxidative stress plays an important role in the pathogenesis of hepatic encephalopathy (HE), but the underlying mechanisms are incompletely understood. Herein, we analyzed a role of heme oxygenase (HO)1, iron and NADPH oxidase 4 (Nox4) for the induction of oxidative stress and senescence in HE. Gene and protein expression in human post-mortem brain samples was analyzed by gene array and western blot analysis. Mechanisms and functional consequences of HO1 upregulation were studied in NH4Cl-exposed astrocytes in vitro by western blot, qPCR and super-resolution microscopy. HO1 and the endoplasmic reticulum (ER) stress marker grp78 were upregulated, together with changes in the expression of multiple iron metabolism-related genes, in post-mortem brain samples from patients with liver cirrhosis and HE. NH4Cl elevated HO1 protein and mRNA in cultured astrocytes through glutamine synthetase (GS)-dependent upregulation of glutamine/fructose amidotransferases 1/2 (GFAT1/2), which blocked the transcription of the HO1-targeting miR326-3p in a O-GlcNAcylation dependent manner. Upregulation of HO1 by NH4Cl triggered ER stress and was associated with elevated levels of free ferrous iron and expression changes in iron metabolism-related genes, which were largely abolished after knockdown or inhibition of GS, GFAT1/2, HO1 or iron chelation. NH4Cl, glucosamine (GlcN) and inhibition of miR326-3p upregulated Nox4, while knockdown of Nox4, GS, GFAT1/2, HO1 or iron chelation prevented NH4Cl-induced RNA oxidation and astrocyte senescence. Elevated levels of grp78 and O-GlcNAcylated proteins were also found in brain samples from patients with liver cirrhosis and HE. The present study identified glucosamine synthesis-dependent protein O-GlcNAcylation as a novel mechanism in the pathogenesis of HE that triggers oxidative and ER stress, as well as senescence, through upregulation of HO1 and Nox4. Oxidative stress in ammonia-exposed astrocytes and in brains from animal models of HE, or patients with liver cirrhosis and HE, is associated with an upregulation of heme oxygenase-1 (HO1).3,9While HO1 was shown to protect from oxidative stress in some brain pathologies such as traumatic brain injury,10 its role in the pathogenesis of HE remains unclear.Likewise, the mechanisms underlying upregulation of HO1 by ammonia are only incompletely understood but may involve downregulation of HO1-targeting microRNAs.11Upregulation of HO1 may also be triggered by endoplasmic reticulum (ER) stress12 but whether upregulation of HO1 by ammonia in astrocytes is a consequence of ER stress is not known.
Cerebral oxidative stress plays an important role in the pathogenesis of hepatic encephalopathy (HE), but the underlying mechanisms are incompletely understood. Herein, we analyzed a role of heme oxygenase (HO)1, iron and NADPH oxidase 4 (Nox4) for the induction of oxidative stress and senescence in HE. Gene and protein expression in human post-mortem brain samples was analyzed by gene array and western blot analysis. Mechanisms and functional consequences of HO1 upregulation were studied in NH4Cl-exposed astrocytes in vitro by western blot, qPCR and super-resolution microscopy. HO1 and the endoplasmic reticulum (ER) stress marker grp78 were upregulated, together with changes in the expression of multiple iron metabolism-related genes, in post-mortem brain samples from patients with liver cirrhosis and HE. NH4Cl elevated HO1 protein and mRNA in cultured astrocytes through glutamine synthetase (GS)-dependent upregulation of glutamine/fructose amidotransferases 1/2 (GFAT1/2), which blocked the transcription of the HO1-targeting miR326-3p in a O-GlcNAcylation dependent manner. Upregulation of HO1 by NH4Cl triggered ER stress and was associated with elevated levels of free ferrous iron and expression changes in iron metabolism-related genes, which were largely abolished after knockdown or inhibition of GS, GFAT1/2, HO1 or iron chelation. NH4Cl, glucosamine (GlcN) and inhibition of miR326-3p upregulated Nox4, while knockdown of Nox4, GS, GFAT1/2, HO1 or iron chelation prevented NH4Cl-induced RNA oxidation and astrocyte senescence. Elevated levels of grp78 and O-GlcNAcylated proteins were also found in brain samples from patients with liver cirrhosis and HE. The present study identified glucosamine synthesis-dependent protein O-GlcNAcylation as a novel mechanism in the pathogenesis of HE that triggers oxidative and ER stress, as well as senescence, through upregulation of HO1 and Nox4. Another functional consequence in ammonia-exposed astrocytes is the glutamine synthesis-dependent O-GlcNAcylation of individual proteins.13This post-translational modification depends on the synthesis of activated sugar nucleotides within the hexosamine biosynthetic pathway and the conversion of glutamine and fructose-6-phosphate to glucosamine-6-phosphate (GlcN-6-P) by GFAT1/2 is rate-controlling.Similar to phosphorylation, O-GlcNAcylation can modulate the function of many proteins in specific ways.14Unfortunately, the functional consequences of the ammonia-induced protein O-GlcNAcylation in astrocytes remained completely unknown.13
To date, studies into the natural history of alcohol-related liver disease (ALD) have lacked long-term follow-up, large numbers of participants, or both. We performed a systematic review to summarise studies that describe the natural history of histologically proven ALD. PubMed and Medline were searched for relevant studies according to pre-specified criteria. Data were extracted to describe the prevalence of ALD, histological progression of disease and mortality. Single-proportion meta-analysis was used to combine data from studies regarding rates of progression or mortality. Thirty-seven studies were included, reporting data from 7,528 participants. Amongst cohorts of hazardous drinkers, on average 15% had normal histological appearance, 27% had hepatic steatosis, 24% had steatohepatitis and 26% had cirrhosis. The annualised rates of progression of pre-cirrhotic disease to cirrhosis were 1% (0–8%) for patients with normal histology, 3% (2–4%) for hepatic steatosis, 10% (6–17%) for steatohepatitis and 8% (3–19%) for fibrosis. Annualised mortality was 6% (4–7%) in patients with steatosis and 8% (5–13%) in cirrhosis. In patients with steatohepatitis on biopsy a marked difference was seen between inpatient cohorts (annual mortality 15%, 8–26%) and mixed cohorts of inpatients and outpatients (annual mortality 5%, 2–10%). Only in steatosis did non-liver-related mortality exceed liver-specific causes of mortality (5% per year vs. 1% per year). These data confirm the observation that alcohol-related hepatic steatohepatitis requiring admission to hospital is the most dangerous subtype of ALD. Alcohol-related steatosis is not a benign condition as it is associated with significant risk of mortality. Alcohol-related liver disease (ALD) is common throughout the world.1ALD is a leading cause of liver-related morbidity and mortality,2 and a frequent cause of death amongst people of working age.3Hazardous drinking – consumption of alcohol at levels that are likely to cause harm – is prevalent globally.4This is a prerequisite for the development of ALD, which covers a spectrum of disease from steatosis, steatohepatitis to cirrhosis.Earlier stages of disease are considered reversible with abstinence from alcohol.5Liver-specific morbidity and mortality is only considered relevant in patients with more advanced disease.
To date, studies into the natural history of alcohol-related liver disease (ALD) have lacked long-term follow-up, large numbers of participants, or both. We performed a systematic review to summarise studies that describe the natural history of histologically proven ALD. PubMed and Medline were searched for relevant studies according to pre-specified criteria. Data were extracted to describe the prevalence of ALD, histological progression of disease and mortality. Single-proportion meta-analysis was used to combine data from studies regarding rates of progression or mortality. Thirty-seven studies were included, reporting data from 7,528 participants. Amongst cohorts of hazardous drinkers, on average 15% had normal histological appearance, 27% had hepatic steatosis, 24% had steatohepatitis and 26% had cirrhosis. The annualised rates of progression of pre-cirrhotic disease to cirrhosis were 1% (0–8%) for patients with normal histology, 3% (2–4%) for hepatic steatosis, 10% (6–17%) for steatohepatitis and 8% (3–19%) for fibrosis. Annualised mortality was 6% (4–7%) in patients with steatosis and 8% (5–13%) in cirrhosis. In patients with steatohepatitis on biopsy a marked difference was seen between inpatient cohorts (annual mortality 15%, 8–26%) and mixed cohorts of inpatients and outpatients (annual mortality 5%, 2–10%). Only in steatosis did non-liver-related mortality exceed liver-specific causes of mortality (5% per year vs. 1% per year). These data confirm the observation that alcohol-related hepatic steatohepatitis requiring admission to hospital is the most dangerous subtype of ALD. Alcohol-related steatosis is not a benign condition as it is associated with significant risk of mortality. Good quality natural history studies exist for most liver diseases but only relatively few describe the prevalence or progression of histologically defined ALD.Population-based studies such as the Dionysius study in northern Italy6 are useful but have necessarily relied largely on ultrasonography to define liver disease.Mixed results have been observed on the association between histological parameters and progression of ALD.Hepatic steatosis, considered the earliest stage of ALD can progress to cirrhosis7,8 and other studies have confirmed the high risk of death in alcohol-related cirrhosis.9The relationship between alcohol intake and prevalence, progression and death is not straightforward in these studies, where other aetiological factors may play a significant role.10However, several prospective population studies have shown that self-reported alcohol intake is a good predictor of the future risk of alcohol-induced diseases.11
C-C motif chemokine receptor 2 (CCR2) has been recognized as a promising target for the treatment of liver fibrosis. PC3-secreted microprotein (PSMP)/microseminoprotein (MSMP) is a novel chemotactic cytokine and its receptor is CCR2. In the present study we investigated the expression and role of PSMP in liver fibrosis/cirrhosis. PSMP expression was studied in patients with fibrosis/cirrhosis and in 3 murine models of liver fibrosis, including mice treated with carbon tetrachloride (CCl4), bile-duct ligation, or a 5-diethoxycarbonyl-1,4-dihydrocollidine diet. The role of PSMP was evaluated in Psmp-/- mice and after treatment with a PSMP antibody in wild-type mice. The direct effects of PSMP on macrophages and hepatic stellate cells were studied in vitro. In this study, we found that PSMP was highly expressed in fibrotic/cirrhotic tissues from patients with different etiologies of liver disease and in the 3 experimental mouse models of fibrosis. Damage-associated molecular pattern molecules HMGB-1 and IL-33 induced hepatocytes to produce PSMP. PSMP deficiency resulted in a marked amelioration of hepatic injury and fibrosis. In CCl4-induced hepatic injury, the infiltration of macrophages and CCR2+ monocytes into the liver was significantly decreased in Psmp-/- mice. Consistent with the decreased levels of intrahepatic macrophages, proinflammatory cytokines were significantly reduced. Moreover, adeno-associated virus-8 vectors successfully overexpressing human PSMP in Psmp-/- mouse livers could reverse the attenuation of liver injury and fibrosis induced by CCl4 in a CCR2-dependent manner. Treatment with a specific PSMP-neutralizing antibody, 3D5, prevented liver injury and fibrosis induced by CCl4 in mice. At the cellular level, PSMP directly promoted M1 polarization of macrophages and activation of LX-2 cells. PSMP enhances liver fibrosis through its receptor, CCR2. PSMP is a potentially attractive therapeutic target for the treatment of patients with liver fibrosis. Liver fibrosis, a wound-healing response to chronic liver injury, is characterized by excessive deposition of extracellular matrix (ECM) in the liver and is triggered by a variety of causes, including hepatitis virus infection, alcohol abuse, cholestasis, autoimmune, drug/toxin and non-alcoholic steatohepatitis (NASH), which eventually lead to loss of liver function and disruption of the liver structure.1,2The initiation of fibrosis crucially depends on an inflammatory phase in which liver resident macrophages, Kupffer cells, are activated and release transforming growth factor-β (TGF-β), as well as other proinflammatory cytokines that activate hepatic stellate cells (HSCs).3–7HSCs are responsible for producing most of the ECM and play a central role in liver fibrogenesis.8,9HSCs are quiescent and located in the space between hepatocytes and sinusoidal endothelium (space of Disse) as retinoid storage cells.10Upon liver injury, HSCs, the major collagen-synthesizing cells in the liver, are activated and transdifferentiate into myofibroblast-like cells, which show enhanced proliferation, chemotaxis, survival and collagen production.8,9,11,12HSC activation is driven by multiple mediators, such as chemokines, reactive oxygen species, growth factors, matrix stiffness, matricellular proteins and damage-associated molecular patterns (DAMPs).5,8,13Currently, there are no approved drugs that can effectively reverse liver fibrosis, further highlighting the urgent clinical need for novel antifibrotic therapies.14
C-C motif chemokine receptor 2 (CCR2) has been recognized as a promising target for the treatment of liver fibrosis. PC3-secreted microprotein (PSMP)/microseminoprotein (MSMP) is a novel chemotactic cytokine and its receptor is CCR2. In the present study we investigated the expression and role of PSMP in liver fibrosis/cirrhosis. PSMP expression was studied in patients with fibrosis/cirrhosis and in 3 murine models of liver fibrosis, including mice treated with carbon tetrachloride (CCl4), bile-duct ligation, or a 5-diethoxycarbonyl-1,4-dihydrocollidine diet. The role of PSMP was evaluated in Psmp-/- mice and after treatment with a PSMP antibody in wild-type mice. The direct effects of PSMP on macrophages and hepatic stellate cells were studied in vitro. In this study, we found that PSMP was highly expressed in fibrotic/cirrhotic tissues from patients with different etiologies of liver disease and in the 3 experimental mouse models of fibrosis. Damage-associated molecular pattern molecules HMGB-1 and IL-33 induced hepatocytes to produce PSMP. PSMP deficiency resulted in a marked amelioration of hepatic injury and fibrosis. In CCl4-induced hepatic injury, the infiltration of macrophages and CCR2+ monocytes into the liver was significantly decreased in Psmp-/- mice. Consistent with the decreased levels of intrahepatic macrophages, proinflammatory cytokines were significantly reduced. Moreover, adeno-associated virus-8 vectors successfully overexpressing human PSMP in Psmp-/- mouse livers could reverse the attenuation of liver injury and fibrosis induced by CCl4 in a CCR2-dependent manner. Treatment with a specific PSMP-neutralizing antibody, 3D5, prevented liver injury and fibrosis induced by CCl4 in mice. At the cellular level, PSMP directly promoted M1 polarization of macrophages and activation of LX-2 cells. PSMP enhances liver fibrosis through its receptor, CCR2. PSMP is a potentially attractive therapeutic target for the treatment of patients with liver fibrosis. Extensive in vitro and in vivo investigations have elucidated the pivotal role played by the chemokine-chemokine receptor system in the pathogenesis of liver diseases.15,16Among these mediators, the C-C motif chemokine receptor 2 (CCR2)/C-C motif chemokine ligand 2 (CCL2) axis was shown to have a predominant role in liver inflammation and fibrosis.17–26Disruption of CCR2 signaling impedes liver fibrosis, as shown by the altered chemotaxis and transdifferentiation of HSCs.17–21In human liver diseases, increased CCL2 is associated with macrophage recruitment and liver fibrosis progression.22In addition, CCL2 inhibition attenuated CCl4-induced liver injury and fibrosis by inhibiting macrophage recruitment.23–26Cenicriviroc (CVC) is a novel oral dual CCR2/CCR5 antagonist with nanomolar potency against both receptors.27Both preclinical and clinical data have indicated that CVC is a safe and potent antifibrotic agent for the treatment of alcohol-induced steatohepatitis and NASH with fibrosis, and this drug is currently being tested in a phase III trial.27–29
C-C motif chemokine receptor 2 (CCR2) has been recognized as a promising target for the treatment of liver fibrosis. PC3-secreted microprotein (PSMP)/microseminoprotein (MSMP) is a novel chemotactic cytokine and its receptor is CCR2. In the present study we investigated the expression and role of PSMP in liver fibrosis/cirrhosis. PSMP expression was studied in patients with fibrosis/cirrhosis and in 3 murine models of liver fibrosis, including mice treated with carbon tetrachloride (CCl4), bile-duct ligation, or a 5-diethoxycarbonyl-1,4-dihydrocollidine diet. The role of PSMP was evaluated in Psmp-/- mice and after treatment with a PSMP antibody in wild-type mice. The direct effects of PSMP on macrophages and hepatic stellate cells were studied in vitro. In this study, we found that PSMP was highly expressed in fibrotic/cirrhotic tissues from patients with different etiologies of liver disease and in the 3 experimental mouse models of fibrosis. Damage-associated molecular pattern molecules HMGB-1 and IL-33 induced hepatocytes to produce PSMP. PSMP deficiency resulted in a marked amelioration of hepatic injury and fibrosis. In CCl4-induced hepatic injury, the infiltration of macrophages and CCR2+ monocytes into the liver was significantly decreased in Psmp-/- mice. Consistent with the decreased levels of intrahepatic macrophages, proinflammatory cytokines were significantly reduced. Moreover, adeno-associated virus-8 vectors successfully overexpressing human PSMP in Psmp-/- mouse livers could reverse the attenuation of liver injury and fibrosis induced by CCl4 in a CCR2-dependent manner. Treatment with a specific PSMP-neutralizing antibody, 3D5, prevented liver injury and fibrosis induced by CCl4 in mice. At the cellular level, PSMP directly promoted M1 polarization of macrophages and activation of LX-2 cells. PSMP enhances liver fibrosis through its receptor, CCR2. PSMP is a potentially attractive therapeutic target for the treatment of patients with liver fibrosis. PSMP, namely, PC3-secreted microprotein, or microseminoprotein (MSMP), was initially found in PC3 cells, benign and malignant prostate tissues.30Our previous study using omics strategies revealed that PSMP is a novel chemotactic cytokine acting as a CCR2 ligand to recruit peripheral blood monocytes and lymphocytes that may influence inflammation and cancer development.31The affinity between PSMP and CCR2 was found to be comparable to that between CCL2 and CCR2.31PSMP was expressed in human colitis tissues and significantly upregulated in dextran sulfate sodium (DSS)-induced mouse colitis.32PSMP plays a vital role in promoting DSS-induced colitis by chemoattraction of Ly6Chi monocytes in a CCR2-dependent manner.32Another study found that PSMP expression is induced with prolonged anti-VEGF therapy, specifically under hypoxia, and has an important proangiogenic role in treatment-resistant ovarian tumors.33
Long noncoding RNAs (lncRNAs) play important roles in various biological processes, regulating gene expression by diverse mechanisms. However, how lncRNAs regulate liver repopulation is unknown. Herein, we aimed to identify lncRNAs that regulate liver repopulation and elucidate the signaling pathways involved. Herein, we performed 70% partial hepatectomy in wild-type and gene knockout mice. We then performed transcriptomic analyses to identify a divergent lncRNA termed lncHand2 that is highly expressed during liver regeneration. LncHand2 is constitutively expressed in the nuclei of pericentral hepatocytes in mouse and human livers. LncHand2 knockout abrogates liver regeneration and repopulation capacity. Mechanistically, lncHand2 recruits the Ino80 remodeling complex to initiate expression of Nkx1-2 in trans, which triggers c-Met (Met) expression in hepatocytes. Finally, knockout of both Nkx1-2 and c-Met causes more severe liver injury and poorer repopulation ability. Thus, lncHand2 promotes liver repopulation via initiating Nkx1-2-induced c-Met signaling. Our findings reveal that lncHand2 acts as a critical mediator regulating liver repopulation. It does this by inducing Nkx1-2 expression, which in turn triggers c-Met signaling. The liver is a central organ for homeostasis with considerable regenerative capacity.1–3Liver transplantation is the only effective therapy for advanced liver disease.However, a shortage of donor organs is a real problem across the world.Hepatocyte cell therapy is thus an attractive alternative to liver transplantation.Mature hepatocytes harbor a remarkable capacity to proliferate upon liver injury.4Liver regeneration occurs after the loss of hepatic tissue as a fundamental parameter.However, the molecular regulatory mechanisms of liver regeneration are still elusive.
Long noncoding RNAs (lncRNAs) play important roles in various biological processes, regulating gene expression by diverse mechanisms. However, how lncRNAs regulate liver repopulation is unknown. Herein, we aimed to identify lncRNAs that regulate liver repopulation and elucidate the signaling pathways involved. Herein, we performed 70% partial hepatectomy in wild-type and gene knockout mice. We then performed transcriptomic analyses to identify a divergent lncRNA termed lncHand2 that is highly expressed during liver regeneration. LncHand2 is constitutively expressed in the nuclei of pericentral hepatocytes in mouse and human livers. LncHand2 knockout abrogates liver regeneration and repopulation capacity. Mechanistically, lncHand2 recruits the Ino80 remodeling complex to initiate expression of Nkx1-2 in trans, which triggers c-Met (Met) expression in hepatocytes. Finally, knockout of both Nkx1-2 and c-Met causes more severe liver injury and poorer repopulation ability. Thus, lncHand2 promotes liver repopulation via initiating Nkx1-2-induced c-Met signaling. Our findings reveal that lncHand2 acts as a critical mediator regulating liver repopulation. It does this by inducing Nkx1-2 expression, which in turn triggers c-Met signaling. Long noncoding RNAs (lncRNAs) are a less well classified group of RNA transcripts with no apparent protein-coding potential.5Actually, lncRNAs are more tissue- or cell type-specific than protein-coding genes, suggesting they have distinct biological functions in physiological and pathological settings.6Specific expression patterns of lncRNAs coordinate cell state, development, differentiation, and disease.7LncRNAs play important roles in various biological processes and regulate gene expression by diverse mechanisms.5,8It has been reported that lncRNAs are implicated in oncogenesis and metastasis.9We recently defined several lncRNAs, including lncTCF7, lnc-β-Catm and lncBRM, that are involved in the self-renewal maintenance of liver cancer stem cells.10,11However, how lncRNAs regulate liver regeneration remains unclear.
In Iceland a nationwide program has been launched offering direct-acting antiviral (DAA) treatment for everyone living with hepatitis C virus (HCV). We estimate (i) the time and treatment scale-up required to achieve the World Health Organization’s HCV elimination target of an 80% reduction in incidence; and (ii) the ongoing frequency of HCV testing and harm reduction coverage among people who inject drugs (PWID) required to minimize the likelihood of future HCV outbreaks occurring. We used a dynamic compartmental model of HCV transmission, liver disease progression and the HCV cascade of care, calibrated to reproduce the epidemic of HCV in Iceland. The model was stratified according to injecting drug use status, age and stage of engagement. Four scenarios were considered for the projections. The model estimated that an 80% reduction in domestic HCV incidence was achievable by 2030, 2025 or 2020 if a minimum of 55/1,000, 75/1,000 and 188/1,000 PWID were treated per year, respectively (a total of 22, 30 and 75 of the estimated 400 PWID in Iceland per year, respectively). Regardless of time frame, this required an increased number of PWID to be diagnosed to generate enough treatment demand, or a 20% scale-up of harm reduction services to complement treatment-as-prevention incidence reductions. When DAA scale-up was combined with annual antibody testing of PWID, the incidence reduction target was reached by 2024. Treatment scale-up with no other changes to current testing and harm reduction services reduced the basic reproduction number of HCV from 1.08 to 0.59, indicating that future outbreaks would be unlikely. HCV elimination in Iceland is achievable by 2020 with some additional screening of PWID. Maintaining current monitoring and harm reduction services while providing ongoing access to DAA therapy for people diagnosed with HCV would ensure that outbreaks are unlikely to occur once elimination targets have been reached. The recent availability of highly tolerable direct-acting antiviral (DAA) treatments for hepatitis C virus (HCV) has led to the development of World Health Organization (WHO) HCV elimination targets,1 which propose an 80% reduction in HCV incidence and a 65% reduction in HCV-related mortality by 2030.Many countries are currently either formulating HCV strategies or determining what resources and policies will be required to reach elimination targets.
In Iceland a nationwide program has been launched offering direct-acting antiviral (DAA) treatment for everyone living with hepatitis C virus (HCV). We estimate (i) the time and treatment scale-up required to achieve the World Health Organization’s HCV elimination target of an 80% reduction in incidence; and (ii) the ongoing frequency of HCV testing and harm reduction coverage among people who inject drugs (PWID) required to minimize the likelihood of future HCV outbreaks occurring. We used a dynamic compartmental model of HCV transmission, liver disease progression and the HCV cascade of care, calibrated to reproduce the epidemic of HCV in Iceland. The model was stratified according to injecting drug use status, age and stage of engagement. Four scenarios were considered for the projections. The model estimated that an 80% reduction in domestic HCV incidence was achievable by 2030, 2025 or 2020 if a minimum of 55/1,000, 75/1,000 and 188/1,000 PWID were treated per year, respectively (a total of 22, 30 and 75 of the estimated 400 PWID in Iceland per year, respectively). Regardless of time frame, this required an increased number of PWID to be diagnosed to generate enough treatment demand, or a 20% scale-up of harm reduction services to complement treatment-as-prevention incidence reductions. When DAA scale-up was combined with annual antibody testing of PWID, the incidence reduction target was reached by 2024. Treatment scale-up with no other changes to current testing and harm reduction services reduced the basic reproduction number of HCV from 1.08 to 0.59, indicating that future outbreaks would be unlikely. HCV elimination in Iceland is achievable by 2020 with some additional screening of PWID. Maintaining current monitoring and harm reduction services while providing ongoing access to DAA therapy for people diagnosed with HCV would ensure that outbreaks are unlikely to occur once elimination targets have been reached. Iceland is an island nation with a total population of approximately 332,0002 and an estimated 880–1,300 people living with chronic HCV,3,4 with domestic HCV transmission occurring primarily among people who inject drugs (PWID).5–7In 2016, a nationwide program was launched offering DAA treatments at no cost for the entire population living with HCV,8 which may lead to Iceland becoming the first country to achieve HCV elimination.
In Iceland a nationwide program has been launched offering direct-acting antiviral (DAA) treatment for everyone living with hepatitis C virus (HCV). We estimate (i) the time and treatment scale-up required to achieve the World Health Organization’s HCV elimination target of an 80% reduction in incidence; and (ii) the ongoing frequency of HCV testing and harm reduction coverage among people who inject drugs (PWID) required to minimize the likelihood of future HCV outbreaks occurring. We used a dynamic compartmental model of HCV transmission, liver disease progression and the HCV cascade of care, calibrated to reproduce the epidemic of HCV in Iceland. The model was stratified according to injecting drug use status, age and stage of engagement. Four scenarios were considered for the projections. The model estimated that an 80% reduction in domestic HCV incidence was achievable by 2030, 2025 or 2020 if a minimum of 55/1,000, 75/1,000 and 188/1,000 PWID were treated per year, respectively (a total of 22, 30 and 75 of the estimated 400 PWID in Iceland per year, respectively). Regardless of time frame, this required an increased number of PWID to be diagnosed to generate enough treatment demand, or a 20% scale-up of harm reduction services to complement treatment-as-prevention incidence reductions. When DAA scale-up was combined with annual antibody testing of PWID, the incidence reduction target was reached by 2024. Treatment scale-up with no other changes to current testing and harm reduction services reduced the basic reproduction number of HCV from 1.08 to 0.59, indicating that future outbreaks would be unlikely. HCV elimination in Iceland is achievable by 2020 with some additional screening of PWID. Maintaining current monitoring and harm reduction services while providing ongoing access to DAA therapy for people diagnosed with HCV would ensure that outbreaks are unlikely to occur once elimination targets have been reached. Iceland’s geographical isolation and relatively small population – comparable in size to many cities globally – makes it an important case study.In general, geographically-targeted policies to reduce transmission among PWID are best implemented at a city rather than country level to account for local needs; for example, setting up testing and treatment programs, needle and syringe programs (NSPs) and opioid substitution therapy (OST) in consultation with local healthcare and community service providers.9,10In larger countries where neighbouring cities can have different program coverages or HCV epidemiology, human mobility and drug market interactions across jurisdictions may influence elimination efforts in unknown ways.It is difficult to predict how inter-city interactions would influence such an attempt at elimination if one city has an intensive elimination program and a neighbouring city has a more limited approach.Thus, the questions arise of what the first city needs to implement to achieve the WHO HCV elimination targets, as well as what further needs to be done to maintain control of the epidemic.Iceland provides a benchmark example where these factors are minimized due to its geographical isolation, and therefore a reference case for answering this question in countries with similar HCV epidemiology to Iceland but a larger population and geographical area.
The accurate diagnosis of occult hepatitis B virus (HBV) infection (OBI) requires the demonstration of HBV DNA in liver biopsies of hepatitis B surface antigen-negative individuals. However, in clinical practice a latent OBI is deduced by the finding of the antibody to the hepatitis B core antigen (anti-HBc). We investigated the true prevalence of OBI and the molecular features of intrahepatic HBV in anti-HBc-positive individuals. The livers of 100 transplant donors (median age 68.2 years; 64 males, 36 females) positive for anti-HBc at standard serologic testing, were examined for total HBV DNA by nested-PCR and for the HBV covalently closed circular DNA (HBV cccDNA) with an in-house droplet digital PCR assay (ddPCR) (Linearity: R2 = 0.9998; lower limit of quantitation and detection of 2.4 and 0.8 copies/105 cells, respectively). A total of 52% (52/100) of the individuals studied were found to have OBI. cccDNA was found in 52% (27/52) of the OBI-positive, with a median 13 copies/105 cells (95% CI 5–25). Using an assay specific for anti-HBc of IgG class, the median antibody level was significantly higher in HBV cccDNA-positive than negative donors (17.0 [7.0–39.2] vs. 5.7 [3.6–9.7] cut-off index [COI], respectively, p = 0.007). By multivariate analysis, an anti-HBc IgG value above 4.4 COI was associated with the finding of intrahepatic HBV cccDNA (odds ratio 8.516, p = 0.009); a lower value ruled out its presence with a negative predictive value of 94.6%. With a new in-house ddPCR-based method, intrahepatic HBV cccDNA was detectable in quantifiable levels in about half of the OBI cases examined. The titer of anti-HBc IgG may be a useful surrogate to predict the risk of OBI reactivation in immunosuppressed patients. Occult hepatitis B virus (HBV) infection (OBI) refers to the presence of intrahepatic HBV DNA in the absence of detectable hepatitis B surface antigen (HBsAg).1OBI is secondary to overt HBV infections; it guarantees the persistence of the virus in a cryptic form protected from the immune response of the host.The virologic key is the covalently closed circular DNA (cccDNA), an HBV DNA form generated as a plasmid-like episome from the protein-linked relaxed circular DNA genome; it resides in the nucleus of infected cells and gives rise to viral sequences, which act as a transcription template for all viral RNAs.2
The accurate diagnosis of occult hepatitis B virus (HBV) infection (OBI) requires the demonstration of HBV DNA in liver biopsies of hepatitis B surface antigen-negative individuals. However, in clinical practice a latent OBI is deduced by the finding of the antibody to the hepatitis B core antigen (anti-HBc). We investigated the true prevalence of OBI and the molecular features of intrahepatic HBV in anti-HBc-positive individuals. The livers of 100 transplant donors (median age 68.2 years; 64 males, 36 females) positive for anti-HBc at standard serologic testing, were examined for total HBV DNA by nested-PCR and for the HBV covalently closed circular DNA (HBV cccDNA) with an in-house droplet digital PCR assay (ddPCR) (Linearity: R2 = 0.9998; lower limit of quantitation and detection of 2.4 and 0.8 copies/105 cells, respectively). A total of 52% (52/100) of the individuals studied were found to have OBI. cccDNA was found in 52% (27/52) of the OBI-positive, with a median 13 copies/105 cells (95% CI 5–25). Using an assay specific for anti-HBc of IgG class, the median antibody level was significantly higher in HBV cccDNA-positive than negative donors (17.0 [7.0–39.2] vs. 5.7 [3.6–9.7] cut-off index [COI], respectively, p = 0.007). By multivariate analysis, an anti-HBc IgG value above 4.4 COI was associated with the finding of intrahepatic HBV cccDNA (odds ratio 8.516, p = 0.009); a lower value ruled out its presence with a negative predictive value of 94.6%. With a new in-house ddPCR-based method, intrahepatic HBV cccDNA was detectable in quantifiable levels in about half of the OBI cases examined. The titer of anti-HBc IgG may be a useful surrogate to predict the risk of OBI reactivation in immunosuppressed patients. OBI has clinical significance; it can reactivate hepatitis B when the immune response of the host is compromised, as in liver transplant patients or those undergoing chemotherapy.It may also accelerate the progression of hepatic fibrosis in patients with chronic hepatitis C and has been considered a risk factor for hepatocellular carcinoma.3
The accurate diagnosis of occult hepatitis B virus (HBV) infection (OBI) requires the demonstration of HBV DNA in liver biopsies of hepatitis B surface antigen-negative individuals. However, in clinical practice a latent OBI is deduced by the finding of the antibody to the hepatitis B core antigen (anti-HBc). We investigated the true prevalence of OBI and the molecular features of intrahepatic HBV in anti-HBc-positive individuals. The livers of 100 transplant donors (median age 68.2 years; 64 males, 36 females) positive for anti-HBc at standard serologic testing, were examined for total HBV DNA by nested-PCR and for the HBV covalently closed circular DNA (HBV cccDNA) with an in-house droplet digital PCR assay (ddPCR) (Linearity: R2 = 0.9998; lower limit of quantitation and detection of 2.4 and 0.8 copies/105 cells, respectively). A total of 52% (52/100) of the individuals studied were found to have OBI. cccDNA was found in 52% (27/52) of the OBI-positive, with a median 13 copies/105 cells (95% CI 5–25). Using an assay specific for anti-HBc of IgG class, the median antibody level was significantly higher in HBV cccDNA-positive than negative donors (17.0 [7.0–39.2] vs. 5.7 [3.6–9.7] cut-off index [COI], respectively, p = 0.007). By multivariate analysis, an anti-HBc IgG value above 4.4 COI was associated with the finding of intrahepatic HBV cccDNA (odds ratio 8.516, p = 0.009); a lower value ruled out its presence with a negative predictive value of 94.6%. With a new in-house ddPCR-based method, intrahepatic HBV cccDNA was detectable in quantifiable levels in about half of the OBI cases examined. The titer of anti-HBc IgG may be a useful surrogate to predict the risk of OBI reactivation in immunosuppressed patients. An accurate diagnosis of OBI would require a liver biopsy to measure the intrahepatic HBV DNA.Yet obtaining liver specimens is difficult.In practice, recognition of OBI is based on the binding of the antibody to the hepatitis B core antigen (anti-HBc), on the premise that this reactivity represents a serological scar to a clinically resolved exposure to HBV and may therefore be an indirect marker of a latent HBV infection.4Intrahepatic HBV cccDNA is resistant to antivirals and cannot be eradicated,5 but it is possible to prevent OBI reactivation by prophylaxis with HBV antivirals.6This strategy is recommended for anti-HBc-positive patients undergoing pharmacological immunosuppression; in default of virologic data, the individual indication to prophylaxis is not determined by parameters of HBV infectivity but by the immunosuppressive potential of therapy.
The liver is the main hematopoietic site in embryos, becoming a crucial organ in both immunity and metabolism in adults. However, how the liver adapts both the immune system and enzymatic profile to challenges in the postnatal period remains elusive. We aimed to identify the mechanisms underlying this adaptation. We analyzed liver samples from mice on day 0 after birth until adulthood. Human biopsies from newborns and adults were also examined. Liver immune cells were phenotyped using mass cytometry (CyTOF) and expression of several genes belonging to immune and metabolic pathways were measured. Mortality rate, bacteremia and hepatic bacterial retention after E. coli challenge were analyzed using intravital and in vitro approaches. In a set of experiments, mice were prematurely weaned and the impact on gene expression of metabolic pathways was evaluated. Human and mouse newborns have a sharply different hepatic cellular composition and arrangement compared to adults. We also found that myeloid cells and immature B cells primarily compose the neonatal hepatic immune system. Although neonatal mice were more susceptible to infections, a rapid evolution to an efficient immune response was observed. Concomitantly, newborns displayed a reduction of several macronutrient metabolic functions and the normal expression level of enzymes belonging to lipid and carbohydrate metabolism was reached around the weaning period. Interestingly, early weaning profoundly disturbed the expression of several hepatic metabolic pathways, providing novel insights into how dietary schemes affect the metabolic maturation of the liver. In newborns, the immune and metabolic profiles of the liver are dramatically different to those of the adult liver, which can be explained by the differences in the liver cell repertoire and phenotype. Also, dietary and antigen cues may be crucial to guide liver development during the postnatal phase. The liver has multiple functions, becoming vitally important from the gestational phase.1The primordial liver is formed in the initial stages of embryonic development (week 4 in humans and E8.5 in mouse), and it is the destination of several immune cell precursors that arise from the yolk sac.2Fetal liver harbors different hepatic progenitor cells and a great number of other mesenchyme-derived cells, including immune cells in different stages of maturation.3In fact, macrophages, hematopoietic stem cells (HSCs), endothelial and mesenchymal stem cells compose a complex cellular repertoire during the major part of hepatic embryogenesis.4The dynamics by which HSCs seed the liver has become a growing field of interest,5 and understanding how tissue macrophages mature within organs and are maintained throughout life may have a significant impact on both basic and clinical investigations.6
The liver is the main hematopoietic site in embryos, becoming a crucial organ in both immunity and metabolism in adults. However, how the liver adapts both the immune system and enzymatic profile to challenges in the postnatal period remains elusive. We aimed to identify the mechanisms underlying this adaptation. We analyzed liver samples from mice on day 0 after birth until adulthood. Human biopsies from newborns and adults were also examined. Liver immune cells were phenotyped using mass cytometry (CyTOF) and expression of several genes belonging to immune and metabolic pathways were measured. Mortality rate, bacteremia and hepatic bacterial retention after E. coli challenge were analyzed using intravital and in vitro approaches. In a set of experiments, mice were prematurely weaned and the impact on gene expression of metabolic pathways was evaluated. Human and mouse newborns have a sharply different hepatic cellular composition and arrangement compared to adults. We also found that myeloid cells and immature B cells primarily compose the neonatal hepatic immune system. Although neonatal mice were more susceptible to infections, a rapid evolution to an efficient immune response was observed. Concomitantly, newborns displayed a reduction of several macronutrient metabolic functions and the normal expression level of enzymes belonging to lipid and carbohydrate metabolism was reached around the weaning period. Interestingly, early weaning profoundly disturbed the expression of several hepatic metabolic pathways, providing novel insights into how dietary schemes affect the metabolic maturation of the liver. In newborns, the immune and metabolic profiles of the liver are dramatically different to those of the adult liver, which can be explained by the differences in the liver cell repertoire and phenotype. Also, dietary and antigen cues may be crucial to guide liver development during the postnatal phase. Although much is known about hepatic embryonic development and the chronology of hematopoiesis during the gestational phase,7 it is still largely elusive how the liver adapts both the immune system and enzymatic profile to react to a plethora of challenges that a newborn faces in the initial periods after birth.In fact, abrupt changes in both microbiota and diet are expected during this phase; thus the transition from intra- to extra-uterine life may demand rapid, complex and well-orchestrated steps to ensure neonatal survival and adaptation.8Advances in this field will help us understand why several infectious diseases that affect the liver lead to higher mortality rates in infants (i.e., Escherichia coli9 and Plasmodium falciparum10) and the putative long-term impacts of inappropriate nutrition on metabolism in children.For instance, because the liver is located in a circulatory interface between the gastrointestinal tract and the systemic circulation, it is expected that the food quality, the rate of calorie intake and the neonate relationship with the gut microbiota dramatically affect these two mainstreams of hepatic developmental fate.
Patients with acute-on-chronic liver failure (ACLF) can be listed for liver transplantation (LT) because LT is the only curative treatment option. We evaluated whether the clinical course of ACLF, particularly ACLF-3, between the time of listing and LT affects 1-year post-transplant survival. We identified patients from the United Network for Organ Sharing database who were transplanted within 28 days of listing and categorized them by ACLF grade at waitlist registration and LT, according to the EASL-CLIF definition. A total of 3,636 patients listed with ACLF-3 underwent LT within 28 days. Among those transplanted, 892 (24.5%) recovered to no ACLF or ACLF grade 1 or 2 (ACLF 0–2) and 2,744 (75.5%) had ACLF-3 at transplantation. One-year survival was 82.0% among those transplanted with ACLF-3 vs. 88.2% among those improving to ACLF 0–2 (p <0.001). Conversely, the survival of patients listed with ACLF 0–2 who progressed to ACLF-3 at LT (n = 2,265) was significantly lower than that of recipients who remained at ACLF 0–2 (n = 17,631) at the time of LT (83.8% vs. 90.2%, p <0.001). Cox modeling demonstrated that recovery from ACLF-3 to ACLF 0–2 at LT was associated with reduced 1-year mortality after transplantation (hazard ratio 0.65; 95% CI 0.53–0.78). Improvement in circulatory failure, brain failure, and removal from mechanical ventilation were also associated with reduced post-LT mortality. Among patients >60 years of age, 1-year survival was significantly higher among those who improved from ACLF-3 to ACLF 0–2 than among those who did not. Improvement from ACLF-3 at listing to ACLF 0–2 at transplantation enhances post-LT survival, particularly in those who recovered from circulatory or brain failure, or were removed from the mechanical ventilator. The beneficial effect of improved ACLF on post-LT survival was also observed among patients >60 years of age. Acute-on-chronic liver failure (ACLF) is associated with severe systemic inflammation and is characterized by acute hepatic decompensation, development of organ failures, and high 28-day mortality.1–3The short-term mortality of patients with ACLF grade 3 (ACLF-3), defined as the development of 3 or more organ failures,1 is particularly high, approaching 80% at 28-days4–6 and possibly surpassing that of acute liver failure.7In certain patients with ACLF-3, liver transplantation (LT) may be the only viable treatment.However, data regarding LT for individuals with ACLF-3 indicate a reduced survival probability, ranging from less than 50%8,9 to 80% at 1 year.10,11Although this suggests a greater likelihood of survival than supportive care without transplantation, the limited availability of donor organs necessitates judicious selection of transplant recipients.
Patients with acute-on-chronic liver failure (ACLF) can be listed for liver transplantation (LT) because LT is the only curative treatment option. We evaluated whether the clinical course of ACLF, particularly ACLF-3, between the time of listing and LT affects 1-year post-transplant survival. We identified patients from the United Network for Organ Sharing database who were transplanted within 28 days of listing and categorized them by ACLF grade at waitlist registration and LT, according to the EASL-CLIF definition. A total of 3,636 patients listed with ACLF-3 underwent LT within 28 days. Among those transplanted, 892 (24.5%) recovered to no ACLF or ACLF grade 1 or 2 (ACLF 0–2) and 2,744 (75.5%) had ACLF-3 at transplantation. One-year survival was 82.0% among those transplanted with ACLF-3 vs. 88.2% among those improving to ACLF 0–2 (p <0.001). Conversely, the survival of patients listed with ACLF 0–2 who progressed to ACLF-3 at LT (n = 2,265) was significantly lower than that of recipients who remained at ACLF 0–2 (n = 17,631) at the time of LT (83.8% vs. 90.2%, p <0.001). Cox modeling demonstrated that recovery from ACLF-3 to ACLF 0–2 at LT was associated with reduced 1-year mortality after transplantation (hazard ratio 0.65; 95% CI 0.53–0.78). Improvement in circulatory failure, brain failure, and removal from mechanical ventilation were also associated with reduced post-LT mortality. Among patients >60 years of age, 1-year survival was significantly higher among those who improved from ACLF-3 to ACLF 0–2 than among those who did not. Improvement from ACLF-3 at listing to ACLF 0–2 at transplantation enhances post-LT survival, particularly in those who recovered from circulatory or brain failure, or were removed from the mechanical ventilator. The beneficial effect of improved ACLF on post-LT survival was also observed among patients >60 years of age. Given the lower patient survival rates associated with transplantation for ACLF-3 than for no ACLF or ACLF grades 1 and 2 (ACLF 0–2), further analysis is warranted to optimize post-LT survival rates in this population.One approach is to direct care based on the recovery of organ failure(s) (both number and type) prior to transplantation.In the non-transplant setting, data from the CANONIC study suggested that ACLF is a dynamic syndrome and a reduction in ACLF grade improves spontaneous survival, whereas an increase in the severity of ACLF portends high mortality.1,12In a small proof of concept retrospective investigation, greater post-LT survival was observed among patients with ACLF who had recovery of at least 1 organ system failure at the time of transplantation.13However, given the small number of patients with ACLF-3 (n = 29) in that study, additional research remains necessary to determine whether improvement in organ system failures augments post-LT survival, particularly among patients with ACLF-3 who have the greatest need for LT but the lowest post-LT survival.
Although the role of inflammation to combat infection is known, the contribution of metabolic changes in response to sepsis is poorly understood. Sepsis induces the release of lipid mediators, many of which activate nuclear receptors such as the peroxisome proliferator-activated receptor (PPAR)α, which controls both lipid metabolism and inflammation. We aimed to elucidate the previously unknown role of hepatic PPARα in the response to sepsis. Sepsis was induced by intraperitoneal injection of Escherichia coli in different models of cell-specific Ppara-deficiency and their controls. The systemic and hepatic metabolic response was analyzed using biochemical, transcriptomic and functional assays. PPARα expression was analyzed in livers from elective surgery and critically ill patients and correlated with hepatic gene expression and blood parameters. Both whole body and non-hematopoietic Ppara-deficiency in mice decreased survival upon bacterial infection. Livers of septic Ppara-deficient mice displayed an impaired metabolic shift from glucose to lipid utilization resulting in more severe hypoglycemia, impaired induction of hyperketonemia and increased steatosis due to lower expression of genes involved in fatty acid catabolism and ketogenesis. Hepatocyte-specific deletion of PPARα impaired the metabolic response to sepsis and was sufficient to decrease survival upon bacterial infection. Hepatic PPARA expression was lower in critically ill patients and correlated positively with expression of lipid metabolism genes, but not with systemic inflammatory markers. During sepsis, Ppara-deficiency in hepatocytes is deleterious as it impairs the adaptive metabolic shift from glucose to FA utilization. Metabolic control by PPARα in hepatocytes plays a key role in the host defense against infection. Sepsis, the systemic inflammatory response to poorly controlled infection, causes significant morbidity/mortality.1Sepsis is often complicated by multiple organ failure, requiring intensive care.Recently, mortality in sepsis has decreased largely due to improved supportive strategies for critically ill patients, such as mechanical ventilation, renal replacement therapy and antibiotics.While current therapeutic strategies targeting the inflammatory response have been disappointing,1,2 metabolic interventions, such as intensive insulin therapy3 and controlled caloric deficit through delayed administration of parenteral nutrition,4 have shown some promise, suggesting that appropriate adaptation of energy metabolism contributes to proper defense against pathogens.5
Although the role of inflammation to combat infection is known, the contribution of metabolic changes in response to sepsis is poorly understood. Sepsis induces the release of lipid mediators, many of which activate nuclear receptors such as the peroxisome proliferator-activated receptor (PPAR)α, which controls both lipid metabolism and inflammation. We aimed to elucidate the previously unknown role of hepatic PPARα in the response to sepsis. Sepsis was induced by intraperitoneal injection of Escherichia coli in different models of cell-specific Ppara-deficiency and their controls. The systemic and hepatic metabolic response was analyzed using biochemical, transcriptomic and functional assays. PPARα expression was analyzed in livers from elective surgery and critically ill patients and correlated with hepatic gene expression and blood parameters. Both whole body and non-hematopoietic Ppara-deficiency in mice decreased survival upon bacterial infection. Livers of septic Ppara-deficient mice displayed an impaired metabolic shift from glucose to lipid utilization resulting in more severe hypoglycemia, impaired induction of hyperketonemia and increased steatosis due to lower expression of genes involved in fatty acid catabolism and ketogenesis. Hepatocyte-specific deletion of PPARα impaired the metabolic response to sepsis and was sufficient to decrease survival upon bacterial infection. Hepatic PPARA expression was lower in critically ill patients and correlated positively with expression of lipid metabolism genes, but not with systemic inflammatory markers. During sepsis, Ppara-deficiency in hepatocytes is deleterious as it impairs the adaptive metabolic shift from glucose to FA utilization. Metabolic control by PPARα in hepatocytes plays a key role in the host defense against infection. The early pro-inflammatory response to infection requires glycolysis and non-insulin-mediated glucose uptake to rapidly meet the high energy demand of innate immune cells.6In this phase, hepatic gluconeogenesis increases to maintain plasma glucose concentrations.7As sepsis sets in, plasma free fatty acid (FFA) and glycerol levels rise due to enhanced adipose tissue (AT) lipolysis.7In response, organs such as the liver, muscle and heart, shift from glucose to fatty acid utilization8 and enhance mitochondrial activity.9
Although the role of inflammation to combat infection is known, the contribution of metabolic changes in response to sepsis is poorly understood. Sepsis induces the release of lipid mediators, many of which activate nuclear receptors such as the peroxisome proliferator-activated receptor (PPAR)α, which controls both lipid metabolism and inflammation. We aimed to elucidate the previously unknown role of hepatic PPARα in the response to sepsis. Sepsis was induced by intraperitoneal injection of Escherichia coli in different models of cell-specific Ppara-deficiency and their controls. The systemic and hepatic metabolic response was analyzed using biochemical, transcriptomic and functional assays. PPARα expression was analyzed in livers from elective surgery and critically ill patients and correlated with hepatic gene expression and blood parameters. Both whole body and non-hematopoietic Ppara-deficiency in mice decreased survival upon bacterial infection. Livers of septic Ppara-deficient mice displayed an impaired metabolic shift from glucose to lipid utilization resulting in more severe hypoglycemia, impaired induction of hyperketonemia and increased steatosis due to lower expression of genes involved in fatty acid catabolism and ketogenesis. Hepatocyte-specific deletion of PPARα impaired the metabolic response to sepsis and was sufficient to decrease survival upon bacterial infection. Hepatic PPARA expression was lower in critically ill patients and correlated positively with expression of lipid metabolism genes, but not with systemic inflammatory markers. During sepsis, Ppara-deficiency in hepatocytes is deleterious as it impairs the adaptive metabolic shift from glucose to FA utilization. Metabolic control by PPARα in hepatocytes plays a key role in the host defense against infection. PPARα is a nuclear receptor activated by fatty acids (FAs) and derivatives regulating both metabolism and inflammation.10PPARα is highly expressed in metabolic tissues, such as the liver, heart, kidney and muscle, the vasculature (endothelial cells, smooth muscle cells) as well as in the immune system (monocytes/macrophages, neutrophils, etc.).11During the fed-to-fasted transition, hepatic PPARα expression increases12 and is activated by the influx of AT-released FFAs, orchestrating a shift from glucose to FA utilization driving ketone body and glucose production by the liver.12,13
Heat shock protein (Hsp) 72 is a molecular chaperone that has broad cytoprotective functions and is upregulated in response to stress. To determine its hepatic functions, we studied its expression in human liver disorders and its biological significance in newly generated transgenic animals. Double transgenic mice overexpressing Hsp72 (gene Hspa1a) under the control of a tissue-specific tetracycline-inducible system (Hsp72-LAP mice) were produced. Acute liver injury was induced by a single injection of acetaminophen (APAP). Feeding with either a methionine choline-deficient (MCD; 8 weeks) or a 3,5-diethoxycarbonyl-1,4-dihydrocollidine-supplemented diet (DDC; 12 weeks) was used to induce lipotoxic injury and Mallory–Denk body (MDB) formation, respectively. Primary hepatocytes were treated with palmitic acid. Patients with non-alcoholic steatohepatitis and chronic hepatitis C infection displayed elevated HSP72 levels. These levels increased with the extent of hepatic inflammation and HSP72 expression was induced after treatment with either interleukin (IL)-1β or IL-6. Hsp72-LAP mice exhibited robust, hepatocyte-specific Hsp72 overexpression. Primary hepatocytes from these animals were more resistant to isolation-induced stress and Hsp72-LAP mice displayed lower levels of hepatic injury in vivo. Mice overexpressing Hsp72 had fewer APAP protein adducts and were protected from oxidative stress and APAP-/MCD-induced cell death. Hsp72-LAP mice and/or hepatocytes displayed significantly attenuated Jnk activation. Overexpression of Hsp72 did not affect steatosis or the extent of MDB formation. Our results demonstrate that HSP72 induction occurs in human liver disease, thus, HSP72 represents an attractive therapeutic target owing to its broad hepatoprotective functions. The liver has an important role in protein and lipid metabolism.It is a key hub for fatty acid synthesis and lipid circulation, while also producing most plasma proteins.1This tremendous metabolic activity results in severe oxidative stress that becomes apparent in multiple liver disorders.For example, protein misfolding with the formation of cytoplasmic aggregates, also referred to as Mallory–Denk bodies (MDBs), is a characteristic feature of specific liver diseases, such as alcoholic and non-alcoholic steatohepatitis (NASH).2Although the exact pathogenesis of MDB formation remains unknown, lipotoxicity, which is a key feature of both alcoholic liver disease and NASH, appears to have an important role.1,3Another source of proteotoxic stress stems from the fact that most xenobiotics are detoxified by the liver.As a prime example, overdose of the common analgesic drug acetaminophen (APAP) leads to depletion of the endogenous antioxidant glutathione and to the formation of APAP-protein adducts, which are key mediators of APAP hepatotoxicity.4
Heat shock protein (Hsp) 72 is a molecular chaperone that has broad cytoprotective functions and is upregulated in response to stress. To determine its hepatic functions, we studied its expression in human liver disorders and its biological significance in newly generated transgenic animals. Double transgenic mice overexpressing Hsp72 (gene Hspa1a) under the control of a tissue-specific tetracycline-inducible system (Hsp72-LAP mice) were produced. Acute liver injury was induced by a single injection of acetaminophen (APAP). Feeding with either a methionine choline-deficient (MCD; 8 weeks) or a 3,5-diethoxycarbonyl-1,4-dihydrocollidine-supplemented diet (DDC; 12 weeks) was used to induce lipotoxic injury and Mallory–Denk body (MDB) formation, respectively. Primary hepatocytes were treated with palmitic acid. Patients with non-alcoholic steatohepatitis and chronic hepatitis C infection displayed elevated HSP72 levels. These levels increased with the extent of hepatic inflammation and HSP72 expression was induced after treatment with either interleukin (IL)-1β or IL-6. Hsp72-LAP mice exhibited robust, hepatocyte-specific Hsp72 overexpression. Primary hepatocytes from these animals were more resistant to isolation-induced stress and Hsp72-LAP mice displayed lower levels of hepatic injury in vivo. Mice overexpressing Hsp72 had fewer APAP protein adducts and were protected from oxidative stress and APAP-/MCD-induced cell death. Hsp72-LAP mice and/or hepatocytes displayed significantly attenuated Jnk activation. Overexpression of Hsp72 did not affect steatosis or the extent of MDB formation. Our results demonstrate that HSP72 induction occurs in human liver disease, thus, HSP72 represents an attractive therapeutic target owing to its broad hepatoprotective functions. The above examples highlight the importance of a functioning proteostatic network in the liver.While recent research has focused mainly on protein degradation pathways, less emphasis has been placed on protein folding and repair,5 which are often facilitated by molecular chaperones from the family of heat shock proteins (HSPs).6Hsp70 proteins represent the largest Hsp subfamily, comprising 7 and 13 functional genes in mice and humans, respectively.7Among them, Hsp72 constitutes the classic, stress-inducible, cytoplasmic Hsp that serves as a helper during stressful conditions, whereas Hsp27 and GRP78, constitute other, well-established, inducible proteins.5,7Hsp72 displays versatile cytoprotective properties.5For example, its overexpression protects against protein aggregate formation in experimental neurodegenerative disorders and against myocardial ischemia or obesity-induced insulin resistance.8–10The cytoprotective properties of Hsp72 are of direct pharmacological relevance because Hsp72 production can be induced by various compounds, such as geranylgeranylacetone or its derivatives,11 or mimicked by chemical chaperones.12
Heat shock protein (Hsp) 72 is a molecular chaperone that has broad cytoprotective functions and is upregulated in response to stress. To determine its hepatic functions, we studied its expression in human liver disorders and its biological significance in newly generated transgenic animals. Double transgenic mice overexpressing Hsp72 (gene Hspa1a) under the control of a tissue-specific tetracycline-inducible system (Hsp72-LAP mice) were produced. Acute liver injury was induced by a single injection of acetaminophen (APAP). Feeding with either a methionine choline-deficient (MCD; 8 weeks) or a 3,5-diethoxycarbonyl-1,4-dihydrocollidine-supplemented diet (DDC; 12 weeks) was used to induce lipotoxic injury and Mallory–Denk body (MDB) formation, respectively. Primary hepatocytes were treated with palmitic acid. Patients with non-alcoholic steatohepatitis and chronic hepatitis C infection displayed elevated HSP72 levels. These levels increased with the extent of hepatic inflammation and HSP72 expression was induced after treatment with either interleukin (IL)-1β or IL-6. Hsp72-LAP mice exhibited robust, hepatocyte-specific Hsp72 overexpression. Primary hepatocytes from these animals were more resistant to isolation-induced stress and Hsp72-LAP mice displayed lower levels of hepatic injury in vivo. Mice overexpressing Hsp72 had fewer APAP protein adducts and were protected from oxidative stress and APAP-/MCD-induced cell death. Hsp72-LAP mice and/or hepatocytes displayed significantly attenuated Jnk activation. Overexpression of Hsp72 did not affect steatosis or the extent of MDB formation. Our results demonstrate that HSP72 induction occurs in human liver disease, thus, HSP72 represents an attractive therapeutic target owing to its broad hepatoprotective functions. Despite the obvious importance of chaperones, the cytoprotective potential of Hsp72 overexpression in the liver has not yet been systematically analyzed, owing to the lack of a suitable animal model.Nonetheless, several previous findings point towards the functional importance of Hsp72 in the liver.For example, injection of arsenite, an established Hsp72 inducer,13 protected against hepatic ischemia-reperfusion injury as did preconditioning with atrial natriuretic peptide,14 whereas geranylgeranylacetone administration protected from APAP hepatotoxicity.15By contrast, chronic feeding with a 3,5-diethoxycarbonyl-1,4-dihydrocollidine (DDC)-supplemented diet, used as an experimental MDB model, was associated with diminished Hsp72 levels and profound chaperone modifications.16Given that Hsp72 represents an established MDB constituent and that MDBs comprise misfolded, ubiquitinated keratins 8/18 as their major components,2,17 Hsp72 dysfunction was proposed as a key mechanism of inclusion formation.16
Induction of cross-reactive antibodies targeting conserved epitopes of the envelope proteins E1E2 is a key requirement for an hepatitis C virus vaccine. Conserved epitopes like the viral CD81-binding site are targeted by rare broadly neutralizing antibodies. However, these viral segments are occluded by variable regions and glycans. We aimed to identify antigens exposing conserved epitopes and to characterize their immunogenicity. We created hepatitis C virus variants with mutated glycosylation sites and/or hypervariable region 1 (HVR1). Exposure of the CD81 binding site and conserved epitopes was quantified by soluble CD81 and antibody interaction and neutralization assays. E2 or E1-E2 heterodimers with mutations causing epitope exposure were used to immunize mice. Vaccine-induced antibodies were examined and compared with patient-derived antibodies. Mutant viruses bound soluble CD81 and antibodies targeting the CD81 binding site with enhanced efficacy. Mice immunized with E2 or E1E2 heterodimers incorporating these modifications mounted strong, cross-binding, and non-interfering antibodies. E2-induced antibodies neutralized the autologous virus but they were not cross-neutralizing. Viruses lacking the HVR1 and selected glycosylation sites expose the CD81 binding site and cross-neutralization antibody epitopes. Recombinant E2 proteins carrying these modifications induce strong cross-binding but not cross-neutralizing antibodies. Hepatitis C virus (HCV) is a global health burden affecting approximately 71 million people worldwide.1Infection often leads to chronic hepatitis, with the subsequent risk of liver cirrhosis and hepatocellular carcinoma.Persistent HCV infection is now curable with the introduction of direct-acting antivirals.However, a prophylactic HCV vaccine is not available.Since viral re-infection is possible and as many HCV infected individuals are not diagnosed, a vaccine against HCV would facilitate global HCV eradication programs.
Induction of cross-reactive antibodies targeting conserved epitopes of the envelope proteins E1E2 is a key requirement for an hepatitis C virus vaccine. Conserved epitopes like the viral CD81-binding site are targeted by rare broadly neutralizing antibodies. However, these viral segments are occluded by variable regions and glycans. We aimed to identify antigens exposing conserved epitopes and to characterize their immunogenicity. We created hepatitis C virus variants with mutated glycosylation sites and/or hypervariable region 1 (HVR1). Exposure of the CD81 binding site and conserved epitopes was quantified by soluble CD81 and antibody interaction and neutralization assays. E2 or E1-E2 heterodimers with mutations causing epitope exposure were used to immunize mice. Vaccine-induced antibodies were examined and compared with patient-derived antibodies. Mutant viruses bound soluble CD81 and antibodies targeting the CD81 binding site with enhanced efficacy. Mice immunized with E2 or E1E2 heterodimers incorporating these modifications mounted strong, cross-binding, and non-interfering antibodies. E2-induced antibodies neutralized the autologous virus but they were not cross-neutralizing. Viruses lacking the HVR1 and selected glycosylation sites expose the CD81 binding site and cross-neutralization antibody epitopes. Recombinant E2 proteins carrying these modifications induce strong cross-binding but not cross-neutralizing antibodies. The extreme diversity of HCV is a major obstacle for vaccine development.2The HCV E1E2 proteins are essential for viral cell entry, they bind the HCV receptor CD81 and they are targets for neutralizing antibodies.Hence, immunogens based on E1E2 represent a major branch of vaccine development3,4 and numerous approaches to induce broadly neutralizing antibodies (bNabs) that target E1E2 have been explored.5For recombinant E1E2, the most advanced HCV subunit vaccine candidate, induction of robust cross-binding and cross-neutralizing antibody responses was observed in multiple animal models and in humans.4,6–8
Induction of cross-reactive antibodies targeting conserved epitopes of the envelope proteins E1E2 is a key requirement for an hepatitis C virus vaccine. Conserved epitopes like the viral CD81-binding site are targeted by rare broadly neutralizing antibodies. However, these viral segments are occluded by variable regions and glycans. We aimed to identify antigens exposing conserved epitopes and to characterize their immunogenicity. We created hepatitis C virus variants with mutated glycosylation sites and/or hypervariable region 1 (HVR1). Exposure of the CD81 binding site and conserved epitopes was quantified by soluble CD81 and antibody interaction and neutralization assays. E2 or E1-E2 heterodimers with mutations causing epitope exposure were used to immunize mice. Vaccine-induced antibodies were examined and compared with patient-derived antibodies. Mutant viruses bound soluble CD81 and antibodies targeting the CD81 binding site with enhanced efficacy. Mice immunized with E2 or E1E2 heterodimers incorporating these modifications mounted strong, cross-binding, and non-interfering antibodies. E2-induced antibodies neutralized the autologous virus but they were not cross-neutralizing. Viruses lacking the HVR1 and selected glycosylation sites expose the CD81 binding site and cross-neutralization antibody epitopes. Recombinant E2 proteins carrying these modifications induce strong cross-binding but not cross-neutralizing antibodies. HCV has evolved mechanisms to evade humoral immune responses including high functional flexibility and variability of immunogenic portions of its envelope proteins.9The highest sequence variability occurs in the first 27 amino acids of the N-terminus of E2, which is referred to as the hypervariable region 1 (HVR1), and which is dispensable for HCV infection in vitro.10,11The HVR1 is immunogenic and most patients mount antibodies targeting this region.12However, these antibodies rapidly select resistant viral variants.13Deletion of the HVR1 renders HCV more susceptible to antibody neutralization, and it increases virus binding to soluble CD81, suggesting that this region occludes key neutralization epitopes and the viral CD81 binding site.10,11HCV E1E2 heterodimers are also heavily glycosylated at multiple sites within both E1 and E2, with glycans modulating glycoprotein function and antibody neutralization.14–16Structural analyses of the E2 core domain show that the conserved CD81 binding site is surrounded by several glycosylation sites and that it overlaps with the epitopes of bNabs isolated from patients, such as HC1 and HC11.17–20(Fig. 1).Strikingly, in vitro HCV is unable to escape antibody pressure by HC1 and HC11 suggesting that immune responses targeting these epitopes may confer robust protection.21,22
Absence or low anti-HBV surface antibody (anti-HBs) is associated with an increased risk of HBV reactivation in patients with lymphoma and resolved HBV infection receiving rituximab-containing chemotherapy. Quantification of anti-HBV core antibody (anti-HBc) is a new marker associated with the natural history and treatment response of chronic HBV infection. This study investigated whether baseline anti-HBc and anti-HBs levels may better predict HBV reactivation. We prospectively measured the HBV DNA levels of patients with lymphoma and resolved HBV infection receiving rituximab–cyclophosphamide, hydroxydaunorubicin, vincristine, and prednisolone-based chemotherapy and started an antiviral therapy upon HBV reactivation, defined as a greater than 10-fold increase in HBV DNA compared with previous nadir levels. Anti-HBs and anti-HBc were quantified by a double-sandwich assay. Receiver-operating-characteristic-curve analysis was used to determine the optimal baseline anti-HBc/anti-HBs levels for predicting HBV reactivation. HBV reactivation occurred in 24 of the 197 patients enrolled, with an incidence of 11.6/100 person-years. For the 192 patients with enough serum samples for analysis, low anti-HBs (<56.48 mIU/ml) and high anti-HBc (≥6.41 IU/ml) at baseline were significantly associated with high risk of HBV reactivation (hazard ratio [HR] 8.48 and 4.52, respectively; p <0.01). The multivariate analysis indicated that (1) patients with both high anti-HBc and low anti-HBs at baseline (36 of 192 patients) had an HR of 17.29 for HBV reactivation (95% CI 3.92–76.30; p <0.001), and (2) HBV reactivation may be associated with inferior overall survival (HR 2.41; 95% CI 1.15–5.05; p = 0.02). Baseline anti-HBc/anti-HBs levels may predict HBV reactivation in these patients with lymphoma and help optimize prophylactic antiviral therapy for high-risk patients. HBV reactivation is a common complication that may lead to a life-threatening liver decompensation in patients with chronic HBV infection (HBsAg positive) who receive cytotoxic or immunosuppressive therapy.Prophylactic antiviral therapy is recommended for this patient population by guidelines of HBV management.1–4For patients with resolved HBV infection (HBsAg negative and anti-HBV core antibody [anti-HBc] positive), HBV reactivation is best characterized in patients with lymphoma who are treated with B cell-depleting agents, such as rituximab, and the incidence of HBV reactivation ranged from about 10% to 30%.5–9
Absence or low anti-HBV surface antibody (anti-HBs) is associated with an increased risk of HBV reactivation in patients with lymphoma and resolved HBV infection receiving rituximab-containing chemotherapy. Quantification of anti-HBV core antibody (anti-HBc) is a new marker associated with the natural history and treatment response of chronic HBV infection. This study investigated whether baseline anti-HBc and anti-HBs levels may better predict HBV reactivation. We prospectively measured the HBV DNA levels of patients with lymphoma and resolved HBV infection receiving rituximab–cyclophosphamide, hydroxydaunorubicin, vincristine, and prednisolone-based chemotherapy and started an antiviral therapy upon HBV reactivation, defined as a greater than 10-fold increase in HBV DNA compared with previous nadir levels. Anti-HBs and anti-HBc were quantified by a double-sandwich assay. Receiver-operating-characteristic-curve analysis was used to determine the optimal baseline anti-HBc/anti-HBs levels for predicting HBV reactivation. HBV reactivation occurred in 24 of the 197 patients enrolled, with an incidence of 11.6/100 person-years. For the 192 patients with enough serum samples for analysis, low anti-HBs (<56.48 mIU/ml) and high anti-HBc (≥6.41 IU/ml) at baseline were significantly associated with high risk of HBV reactivation (hazard ratio [HR] 8.48 and 4.52, respectively; p <0.01). The multivariate analysis indicated that (1) patients with both high anti-HBc and low anti-HBs at baseline (36 of 192 patients) had an HR of 17.29 for HBV reactivation (95% CI 3.92–76.30; p <0.001), and (2) HBV reactivation may be associated with inferior overall survival (HR 2.41; 95% CI 1.15–5.05; p = 0.02). Baseline anti-HBc/anti-HBs levels may predict HBV reactivation in these patients with lymphoma and help optimize prophylactic antiviral therapy for high-risk patients. Prophylactic antiviral therapy has been demonstrated to reduce the incidence of HBV reactivation in patients with lymphoma and resolved HBV infection who received rituximab-containing chemotherapy, and is recommended by most clinical practice guidelines.1–4Alternatively, the patients can be followed up regularly by monitoring HBV DNA and alanine aminotransferase (ALT) levels every 1–3 months, and start antiviral therapy upon HBV reactivation.7,8In HBV endemic area, either prophylactic antiviral therapy or regular HBV DNA monitoring may cause significant burden on health resource allocation because approximately 60% of the adult population has resolved HBV infections.10Therefore, the identification of high-risk patients for HBV reactivation will help in the optimization of a preventive strategy and resource allocation.
Absence or low anti-HBV surface antibody (anti-HBs) is associated with an increased risk of HBV reactivation in patients with lymphoma and resolved HBV infection receiving rituximab-containing chemotherapy. Quantification of anti-HBV core antibody (anti-HBc) is a new marker associated with the natural history and treatment response of chronic HBV infection. This study investigated whether baseline anti-HBc and anti-HBs levels may better predict HBV reactivation. We prospectively measured the HBV DNA levels of patients with lymphoma and resolved HBV infection receiving rituximab–cyclophosphamide, hydroxydaunorubicin, vincristine, and prednisolone-based chemotherapy and started an antiviral therapy upon HBV reactivation, defined as a greater than 10-fold increase in HBV DNA compared with previous nadir levels. Anti-HBs and anti-HBc were quantified by a double-sandwich assay. Receiver-operating-characteristic-curve analysis was used to determine the optimal baseline anti-HBc/anti-HBs levels for predicting HBV reactivation. HBV reactivation occurred in 24 of the 197 patients enrolled, with an incidence of 11.6/100 person-years. For the 192 patients with enough serum samples for analysis, low anti-HBs (<56.48 mIU/ml) and high anti-HBc (≥6.41 IU/ml) at baseline were significantly associated with high risk of HBV reactivation (hazard ratio [HR] 8.48 and 4.52, respectively; p <0.01). The multivariate analysis indicated that (1) patients with both high anti-HBc and low anti-HBs at baseline (36 of 192 patients) had an HR of 17.29 for HBV reactivation (95% CI 3.92–76.30; p <0.001), and (2) HBV reactivation may be associated with inferior overall survival (HR 2.41; 95% CI 1.15–5.05; p = 0.02). Baseline anti-HBc/anti-HBs levels may predict HBV reactivation in these patients with lymphoma and help optimize prophylactic antiviral therapy for high-risk patients. Low or absent anti-HBV surface antibody (anti-HBs) at baseline was the best characterized risk factor of HBV reactivation in a previous series of patients with lymphoma and resolved HBV infection.6–9However, the diagnosis criteria of HBV reactivation varied in previous studies, and some reported HBV reactivations may represent a transient HBV DNA increase that is not clinically significant.Exploration of markers that may help predict the clinical outcome of HBV reactivation will further focus the limited health-care resources to the most vulnerable patient groups.
Absence or low anti-HBV surface antibody (anti-HBs) is associated with an increased risk of HBV reactivation in patients with lymphoma and resolved HBV infection receiving rituximab-containing chemotherapy. Quantification of anti-HBV core antibody (anti-HBc) is a new marker associated with the natural history and treatment response of chronic HBV infection. This study investigated whether baseline anti-HBc and anti-HBs levels may better predict HBV reactivation. We prospectively measured the HBV DNA levels of patients with lymphoma and resolved HBV infection receiving rituximab–cyclophosphamide, hydroxydaunorubicin, vincristine, and prednisolone-based chemotherapy and started an antiviral therapy upon HBV reactivation, defined as a greater than 10-fold increase in HBV DNA compared with previous nadir levels. Anti-HBs and anti-HBc were quantified by a double-sandwich assay. Receiver-operating-characteristic-curve analysis was used to determine the optimal baseline anti-HBc/anti-HBs levels for predicting HBV reactivation. HBV reactivation occurred in 24 of the 197 patients enrolled, with an incidence of 11.6/100 person-years. For the 192 patients with enough serum samples for analysis, low anti-HBs (<56.48 mIU/ml) and high anti-HBc (≥6.41 IU/ml) at baseline were significantly associated with high risk of HBV reactivation (hazard ratio [HR] 8.48 and 4.52, respectively; p <0.01). The multivariate analysis indicated that (1) patients with both high anti-HBc and low anti-HBs at baseline (36 of 192 patients) had an HR of 17.29 for HBV reactivation (95% CI 3.92–76.30; p <0.001), and (2) HBV reactivation may be associated with inferior overall survival (HR 2.41; 95% CI 1.15–5.05; p = 0.02). Baseline anti-HBc/anti-HBs levels may predict HBV reactivation in these patients with lymphoma and help optimize prophylactic antiviral therapy for high-risk patients. The positivity of anti-HBc antibodies was used to be a marker of past HBV infections.Recently, the quantification of anti-HBc levels by using a newly developed double-sandwich immunoassay has emerged as a new marker of chronic HBV infection.11,12In the natural history of HBV infection, patients in the phases with hepatitis activity (the immune clearance and HBeAg-negative hepatitis phases) have higher anti-HBc levels than those in the immune tolerant or the inactive carrier phases.13,14Anti-HBc levels were well correlated with ALT levels, an indicator of liver injury, and associated with the natural history and treatment response of HBV infection.13–16In patients who received interferon or nucleos(t)ide analogue treatment, those with higher anti-HBc levels had a higher chance of HBeAg seroconversion than those with lower anti-HBc.17
Several steps in the HBV life cycle remain obscure because of a lack of robust in vitro infection models. These steps include particle entry, formation and maintenance of covalently closed circular (ccc) DNA, kinetics of gene expression and viral transmission routes. This study aimed to investigate infection kinetics and cccDNA dynamics during long-term culture. We selected a highly permissive HepG2-NTCP-K7 cell clone engineered to express sodium taurocholate co-transporting polypeptide (NTCP) that supports the full HBV life cycle. We characterized the replication kinetics and dynamics of HBV over six weeks of infection. HBV infection kinetics showed a slow infection process. Nuclear cccDNA was only detected 24 h post-infection and increased until 3 days post-infection (dpi). Viral RNAs increased from 3 dpi reaching a plateau at 6 dpi. HBV protein levels followed similar kinetics with HBx levels reaching a plateau first. cccDNA levels modestly increased throughout the 45-day study period with 5–12 copies per infected cell. Newly produced relaxed circular DNA within capsids was reimported into the nucleus and replenished the cccDNA pool. In addition to intracellular recycling of HBV genomes, secondary de novo infection events resulted in cccDNA formation. Inhibition of relaxed circular DNA formation by nucleoside analogue treatment of infected cells enabled us to measure cccDNA dynamics. HBV cccDNA decayed slowly with a half-life of about 40 days. After a slow infection process, HBV maintains a stable cccDNA pool by intracellular recycling of HBV genomes and via secondary infection. Our results provide important insights into the dynamics of HBV infection and support the future design and evaluation of new antiviral agents. Hepatitis B virus (HBV) chronically infects 257 million individuals worldwide and is a major driver of end-stage liver diseases such as cirrhosis and hepatocellular carcinoma [WHO 2017].The immense death toll of 887,000 individuals/year has driven an intensive search for curative treatment approaches.However, a more detailed understanding of infection kinetics and the genesis and maintenance of episomal nuclear DNA pools, so called covalently closed circular (ccc) DNA, is needed to guide the development of an HBV cure.
Several steps in the HBV life cycle remain obscure because of a lack of robust in vitro infection models. These steps include particle entry, formation and maintenance of covalently closed circular (ccc) DNA, kinetics of gene expression and viral transmission routes. This study aimed to investigate infection kinetics and cccDNA dynamics during long-term culture. We selected a highly permissive HepG2-NTCP-K7 cell clone engineered to express sodium taurocholate co-transporting polypeptide (NTCP) that supports the full HBV life cycle. We characterized the replication kinetics and dynamics of HBV over six weeks of infection. HBV infection kinetics showed a slow infection process. Nuclear cccDNA was only detected 24 h post-infection and increased until 3 days post-infection (dpi). Viral RNAs increased from 3 dpi reaching a plateau at 6 dpi. HBV protein levels followed similar kinetics with HBx levels reaching a plateau first. cccDNA levels modestly increased throughout the 45-day study period with 5–12 copies per infected cell. Newly produced relaxed circular DNA within capsids was reimported into the nucleus and replenished the cccDNA pool. In addition to intracellular recycling of HBV genomes, secondary de novo infection events resulted in cccDNA formation. Inhibition of relaxed circular DNA formation by nucleoside analogue treatment of infected cells enabled us to measure cccDNA dynamics. HBV cccDNA decayed slowly with a half-life of about 40 days. After a slow infection process, HBV maintains a stable cccDNA pool by intracellular recycling of HBV genomes and via secondary infection. Our results provide important insights into the dynamics of HBV infection and support the future design and evaluation of new antiviral agents. HBV is a hepatotropic enveloped DNA virus consisting of a 3.2 kb partially double-stranded genome, termed relaxed circular (rc) DNA.1Initial interaction with heparan sulfate proteoglycans enables the virus to attach to the plasma membrane2,3 and is critical for subsequent interactions with sodium taurocholate co-transporting polypeptide (NTCP) as a functional receptor required for HBV infection.4,5Upon interaction with NTCP, the viral particle is internalized and following fusion of the viral and cellular membranes the capsid is released into the cytoplasm and transported to the nucleus.At the nuclear pore, rcDNA is released into the nucleus where it is converted into cccDNA.cccDNA serves as the transcriptional template for pregenomic RNA (pgRNA) and subgenomic RNAs and permits the persistence of HBV infection (summarized in6).The pgRNA is packaged together with HBV polymerase into newly formed capsids and reverse transcribed, giving rise to progeny rcDNA.Mature rcDNA-containing capsids are enveloped at the endoplasmic reticulum and secreted via multivesicular bodies.7Alternatively, these capsids can traffic back to the nucleus, where they release newly formed rcDNA that can replenish the cccDNA pool (termed intracellular recycling or intracellular amplification).8,9This cycle can be prevented by inhibitors of reverse transcription or by core protein allosteric modulators that have not only been reported to inhibit formation of rcDNA-containing capsids but also to perturb cccDNA biosynthesis (unpublished data and10).
Several steps in the HBV life cycle remain obscure because of a lack of robust in vitro infection models. These steps include particle entry, formation and maintenance of covalently closed circular (ccc) DNA, kinetics of gene expression and viral transmission routes. This study aimed to investigate infection kinetics and cccDNA dynamics during long-term culture. We selected a highly permissive HepG2-NTCP-K7 cell clone engineered to express sodium taurocholate co-transporting polypeptide (NTCP) that supports the full HBV life cycle. We characterized the replication kinetics and dynamics of HBV over six weeks of infection. HBV infection kinetics showed a slow infection process. Nuclear cccDNA was only detected 24 h post-infection and increased until 3 days post-infection (dpi). Viral RNAs increased from 3 dpi reaching a plateau at 6 dpi. HBV protein levels followed similar kinetics with HBx levels reaching a plateau first. cccDNA levels modestly increased throughout the 45-day study period with 5–12 copies per infected cell. Newly produced relaxed circular DNA within capsids was reimported into the nucleus and replenished the cccDNA pool. In addition to intracellular recycling of HBV genomes, secondary de novo infection events resulted in cccDNA formation. Inhibition of relaxed circular DNA formation by nucleoside analogue treatment of infected cells enabled us to measure cccDNA dynamics. HBV cccDNA decayed slowly with a half-life of about 40 days. After a slow infection process, HBV maintains a stable cccDNA pool by intracellular recycling of HBV genomes and via secondary infection. Our results provide important insights into the dynamics of HBV infection and support the future design and evaluation of new antiviral agents. An alternative pathway to maintain the cccDNA pool is via secondary infection of naïve cells or pre-infected cells by extracellular viral particles.Enveloped viruses can initiate new infection events via the transfer of extracellular particles to naïve target cells or via direct cell-to-cell contacts.11Cell-free spread of extracellular progeny virus allows transition throughout the body to infect distant cells or organs or even a new host.It can be prevented by neutralizing antibodies or entry inhibitors targeting virus-receptor interaction.In contrast, direct cell-to-cell transmission routes enable a virus to evade neutralizing antibodies or complement and are resistant to entry inhibitors.For hepatitis C virus (HCV) it has been proposed that cell-to-cell spread facilitates immune escape and circumvents rate-limiting steps of the viral life cycle, like attachment and entry.12
Several steps in the HBV life cycle remain obscure because of a lack of robust in vitro infection models. These steps include particle entry, formation and maintenance of covalently closed circular (ccc) DNA, kinetics of gene expression and viral transmission routes. This study aimed to investigate infection kinetics and cccDNA dynamics during long-term culture. We selected a highly permissive HepG2-NTCP-K7 cell clone engineered to express sodium taurocholate co-transporting polypeptide (NTCP) that supports the full HBV life cycle. We characterized the replication kinetics and dynamics of HBV over six weeks of infection. HBV infection kinetics showed a slow infection process. Nuclear cccDNA was only detected 24 h post-infection and increased until 3 days post-infection (dpi). Viral RNAs increased from 3 dpi reaching a plateau at 6 dpi. HBV protein levels followed similar kinetics with HBx levels reaching a plateau first. cccDNA levels modestly increased throughout the 45-day study period with 5–12 copies per infected cell. Newly produced relaxed circular DNA within capsids was reimported into the nucleus and replenished the cccDNA pool. In addition to intracellular recycling of HBV genomes, secondary de novo infection events resulted in cccDNA formation. Inhibition of relaxed circular DNA formation by nucleoside analogue treatment of infected cells enabled us to measure cccDNA dynamics. HBV cccDNA decayed slowly with a half-life of about 40 days. After a slow infection process, HBV maintains a stable cccDNA pool by intracellular recycling of HBV genomes and via secondary infection. Our results provide important insights into the dynamics of HBV infection and support the future design and evaluation of new antiviral agents. HBV transmission routes that establish and maintain a cccDNA pool are poorly understood, reflecting the paucity of in vitro culture systems that support long-term and secondary infection events.Neither the longevity nor the size of the cccDNA established by HBV has been accurately measured.6In this study, we developed a highly permissive HepG2 clone stably expressing NTCP, designated HepG2-NTCP-K7, that enabled us to determine the kinetics of the viral life cycle and to study HBV transmission pathways that underly cccDNA persistence.
The variety of alterations found in hepatocellular carcinoma (HCC) makes the identification of functionally relevant genes and their combinatorial actions in tumorigenesis challenging. Deregulation of receptor tyrosine kinases (RTKs) is frequent in HCC, yet little is known about the molecular events that cooperate with RTKs and whether these cooperative events play an active role at the root of liver tumorigenesis. A forward genetic screen was performed using Sleeping Beauty transposon insertional mutagenesis to accelerate liver tumour formation in a genetic context in which subtly increased MET RTK levels predispose mice to tumorigenesis. Systematic sequencing of tumours identified common transposon insertion sites, thus uncovering putative RTK cooperators for liver cancer. Bioinformatic analyses were applied to transposon outcomes and human HCC datasets. In vitro and in vivo (through xenografts) functional screens were performed to assess the relevance of distinct cooperative modes to the tumorigenic properties conferred by RTKs. We identified 275 genes, most of which are altered in patients with HCC. Unexpectedly, these genes are not restricted to a small set of pathway/cellular processes, but cover a large spectrum of cellular functions, including signalling, metabolism, chromatin remodelling, mRNA degradation, proteasome, ubiquitination, cell cycle regulation, and chromatid segregation. We validated 15 tumour suppressor candidates, as shRNA-mediated targeting confers tumorigenicity to RTK-sensitized cells, but not to cells with basal RTK levels. This demonstrates that the context of enhanced RTK levels is essential for their action in tumour initiation. Our study identifies unanticipated genetic interactions underlying gene cooperativity with RTKs in HCC. Moreover, these results show how subtly increased levels of wild-type RTKs provide a tumour permissive cellular environment allowing a large spectrum of deregulated mechanisms to initiate liver cancer. Hepatocellular carcinoma (HCC) is among the most aggressive cancers, with an increasing incidence, and few therapeutic options.1The exceptional investments on -omics studies over the last decade have unveiled not only an impressive list of alterations, but also a high degree of molecular heterogeneity between patients with HCC.2,3The uniqueness of HCC in its alterations and heterogeneity may explain how treatments effective in other cancers have largely failed when applied to HCC.4Such context challenges the interpretation of -omics data, with the necessity to: i) determine which of these alterations are functionally relevant for tumorigenic properties, ii) distinguish sets of alterations with a tumour-boosting efficiency linked to specific patient subtypes or genetic contexts, and iii) elucidate how different combinatorial alterations can lead to equivalent vs. divergent fitness outcomes in cancer cells.The identification of functionally relevant signals, and of functional synergistic interactions between co-occurring events, is further complicated by the fact that some signals, although rarely mutated in HCC, are frequently activated in a high proportion of patients and are considered key regulators of tumorigenesis.This is the case, for example, for some receptor tyrosine kinase (RTK) pathway genes.4,5The implication of deregulated RTK signalling in HCC is well established and RTK targeting agents are actively explored for combined therapies.6
The variety of alterations found in hepatocellular carcinoma (HCC) makes the identification of functionally relevant genes and their combinatorial actions in tumorigenesis challenging. Deregulation of receptor tyrosine kinases (RTKs) is frequent in HCC, yet little is known about the molecular events that cooperate with RTKs and whether these cooperative events play an active role at the root of liver tumorigenesis. A forward genetic screen was performed using Sleeping Beauty transposon insertional mutagenesis to accelerate liver tumour formation in a genetic context in which subtly increased MET RTK levels predispose mice to tumorigenesis. Systematic sequencing of tumours identified common transposon insertion sites, thus uncovering putative RTK cooperators for liver cancer. Bioinformatic analyses were applied to transposon outcomes and human HCC datasets. In vitro and in vivo (through xenografts) functional screens were performed to assess the relevance of distinct cooperative modes to the tumorigenic properties conferred by RTKs. We identified 275 genes, most of which are altered in patients with HCC. Unexpectedly, these genes are not restricted to a small set of pathway/cellular processes, but cover a large spectrum of cellular functions, including signalling, metabolism, chromatin remodelling, mRNA degradation, proteasome, ubiquitination, cell cycle regulation, and chromatid segregation. We validated 15 tumour suppressor candidates, as shRNA-mediated targeting confers tumorigenicity to RTK-sensitized cells, but not to cells with basal RTK levels. This demonstrates that the context of enhanced RTK levels is essential for their action in tumour initiation. Our study identifies unanticipated genetic interactions underlying gene cooperativity with RTKs in HCC. Moreover, these results show how subtly increased levels of wild-type RTKs provide a tumour permissive cellular environment allowing a large spectrum of deregulated mechanisms to initiate liver cancer. The hepatocyte growth factor (HGF) receptor MET is one of the RTKs highly relevant in HCC.Although MET mutations are rare in HCC,7 MET is activated in close to 50% of cases,8 which correlates with poor prognosis.9–12Overall, evidence implicating MET in HCC is sufficiently strong to have warranted several clinical trials of MET inhibitors.11,13,14We have engineered a unique conditional transgenic mouse model (R26stopMet mice)15–17 in which expression of wild-type (WT) MET can be slightly enhanced above its endogenous level in the liver (Alb-R26Met mice).We demonstrated that an approximatively 3-fold enhancement of MET levels in Alb-R26Met mice is enough to perturb tissue homeostasis overtime in the liver, consecutively leading to tumour initiation and evolution into HCC.18The clinical relevance of this liver cancer model is supported by studies showing that Alb-R26Met HCCs: i) express MET levels comparable to those reported in ∼20% of human HCC cases;18 ii) exhibit active, phosphorylated MET, as observed in close to 50% of patients with HCC;8 iii) model the so-called HCC “proliferative-progenitor” patient subclass;18 iv) while resistant to treatment with sorafenib, are sensitive to new synthetic lethal interactions we identified.18We recently illustrated that Alb-R26Met HCC are strikingly enriched in genes that are simultaneously overexpressed and hypermethylated in gene body CpG islands (CGIs), again similar to the “proliferative-progenitor” patient subclass.19Such events are predictive of elevated levels of proto-oncogenes, which together act as an “oncogene module”.19Therefore, the Alb-R26Met mouse model is particularly suitable to identify non-predictable genetic interactions underlying gene cooperativity with RTKs during liver tumour initiation.
Immunosuppressed patients with chronic hepatitis E virus infection (cHEV), who are ineligible or have failed current treatment with off-label ribavirin, are a potential target population for T cell-based therapy. T cell responses are important for viral control. Herein, we aimed to identify human leukocyte antigen (HLA)-A2 restricted HEV-specific CD8+ T cell epitopes and T cell receptors (TCR) targeting these epitopes, as the basis for a redirected TCR treatment approach for patients with cHEV. HEV genotype 3 overlapping peptide pools were used to screen HEV-specific CD8+ T cell immune responses in HLA-A2+ patients with acute HEV infection and healthy donors, by intracellular cytokine staining. CD8+ T cells targeting the identified epitopes were sorted for sequencing of the TCR repertoires by next generation sequencing. Messenger RNA encoding these TCRs were introduced into lymphocytes of healthy donors and patients with cHEV through TCR redirection. TCR-engineered lymphocytes were evaluated for Dextramer®-binding capacity, target sensitivity and cytotoxicity against peptide-loaded T2 cells. HEV-specific responses were observed across open reading frame (ORF)1 and ORF2 of the HEV genome in patients with acute resolving HEV infection. HLA-A2-restricted HEV-specific CD8+ T cell epitopes targeting the HEV RNA helicase and RNA-dependent RNA polymerase were selected for functional studies. Introduction of HEV-specific TCRs into lymphocytes of immunocompetent donors and patients with chronic hepatitis E enabled the lymphocytes to bind HEV Dextramers, secrete multiple cytokines and exert cytotoxicity in a target-specific manner. We identified TCRs that target HEV-specific CD8+ T cell epitopes, and characterized their immune properties, which may have clinical potential in future T cell-based therapy. Hepatitis E virus (HEV) infection is the most common cause of acute viral hepatitis in the global population.HEV is an emerging public health risk found in both the developed and developing countries.1,2There are several genotypes of HEV but only one serotype.Genotypes 1 and 2 only infect humans, and are distributed mainly in endemic regions where poor hygiene is the main cause of transmission via the fecal-oral route.Genotypes 3 and 4 are zoonotic; their hosts include both humans and animals, and are found predominantly in urbanized countries.Genotype 3 virus is transmitted by consumption of raw or improperly cooked meat or viscera of infected animals,2 or through transfusion of contaminated blood products.3Usually, patients with acute hepatitis E spontaneously clear the virus.However, immunosuppressed patients, such as solid organ transplant patients, may develop chronic hepatitis E in around 50% of cases.4,5Reversion or reduction of the immunosuppressive status may induce spontaneous viral clearance.4–6If this is not effective or possible, treatment choices are limited.There is no approved therapy for chronic hepatitis E, but interferon-α or ribavirin are off-label treatment options.Unfortunately, interferon-α is contraindicated in patients receiving lung, heart or kidney transplants as it can trigger graft rejection.6Therefore, for these patients, ribavirin is their only option.However, side effects of ribavirin such as hemolytic anemia limit its use.Ribavirin is effective in approximately 80% of patients who are able to tolerate its use.7For those who fail treatment, chronic hepatitis E can progress to liver cirrhosis.6,8Additionally, a recent study has shown that ribavirin induces mutations in HEV, 9 leading to enhanced viral replication and eventually drug failure.10
Immunosuppressed patients with chronic hepatitis E virus infection (cHEV), who are ineligible or have failed current treatment with off-label ribavirin, are a potential target population for T cell-based therapy. T cell responses are important for viral control. Herein, we aimed to identify human leukocyte antigen (HLA)-A2 restricted HEV-specific CD8+ T cell epitopes and T cell receptors (TCR) targeting these epitopes, as the basis for a redirected TCR treatment approach for patients with cHEV. HEV genotype 3 overlapping peptide pools were used to screen HEV-specific CD8+ T cell immune responses in HLA-A2+ patients with acute HEV infection and healthy donors, by intracellular cytokine staining. CD8+ T cells targeting the identified epitopes were sorted for sequencing of the TCR repertoires by next generation sequencing. Messenger RNA encoding these TCRs were introduced into lymphocytes of healthy donors and patients with cHEV through TCR redirection. TCR-engineered lymphocytes were evaluated for Dextramer®-binding capacity, target sensitivity and cytotoxicity against peptide-loaded T2 cells. HEV-specific responses were observed across open reading frame (ORF)1 and ORF2 of the HEV genome in patients with acute resolving HEV infection. HLA-A2-restricted HEV-specific CD8+ T cell epitopes targeting the HEV RNA helicase and RNA-dependent RNA polymerase were selected for functional studies. Introduction of HEV-specific TCRs into lymphocytes of immunocompetent donors and patients with chronic hepatitis E enabled the lymphocytes to bind HEV Dextramers, secrete multiple cytokines and exert cytotoxicity in a target-specific manner. We identified TCRs that target HEV-specific CD8+ T cell epitopes, and characterized their immune properties, which may have clinical potential in future T cell-based therapy. Thus, there is an unmet need for alternative treatment options.Previous findings demonstrated that CD8+ T cells play an important role in mediating immunity against HEV infection.11We hereby propose the pioneering idea of using T cell therapy as the alternative treatment for patients with chronic hepatitis E.
The hepatic injury caused by ischemia/reperfusion (I/R) insult is predominantly determined by the complex interplay of sterile inflammation and liver cell death. Caspase recruitment domain family member 6 (CARD6) was initially shown to play important roles in NF-κB activation. In our preliminary studies, CARD6 downregulation was closely related to hepatic I/R injury in liver transplantation patients and mouse models. Thus, we hypothesized that CARD6 protects against hepatic I/R injury and investigated the underlying molecular mechanisms. A partial hepatic I/R operation was performed in hepatocyte-specific Card6 knockout mice (HKO), Card6 transgenic mice with CARD6 overexpression specifically in hepatocytes (HTG), and the corresponding control mice. Hepatic histology, serum aminotransferases, inflammatory cytokines/chemokines, cell death, and inflammatory signaling were examined to assess liver damage. The molecular mechanisms of CARD6 function were explored in vivo and in vitro. Liver injury was alleviated in Card6-HTG mice compared with control mice as shown by decreased cell death, lower serum aminotransferase levels, and reduced inflammation and infiltration, whereas Card6-HKO mice had the opposite phenotype. Mechanistically, phosphorylation of ASK1 and its downstream effectors JNK and p38 were increased in the livers of Card6-HKO mice but repressed in those of Card6-HTG mice. Furthermore, ASK1 knockdown normalized the effect of CARD6 deficiency on the activation of NF-κB, JNK and p38, while ASK1 overexpression abrogated the suppressive effect of CARD6. CARD6 was also shown to interact with ASK1. Mutant CARD6 that lacked the ability to interact with ASK1 could not inhibit ASK1 and failed to protect against hepatic I/R injury. CARD6 is a novel protective factor against hepatic I/R injury that suppresses inflammation and liver cell death by inhibiting the ASK1 signaling pathway. Ischemia/reperfusion (I/R) injury is an important cause of liver damage that occurs during surgical procedures, including liver tumor resection and liver transplantation.1During this process, the initial hypoxic insult causes direct cellular damage, while the subsequent return of blood flow further aggravates liver dysfunction and injury due to the propagation of inflammation.The mechanism underlying the propagation of the inflammatory response and further tissue damage involves a complex interplay of cytokines/chemokines, multiple cell types, and various signaling pathways.2,3Identification of pivotal regulators controlling this process and elucidation of the underlying mechanisms are essential for the development of novel strategies for clinical intervention of hepatic I/R injury.
The hepatic injury caused by ischemia/reperfusion (I/R) insult is predominantly determined by the complex interplay of sterile inflammation and liver cell death. Caspase recruitment domain family member 6 (CARD6) was initially shown to play important roles in NF-κB activation. In our preliminary studies, CARD6 downregulation was closely related to hepatic I/R injury in liver transplantation patients and mouse models. Thus, we hypothesized that CARD6 protects against hepatic I/R injury and investigated the underlying molecular mechanisms. A partial hepatic I/R operation was performed in hepatocyte-specific Card6 knockout mice (HKO), Card6 transgenic mice with CARD6 overexpression specifically in hepatocytes (HTG), and the corresponding control mice. Hepatic histology, serum aminotransferases, inflammatory cytokines/chemokines, cell death, and inflammatory signaling were examined to assess liver damage. The molecular mechanisms of CARD6 function were explored in vivo and in vitro. Liver injury was alleviated in Card6-HTG mice compared with control mice as shown by decreased cell death, lower serum aminotransferase levels, and reduced inflammation and infiltration, whereas Card6-HKO mice had the opposite phenotype. Mechanistically, phosphorylation of ASK1 and its downstream effectors JNK and p38 were increased in the livers of Card6-HKO mice but repressed in those of Card6-HTG mice. Furthermore, ASK1 knockdown normalized the effect of CARD6 deficiency on the activation of NF-κB, JNK and p38, while ASK1 overexpression abrogated the suppressive effect of CARD6. CARD6 was also shown to interact with ASK1. Mutant CARD6 that lacked the ability to interact with ASK1 could not inhibit ASK1 and failed to protect against hepatic I/R injury. CARD6 is a novel protective factor against hepatic I/R injury that suppresses inflammation and liver cell death by inhibiting the ASK1 signaling pathway. Caspase recruitment domain family member 6 (CARD6) is a typical member of CARD domain-containing proteins, which are generally involved in processes related to inflammation and apoptosis.4CARD domains usually mediate the formation of large protein complexes via direct interactions with individual CARD domains.CARD6 has been shown to interact with microtubules and localizes in the cytosol.5CARD6 also interacts with other members of the CARD family, such as receptor-interacting protein kinases 2 (RIPK2), and positively modulates signal transduction pathways converging on the activation of NF-κB.5CARD6 has also been associated with the activation of the NF-κB pathway in cancers and may play a role in the development of gastrointestinal cancers.6Paradoxically, other studies showed that CARD6 suppressed NF-κB activation by NOD1 or RIPK2 but did not interfere with NF-κB activation by BCL10 or TNFα.7Furthermore, an in vivo study demonstrated that CARD6 was induced by interferon and that CARD6-deficient mice did not exhibit abnormalities in signaling pathways mediating innate and adaptive immune responses, including the NOD pathways.8Interestingly, pressure overload upregulated CARD6 that protected against cardiac hypertrophy by suppressing mitogen-activated protein kinase kinase (MEK) kinase-1-dependent MEK-extracellular signal-regulated protein kinase 1/2 (ERK1/2) and c-Jun N-terminal kinase 1/2 (JNK1/2) activation.9Thus, the pathophysiological function of CARD6 is still unclear.In addition, pathways proximal to CARD6 resulting in its activation have not been characterized and its associated signaling complexes remain obscure.Considering the critical roles of the NF-κB and JNK signaling pathways in causing inflammation and cell death, CARD6 may regulate these processes and play a role in hepatic I/R injury.
Recent studies reveal that the rate of normal on-treatment alanine aminotransferase (ALT) appears different for different nucleos(t)ide analogues (NAs); yet its clinical significance is unclear. We aimed to evaluate the impact of normal on-treatment ALT during antiviral treatment with entecavir (ETV) or tenofovir disoproxil fumarate (TDF) in patients with chronic hepatitis B (CHB). A territory-wide cohort of patients with CHB who received ETV and/or TDF in 2005–2016 was identified. Serial on-treatment ALT levels were collected and analyzed. Normal on-treatment ALT (ALT-N) was defined as ALT <30 U/L in males and <19 U/L in females. The primary and secondary outcomes were composite hepatic events (including hepatocellular carcinoma) based on diagnostic codes. Patients with hepatic events before or during the first year of antiviral treatment or follow-up <1 year were excluded. A total of 21,182 patients with CHB (10,437 with and 10,745 without ALT-N at 12 months after antiviral treatment) were identified and followed for 4.0 ± 1.7 years. Patients with and without ALT-N differed in baseline ALT (58 vs. 61 U/L), hepatitis B virus DNA (4.9 vs. 5.1 log10 IU/ml) and cirrhosis status (8.8% vs. 10.5%). A total of 627 (3.0%) patients developed composite hepatic events. Compared to no ALT-N, ALT-N at 3, 6, 9 and 12 months reduced the risk of hepatic events, after adjustment for baseline ALT and other important covariates, with adjusted hazard ratios (95% CI) of 0.61 (0.49–0.77), 0.55 (0.45–0.67), 0.54 (0.44–0.65) and 0.51 (0.42–0.61) respectively (all p <0.001). The cumulative incidence (95% CI) of composite hepatic events at six years was 3.51% (3.06%-4.02%) in ALT-N and 5.70% (5.15%–6.32%) in the no ALT-N group (p <0.001). Normal on-treatment ALT is associated with a lower risk of hepatic events in patients with CHB receiving NA treatment, translating into improved clinical outcomes in these patients. Elevated alanine aminotransferase (ALT) above two times the upper limit of normal (ULN) in patients with chronic hepatitis B (CHB) is one of the key indications for antiviral treatment recommended by international guidelines.1–3Normal on-treatment ALT is often regarded as a biochemical response to antiviral treatment.Being one of the most commonly used tests for patients with CHB, ALT level correlates with hepatic necroinflammation.4The optimal ALT cutoffs are now set at 30 IU/L for men and 19 IU/L for women by the American Association for the Study of Liver Diseases (AASLD),3 as high-normal ALT levels according to traditional cutoffs ranging from 40 U/L to 70 U/L are also associated with cirrhosis5 and liver-related mortality.6
Recent studies reveal that the rate of normal on-treatment alanine aminotransferase (ALT) appears different for different nucleos(t)ide analogues (NAs); yet its clinical significance is unclear. We aimed to evaluate the impact of normal on-treatment ALT during antiviral treatment with entecavir (ETV) or tenofovir disoproxil fumarate (TDF) in patients with chronic hepatitis B (CHB). A territory-wide cohort of patients with CHB who received ETV and/or TDF in 2005–2016 was identified. Serial on-treatment ALT levels were collected and analyzed. Normal on-treatment ALT (ALT-N) was defined as ALT <30 U/L in males and <19 U/L in females. The primary and secondary outcomes were composite hepatic events (including hepatocellular carcinoma) based on diagnostic codes. Patients with hepatic events before or during the first year of antiviral treatment or follow-up <1 year were excluded. A total of 21,182 patients with CHB (10,437 with and 10,745 without ALT-N at 12 months after antiviral treatment) were identified and followed for 4.0 ± 1.7 years. Patients with and without ALT-N differed in baseline ALT (58 vs. 61 U/L), hepatitis B virus DNA (4.9 vs. 5.1 log10 IU/ml) and cirrhosis status (8.8% vs. 10.5%). A total of 627 (3.0%) patients developed composite hepatic events. Compared to no ALT-N, ALT-N at 3, 6, 9 and 12 months reduced the risk of hepatic events, after adjustment for baseline ALT and other important covariates, with adjusted hazard ratios (95% CI) of 0.61 (0.49–0.77), 0.55 (0.45–0.67), 0.54 (0.44–0.65) and 0.51 (0.42–0.61) respectively (all p <0.001). The cumulative incidence (95% CI) of composite hepatic events at six years was 3.51% (3.06%-4.02%) in ALT-N and 5.70% (5.15%–6.32%) in the no ALT-N group (p <0.001). Normal on-treatment ALT is associated with a lower risk of hepatic events in patients with CHB receiving NA treatment, translating into improved clinical outcomes in these patients. Recent studies showed that patients receiving tenofovir alafenamide (TAF) are more likely to achieve normal on-treatment ALT than those receiving tenofovir disoproxil fumarate (TDF) based on AASLD criteria (but not central laboratory criteria), despite a similar rate of viral suppression.7,8In pooled analyses and individual trials, normal on-treatment ALT rates by AASLD and central laboratory criteria were significantly higher in TAF than TDF recipients at most assessed time points up to 96 weeks.9Yet the exact underlying mechanisms of this observation remain obscured.
Recent studies reveal that the rate of normal on-treatment alanine aminotransferase (ALT) appears different for different nucleos(t)ide analogues (NAs); yet its clinical significance is unclear. We aimed to evaluate the impact of normal on-treatment ALT during antiviral treatment with entecavir (ETV) or tenofovir disoproxil fumarate (TDF) in patients with chronic hepatitis B (CHB). A territory-wide cohort of patients with CHB who received ETV and/or TDF in 2005–2016 was identified. Serial on-treatment ALT levels were collected and analyzed. Normal on-treatment ALT (ALT-N) was defined as ALT <30 U/L in males and <19 U/L in females. The primary and secondary outcomes were composite hepatic events (including hepatocellular carcinoma) based on diagnostic codes. Patients with hepatic events before or during the first year of antiviral treatment or follow-up <1 year were excluded. A total of 21,182 patients with CHB (10,437 with and 10,745 without ALT-N at 12 months after antiviral treatment) were identified and followed for 4.0 ± 1.7 years. Patients with and without ALT-N differed in baseline ALT (58 vs. 61 U/L), hepatitis B virus DNA (4.9 vs. 5.1 log10 IU/ml) and cirrhosis status (8.8% vs. 10.5%). A total of 627 (3.0%) patients developed composite hepatic events. Compared to no ALT-N, ALT-N at 3, 6, 9 and 12 months reduced the risk of hepatic events, after adjustment for baseline ALT and other important covariates, with adjusted hazard ratios (95% CI) of 0.61 (0.49–0.77), 0.55 (0.45–0.67), 0.54 (0.44–0.65) and 0.51 (0.42–0.61) respectively (all p <0.001). The cumulative incidence (95% CI) of composite hepatic events at six years was 3.51% (3.06%-4.02%) in ALT-N and 5.70% (5.15%–6.32%) in the no ALT-N group (p <0.001). Normal on-treatment ALT is associated with a lower risk of hepatic events in patients with CHB receiving NA treatment, translating into improved clinical outcomes in these patients. In patients receiving first-line nucleos(t)ide analogue (NA) treatment, most of them would have achieved complete viral suppression within 12 months.7,8,10,11Even so, up to 58% had elevated on-treatment ALT by AASLD criteria at two years.12Persistently elevated ALT in patients with complete viral suppression may imply the co-existence of fatty liver,13 and was associated with a lower likelihood of cirrhosis regression in patients treated with TDF for five years.10Nonetheless, whether early normal on-treatment ALT confers better clinical outcomes is yet to be proven, as most studies only considered baseline ALT.Furthermore, the effect of specific NA on normal on-treatment ALT has not been adequately addressed.
The progression of hepatosteatosis to non-alcoholic steatohepatitis (NASH) is a critical step in the pathogenesis of hepatocellular cancer. However, the underlying mechanism(s) for this progression is essentially unknown. This study was designed to determine the role of miR-378 in regulating NASH progression. We used immunohistochemistry, luciferase assays and immunoblotting to study the role of miR-378 in modulating an inflammatory pathway. Wild-type mice kept on a high-fat diet (HFD) were injected with miR-378 inhibitors or a mini-circle expression system containing miR-378, to study loss and gain-of functions of miR-378. MiR-378 expression is increased in livers of dietary obese mice and patients with NASH. Further studies revealed that miR-378 directly targeted Prkag2 that encodes AMP-activated protein kinase γ 2 (AMPKγ2). AMPK signaling negatively regulates the NF-κB-TNFα inflammatory axis by increasing deacetylase activity of sirtuin 1. By targeting Prkag2, miR-378 reduced sirtuin 1 activity and facilitated an inflammatory pathway involving NF-κB-TNFα. In contrast, miR-378 knockdown induced expression of Prkag2, increased sirtuin 1 activity and blocked the NF-κB-TNFα axis. Additionally, knockdown of increased Prkag2 offset the inhibitory effects of miR-378 inhibitor on the NF-κB-TNFα axis, suggesting that AMPK signaling mediates the role of miR-378 in facilitating this inflammatory pathway. Liver-specific expression of miR-378 triggered the development of NASH and fibrosis by activating TNFα signaling. Ablation of TNFα in miR-378-treated mice impaired the ability of miR-378 to facilitate hepatic inflammation and fibrosis, suggesting that TNFα signaling is required for miR-378 to promote NASH. MiR-378 plays a key role in the development of hepatic inflammation and fibrosis by positively regulating the NF-κB-TNFα axis. MiR-378 is a potential therapeutic target for the treatment of NASH. The incidence of non-alcoholic fatty liver disease (NAFLD) is estimated to be 20–45% in the general population of Western countries.1,2Although NAFLD carries a relatively benign prognosis, a significant proportion of patients can progress to non-alcoholic steatohepatitis (NASH) and later cirrhosis with the risk of developing hepatocellular carcinoma (HCC).3,4It is estimated that NASH-related HCC accounts for ≥13% of cases in the US.5–7Although the progression of hepatosteatosis to NASH has been studied extensively, the precise mechanism(s) is still under intense investigation.
The progression of hepatosteatosis to non-alcoholic steatohepatitis (NASH) is a critical step in the pathogenesis of hepatocellular cancer. However, the underlying mechanism(s) for this progression is essentially unknown. This study was designed to determine the role of miR-378 in regulating NASH progression. We used immunohistochemistry, luciferase assays and immunoblotting to study the role of miR-378 in modulating an inflammatory pathway. Wild-type mice kept on a high-fat diet (HFD) were injected with miR-378 inhibitors or a mini-circle expression system containing miR-378, to study loss and gain-of functions of miR-378. MiR-378 expression is increased in livers of dietary obese mice and patients with NASH. Further studies revealed that miR-378 directly targeted Prkag2 that encodes AMP-activated protein kinase γ 2 (AMPKγ2). AMPK signaling negatively regulates the NF-κB-TNFα inflammatory axis by increasing deacetylase activity of sirtuin 1. By targeting Prkag2, miR-378 reduced sirtuin 1 activity and facilitated an inflammatory pathway involving NF-κB-TNFα. In contrast, miR-378 knockdown induced expression of Prkag2, increased sirtuin 1 activity and blocked the NF-κB-TNFα axis. Additionally, knockdown of increased Prkag2 offset the inhibitory effects of miR-378 inhibitor on the NF-κB-TNFα axis, suggesting that AMPK signaling mediates the role of miR-378 in facilitating this inflammatory pathway. Liver-specific expression of miR-378 triggered the development of NASH and fibrosis by activating TNFα signaling. Ablation of TNFα in miR-378-treated mice impaired the ability of miR-378 to facilitate hepatic inflammation and fibrosis, suggesting that TNFα signaling is required for miR-378 to promote NASH. MiR-378 plays a key role in the development of hepatic inflammation and fibrosis by positively regulating the NF-κB-TNFα axis. MiR-378 is a potential therapeutic target for the treatment of NASH. Chronic hepatic inflammation is considered a major culprit for the development of NASH.Both hepatocytes and Kupffer cells have been reported to contribute to hepatic inflammation.8In hepatosteatosis, free fatty acid delivery to the liver is markedly increased, and now known to be harmful to hepatocytes.9This so-called lipotoxicity leads to oxidative and endoplasmic reticulum stress.9Thus, prolonged cellular stresses can cause severe damage, leading to hepatocyte death.9Lipotoxicity from free fatty acids, hepatocyte injury and apoptosis can activate Kupffer cells and promote inflammatory responses.10Despite the important role of Kupffer cells in promoting NASH, injured hepatocytes also generate a number of pro-inflammatory cytokines and chemokines including tumor necrosis factor alpha (TNFα), which contributes to hepatic inflammation.10Mechanistically, a complex regulatory network of transcription factors, including nuclear factor kappa B (NF-κB) and peroxisome proliferator-activated receptors, orchestrate these events in hepatic cells.11,12NF-κB signaling is also activated in damaged hepatocytes, which can activate expression of genes encoding TNFα and interleukin 6 (IL6),13 2 important pro-inflammatory cytokines.A positive feedback loop between NF-κB and TNFα appears to aggravate the levels of inflammation and injury to hepatocytes.13The underlying mechanisms involved in the pathogenesis of NASH are complex and involve heretofore unrecognized regulatory networks.
The progression of hepatosteatosis to non-alcoholic steatohepatitis (NASH) is a critical step in the pathogenesis of hepatocellular cancer. However, the underlying mechanism(s) for this progression is essentially unknown. This study was designed to determine the role of miR-378 in regulating NASH progression. We used immunohistochemistry, luciferase assays and immunoblotting to study the role of miR-378 in modulating an inflammatory pathway. Wild-type mice kept on a high-fat diet (HFD) were injected with miR-378 inhibitors or a mini-circle expression system containing miR-378, to study loss and gain-of functions of miR-378. MiR-378 expression is increased in livers of dietary obese mice and patients with NASH. Further studies revealed that miR-378 directly targeted Prkag2 that encodes AMP-activated protein kinase γ 2 (AMPKγ2). AMPK signaling negatively regulates the NF-κB-TNFα inflammatory axis by increasing deacetylase activity of sirtuin 1. By targeting Prkag2, miR-378 reduced sirtuin 1 activity and facilitated an inflammatory pathway involving NF-κB-TNFα. In contrast, miR-378 knockdown induced expression of Prkag2, increased sirtuin 1 activity and blocked the NF-κB-TNFα axis. Additionally, knockdown of increased Prkag2 offset the inhibitory effects of miR-378 inhibitor on the NF-κB-TNFα axis, suggesting that AMPK signaling mediates the role of miR-378 in facilitating this inflammatory pathway. Liver-specific expression of miR-378 triggered the development of NASH and fibrosis by activating TNFα signaling. Ablation of TNFα in miR-378-treated mice impaired the ability of miR-378 to facilitate hepatic inflammation and fibrosis, suggesting that TNFα signaling is required for miR-378 to promote NASH. MiR-378 plays a key role in the development of hepatic inflammation and fibrosis by positively regulating the NF-κB-TNFα axis. MiR-378 is a potential therapeutic target for the treatment of NASH. The discovery of a class of naturally-occurring small non-coding RNAs, termed microRNAs (miRNAs),14 has stimulated a new field of research on the mechanism of NASH progression.Alterations in miRNA expression have been reported in patients with hepatosteatosis and NASH.15,16Some miRNAs such as miR-155 and miR-122, appear to be involved in NASH, as studied in mouse models treated with methionine-choline-deficient diet (MCD).17,18Unfortunately, the metabolic profile of the MCD model is the opposite to that of human NASH.19,20In contrast, the histopathology and pathophysiology of an high-fat diet (HFD)-induced NASH mouse model most resembles the pathophysiology of human NASH.19,20In this study, we used HFD-treated mice to evaluate the role of miRNAs in the pathogenesis of NASH.
In cholangiocarcinoma, early metastatic spread via lymphatic vessels often precludes curative therapies. Cholangiocarcinoma invasiveness is fostered by an extensive stromal reaction, enriched in cancer-associated fibroblasts (CAFs) and lymphatic endothelial cells (LECs). Cholangiocarcinoma cells recruit and activate CAFs by secreting PDGF-D. Herein, we investigated the role of PDGF-D and liver myofibroblasts in promoting lymphangiogenesis in cholangiocarcinoma. Human cholangiocarcinoma specimens were immunostained for podoplanin (LEC marker), α-SMA (CAF marker), VEGF-A, VEGF-C, and their cognate receptors (VEGFR2, VEGFR3). VEGF-A and VEGF-C secretion was evaluated in human fibroblasts obtained from primary sclerosing cholangitis explants. Using human LECs incubated with conditioned medium from PDGF-D-stimulated fibroblasts we assessed migration, 3D vascular assembly, transendothelial electric resistance and transendothelial migration of cholangiocarcinoma cells (EGI-1). We then studied the effects of selective CAF depletion induced by the BH3 mimetic navitoclax on LEC density and lymph node metastases in vivo. In cholangiocarcinoma specimens, CAFs and LECs were closely adjacent. CAFs expressed VEGF-A and VEGF-C, while LECs expressed VEGFR2 and VEGFR3. Upon PDGF-D stimulation, fibroblasts secreted increased levels of VEGF-C and VEGF-A. Fibroblasts, stimulated by PDGF-D induced LEC recruitment and 3D assembly, increased LEC monolayer permeability, and promoted transendothelial EGI-1 migration. These effects were all suppressed by the PDGFRβ inhibitor, imatinib. In the rat model of cholangiocarcinoma, navitoclax-induced CAF depletion, markedly reduced lymphatic vascularization and reduced lymph node metastases. PDGF-D stimulates VEGF-C and VEGF-A production by fibroblasts, resulting in expansion of the lymphatic vasculature and tumor cell intravasation. This critical process in the early metastasis of cholangiocarcinoma may be blocked by inducing CAF apoptosis or by inhibiting the PDGF-D-induced axis. Cholangiocarcinoma (CCA) originates from the intrahepatic or extrahepatic bile ducts and carries a very poor prognosis.1Despite its increasing incidence, effective treatment options for CCA are scarce and limited to surgical resection or liver transplantation in few highly selected patients.1Less than one-third of patients are eligible for curative surgery at the time of diagnosis due to a proclivity for early lymph node metastasis.1,2Although mechanisms promoting CCA invasiveness are still unclear,3 the lymphatic vessels that develop within the tumor provide an important initial route of metastatic dissemination.Indeed, several lines of evidence indicate that the expansion of the lymphatic bed correlates with both increased metastasis and poor prognosis in CCA.4,5
In cholangiocarcinoma, early metastatic spread via lymphatic vessels often precludes curative therapies. Cholangiocarcinoma invasiveness is fostered by an extensive stromal reaction, enriched in cancer-associated fibroblasts (CAFs) and lymphatic endothelial cells (LECs). Cholangiocarcinoma cells recruit and activate CAFs by secreting PDGF-D. Herein, we investigated the role of PDGF-D and liver myofibroblasts in promoting lymphangiogenesis in cholangiocarcinoma. Human cholangiocarcinoma specimens were immunostained for podoplanin (LEC marker), α-SMA (CAF marker), VEGF-A, VEGF-C, and their cognate receptors (VEGFR2, VEGFR3). VEGF-A and VEGF-C secretion was evaluated in human fibroblasts obtained from primary sclerosing cholangitis explants. Using human LECs incubated with conditioned medium from PDGF-D-stimulated fibroblasts we assessed migration, 3D vascular assembly, transendothelial electric resistance and transendothelial migration of cholangiocarcinoma cells (EGI-1). We then studied the effects of selective CAF depletion induced by the BH3 mimetic navitoclax on LEC density and lymph node metastases in vivo. In cholangiocarcinoma specimens, CAFs and LECs were closely adjacent. CAFs expressed VEGF-A and VEGF-C, while LECs expressed VEGFR2 and VEGFR3. Upon PDGF-D stimulation, fibroblasts secreted increased levels of VEGF-C and VEGF-A. Fibroblasts, stimulated by PDGF-D induced LEC recruitment and 3D assembly, increased LEC monolayer permeability, and promoted transendothelial EGI-1 migration. These effects were all suppressed by the PDGFRβ inhibitor, imatinib. In the rat model of cholangiocarcinoma, navitoclax-induced CAF depletion, markedly reduced lymphatic vascularization and reduced lymph node metastases. PDGF-D stimulates VEGF-C and VEGF-A production by fibroblasts, resulting in expansion of the lymphatic vasculature and tumor cell intravasation. This critical process in the early metastasis of cholangiocarcinoma may be blocked by inducing CAF apoptosis or by inhibiting the PDGF-D-induced axis. Tumor-associated lymphangiogenesis is driven by a number of soluble mediators, including vascular endothelial growth factor (VEGF)-A, VEGF-C, VEGF-D, angiopoietin (Ang)-1 and Ang-2, together with their cognate receptors VEGFR2 (for VEGF-A, VEGF-C and VEGF-D), VEGFR3 (for VEGF-C and VEGF-D), and Tie2 (for angiopoietins).6,7In CCA, the inflammatory cells and fibroblasts of the tumor microenvironment represent the main source of VEGF.8In fact, as in other ductal carcinomas with pronounced invasiveness (e.g., breast and pancreatic cancer),9,10 growth of tumoral bile ducts occurs in contiguity with a rich stromal reaction, termed tumor reactive stroma, mainly composed of cancer-associated fibroblasts (CAFs), tumor-associated macrophages, and lymphatic endothelial cells (LECs).11,12
In cholangiocarcinoma, early metastatic spread via lymphatic vessels often precludes curative therapies. Cholangiocarcinoma invasiveness is fostered by an extensive stromal reaction, enriched in cancer-associated fibroblasts (CAFs) and lymphatic endothelial cells (LECs). Cholangiocarcinoma cells recruit and activate CAFs by secreting PDGF-D. Herein, we investigated the role of PDGF-D and liver myofibroblasts in promoting lymphangiogenesis in cholangiocarcinoma. Human cholangiocarcinoma specimens were immunostained for podoplanin (LEC marker), α-SMA (CAF marker), VEGF-A, VEGF-C, and their cognate receptors (VEGFR2, VEGFR3). VEGF-A and VEGF-C secretion was evaluated in human fibroblasts obtained from primary sclerosing cholangitis explants. Using human LECs incubated with conditioned medium from PDGF-D-stimulated fibroblasts we assessed migration, 3D vascular assembly, transendothelial electric resistance and transendothelial migration of cholangiocarcinoma cells (EGI-1). We then studied the effects of selective CAF depletion induced by the BH3 mimetic navitoclax on LEC density and lymph node metastases in vivo. In cholangiocarcinoma specimens, CAFs and LECs were closely adjacent. CAFs expressed VEGF-A and VEGF-C, while LECs expressed VEGFR2 and VEGFR3. Upon PDGF-D stimulation, fibroblasts secreted increased levels of VEGF-C and VEGF-A. Fibroblasts, stimulated by PDGF-D induced LEC recruitment and 3D assembly, increased LEC monolayer permeability, and promoted transendothelial EGI-1 migration. These effects were all suppressed by the PDGFRβ inhibitor, imatinib. In the rat model of cholangiocarcinoma, navitoclax-induced CAF depletion, markedly reduced lymphatic vascularization and reduced lymph node metastases. PDGF-D stimulates VEGF-C and VEGF-A production by fibroblasts, resulting in expansion of the lymphatic vasculature and tumor cell intravasation. This critical process in the early metastasis of cholangiocarcinoma may be blocked by inducing CAF apoptosis or by inhibiting the PDGF-D-induced axis. Within the tumor reactive stroma, a multitude of paracrine signals are exchanged between the cancer and its stromal compartment, aimed at fostering local invasiveness and metastatic spread of the epithelial counterpart.8,13CAFs are the most abundant cell type in the tumor stroma in CCA, and we recently demonstrated that they are locally recruited by malignant cholangiocytes that secrete platelet-derived growth factor (PDGF)-D.14 PDGF-D is, in fact, specifically produced by CCA cells upon hypoxic stimulus, and binds its cognate receptor PDGFRβ expressed by CAFs.14Furthermore, the concept that CAFs are essential drivers of CCA growth has been highlighted by the observation that in a syngeneic rat model of CCA, selective CAF depletion from the tumor microenvironment by the pro-apoptotic BH3 mimetic navitoclax, a specific inhibitor of the anti-apoptotic Bcl2 proteins, suppressed tumor growth and improved animal survival.15
A better identification of factors predicting death is needed in alcoholic hepatitis (AH). Acute-on-chronic liver failure (ACLF) occurs during the course of liver disease and can be identified when AH is diagnosed (prevalent ACLF [pACLF]) or during follow-up (incidental ACLF [iACLF]). This study analyzed the impact of ACLF on outcomes in AH and the role of infection on the onset of ACLF and death. Patients admitted from July 2006 to July 2015 suffering from biopsy-proven severe (s)AH with a Maddrey discriminant function (mDF) ≥32 were included. Infectious episodes, ACLF, and mortality were assessed during a 168-day follow-up period. Results were validated on an independent cohort. One hundred sixty-five patients were included. Mean mDF was 66.3 ± 20.7 and mean model for end-stage liver disease score was 26.8 ± 7.4. The 28-day cumulative incidence of death (CID) was 31% (95% CI 24–39%). Seventy-nine patients (47.9%) had pACLF. The 28-day CID without pACLF and with pACLF-1, pACLF-2, and pACLF-3 were 10.4% (95% CI 5.1–18.0), 30.8% (95% CI 14.3–49.0), 58.3% (95% CI 35.6–75.5), and 72.4% (95% CI 51.3–85.5), respectively, p <0.0001. Twenty-nine patients (17.5%) developed iACLF. The 28-day relative risk of death in patients developing iACLF was 41.87 (95% CI 5.2–335.1; p <0.001). A previous infection was the only independent risk factor for developing iACLF during the follow-up. Prevalence, incidence, and impact on prognosis of ACLF were confirmed in a validation cohort of 97 patients with probable sAH. ACLF is frequent during the course of sAH and is associated with high mortality. Infection strongly predicts the development of ACLF in this setting. Severe alcoholic hepatitis (sAH) is a complication of alcoholic liver disease associated with high mortality.1–4Several studies have shown that in patients suffering from sAH, with a Maddrey discriminant function (mDF) score of more than 32, the use of corticosteroids (CS) significantly improves survival.2,5–8Nevertheless, the long-term benefit of CS on survival of patients with sAH has not been clearly established.A recent large randomized controlled study (the STOPAH study) provided evidence that 90-day mortality for sAH was about 30% with or without this treatment.9A more accurate stratification system is needed to help clinicians identify patients at high risk of death in sAH.10
A better identification of factors predicting death is needed in alcoholic hepatitis (AH). Acute-on-chronic liver failure (ACLF) occurs during the course of liver disease and can be identified when AH is diagnosed (prevalent ACLF [pACLF]) or during follow-up (incidental ACLF [iACLF]). This study analyzed the impact of ACLF on outcomes in AH and the role of infection on the onset of ACLF and death. Patients admitted from July 2006 to July 2015 suffering from biopsy-proven severe (s)AH with a Maddrey discriminant function (mDF) ≥32 were included. Infectious episodes, ACLF, and mortality were assessed during a 168-day follow-up period. Results were validated on an independent cohort. One hundred sixty-five patients were included. Mean mDF was 66.3 ± 20.7 and mean model for end-stage liver disease score was 26.8 ± 7.4. The 28-day cumulative incidence of death (CID) was 31% (95% CI 24–39%). Seventy-nine patients (47.9%) had pACLF. The 28-day CID without pACLF and with pACLF-1, pACLF-2, and pACLF-3 were 10.4% (95% CI 5.1–18.0), 30.8% (95% CI 14.3–49.0), 58.3% (95% CI 35.6–75.5), and 72.4% (95% CI 51.3–85.5), respectively, p <0.0001. Twenty-nine patients (17.5%) developed iACLF. The 28-day relative risk of death in patients developing iACLF was 41.87 (95% CI 5.2–335.1; p <0.001). A previous infection was the only independent risk factor for developing iACLF during the follow-up. Prevalence, incidence, and impact on prognosis of ACLF were confirmed in a validation cohort of 97 patients with probable sAH. ACLF is frequent during the course of sAH and is associated with high mortality. Infection strongly predicts the development of ACLF in this setting. Acute-on-chronic liver failure (ACLF) is a newly described syndrome that can occur during the course of chronic liver disease.11,12It is characterized by an acute decompensation of cirrhosis (ascites, encephalopathy, gastrointestinal hemorrhage, and/or bacterial infection) along with at least one failure of the following organs or systems: liver, kidney, brain, coagulation, circulation, or lungs.13The onset of ACLF dramatically reduces survival in chronic liver disease.14,15ACLF develops as a consequence of an acute burst of systemic inflammation and potentially occurs secondarily to well-defined precipitating events.12Both bacterial infections and active alcoholism have been identified as major events that can act as triggers for ACLF.16The identification of ACLF is frequent during the time course of sAH.ACLF can be established at the time of diagnosis of AH (prevalent ACLF [pACLF]) but can also develop during the course of medical management of this disease (incidental ACLF [iACLF]).14Questions remain regarding the prognostic role of ACLF on mortality in sAH.Moreover, the relationship between underlying infection and the occurrence of ACLF in sAH has yet to be determined.
Recent studies suggest an association between hepatitis C virus (HCV) infection and cardiovascular damage, including carotid atherosclerosis, with a possible effect of HCV clearance on cardiovascular outcomes. We aimed to examine whether HCV eradication by direct-acting antiviral agents (DAA) improves carotid atherosclerosis in HCV-infected patients with advanced fibrosis/compensated cirrhosis. One hundred eighty-two consecutive patients with HCV and advanced fibrosis or compensated cirrhosis were evaluated. All patients underwent DAA-based antiviral therapy according to AISF/EASL guidelines. Intima-media thickness (IMT), carotid thickening (IMT ≥1 mm) and carotid plaques, defined as focal thickening of ≥1.5 mm at the level of the common carotid, were evaluated by ultrasonography (US) at baseline and 9–12 months after the end of therapy. Fifty-six percent of patients were male, mean age 63.1 ± 10.4 years, and 65.9% had compensated cirrhosis. One in five had diabetes, 14.3% were obese, 41.8% had arterial hypertension and 35.2% were smokers. At baseline, mean IMT was 0.94 ± 0.29 mm, 42.8% had IMT ≥1 mm, and 42.8% had carotid plaques. All patients achieved a 12-week sustained virological response. IMT significantly decreased from baseline to follow-up (0.94 ± 0.29 mm vs. 0.81 ± 0.27, p <0.001). Consistently, a significant reduction in the prevalence of patients with carotid thickening from baseline to follow-up was observed (42.8% vs. 17%, p <0.001), while no changes were reported for carotid plaques (42.8% vs. 47.8%, p = 0.34). These results were confirmed in subgroups of patients stratified for cardiovascular risk factors and liver disease severity. HCV eradication by DAA improves carotid atherosclerosis in patients with severe fibrosis with or without additional metabolic risk factors. The impact of this improvement in the atherosclerotic burden in terms of reduction of major cardiovascular outcomes is worth investigating in the long term. Hepatitis C virus (HCV) infection affected roughly 71.1 million of individuals in 2015 with an estimated global prevalence of 1.0% even if with major geographical heterogeneity.1The clinical burden and the prognosis of HCV infection depends not only on the higher risk of liver-related complications and death, but also of the increase in extrahepatic complications.2Consistent with these data, a recent meta-analysis highlighted that HCV-infected patients are at higher risk of extrahepatic manifestations related to immune dysregulation (mixed cryoglobulinemia, lymphoma, etc.) and metabolic dysfunction (type 2 diabetes, etc.) compared to subjects without infection.3Notably, these epidemiological data are supported by experimental evidence, and by studies reporting a positive effect of HCV eradication by both interferon-based and direct-acting antiviral agents (DAA)-based therapies on HCV-related extrahepatic manifestations.2
Recent studies suggest an association between hepatitis C virus (HCV) infection and cardiovascular damage, including carotid atherosclerosis, with a possible effect of HCV clearance on cardiovascular outcomes. We aimed to examine whether HCV eradication by direct-acting antiviral agents (DAA) improves carotid atherosclerosis in HCV-infected patients with advanced fibrosis/compensated cirrhosis. One hundred eighty-two consecutive patients with HCV and advanced fibrosis or compensated cirrhosis were evaluated. All patients underwent DAA-based antiviral therapy according to AISF/EASL guidelines. Intima-media thickness (IMT), carotid thickening (IMT ≥1 mm) and carotid plaques, defined as focal thickening of ≥1.5 mm at the level of the common carotid, were evaluated by ultrasonography (US) at baseline and 9–12 months after the end of therapy. Fifty-six percent of patients were male, mean age 63.1 ± 10.4 years, and 65.9% had compensated cirrhosis. One in five had diabetes, 14.3% were obese, 41.8% had arterial hypertension and 35.2% were smokers. At baseline, mean IMT was 0.94 ± 0.29 mm, 42.8% had IMT ≥1 mm, and 42.8% had carotid plaques. All patients achieved a 12-week sustained virological response. IMT significantly decreased from baseline to follow-up (0.94 ± 0.29 mm vs. 0.81 ± 0.27, p <0.001). Consistently, a significant reduction in the prevalence of patients with carotid thickening from baseline to follow-up was observed (42.8% vs. 17%, p <0.001), while no changes were reported for carotid plaques (42.8% vs. 47.8%, p = 0.34). These results were confirmed in subgroups of patients stratified for cardiovascular risk factors and liver disease severity. HCV eradication by DAA improves carotid atherosclerosis in patients with severe fibrosis with or without additional metabolic risk factors. The impact of this improvement in the atherosclerotic burden in terms of reduction of major cardiovascular outcomes is worth investigating in the long term. Emerging data also support a link between HCV infection and cardiovascular alterations.HCV infection has been associated with an increased risk of both carotid4–6 and coronary7–9 atherosclerosis, myocardial injury,10 peripheral artery disease,11 cerebro- and cardiovascular events,9,12 and finally cardiovascular mortality.13,14However, contrasting data have also been reported, and the basis of this association stems from correlative data, theoretical speculations, and inconclusive experimental evidence.15A recent meta-analysis summarised the available evidence, overall confirming the negative impact of HCV infection on cardiovascular alterations/mortality even if in a context of high heterogeneity, suggesting that a better understanding of this potential association is still required.16
The efficacy and safety of glecaprevir/pibrentasvir (G/P) for patients infected with hepatitis C virus (HCV) have only been investigated in clinical trials, with no real-world data currently available. The aim of our study was to investigate the effectiveness and safety of G/P in a real-world setting. All patients with HCV consecutively starting G/P between October 2017 and January 2018 within the NAVIGATORE-Lombardia Network were analyzed. G/P was administered according to drug label (8, 12 or 16 weeks). Fibrosis was staged either histologically or by liver stiffness measurement. Sustained virological response (SVR) was defined as undetectable HCV-RNA 12 weeks after the end of treatment. A total of 723 patients (50% males) were treated with G/P, 89% for 8 weeks. The median age of our cohort was 58 years, with a median body mass index of 23.9 kg/m2, and median liver stiffness measurement of 6.1 kPa; 84% were F0-2 and 16% were interferon-experienced. Median HCV-RNA was 1,102,600 IU/ml, and 49% of patients had HCV genotype 1 (32% 1b), 28% genotype 2, 10% genotype 3 and 13% genotype 4. The median estimated glomerular filtration rate was 90.2 ml/min, platelet count 209x103/mm3 and albumin 4.3 g/dl. The SVR rates were 94% in intention-to-treat and 99.3% in per protocol analysis (8-week vs. 12 or 16-week: 99.2% vs. 100%). Five patients failed therapy because of post-treatment relapse; a post-treatment NS5A resistance-associated substitution was detected in 1 case. SVR rates were lower in males (p = 0.002) and in HCV genotype-3 (p = 0.046) patients treated for 8 weeks, but independent of treatment duration, fibrosis stage, baseline HCV-RNA, HIV co-infection, chronic kidney disease stage and viral kinetics. Mild adverse events were reported in 8.3% of the patients, and 0.7% of them prematurely withdrew treatment. Three patients died of drug-unrelated causes. In a large real-world cohort of Italian patients, we confirmed the excellent effectiveness and safety of G/P administered for 8, 12 or 16 weeks. Chronic infection with hepatitis C virus (HCV) still remains a leading cause of morbidity and mortality worldwide, with more than 70 million individuals infected despite recent improvements in antiviral therapies.1,2In the last years, direct-acting antivirals (DAA) have dramatically revolutionized this scenario, since the availability of potent interferon (IFN)-free regimens has led to an increase in sustained virological response (SVR) rates, especially among patients in whom IFN-based antiviral therapies were previously contraindicated.2,3Moreover, because of the lack of tolerability issues, treatment indications have been widely extended to include patients with milder liver diseases, thus contributing to the goal of HCV eradication.1
The efficacy and safety of glecaprevir/pibrentasvir (G/P) for patients infected with hepatitis C virus (HCV) have only been investigated in clinical trials, with no real-world data currently available. The aim of our study was to investigate the effectiveness and safety of G/P in a real-world setting. All patients with HCV consecutively starting G/P between October 2017 and January 2018 within the NAVIGATORE-Lombardia Network were analyzed. G/P was administered according to drug label (8, 12 or 16 weeks). Fibrosis was staged either histologically or by liver stiffness measurement. Sustained virological response (SVR) was defined as undetectable HCV-RNA 12 weeks after the end of treatment. A total of 723 patients (50% males) were treated with G/P, 89% for 8 weeks. The median age of our cohort was 58 years, with a median body mass index of 23.9 kg/m2, and median liver stiffness measurement of 6.1 kPa; 84% were F0-2 and 16% were interferon-experienced. Median HCV-RNA was 1,102,600 IU/ml, and 49% of patients had HCV genotype 1 (32% 1b), 28% genotype 2, 10% genotype 3 and 13% genotype 4. The median estimated glomerular filtration rate was 90.2 ml/min, platelet count 209x103/mm3 and albumin 4.3 g/dl. The SVR rates were 94% in intention-to-treat and 99.3% in per protocol analysis (8-week vs. 12 or 16-week: 99.2% vs. 100%). Five patients failed therapy because of post-treatment relapse; a post-treatment NS5A resistance-associated substitution was detected in 1 case. SVR rates were lower in males (p = 0.002) and in HCV genotype-3 (p = 0.046) patients treated for 8 weeks, but independent of treatment duration, fibrosis stage, baseline HCV-RNA, HIV co-infection, chronic kidney disease stage and viral kinetics. Mild adverse events were reported in 8.3% of the patients, and 0.7% of them prematurely withdrew treatment. Three patients died of drug-unrelated causes. In a large real-world cohort of Italian patients, we confirmed the excellent effectiveness and safety of G/P administered for 8, 12 or 16 weeks. More recently, several efforts have been made to further simplify anti-HCV therapies in order to optimize treatment management and improve patient adherence.Glecaprevir/pibrentasvir (G/P) is a pangenotypic regimen recently approved for the treatment of chronic HCV infection.Glecaprevir is a non-structural (NS) protein 3/4 A protease inhibitor, which is co-formulated with the NS5A inhibitor pibrentasvir.In phase II and III registration trials, SVR rates >95% were achieved when this combination was administered for 8 to 16 weeks, without safety issues.4–11However, in clinical trials, restricted inclusion and exclusion criteria for the targeted patients may influence outcomes.In fact, patients who receive treatment in the real-world can differ from those enrolled in clinical trials due to older age, more advanced stages of fibrosis, and higher prevalence of co-morbidities and co-medications, all conditions potentially affecting SVR rates.
The main stages of cirrhosis (compensated and decompensated) have been sub-staged based on clinical, endoscopic, and portal pressure (determined by the hepatic venous pressure gradient [HVPG]) features. Vasodilation leading to a hyperdynamic circulatory state is central in the development of a late decompensated stage, with inflammation currently considered a key driver. We aimed to assess hepatic/systemic hemodynamics and inflammation (by C-reactive protein [CRP]) among the different sub-stages of cirrhosis and to investigate their interrelationship and prognostic relevance. A single center, prospective cohort of patients with cirrhosis undergoing per protocol hepatic and right-heart catheterization and CRP measurement, were classified into recently defined prognostic stages (PS) of compensated (PS1: HVPG ≥6 mmHg but <10 mmHg; PS2: HVPG ≥10 mmHg without gastroesophageal varices; PS3: patients with gastroesophageal varices) and decompensated (PS4: diuretic-responsive ascites; PS5: refractory ascites) disease. Cardiodynamic states based on cardiac index (L/min/m2) were created: relatively hypodynamic (<3.2), normodynamic (3.2–4.2) and hyperdynamic (>4.2). Of 238 patients, 151 were compensated (PS1 = 25; PS2 = 36; PS3 = 90) and 87 were decompensated (PS4 = 48; PS5 = 39). Mean arterial pressure decreased progressively from PS1 to PS5, cardiac index increased progressively from PS1-to-PS4 but decreased in PS5. HVPG, model for end-stage liver disease (MELD), and CRP increased progressively from PS1-to-PS5. Among compensated patients, age, HVPG, relatively hypodynamic/hyperdynamic state and CRP were predictive of decompensation. Among patients with ascites, MELD, relatively hypodynamic/hyperdynamic state, post-capillary pulmonary hypertension, and CRP were independent predictors of death/liver transplant. Our study demonstrates that, in addition to known parameters, cardiopulmonary hemodynamics and CRP are predictive of relevant outcomes, both in patients with compensated and decompensated cirrhosis. Cirrhosis represents the end-stage of chronic liver disease, with a course characterized by a transition from an asymptomatic compensated stage to a symptomatic decompensated stage.1These stages have entirely different mortalities and should therefore be considered distinct entities.Decompensation is defined by the development of overt clinical complications of cirrhosis: ascites, variceal hemorrhage (VH), encephalopathy, and jaundice.1
The main stages of cirrhosis (compensated and decompensated) have been sub-staged based on clinical, endoscopic, and portal pressure (determined by the hepatic venous pressure gradient [HVPG]) features. Vasodilation leading to a hyperdynamic circulatory state is central in the development of a late decompensated stage, with inflammation currently considered a key driver. We aimed to assess hepatic/systemic hemodynamics and inflammation (by C-reactive protein [CRP]) among the different sub-stages of cirrhosis and to investigate their interrelationship and prognostic relevance. A single center, prospective cohort of patients with cirrhosis undergoing per protocol hepatic and right-heart catheterization and CRP measurement, were classified into recently defined prognostic stages (PS) of compensated (PS1: HVPG ≥6 mmHg but <10 mmHg; PS2: HVPG ≥10 mmHg without gastroesophageal varices; PS3: patients with gastroesophageal varices) and decompensated (PS4: diuretic-responsive ascites; PS5: refractory ascites) disease. Cardiodynamic states based on cardiac index (L/min/m2) were created: relatively hypodynamic (<3.2), normodynamic (3.2–4.2) and hyperdynamic (>4.2). Of 238 patients, 151 were compensated (PS1 = 25; PS2 = 36; PS3 = 90) and 87 were decompensated (PS4 = 48; PS5 = 39). Mean arterial pressure decreased progressively from PS1 to PS5, cardiac index increased progressively from PS1-to-PS4 but decreased in PS5. HVPG, model for end-stage liver disease (MELD), and CRP increased progressively from PS1-to-PS5. Among compensated patients, age, HVPG, relatively hypodynamic/hyperdynamic state and CRP were predictive of decompensation. Among patients with ascites, MELD, relatively hypodynamic/hyperdynamic state, post-capillary pulmonary hypertension, and CRP were independent predictors of death/liver transplant. Our study demonstrates that, in addition to known parameters, cardiopulmonary hemodynamics and CRP are predictive of relevant outcomes, both in patients with compensated and decompensated cirrhosis. Decompensation is mainly driven by portal hypertension (PH), with a portal pressure, as determined by the hepatic venous pressure gradient (HVPG), ≥10 mmHg.2Therefore, compensated cirrhosis has been recently sub-staged into mild PH (HVPG >5 but <10 mmHg) and clinically significant portal hypertension ([CSPH] HVPG ≥10 mmHg).2,3The latter subgroup has in turn been sub-classified into those with or without gastroesophageal varices (GEV), because patients with compensated cirrhosis and GEV have a higher risk of decompensation and death, compared to those without GEV.4
The main stages of cirrhosis (compensated and decompensated) have been sub-staged based on clinical, endoscopic, and portal pressure (determined by the hepatic venous pressure gradient [HVPG]) features. Vasodilation leading to a hyperdynamic circulatory state is central in the development of a late decompensated stage, with inflammation currently considered a key driver. We aimed to assess hepatic/systemic hemodynamics and inflammation (by C-reactive protein [CRP]) among the different sub-stages of cirrhosis and to investigate their interrelationship and prognostic relevance. A single center, prospective cohort of patients with cirrhosis undergoing per protocol hepatic and right-heart catheterization and CRP measurement, were classified into recently defined prognostic stages (PS) of compensated (PS1: HVPG ≥6 mmHg but <10 mmHg; PS2: HVPG ≥10 mmHg without gastroesophageal varices; PS3: patients with gastroesophageal varices) and decompensated (PS4: diuretic-responsive ascites; PS5: refractory ascites) disease. Cardiodynamic states based on cardiac index (L/min/m2) were created: relatively hypodynamic (<3.2), normodynamic (3.2–4.2) and hyperdynamic (>4.2). Of 238 patients, 151 were compensated (PS1 = 25; PS2 = 36; PS3 = 90) and 87 were decompensated (PS4 = 48; PS5 = 39). Mean arterial pressure decreased progressively from PS1 to PS5, cardiac index increased progressively from PS1-to-PS4 but decreased in PS5. HVPG, model for end-stage liver disease (MELD), and CRP increased progressively from PS1-to-PS5. Among compensated patients, age, HVPG, relatively hypodynamic/hyperdynamic state and CRP were predictive of decompensation. Among patients with ascites, MELD, relatively hypodynamic/hyperdynamic state, post-capillary pulmonary hypertension, and CRP were independent predictors of death/liver transplant. Our study demonstrates that, in addition to known parameters, cardiopulmonary hemodynamics and CRP are predictive of relevant outcomes, both in patients with compensated and decompensated cirrhosis. The most frequent decompensating event in cirrhosis and the one that carries the highest mortality risk is ascites.5,6Unlike VH and encephalopathy, ascites is a progressive complication that results most directly from a splanchnic venous pooling in the upright posture, which triggers sodium and water retention.7Shear stress-induced splanchnic arterial vasodilation then appears in the supine position because of blood volume redistribution and cardiac output increase (i.e. vasodilation hypothesis and hyperdynamic circulatory state).7Worsening of this state leads to a stage of “further” decompensation, characterized by refractory ascites.8
The main stages of cirrhosis (compensated and decompensated) have been sub-staged based on clinical, endoscopic, and portal pressure (determined by the hepatic venous pressure gradient [HVPG]) features. Vasodilation leading to a hyperdynamic circulatory state is central in the development of a late decompensated stage, with inflammation currently considered a key driver. We aimed to assess hepatic/systemic hemodynamics and inflammation (by C-reactive protein [CRP]) among the different sub-stages of cirrhosis and to investigate their interrelationship and prognostic relevance. A single center, prospective cohort of patients with cirrhosis undergoing per protocol hepatic and right-heart catheterization and CRP measurement, were classified into recently defined prognostic stages (PS) of compensated (PS1: HVPG ≥6 mmHg but <10 mmHg; PS2: HVPG ≥10 mmHg without gastroesophageal varices; PS3: patients with gastroesophageal varices) and decompensated (PS4: diuretic-responsive ascites; PS5: refractory ascites) disease. Cardiodynamic states based on cardiac index (L/min/m2) were created: relatively hypodynamic (<3.2), normodynamic (3.2–4.2) and hyperdynamic (>4.2). Of 238 patients, 151 were compensated (PS1 = 25; PS2 = 36; PS3 = 90) and 87 were decompensated (PS4 = 48; PS5 = 39). Mean arterial pressure decreased progressively from PS1 to PS5, cardiac index increased progressively from PS1-to-PS4 but decreased in PS5. HVPG, model for end-stage liver disease (MELD), and CRP increased progressively from PS1-to-PS5. Among compensated patients, age, HVPG, relatively hypodynamic/hyperdynamic state and CRP were predictive of decompensation. Among patients with ascites, MELD, relatively hypodynamic/hyperdynamic state, post-capillary pulmonary hypertension, and CRP were independent predictors of death/liver transplant. Our study demonstrates that, in addition to known parameters, cardiopulmonary hemodynamics and CRP are predictive of relevant outcomes, both in patients with compensated and decompensated cirrhosis. Translocation of bacterial or pathogen-associated molecules from the intestinal lumen to the systemic circulation and extraintestinal organs, has been associated with the hyperdynamic circulatory state of advanced experimental and human cirrhosis through an increased production of proinflammatory cytokines.9–11A recent paper12 has de-emphasized the importance of the vasodilation hypothesis in favor of a systemic inflammatory response as the predominant mechanism in decompensated cirrhosis and in acute-on-chronic liver failure (ACLF), but has recommended the performance of studies assessing the relationship among clinical stages, splanchnic and systemic hemodynamics and systemic inflammation.
Non-alcoholic steatohepatitis (NASH) is characterized by hepatocyte steatosis, ballooning, and lobular inflammation which may lead to fibrosis. Lipotoxicity activates caspases, which cause apoptosis and inflammatory cytokine (IL-1β and IL-18) production. Emricasan is a pan-caspase inhibitor that decreases serum aminotransferases and caspase activation in NASH patients. This study postulated that 72 weeks of emricasan treatment would improve liver fibrosis without worsening of NASH. This double-blind, placebo-controlled study randomized 318 subjects 1:1:1 to twice-daily treatment with emricasan (5 or 50 mg) or matching placebo for 72 weeks. Subjects had definite NASH and NASH CRN fibrosis stage F1-F3, as determined by a central reader, on a liver biopsy obtained within 6 months of randomization. Emricasan treatment did not achieve the primary objective of fibrosis improvement without worsening of NASH (emricasan 5 mg: 11.2%; emricasan 50 mg: 12.3%; placebo: 19.0%; odds ratios vs. placebo 0.530 and 0.588, with p=0.972 and 0.972, respectively) or the secondary objective of NASH resolution without worsening of fibrosis (emricasan 5 mg: 3.7%; emricasan 50 mg: 6.6%; placebo: 10.5%; odds ratios vs. placebo 0.334 and 0.613, with p=0.070 and 0.335, respectively). In the small subset of subjects with consistent normalization of serum ALT over 72 weeks, emricasan may have improved histologic outcomes. Emricasan treatment did not improve liver histology in subjects with NASH fibrosis despite target engagement and may have worsened fibrosis and ballooning. Caspase inhibition lowered serum ALT in the short-term but may have directed cells to alternative mechanisms of cell death, resulting in more liver fibrosis and hepatocyte ballooning. Non-alcoholic fatty liver disease (NAFLD) is becoming a major epidemic.Up to 30% of Western populations have NAFLD, 10-20% of those patients will eventually develop non-alcoholic steatohepatitis (NASH) and fibrosis, and 10-20% of those NASH patients will develop cirrhosis [1, 2].NASH with fibrosis is the most important risk factor for developing cirrhosis and liver-related morbidity [3].Unlike hepatitis C infection, where the paradigm of hepatocellular carcinoma occurs typically via a cirrhotic pathway, newer data suggest that a significant number of new cases of hepatocellular carcinoma in NAFLD may arise outside of a cirrhotic phenotype [4, 5].Consequently, NASH cirrhosis and hepatocellular carcinoma due to NASH are a leading indication for liver transplantation in the U.S. [6].
Non-alcoholic steatohepatitis (NASH) is characterized by hepatocyte steatosis, ballooning, and lobular inflammation which may lead to fibrosis. Lipotoxicity activates caspases, which cause apoptosis and inflammatory cytokine (IL-1β and IL-18) production. Emricasan is a pan-caspase inhibitor that decreases serum aminotransferases and caspase activation in NASH patients. This study postulated that 72 weeks of emricasan treatment would improve liver fibrosis without worsening of NASH. This double-blind, placebo-controlled study randomized 318 subjects 1:1:1 to twice-daily treatment with emricasan (5 or 50 mg) or matching placebo for 72 weeks. Subjects had definite NASH and NASH CRN fibrosis stage F1-F3, as determined by a central reader, on a liver biopsy obtained within 6 months of randomization. Emricasan treatment did not achieve the primary objective of fibrosis improvement without worsening of NASH (emricasan 5 mg: 11.2%; emricasan 50 mg: 12.3%; placebo: 19.0%; odds ratios vs. placebo 0.530 and 0.588, with p=0.972 and 0.972, respectively) or the secondary objective of NASH resolution without worsening of fibrosis (emricasan 5 mg: 3.7%; emricasan 50 mg: 6.6%; placebo: 10.5%; odds ratios vs. placebo 0.334 and 0.613, with p=0.070 and 0.335, respectively). In the small subset of subjects with consistent normalization of serum ALT over 72 weeks, emricasan may have improved histologic outcomes. Emricasan treatment did not improve liver histology in subjects with NASH fibrosis despite target engagement and may have worsened fibrosis and ballooning. Caspase inhibition lowered serum ALT in the short-term but may have directed cells to alternative mechanisms of cell death, resulting in more liver fibrosis and hepatocyte ballooning. Hepatocyte steatosis accompanied by ballooning, lobular and portal inflammation, with or without fibrosis are histologic hallmarks of NASH [7].NASH histologic activity is quantified by the non-alcoholic fatty liver disease activity score (NAS) and fibrosis is staged according to the NASH Clinical Research Network (CRN) criteria [8].Improvements in NAS or fibrosis may be acceptable components of registrational endpoints for NASH studies [9].
Non-alcoholic steatohepatitis (NASH) is characterized by hepatocyte steatosis, ballooning, and lobular inflammation which may lead to fibrosis. Lipotoxicity activates caspases, which cause apoptosis and inflammatory cytokine (IL-1β and IL-18) production. Emricasan is a pan-caspase inhibitor that decreases serum aminotransferases and caspase activation in NASH patients. This study postulated that 72 weeks of emricasan treatment would improve liver fibrosis without worsening of NASH. This double-blind, placebo-controlled study randomized 318 subjects 1:1:1 to twice-daily treatment with emricasan (5 or 50 mg) or matching placebo for 72 weeks. Subjects had definite NASH and NASH CRN fibrosis stage F1-F3, as determined by a central reader, on a liver biopsy obtained within 6 months of randomization. Emricasan treatment did not achieve the primary objective of fibrosis improvement without worsening of NASH (emricasan 5 mg: 11.2%; emricasan 50 mg: 12.3%; placebo: 19.0%; odds ratios vs. placebo 0.530 and 0.588, with p=0.972 and 0.972, respectively) or the secondary objective of NASH resolution without worsening of fibrosis (emricasan 5 mg: 3.7%; emricasan 50 mg: 6.6%; placebo: 10.5%; odds ratios vs. placebo 0.334 and 0.613, with p=0.070 and 0.335, respectively). In the small subset of subjects with consistent normalization of serum ALT over 72 weeks, emricasan may have improved histologic outcomes. Emricasan treatment did not improve liver histology in subjects with NASH fibrosis despite target engagement and may have worsened fibrosis and ballooning. Caspase inhibition lowered serum ALT in the short-term but may have directed cells to alternative mechanisms of cell death, resulting in more liver fibrosis and hepatocyte ballooning. Steatosis and toxic saturated fatty acids and lysophospholipids can activate hepatocyte cell membrane death receptors and cause endoplasmic reticulum and mitochondrial toxicity which activates caspases [10].Caspases are intracellular proteases that orchestrate apoptotic cell death by cleavage of cytoskeletal proteins such as keratin-18 (cCK18) contributing to hepatocyte ballooning, and activate proinflammatory cytokines such as IL-1β [11, 12].Apoptosis is increased in patients with NASH [13] and levels of cleaved keratin-18 (cCK18) correlate with apoptosis and liver fibrosis [14].Emricasan is a pan-caspase inhibitor that decreased caspase-3/7 activity, cCK18 and serum alanine aminotransferase (ALT) in patients with NASH [15], suggesting that pan-caspase inhibition could be therapeutically useful.
MSDC-0602K is a novel insulin sensitizer designed to preferentially target the mitochondrial pyruvate carrier while minimizing direct binding to the transcriptional factor PPARγ. Herein, we aimed to assess the efficacy and safety of MSDC-0602K in patients with non-alcoholic steatohepatitis. Patients with biopsy-confirmed NASH and fibrosis (F1-F3) were randomized to daily oral placebo, or 1 of 3 MSDC-0602K doses in a 52-week double-blind study. The primary efficacy endpoint was hepatic histological improvement of ≥2 points in non-alcoholic fatty liver disease activity score (NAS) with a ≥1-point reduction in either ballooning or lobular inflammation and no increase in fibrosis stage at 12 months. Secondary endpoints included NAS improvement without worsening fibrosis, NASH resolution, and fibrosis reduction. Exploratory endpoints included changes in insulin sensitivity, liver injury and liver fibrosis markers. Patients were randomly assigned to placebo (n = 94), or 62.5 mg (n = 99), 125 mg (n = 98), or 250 mg (n = 101) of MSDC-0602K. At baseline, glycated hemoglobin was 6.4 ± 1.0%, 61.5% of patients had fibrosis F2/F3 and the average NAS was 5.3. The primary endpoint was reached in 29.7%, 29.8%, 32.9% and 39.5% of patients in the placebo, 62.5 mg, 125 mg and 250 mg dose arms, respectively, with adjusted odds ratios relative to placebo of 0.89 (95% CI 0.44–1.81), 1.22 (95% CI 0.60–2.48), and 1.64 (95% CI 0.83–3.27). The 2 highest doses of MSDC-0602K led to significant reductions in glucose, glycated hemoglobin, insulin, liver enzymes and NAS compared to placebo. The incidence of hypoglycemia and PPARγ-agonist-associated events such as edema and fractures were similar in the placebo and MSDC-0602K groups. MSDC-0602K did not demonstrate statistically significant effects on primary and secondary liver histology endpoints. However, effects on non-invasive measures of liver cell injury and glucose metabolism support further exploration of MSDC-0602K’s safety and potential efficacy in patients with type 2 diabetes and liver injury. [ ClinicalTrials.gov Identifier: NCT02784444]. Non-alcoholic fatty liver disease (NAFLD) and non-alcoholic steatohepatitis (NASH) are hepatic manifestations of metabolic syndrome and are increasing globally.1,2Patients with NAFLD/NASH often present with either type 2 diabetes (T2D) or have insulin resistance and elevated fasting insulin levels, which are all established precursors for the development of diabetes and for adverse macrovascular sequalae.3–5NASH, T2D, and insulin resistance are thought to be growing in incidence and severity because of the increased availability of nutrients and consequent overnutrition.2
MSDC-0602K is a novel insulin sensitizer designed to preferentially target the mitochondrial pyruvate carrier while minimizing direct binding to the transcriptional factor PPARγ. Herein, we aimed to assess the efficacy and safety of MSDC-0602K in patients with non-alcoholic steatohepatitis. Patients with biopsy-confirmed NASH and fibrosis (F1-F3) were randomized to daily oral placebo, or 1 of 3 MSDC-0602K doses in a 52-week double-blind study. The primary efficacy endpoint was hepatic histological improvement of ≥2 points in non-alcoholic fatty liver disease activity score (NAS) with a ≥1-point reduction in either ballooning or lobular inflammation and no increase in fibrosis stage at 12 months. Secondary endpoints included NAS improvement without worsening fibrosis, NASH resolution, and fibrosis reduction. Exploratory endpoints included changes in insulin sensitivity, liver injury and liver fibrosis markers. Patients were randomly assigned to placebo (n = 94), or 62.5 mg (n = 99), 125 mg (n = 98), or 250 mg (n = 101) of MSDC-0602K. At baseline, glycated hemoglobin was 6.4 ± 1.0%, 61.5% of patients had fibrosis F2/F3 and the average NAS was 5.3. The primary endpoint was reached in 29.7%, 29.8%, 32.9% and 39.5% of patients in the placebo, 62.5 mg, 125 mg and 250 mg dose arms, respectively, with adjusted odds ratios relative to placebo of 0.89 (95% CI 0.44–1.81), 1.22 (95% CI 0.60–2.48), and 1.64 (95% CI 0.83–3.27). The 2 highest doses of MSDC-0602K led to significant reductions in glucose, glycated hemoglobin, insulin, liver enzymes and NAS compared to placebo. The incidence of hypoglycemia and PPARγ-agonist-associated events such as edema and fractures were similar in the placebo and MSDC-0602K groups. MSDC-0602K did not demonstrate statistically significant effects on primary and secondary liver histology endpoints. However, effects on non-invasive measures of liver cell injury and glucose metabolism support further exploration of MSDC-0602K’s safety and potential efficacy in patients with type 2 diabetes and liver injury. [ ClinicalTrials.gov Identifier: NCT02784444]. First-generation insulin-sensitizing thiazolidinediones (TZDs) were developed more than 30 years ago and are still in use to treat T2D.These compounds are known as peroxisome proliferator-activated receptor-gamma (PPARγ) agonists because they directly bind to and activate the PPARγ nuclear hormone receptor.6TZDs have been shown to improve insulin resistance and glucose metabolism resulting in lower glucose concentrations, and to reduce the risk of downstream cardiovascular (CV) outcomes.7,8However, TZDs are associated with significant side effects including edema, bone fractures mediated by PPARγ and hypoglycemia.A second target for the insulin-sensitizing TZDs – the mitochondrial pyruvate carrier (MPC) – was recently identified.9The insulin-sensitizing TZDs all slow the entry of pyruvate, the end product of cytosolic glucose metabolism, into the mitochondria, where much of cellular metabolism is coordinated.10The potential routes by which compounds, including the earlier TZDs such as pioglitazone, can initiate insulin-sensitizing pharmacology by slowing the entry of pyruvate into the mitochondria, have been reviewed elsewhere.11,12Briefly, changing the entry of pyruvate changes the mitochondrial metabolism of other nutrients, altering multiple inputs of metabolism, especially in the face of overnutrition, to downstream signals including mTOR and transcriptional networks that control many cellular functions including sensitivity to insulin and growth factors.11,12Since this target can be addressed without directly activating nuclear receptors, MPC agonism may offer an advantageous treatment modality.11,12Moreover, MSDC-0602K was designed to minimize binding to PPARγ, while still producing insulin-sensitizing pharmacology in diabetic13 and NASH animal models.14
Upon ligand binding, tyrosine kinase receptors, such as epidermal growth factor receptor (EGFR), are recruited into clathrin-coated pits for internalization by endocytosis, which is relevant for signalling and/or receptor degradation. In liver cells, transforming growth factor-β (TGF-β) induces both pro- and anti-apoptotic signals; the latter are mediated by the EGFR pathway. Since EGFR mainly traffics via clathrin-coated vesicles, we aimed to analyse the potential role of clathrin in TGF-β-induced signalling in liver cells and its relevance in liver cancer. Real-Time PCR and immunohistochemistry were used to analyse clathrin heavy-chain expression in human (CLTC) and mice (Cltc) liver tumours. Transient knockdown (siRNA) or overexpression of CLTC were used to analyse its role on TGF-β and EGFR signalling in vitro. Bioinformatic analysis was used to determine the effect of CLTC and TGFB1 expression on prognosis and overall survival in patients with hepatocellular carcinoma (HCC). Clathrin expression increased during liver tumorigenesis in humans and mice. CLTC knockdown cells responded to TGF-β phosphorylating SMADs (canonical signalling) but showed impairment in the anti-apoptotic signals (EGFR transactivation). Experiments of loss or gain of function in HCC cells reveal an essential role for clathrin in inhibiting TGF-β-induced apoptosis and upregulation of its pro-apoptotic target NOX4. Autocrine TGF-β signalling in invasive HCC cells upregulates CLTC expression, switching its role to pro-tumorigenic. A positive correlation between TGFB1 and CLTC was found in HCC cells and patients. Patients expressing high levels of TGFB1 and CLTC had a worse prognosis and lower overall survival. This work describes a novel role for clathrin in liver tumorigenesis, favouring non-canonical pro-tumorigenic TGF-β pathways. CLTC expression in human HCC samples could help select patients that would benefit from TGF-β-targeted therapy. Primary hepatic endocytic functions are important in several physiological and pathological processes.Despite this, their molecular mechanisms remain poorly defined and remarkably understudied.1Ligand-induced internalization and degradation of receptor tyrosine kinases, such as epidermal growth factor receptor (EGFR) or hepatocyte growth factor receptor (c-MET), are relevant for maintenance and inhibition of their signalling pathways.Upon binding their respective ligands, each of these receptors are recruited into clathrin-coated pits eventually leading to endocytosis.However, clathrin might play additional roles, since Akt signalling following EGFR or MET activation requires clathrin, but could not require receptor endocytosis.2EGFR mediates differential signalling depending on its localization in the cell.3,4At the plasma membrane, clathrin is present in microdomains,5,6 where EGFR clustering occurs.7In these microdomains, clathrin may act as a scaffold protein, recruiting signalling adaptors.In human hepatocellular carcinoma (HCC), levels of clathrin heavy-chain protein help to distinguish early HCC from benign tumours and its expression is stronger in poorly differentiated HCC than in well differentiated HCC.8–10However, little is known about the molecular mechanisms behind these results.
Upon ligand binding, tyrosine kinase receptors, such as epidermal growth factor receptor (EGFR), are recruited into clathrin-coated pits for internalization by endocytosis, which is relevant for signalling and/or receptor degradation. In liver cells, transforming growth factor-β (TGF-β) induces both pro- and anti-apoptotic signals; the latter are mediated by the EGFR pathway. Since EGFR mainly traffics via clathrin-coated vesicles, we aimed to analyse the potential role of clathrin in TGF-β-induced signalling in liver cells and its relevance in liver cancer. Real-Time PCR and immunohistochemistry were used to analyse clathrin heavy-chain expression in human (CLTC) and mice (Cltc) liver tumours. Transient knockdown (siRNA) or overexpression of CLTC were used to analyse its role on TGF-β and EGFR signalling in vitro. Bioinformatic analysis was used to determine the effect of CLTC and TGFB1 expression on prognosis and overall survival in patients with hepatocellular carcinoma (HCC). Clathrin expression increased during liver tumorigenesis in humans and mice. CLTC knockdown cells responded to TGF-β phosphorylating SMADs (canonical signalling) but showed impairment in the anti-apoptotic signals (EGFR transactivation). Experiments of loss or gain of function in HCC cells reveal an essential role for clathrin in inhibiting TGF-β-induced apoptosis and upregulation of its pro-apoptotic target NOX4. Autocrine TGF-β signalling in invasive HCC cells upregulates CLTC expression, switching its role to pro-tumorigenic. A positive correlation between TGFB1 and CLTC was found in HCC cells and patients. Patients expressing high levels of TGFB1 and CLTC had a worse prognosis and lower overall survival. This work describes a novel role for clathrin in liver tumorigenesis, favouring non-canonical pro-tumorigenic TGF-β pathways. CLTC expression in human HCC samples could help select patients that would benefit from TGF-β-targeted therapy. In hepatocytes, transforming growth factor-β (TGF-β) induces both pro- and anti-apoptotic signals.11The anti-apoptotic signals are mediated by the EGFR pathway, which is transactivated by TGF-β through a mechanism that involves EGFR ligands’ upregulation and activation of the metalloprotease TACE/ADAM17, responsible for their shedding.12–14In HCC cells, TGF-β also upregulates the expression of EGFR ligands, which transactivates the EGFR pathway, counteracting its pro-apoptotic response.15EGFR targeting knockdown, or pharmacological inhibition, significantly enhances TGF-β-induced cell death, correlating with higher levels of the NADPH oxidase NOX4 and changes in the expression of BCL-2 and IAP families.Once cells overcome apoptosis, they respond to TGF-β by undergoing epithelial-mesenchymal transition (EMT), which confers migratory/invasive capacities and stem cell properties.16
Upon ligand binding, tyrosine kinase receptors, such as epidermal growth factor receptor (EGFR), are recruited into clathrin-coated pits for internalization by endocytosis, which is relevant for signalling and/or receptor degradation. In liver cells, transforming growth factor-β (TGF-β) induces both pro- and anti-apoptotic signals; the latter are mediated by the EGFR pathway. Since EGFR mainly traffics via clathrin-coated vesicles, we aimed to analyse the potential role of clathrin in TGF-β-induced signalling in liver cells and its relevance in liver cancer. Real-Time PCR and immunohistochemistry were used to analyse clathrin heavy-chain expression in human (CLTC) and mice (Cltc) liver tumours. Transient knockdown (siRNA) or overexpression of CLTC were used to analyse its role on TGF-β and EGFR signalling in vitro. Bioinformatic analysis was used to determine the effect of CLTC and TGFB1 expression on prognosis and overall survival in patients with hepatocellular carcinoma (HCC). Clathrin expression increased during liver tumorigenesis in humans and mice. CLTC knockdown cells responded to TGF-β phosphorylating SMADs (canonical signalling) but showed impairment in the anti-apoptotic signals (EGFR transactivation). Experiments of loss or gain of function in HCC cells reveal an essential role for clathrin in inhibiting TGF-β-induced apoptosis and upregulation of its pro-apoptotic target NOX4. Autocrine TGF-β signalling in invasive HCC cells upregulates CLTC expression, switching its role to pro-tumorigenic. A positive correlation between TGFB1 and CLTC was found in HCC cells and patients. Patients expressing high levels of TGFB1 and CLTC had a worse prognosis and lower overall survival. This work describes a novel role for clathrin in liver tumorigenesis, favouring non-canonical pro-tumorigenic TGF-β pathways. CLTC expression in human HCC samples could help select patients that would benefit from TGF-β-targeted therapy. We have recently reported that caveolin-1, a protein involved in intracellular traffic for which a role in HCC has been proposed,17 is necessary for the TGF-β-induced transactivation of the EGFR,18 switching the response to TGF-β from cytostatic to tumorigenic in liver tumour cells.19Much less is known about the potential role of clathrin in the TGF-β signalling in liver cells.In hepatocytes, Dooley’s group described that blocking clathrin trafficking does not alter the canonical-SMAD phosphorylation induced by TGF-β,20 which indicates that clathrin-dependent endocytosis is not required for the early signals induced by TGF-β.Nevertheless, the role of clathrin in other endocytosis-independent responses, or in the crosstalk between TGF-β and the EGFR pathway, is completely unknown.More knowledge in this area is necessary to understand the role of clathrin in hepatocarcinogenesis.
In order to design an effective vaccine against hepatitis C virus (HCV) infection, it is necessary to understand immune protection. A number of broadly reactive neutralizing antibodies have been isolated from B cells of HCV-infected patients. However, it remains unclear whether B cells producing such antibodies contribute to HCV clearance and long-term immune protection against HCV. We analysed the B cell repertoire of 13 injecting drug users from the Amsterdam Cohort Study, who were followed up for a median of 17.5 years after primary infection. Individuals were classified into 2 groups based on the outcome of HCV infection: 5 who became chronically infected either after primary infection or after reinfection, and 8 who were HCV RNA negative following spontaneous clearance of ≥1 HCV infection(s). From each individual, 10,000 CD27+IgG+B cells, collected 0.75 year after HCV infection, were cultured to characterize the antibody repertoire. Using a multiplex flow cytometry-based assay to study the antibody binding to E1E2 from genotype 1 to 6, we found that a high frequency of cross-genotype antibodies was associated with spontaneous clearance of 1 or multiple infections (p = 0.03). Epitope specificity of these cross-genotype antibodies was determined by alanine mutant scanning in 4 individuals who were HCV RNA negative following spontaneous clearance of 1 or multiple infections. Interestingly, the cross-genotype antibodies were mainly antigenic region 3 (AR3)-specific and showed cross-neutralizing activity against HCV. In addition to AR3 antibodies, 3 individuals developed antibodies recognizing antigenic region 4, of which 1 monoclonal antibody showed cross-neutralizing capacity. Together, these data suggest that a strong B cell response producing cross-genotype and neutralizing antibodies, especially targeting AR3, contributes to HCV clearance and long-term immune protection against HCV. Hepatitis C virus (HCV) is one of the major global public health problems, with 71 million chronically infected people worldwide, which results in 350,000 to 500,000 liver-related deaths per year.1Current direct-acting antiviral (DAA) treatment is very effective in clearing infection.2However, despite high DAA efficacy, treatment alone is unlikely to eliminate HCV by the year 2030 as envisioned by the World Health Organization (WHO),1 since treated individuals may become reinfected if exposure continues.Moreover, worldwide, most of the HCV-infected individuals are unaware of their HCV status because of the prolonged asymptomatic nature of HCV infection and limited access to diagnostic tests.3In addition, as a result of the high cost of treatment, a large proportion of the HCV-infected individuals are left untreated.3Therefore, for the global elimination of HCV, a preventive vaccine is urgently needed.
In order to design an effective vaccine against hepatitis C virus (HCV) infection, it is necessary to understand immune protection. A number of broadly reactive neutralizing antibodies have been isolated from B cells of HCV-infected patients. However, it remains unclear whether B cells producing such antibodies contribute to HCV clearance and long-term immune protection against HCV. We analysed the B cell repertoire of 13 injecting drug users from the Amsterdam Cohort Study, who were followed up for a median of 17.5 years after primary infection. Individuals were classified into 2 groups based on the outcome of HCV infection: 5 who became chronically infected either after primary infection or after reinfection, and 8 who were HCV RNA negative following spontaneous clearance of ≥1 HCV infection(s). From each individual, 10,000 CD27+IgG+B cells, collected 0.75 year after HCV infection, were cultured to characterize the antibody repertoire. Using a multiplex flow cytometry-based assay to study the antibody binding to E1E2 from genotype 1 to 6, we found that a high frequency of cross-genotype antibodies was associated with spontaneous clearance of 1 or multiple infections (p = 0.03). Epitope specificity of these cross-genotype antibodies was determined by alanine mutant scanning in 4 individuals who were HCV RNA negative following spontaneous clearance of 1 or multiple infections. Interestingly, the cross-genotype antibodies were mainly antigenic region 3 (AR3)-specific and showed cross-neutralizing activity against HCV. In addition to AR3 antibodies, 3 individuals developed antibodies recognizing antigenic region 4, of which 1 monoclonal antibody showed cross-neutralizing capacity. Together, these data suggest that a strong B cell response producing cross-genotype and neutralizing antibodies, especially targeting AR3, contributes to HCV clearance and long-term immune protection against HCV. Development of a protective vaccine against HCV seems feasible, since spontaneous clearance of the virus without antiviral therapy occurs in 25 to 40% of individuals after primary infection4 and in 30 to 60% of those who are reinfected following clearance of a primary infection.5While clearance of HCV infection has been associated with strong and broad T cell responses,6,7 little is known about the role of antibodies in spontaneous clearance of HCV infection and protection upon re-exposure.
In order to design an effective vaccine against hepatitis C virus (HCV) infection, it is necessary to understand immune protection. A number of broadly reactive neutralizing antibodies have been isolated from B cells of HCV-infected patients. However, it remains unclear whether B cells producing such antibodies contribute to HCV clearance and long-term immune protection against HCV. We analysed the B cell repertoire of 13 injecting drug users from the Amsterdam Cohort Study, who were followed up for a median of 17.5 years after primary infection. Individuals were classified into 2 groups based on the outcome of HCV infection: 5 who became chronically infected either after primary infection or after reinfection, and 8 who were HCV RNA negative following spontaneous clearance of ≥1 HCV infection(s). From each individual, 10,000 CD27+IgG+B cells, collected 0.75 year after HCV infection, were cultured to characterize the antibody repertoire. Using a multiplex flow cytometry-based assay to study the antibody binding to E1E2 from genotype 1 to 6, we found that a high frequency of cross-genotype antibodies was associated with spontaneous clearance of 1 or multiple infections (p = 0.03). Epitope specificity of these cross-genotype antibodies was determined by alanine mutant scanning in 4 individuals who were HCV RNA negative following spontaneous clearance of 1 or multiple infections. Interestingly, the cross-genotype antibodies were mainly antigenic region 3 (AR3)-specific and showed cross-neutralizing activity against HCV. In addition to AR3 antibodies, 3 individuals developed antibodies recognizing antigenic region 4, of which 1 monoclonal antibody showed cross-neutralizing capacity. Together, these data suggest that a strong B cell response producing cross-genotype and neutralizing antibodies, especially targeting AR3, contributes to HCV clearance and long-term immune protection against HCV. HCV clearance has been associated with a neutralizing antibody response against HCV E1E2 glycoproteins during the early phase of infection.8,9In addition, a number of broadly neutralizing antibodies have been isolated from B cells of chronically infected individuals.In general, they target 1 of 3 epitopes which are located in or around the CD81 binding site: epitope I (amino acids 412–423), epitope II (amino acids 434–446) and domain B (amino acids 523–535).10–12Interestingly, broadly neutralizing antibodies targeting epitope I or epitope II (AP33, HC84.26) protected humanized mice against HCV genotype 1b infection.13,14It has also been shown that AR3A and AR3B antibodies targeting both epitope II and domain B, jointly referred to as antigenic region 3 (AR3), protected mice against HCV infection from isolate H77 (genotype 1a) and isolate J6 (genotype 2a) in combination with AR4A, an antibody recognizing antigenic region 4 (AR4).15–17However, whether such broadly neutralizing antibodies contribute to spontaneous clearance and long-term immune protection in humans with ongoing exposure remains unanswered.
In order to design an effective vaccine against hepatitis C virus (HCV) infection, it is necessary to understand immune protection. A number of broadly reactive neutralizing antibodies have been isolated from B cells of HCV-infected patients. However, it remains unclear whether B cells producing such antibodies contribute to HCV clearance and long-term immune protection against HCV. We analysed the B cell repertoire of 13 injecting drug users from the Amsterdam Cohort Study, who were followed up for a median of 17.5 years after primary infection. Individuals were classified into 2 groups based on the outcome of HCV infection: 5 who became chronically infected either after primary infection or after reinfection, and 8 who were HCV RNA negative following spontaneous clearance of ≥1 HCV infection(s). From each individual, 10,000 CD27+IgG+B cells, collected 0.75 year after HCV infection, were cultured to characterize the antibody repertoire. Using a multiplex flow cytometry-based assay to study the antibody binding to E1E2 from genotype 1 to 6, we found that a high frequency of cross-genotype antibodies was associated with spontaneous clearance of 1 or multiple infections (p = 0.03). Epitope specificity of these cross-genotype antibodies was determined by alanine mutant scanning in 4 individuals who were HCV RNA negative following spontaneous clearance of 1 or multiple infections. Interestingly, the cross-genotype antibodies were mainly antigenic region 3 (AR3)-specific and showed cross-neutralizing activity against HCV. In addition to AR3 antibodies, 3 individuals developed antibodies recognizing antigenic region 4, of which 1 monoclonal antibody showed cross-neutralizing capacity. Together, these data suggest that a strong B cell response producing cross-genotype and neutralizing antibodies, especially targeting AR3, contributes to HCV clearance and long-term immune protection against HCV. The Amsterdam Cohort Study (ACS) among drug users (DU), is a prospective cohort study investigating human immunodeficiency virus (HIV) and HCV infections, in which serum, peripheral blood mononuclear cells (PBMCs), and socio-demographic and risk behavioural data are collected regularly.18The ACS-DU cohort started in 1985 and came to an end in 2016, making it a unique cohort with one of the longest follow-up periods recorded in DUs globally.19Importantly, all ACS participants who are injecting drug users (IDU) are, most likely, frequently (re-)exposed to HCV, since 65% reported at least weekly injecting drug use at enrolment in the cohort, with a baseline HCV antibody prevalence of 82% and an incidence of up to 28 primary infections per 100 person-years among IDU.20Such a well-characterized cohort of highly exposed individuals with a long follow-up presents a unique opportunity to study the role of broadly neutralizing antibodies in the spontaneous clearance of HCV infection and protection upon re-exposure.
The therapeutic outcomes of surgical resection (SR) or radiofrequency ablation (RFA) for perivascular hepatocellular carcinoma (HCC) have not been compared. The aim of this study was to compare SR with RFA as first-line treatment in patients with perivascular HCC and to evaluate the long-term outcomes of both therapies. This retrospective study was approved by the institutional review board. The requirement for informed consent was waived. Between January 2006 and December 2010, a total of 283 consecutive patients with small perivascular HCCs (≤3 cm, Barcelona Clinic Liver Cancer stage 0 or A) underwent SR (n = 182) or RFA (n = 101) as a first-line treatment. The progression-free survival (PFS) and overall survival (OS) rates were compared by propensity score matching. Subgroup analysis of these outcomes was conducted according to the type of hepatic vessels. The median follow-up was 7.8 years. Matching yielded 62 pairs of patients. In the two matched groups, the PFS rates at 5 and 10 years were 58.0% and 17.8%, respectively, in the SR group, and 25.4% and 14.1%, respectively, in the RFA group (p <0.001). The corresponding OS rates at 5 and 10 years were 93.5% and 91.9% in the SR group and 82.3% and 74.1% in the RFA group, respectively (p <0.001). In contrast to those in patients with perivenous HCCs, subgroup analysis indicated that extrahepatic recurrence and OS were significantly different according to the treatment modality in patients with periportal HCCs (p = 0.004 and p <0.001, respectively). In patients with small perivascular HCCs, SR provided better long-term tumor control and OS than RFA, particularly for periportal tumors. Recent clinical guidelines for the management of hepatocellular carcinoma (HCC) have indicated that liver transplantation, surgical resection (SR), and radiofrequency ablation (RFA) are curative treatment modalities for very early or early stage HCC.1,2Although the best therapy is liver transplantation, owing to the scarcity of donor organs, high cost, and longer waiting period of transplantation, SR and RFA have been mainstays for the curative treatment of HCC in clinical practice.3Many studies compared the therapeutic efficacy of these two therapies, and most of these studies demonstrated that the efficacy of RFA and SR in terms of survival outcomes was similar for a single small HCC of ≤3 cm.4–7
The therapeutic outcomes of surgical resection (SR) or radiofrequency ablation (RFA) for perivascular hepatocellular carcinoma (HCC) have not been compared. The aim of this study was to compare SR with RFA as first-line treatment in patients with perivascular HCC and to evaluate the long-term outcomes of both therapies. This retrospective study was approved by the institutional review board. The requirement for informed consent was waived. Between January 2006 and December 2010, a total of 283 consecutive patients with small perivascular HCCs (≤3 cm, Barcelona Clinic Liver Cancer stage 0 or A) underwent SR (n = 182) or RFA (n = 101) as a first-line treatment. The progression-free survival (PFS) and overall survival (OS) rates were compared by propensity score matching. Subgroup analysis of these outcomes was conducted according to the type of hepatic vessels. The median follow-up was 7.8 years. Matching yielded 62 pairs of patients. In the two matched groups, the PFS rates at 5 and 10 years were 58.0% and 17.8%, respectively, in the SR group, and 25.4% and 14.1%, respectively, in the RFA group (p <0.001). The corresponding OS rates at 5 and 10 years were 93.5% and 91.9% in the SR group and 82.3% and 74.1% in the RFA group, respectively (p <0.001). In contrast to those in patients with perivenous HCCs, subgroup analysis indicated that extrahepatic recurrence and OS were significantly different according to the treatment modality in patients with periportal HCCs (p = 0.004 and p <0.001, respectively). In patients with small perivascular HCCs, SR provided better long-term tumor control and OS than RFA, particularly for periportal tumors. In contrast to SR using a laparoscopic or open approach, tumor location is an important factor in the outcomes of RFA, because it is mainly performed using a percutaneous approach for minimal invasiveness.8Therefore, the high-risk locations of HCC adjacent to extrahepatic vital organs or large intrahepatic vessels can affect the treatment outcomes after RFA.9,10In particular, there are controversies on the local tumor control by RFA because of the heat-sink effect, which can modify the size of the ablation zone considerably in patients with perivascular HCCs.11–13In addition, recent studies suggested that the iatrogenic transportal tumor spread may occur in periportal tumors during RFA because RFA cannot remove a hepatic segment confined to tumor-bearing portal tributaries.14,15
The therapeutic outcomes of surgical resection (SR) or radiofrequency ablation (RFA) for perivascular hepatocellular carcinoma (HCC) have not been compared. The aim of this study was to compare SR with RFA as first-line treatment in patients with perivascular HCC and to evaluate the long-term outcomes of both therapies. This retrospective study was approved by the institutional review board. The requirement for informed consent was waived. Between January 2006 and December 2010, a total of 283 consecutive patients with small perivascular HCCs (≤3 cm, Barcelona Clinic Liver Cancer stage 0 or A) underwent SR (n = 182) or RFA (n = 101) as a first-line treatment. The progression-free survival (PFS) and overall survival (OS) rates were compared by propensity score matching. Subgroup analysis of these outcomes was conducted according to the type of hepatic vessels. The median follow-up was 7.8 years. Matching yielded 62 pairs of patients. In the two matched groups, the PFS rates at 5 and 10 years were 58.0% and 17.8%, respectively, in the SR group, and 25.4% and 14.1%, respectively, in the RFA group (p <0.001). The corresponding OS rates at 5 and 10 years were 93.5% and 91.9% in the SR group and 82.3% and 74.1% in the RFA group, respectively (p <0.001). In contrast to those in patients with perivenous HCCs, subgroup analysis indicated that extrahepatic recurrence and OS were significantly different according to the treatment modality in patients with periportal HCCs (p = 0.004 and p <0.001, respectively). In patients with small perivascular HCCs, SR provided better long-term tumor control and OS than RFA, particularly for periportal tumors. However, comparative studies of therapeutic outcomes for perivascular HCC between SR and RFA have not been performed.In addition, previous retrospective studies16,17 have indicated that, compared with surgical candidates, patients who underwent RFA were likely to be older and present poor liver function, which could affect the long-term outcomes of each treatment.
Two major body compartments, skeletal muscle and adipose tissue, exhibit independent functions. We aimed to explore the prognostic significance of skeletal muscle, visceral and subcutaneous adipose tissue, according to sex, in patients with cirrhosis assessed for liver transplantation (LT). CT images taken at the 3rd lumbar vertebra from 677 patients were quantified for three body composition indexes (cm2/m2), visceral adipose tissue index, subcutaneous adipose tissue index (SATI), and skeletal muscle index (SMI). Cox proportional and competing-risk analysis hazard models were conducted to assess associations between mortality and body composition. The majority of patients were male (67%) with a mean age of 57 ± 7 years, model for end-stage liver disease (MELD) score of 14 ± 8 and mean body mass index of 27 ± 6 kg/m2. Despite similar body mass index between the sexes, male patients had greater SMI (53 ± 12 vs. 45 ± 9 cm2/m2), whereas SATI (67 ± 52 vs. 48 ± 37 cm2/m2) was higher in females (p <0.001 for each). In sex stratified multivariate analyses after adjustment for MELD score and other confounding variables, SATI in females (hazard ratio [HR] 0.99; 95% CI 0.98–1.00; p = 0.01) and SMI in males (HR 0.98; 95% CI 0.96–1.00; p = 0.02) were significant predictors of mortality. Female patients with low SATI (<60 cm2/m2) had a higher risk of mortality (HR 2.06; 95% CI 1.08–3.91; p = 0.03). Using competitive risk analysis in female patients listed for LT, low SATI was also an independent predictor of mortality (subdistribution HR 2.80; 95% CI 1.28–6.12; p = 0.01) after adjusting for MELD, and other confounding factors. A lower SATI is associated with higher mortality in female patients with cirrhosis. Subcutaneous adipose tissue has a favorable metabolic profile – low SATI may reflect depletion of this major energy reservoir, leading to poor clinical outcomes. Differences in body composition between the sexes exist in healthy subjects,1,2 and populations with cancer3 and cirrhosis,4–6 with females having higher adipose tissue mass (adiposity) and males having greater muscularity.Body composition of people with cirrhosis has been assessed using approaches such as air displacement plethysmography, bioelectrical impedance analysis, and dual-energy X-ray absorptiometry.4,7However, the ability of these techniques to differentiate between two major body compartments, muscle and adipose tissue, as well as their capability to provide a specific measure of adipose tissue depots (i.e., visceral, subcutaneous) is limited.Moreover, their accuracy might also be affected by changes in fluid homeostasis, which is a frequent complication in cirrhosis.7Computed tomography (CT) image analysis has emerged as a specific and precise method for body composition assessment, that enables quantification of muscle and different adipose tissue depots.8In the clinical setting, CT is frequently requested for patients with cirrhosis as part of liver transplantation (LT) assessment, in order to pre-operatively map the vascular and biliary anatomy and screen for hepatocellular carcinoma (HCC).
Two major body compartments, skeletal muscle and adipose tissue, exhibit independent functions. We aimed to explore the prognostic significance of skeletal muscle, visceral and subcutaneous adipose tissue, according to sex, in patients with cirrhosis assessed for liver transplantation (LT). CT images taken at the 3rd lumbar vertebra from 677 patients were quantified for three body composition indexes (cm2/m2), visceral adipose tissue index, subcutaneous adipose tissue index (SATI), and skeletal muscle index (SMI). Cox proportional and competing-risk analysis hazard models were conducted to assess associations between mortality and body composition. The majority of patients were male (67%) with a mean age of 57 ± 7 years, model for end-stage liver disease (MELD) score of 14 ± 8 and mean body mass index of 27 ± 6 kg/m2. Despite similar body mass index between the sexes, male patients had greater SMI (53 ± 12 vs. 45 ± 9 cm2/m2), whereas SATI (67 ± 52 vs. 48 ± 37 cm2/m2) was higher in females (p <0.001 for each). In sex stratified multivariate analyses after adjustment for MELD score and other confounding variables, SATI in females (hazard ratio [HR] 0.99; 95% CI 0.98–1.00; p = 0.01) and SMI in males (HR 0.98; 95% CI 0.96–1.00; p = 0.02) were significant predictors of mortality. Female patients with low SATI (<60 cm2/m2) had a higher risk of mortality (HR 2.06; 95% CI 1.08–3.91; p = 0.03). Using competitive risk analysis in female patients listed for LT, low SATI was also an independent predictor of mortality (subdistribution HR 2.80; 95% CI 1.28–6.12; p = 0.01) after adjusting for MELD, and other confounding factors. A lower SATI is associated with higher mortality in female patients with cirrhosis. Subcutaneous adipose tissue has a favorable metabolic profile – low SATI may reflect depletion of this major energy reservoir, leading to poor clinical outcomes. Low body mass index (BMI) has been associated with increased mortality in patients with cirrhosis.9,10Considering variability in body composition within each BMI category,11 BMI cannot be applied as an appropriate indicator of skeletal muscle and adipose tissue, the two main body composition compartments, both of which exhibit divergent characteristics and functions.
Two major body compartments, skeletal muscle and adipose tissue, exhibit independent functions. We aimed to explore the prognostic significance of skeletal muscle, visceral and subcutaneous adipose tissue, according to sex, in patients with cirrhosis assessed for liver transplantation (LT). CT images taken at the 3rd lumbar vertebra from 677 patients were quantified for three body composition indexes (cm2/m2), visceral adipose tissue index, subcutaneous adipose tissue index (SATI), and skeletal muscle index (SMI). Cox proportional and competing-risk analysis hazard models were conducted to assess associations between mortality and body composition. The majority of patients were male (67%) with a mean age of 57 ± 7 years, model for end-stage liver disease (MELD) score of 14 ± 8 and mean body mass index of 27 ± 6 kg/m2. Despite similar body mass index between the sexes, male patients had greater SMI (53 ± 12 vs. 45 ± 9 cm2/m2), whereas SATI (67 ± 52 vs. 48 ± 37 cm2/m2) was higher in females (p <0.001 for each). In sex stratified multivariate analyses after adjustment for MELD score and other confounding variables, SATI in females (hazard ratio [HR] 0.99; 95% CI 0.98–1.00; p = 0.01) and SMI in males (HR 0.98; 95% CI 0.96–1.00; p = 0.02) were significant predictors of mortality. Female patients with low SATI (<60 cm2/m2) had a higher risk of mortality (HR 2.06; 95% CI 1.08–3.91; p = 0.03). Using competitive risk analysis in female patients listed for LT, low SATI was also an independent predictor of mortality (subdistribution HR 2.80; 95% CI 1.28–6.12; p = 0.01) after adjusting for MELD, and other confounding factors. A lower SATI is associated with higher mortality in female patients with cirrhosis. Subcutaneous adipose tissue has a favorable metabolic profile – low SATI may reflect depletion of this major energy reservoir, leading to poor clinical outcomes. Sarcopenia, defined as low muscle mass, has emerged as an independent predictor of mortality in patients with cirrhosis.5,6,12,13Sarcopenia is associated with elevated mortality on the LT waiting list6 and longer hospital stays following LT.14 There is a clear sex predisposition for sarcopenia in cirrhosis, being more prevalent in males than female patients.4,6,14In studies of the association between body composition and outcomes in cirrhosis the study participants are primarily male, potentially masking the effects of females, and making it imperative that the association between body composition and mortality is examined by sex.
Intratumor heterogeneity has frequently been reported in patients with hepatocellular carcinoma (HCC). Thus, the reliability of single-region tumor samples for evaluation of the tumor immune microenvironment is also debatable. We conducted a prospective study to analyze the similarity in tumor immune microenvironments among different regions of a single tumor. Multi-region sampling was performed on newly resected tumors. The tumor immune microenvironment was evaluated by immunohistochemical staining of PD-L1, CD4, CD8, CD20, FoxP3, DC-LAMP (or LAMP3), CD68, MPO, and tertiary lymphoid structures (TLSs). PD-L1 expression was manually quantified according to the percentage of PD-L1-stained tumor or stromal cells. The densities (number/mm2) of immune cells and the number of TLSs per sample were determined by whole-section counting. RNA-sequencing was applied in selected samples. Similarities in tumor immune microenvironments within each tumor were evaluated by multivariate Mahalanobis distance analyses. Thirteen tumors were collected from 12 patients. The median diameter of tumors was 9 cm (range 3–16 cm). A median of 6 samples (range 3–12) were obtained from each tumor. Nine (69.2%) tumors exhibited uniform expression of PD-L1 in all regions of the tumor. Out of 13 tumors analyzed by immunohistochemical staining, 8 (61.5%) tumors displayed a narrow Mahalanobis distance for all regions within the tumor; while 8 (66.7%) of the 12 tumors analyzed by RNA-sequencing displayed a narrow Mahalanobis distance. Immunohistochemistry and RNA-sequencing had a high concordance rate (83.3%; 10 of 12 tumors) for the evaluation of similarities between tumor immune microenvironments within a tumor. A single-region tumor sample might be reliable for the evaluation of tumor immune microenvironments in approximately 60–70% of patients with HCC. Identification of tissue-based immunological biomarkers is a critical step toward personalized immunotherapy and is usually conducted using formalin-fixed paraffin-embedded (FFPE) sections from a core biopsy specimen or a wax block from a previously resected tumor.However, the identification of predictive biomarkers for programmed cell death-1 (PD-1) inhibitors in patients with hepatocellular carcinoma (HCC) has not been achieved.Theoretically, the expression of programmed cell death ligand 1 (PD-L1) or the quantification of tumor-infiltrating T cells could predict responses to PD-1/PD-L1 blockade.However, none of these methods could clearly differentiate responders from non-responders in clinical trials.1,2
Intratumor heterogeneity has frequently been reported in patients with hepatocellular carcinoma (HCC). Thus, the reliability of single-region tumor samples for evaluation of the tumor immune microenvironment is also debatable. We conducted a prospective study to analyze the similarity in tumor immune microenvironments among different regions of a single tumor. Multi-region sampling was performed on newly resected tumors. The tumor immune microenvironment was evaluated by immunohistochemical staining of PD-L1, CD4, CD8, CD20, FoxP3, DC-LAMP (or LAMP3), CD68, MPO, and tertiary lymphoid structures (TLSs). PD-L1 expression was manually quantified according to the percentage of PD-L1-stained tumor or stromal cells. The densities (number/mm2) of immune cells and the number of TLSs per sample were determined by whole-section counting. RNA-sequencing was applied in selected samples. Similarities in tumor immune microenvironments within each tumor were evaluated by multivariate Mahalanobis distance analyses. Thirteen tumors were collected from 12 patients. The median diameter of tumors was 9 cm (range 3–16 cm). A median of 6 samples (range 3–12) were obtained from each tumor. Nine (69.2%) tumors exhibited uniform expression of PD-L1 in all regions of the tumor. Out of 13 tumors analyzed by immunohistochemical staining, 8 (61.5%) tumors displayed a narrow Mahalanobis distance for all regions within the tumor; while 8 (66.7%) of the 12 tumors analyzed by RNA-sequencing displayed a narrow Mahalanobis distance. Immunohistochemistry and RNA-sequencing had a high concordance rate (83.3%; 10 of 12 tumors) for the evaluation of similarities between tumor immune microenvironments within a tumor. A single-region tumor sample might be reliable for the evaluation of tumor immune microenvironments in approximately 60–70% of patients with HCC. One of the possible barriers to successful tissue-based biomarker identification is the considerable spatial heterogeneity of PD-L1 expression, T cell infiltration, or other immunological biomarkers within a tumor.For example, discordance of PD-L1 expression in different regions of a tumor has frequently been reported in cancer types with a higher mutational burden, such as non-small-cell lung cancer, melanoma, and head and neck cancer.3–7Some PD-L1-positive tumors may be misclassified as PD-L1-negative tumors, which leads to an unexpectedly high response rate of PD-1/PD-L1-blockade in patients with PD-L1-negative tumors.If the potential spatial heterogeneity of immunological biomarkers is a serious concern, multi-region tumor samples are preferred for biomarker studies to avoid biased biomarker-based patient selection or outcome correlation.Nevertheless, multi-region tumor samples are generally unavailable in cancer patients with disseminated metastasis when core needle biopsy is the only method for obtaining tumor tissue for analysis.
Intratumor heterogeneity has frequently been reported in patients with hepatocellular carcinoma (HCC). Thus, the reliability of single-region tumor samples for evaluation of the tumor immune microenvironment is also debatable. We conducted a prospective study to analyze the similarity in tumor immune microenvironments among different regions of a single tumor. Multi-region sampling was performed on newly resected tumors. The tumor immune microenvironment was evaluated by immunohistochemical staining of PD-L1, CD4, CD8, CD20, FoxP3, DC-LAMP (or LAMP3), CD68, MPO, and tertiary lymphoid structures (TLSs). PD-L1 expression was manually quantified according to the percentage of PD-L1-stained tumor or stromal cells. The densities (number/mm2) of immune cells and the number of TLSs per sample were determined by whole-section counting. RNA-sequencing was applied in selected samples. Similarities in tumor immune microenvironments within each tumor were evaluated by multivariate Mahalanobis distance analyses. Thirteen tumors were collected from 12 patients. The median diameter of tumors was 9 cm (range 3–16 cm). A median of 6 samples (range 3–12) were obtained from each tumor. Nine (69.2%) tumors exhibited uniform expression of PD-L1 in all regions of the tumor. Out of 13 tumors analyzed by immunohistochemical staining, 8 (61.5%) tumors displayed a narrow Mahalanobis distance for all regions within the tumor; while 8 (66.7%) of the 12 tumors analyzed by RNA-sequencing displayed a narrow Mahalanobis distance. Immunohistochemistry and RNA-sequencing had a high concordance rate (83.3%; 10 of 12 tumors) for the evaluation of similarities between tumor immune microenvironments within a tumor. A single-region tumor sample might be reliable for the evaluation of tumor immune microenvironments in approximately 60–70% of patients with HCC. As more novel immunotherapies targeting various immune cells, such as macrophages, regulatory T cells (Treg), and natural killer (NK) cells, are emerging for patients with HCC, biomarker studies beyond PD-L1 expression or T cell infiltration are expected in the near future.It remains to be determined whether a random, single-region HCC sample is reliable for evaluating the complex tumor immune microenvironment of a whole tumor.
The effects of long-term antiviral therapy on survival have not been adequately assessed in chronic hepatitis B (CHB). In this 10-centre, ongoing cohort study, we evaluated the probability of survival and factors affecting survival in Caucasian CHB patients who received long-term entecavir/tenofovir therapy. We included 1,951 adult Caucasians with CHB, with or without compensated cirrhosis and without hepatocellular carcinoma (HCC) at baseline, who received entecavir/tenofovir for ≥12 months (median, six years). Kaplan–Meier estimates of cumulative survival over time were obtained. Standardized mortality ratios (SMRs) were calculated by comparing death rates with those in the Human Mortality Database. The one-, five-, and eight-year cumulative probabilities were 99.7, 95.9, and 94.1% for overall survival, 99.9, 98.3, and 97.4% for liver-related survival, and 99.9, 97.8, and 95.8% for transplantation-free liver-related survival, respectively. Overall mortality was independently associated with older age and HCC development, liver-related mortality was associated with HCC development only, and transplantation-free liver-related mortality was independently associated with HCC development and lower platelet levels at baseline. Baseline cirrhosis was not independently associated with any type of mortality. Compared with the general population, in all CHB patients mortality was not significantly different (SMR 0.82), whereas it was lower in patients without HCC regardless of baseline cirrhosis (SMR 0.58) and was higher in patients who developed HCC (SMR 3.09). Caucasian patients with CHB and compensated liver disease who receive long-term entecavir/tenofovir therapy have excellent overall and liver-related eight-year survival, which is similar to that of the general population. HCC is the main factor affecting their overall mortality, and is the only factor affecting their liver-related mortality. Chronic infection with hepatitis B virus (HBV) is one of the most common causes of chronic liver disease worldwide.1–3Patients with chronic HBV infection may have low viral replication and no significant histological lesions, but a substantial proportion of them develop chronic hepatitis B (CHB) with high viral replication and active histological lesions.1,4,5If left untreated, CHB leads to accumulation of liver fibrosis and eventually progresses to cirrhosis and liver decompensation, and is thus associated with high morbidity and mortality.1,4,5It is estimated that 2–10% of untreated CHB patients develop cirrhosis every year,1,4,6 and that only 55–85% of untreated patients with active HBV cirrhosis are alive five years later.4,6,7In addition, all patients with chronic HBV infection are at higher risk of hepatocellular carcinoma (HCC) when compared with the general population, but the risk is highest when cirrhosis is present.1,5,6Thus, more than 750,000 people die every year of HBV-related causes.8
The effects of long-term antiviral therapy on survival have not been adequately assessed in chronic hepatitis B (CHB). In this 10-centre, ongoing cohort study, we evaluated the probability of survival and factors affecting survival in Caucasian CHB patients who received long-term entecavir/tenofovir therapy. We included 1,951 adult Caucasians with CHB, with or without compensated cirrhosis and without hepatocellular carcinoma (HCC) at baseline, who received entecavir/tenofovir for ≥12 months (median, six years). Kaplan–Meier estimates of cumulative survival over time were obtained. Standardized mortality ratios (SMRs) were calculated by comparing death rates with those in the Human Mortality Database. The one-, five-, and eight-year cumulative probabilities were 99.7, 95.9, and 94.1% for overall survival, 99.9, 98.3, and 97.4% for liver-related survival, and 99.9, 97.8, and 95.8% for transplantation-free liver-related survival, respectively. Overall mortality was independently associated with older age and HCC development, liver-related mortality was associated with HCC development only, and transplantation-free liver-related mortality was independently associated with HCC development and lower platelet levels at baseline. Baseline cirrhosis was not independently associated with any type of mortality. Compared with the general population, in all CHB patients mortality was not significantly different (SMR 0.82), whereas it was lower in patients without HCC regardless of baseline cirrhosis (SMR 0.58) and was higher in patients who developed HCC (SMR 3.09). Caucasian patients with CHB and compensated liver disease who receive long-term entecavir/tenofovir therapy have excellent overall and liver-related eight-year survival, which is similar to that of the general population. HCC is the main factor affecting their overall mortality, and is the only factor affecting their liver-related mortality. Over the past 15 years, the development of safe and effective therapies has improved the outcome of CHB patients who receive appropriate diagnosis and treatment.5,9In particular, during the past 10 years, the use of current first-line nucleos(t)ide analogues, such as entecavir (ETV) and tenofovir disoproxil fumarate (TDF), has offered prolonged inhibition of HBV replication in almost all adherent CHB patients, reduction of liver necroinflammation and fibrosis, sometimes reversion of histological cirrhosis, and prevention or even reversal of early liver decompensation.1,5Moreover, long-term ETV or TDF therapy has been shown to decrease but not eliminate the risk of HCC.10–15However, the effect of long-term antiviral therapy on survival has not been adequately assessed in this setting.
Progressive familial intrahepatic cholestasis type 3 (PFIC3), for which there are limited therapeutic options, often leads to end-stage liver disease before adulthood due to impaired ABCB4-dependent phospholipid transport to bile. Using adeno-associated virus serotype 8 (AAV8)-mediated gene therapy, we aimed to restore the phospholipid content in bile to levels that prevent liver damage, thereby enabling stable hepatic ABCB4 expression and long-term correction of the phenotype in a murine model of PFIC3. Ten-week-old Abcb4−/− mice received a single dose of AAV8-hABCB4 (n = 10) or AAV8-GFP (n = 7) under control of a liver specific promoter via tail vein injection. Animals were sacrificed either 10 or 26 weeks after vector administration to assess transgene persistence, after being challenged with a 0.1% cholate diet for 2 weeks. Periodic evaluation of plasma cholestatic markers was performed and bile duct cannulation enabled analysis of biliary phospholipids. Liver fibrosis and the Ki67 proliferation index were assessed by immunohistochemistry. Stable transgene expression was achieved in all animals that received AAV8-hABCB4 up to 26 weeks after administration. AAV8-hABCB4 expression restored biliary phospholipid excretion, increasing the phospholipid and cholesterol content in bile to levels that ameliorate liver damage. This resulted in normalization of the plasma cholestatic markers, alkaline phosphatase and bilirubin. In addition, AAV8-hABCB4 prevented progressive liver fibrosis and reduced hepatocyte proliferation for the duration of the study. Liver-directed gene therapy provides stable hepatic ABCB4 expression and long-term correction of the phenotype in a murine model of PFIC3. Translational studies that verify the clinical feasibility of this approach are warranted. Progressive familial intrahepatic cholestasis type 3 (PFIC3) is an autosomal-recessive liver disorder.Patients with PFIC3 present with cholestasis at a young age, which progresses to cirrhosis and end-stage liver disease before adulthood.1–3PFIC3 is caused by impairment of phosphatidylcholine (PC) translocation to bile by the canalicular membrane protein ATP binding cassette subfamily B member 4 (ABCB4), formerly known as multidrug resistance protein 3 (MDR3), encoded by the ABCB4 gene.4–6In bile, PC is essential in the formation of mixed micelles with bile salts that protect the lining of the biliary tree from the detergent properties of bile salts.In the absence of PC transport, bile salt–induced cytotoxicity causes progressive destruction of cholangiocytes, mainly of small bile ducts, and hepatocytes leading to intrahepatic cholestasis and progressive liver damage.4,7
Progressive familial intrahepatic cholestasis type 3 (PFIC3), for which there are limited therapeutic options, often leads to end-stage liver disease before adulthood due to impaired ABCB4-dependent phospholipid transport to bile. Using adeno-associated virus serotype 8 (AAV8)-mediated gene therapy, we aimed to restore the phospholipid content in bile to levels that prevent liver damage, thereby enabling stable hepatic ABCB4 expression and long-term correction of the phenotype in a murine model of PFIC3. Ten-week-old Abcb4−/− mice received a single dose of AAV8-hABCB4 (n = 10) or AAV8-GFP (n = 7) under control of a liver specific promoter via tail vein injection. Animals were sacrificed either 10 or 26 weeks after vector administration to assess transgene persistence, after being challenged with a 0.1% cholate diet for 2 weeks. Periodic evaluation of plasma cholestatic markers was performed and bile duct cannulation enabled analysis of biliary phospholipids. Liver fibrosis and the Ki67 proliferation index were assessed by immunohistochemistry. Stable transgene expression was achieved in all animals that received AAV8-hABCB4 up to 26 weeks after administration. AAV8-hABCB4 expression restored biliary phospholipid excretion, increasing the phospholipid and cholesterol content in bile to levels that ameliorate liver damage. This resulted in normalization of the plasma cholestatic markers, alkaline phosphatase and bilirubin. In addition, AAV8-hABCB4 prevented progressive liver fibrosis and reduced hepatocyte proliferation for the duration of the study. Liver-directed gene therapy provides stable hepatic ABCB4 expression and long-term correction of the phenotype in a murine model of PFIC3. Translational studies that verify the clinical feasibility of this approach are warranted. Currently, therapeutic options for PFIC3 are limited.Treatment with ursodeoxycholic acid (UDCA) achieves biochemical improvement by its anticholestatic effects and by stimulating biliary HCO3− secretion at the level of the hepatocyte and cholangiocyte, shielding the cellular membranes from entry of toxic protonated glycine-conjugated bile salts (biliary HCO3− umbrella hypothesis).3,8,9Evidence for long-term clinical benefit is lacking due to the lack of prospective randomized, placebo-controlled trials.Partial external biliary diversion (PEBD) is the primary surgical intervention in the management of patients with PFIC and can postpone the need for eventual liver transplantation, but only if attempted before major liver damage has occurred.10,11We have to note that the reported number of PEBD performed in patients with PFIC3, as opposed to those with PFIC1 and PFIC2, is extremely low and that only a marginal therapeutic effect can be expected based on the pathophysiology.Liver transplantation is the only curative treatment for PFIC3.Despite the risk of short- and long-term complications after liver transplantation and procedure-associated mortality, the reported patient survival of 75–100% and primary graft survival of 73–89% are high.12,13It is mainly the lifetime burden of immunosuppressive therapy and the limited availability of donor livers that warrant the development of alternative therapeutic strategies.
Progressive familial intrahepatic cholestasis type 3 (PFIC3), for which there are limited therapeutic options, often leads to end-stage liver disease before adulthood due to impaired ABCB4-dependent phospholipid transport to bile. Using adeno-associated virus serotype 8 (AAV8)-mediated gene therapy, we aimed to restore the phospholipid content in bile to levels that prevent liver damage, thereby enabling stable hepatic ABCB4 expression and long-term correction of the phenotype in a murine model of PFIC3. Ten-week-old Abcb4−/− mice received a single dose of AAV8-hABCB4 (n = 10) or AAV8-GFP (n = 7) under control of a liver specific promoter via tail vein injection. Animals were sacrificed either 10 or 26 weeks after vector administration to assess transgene persistence, after being challenged with a 0.1% cholate diet for 2 weeks. Periodic evaluation of plasma cholestatic markers was performed and bile duct cannulation enabled analysis of biliary phospholipids. Liver fibrosis and the Ki67 proliferation index were assessed by immunohistochemistry. Stable transgene expression was achieved in all animals that received AAV8-hABCB4 up to 26 weeks after administration. AAV8-hABCB4 expression restored biliary phospholipid excretion, increasing the phospholipid and cholesterol content in bile to levels that ameliorate liver damage. This resulted in normalization of the plasma cholestatic markers, alkaline phosphatase and bilirubin. In addition, AAV8-hABCB4 prevented progressive liver fibrosis and reduced hepatocyte proliferation for the duration of the study. Liver-directed gene therapy provides stable hepatic ABCB4 expression and long-term correction of the phenotype in a murine model of PFIC3. Translational studies that verify the clinical feasibility of this approach are warranted. Adeno-associated virus (AAV) vector mediated gene therapy is an attractive, potentially curative approach to treat inherited gene deficiencies and has taken a leap towards clinical application in the last decade.AAV-mediated transfer of a functional copy of the clotting factor IX gene into hepatocytes has shown to be safe and provided a long-term reduction of bleeding episodes in patients with hemophilia B.14 This approach is currently under clinical investigation for other indications such as Crigler-Najjar syndrome (NCT03466463),15 Mucopolysaccharidosis type VI (NCT03173521)16 and Pompe disease (NCT03533673).17The AAV system is established as the vector of choice because of its superior safety profile and non-integrative nature.18AAV-mediated gene transfer results in episomal transgene expression, which is a major safety advantage over gene editing approaches, but has limited its application to quiescent tissue to reduce the chance of gradual vector loss upon cell division.19–21Low transgene persistence is expected when targeting proliferative tissue.This is especially undesirable since effective vector re-administration is impaired by neutralizing antibody formation, triggered by the initial vector exposure.22Despite the current paradigm, we believe that AAV-mediated gene therapy directed to the liver can be an attractive approach to treat PFIC3 even though the disorder is associated with considerable hepatocyte proliferation.For a single dose of AAV vector to be effective and provide long-term correction in PFIC3, it would be necessary to completely eliminate the proliferative trigger and achieve stable transgene expression.Our current study was set up to test this hypothesis.
Resection is the most widely used potentially curative treatment for patients with early hepatocellular carcinoma (HCC). However, recurrence within 2 years occurs in 30–50% of patients, being the major cause of mortality. Herein, we describe 2 models, both based on widely available clinical data, which permit risk of early recurrence to be assessed before and after resection. A total of 3,903 patients undergoing surgical resection with curative intent were recruited from 6 different centres. We built 2 models for early recurrence, 1 using preoperative and 1 using pre and post-operative data, which were internally validated in the Hong Kong cohort. The models were then externally validated in European, Chinese and US cohorts. We developed 2 online calculators to permit easy clinical application. Multivariable analysis identified male gender, large tumour size, multinodular tumour, high albumin-bilirubin (ALBI) grade and high serum alpha-fetoprotein as the key parameters related to early recurrence. Using these variables, a preoperative model (ERASL-pre) gave 3 risk strata for recurrence-free survival (RFS) in the entire cohort – low risk: 2-year RFS 64.8%, intermediate risk: 2-year RFS 42.5% and high risk: 2-year RFS 20.7%. Median survival in each stratum was similar between centres and the discrimination between the 3 strata was enhanced in the post-operative model (ERASL-post) which included ‘microvascular invasion’. Statistical models that can predict the risk of early HCC recurrence after resection have been developed, extensively validated and shown to be applicable in the international setting. Such models will be valuable in guiding surveillance follow-up and in the design of post-resection adjuvant therapy trials. Worldwide, hepatocellular carcinoma (HCC) is the sixth most frequent malignancy and the second most common cause of cancer-related death.1There is a wide variety of therapeutic options for patients with HCC, depending on tumour burden, liver function and performance status.2Potentially curative therapy recommended for those patients with very early/early stage tumour (Barcelona Clinic Liver Cancer [BCLC] 0/A) consists of surgical resection, liver transplantation or local ablation.Because of the scarcity of donor organs, surgical resection and ablation are the mainstay of curative treatment options in Asian-Pacific countries, which account for three-quarters of all new patients globally.1Surgical resection provides better clinical outcome than local ablation particularly among patients with well-preserved hepatic function.3,4
Resection is the most widely used potentially curative treatment for patients with early hepatocellular carcinoma (HCC). However, recurrence within 2 years occurs in 30–50% of patients, being the major cause of mortality. Herein, we describe 2 models, both based on widely available clinical data, which permit risk of early recurrence to be assessed before and after resection. A total of 3,903 patients undergoing surgical resection with curative intent were recruited from 6 different centres. We built 2 models for early recurrence, 1 using preoperative and 1 using pre and post-operative data, which were internally validated in the Hong Kong cohort. The models were then externally validated in European, Chinese and US cohorts. We developed 2 online calculators to permit easy clinical application. Multivariable analysis identified male gender, large tumour size, multinodular tumour, high albumin-bilirubin (ALBI) grade and high serum alpha-fetoprotein as the key parameters related to early recurrence. Using these variables, a preoperative model (ERASL-pre) gave 3 risk strata for recurrence-free survival (RFS) in the entire cohort – low risk: 2-year RFS 64.8%, intermediate risk: 2-year RFS 42.5% and high risk: 2-year RFS 20.7%. Median survival in each stratum was similar between centres and the discrimination between the 3 strata was enhanced in the post-operative model (ERASL-post) which included ‘microvascular invasion’. Statistical models that can predict the risk of early HCC recurrence after resection have been developed, extensively validated and shown to be applicable in the international setting. Such models will be valuable in guiding surveillance follow-up and in the design of post-resection adjuvant therapy trials. However, tumour recurrence is a major post-operative complication and is generally classified into early or late recurrence by using 2 years as the cut-off.5,6Early recurrence (i.e. within 2 years of resection) accounts for more than 70% of tumour recurrence and is assumed to represent ‘true recurrence’ whereas after this period “recurrences” are assumed to be largely accounted for by ‘de novo’ tumours.7The 2-year recurrence-free survival (RFS) is about 50% and 30% among those with BCLC 0 or A tumours, respectively.7–9Identification of patients after potentially curative surgery who are at high risk of recurrence allows clinicians to provide appropriate surveillance to detect recurrent HCC at its earliest stage, when curative therapy may still be feasible.
Resection is the most widely used potentially curative treatment for patients with early hepatocellular carcinoma (HCC). However, recurrence within 2 years occurs in 30–50% of patients, being the major cause of mortality. Herein, we describe 2 models, both based on widely available clinical data, which permit risk of early recurrence to be assessed before and after resection. A total of 3,903 patients undergoing surgical resection with curative intent were recruited from 6 different centres. We built 2 models for early recurrence, 1 using preoperative and 1 using pre and post-operative data, which were internally validated in the Hong Kong cohort. The models were then externally validated in European, Chinese and US cohorts. We developed 2 online calculators to permit easy clinical application. Multivariable analysis identified male gender, large tumour size, multinodular tumour, high albumin-bilirubin (ALBI) grade and high serum alpha-fetoprotein as the key parameters related to early recurrence. Using these variables, a preoperative model (ERASL-pre) gave 3 risk strata for recurrence-free survival (RFS) in the entire cohort – low risk: 2-year RFS 64.8%, intermediate risk: 2-year RFS 42.5% and high risk: 2-year RFS 20.7%. Median survival in each stratum was similar between centres and the discrimination between the 3 strata was enhanced in the post-operative model (ERASL-post) which included ‘microvascular invasion’. Statistical models that can predict the risk of early HCC recurrence after resection have been developed, extensively validated and shown to be applicable in the international setting. Such models will be valuable in guiding surveillance follow-up and in the design of post-resection adjuvant therapy trials. Curative therapy offers much more favourable long-term survival than palliative therapy among patients with recurrent HCC.3,10,11Patients at high risk of early recurrence are potential candidates for clinical trials of adjuvant therapy although there is no standard of care for adjuvant therapy for surgically treated patients with HCC.6,12–15
Resection is the most widely used potentially curative treatment for patients with early hepatocellular carcinoma (HCC). However, recurrence within 2 years occurs in 30–50% of patients, being the major cause of mortality. Herein, we describe 2 models, both based on widely available clinical data, which permit risk of early recurrence to be assessed before and after resection. A total of 3,903 patients undergoing surgical resection with curative intent were recruited from 6 different centres. We built 2 models for early recurrence, 1 using preoperative and 1 using pre and post-operative data, which were internally validated in the Hong Kong cohort. The models were then externally validated in European, Chinese and US cohorts. We developed 2 online calculators to permit easy clinical application. Multivariable analysis identified male gender, large tumour size, multinodular tumour, high albumin-bilirubin (ALBI) grade and high serum alpha-fetoprotein as the key parameters related to early recurrence. Using these variables, a preoperative model (ERASL-pre) gave 3 risk strata for recurrence-free survival (RFS) in the entire cohort – low risk: 2-year RFS 64.8%, intermediate risk: 2-year RFS 42.5% and high risk: 2-year RFS 20.7%. Median survival in each stratum was similar between centres and the discrimination between the 3 strata was enhanced in the post-operative model (ERASL-post) which included ‘microvascular invasion’. Statistical models that can predict the risk of early HCC recurrence after resection have been developed, extensively validated and shown to be applicable in the international setting. Such models will be valuable in guiding surveillance follow-up and in the design of post-resection adjuvant therapy trials. Currently, there is no consensus regarding the optimal tool for risk stratification, which may partially contribute to failure of clinical trials of adjuvant therapy because of suboptimal patient selection.Except for the American Joint Committee on Cancer (AJCC) tumour-node-metastasis (TNM) system, the majority of HCC staging systems are not derived from surgically managed patients.Their prognostic performances on classifying post-operative early recurrence have not been fully evaluated.A few models including the Singapore Liver Cancer Recurrence (SLICER) score, the Korean model, Surgery-Specific Cancer of the Liver Italian Program (SS-CLIP), have been developed specifically to detect tumour recurrence after surgical resection but none of them have been externally validated.8,9,16Moreover, microvascular invasion is an important component of AJCC TNM, SLICER, SS-CLIP and Korean models, but can only be evaluated pathologically in the resected specimen after operation.A prognostic model that only requires parameters that are available preoperatively may help surgeons to better select surgical candidates.
Liver cancer is the second leading cause of cancer death worldwide. Hepatocellular carcinoma (HCC) is the most common type of primary liver cancer in adults. The aim of this study was to define the role of the long non-coding RNA lncHDAC2 in the tumorigenesis of HCC. CD13+CD133+ cells (hereafter called liver cancer stem cells [CSCs]) and CD13-CD133- cells (referred to as non-CSCs) were sorted from 3 primary HCC tumor tissues and followed by transcriptome microarray. The expression and function of lncHDAC2 were further assessed by northern blot, sphere formation and xenograft tumor models. LncHDAC2 is highly expressed in HCC tumors and liver CSCs. LncHDAC2 promotes the self-renewal of liver CSCs and tumor propagation. In liver CSCs, lncHDAC2 recruits the NuRD complex onto the promoter of PTCH1 to inhibit its expression, leading to activation of Hedgehog signaling. Moreover, HDAC2 expression levels are positively related to HCC severity and PTCH1 levels are negatively related to HCC severity. Additionally, the Smo inhibitor cyclopamine was shown to impair the self-renewal of liver CSCs and suppress tumor propagation. Our findings reveal that lncHDAC2 promotes the self-renewal of liver CSCs and tumor propagation by activating the Hedgehog signaling pathway. Downregulating lncHDAC2 is a promising antitumor strategy in HCC. Hepatocellular carcinoma (HCC), the most common type of primary liver cancer, is one of the leading causes of cancer death globally.The highest incidence of HCC is in East and South-East Asia and Northern and Western Africa.1However, the incidence of liver cancer, including HCC, has risen in areas with historically low rates, for instance, Western Europe, Northern America and parts of Oceania.Infection with hepatitis B virus and hepatitis C virus, as well as metabolic disorders, are etiologically responsible for HCC.2The high rate of recurrence and heterogeneity render HCC intractable.3Therefore, the mechanism underlying liver carcinogenesis remain elusive.
Liver cancer is the second leading cause of cancer death worldwide. Hepatocellular carcinoma (HCC) is the most common type of primary liver cancer in adults. The aim of this study was to define the role of the long non-coding RNA lncHDAC2 in the tumorigenesis of HCC. CD13+CD133+ cells (hereafter called liver cancer stem cells [CSCs]) and CD13-CD133- cells (referred to as non-CSCs) were sorted from 3 primary HCC tumor tissues and followed by transcriptome microarray. The expression and function of lncHDAC2 were further assessed by northern blot, sphere formation and xenograft tumor models. LncHDAC2 is highly expressed in HCC tumors and liver CSCs. LncHDAC2 promotes the self-renewal of liver CSCs and tumor propagation. In liver CSCs, lncHDAC2 recruits the NuRD complex onto the promoter of PTCH1 to inhibit its expression, leading to activation of Hedgehog signaling. Moreover, HDAC2 expression levels are positively related to HCC severity and PTCH1 levels are negatively related to HCC severity. Additionally, the Smo inhibitor cyclopamine was shown to impair the self-renewal of liver CSCs and suppress tumor propagation. Our findings reveal that lncHDAC2 promotes the self-renewal of liver CSCs and tumor propagation by activating the Hedgehog signaling pathway. Downregulating lncHDAC2 is a promising antitumor strategy in HCC. Most human tumors exhibit cellular heterogeneity due to inherently high genetic instability, resulting in the evolution of cells with the capacity of tumor initiation.Cancer stem cells (CSCs) refer to a subset of tumor cells with many phenotypic and functional properties of normal stem cells, including the ability of self-renewal and differentiation.4–6Numerous cell surface markers have been identified to isolate liver CSCs, including CD13, CD133, EpCAM, CD24, CD90 and CD44.7,8CSCs display some characteristics of embryonic or tissue stem cells, and typically harbor persistent activation of one or more highly conserved stemness signaling pathways, such as Hedgehog (Hh), Notch, and Wnt pathways.9–11Activation of the Hh signaling pathway is implicated in the initiation of distinct cancers of the liver, brain, muscle and skin.12–15The Hh ligands include Sonic (Shh), Desert (Dhh) and Indian (Ihh) Hh, which initiate a signaling cascade via binding with their common receptor Patched 1 (PTCH1).16Following Hh engagement, PTCH1 stops inhibiting the regulatory transmembrane protein Smoothened (Smo), allowing it to migrate to the tip of the primary cilium and activate Gli transcription factors that drive tumor initiation.17However, how Hh signaling regulates liver CSCs remains largely unknown.
Alpha-1 antitrypsin deficiency (AATD) is an uncommonly recognized cause of liver disease in adults, with descriptions of its natural history limited to case series and patient-reported data from disease registries. Liver pathology is limited to selected patients or unavailable. Therefore, we aimed to determine the prevalence and severity of liver fibrosis in an adult AATD population who were not known to have cirrhosis, while defining risk factors for fibrosis and testing non-invasive markers of disease. A total of 94 adults with classic genotype ‘PI*ZZ’ AATD were recruited from North America and prospectively enrolled in the study. Liver aminotransferases and markers of synthetic function, transient elastography, and liver biopsy were performed. The prevalence of clinically significant liver fibrosis (F ≥ 2) was 35.1%. Alanine aminotransferase, aspartate aminotransferase and gamma-glutamyltransferase values were higher in the F ≥ 2 group. Metabolic syndrome was associated with the presence of clinically significant fibrosis (OR 14.2; 95% CI 3.7–55; p <0.001). Additionally, the presence of accumulated abnormal AAT in hepatocytes, portal inflammation, and hepatocellular degeneration were associated with clinically significant fibrosis. The accuracy of transient elastography to detect F ≥ 2 fibrosis was fair, with an AUC of 0.70 (95% CI 0.58–0.82). Over one-third of asymptomatic and lung affected adults with ‘PI*ZZ’ AATD have significant underlying liver fibrosis. Liver biopsies demonstrated variable amounts of accumulated Z AAT. The risk of liver fibrosis increases in the presence of metabolic syndrome, accumulation of AAT in hepatocytes, and portal inflammation on baseline biopsy. The results support the hypothesis that liver disease in this genetic condition may be related to a “toxic gain of function” from accumulation of AAT in hepatocytes. Alpha-1 antitrypsin deficiency (AATD) is an inherited cause of liver disease in adults and children.The underlying genetic change is a point mutation in the serine protease inhibitor alpha-1-antitrypsin.1In the classic form of AATD, affected individuals are homozygous for the Z allele (ZZ), where a Glu342Lys substitution leads to a misfolded protein.The resulting protein conformation alpha-1 antitrypsin Z (ATZ) favors retention and subsequent polymerization within the hepatocyte endoplasmic reticulum (ER).2These polymers form the PAS-positive inclusions that are the hallmark of AATD liver disease on biopsy.Accumulation of ATZ within the ER leads to activation of intracellular disposal mechanisms for misfolded proteins and clearance from hepatocytes.3Insufficient clearance of ATZ leads to further polymerization and retention, which is widely accepted as the cause of liver disease.Experimentally, hepatocytes with the highest burden of ATZ retention and accumulation are the most susceptible to injury and cell death.4Conversely, blocking production and/or buildup of ATZ leads to reversal of liver injury in disease models.5,6
Alpha-1 antitrypsin deficiency (AATD) is an uncommonly recognized cause of liver disease in adults, with descriptions of its natural history limited to case series and patient-reported data from disease registries. Liver pathology is limited to selected patients or unavailable. Therefore, we aimed to determine the prevalence and severity of liver fibrosis in an adult AATD population who were not known to have cirrhosis, while defining risk factors for fibrosis and testing non-invasive markers of disease. A total of 94 adults with classic genotype ‘PI*ZZ’ AATD were recruited from North America and prospectively enrolled in the study. Liver aminotransferases and markers of synthetic function, transient elastography, and liver biopsy were performed. The prevalence of clinically significant liver fibrosis (F ≥ 2) was 35.1%. Alanine aminotransferase, aspartate aminotransferase and gamma-glutamyltransferase values were higher in the F ≥ 2 group. Metabolic syndrome was associated with the presence of clinically significant fibrosis (OR 14.2; 95% CI 3.7–55; p <0.001). Additionally, the presence of accumulated abnormal AAT in hepatocytes, portal inflammation, and hepatocellular degeneration were associated with clinically significant fibrosis. The accuracy of transient elastography to detect F ≥ 2 fibrosis was fair, with an AUC of 0.70 (95% CI 0.58–0.82). Over one-third of asymptomatic and lung affected adults with ‘PI*ZZ’ AATD have significant underlying liver fibrosis. Liver biopsies demonstrated variable amounts of accumulated Z AAT. The risk of liver fibrosis increases in the presence of metabolic syndrome, accumulation of AAT in hepatocytes, and portal inflammation on baseline biopsy. The results support the hypothesis that liver disease in this genetic condition may be related to a “toxic gain of function” from accumulation of AAT in hepatocytes. The natural history of AATD is not well defined, and clinical liver disease may appear at any age.7,8Cirrhosis remains a significant cause of mortality and is more commonly an indication for liver transplantation in adults with AATD than children.9–12A Swedish newborn screening cohort reported only 10% of ZZ children developed clinically significant liver disease in the first few years of life.13This cohort has been followed for approximately 40 years.Since age 18, mild elevations in aminotransferases were seen in 5%, and cirrhosis was not evident clinically.14–16Other populations studied provide estimates of cirrhosis and liver disease prevalence.The results vary widely, ranging from 2–63%, and in some studies, the risk increases with age >50 and male gender.10,17–22Reasons for the variability are many: selection and detection bias introduced by the retrospective nature of the work, differences in definitions of liver disease and patient populations, diagnosis ascertainment, and lack of confirmatory histology.
GS-9620, an oral agonist of toll-like receptor 7 (TLR7), is in clinical development for the treatment of chronic hepatitis B (CHB). GS-9620 was previously shown to induce prolonged suppression of serum viral DNA and antigens in the woodchuck and chimpanzee models of CHB. Herein, we investigated the molecular mechanisms that contribute to the antiviral response to GS-9620 using in vitro models of hepatitis B virus (HBV) infection. Cryopreserved primary human hepatocytes (PHH) and differentiated HepaRG (dHepaRG) cells were infected with HBV and treated with GS-9620, conditioned media from human peripheral blood mononuclear cells treated with GS-9620 (GS-9620 conditioned media [GS-9620-CM]), or other innate immune stimuli. The antiviral and transcriptional response to these agents was determined. GS-9620 had no antiviral activity in HBV-infected PHH, consistent with low level TLR7 mRNA expression in human hepatocytes. In contrast, GS-9620-CM induced prolonged reduction of HBV DNA, RNA, and antigen levels in PHH and dHepaRG cells via a type I interferon (IFN)-dependent mechanism. GS-9620-CM did not reduce covalently closed circular DNA (cccDNA) levels in either cell type. Transcriptional profiling demonstrated that GS-9620-CM strongly induced various HBV restriction factors – although not APOBEC3A or the Smc5/6 complex – and indicated that established HBV infection does not modulate innate immune sensing or signaling in cryopreserved PHH. GS-9620-CM also induced expression of immunoproteasome subunits and enhanced presentation of an immunodominant viral peptide in HBV-infected PHH. Type I IFN induced by GS-9620 durably suppressed HBV in human hepatocytes without reducing cccDNA levels. Moreover, HBV antigen presentation was enhanced, suggesting additional components of the TLR7-induced immune response played a role in the antiviral response to GS-9620 in animal models of CHB. Approximately 240 million individuals are chronically infected with hepatitis B virus and over 650,000 people die each year because of HBV-associated liver diseases, such as cirrhosis and hepatocellular carcinoma.Immunologic control of chronic hepatitis B (CHB), recognized as a “functional cure”, is defined as sustained loss of HBV surface antigen (HBsAg) off therapy with or without seroconversion to anti-HBs antibody.Current therapies for CHB are limited to nucleos(t)ides inhibitors and interferon-alpha (IFN-α).These agents reduce viral load and improve long-term outcome, but they rarely lead to cure.Therefore, there is an urgent need for new therapies that induce durable immune control of chronic HBV infection.
GS-9620, an oral agonist of toll-like receptor 7 (TLR7), is in clinical development for the treatment of chronic hepatitis B (CHB). GS-9620 was previously shown to induce prolonged suppression of serum viral DNA and antigens in the woodchuck and chimpanzee models of CHB. Herein, we investigated the molecular mechanisms that contribute to the antiviral response to GS-9620 using in vitro models of hepatitis B virus (HBV) infection. Cryopreserved primary human hepatocytes (PHH) and differentiated HepaRG (dHepaRG) cells were infected with HBV and treated with GS-9620, conditioned media from human peripheral blood mononuclear cells treated with GS-9620 (GS-9620 conditioned media [GS-9620-CM]), or other innate immune stimuli. The antiviral and transcriptional response to these agents was determined. GS-9620 had no antiviral activity in HBV-infected PHH, consistent with low level TLR7 mRNA expression in human hepatocytes. In contrast, GS-9620-CM induced prolonged reduction of HBV DNA, RNA, and antigen levels in PHH and dHepaRG cells via a type I interferon (IFN)-dependent mechanism. GS-9620-CM did not reduce covalently closed circular DNA (cccDNA) levels in either cell type. Transcriptional profiling demonstrated that GS-9620-CM strongly induced various HBV restriction factors – although not APOBEC3A or the Smc5/6 complex – and indicated that established HBV infection does not modulate innate immune sensing or signaling in cryopreserved PHH. GS-9620-CM also induced expression of immunoproteasome subunits and enhanced presentation of an immunodominant viral peptide in HBV-infected PHH. Type I IFN induced by GS-9620 durably suppressed HBV in human hepatocytes without reducing cccDNA levels. Moreover, HBV antigen presentation was enhanced, suggesting additional components of the TLR7-induced immune response played a role in the antiviral response to GS-9620 in animal models of CHB. GS-9620 is a potent, orally bioavailable small molecule agonist of toll-like receptor 7 (TLR7) in clinical development for the treatment of CHB.1TLR7 is expressed in a subset of human immune cells, primarily plasmacytoid dendritic cells (pDCs) and B cells, and recognizes single-stranded RNA as well as small molecule agonists.2TLR7 activation induces innate and adaptive immune responses via induction of various cytokines (including multiple IFN-α subtypes) and chemokines, direct activation of B cells, and cross-priming of cytotoxic CD8+ T cells.3–5GS-9620 was previously shown to induce prolonged suppression of serum viral DNA and antigens in the chimpanzee and woodchuck models of CHB.6,7Various immunomodulatory activities may account for the antiviral effects of GS-9620 in these animal models (e.g., induction of antiviral cytokines, activation of natural killer (NK)_cells, CD8+ T cells and B cells).However, the exact mechanism remains unclear.Defining the molecular basis for response is an important goal because mechanistic understanding of GS-9620 activity could guide rational design of novel immunotherapeutic strategies.
It is widely believed that autoimmune hepatitis accumulates in families, but the degree of familial clustering has not been clarified. We conducted a population-based study on the family occurrence of autoimmune hepatitis. Through Danish nationwide registries we identified 8,582 first-degree and 9,230 second-degree relatives of index patients diagnosed with autoimmune hepatitis in 1994–2015; and 64 co-twins of index patients diagnosed with autoimmune hepatitis in 1977–2011. For first- and second-degree relatives we calculated the sex- and age-adjusted standardized incidence ratio of autoimmune hepatitis relative to the general population, and we calculated the cumulative risk, i.e. the cumulative incidence, of developing autoimmune hepatitis from the time of the index patient’s diagnosis. For co-twins, we estimated the standardized incidence ratio and the concordance rate of autoimmune hepatitis. In first-degree relatives, there were six incident autoimmune hepatitis diagnoses during 64,020 years of follow-up: the standardized incidence ratio was 4.9 (95% CI 1.8–10.7), and the 10-year cumulative risk was 0.10% (95% CI 0.04–0.23). In the second-degree relatives, there were no incident autoimmune hepatitis diagnoses (expected number assuming incidence rate as in the Danish general population = 0.8). In the co-twins, there was one incident autoimmune hepatitis diagnosis during 1,112 years of follow-up, and the standardized incidence ratio was 53.9 (95% CI 1.4–300.4). The probandwise concordance rate, a measure of heritability, was higher in monozygotic than in dizygotic twins (8.7% [95% CI 1.1–28.0] vs. 0%). This nationwide study indicates that only first-degree relatives of index patients with autoimmune hepatitis are at increased risk of autoimmune hepatitis from the time of the index patient’s diagnosis, but the absolute risk is very low. Autoimmune hepatitis (AIH) is a chronic autoimmune liver disease with an incidence of 1 per 100,000 population per year around the world.1It is a frequently held notion that AIH accumulates in families,2 but the degree of familial clustering has rarely been studied.The existing literature suggests that AIH accumulates in twins,3,4 siblings,5 parents and children,3,6–8 second-degree relatives,3 and family members of unspecified relationship,9,10 but those studies were limited by small numbers of patients and did not compare AIH incidence with the general population.Numerous genetic risk factors for AIH have been suggested,11,12 but genome-wide association studies indicate that genetics plays only a minor role.12The basis for AIH family counselling thus remains inadequate.
The sequence of events in hepatic encephalopathy (HE) remains unclear. Using the advantages of in vivo 1H-MRS (9.4T) we aimed to analyse the time-course of disease in an established model of type C HE by analysing the longitudinal changes in a large number of brain metabolites together with biochemical, histological and behavioural assessment. We hypothesized that neurometabolic changes are detectable very early, and that these early changes will offer insight into the primary events underpinning HE. Wistar rats underwent bile-duct ligation (BDL) and were studied before BDL and at post-operative weeks 2, 4, 6 and 8 (n = 26). In vivo short echo-time 1H-MRS (9.4T) of the hippocampus was performed in a longitudinal manner, as were biochemical (plasma), histological and behavioural tests. Plasma ammonium increased early after BDL and remained high during the study. Brain glutamine increased (+47%) as early as 2–4 weeks post-BDL while creatine (−8%) and ascorbate (−12%) decreased. Brain glutamine and ascorbate correlated closely with rising plasma ammonium, while brain creatine correlated with brain glutamine. The increases in brain glutamine and plasma ammonium were correlated, while plasma ammonium correlated negatively with distance moved. Changes in astrocyte morphology were observed at 4 weeks. These early changes were further accentuated at 6–8 weeks post-BDL, concurrently with the known decreases in brain organic osmolytes. Using a multimodal, in vivo and longitudinal approach we have shown that neurometabolic changes are already noticeable 2 weeks after BDL. These early changes are suggestive of osmotic/oxidative stress and are likely the premise of some later changes. Early decreases in cerebral creatine and ascorbate are novel findings offering new avenues to explore neuroprotective strategies for HE treatment. Chronic hepatic encephalopathy (HE) is a complication of chronic liver disease (CLD) and is characterized by cognitive and motor deficits.1–3While minimal and overt HE may affect 20–70% of patients with CLD, the diagnosis is difficult in its early stages.4,5Tools enabling the detection of early neurometabolic changes in patients may allow for timely intervention, thereby minimizing the number of overt HE episodes and ensuing complications and costs.
The sequence of events in hepatic encephalopathy (HE) remains unclear. Using the advantages of in vivo 1H-MRS (9.4T) we aimed to analyse the time-course of disease in an established model of type C HE by analysing the longitudinal changes in a large number of brain metabolites together with biochemical, histological and behavioural assessment. We hypothesized that neurometabolic changes are detectable very early, and that these early changes will offer insight into the primary events underpinning HE. Wistar rats underwent bile-duct ligation (BDL) and were studied before BDL and at post-operative weeks 2, 4, 6 and 8 (n = 26). In vivo short echo-time 1H-MRS (9.4T) of the hippocampus was performed in a longitudinal manner, as were biochemical (plasma), histological and behavioural tests. Plasma ammonium increased early after BDL and remained high during the study. Brain glutamine increased (+47%) as early as 2–4 weeks post-BDL while creatine (−8%) and ascorbate (−12%) decreased. Brain glutamine and ascorbate correlated closely with rising plasma ammonium, while brain creatine correlated with brain glutamine. The increases in brain glutamine and plasma ammonium were correlated, while plasma ammonium correlated negatively with distance moved. Changes in astrocyte morphology were observed at 4 weeks. These early changes were further accentuated at 6–8 weeks post-BDL, concurrently with the known decreases in brain organic osmolytes. Using a multimodal, in vivo and longitudinal approach we have shown that neurometabolic changes are already noticeable 2 weeks after BDL. These early changes are suggestive of osmotic/oxidative stress and are likely the premise of some later changes. Early decreases in cerebral creatine and ascorbate are novel findings offering new avenues to explore neuroprotective strategies for HE treatment. Neurological alterations in HE are accepted to be the result of declining liver function, leading to impaired detoxification of ammonium (NH4+) and other substances that reach the brain.3,6–11Although the molecular mechanisms leading to HE remain unclear, NH4+ and glutamine (Gln) are a common thread in the complex and multifactorial model of HE pathogenesis.They are accepted to contribute to the incompletely understood metabolic cascades underlying the neurological manifestations of HE.
The sequence of events in hepatic encephalopathy (HE) remains unclear. Using the advantages of in vivo 1H-MRS (9.4T) we aimed to analyse the time-course of disease in an established model of type C HE by analysing the longitudinal changes in a large number of brain metabolites together with biochemical, histological and behavioural assessment. We hypothesized that neurometabolic changes are detectable very early, and that these early changes will offer insight into the primary events underpinning HE. Wistar rats underwent bile-duct ligation (BDL) and were studied before BDL and at post-operative weeks 2, 4, 6 and 8 (n = 26). In vivo short echo-time 1H-MRS (9.4T) of the hippocampus was performed in a longitudinal manner, as were biochemical (plasma), histological and behavioural tests. Plasma ammonium increased early after BDL and remained high during the study. Brain glutamine increased (+47%) as early as 2–4 weeks post-BDL while creatine (−8%) and ascorbate (−12%) decreased. Brain glutamine and ascorbate correlated closely with rising plasma ammonium, while brain creatine correlated with brain glutamine. The increases in brain glutamine and plasma ammonium were correlated, while plasma ammonium correlated negatively with distance moved. Changes in astrocyte morphology were observed at 4 weeks. These early changes were further accentuated at 6–8 weeks post-BDL, concurrently with the known decreases in brain organic osmolytes. Using a multimodal, in vivo and longitudinal approach we have shown that neurometabolic changes are already noticeable 2 weeks after BDL. These early changes are suggestive of osmotic/oxidative stress and are likely the premise of some later changes. Early decreases in cerebral creatine and ascorbate are novel findings offering new avenues to explore neuroprotective strategies for HE treatment. Gln synthesis in the central nervous system (CNS) is largely confined to astrocytes (glutamine synthetase activity12).In liver dysfunction or portosystemic shunting, increased NH4+ delivery to the brain increases astrocytic Gln, raising intracellular osmotic pressure and leading to astrocyte swelling and cytotoxic brain oedema.13–16In addition, increased astrocytic Gln can lead to the opening of the mitochondrial permeability transition pore and interfere with glutamatergic neurotransmission.17–19The mechanisms of NH4+ neurotoxicity also implicate oxidative/nitrosative stress, mitochondrial dysfunction, cell-signalling disruption, neuroinflammation and alterations in neuronal process growth.9,10,16,20–24While all these mechanisms appear implicated in the pathogenesis of HE, the sequence of events remains unclear, owing largely to the lack of reliable longitudinal studies.
The sequence of events in hepatic encephalopathy (HE) remains unclear. Using the advantages of in vivo 1H-MRS (9.4T) we aimed to analyse the time-course of disease in an established model of type C HE by analysing the longitudinal changes in a large number of brain metabolites together with biochemical, histological and behavioural assessment. We hypothesized that neurometabolic changes are detectable very early, and that these early changes will offer insight into the primary events underpinning HE. Wistar rats underwent bile-duct ligation (BDL) and were studied before BDL and at post-operative weeks 2, 4, 6 and 8 (n = 26). In vivo short echo-time 1H-MRS (9.4T) of the hippocampus was performed in a longitudinal manner, as were biochemical (plasma), histological and behavioural tests. Plasma ammonium increased early after BDL and remained high during the study. Brain glutamine increased (+47%) as early as 2–4 weeks post-BDL while creatine (−8%) and ascorbate (−12%) decreased. Brain glutamine and ascorbate correlated closely with rising plasma ammonium, while brain creatine correlated with brain glutamine. The increases in brain glutamine and plasma ammonium were correlated, while plasma ammonium correlated negatively with distance moved. Changes in astrocyte morphology were observed at 4 weeks. These early changes were further accentuated at 6–8 weeks post-BDL, concurrently with the known decreases in brain organic osmolytes. Using a multimodal, in vivo and longitudinal approach we have shown that neurometabolic changes are already noticeable 2 weeks after BDL. These early changes are suggestive of osmotic/oxidative stress and are likely the premise of some later changes. Early decreases in cerebral creatine and ascorbate are novel findings offering new avenues to explore neuroprotective strategies for HE treatment. In vivo proton magnetic resonance spectroscopy (1H-MRS) is widely used to investigate brain metabolism non-invasively, allowing for longitudinal follow-up of disease and response to treatment.1H-MRS was among the first techniques to provide indirect evidence in vivo of osmoregulatory disturbances: an increase in the sum Gln + glutamate (Glu; Gln + Glu = Glx) concentration together with a decrease in brain osmolyte myo-inositol (Ins).16These changes have since been thought to underlie the low grade cerebral oedema observed in chronic HE in humans.Later, other magnetic resonance imaging (MRI) techniques (i.e. diffusion weighted/tensor imaging, water mapping) further expanded our understanding of chronic HE by demonstrating the presence of low grade cerebral oedema.13,25–28Although these studies were instrumental in demonstrating neurometabolic disturbances, their contribution was limited by comparatively low magnetic field strengths used and the ensuing limitation in the number of metabolites quantified (i.e. Glx, total creatine (tCr), total choline (tCho) and Ins5,28–31).Therefore, present day availability of very high magnetic field strengths (≥7T) combined with spectra acquisition at very short echo times (<10 ms) offer unprecedented opportunities to further understand the longitudinal neurometabolic events of chronic HE in detail by expanding the number of brain metabolites detectable in vivo to about 18 both in animal models and humans.32–35
The sequence of events in hepatic encephalopathy (HE) remains unclear. Using the advantages of in vivo 1H-MRS (9.4T) we aimed to analyse the time-course of disease in an established model of type C HE by analysing the longitudinal changes in a large number of brain metabolites together with biochemical, histological and behavioural assessment. We hypothesized that neurometabolic changes are detectable very early, and that these early changes will offer insight into the primary events underpinning HE. Wistar rats underwent bile-duct ligation (BDL) and were studied before BDL and at post-operative weeks 2, 4, 6 and 8 (n = 26). In vivo short echo-time 1H-MRS (9.4T) of the hippocampus was performed in a longitudinal manner, as were biochemical (plasma), histological and behavioural tests. Plasma ammonium increased early after BDL and remained high during the study. Brain glutamine increased (+47%) as early as 2–4 weeks post-BDL while creatine (−8%) and ascorbate (−12%) decreased. Brain glutamine and ascorbate correlated closely with rising plasma ammonium, while brain creatine correlated with brain glutamine. The increases in brain glutamine and plasma ammonium were correlated, while plasma ammonium correlated negatively with distance moved. Changes in astrocyte morphology were observed at 4 weeks. These early changes were further accentuated at 6–8 weeks post-BDL, concurrently with the known decreases in brain organic osmolytes. Using a multimodal, in vivo and longitudinal approach we have shown that neurometabolic changes are already noticeable 2 weeks after BDL. These early changes are suggestive of osmotic/oxidative stress and are likely the premise of some later changes. Early decreases in cerebral creatine and ascorbate are novel findings offering new avenues to explore neuroprotective strategies for HE treatment. The bile-duct ligation (BDL) rat model is accepted by ISHEN (International Society for Hepatic Encephalopathy and Nitrogen Metabolism) as a model of type C HE, namely HE associated with liver disease, portal hypertension and hyperammonemia.36To date, the few studies in this model have used in vivo or ex vivo 1H-MRS combined with MRI or behavioural tests at late time points.The main findings of these studies were that plasma NH4+ and brain Gln increased while other brain osmolytes decreased (Ins and tCho).They also brought controversial data regarding fluctuations in lactate (Lac), Glu, N-acetyl-aspartate (NAA) and brain water content.26,37–39Further studies are required to understand the significance of these findings and to comprehend the chronology of events leading to these metabolic perturbations, including early molecular and cellular brain changes which may offer insight into the primary mechanisms of HE.
High red and processed meat consumption is related to type 2 diabetes. In addition, cooking meat at high temperatures for a long duration forms heterocyclic amines (HCAs), which are related to oxidative stress. However, the association between meat consumption and non-alcoholic fatty liver disease (NAFLD) is yet to be thoroughly tested. Therefore, we aimed to test the association of meat type and cooking method with NAFLD and insulin resistance (IR). This was a cross-sectional study in individuals who were 40–70 years old and underwent screening colonoscopy between 2013 and 2015 in a single center in Israel. NAFLD and IR were evaluated by ultrasonography and homeostasis model assessment. Meat type and cooking method were measured by a food frequency questionnaire (FFQ) and a detailed meat questionnaire. Unhealthy cooking methods were considered as frying and grilling to a level of well done and very well done. Dietary HCA intake was calculated. A total of 789 individuals had a valid FFQ and 357 had a valid meat questionnaire. High consumption of total meat (portions/day above the median) (odds ratio [OR] 1.49; 95% CI 1.05–2.13; p = 0.028; OR 1.63; 1.12–2.37; p = 0.011), red and/or processed meat (OR 1.47; 95% CI 1.04–2.09; p = 0.031; OR 1.55; 1.07–2.23; p = 0.020) was independently associated with higher odds of NAFLD and IR, respectively, when adjusted for: body mass index, physical activity, smoking, alcohol, energy, saturated fat and cholesterol intake. High intake of meat cooked using unhealthy methods (OR 1.92; 95% CI 1.12–3.30; p = 0.018) and HCAs (OR 2.22; 95% CI 1.28–3.86; p = 0.005) were independently associated with higher odds of IR. High consumption of red and/or processed meat is associated with both NAFLD and IR. High HCA intake is associated with IR. If confirmed in prospective studies, limiting the consumption of unhealthy meat types and improving preparation methods may be considered as part of NAFLD lifestyle treatment. Non-alcoholic Fatty liver disease (NAFLD) is becoming a major global health burden in both developed and developing countries.1NAFLD is considered as the hepatic component of the metabolic syndrome, with insulin resistance (IR) as the key factor in its pathophysiology.2Unhealthy Western lifestyle plays a major role in the development and progression of NAFLD,3 namely, lack of physical activity and high consumption of fructose and saturated fat.4,5There are other common foods in the Western diet, namely red and processed meats, which may also increase the risk for NAFLD.6Meat in general contains valuable nutrients for human health including protein, iron, zinc and vitamin B12.7However, meat also contains saturated fatty acids (SFA) and cholesterol, both harmful for patients with NAFLD,8–11 as well as other potentially harmful compounds such as heme-iron,12 sodium,13 other preservatives12 and advanced glycation end products (AGEs).12,14Indeed, high meat consumption has been demonstrated to be associated with IR and type 2 diabetes,15–17 the metabolic syndrome17 and oxidative stress.18More specifically, red meat has been shown to be associated with a higher risk of mortality, owing to chronic liver disease and hepatocellular carcinoma.19The association between meat consumption and NAFLD was demonstrated in a few studies,5,6,20,21 in which meat type and cooking method were not fully addressed.We have previously demonstrated an independent association between high meat consumption and NAFLD,20 with no distinction between meat types or cooking methods, because of a small sample size and lack of information on the cooking methods in the standard food frequency questionnaire (FFQ).
Liver failure results in hyperammonaemia, impaired regulation of cerebral microcirculation, encephalopathy, and death. However, the key mediator that alters cerebral microcirculation remains unidentified. In this study we show that topically applied ammonium significantly increases periarteriolar adenosine tone on the brain surface of healthy rats and is associated with a disturbed microcirculation. Cranial windows were prepared in anaesthetized Wistar rats. The flow velocities were measured by speckle contrast imaging and compared before and after 30 min of exposure to 10 mM ammonium chloride applied on the brain surface. These flow velocities were compared with those for control groups exposed to artificial cerebrospinal fluid or ammonium plus an adenosine receptor antagonist. A flow preservation curve was obtained by analysis of flow responses to a haemorrhagic hypotensive challenge and during stepwise exsanguination. The periarteriolar adenosine concentration was measured with enzymatic biosensors inserted in the cortex. After ammonium exposure the arteriolar flow velocity increased by a median (interquartile range) of 21.7% (23.4%) vs. 7.2% (10.2%) in controls (n = 10 and n = 6, respectively, p <0.05), and the arteriolar surface area increased. There was a profound rise in the periarteriolar adenosine concentration. During the hypotensive challenge the flow decreased by 27.8% (14.9%) vs. 9.2% (14.9%) in controls (p <0.05). The lower limit of flow preservation remained unaffected, 27.7 (3.9) mmHg vs. 27.6 (6.4) mmHg, whereas the autoregulatory index increased, 0.29 (0.33) flow units per millimetre of mercury vs. 0.03 (0.21) flow units per millimetre of mercury (p <0.05). When ammonium exposure was combined with topical application of an adenosine receptor antagonist, the autoregulatory index was normalized. Vasodilation of the cerebral microcirculation during exposure to ammonium chloride is associated with an increase in the adenosine tone. Application of a specific adenosine receptor antagonist restores the regulation of the microcirculation. This indicates that adenosine could be a key mediator of the brain dysfunction seen during hyperammonaemia and is a potential therapeutic target. Ammonium has repeatedly been found to be a key factor in the pathogenesis of hepatic encephalopathy (HE)1 and development of brain oedema in acute liver failure.2Yet, the underlying mechanisms are still not understood in detail; specifically, a mediator responsible for disturbances in the microcirculation and vasodilation remains to be identified.
Liver failure results in hyperammonaemia, impaired regulation of cerebral microcirculation, encephalopathy, and death. However, the key mediator that alters cerebral microcirculation remains unidentified. In this study we show that topically applied ammonium significantly increases periarteriolar adenosine tone on the brain surface of healthy rats and is associated with a disturbed microcirculation. Cranial windows were prepared in anaesthetized Wistar rats. The flow velocities were measured by speckle contrast imaging and compared before and after 30 min of exposure to 10 mM ammonium chloride applied on the brain surface. These flow velocities were compared with those for control groups exposed to artificial cerebrospinal fluid or ammonium plus an adenosine receptor antagonist. A flow preservation curve was obtained by analysis of flow responses to a haemorrhagic hypotensive challenge and during stepwise exsanguination. The periarteriolar adenosine concentration was measured with enzymatic biosensors inserted in the cortex. After ammonium exposure the arteriolar flow velocity increased by a median (interquartile range) of 21.7% (23.4%) vs. 7.2% (10.2%) in controls (n = 10 and n = 6, respectively, p <0.05), and the arteriolar surface area increased. There was a profound rise in the periarteriolar adenosine concentration. During the hypotensive challenge the flow decreased by 27.8% (14.9%) vs. 9.2% (14.9%) in controls (p <0.05). The lower limit of flow preservation remained unaffected, 27.7 (3.9) mmHg vs. 27.6 (6.4) mmHg, whereas the autoregulatory index increased, 0.29 (0.33) flow units per millimetre of mercury vs. 0.03 (0.21) flow units per millimetre of mercury (p <0.05). When ammonium exposure was combined with topical application of an adenosine receptor antagonist, the autoregulatory index was normalized. Vasodilation of the cerebral microcirculation during exposure to ammonium chloride is associated with an increase in the adenosine tone. Application of a specific adenosine receptor antagonist restores the regulation of the microcirculation. This indicates that adenosine could be a key mediator of the brain dysfunction seen during hyperammonaemia and is a potential therapeutic target. An associated complication of HE/brain oedema in severe cases of liver failure is the loss of autoregulation of cerebral blood flow (CBF)3,4 and an impairment of the otherwise tight coupling between flow and metabolism.This leaves the brain very susceptible to variations in the perfusion pressure, and interferes with the basic function of the neurovascular unit.For example, brief episodes of systemic hypotension can lead to ischaemic damage to the brain, and intermittent hypertension can result in severe hyperperfusion and risk of vasogenic oedema.Indeed, haemodynamic instability is a common complication of the failing liver, both acute liver failure5 and acute-on-chronic liver failure,6 and even with management in an intensive care unit, keeping the systemic blood pressure stable can be a difficult task.
The effect of hepatocellular carcinoma (HCC) on the response to interferon-free direct-acting antiviral (DAA) therapy in patients with chronic hepatitis C (CHC) infection remains unclear. Using a systematic review and meta-analysis approach, we aimed to investigate the effect of DAA therapy on sustained virologic response (SVR) among patients with CHC and either active, inactive or no HCC. PubMed, Embase, Web of Science, and the Cochrane Central Register of Controlled Trials were searched from 1/1/2013 to 9/24/2018. The pooled SVR rates were computed using DerSimonian-Laird random-effects models. We included 49 studies from 15 countries, comprised of 3,341 patients with HCC and 35,701 without HCC. Overall, the pooled SVR was lower in patients with HCC than in those without HCC (89.6%, 95% CI 86.8–92.1%, I2 = 79.1% vs. 93.3%, 95% CI 91.9–94.7%, I2 = 95.0%, p = 0.0012), translating to a 4.8% (95% CI 0.2–7.4%) SVR reduction by meta-regression analysis. The largest SVR reduction (18.8%) occurred in patients with active/residual HCC vs. inactive/ablated HCC (SVR 73.1% vs. 92.6%, p = 0.002). Meanwhile, patients with HCC who received a prior liver transplant had higher SVR rates than those who did not (p <0.001). Regarding specific DAA regimens, patients with HCC treated with ledipasvir/sofosbuvir had lower SVR rates than patients without HCC (92.6%, n = 884 vs. 97.8%, n = 13,141, p = 0.026), but heterogeneity was high (I2 = 84.7%, p <0.001). The SVR rate was similar in patients with/without HCC who were treated with ombitasvir/paritaprevir/ritonavir ± dasabuvir (n = 101) (97.2% vs. 94.8%, p = 0.79), or daclatasvir/asunaprevir (91.7% vs. 89.8%, p = 0.66). Overall, SVR rates were lower in patients with HCC, especially with active HCC, compared to those without HCC, though heterogeneity was high. Continued efforts are needed to aggressively screen, diagnose, and treat HCC to ensure higher CHC cure rates. Chronic hepatitis C virus (HCV) infection affected an estimated 71.1 million patients worldwide in 2015 and is a leading cause of liver cirrhosis and hepatocellular carcinoma (HCC).1Among patients who have undergone treatment with curative intent for HCC, early hepatic decompensation and HCC recurrence were the major drivers of mortality.2In recent studies of chronic hepatitis B-related HCC, antiviral therapy was shown to significantly reduce overall long-term mortality even in patients with very advanced HCC or decompensated cirrhosis, including those who were only receiving palliative treatment for HCC.3–6Prior to the advent of interferon (IFN)-free direct-acting antiviral (DAA) therapy, patients with HCV-related HCC were often excluded from anti-HCV therapy as they tended to be older and had multiple non-liver and liver comorbidities, many of which rendered them unsuitable candidates for IFN-based therapy.Since 2014, many of these patients with HCC became treatment candidates for their chronic hepatitis C (CHC), despite the presence of advanced liver disease and comorbidities, as DAA therapy is not only highly efficacious but well tolerated.7Individual real-world studies to date have included patients with HCC from both the East and West, and some have reported significantly lower cure rates.8–14However, most studies had small sample sizes and heterogeneous patient demographic and clinical characteristics.
Most hepatitis C virus (HCV)-infected patients failing NS5A inhibitors develop resistance-associated substitutions (RASs). Here we report the use of resistance-guided retreatment of patients who failed prior NS5A inhibitor-containing regimens in the GEHEP-004 cohort. This is the largest direct-acting antiviral (DAA)-resistance cohort study conducted in Spain. We aim to provide indications on how to use resistance information in settings where sofosbuvir/velpatasvir/voxilaprevir may not be available. GEHEP-004 is a prospective multicenter cohort enrolling HCV-infected patients treated with interferon (IFN)-free DAA regimens. Prior to retreatment, population-based sequencing of HCV NS3, NS5A and NS5B genes was performed. After receiving a comprehensive resistance interpretation report, the retreatment regimen was chosen and the sustained virological response (SVR) at 12 weeks after treatment completion (SVR12) was recorded. A total of 342 patients experiencing virological failure after treatment with sofosbuvir/ledipasvir±ribavirin (54%), sofosbuvir/daclatasvir±ribavirin (23%), or paritaprevir-ritonavir/ombitasvir±dasabuvir±ribavirin (20%) were studied. After a resistance report, 186 patients were retreated. An SVR12 was achieved for 88.1% of the patients who failed after sofosbuvir/ledipasvir±ribavirin, 83.3% of the patients who failed after sofosbuvir/daclatasvir±ribavirin, 93.7% of the patients who failed after paritaprevir-ritonavir+ombitasvir±dasabuvir±ribavirin. In our study, we show how resistance-guided retreatment in conjunction with an interpreted report allows patients to achieve SVR rates close to 90%. We hypothesize that SVR rates may even be improved if resistance data are discussed between experienced virologists and treating clinicians. We believe that our data may be relevant for countries where the access to new DAA combination regimens is limited. According to the World Health Organization, there are approximately 71 million people infected with hepatitis C virus (HCV) worldwide and 1.75 million people are diagnosed each year.1In the absence of antiviral treatment, HCV leads to cirrhosis, hepatocellular carcinoma, liver failure and death.2Treatment with direct-acting antivirals (DAA) is highly efficacious and it has limited side effects.3Current DAA combinations that are recommended as first-line treatment of HCV-infected patients by the AASLD-IDSA4 and EASL guidelines5 enable patients to achieve sustained virological response (SVR) rates >90% for all HCV genotypes.
Most hepatitis C virus (HCV)-infected patients failing NS5A inhibitors develop resistance-associated substitutions (RASs). Here we report the use of resistance-guided retreatment of patients who failed prior NS5A inhibitor-containing regimens in the GEHEP-004 cohort. This is the largest direct-acting antiviral (DAA)-resistance cohort study conducted in Spain. We aim to provide indications on how to use resistance information in settings where sofosbuvir/velpatasvir/voxilaprevir may not be available. GEHEP-004 is a prospective multicenter cohort enrolling HCV-infected patients treated with interferon (IFN)-free DAA regimens. Prior to retreatment, population-based sequencing of HCV NS3, NS5A and NS5B genes was performed. After receiving a comprehensive resistance interpretation report, the retreatment regimen was chosen and the sustained virological response (SVR) at 12 weeks after treatment completion (SVR12) was recorded. A total of 342 patients experiencing virological failure after treatment with sofosbuvir/ledipasvir±ribavirin (54%), sofosbuvir/daclatasvir±ribavirin (23%), or paritaprevir-ritonavir/ombitasvir±dasabuvir±ribavirin (20%) were studied. After a resistance report, 186 patients were retreated. An SVR12 was achieved for 88.1% of the patients who failed after sofosbuvir/ledipasvir±ribavirin, 83.3% of the patients who failed after sofosbuvir/daclatasvir±ribavirin, 93.7% of the patients who failed after paritaprevir-ritonavir+ombitasvir±dasabuvir±ribavirin. In our study, we show how resistance-guided retreatment in conjunction with an interpreted report allows patients to achieve SVR rates close to 90%. We hypothesize that SVR rates may even be improved if resistance data are discussed between experienced virologists and treating clinicians. We believe that our data may be relevant for countries where the access to new DAA combination regimens is limited. Despite the high efficacy of current DAAs, 2–5% of the patients starting their first interferon (IFN)-free regimen fail to achieve HCV cure (SVR) in clinical trials and the real world for virological reasons.DAA failure is an unfortunate event that can occur with all HCV genotypes.DAA failure is frequently, but not always, associated with the presence of HCV resistance-associated substitutions (RASs).4–7In general, RASs detected at failure are selected during treatment, though, in some patients they may pre-exist as naturally occurring variants before treatment, impairing the efficacy of certain DAA combinations in patients infected with genotypes 1a and 3.5,8–10
Most hepatitis C virus (HCV)-infected patients failing NS5A inhibitors develop resistance-associated substitutions (RASs). Here we report the use of resistance-guided retreatment of patients who failed prior NS5A inhibitor-containing regimens in the GEHEP-004 cohort. This is the largest direct-acting antiviral (DAA)-resistance cohort study conducted in Spain. We aim to provide indications on how to use resistance information in settings where sofosbuvir/velpatasvir/voxilaprevir may not be available. GEHEP-004 is a prospective multicenter cohort enrolling HCV-infected patients treated with interferon (IFN)-free DAA regimens. Prior to retreatment, population-based sequencing of HCV NS3, NS5A and NS5B genes was performed. After receiving a comprehensive resistance interpretation report, the retreatment regimen was chosen and the sustained virological response (SVR) at 12 weeks after treatment completion (SVR12) was recorded. A total of 342 patients experiencing virological failure after treatment with sofosbuvir/ledipasvir±ribavirin (54%), sofosbuvir/daclatasvir±ribavirin (23%), or paritaprevir-ritonavir/ombitasvir±dasabuvir±ribavirin (20%) were studied. After a resistance report, 186 patients were retreated. An SVR12 was achieved for 88.1% of the patients who failed after sofosbuvir/ledipasvir±ribavirin, 83.3% of the patients who failed after sofosbuvir/daclatasvir±ribavirin, 93.7% of the patients who failed after paritaprevir-ritonavir+ombitasvir±dasabuvir±ribavirin. In our study, we show how resistance-guided retreatment in conjunction with an interpreted report allows patients to achieve SVR rates close to 90%. We hypothesize that SVR rates may even be improved if resistance data are discussed between experienced virologists and treating clinicians. We believe that our data may be relevant for countries where the access to new DAA combination regimens is limited. Until the newer pangenotypic regimens, with high genetic barrier-to-resistance and antiviral potency, become extensively available in all countries, preliminary data suggest that retreatment can be optimized based on RAS testing after a DAA failure (5), particularly for tailoring personalized treatments.11–13According to the 2018 EASL guidelines, if resistance testing is performed, then retreatment may be guided by probabilities of response according to the resistance profile observed and the treating team’s experience.
Most hepatitis C virus (HCV)-infected patients failing NS5A inhibitors develop resistance-associated substitutions (RASs). Here we report the use of resistance-guided retreatment of patients who failed prior NS5A inhibitor-containing regimens in the GEHEP-004 cohort. This is the largest direct-acting antiviral (DAA)-resistance cohort study conducted in Spain. We aim to provide indications on how to use resistance information in settings where sofosbuvir/velpatasvir/voxilaprevir may not be available. GEHEP-004 is a prospective multicenter cohort enrolling HCV-infected patients treated with interferon (IFN)-free DAA regimens. Prior to retreatment, population-based sequencing of HCV NS3, NS5A and NS5B genes was performed. After receiving a comprehensive resistance interpretation report, the retreatment regimen was chosen and the sustained virological response (SVR) at 12 weeks after treatment completion (SVR12) was recorded. A total of 342 patients experiencing virological failure after treatment with sofosbuvir/ledipasvir±ribavirin (54%), sofosbuvir/daclatasvir±ribavirin (23%), or paritaprevir-ritonavir/ombitasvir±dasabuvir±ribavirin (20%) were studied. After a resistance report, 186 patients were retreated. An SVR12 was achieved for 88.1% of the patients who failed after sofosbuvir/ledipasvir±ribavirin, 83.3% of the patients who failed after sofosbuvir/daclatasvir±ribavirin, 93.7% of the patients who failed after paritaprevir-ritonavir+ombitasvir±dasabuvir±ribavirin. In our study, we show how resistance-guided retreatment in conjunction with an interpreted report allows patients to achieve SVR rates close to 90%. We hypothesize that SVR rates may even be improved if resistance data are discussed between experienced virologists and treating clinicians. We believe that our data may be relevant for countries where the access to new DAA combination regimens is limited. Patients that have failed their first IFN-free regimen based on sofosbuvir plus a NS3 inhibitor are easy to retreat.5In fact, these patients are naïve to NS5A inhibitors, that is, they have never been treated with NS5A inhibitors for their HCV infection.However, sofosbuvir is an NS5B inhibitor with a very high genetic barrier-to-resistance; hence, the retreatment of these patients with a NS5A inhibitor may be considered as another “first-line” treatment.Patients who failed a prior regimen based on sofosbuvir and a first generation NS5A inhibitor that are going to be retreated with a NS5A inhibitor face a different scenario.Although there are some important reports on how patients fail DAA regimens in real world,14–17 there is limited evidence on how RAS-guided retreatment of NS5A failures impacts on the efficacy of retreatment.18
Hepatitis B virus (HBV) and D virus (HDV) co-infections cause the most severe form of viral hepatitis. HDV induces an innate immune response, but it is unknown how the host cell senses HDV and if this defense affects HDV replication. We aim to characterize interferon (IFN) activation by HDV, identify the responsible sensor and evaluate the effect of IFN on HDV replication. HDV and HBV susceptible hepatoma cell lines and primary human hepatocytes (PHH) were used for infection studies. Viral markers and cellular gene expression were analyzed at different time points after infection. Pattern recognition receptors (PRRs) required for HDV-mediated IFN activation and the impact on HDV replication were studied using stable knock-down or overexpression of the PRRs. Microarray analysis revealed that HDV but not HBV infection activated a broad range of interferon stimulated genes (ISGs) in HepG2NTCP cells. HDV strongly activated IFN-β and IFN-λ in cell lines and PHH. HDV induced IFN levels remained unaltered upon RIG-I (DDX58) or TLR3 knock-down, but were almost completely abolished upon MDA5 (IFIH1) depletion. Conversely, overexpression of MDA5 but not RIG-I and TLR3 in HuH7.5NTCP cells partially restored ISG induction. During long-term infection, IFN levels gradually diminished in both HepG2NTCP and HepaRGNTCP cell lines. MDA5 depletion had little effect on HDV replication despite dampening HDV-induced IFN response. Moreover, treatment with type I or type III IFNs did not abolish HDV replication. Active replication of HDV induces an IFN-β/λ response, which is predominantly mediated by MDA5. This IFN response and exogenous IFN treatment have only a moderate effect on HDV replication in vitro indicating the adaption of HDV replication to an IFN-activated state. Among the 240 million hepatitis B virus (HBV)-infected people worldwide,1 15–25 million are coinfected with hepatitis D virus (HDV) (WHO fact sheet, July 2016), a satellite virus which requires HBV envelope proteins for particle assembly and spread.HBV/HDV coinfection leads to the most severe form of viral hepatitis with an accelerated progression to liver fibrosis, cirrhosis and hepatocellular carcinoma.2Therapeutic options for chronically HBV/HDV coinfected patients are still limited to IFN-α therapy for eligible patients, however, this therapy is not curative in the vast majority of patients.3,4HDV consists of a single-stranded circular 1.7 kb RNA genome of negative polarity and represents the smallest mammalian virus genome identified to date.5HDV RNA replication takes place in the nucleus of hepatocytes through a double rolling circle mechanism, producing antigenomic RNA, genomic RNA and mRNAs encoding the small and large hepatitis delta antigens (S- and L-HDAg).The circular genomic HDV RNA is highly self-complementary (∼74% base pairing) and therefore forms rod-shaped viroid-like tertiary structures.6During assembly, the newly synthesized genomic RNA associates with HDAg molecules to form a ribonucleoprotein (RNP) complex.7These RNPs become enveloped by budding into the ER lumen at HBV envelope protein (L-/M- and S-HBsAg) assembly sites, which are provided by either covalently closed circular DNA of HBV coinfected hepatocytes or possibly integrated HBV sequences.8
Hepatitis B virus (HBV) and D virus (HDV) co-infections cause the most severe form of viral hepatitis. HDV induces an innate immune response, but it is unknown how the host cell senses HDV and if this defense affects HDV replication. We aim to characterize interferon (IFN) activation by HDV, identify the responsible sensor and evaluate the effect of IFN on HDV replication. HDV and HBV susceptible hepatoma cell lines and primary human hepatocytes (PHH) were used for infection studies. Viral markers and cellular gene expression were analyzed at different time points after infection. Pattern recognition receptors (PRRs) required for HDV-mediated IFN activation and the impact on HDV replication were studied using stable knock-down or overexpression of the PRRs. Microarray analysis revealed that HDV but not HBV infection activated a broad range of interferon stimulated genes (ISGs) in HepG2NTCP cells. HDV strongly activated IFN-β and IFN-λ in cell lines and PHH. HDV induced IFN levels remained unaltered upon RIG-I (DDX58) or TLR3 knock-down, but were almost completely abolished upon MDA5 (IFIH1) depletion. Conversely, overexpression of MDA5 but not RIG-I and TLR3 in HuH7.5NTCP cells partially restored ISG induction. During long-term infection, IFN levels gradually diminished in both HepG2NTCP and HepaRGNTCP cell lines. MDA5 depletion had little effect on HDV replication despite dampening HDV-induced IFN response. Moreover, treatment with type I or type III IFNs did not abolish HDV replication. Active replication of HDV induces an IFN-β/λ response, which is predominantly mediated by MDA5. This IFN response and exogenous IFN treatment have only a moderate effect on HDV replication in vitro indicating the adaption of HDV replication to an IFN-activated state. The cellular innate immune system constitutes the first line of defense against microbial infections and plays a crucial role in inhibition and clearance of invading pathogens.9The innate immune response is also crucial for mounting robust adaptive immune response (reviewed in10,11) and thereby determine the outcome of infection, i.e. clearance or persistence, with the latter often associated with chronic inflammation.12Innate immune responses are initiated by the recognition of pathogen-associated molecular patterns (PAMPs), such as viral genomes or replication intermediates like double-stranded RNA (dsRNA).Thereby, pattern recognition receptors (PRRs) i.e. toll-like receptor 3 (TLR3), the retinoic acid inducible gene I (RIG-I [DDX58])-like receptors (RLRs) including RIG-I and the melanoma differentiation antigen 5 (MDA5 [IFIH1]) (reviewed in13,14) sense the PAMPs and initiate a cascade of signaling events that lead to the production of type I (IFN-α/β) and type III interferons (IFN-λ).These are secreted and bind their cognate IFN-receptors on the membrane of infected and non-infected neighboring cells, activating hundreds of interferon stimulated genes (ISGs) that counteract viral infections by diverse mechanisms.15
The thymocyte selection-associated high mobility group box protein (TOX) plays a vital role in T cell development and differentiation, however, its role in T cell exhaustion was unexplored. Here, we aim to investigate the role of TOX in regulating the antitumor effect of CD8+ T cells in hepatocellular carcinoma. Fully functional, partially and severely exhausted tumor-infiltrating CD8+ T cells were sorted by flow cytometry and subjected to transcriptome sequencing analysis. Upregulated TOX expression was validated by flow cytometry. The antitumor function of CD8+ T cells with TOX downregulation or overexpression was studied in a mouse HCC model and HCC patient-derived xenograft mouse model. Transcriptome sequencing analysis was performed in TOX-overexpressing and control CD8+ T cells. The mechanism underlying the TOX-mediated regulation of PD1 expression was studied by laser confocal detection, immune co-precipitation and flow cytometer. TOX was upregulated in exhausted CD8+ T cells in hepatocellular carcinoma. TOX downregulation in CD8+ T cells inhibited tumor growth, increased CD8+ T cell infiltration, alleviated CD8+ T cell exhaustion and improved the anti-PD1 response of CD8+ T cells. The mechanism behind this involved the binding of TOX to PD1 in the cytoplasm, which facilitated the endocytic recycling of PD1, thus maintaining abundant PD1 expression at the cell surface. High expression of TOX in peripheral CD8+ T cells correlated with poorer anti-PD1 responses and prognosis. TOX promotes CD8+ T cell exhaustion in hepatocellular carcinoma by regulating endocytic recycling of PD1. Downregulating TOX expression in CD8+ T cells exerts synergistic effects with anti-PD1 therapy, highlighting a promising strategy for cancer immunotherapy. Compromised antitumor immunity characterized by the presence of dysfunctional CD8+ T cells in the tumor microenvironment (TME) is a hallmark of cancer.1,2Long-term persistence of tumor antigens and/or the suppressive TME drive the progression of antitumor effector CD8+ T cells into a functionally impaired state called ‘T cell exhaustion’.3Exhausted CD8+ T cells possess diminished effector functions and a distinct transcriptional profile relative to those of effector cells.Exhausted CD8+ T cells also express high amounts of inhibitory receptors, such as programmed cell death-1 (PD1), T-cell immunoglobulin and mucin-domain containing-3 (TIM3), lymphocyte-activation gene 3 (LAG3) and cytotoxic T lymphocyte-associated antigen-4 (CTLA4).In addition, exhausted CD8+ T cells show decreases in proliferative potential, diminished cytotoxic function, and reduced ability to produce effector cytokines.1,4T cell exhaustion plays a vital role in the development and progression of cancer, but it also provides a new avenue for cancer treatment.4It has been well documented that reversing T cell dysfunction can re-establish immune responses against virus or cancer cells.4This effect is also evident in cancer immunotherapies that target exhausted CD8+ T cells to reinvigorate their anti- tumorigenic function by blocking inhibitory receptors.5–7
The thymocyte selection-associated high mobility group box protein (TOX) plays a vital role in T cell development and differentiation, however, its role in T cell exhaustion was unexplored. Here, we aim to investigate the role of TOX in regulating the antitumor effect of CD8+ T cells in hepatocellular carcinoma. Fully functional, partially and severely exhausted tumor-infiltrating CD8+ T cells were sorted by flow cytometry and subjected to transcriptome sequencing analysis. Upregulated TOX expression was validated by flow cytometry. The antitumor function of CD8+ T cells with TOX downregulation or overexpression was studied in a mouse HCC model and HCC patient-derived xenograft mouse model. Transcriptome sequencing analysis was performed in TOX-overexpressing and control CD8+ T cells. The mechanism underlying the TOX-mediated regulation of PD1 expression was studied by laser confocal detection, immune co-precipitation and flow cytometer. TOX was upregulated in exhausted CD8+ T cells in hepatocellular carcinoma. TOX downregulation in CD8+ T cells inhibited tumor growth, increased CD8+ T cell infiltration, alleviated CD8+ T cell exhaustion and improved the anti-PD1 response of CD8+ T cells. The mechanism behind this involved the binding of TOX to PD1 in the cytoplasm, which facilitated the endocytic recycling of PD1, thus maintaining abundant PD1 expression at the cell surface. High expression of TOX in peripheral CD8+ T cells correlated with poorer anti-PD1 responses and prognosis. TOX promotes CD8+ T cell exhaustion in hepatocellular carcinoma by regulating endocytic recycling of PD1. Downregulating TOX expression in CD8+ T cells exerts synergistic effects with anti-PD1 therapy, highlighting a promising strategy for cancer immunotherapy. There are 2 major varieties of exhausted CD8+ T cells in TME: fully exhausted cells expressing high levels of PD1 on the cell surface, and partially exhausted cells with intermediate levels of PD1 cell surface expression.3The function of partially exhausted CD8+ T cells could be partially reversed by immune checkpoint blockers, such as anti-PD1 or anti-PDL1 antibodies, which are currently used to treat diverse cancers.3,8Recent studies have found several key genes that determine CD8+ T cell exhaustion, such as NFAT, BLIMP1, GATA3 and PSGL1,4,9–10 which represent potential targets for inhibiting CD8+ T exhaustion and/or restoring CD8+ T cell functionality in the partial or full exhaustion state.However, whether other factors are involved in regulating CD8+ T cell exhaustion in the TME remains largely unknown.
Binge alcohol exposure causes gut leakiness, contributing to increased endotoxemia and inflammatory liver injury, although the molecular mechanisms are still elusive. This study was aimed at investigating the roles of apoptosis of enterocytes and nitration followed by degradation of intestinal tight junction (TJ) and adherens junction (AJ) proteins in binge alcohol-induced gut leakiness. The levels of intestinal (ileum) junctional complex proteins, oxidative stress markers and apoptosis-related proteins in rodents, T84 colonic cells and autopsied human ileums were determined by immunoblot, immunoprecipitation, immunofluorescence, and mass-spectral analyses. Binge alcohol exposure caused apoptosis of gut enterocytes with elevated serum endotoxin and liver injury. The levels of intestinal CYP2E1, iNOS, nitrated proteins and apoptosis-related marker proteins were significantly elevated in binge alcohol-exposed rodents. Differential, quantitative mass-spectral analyses of the TJ-enriched fractions of intestinal epithelial layers revealed that several TJ, AJ and desmosome proteins were decreased in binge alcohol-exposed rats compared to controls. Consistently, the levels of TJ proteins (claudin-1, claudin-4, occludin and zonula occludens-1), AJ proteins (β-catenin and E-cadherin) and desmosome plakoglobin were very low in binge alcohol-exposed rats, wild-type mice, and autopsied human ileums but not in Cyp2e1-null mice. Additionally, pretreatment with specific inhibitors of CYP2E1 and iNOS prevented disorganization and/or degradation of TJ proteins in alcohol-exposed T84 colonic cells. Furthermore, immunoprecipitation followed by immunoblot confirmed that intestinal TJ and AJ proteins were nitrated and degraded via ubiquitin-dependent proteolysis, resulting in their decreased levels. These results demonstrated for the first time the critical roles of CYP2E1, apoptosis of enterocytes, and nitration followed by ubiquitin-dependent proteolytic degradation of the junctional complex proteins, in promoting binge alcohol-induced gut leakiness and endotoxemia, contributing to inflammatory liver disease. Excessive alcohol intake can damage many organs, causing deaths in severe cases.Heavy alcohol intake is also known to cause gut leakiness, contributing to increased endotoxemia and inflammatory tissue damage in the liver and brain.1–4Various pathological conditions, such as HIV infection,5–7 obesity,8 and burn injury,9 are known to increase gut leakiness and endotoxemia.In addition, binge alcohol10 and non-alcoholic substances such as western-style high-fat diets11 and fructose12 can stimulate gut leakiness, leading to elevated serum endotoxin and liver inflammation.Furthermore, alcoholic patients with cirrhosis had higher levels of endotoxin than those without cirrhosis.4,13These conditions observed in humans can be replicated in experimental animals or cultured Caco-2 and T84 colonic cells.In fact, both other laboratories and ourselves have reported that binge alcohol caused gut leakiness in an ethanol-inducible cytochrome P450-2E1 (CYP2E1)-dependent manner,1,2 since Cyp2e1-null mice were resistant to alcohol-induced gut leakiness despite extremely high doses of alcohol (three oral doses of 6 g ethanol/kg/dose) at 12 h intervals.10Furthermore, pretreatment with a specific siRNA to CYP2E1 in Caco-2 human colonic cells2 or a chemical inhibitor of CYP2E1 chlormethiazole (CMZ) or an antioxidant N-acetylcysteine10 efficiently prevented alcohol-induced epithelial barrier dysfunction or gut leakiness, supporting the contributing role of CYP2E1-dependent oxidative stress in gut leakiness.Despites numerous reports, the detailed molecular mechanisms by which binge alcohol and CYP2E1-mediated oxidative stress stimulate gut leakiness are still poorly understood.
Several studies have shown that chronic hepatitis C (CHC) infection has a negative impact on kidney function, as well as survival, in patients with chronic kidney disease (CKD) or on hemodialysis. The aim of this nationwide registry study was to describe renal disease in Swedish patients with CHC. In the present study, patients were identified for CHC (B18.2) and CKD (N18) according to the International Classification of Diseases (ICD)-10 in the nationwide Swedish inpatient care day surgery (1997–2013) and non-primary outpatient care (2001–2013) patient registries. Hemodialysis was defined using the procedure code in the non-primary outpatient care. For each patient, up to five non-CHC diagnosed age/sex/place of residency-matched comparators were drawn from the general population at the time of diagnosis. Follow-up started at the date of CHC diagnosis and patients accrued person-time until, whichever came first, death, emigration or December 31st, 2013. Between 2001 and 2013, 42,522 patients received a CHC diagnosis. Of these patients, 2.5% (1,077/45,222) were diagnosed with CKD during 280,123 person-years, compared with 0.7% (1,454/202,694) in the matched general population comparators (1,504,765 person-years), resulting in a standardized incidence ratio (SIR) of 4.0. There was a 3.3–7.0-fold risk of patients with CHC requiring hemodialysis. Overall, 17% of patients with CHC receiving hemodialysis were treated for CHC; 24% in the treated cohort died compared with 56% of the untreated cohort (p <0.0001), with antiviral treatment improving survival with an odds ratio of 3.901 (p = 0.001). The results from this nationwide registry study showed that patients with CHC are at a higher risk of developing CKD. Furthermore, hepatitis C treatment seemed to improve survival for patients with CHC on hemodialysis compared with untreated patients. Hepatitis C virus (HCV) infection is a major cause of viral hepatitis with a global seroprevalence estimated to be greater than 185 million.1Approximately 75% to 85% of patients with HCV infection develop a chronic hepatitis C (CHC) infection.2In addition to the direct negative impact of the virus on the liver, where more than 40% of CHC infections lead to liver cirrhosis after 30 years,3 CHC infection is also associated with extrahepatic manifestations including kidney disease, most commonly membranoproliferative glomerulonephritis with or without cryoglobulinemia.4
Several studies have shown that chronic hepatitis C (CHC) infection has a negative impact on kidney function, as well as survival, in patients with chronic kidney disease (CKD) or on hemodialysis. The aim of this nationwide registry study was to describe renal disease in Swedish patients with CHC. In the present study, patients were identified for CHC (B18.2) and CKD (N18) according to the International Classification of Diseases (ICD)-10 in the nationwide Swedish inpatient care day surgery (1997–2013) and non-primary outpatient care (2001–2013) patient registries. Hemodialysis was defined using the procedure code in the non-primary outpatient care. For each patient, up to five non-CHC diagnosed age/sex/place of residency-matched comparators were drawn from the general population at the time of diagnosis. Follow-up started at the date of CHC diagnosis and patients accrued person-time until, whichever came first, death, emigration or December 31st, 2013. Between 2001 and 2013, 42,522 patients received a CHC diagnosis. Of these patients, 2.5% (1,077/45,222) were diagnosed with CKD during 280,123 person-years, compared with 0.7% (1,454/202,694) in the matched general population comparators (1,504,765 person-years), resulting in a standardized incidence ratio (SIR) of 4.0. There was a 3.3–7.0-fold risk of patients with CHC requiring hemodialysis. Overall, 17% of patients with CHC receiving hemodialysis were treated for CHC; 24% in the treated cohort died compared with 56% of the untreated cohort (p <0.0001), with antiviral treatment improving survival with an odds ratio of 3.901 (p = 0.001). The results from this nationwide registry study showed that patients with CHC are at a higher risk of developing CKD. Furthermore, hepatitis C treatment seemed to improve survival for patients with CHC on hemodialysis compared with untreated patients. A 70% higher risk of chronic kidney disease (CKD) (defined as an estimated glomerular filtration rate of <60 ml/min/1.73 m2) has been observed in HCV seropositive patients compared to seronegative patients.5In a nationwide study from Taiwan, Chen and colleagues showed that HCV is a causal risk factor for CKD beyond traditional risk factors.6Worldwide, diabetes mellitus is the most common cause of end-stage renal disease, and in patients with diabetes, the presence of HCV adds to the risk of CKD and end-stage renal disease.7,8In the transplant setting the presence of HCV has repeatedly been shown to be an independent risk factor for poorer patient and graft survival.9,10The prevalence of anti-HCV positive patients on long-term dialysis in northern Europe is less than 5%, approximately 10% in most of southern Europe and the United States, and between 10% and 70% in many countries of the developing world.11
Several studies have shown that chronic hepatitis C (CHC) infection has a negative impact on kidney function, as well as survival, in patients with chronic kidney disease (CKD) or on hemodialysis. The aim of this nationwide registry study was to describe renal disease in Swedish patients with CHC. In the present study, patients were identified for CHC (B18.2) and CKD (N18) according to the International Classification of Diseases (ICD)-10 in the nationwide Swedish inpatient care day surgery (1997–2013) and non-primary outpatient care (2001–2013) patient registries. Hemodialysis was defined using the procedure code in the non-primary outpatient care. For each patient, up to five non-CHC diagnosed age/sex/place of residency-matched comparators were drawn from the general population at the time of diagnosis. Follow-up started at the date of CHC diagnosis and patients accrued person-time until, whichever came first, death, emigration or December 31st, 2013. Between 2001 and 2013, 42,522 patients received a CHC diagnosis. Of these patients, 2.5% (1,077/45,222) were diagnosed with CKD during 280,123 person-years, compared with 0.7% (1,454/202,694) in the matched general population comparators (1,504,765 person-years), resulting in a standardized incidence ratio (SIR) of 4.0. There was a 3.3–7.0-fold risk of patients with CHC requiring hemodialysis. Overall, 17% of patients with CHC receiving hemodialysis were treated for CHC; 24% in the treated cohort died compared with 56% of the untreated cohort (p <0.0001), with antiviral treatment improving survival with an odds ratio of 3.901 (p = 0.001). The results from this nationwide registry study showed that patients with CHC are at a higher risk of developing CKD. Furthermore, hepatitis C treatment seemed to improve survival for patients with CHC on hemodialysis compared with untreated patients. The treatment of patients with CHC infection on hemodialysis using interferon (IFN)-α with or without ribavirin (RBV) has been cumbersome because of an increased risk of adverse events and a lack of expertise in treating this special population.In a recent large international observational study (Dialysis Outcomes and Practice Patterns Study [DOPPS] phase I-IV), it was noted that only 1.0% of patients with CHC infection on hemodialysis had received antiviral treatment.12After the approval of sofosbuvir (SOF)-based, IFN-free, all-oral direct acting antivirals (DAAs) in 2014, there has been an extraordinary improvement in treatment outcome, with high cure rates in patients with CHC infection and normal kidney function.As SOF is renally excreted, the safety of SOF has not been fully assessed in renally impaired patients.13In fact, as reported from an even larger observational study conducted in patients with CHC on hemodialysis between 2012–2015 (DOPPS phase V), even after the introduction of DAAs the antiviral treatment rate only increased to 2.1% with only a limited number of patients actually treated with DAAs.14The same report pointed out that HCV infection among patients on hemodialysis is associated with higher risk of death, hospitalization, anemic complications, and worse quality-of-life scores, thereby highlighting the urgent need for effective HCV treatment in this population.However, recent approvals of the SOF-free DAAs dasabuvir, ombitasvir/paritaprevir/ritonavir with or without RBV, and elbasvir/grazoprevir have been shown to be well-tolerated in patients with genotype 1 CHC and CKD 4 or 5 (including hemodialysis), with virologic cure rates of above 90%, in both single-arm and placebo-controlled studies.15,16Moreover, it was recently shown that 98% of patients with genotype 1 to 6 CHC and CKD stage 4 or 5 (including hemodialysis) treated with a recently approved combination containing glecaprevir/pibrentasvir achieved a sustained virologic response.17
Subclinical inflammatory changes are commonly described in long-term transplant recipients undergoing protocol liver biopsies. The pathogenesis of these lesions remains unclear. The aim of this study was to identify the key molecular pathways driving progressive subclinical inflammatory liver allograft damage. All liver recipients followed at Hospital Clínic Barcelona who were >10 years post-transplant were screened for participation in the study. Patients with recurrence of underlying liver disease, biliary or vascular complications, chronic rejection, and abnormal liver function tests were excluded. Sixty-seven patients agreed to participate and underwent blood and serological tests, transient elastography and a liver biopsy. Transcriptome profiling was performed on RNA extracted from 49 out of the 67 biopsies employing a whole genome next generation sequencing platform. Patients were followed for a median of 6.8 years following the index liver biopsy. Median time since transplantation to liver biopsy was 13 years (10–22). The most frequently observed histological abnormality was portal inflammation with different degrees of fibrosis, present in 45 biopsies (67%). Two modules of 102 and 425 co-expressed genes were significantly correlated with portal inflammation, interface hepatitis and portal fibrosis. These modules were enriched in molecular pathways known to be associated with T cell mediated rejection. Liver allografts showing the highest expression levels for the two modules recapitulated the transcriptional profile of biopsies with clinically apparent rejection and developed progressive damage over time, as assessed by non-invasive markers of fibrosis. A large proportion of adult liver transplant recipients who survive long-term exhibit subclinical histological abnormalities. The transcriptomic profile of these patients’ liver tissue closely resembles that of T cell mediated rejection and may result in progressive allograft damage. Routine serum markers of liver injury such as aspartate aminotransferase (AST), alanine aminotransferases (ALT), gamma-glutamyl-transpeptidase (GGT) or alkaline phosphatase (AP) are known to be insensitive and nonspecific indicators of allograft rejection in liver transplantation (LT).1,2Despite this, the long-term management of LT recipients continues to rely on a combination of serum liver biochemistry tests and calcineurin inhibitor pharmacokinetic markers.The performance of protocol, or surveillance, liver biopsies has been proposed as a more accurate strategy to assess graft function and potentially to personalize the use of immunosuppression.3–5This is based on a multiplicity of studies showing that a very large proportion of patients with normal liver biochemistry tests exhibit clinically significant histological lesions, with chronic hepatitis not attributable to recognizable causes, such as viral infection or autoimmune hepatitis, being the most frequently described abnormality.1,6–8However, the clinical utility of protocol liver biopsies remains contentious,5 and as a result they are not performed in the vast majority of adult liver transplant programs.The controversy stems from an incomplete understanding of the natural history and pathogenesis of the so-called idiopathic inflammatory lesions.2,9This is due to the paucity of prospective clinical studies and the lack of in-depth studies comparing the molecular signatures of these lesions with those of well-characterized histological phenotypes.
Cancer is a major cause of death in patients with non-alcoholic fatty liver disease (NAFLD). Obesity is a risk factor for cancers; however, the role of NAFLD in this association is unknown. We investigated the effect of NAFLD versus obesity on incident cancers. We identified all incident cases of NAFLD in a US population between 1997-2016. Individuals with NAFLD were matched by age and sex to referent individuals from the same population (1:3) on the index diagnosis date. We ascertained the incidence of cancer after index date until death, loss to follow-up or study end. NAFLD and cancer were defined using a code-based algorithm with high validity and tested by medical record review. The association between NAFLD or obesity and cancer risk was examined using Poisson regression. A total of 4,722 individuals with NAFLD (median age 54, 46% male) and 14,441 age- and sex-matched referent individuals were followed for a median of 8 (range 1–21) years, during which 2,224 incident cancers occurred. NAFLD was associated with 90% higher risk of malignancy: incidence rate ratio (IRR) = 1.9 (95% CI 1.3–2.7). The highest risk increase was noted in liver cancer, IRR = 2.8 (95% CI 1.6–5.1), followed by uterine IRR = 2.3 (95% CI 1.4–4.1), stomach IRR = 2.3 (95% CI 1.3–4.1), pancreas IRR = 2.0 (95% CI 1.2–3.3) and colon cancer IRR = 1.8 (95% CI 1.1–2.8). In reference to non-obese controls, NAFLD was associated with a higher risk of incident cancers (IRR = 2.0, 95% CI 1.5–2.9), while obesity alone was not (IRR = 1.0, 95% CI 0.8–1.4). NAFLD was associated with increased cancer risk, particularity of gastrointestinal types. In the absence of NAFLD, the association between obesity and cancer risk is small, suggesting that NAFLD may be a mediator of the obesity-cancer association. Cancer is a major cause of death in the United States and worldwide.1,2Numerous meta-analyses support the link between the risk of malignancy and excess body weight.3–5Some associations are flawed because of bias that exaggerates the effect of obesity on cancer incidence, but strong evidence supports this association with 11 cancers, predominantly among digestive organs and hormone-related malignancies in women.4
Cancer is a major cause of death in patients with non-alcoholic fatty liver disease (NAFLD). Obesity is a risk factor for cancers; however, the role of NAFLD in this association is unknown. We investigated the effect of NAFLD versus obesity on incident cancers. We identified all incident cases of NAFLD in a US population between 1997-2016. Individuals with NAFLD were matched by age and sex to referent individuals from the same population (1:3) on the index diagnosis date. We ascertained the incidence of cancer after index date until death, loss to follow-up or study end. NAFLD and cancer were defined using a code-based algorithm with high validity and tested by medical record review. The association between NAFLD or obesity and cancer risk was examined using Poisson regression. A total of 4,722 individuals with NAFLD (median age 54, 46% male) and 14,441 age- and sex-matched referent individuals were followed for a median of 8 (range 1–21) years, during which 2,224 incident cancers occurred. NAFLD was associated with 90% higher risk of malignancy: incidence rate ratio (IRR) = 1.9 (95% CI 1.3–2.7). The highest risk increase was noted in liver cancer, IRR = 2.8 (95% CI 1.6–5.1), followed by uterine IRR = 2.3 (95% CI 1.4–4.1), stomach IRR = 2.3 (95% CI 1.3–4.1), pancreas IRR = 2.0 (95% CI 1.2–3.3) and colon cancer IRR = 1.8 (95% CI 1.1–2.8). In reference to non-obese controls, NAFLD was associated with a higher risk of incident cancers (IRR = 2.0, 95% CI 1.5–2.9), while obesity alone was not (IRR = 1.0, 95% CI 0.8–1.4). NAFLD was associated with increased cancer risk, particularity of gastrointestinal types. In the absence of NAFLD, the association between obesity and cancer risk is small, suggesting that NAFLD may be a mediator of the obesity-cancer association. The prevalence of obesity has more than doubled in the last 4 decades6,7 and as a result, the incidence of non-alcoholic fatty liver disease (NAFLD) has increased substantially.8–10Large population studies have clearly established that malignancy is among the top 2 causes of death in NAFLD, vastly surpassing liver-related mortality, which occurs in 1–2% of patients.11,12However, the specific types of cancer that patients with NAFLD are at increased risk for, or the magnitude of risk compared to those without NAFLD is not known.Moreover, it is not clear whether there are particular characteristics of malignancy risk among those with NAFLD that are distinct from those with obesity alone.Such data have important implications in patient education, counseling and the application of screening strategies in this high-risk population.
Excess liver iron content is common and is linked to the risk of hepatic and extrahepatic diseases. We aimed to identify genetic variants influencing liver iron content and use genetics to understand its link to other traits and diseases. First, we performed a genome-wide association study (GWAS) in 8,289 individuals from UK Biobank, whose liver iron level had been quantified by magnetic resonance imaging, before validating our findings in an independent cohort (n = 1,513 from IMI DIRECT). Second, we used Mendelian randomisation to test the causal effects of 25 predominantly metabolic traits on liver iron content. Third, we tested phenome-wide associations between liver iron variants and 770 traits and disease outcomes. We identified 3 independent genetic variants (rs1800562 [C282Y] and rs1799945 [H63D] in HFE and rs855791 [V736A] in TMPRSS6) associated with liver iron content that reached the GWAS significance threshold (p <5 × 10−8). The 2 HFE variants account for ∼85% of all cases of hereditary haemochromatosis. Mendelian randomisation analysis provided evidence that higher central obesity plays a causal role in increased liver iron content. Phenome-wide association analysis demonstrated shared aetiopathogenic mechanisms for elevated liver iron, high blood pressure, cirrhosis, malignancies, neuropsychiatric and rheumatological conditions, while also highlighting inverse associations with anaemias, lipidaemias and ischaemic heart disease. Our study provides genetic evidence that mechanisms underlying higher liver iron content are likely systemic rather than organ specific, that higher central obesity is causally associated with higher liver iron, and that liver iron shares common aetiology with multiple metabolic and non-metabolic diseases. Liver disease constitutes the third most common cause of premature death in the UK, and its prevalence is substantially higher compared to other countries in Western Europe.1–3Excess liver iron is associated with increased severity and progression of liver diseases including cirrhosis and hepatocellular carcinoma in individuals with non-alcoholic fatty liver disease (NAFLD),4–6 and is the direct cause of liver disease in those with hereditary haemochromatosis and thalassaemia.7,8Observational associations have been described between excess liver iron content and several metabolic diseases such as high blood pressure, obesity, polycystic ovarian syndrome and type 2 diabetes, in a condition recognised as dysmetabolic iron overload syndrome (DIOS) which affects up to 5–10% of the general population.9,10
Excess liver iron content is common and is linked to the risk of hepatic and extrahepatic diseases. We aimed to identify genetic variants influencing liver iron content and use genetics to understand its link to other traits and diseases. First, we performed a genome-wide association study (GWAS) in 8,289 individuals from UK Biobank, whose liver iron level had been quantified by magnetic resonance imaging, before validating our findings in an independent cohort (n = 1,513 from IMI DIRECT). Second, we used Mendelian randomisation to test the causal effects of 25 predominantly metabolic traits on liver iron content. Third, we tested phenome-wide associations between liver iron variants and 770 traits and disease outcomes. We identified 3 independent genetic variants (rs1800562 [C282Y] and rs1799945 [H63D] in HFE and rs855791 [V736A] in TMPRSS6) associated with liver iron content that reached the GWAS significance threshold (p <5 × 10−8). The 2 HFE variants account for ∼85% of all cases of hereditary haemochromatosis. Mendelian randomisation analysis provided evidence that higher central obesity plays a causal role in increased liver iron content. Phenome-wide association analysis demonstrated shared aetiopathogenic mechanisms for elevated liver iron, high blood pressure, cirrhosis, malignancies, neuropsychiatric and rheumatological conditions, while also highlighting inverse associations with anaemias, lipidaemias and ischaemic heart disease. Our study provides genetic evidence that mechanisms underlying higher liver iron content are likely systemic rather than organ specific, that higher central obesity is causally associated with higher liver iron, and that liver iron shares common aetiology with multiple metabolic and non-metabolic diseases. The associations between excess liver iron and hepatic and non-hepatic diseases necessitate exploration of underlying pathophysiological mechanisms.Studies of patients with hereditary haemochromatosis, with autosomal recessive mutations, show they have higher liver iron, measured from biopsies, when compared to controls.However, no studies have been performed in unselected populations.Furthermore, it is unknown whether iron accumulation is a systemic disorder involving multiple organs or whether there are mechanisms specific to the liver.Previous genome-wide association studies (GWAS) have focussed on peripheral biochemical markers of iron status that do not correlate well with liver iron.11
Excess liver iron content is common and is linked to the risk of hepatic and extrahepatic diseases. We aimed to identify genetic variants influencing liver iron content and use genetics to understand its link to other traits and diseases. First, we performed a genome-wide association study (GWAS) in 8,289 individuals from UK Biobank, whose liver iron level had been quantified by magnetic resonance imaging, before validating our findings in an independent cohort (n = 1,513 from IMI DIRECT). Second, we used Mendelian randomisation to test the causal effects of 25 predominantly metabolic traits on liver iron content. Third, we tested phenome-wide associations between liver iron variants and 770 traits and disease outcomes. We identified 3 independent genetic variants (rs1800562 [C282Y] and rs1799945 [H63D] in HFE and rs855791 [V736A] in TMPRSS6) associated with liver iron content that reached the GWAS significance threshold (p <5 × 10−8). The 2 HFE variants account for ∼85% of all cases of hereditary haemochromatosis. Mendelian randomisation analysis provided evidence that higher central obesity plays a causal role in increased liver iron content. Phenome-wide association analysis demonstrated shared aetiopathogenic mechanisms for elevated liver iron, high blood pressure, cirrhosis, malignancies, neuropsychiatric and rheumatological conditions, while also highlighting inverse associations with anaemias, lipidaemias and ischaemic heart disease. Our study provides genetic evidence that mechanisms underlying higher liver iron content are likely systemic rather than organ specific, that higher central obesity is causally associated with higher liver iron, and that liver iron shares common aetiology with multiple metabolic and non-metabolic diseases. Measuring liver iron has traditionally been difficult.Liver biopsy, the “gold standard” for assessment of liver iron, is an invasive procedure and therefore unsuitable for population research studies.An alternative is magnetic resonance imaging (MRI); a non-invasive, quick, robust and validated method for quantifying liver iron content.12The availability of genetic and clinical data, as well as MRI scans of livers in the UK Biobank cohort has provided an unparalleled opportunity to study the genetics of liver iron content in a population-based cohort.
The pathogenesis of non-alcoholic fatty liver disease (NAFLD) and steatohepatitis (NASH) is likely due to the interaction between a deranged metabolic milieu and local mediators of hepatic inflammation and fibrosis. We undertook this study to elucidate the interplay between macrophage activation, insulin resistance (IR) in target organs/tissues and hepatic damage. In 40 non-diabetic patients with biopsy-proven NAFLD we assessed: i) endogenous glucose production (EGP), glucose clearance and indexes of IR in the adipose tissue (Adipo-IR and Lipo-IR) and in the liver (Hep-IR) by tracer infusion ([6,6-2H2]glucose and [2H5]glycerol); ii) macrophage activity (by soluble sCD163) and iii) hepatic expression of CD163 (hCD163). We found that sCD163 levels paralleled both the plasma free fatty acid (FFA) levels and lipolysis from adipose tissue. Consistently, sCD163 significantly correlated with adipose tissue IR (Adipo-IR: r = 0.32, p = 0.042; Lipo-IR: r = 0.39, p = 0.012). At multiple regression analysis, sCD163 levels were associated with FFA levels (rp = 0.35, p = 0.026). In vitro exposure of human monocyte-derived macrophages to palmitate enhanced sCD163 secretion. Conversely, sCD163 did not correlate with EGP or with Hep-IR. In the liver, hCD163 positively correlated with sCD163 (r = 0.58, p = 0.007) and the degree of steatosis (r = 0.34, p = 0.048), but not with EGP or Hep-IR (r = −0.27 and r = 0.11, respectively, p >0.10, both). Our findings suggest a link between deranged metabolism in the adipose tissue and activation of hepatic macrophages in patients with NAFLD, possibly in response to FFA overflow and independent of obesity and diabetes. Conversely, our findings do not support a link between activated hepatic macrophages and glucose metabolism (EGP or Hep-IR). The relationship between adipose tissue IR and hepatic macrophages should be considered to define therapeutic targets for NAFLD. The recent epidemic of chronic liver disease is related to the burden of non-alcoholic fatty liver disease (NAFLD), paralleling the worldwide increase of obesity.1NAFLD is a complex condition related to metabolic derangements in insulin resistance (IR), but in a subset of patients the liver becomes the target of multiple hits leading to non-alcoholic steatohepatitis (NASH), the histological phenotype that may progressively lead to the development of liver fibrosis, cirrhosis and possibly hepatocellular carcinoma.1,2Understanding the biological and environmental factors that drive the progression to NASH and beyond in some individuals is fundamental to the development of robust methods for diagnosis, risk stratification and therapy.2
The pathogenesis of non-alcoholic fatty liver disease (NAFLD) and steatohepatitis (NASH) is likely due to the interaction between a deranged metabolic milieu and local mediators of hepatic inflammation and fibrosis. We undertook this study to elucidate the interplay between macrophage activation, insulin resistance (IR) in target organs/tissues and hepatic damage. In 40 non-diabetic patients with biopsy-proven NAFLD we assessed: i) endogenous glucose production (EGP), glucose clearance and indexes of IR in the adipose tissue (Adipo-IR and Lipo-IR) and in the liver (Hep-IR) by tracer infusion ([6,6-2H2]glucose and [2H5]glycerol); ii) macrophage activity (by soluble sCD163) and iii) hepatic expression of CD163 (hCD163). We found that sCD163 levels paralleled both the plasma free fatty acid (FFA) levels and lipolysis from adipose tissue. Consistently, sCD163 significantly correlated with adipose tissue IR (Adipo-IR: r = 0.32, p = 0.042; Lipo-IR: r = 0.39, p = 0.012). At multiple regression analysis, sCD163 levels were associated with FFA levels (rp = 0.35, p = 0.026). In vitro exposure of human monocyte-derived macrophages to palmitate enhanced sCD163 secretion. Conversely, sCD163 did not correlate with EGP or with Hep-IR. In the liver, hCD163 positively correlated with sCD163 (r = 0.58, p = 0.007) and the degree of steatosis (r = 0.34, p = 0.048), but not with EGP or Hep-IR (r = −0.27 and r = 0.11, respectively, p >0.10, both). Our findings suggest a link between deranged metabolism in the adipose tissue and activation of hepatic macrophages in patients with NAFLD, possibly in response to FFA overflow and independent of obesity and diabetes. Conversely, our findings do not support a link between activated hepatic macrophages and glucose metabolism (EGP or Hep-IR). The relationship between adipose tissue IR and hepatic macrophages should be considered to define therapeutic targets for NAFLD. The prevailing notion of NASH pathogenesis is that a deranged metabolic milieu specifically interacts with local mediators of hepatic inflammation and fibrosis, but the nature of these interactions has not been fully elucidated.3,4It is generally believed that adipose tissue IR plays a pivotal role in the onset and progression of NAFLD.5,6Briefly, weight gain leads to expansion of adipose tissue and recruitment of macrophages through the secretion of various chemo- and cytokines.7Inflamed and dysfunctional adipose tissue actively releases free fatty acids (FFAs) into the bloodstream, promotes lipotoxicity in the liver, muscle and pancreas,3 and contributes to systemic inflammation.In the normal liver, resident macrophages or Kupffer cells (KCs) play important regulatory roles through crosstalk with the different cell types and particularly with hepatocytes.8The pro-inflammatory polarization of hepatic macrophages is considered a hallmark of progressive disease, in the livers of patients with NASH, and an attractive therapeutic target as recently reviewed.8Hepatic lipid accumulation facilitates pro-inflammatory KC polarization, possibly as a consequence of FFA excess, or signals from surrounding steatotic hepatocytes, such as histidine-rich glycoprotein, extracellular vesicles or damage-associated molecular patterns.8More recently, data derived from animal models and in vitro studies suggest that both pro-inflammatory KCs and recruited hepatic macrophages (Ly6Chi) contribute to decreased hepatic insulin sensitivity by inhibiting insulin signaling and activating hepatic glucose production.9However, most data available have been derived from mouse models, which are not fully representative of human NASH, since they reflect certain aspects of the pathogenesis and rarely incorporate the full spectrum of etiology-specific mechanisms.8
The pathogenesis of non-alcoholic fatty liver disease (NAFLD) and steatohepatitis (NASH) is likely due to the interaction between a deranged metabolic milieu and local mediators of hepatic inflammation and fibrosis. We undertook this study to elucidate the interplay between macrophage activation, insulin resistance (IR) in target organs/tissues and hepatic damage. In 40 non-diabetic patients with biopsy-proven NAFLD we assessed: i) endogenous glucose production (EGP), glucose clearance and indexes of IR in the adipose tissue (Adipo-IR and Lipo-IR) and in the liver (Hep-IR) by tracer infusion ([6,6-2H2]glucose and [2H5]glycerol); ii) macrophage activity (by soluble sCD163) and iii) hepatic expression of CD163 (hCD163). We found that sCD163 levels paralleled both the plasma free fatty acid (FFA) levels and lipolysis from adipose tissue. Consistently, sCD163 significantly correlated with adipose tissue IR (Adipo-IR: r = 0.32, p = 0.042; Lipo-IR: r = 0.39, p = 0.012). At multiple regression analysis, sCD163 levels were associated with FFA levels (rp = 0.35, p = 0.026). In vitro exposure of human monocyte-derived macrophages to palmitate enhanced sCD163 secretion. Conversely, sCD163 did not correlate with EGP or with Hep-IR. In the liver, hCD163 positively correlated with sCD163 (r = 0.58, p = 0.007) and the degree of steatosis (r = 0.34, p = 0.048), but not with EGP or Hep-IR (r = −0.27 and r = 0.11, respectively, p >0.10, both). Our findings suggest a link between deranged metabolism in the adipose tissue and activation of hepatic macrophages in patients with NAFLD, possibly in response to FFA overflow and independent of obesity and diabetes. Conversely, our findings do not support a link between activated hepatic macrophages and glucose metabolism (EGP or Hep-IR). The relationship between adipose tissue IR and hepatic macrophages should be considered to define therapeutic targets for NAFLD. Soluble CD163 (sCD163) is the ecto-domain of the hemoglobin-haptoglobin scavenger receptor which is exclusively expressed on macrophages and monocytes.It is shed to the circulation during macrophage activation by metalloprotease activity (e.g. tumor necrosis factor-alpha converting enzyme (TACE/ADAM17)).10,11CD163-positive macrophages are highly expressed in human adipose tissue from obese individuals, while sCD163 levels are associated with hepatic inflammation and fibrosis in patients with NAFLD12,13 and decrease after successful life-style intervention and bariatric surgery.14–16
The risk of hepatocellular carcinoma (HCC) during antiviral therapy in patients with chronic hepatitis B (CHB) is inadequately predicted by the scores built from untreated patients. We aimed at developing and validating a risk score to predict HCC in patients with CHB on entecavir or tenofovir treatment. This study analysed population-wide data from the healthcare databases in Taiwan and Hong Kong to identify patients with CHB continuously receiving entecavir or tenofovir. The development cohort included 23,851 patients from Taiwan; 596 (2.50%) of them developed HCC with a three-year cumulative incidence of 3.56% (95% CI 3.26–3.86%). The multivariable Cox proportional hazards model found that cirrhosis, age (cirrhosis and age interacted with each other), male sex, and diabetes mellitus were the risk determinants. These variables were weighted to develop the cirrhosis, age, male sex, and diabetes mellitus (CAMD) score ranging from 0 to 19 points. The score was externally validated in 19,321 patients from Hong Kong. The c indices for HCC in the development cohort were 0.83 (95% CI 0.81–0.84), 0.82 (95% CI 0.81–0.84), and 0.82 (95% CI 0.80–0.83) at the first, second, and third years of therapy, respectively. In the validation cohort, the c indices were 0.74 (95% CI 0.71–0.77), 0.75 (95% CI 0.73–0.78), and 0.75 (95% CI 0.72–0.77) during the first three years, and 0.76 (95% CI 0.74–0.78) and 0.76 (95% CI 0.74–0.77) in the extrapolated fourth and fifth years, respectively. The predicted and observed probabilities of HCC were calibrated in both cohorts. A score <8 and >13 points identified patients at distinctly low and high risks. The easily calculable CAMD score can predict HCC and may inform surveillance policy in patients with CHB during oral antiviral therapy. Hepatitis B virus (HBV) infection is the leading aetiology of hepatocellular carcinoma (HCC) around the globe.1,2The risk of HCC is a lifelong threat to patients with chronic hepatitis B (CHB).3Antiviral therapy using nucleos(t)ide analogues (NAs) inhibits HBV replication,4–6 ameliorates hepatic inflammation,7 reverses liver fibrosis,8 and may attenuate hepatocellular carcinogenesis.We and others have shown that NA treatment is associated with risk reduction of HCC in patients with CHB.9–12In addition, the incidences of HCC decreased over the years while on therapies.13–15However, antiviral treatment does not completely eliminate the risk of HCC.16Beyond viral suppression, it remains unclear how to lower the risk further.
The risk of hepatocellular carcinoma (HCC) during antiviral therapy in patients with chronic hepatitis B (CHB) is inadequately predicted by the scores built from untreated patients. We aimed at developing and validating a risk score to predict HCC in patients with CHB on entecavir or tenofovir treatment. This study analysed population-wide data from the healthcare databases in Taiwan and Hong Kong to identify patients with CHB continuously receiving entecavir or tenofovir. The development cohort included 23,851 patients from Taiwan; 596 (2.50%) of them developed HCC with a three-year cumulative incidence of 3.56% (95% CI 3.26–3.86%). The multivariable Cox proportional hazards model found that cirrhosis, age (cirrhosis and age interacted with each other), male sex, and diabetes mellitus were the risk determinants. These variables were weighted to develop the cirrhosis, age, male sex, and diabetes mellitus (CAMD) score ranging from 0 to 19 points. The score was externally validated in 19,321 patients from Hong Kong. The c indices for HCC in the development cohort were 0.83 (95% CI 0.81–0.84), 0.82 (95% CI 0.81–0.84), and 0.82 (95% CI 0.80–0.83) at the first, second, and third years of therapy, respectively. In the validation cohort, the c indices were 0.74 (95% CI 0.71–0.77), 0.75 (95% CI 0.73–0.78), and 0.75 (95% CI 0.72–0.77) during the first three years, and 0.76 (95% CI 0.74–0.78) and 0.76 (95% CI 0.74–0.77) in the extrapolated fourth and fifth years, respectively. The predicted and observed probabilities of HCC were calibrated in both cohorts. A score <8 and >13 points identified patients at distinctly low and high risks. The easily calculable CAMD score can predict HCC and may inform surveillance policy in patients with CHB during oral antiviral therapy. Prior to the current era of antiviral therapy, several scoring systems, such as CU-HCC, GAG-HCC, and REACH-B, have been built to predict the occurrence of HCC in the natural history of CHB.17–19Although these systems were externally validated and could attain a fairly good performance in untreated patients, they do not adequately predict HCC in patients on NAs.20–22Because long-term suppressive treatment with potent NA currently remains the therapeutic strategy of choice, there is a need for an accurate tool to stratify patients at different risks of HCC during antiviral treatment.Such knowledge is pivotal to inform clinical practice and to direct resource allocation.
Neutralising antibodies (NAb) play a key role in clearance of hepatitis C virus (HCV). NAbs have been isolated and mapped to several domains on the HCV Envelope proteins. However, the immunodominance of these epitopes in HCV infection remains unknown, hindering vaccine efforts to elicit optimal epitope-specific responses. Furthermore, it remains unclear which epitope-specific responses are associated with broad NAb (bNAb) activity in primary HCV infection. The aim of this study was to define B cell immunodominance in primary HCV, and its implications on neutralisation breadth and clearance. Using samples from 168 subjects with primary HCV infection, the antibody responses targeted two immunodominant domains, termed domains B and C. Genotype 1 and 3 infections were associated with responses targeted towards different bNAb domains. No epitopes were uniquely targeted by clearers versus those who developed chronic infection. Samples with bNAb activity were enriched for multi-specific responses directed towards epitopes AR3, AR4 and domain D, and did not target non-neutralising domains. This study outlines for the first time a clear NAb immunodominance profile in primary HCV infection, and indicates that it is influenced by the infecting virus. It also highlights the need for a vaccination strategy to induce multi-specific responses that do not target non-neutralising domains. Induction of neutralising antibodies is a major goal in viral vaccine design.For highly mutable RNA viruses such as human immunodeficiency virus (HIV), hepatitis C virus (HCV) and influenza, broadly neutralising antibodies (bNAb) are thought to be required if the vaccine is to protect against the diverse strains in circulation, as well as mutants generated within-host under immune pressure.Understanding the mechanisms underlying the development of bNAb in the context of these human viral infections is therefore essential for the rational design of bNAb-inducing vaccines [1].
Neutralising antibodies (NAb) play a key role in clearance of hepatitis C virus (HCV). NAbs have been isolated and mapped to several domains on the HCV Envelope proteins. However, the immunodominance of these epitopes in HCV infection remains unknown, hindering vaccine efforts to elicit optimal epitope-specific responses. Furthermore, it remains unclear which epitope-specific responses are associated with broad NAb (bNAb) activity in primary HCV infection. The aim of this study was to define B cell immunodominance in primary HCV, and its implications on neutralisation breadth and clearance. Using samples from 168 subjects with primary HCV infection, the antibody responses targeted two immunodominant domains, termed domains B and C. Genotype 1 and 3 infections were associated with responses targeted towards different bNAb domains. No epitopes were uniquely targeted by clearers versus those who developed chronic infection. Samples with bNAb activity were enriched for multi-specific responses directed towards epitopes AR3, AR4 and domain D, and did not target non-neutralising domains. This study outlines for the first time a clear NAb immunodominance profile in primary HCV infection, and indicates that it is influenced by the infecting virus. It also highlights the need for a vaccination strategy to induce multi-specific responses that do not target non-neutralising domains. Neutralising antibodies generated against HCV are primarily directed towards the surface Envelope glycoprotein E2 [2-8], but a few NAbs targeting E1, or the E1/E2 heterodimer have also been identified [9, 10].Extensive efforts to characterise the specificity of these NAbs have defined multiple epitopes within the surface proteins.The best characterised immunogenic epitopes are domains A-E, and antigenic regions (AR) 1-5.Residues that define these epitopes are usually determined by alanine scanning mutagenesis studies, which have revealed partial overlap between some sites such as domain B and AR3 (Table S1).Domains D and E (also termed epitope I) are defined by linear E2 peptides corresponding to sites 428-448 and 412-423, respectively, while the other domains are all defined by non-continuous sites.The neutralising activity of mAbs that bind these epitopes varies widely: domain A and AR1 specific antibodies have limited to no neutralising activity, while domain C and AR2 antibodies have neutralisation restricted to certain genotypes/isolates [5, 11].AR3-, AR4-, domain D and domain E-binding mAbs have particularly broad neutralisation activities across multiple genotypes [3, 10].
Neutralising antibodies (NAb) play a key role in clearance of hepatitis C virus (HCV). NAbs have been isolated and mapped to several domains on the HCV Envelope proteins. However, the immunodominance of these epitopes in HCV infection remains unknown, hindering vaccine efforts to elicit optimal epitope-specific responses. Furthermore, it remains unclear which epitope-specific responses are associated with broad NAb (bNAb) activity in primary HCV infection. The aim of this study was to define B cell immunodominance in primary HCV, and its implications on neutralisation breadth and clearance. Using samples from 168 subjects with primary HCV infection, the antibody responses targeted two immunodominant domains, termed domains B and C. Genotype 1 and 3 infections were associated with responses targeted towards different bNAb domains. No epitopes were uniquely targeted by clearers versus those who developed chronic infection. Samples with bNAb activity were enriched for multi-specific responses directed towards epitopes AR3, AR4 and domain D, and did not target non-neutralising domains. This study outlines for the first time a clear NAb immunodominance profile in primary HCV infection, and indicates that it is influenced by the infecting virus. It also highlights the need for a vaccination strategy to induce multi-specific responses that do not target non-neutralising domains. A major challenge in directing antibody responses towards these epitopes is that little is known about their immunodominance hierarchy in natural HCV infections.The domains of some of the most potent bNAbs were found to be rarely targeted in natural HCV infections [12], which suggests that such viral domains are naturally shielded from the immune system.This infers a difficult path towards the development of immunogens which elicit responses against these epitopes in a population-level vaccine strategy, despite their appealing neutralisation potential.The hypervariable region 1 (HVR1) within E2 forms an immunodominant epitope in HCV infection, however antibodies that target this region are highly strain-specific.Furthermore, HVR1 is prone to high levels of mutation, driving immune escape [13], making it unsuitable for vaccine design.Characterising the immunodominance hierarchy of epitopes with favourable neutralising potential is therefore required to understand what constitutes protective immunity to HCV.
Neutralising antibodies (NAb) play a key role in clearance of hepatitis C virus (HCV). NAbs have been isolated and mapped to several domains on the HCV Envelope proteins. However, the immunodominance of these epitopes in HCV infection remains unknown, hindering vaccine efforts to elicit optimal epitope-specific responses. Furthermore, it remains unclear which epitope-specific responses are associated with broad NAb (bNAb) activity in primary HCV infection. The aim of this study was to define B cell immunodominance in primary HCV, and its implications on neutralisation breadth and clearance. Using samples from 168 subjects with primary HCV infection, the antibody responses targeted two immunodominant domains, termed domains B and C. Genotype 1 and 3 infections were associated with responses targeted towards different bNAb domains. No epitopes were uniquely targeted by clearers versus those who developed chronic infection. Samples with bNAb activity were enriched for multi-specific responses directed towards epitopes AR3, AR4 and domain D, and did not target non-neutralising domains. This study outlines for the first time a clear NAb immunodominance profile in primary HCV infection, and indicates that it is influenced by the infecting virus. It also highlights the need for a vaccination strategy to induce multi-specific responses that do not target non-neutralising domains. Understanding the functional consequences of variations in the immunodominance hierarchy is essential for the rational design of vaccine candidates that generate protective immunity.While early induction of NAb responses is associated with HCV clearance [14, 15], it remains unclear whether these responses are the result of unique antibody specificities.Indeed, until recently [16, 17], no antibodies had been isolated from patients that cleared HCV infection, hindering such analyses.In addition, while the neutralising activity and breadth of mAbs with unique specificities have been analysed in vitro, it remains to be resolved which responses are associated with potent and broad neutralisation of HCV at the population level in natural infections.Furthermore, it is unknown whether mounting an immune response towards a single epitope is sufficient, or whether multi-specific antibody responses explain the bNAb activity observed in some patients.
Hepatocellular carcinoma (HCC) risk varies dramatically in patients with cirrhosis according to well-described, readily available predictors. We aimed to develop simple models estimating HCC risk in patients with alcohol-related liver disease (ALD)-cirrhosis or non-alcoholic fatty liver disease (NAFLD)-cirrhosis and calculate the net benefit that would be derived by implementing HCC surveillance strategies based on HCC risk as predicted by our models. We identified 7,068 patients with NAFLD-cirrhosis and 16,175 with ALD-cirrhosis who received care in the Veterans Affairs (VA) healthcare system in 2012. We retrospectively followed them for the development of incident HCC until January 2018. We used Cox proportional hazards regression to develop and internally validate models predicting HCC risk using baseline characteristics at entry into the cohort in 2012. We plotted decision curves of net benefit against HCC screening thresholds. We identified 1,278 incident cases of HCC during a mean follow-up period of 3.7 years. Mean annualized HCC incidence was 1.56% in NAFLD-cirrhosis and 1.44% in ALD-cirrhosis. The final models estimating HCC were developed separately for NAFLD-cirrhosis and ALD-cirrhosis and included 7 predictors: age, gender, diabetes, body mass index, platelet count, serum albumin and aspartate aminotransferase to √alanine aminotransferase ratio. The models exhibited very good measures of discrimination and calibration and an area under the receiver operating characteristic curve of 0.75 for NAFLD-cirrhosis and 0.76 for ALD-cirrhosis. Decision curves showed higher standardized net benefit of risk-based screening using our prediction models compared to the screen-all approach. We developed simple models estimating HCC risk in patients with NAFLD-cirrhosis or ALD-cirrhosis, which are available as web-based tools (www.hccrisk.com). Risk stratification can be used to inform risk-based HCC surveillance strategies in individual patients or healthcare systems or to identify high-risk patients for clinical trials. Annual HCC risk varies greatly in patients with cirrhosis ranging from as little as <0.2% to as high as >5%.Although this variability is well recognized, few models exist to estimate HCC risk in patients with cirrhosis and none are commonly used.Liver societies recommend the same screening strategy (abdominal ultrasonography every 6 months with or without concomitant serum alpha-fetoprotein [AFP]) irrespective of HCC risk.1–3Studies show poor compliance with these screening recommendations.4,5Stratification of HCC risk in patients with cirrhosis into low (e.g. <1% per year), medium (e.g. 1–3% per year) and high (e.g. >3% per year) would enable optimization and individualization of outreach efforts and screening strategies in patients with cirrhosis.It would also enable identification of high-risk patients for clinical trials of HCC screening.
Hepatocellular carcinoma (HCC) risk varies dramatically in patients with cirrhosis according to well-described, readily available predictors. We aimed to develop simple models estimating HCC risk in patients with alcohol-related liver disease (ALD)-cirrhosis or non-alcoholic fatty liver disease (NAFLD)-cirrhosis and calculate the net benefit that would be derived by implementing HCC surveillance strategies based on HCC risk as predicted by our models. We identified 7,068 patients with NAFLD-cirrhosis and 16,175 with ALD-cirrhosis who received care in the Veterans Affairs (VA) healthcare system in 2012. We retrospectively followed them for the development of incident HCC until January 2018. We used Cox proportional hazards regression to develop and internally validate models predicting HCC risk using baseline characteristics at entry into the cohort in 2012. We plotted decision curves of net benefit against HCC screening thresholds. We identified 1,278 incident cases of HCC during a mean follow-up period of 3.7 years. Mean annualized HCC incidence was 1.56% in NAFLD-cirrhosis and 1.44% in ALD-cirrhosis. The final models estimating HCC were developed separately for NAFLD-cirrhosis and ALD-cirrhosis and included 7 predictors: age, gender, diabetes, body mass index, platelet count, serum albumin and aspartate aminotransferase to √alanine aminotransferase ratio. The models exhibited very good measures of discrimination and calibration and an area under the receiver operating characteristic curve of 0.75 for NAFLD-cirrhosis and 0.76 for ALD-cirrhosis. Decision curves showed higher standardized net benefit of risk-based screening using our prediction models compared to the screen-all approach. We developed simple models estimating HCC risk in patients with NAFLD-cirrhosis or ALD-cirrhosis, which are available as web-based tools (www.hccrisk.com). Risk stratification can be used to inform risk-based HCC surveillance strategies in individual patients or healthcare systems or to identify high-risk patients for clinical trials. Many determinants of HCC risk in cirrhotic patients have already been described including routinely available clinical characteristics (e.g. etiology of cirrhosis, age, gender, race, body mass index, diabetes, HCV genotype) and serum laboratory tests (e.g. platelet count, albumin, aspartate aminotransferase [AST], alanine aminotransferase [ALT] and AFP).6These readily available predictors could potentially be combined into HCC risk estimation models with adequate discrimination and calibration.
Hepatocellular carcinoma (HCC) risk varies dramatically in patients with cirrhosis according to well-described, readily available predictors. We aimed to develop simple models estimating HCC risk in patients with alcohol-related liver disease (ALD)-cirrhosis or non-alcoholic fatty liver disease (NAFLD)-cirrhosis and calculate the net benefit that would be derived by implementing HCC surveillance strategies based on HCC risk as predicted by our models. We identified 7,068 patients with NAFLD-cirrhosis and 16,175 with ALD-cirrhosis who received care in the Veterans Affairs (VA) healthcare system in 2012. We retrospectively followed them for the development of incident HCC until January 2018. We used Cox proportional hazards regression to develop and internally validate models predicting HCC risk using baseline characteristics at entry into the cohort in 2012. We plotted decision curves of net benefit against HCC screening thresholds. We identified 1,278 incident cases of HCC during a mean follow-up period of 3.7 years. Mean annualized HCC incidence was 1.56% in NAFLD-cirrhosis and 1.44% in ALD-cirrhosis. The final models estimating HCC were developed separately for NAFLD-cirrhosis and ALD-cirrhosis and included 7 predictors: age, gender, diabetes, body mass index, platelet count, serum albumin and aspartate aminotransferase to √alanine aminotransferase ratio. The models exhibited very good measures of discrimination and calibration and an area under the receiver operating characteristic curve of 0.75 for NAFLD-cirrhosis and 0.76 for ALD-cirrhosis. Decision curves showed higher standardized net benefit of risk-based screening using our prediction models compared to the screen-all approach. We developed simple models estimating HCC risk in patients with NAFLD-cirrhosis or ALD-cirrhosis, which are available as web-based tools (www.hccrisk.com). Risk stratification can be used to inform risk-based HCC surveillance strategies in individual patients or healthcare systems or to identify high-risk patients for clinical trials. The incidence of HCC has been rising dramatically over time.7,8Therefore, HCC risk prediction models must be based on a recent cohort to avoid underestimating HCC risk.HCC risk is frequently calculated in patients with cirrhosis from the time they were first diagnosed with cirrhosis (i.e. as an “inception cohort”).However, this results in the diagnosis of many prevalent cases as well as incident cases in the early period after the inception of the cohort.For practicing clinicians, a more useful scenario would be to estimate HCC risk in a patient with cirrhosis when they are seen in clinic at some point during their natural history, not necessarily only when they first get diagnosed with cirrhosis.Also, HCC risk prediction models developed specifically to inform screening strategies, need to have a time horizon of approximately 5 years or less.This is counterintuitive since longer follow-up is usually considered better; however, HCCs destined to develop in 5 to 10 years from now are not going to be diagnosed by screening occurring “now” and therefore should not influence the prediction models.
Hepatocellular carcinoma (HCC) risk varies dramatically in patients with cirrhosis according to well-described, readily available predictors. We aimed to develop simple models estimating HCC risk in patients with alcohol-related liver disease (ALD)-cirrhosis or non-alcoholic fatty liver disease (NAFLD)-cirrhosis and calculate the net benefit that would be derived by implementing HCC surveillance strategies based on HCC risk as predicted by our models. We identified 7,068 patients with NAFLD-cirrhosis and 16,175 with ALD-cirrhosis who received care in the Veterans Affairs (VA) healthcare system in 2012. We retrospectively followed them for the development of incident HCC until January 2018. We used Cox proportional hazards regression to develop and internally validate models predicting HCC risk using baseline characteristics at entry into the cohort in 2012. We plotted decision curves of net benefit against HCC screening thresholds. We identified 1,278 incident cases of HCC during a mean follow-up period of 3.7 years. Mean annualized HCC incidence was 1.56% in NAFLD-cirrhosis and 1.44% in ALD-cirrhosis. The final models estimating HCC were developed separately for NAFLD-cirrhosis and ALD-cirrhosis and included 7 predictors: age, gender, diabetes, body mass index, platelet count, serum albumin and aspartate aminotransferase to √alanine aminotransferase ratio. The models exhibited very good measures of discrimination and calibration and an area under the receiver operating characteristic curve of 0.75 for NAFLD-cirrhosis and 0.76 for ALD-cirrhosis. Decision curves showed higher standardized net benefit of risk-based screening using our prediction models compared to the screen-all approach. We developed simple models estimating HCC risk in patients with NAFLD-cirrhosis or ALD-cirrhosis, which are available as web-based tools (www.hccrisk.com). Risk stratification can be used to inform risk-based HCC surveillance strategies in individual patients or healthcare systems or to identify high-risk patients for clinical trials. The 3 most common etiologies of cirrhosis in the United States, which also account for the majority of HCC cases, are hepatitis C virus (HCV), alcoholic liver disease (ALD) and non-alcoholic fatty liver disease (NAFLD).HCC risk prediction models need to be developed separately for different major etiologies of cirrhosis for a number of reasons.Firstly, HCC risk is greater in cirrhotic patients with HCV than those with other underlying liver diseases.6,9Secondly, certain predictors may be more important for some etiologies than others (e.g. obesity and diabetes may be more important in NAFLD-cirrhosis than in HCV-cirrhosis) while some predictors are unique to some etiologies (e.g. HCV genotype is unique to HCV-cirrhosis).Thirdly, direct-acting antiviral treatments have a dramatic effect on HCC risk in HCV-related cirrhosis, such that HCC risk has to be calculated specifically relative to the time of receipt of antiviral treatment.The prevalence of HCV-related cirrhosis is declining rapidly due to the decreasing prevalence of HCV since the year 2001 and the more recent dramatic increase in HCV cures after the introduction of direct-acting antiviral treatments.In contrast, the prevalence of cirrhosis related to NAFLD and ALD is increasing.7,10
Alterations of individual genes variably affect the development of hepatocellular carcinoma (HCC). Thus, we aimed to characterize the function of tumor-promoting genes in the context of gene regulatory networks (GRNs). Using data from The Cancer Genome Atlas, from the LIRI-JP (Liver Cancer – RIKEN, JP project), and from our transcriptomic, transfection and mouse transgenic experiments, we identify a GRN which functionally links LIN28B-dependent dedifferentiation with dysfunction of β-catenin (CTNNB1). We further generated and validated a quantitative mathematical model of the GRN using human cell lines and in vivo expression data. We found that LIN28B and CTNNB1 form a GRN with SMARCA4, Let-7b (MIRLET7B), SOX9, TP53 and MYC. GRN functionality is detected in HCC and gastrointestinal cancers, but not in other cancer types. GRN status negatively correlates with HCC prognosis, and positively correlates with hyperproliferation, dedifferentiation and HGF/MET pathway activation, suggesting that it contributes to a transcriptomic profile typical of the proliferative class of HCC. The mathematical model predicts how the expression of GRN components changes when the expression of another GRN member varies or is inhibited by a pharmacological drug. The dynamics of GRN component expression reveal distinct cell states that can switch reversibly in normal conditions, and irreversibly in HCC. The mathematical model is available via a web-based tool which can evaluate the GRN status of HCC samples and predict the impact of therapeutic agents on the GRN. We conclude that identification and modelling of the GRN provide insights into the prognosis of HCC and the mechanisms by which tumor-promoting genes impact on HCC development. Various etiologies are associated with HCC, leading to heterogeneity in clinical outcome, histology, transcriptomic profile and mutational spectrum.1–5Such heterogeneity causes a variable response to therapeutic agents, as in mouse models with Ctnnb1-induced HCCs which show heterogeneous sensitivity to CTNNB1 inhibitors.6Thus, designing novel therapeutic strategies against HCC requires the identification of inhibitors of individual tumor-promoting genes and also the characterization of the molecular networks in which those genes exert their functions.
Alterations of individual genes variably affect the development of hepatocellular carcinoma (HCC). Thus, we aimed to characterize the function of tumor-promoting genes in the context of gene regulatory networks (GRNs). Using data from The Cancer Genome Atlas, from the LIRI-JP (Liver Cancer – RIKEN, JP project), and from our transcriptomic, transfection and mouse transgenic experiments, we identify a GRN which functionally links LIN28B-dependent dedifferentiation with dysfunction of β-catenin (CTNNB1). We further generated and validated a quantitative mathematical model of the GRN using human cell lines and in vivo expression data. We found that LIN28B and CTNNB1 form a GRN with SMARCA4, Let-7b (MIRLET7B), SOX9, TP53 and MYC. GRN functionality is detected in HCC and gastrointestinal cancers, but not in other cancer types. GRN status negatively correlates with HCC prognosis, and positively correlates with hyperproliferation, dedifferentiation and HGF/MET pathway activation, suggesting that it contributes to a transcriptomic profile typical of the proliferative class of HCC. The mathematical model predicts how the expression of GRN components changes when the expression of another GRN member varies or is inhibited by a pharmacological drug. The dynamics of GRN component expression reveal distinct cell states that can switch reversibly in normal conditions, and irreversibly in HCC. The mathematical model is available via a web-based tool which can evaluate the GRN status of HCC samples and predict the impact of therapeutic agents on the GRN. We conclude that identification and modelling of the GRN provide insights into the prognosis of HCC and the mechanisms by which tumor-promoting genes impact on HCC development. Dedifferentiation of hepatic cells contributes to HCC progression.7–9In this context, poorly differentiated HCC develop as a result of forced induction of LIN28B, an RNA-binding protein which is repressed during normal hepatic cell differentiation.LIN28B is re-expressed in a subset of human HCCs characterized by high serum levels of α-fetoprotein,10,11 thereby associating dedifferentiation, HCC progression and LIN28B expression.12In parallel, CTNNB1 is one of the most frequently mutated genes in HCC.13Therefore, we explore the possibility that HCC progression depends on a gene regulatory network (GRN) linking LIN28B-dependent dedifferentiation with CTNNB1 dysfunction.
The nuclear farnesoid X receptor (FXR) agonist obeticholic acid (OCA) has been developed for the treatment of liver diseases. We aimed to determine whether OCA treatment increases the risk of gallstone formation. Twenty patients awaiting laparoscopic cholecystectomy were randomized to treatment with OCA (25 mg/day) or placebo for 3 weeks until the day before surgery. Serum bile acids (BAs), the BA synthesis marker C4 (7α-hydroxy-4-cholesten-3-one), and fibroblast growth factor 19 (FGF19) were measured before and after treatment. During surgery, biopsies from the liver and the whole bile-filled gallbladder were collected for analyses of gene expression, biliary lipids and FGF19. In serum, OCA increased FGF19 (from 95.0 ± 8.5 to 234.4 ± 35.6 ng/L) and decreased C4 (from 31.4 ± 22.8 to 2.8 ± 4.0 nmol/L) and endogenous BAs (from 1,312.2 ± 236.2 to 517.7 ± 178.9 nmol/L; all p <0.05). At surgery, BAs in gallbladder bile were lower in patients that received OCA than in controls (OCA, 77.9 ± 53.6 mmol/L; placebo, 196.4 ± 99.3 mmol/L; p <0.01), resulting in a higher cholesterol saturation index (OCA, 2.8 ± 1.1; placebo, 1.8 ± 0.8; p <0.05). In addition, hydrophobic OCA conjugates accounted for 13.6 ± 5.0% of gallbladder BAs after OCA treatment, resulting in a higher hydrophobicity index (OCA, 0.43 ± 0.09; placebo, 0.34 ± 0.07, p <0.05). Gallbladder FGF19 levels were 3-fold higher in OCA patients than in controls (OCA, 40.3 ± 16.5 ng/L; placebo, 13.5 ± 13.1 ng/ml; p <0.005). Gene expression analysis indicated that FGF19 mainly originated from the gallbladder epithelium. Our results show for the first time an enrichment of FGF19 in human bile after OCA treatment. In accordance with its murine homolog FGF15, FGF19 might trigger relaxation and filling of the gallbladder which, in combination with increased cholesterol saturation and BA hydrophobicity, would enhance the risk of gallstone development. Bile acids (BAs) are amphipathic molecules that are synthesized from cholesterol in the liver.Once synthesized, they are conjugated with glycine or taurine and then excreted with bile into the small bowel from where about 95% are reabsorbed in the terminal ileum via the enterohepatic circulation.In addition to their detergent properties that aid lipid digestion, BAs serve as signaling molecules by activating various nuclear and membrane-bound receptors, in particular the nuclear farnesoid X receptor (FXR), which regulates BA, glucose and lipid metabolism.1,2BA homeostasis is maintained through negative feedback activation of FXR both within the liver (via SHP-LRH-1/HNF-4α) and from the small intestine (via FGF19-FGFR4/ß-klotho) by decreasing the expression of cholesterol 7α-hydroxylase (CYP7A1), the key enzyme in BA synthesis.3,4
The nuclear farnesoid X receptor (FXR) agonist obeticholic acid (OCA) has been developed for the treatment of liver diseases. We aimed to determine whether OCA treatment increases the risk of gallstone formation. Twenty patients awaiting laparoscopic cholecystectomy were randomized to treatment with OCA (25 mg/day) or placebo for 3 weeks until the day before surgery. Serum bile acids (BAs), the BA synthesis marker C4 (7α-hydroxy-4-cholesten-3-one), and fibroblast growth factor 19 (FGF19) were measured before and after treatment. During surgery, biopsies from the liver and the whole bile-filled gallbladder were collected for analyses of gene expression, biliary lipids and FGF19. In serum, OCA increased FGF19 (from 95.0 ± 8.5 to 234.4 ± 35.6 ng/L) and decreased C4 (from 31.4 ± 22.8 to 2.8 ± 4.0 nmol/L) and endogenous BAs (from 1,312.2 ± 236.2 to 517.7 ± 178.9 nmol/L; all p <0.05). At surgery, BAs in gallbladder bile were lower in patients that received OCA than in controls (OCA, 77.9 ± 53.6 mmol/L; placebo, 196.4 ± 99.3 mmol/L; p <0.01), resulting in a higher cholesterol saturation index (OCA, 2.8 ± 1.1; placebo, 1.8 ± 0.8; p <0.05). In addition, hydrophobic OCA conjugates accounted for 13.6 ± 5.0% of gallbladder BAs after OCA treatment, resulting in a higher hydrophobicity index (OCA, 0.43 ± 0.09; placebo, 0.34 ± 0.07, p <0.05). Gallbladder FGF19 levels were 3-fold higher in OCA patients than in controls (OCA, 40.3 ± 16.5 ng/L; placebo, 13.5 ± 13.1 ng/ml; p <0.005). Gene expression analysis indicated that FGF19 mainly originated from the gallbladder epithelium. Our results show for the first time an enrichment of FGF19 in human bile after OCA treatment. In accordance with its murine homolog FGF15, FGF19 might trigger relaxation and filling of the gallbladder which, in combination with increased cholesterol saturation and BA hydrophobicity, would enhance the risk of gallstone development. The potent FXR agonist obeticholic acid (OCA, 6α-ethyl-chenodeoxycholic acid) has been developed for the treatment of various cholestatic and metabolic liver diseases such as primary biliary cholangitis (PBC) and non-alcoholic steatohepatitis (NASH).5,6Currently, OCA is provisionally approved as a second-line treatment option in PBC.It is important to note that the downstream effects of FXR activation in humans are largely unknown.Almost all studies exploring these mechanisms have been conducted in murine models; however, because of substantial differences in BA metabolism and BA profiles between mice and humans, it is difficult to translate findings from these models into humans.1,2
No-touch multibipolar radiofrequency ablation (NTM-RFA) represents a novel therapy that surpasses standard RFA for hepatocellular carcinoma (HCC), but it has not been compared to surgical resection (SR). We aimed to compare the outcomes of NTM-RFA and SR for intermediate-sized HCC. Between 2012 and 2016, 141 patients with solitary HCC ranging from 2 to 5 cm were treated by NTM-RFA or SR at a single-center. The outcomes of 128 patients were compared after using inverse probability of treatment weighting (IPTW). Seventy-nine patients had NTM-RFA and 62 had SR. After IPTW, the two groups were well-balanced for most baseline characteristics including tumor size, location, etiology, severity of underlying liver disease and alpha-fetoprotein level. Morbidity was higher (67.9% vs. 50.0%, p = 0.042) and hospital stay was longer (12 [IQR 8–13] vs. 7 [IQR 5–9] days, p <0.001) after SR. Local recurrence rates at one and three years were 5.5% and 10.0% after NTM-RFA and 1.9% and 1.9% after SR, respectively (p = 0.065). The rates of systematized recurrence (within the treated segment or in an adjacent segment within a 2 cm distance from treatment site) were higher after NTM-RFA (7.4% vs. 1.9% at one year, 27.8% vs. 3.3% at three years, p = 0.008). Most patients with recurrence were eligible for rescue treatment, resulting in similar overall survival (86.7% after NTM-RFA, 91.4% after SR at three years, p = 0.954) and disease-free survival (40.8% after NTM-RFA, 56.4% after SR at three years, p = 0.119). Compared to SR, NTM-RFA for solitary intermediate-sized HCC was associated with less morbidity and more systematized recurrence, while the rate of local recurrence was not significantly different. Most patients with intrahepatic recurrence remained eligible for rescue therapies, resulting in equivalent long-term oncological results after both treatments. Surgical resection (SR) and radiofrequency ablation (RFA) represent two treatments of choice for the curative management of early hepatocellular carcinoma (HCC) in patients not eligible for or awaiting liver transplantation.1–4In the past, several studies mainly from Eastern centers suggested that RFA was as effective as SR for solitary HCCs of less than 2 to 3 cm.Furthermore, it is associated with less procedure-related morbidity,5–8 resulting in better cost-effectiveness9 and quality of life.10However, for larger lesions, SR appears to be superior to the various existing modalities of thermoablation, such as radiofrequency or microwave ablation in terms of local control and disease-free survival.8,11–14Therefore, SR is currently considered the best therapeutic option in patients with intermediate-sized HCC ranging from 2 to 5 cm, with no contraindication for surgery.1,3,4,15
No-touch multibipolar radiofrequency ablation (NTM-RFA) represents a novel therapy that surpasses standard RFA for hepatocellular carcinoma (HCC), but it has not been compared to surgical resection (SR). We aimed to compare the outcomes of NTM-RFA and SR for intermediate-sized HCC. Between 2012 and 2016, 141 patients with solitary HCC ranging from 2 to 5 cm were treated by NTM-RFA or SR at a single-center. The outcomes of 128 patients were compared after using inverse probability of treatment weighting (IPTW). Seventy-nine patients had NTM-RFA and 62 had SR. After IPTW, the two groups were well-balanced for most baseline characteristics including tumor size, location, etiology, severity of underlying liver disease and alpha-fetoprotein level. Morbidity was higher (67.9% vs. 50.0%, p = 0.042) and hospital stay was longer (12 [IQR 8–13] vs. 7 [IQR 5–9] days, p <0.001) after SR. Local recurrence rates at one and three years were 5.5% and 10.0% after NTM-RFA and 1.9% and 1.9% after SR, respectively (p = 0.065). The rates of systematized recurrence (within the treated segment or in an adjacent segment within a 2 cm distance from treatment site) were higher after NTM-RFA (7.4% vs. 1.9% at one year, 27.8% vs. 3.3% at three years, p = 0.008). Most patients with recurrence were eligible for rescue treatment, resulting in similar overall survival (86.7% after NTM-RFA, 91.4% after SR at three years, p = 0.954) and disease-free survival (40.8% after NTM-RFA, 56.4% after SR at three years, p = 0.119). Compared to SR, NTM-RFA for solitary intermediate-sized HCC was associated with less morbidity and more systematized recurrence, while the rate of local recurrence was not significantly different. Most patients with intrahepatic recurrence remained eligible for rescue therapies, resulting in equivalent long-term oncological results after both treatments. No-touch multibipolar RFA (NTM-RFA) represents a novel technique of local ablation, which consists of the delivery of high-intensity energy between pairs of electrodes placed around the targeted tumor, enabling treatment of large lesions with safety margins according to the “no-touch” principle.16,17It was recently reported as a more effective ablation technique than standard monopolar RFA in terms of histological response and local control, both for small (<3 cm) and large HCC.18,19Thus, it may represent an alternative to SR for larger HCCs of up to 5 cm, but no comparative study between NTM-RFA and SR has yet been performed.20
Tertiary lymphoid structures (TLSs) provide a local and critical microenvironment for generating anti-tumor cellular and humoral immune responses. TLSs are associated with improved clinical outcomes in most solid tumors investigated to date. However, their role in hepatocellular carcinoma (HCC) is debated, as they have recently been shown to promote the growth of malignant hepatocyte progenitors in the non-tumoral liver. We aimed to determine, by pathological review, the prognostic significance of both intra-tumoral and non-tumoral TLSs in a series of 273 patients with HCC treated by surgical resection in Henri Mondor University Hospital. Findings were further validated by gene expression profiling using a public data set (LCI cohort). TLSs were identified in 47% of the tumors, by pathological review, with lymphoid aggregates, primary and secondary follicles in 26%, 16% and 5% of the cases, respectively. Univariate and multivariate analyses showed that intra-tumoral TLSs significantly correlated with a lower risk of early relapse (<2 years after surgery, hazard ratio 0.46, p = 0.005). Interestingly, the risk of recurrence was also related to the degree of TLS maturation (primary or secondary follicles vs. lymphoid aggregates, p = 0.01). A gene expression signature associated with the presence of intra-tumoral TLS was also independently associated with a lower risk of early relapse in the LCI cohort. No association between the density of TLSs located in the adjacent non-tumoral liver and early or late recurrence was observed. We have shown that intra-tumoral TLSs are associated with a lower risk of early relapse in 2 independent cohorts of patients with HCC treated by surgical resection. Thus, intra-tumoral TLSs may reflect the existence of ongoing, effective anti-tumor immunity. Hepatocellular carcinoma (HCC) is the fifth most frequent cancer worldwide and the second leading cause of cancer-related deaths.1The vast majority of cases develop in patients with chronic liver diseases, the main risk factors being hepatitis B virus (HBV) and hepatitis C virus (HCV) infection, alcohol intake and metabolic syndrome.2,3Clinical outcomes remain poor, with only around one-third of patients eligible for potentially curative treatments such as surgical resection, radiofrequency ablation or liver transplantation.3The standard of care for advanced cases is the multikinase inhibitor sorafenib, which unfortunately has limited survival benefit.4
Tertiary lymphoid structures (TLSs) provide a local and critical microenvironment for generating anti-tumor cellular and humoral immune responses. TLSs are associated with improved clinical outcomes in most solid tumors investigated to date. However, their role in hepatocellular carcinoma (HCC) is debated, as they have recently been shown to promote the growth of malignant hepatocyte progenitors in the non-tumoral liver. We aimed to determine, by pathological review, the prognostic significance of both intra-tumoral and non-tumoral TLSs in a series of 273 patients with HCC treated by surgical resection in Henri Mondor University Hospital. Findings were further validated by gene expression profiling using a public data set (LCI cohort). TLSs were identified in 47% of the tumors, by pathological review, with lymphoid aggregates, primary and secondary follicles in 26%, 16% and 5% of the cases, respectively. Univariate and multivariate analyses showed that intra-tumoral TLSs significantly correlated with a lower risk of early relapse (<2 years after surgery, hazard ratio 0.46, p = 0.005). Interestingly, the risk of recurrence was also related to the degree of TLS maturation (primary or secondary follicles vs. lymphoid aggregates, p = 0.01). A gene expression signature associated with the presence of intra-tumoral TLS was also independently associated with a lower risk of early relapse in the LCI cohort. No association between the density of TLSs located in the adjacent non-tumoral liver and early or late recurrence was observed. We have shown that intra-tumoral TLSs are associated with a lower risk of early relapse in 2 independent cohorts of patients with HCC treated by surgical resection. Thus, intra-tumoral TLSs may reflect the existence of ongoing, effective anti-tumor immunity. The recent success of immunotherapy in various solid or hematological malignancies underscores the need to better understand the mechanisms that lead to effective anti-tumoral responses.5–7Intra-tumoral infiltration by cytotoxic CD8+ lymphocytes has been extensively investigated and is associated with improved survival in most tumors.7More recently, other studies have focused on the significance of tertiary lymphoid structures (TLSs).8–10TLSs are classically defined as lymphoid aggregates forming in non-hematopoietic organs in response to chronic and non-resolving inflammatory processes, such as infection, graft rejection, autoimmune disease and cancer.8They are thought to play a critical role in the anti-tumor specific immune responses by allowing the generation of effector and central memory T cells and plasma cells.Their presence is correlated with a reduced risk of recurrence and improved survival in virtually all solid tumors.7
Tertiary lymphoid structures (TLSs) provide a local and critical microenvironment for generating anti-tumor cellular and humoral immune responses. TLSs are associated with improved clinical outcomes in most solid tumors investigated to date. However, their role in hepatocellular carcinoma (HCC) is debated, as they have recently been shown to promote the growth of malignant hepatocyte progenitors in the non-tumoral liver. We aimed to determine, by pathological review, the prognostic significance of both intra-tumoral and non-tumoral TLSs in a series of 273 patients with HCC treated by surgical resection in Henri Mondor University Hospital. Findings were further validated by gene expression profiling using a public data set (LCI cohort). TLSs were identified in 47% of the tumors, by pathological review, with lymphoid aggregates, primary and secondary follicles in 26%, 16% and 5% of the cases, respectively. Univariate and multivariate analyses showed that intra-tumoral TLSs significantly correlated with a lower risk of early relapse (<2 years after surgery, hazard ratio 0.46, p = 0.005). Interestingly, the risk of recurrence was also related to the degree of TLS maturation (primary or secondary follicles vs. lymphoid aggregates, p = 0.01). A gene expression signature associated with the presence of intra-tumoral TLS was also independently associated with a lower risk of early relapse in the LCI cohort. No association between the density of TLSs located in the adjacent non-tumoral liver and early or late recurrence was observed. We have shown that intra-tumoral TLSs are associated with a lower risk of early relapse in 2 independent cohorts of patients with HCC treated by surgical resection. Thus, intra-tumoral TLSs may reflect the existence of ongoing, effective anti-tumor immunity. Their role in HCC, an inflammation-driven cancer, is however debated.Finkin et al. observed that the existence of TLSs in non-tumoral liver correlated with an increased risk for late recurrence and a trend toward decreased overall survival after surgical resection of HCC.11Moreover, using a mouse model of chronic NF-κB activation, they showed that TLSs constituted immunopathological microniches that favored the growth of malignant hepatocyte progenitors through the production of pro-tumoral cytokines.11Although this elegant work challenged the current dogma that TLSs coordinate anti-tumoral responses, the significance of intra-tumoral TLSs was not investigated and thus remains to be determined.
Non-alcoholic fatty liver disease (NAFLD) and non-alcoholic steatohepatitis (NASH) are increasingly a cause of cirrhosis and hepatocellular carcinoma globally. This burden is expected to increase as epidemics of obesity, diabetes and metabolic syndrome continue to grow. The goal of this analysis was to use a Markov model to forecast NAFLD disease burden using currently available data. A model was used to estimate NAFLD and NASH disease progression in eight countries based on data for adult prevalence of obesity and type 2 diabetes mellitus (DM). Published estimates and expert consensus were used to build and validate the model projections. If obesity and DM level off in the future, we project a modest growth in total NAFLD cases (0–30%), between 2016–2030, with the highest growth in China as a result of urbanization and the lowest growth in Japan as a result of a shrinking population. However, at the same time, NASH prevalence will increase 15–56%, while liver mortality and advanced liver disease will more than double as a result of an aging/increasing population. NAFLD and NASH represent a large and growing public health problem and efforts to understand this epidemic and to mitigate the disease burden are needed. If obesity and DM continue to increase at current and historical rates, both NAFLD and NASH prevalence are expected to increase. Since both are reversible, public health campaigns to increase awareness and diagnosis, and to promote diet and exercise can help manage the growth in future disease burden. Non-alcoholic fatty liver disease (NAFLD) is a leading cause of liver disease globally.1–3This condition is characterized by excess liver fat in the absence of other causes such as alcohol consumption.4,5Obesity, type 2 diabetes mellitus (DM) and metabolic syndrome are consistently identified as the most important risk factors for NAFLD.4,6
Non-alcoholic fatty liver disease (NAFLD) and non-alcoholic steatohepatitis (NASH) are increasingly a cause of cirrhosis and hepatocellular carcinoma globally. This burden is expected to increase as epidemics of obesity, diabetes and metabolic syndrome continue to grow. The goal of this analysis was to use a Markov model to forecast NAFLD disease burden using currently available data. A model was used to estimate NAFLD and NASH disease progression in eight countries based on data for adult prevalence of obesity and type 2 diabetes mellitus (DM). Published estimates and expert consensus were used to build and validate the model projections. If obesity and DM level off in the future, we project a modest growth in total NAFLD cases (0–30%), between 2016–2030, with the highest growth in China as a result of urbanization and the lowest growth in Japan as a result of a shrinking population. However, at the same time, NASH prevalence will increase 15–56%, while liver mortality and advanced liver disease will more than double as a result of an aging/increasing population. NAFLD and NASH represent a large and growing public health problem and efforts to understand this epidemic and to mitigate the disease burden are needed. If obesity and DM continue to increase at current and historical rates, both NAFLD and NASH prevalence are expected to increase. Since both are reversible, public health campaigns to increase awareness and diagnosis, and to promote diet and exercise can help manage the growth in future disease burden. In order to classify the population, NAFLD may be divided into two groups: NAFL (steatosis only) or non-alcoholic steatohepatitis (NASH), where steatosis is accompanied by inflammation and ballooning.NASH frequently progresses to liver fibrosis,7 which is the main risk factor for liver-related mortality. 8Odds of progression to advanced liver disease, including hepatic decompensation and hepatocellular carcinoma (HCC), are higher among those with NASH compared to those with NAFL.7Increasing age, obesity, DM and the presence of NASH have been consistently identified as risk factors for progression to cirrhosis.6,9
HCV subtypes which are unusual in Europe are more prevalent in the African region, but little is known of their response to direct-acting antivirals (DAAs). These include non-1a/1b/ non-subtypeable genotype 1 (G1) or non-4a/4d (G4). In this report we aimed to describe the genotype distribution and treatment outcome in a south London cohort of African patients. We identified all patients born in Africa who attended our clinic from 2010-2018. Information on HCV genotype, treatment regimen and outcome were obtained. Non-subtypeable samples were analysed using Glasgow NimbleGen next-generation sequencing (NGS). Phylogenetic analysis was carried out by generating an uncorrected nucleotide p-distance tree from the complete coding regions of our sequences. Of 91 African patients, 47 (52%) were infected with an unusual subtype. Fourteen novel, as yet undesignated subtypes (G1*), were identified by NGS. Three individuals were infected with the same subtype, now designated as subtype 1p. Baseline sequences were available for 22 patients; 18/22 (82%) had baseline NS5A resistance-associated substitutions (RASs). Sustained virological response (SVR) was achieved in 56/63 (89%) overall, yet only in 21/28 (75%) of those with unusual G1 subtypes, with failure in 3/16 G1*, 1/2 G1p and 3/3 in G1l. Six treatment failures occurred with sofosbuvir/ledipasvir compared to 1 failure on a PI-based regimen. The SVR rate for all other genotypes and subtypes was 35/35 (100%). Most individuals in an unselected cohort of African patients were infected with an unusual genotype, including novel subtype 1p. The SVR rate of those with unusual G1 subtypes was 75%, raising concern about expansion of DAAs across Africa. Depending on the regimen used, higher failure rates in African cohorts could jeopardise HCV elimination. Direct-acting antiviral therapy (DAA) therapy has revolutionised hepatitis C (HCV) treatment.High cure rates with short courses of treatment make global eradication of HCV feasible; consequently, the World Health Organisation has promulgated a call for elimination of viral hepatitis as a public health threat by 2030.1
HCV subtypes which are unusual in Europe are more prevalent in the African region, but little is known of their response to direct-acting antivirals (DAAs). These include non-1a/1b/ non-subtypeable genotype 1 (G1) or non-4a/4d (G4). In this report we aimed to describe the genotype distribution and treatment outcome in a south London cohort of African patients. We identified all patients born in Africa who attended our clinic from 2010-2018. Information on HCV genotype, treatment regimen and outcome were obtained. Non-subtypeable samples were analysed using Glasgow NimbleGen next-generation sequencing (NGS). Phylogenetic analysis was carried out by generating an uncorrected nucleotide p-distance tree from the complete coding regions of our sequences. Of 91 African patients, 47 (52%) were infected with an unusual subtype. Fourteen novel, as yet undesignated subtypes (G1*), were identified by NGS. Three individuals were infected with the same subtype, now designated as subtype 1p. Baseline sequences were available for 22 patients; 18/22 (82%) had baseline NS5A resistance-associated substitutions (RASs). Sustained virological response (SVR) was achieved in 56/63 (89%) overall, yet only in 21/28 (75%) of those with unusual G1 subtypes, with failure in 3/16 G1*, 1/2 G1p and 3/3 in G1l. Six treatment failures occurred with sofosbuvir/ledipasvir compared to 1 failure on a PI-based regimen. The SVR rate for all other genotypes and subtypes was 35/35 (100%). Most individuals in an unselected cohort of African patients were infected with an unusual genotype, including novel subtype 1p. The SVR rate of those with unusual G1 subtypes was 75%, raising concern about expansion of DAAs across Africa. Depending on the regimen used, higher failure rates in African cohorts could jeopardise HCV elimination. With 11 million people infected, HCV has an estimated prevalence of 1% in the African region.1Despite this, there have been few clinical trials conducted in African cohorts and data is lacking on the prevalence, geographical distribution and treatment response of African sub-genotypes.2,3
HCV subtypes which are unusual in Europe are more prevalent in the African region, but little is known of their response to direct-acting antivirals (DAAs). These include non-1a/1b/ non-subtypeable genotype 1 (G1) or non-4a/4d (G4). In this report we aimed to describe the genotype distribution and treatment outcome in a south London cohort of African patients. We identified all patients born in Africa who attended our clinic from 2010-2018. Information on HCV genotype, treatment regimen and outcome were obtained. Non-subtypeable samples were analysed using Glasgow NimbleGen next-generation sequencing (NGS). Phylogenetic analysis was carried out by generating an uncorrected nucleotide p-distance tree from the complete coding regions of our sequences. Of 91 African patients, 47 (52%) were infected with an unusual subtype. Fourteen novel, as yet undesignated subtypes (G1*), were identified by NGS. Three individuals were infected with the same subtype, now designated as subtype 1p. Baseline sequences were available for 22 patients; 18/22 (82%) had baseline NS5A resistance-associated substitutions (RASs). Sustained virological response (SVR) was achieved in 56/63 (89%) overall, yet only in 21/28 (75%) of those with unusual G1 subtypes, with failure in 3/16 G1*, 1/2 G1p and 3/3 in G1l. Six treatment failures occurred with sofosbuvir/ledipasvir compared to 1 failure on a PI-based regimen. The SVR rate for all other genotypes and subtypes was 35/35 (100%). Most individuals in an unselected cohort of African patients were infected with an unusual genotype, including novel subtype 1p. The SVR rate of those with unusual G1 subtypes was 75%, raising concern about expansion of DAAs across Africa. Depending on the regimen used, higher failure rates in African cohorts could jeopardise HCV elimination. There are 8 known HCV genotypes which have been classified based on the analysis of HCV genetic sequences.4Except for genotypes 5 and 8, each genotype is further divided into a number of subtypes.A genome-wide nucleotide sequence difference of 31–33% is considered sufficient to differentiate a genotype and a difference of 12–15% is sufficient to distinguish a sub-genotype (or subtype), although these boundaries are not strict and phylogeny is also considered.
HCV subtypes which are unusual in Europe are more prevalent in the African region, but little is known of their response to direct-acting antivirals (DAAs). These include non-1a/1b/ non-subtypeable genotype 1 (G1) or non-4a/4d (G4). In this report we aimed to describe the genotype distribution and treatment outcome in a south London cohort of African patients. We identified all patients born in Africa who attended our clinic from 2010-2018. Information on HCV genotype, treatment regimen and outcome were obtained. Non-subtypeable samples were analysed using Glasgow NimbleGen next-generation sequencing (NGS). Phylogenetic analysis was carried out by generating an uncorrected nucleotide p-distance tree from the complete coding regions of our sequences. Of 91 African patients, 47 (52%) were infected with an unusual subtype. Fourteen novel, as yet undesignated subtypes (G1*), were identified by NGS. Three individuals were infected with the same subtype, now designated as subtype 1p. Baseline sequences were available for 22 patients; 18/22 (82%) had baseline NS5A resistance-associated substitutions (RASs). Sustained virological response (SVR) was achieved in 56/63 (89%) overall, yet only in 21/28 (75%) of those with unusual G1 subtypes, with failure in 3/16 G1*, 1/2 G1p and 3/3 in G1l. Six treatment failures occurred with sofosbuvir/ledipasvir compared to 1 failure on a PI-based regimen. The SVR rate for all other genotypes and subtypes was 35/35 (100%). Most individuals in an unselected cohort of African patients were infected with an unusual genotype, including novel subtype 1p. The SVR rate of those with unusual G1 subtypes was 75%, raising concern about expansion of DAAs across Africa. Depending on the regimen used, higher failure rates in African cohorts could jeopardise HCV elimination. Clinical trial and real-life data routinely report sustained virological response (SVR) rates in excess of 95% for genotype 1a, (G1a) 1b, 3 and 4.There are fewer data available on less prevalent genotypes such as 5 or 6, nor are unusual subtypes well represented.In an analysis of over 1,700 patients with genotype 1 HCV, who had been enrolled in clinical trials of NS5A inhibitors, less than 1% had subtypes of genotype 1 other than 1a or 1b.5The available evidence suggests that other genotype 1 subtypes are common in Africa but less frequent in the industrialised countries where clinical trials have been centred.6,16In this paper we refer to these as “unusual genotypes”.We choose this nomenclature for clarity as these genotypes are unusual in Europe, but not unusual in Africa, as we will discuss.However, there is a paucity of data as to whether the treatment response in patients with these less well characterised subtypes is comparable to more common subtypes.This has led to calls for more treatment outcome data in well characterised cohorts of patients.7,8
Death rates on liver transplant waiting lists range from 5%–25%. Herein, we report a unique experience with 50 anonymous individuals who volunteered to address this gap by offering to donate part of their liver to a recipient with whom they had no biological connection or prior relationship, so called anonymous live liver donation (A-LLD). Candidates were screened to confirm excellent physical, mental, social, and financial health. Demographics and surgical outcomes were analyzed. Qualitative interviews after donation examined motivation and experiences. Validated self-reported questionnaires assessed personality traits and psychological impact. A total of 50 A-LLD liver transplants were performed between 2005 and 2017. Most donors had a university education, a middle-class income, and a history of prior altruism. Half were women. Median age was 38.5 years (range 20–59). Thirty-three (70%) learned about this opportunity through public or social media. Saving a life, helping others, generativity, and reciprocity for past generosity were motivators. Social, financial, healthcare, and legal support in Canada were identified as facilitators. A-LLD identified most with the personality traits of agreeableness and conscientiousness. The median hospital stay was 6 days. One donor experienced a Dindo-Clavien Grade 3 complication that completely resolved. One-year recipient survival was 91% in 22 adults and 97% in 28 children. No A-LLD reported regretting their decision. This is the first and only report of the characteristics, motivations and facilitators of A-LLD in a large cohort. With rigorous protocols, outcomes are excellent. A-LLD has significant potential to reduce the gap between transplant organ demand and availability. Death rates on liver transplant (LT) waiting lists in the Western world range from 5-25%.1–4This is disheartening since most LT recipients now survive for decades with good health and near normal quality of life.1,5,6In selected locations, live liver donation (LLD) has been used to mitigate the shortage of deceased donor livers with excellent recipient outcomes.LLD is associated with a 30% morbidity rate and an estimated 0.3% donor mortality risk.5,7–11,12Our program and others have confirmed that donors with biological relationships or close emotional bonds with the recipient have few regrets and good outcomes.8,13
Death rates on liver transplant waiting lists range from 5%–25%. Herein, we report a unique experience with 50 anonymous individuals who volunteered to address this gap by offering to donate part of their liver to a recipient with whom they had no biological connection or prior relationship, so called anonymous live liver donation (A-LLD). Candidates were screened to confirm excellent physical, mental, social, and financial health. Demographics and surgical outcomes were analyzed. Qualitative interviews after donation examined motivation and experiences. Validated self-reported questionnaires assessed personality traits and psychological impact. A total of 50 A-LLD liver transplants were performed between 2005 and 2017. Most donors had a university education, a middle-class income, and a history of prior altruism. Half were women. Median age was 38.5 years (range 20–59). Thirty-three (70%) learned about this opportunity through public or social media. Saving a life, helping others, generativity, and reciprocity for past generosity were motivators. Social, financial, healthcare, and legal support in Canada were identified as facilitators. A-LLD identified most with the personality traits of agreeableness and conscientiousness. The median hospital stay was 6 days. One donor experienced a Dindo-Clavien Grade 3 complication that completely resolved. One-year recipient survival was 91% in 22 adults and 97% in 28 children. No A-LLD reported regretting their decision. This is the first and only report of the characteristics, motivations and facilitators of A-LLD in a large cohort. With rigorous protocols, outcomes are excellent. A-LLD has significant potential to reduce the gap between transplant organ demand and availability. Early in the development of our LLD program, a donor candidate challenged the requirement for a pre-existing connection between the live donor and recipient.We acknowledged that few centers offer anonymous kidney donation but noted that the latter operation is associated with a much smaller risk.Nonetheless, after a thorough ethical review, we decided to cautiously develop a unique program for anonymous-LLD (A-LLD) and reported favorable preliminary outcomes.13,14Herein, we report the characteristics and surgical outcome of the larger A-LLD experience (n = 50 cases) to date.Moreover, we provide rigorous quantitative and qualitative study data from 26/50 A-LLD who agreed to participate in a mixed methods study about their A-LLD experiences.We explored the reasons why people volunteer to become LLD despite the significant risks; factors that facilitate this choice; how they feel about this choice afterwards; and the potential of this option to reduce deaths on LT waiting lists.
Fatty liver disease, including non-alcoholic fatty liver (NAFLD) and steatohepatitis (NASH), has been associated with increased intestinal barrier permeability and translocation of bacteria or bacterial products into the blood circulation. In this study, we aimed to unravel the role of both intestinal barrier integrity and microbiota in NAFLD/NASH development. C57BL/6J mice were fed with high-fat diet (HFD) or methionine-choline-deficient diet for 1 week or longer to recapitulate aspects of NASH (steatosis, inflammation, insulin resistance). Genetic and pharmacological strategies were then used to modulate intestinal barrier integrity. We show that disruption of the intestinal epithelial barrier and gut vascular barrier (GVB) are early events in NASH pathogenesis. Mice fed HFD for only 1 week undergo a diet-induced dysbiosis that drives GVB damage and bacterial translocation into the liver. Fecal microbiota transplantation from HFD-fed mice into specific pathogen-free recipients induces GVB damage and epididymal adipose tissue enlargement. GVB disruption depends on interference with the WNT/β-catenin signaling pathway, as shown by genetic intervention driving β-catenin activation only in endothelial cells, preventing GVB disruption and NASH development. The bile acid analogue and farnesoid X receptor agonist obeticholic acid (OCA) drives β-catenin activation in endothelial cells. Accordingly, pharmacologic intervention with OCA protects against GVB disruption, both as a preventive and therapeutic agent. Importantly, we found upregulation of the GVB leakage marker in the colon of patients with NASH. We have identified a new player in NASH development, the GVB, whose damage leads to bacteria or bacterial product translocation into the blood circulation. Treatment aimed at restoring β-catenin activation in endothelial cells, such as administration of OCA, protects against GVB damage and NASH development. Fatty liver disease is characterized by a series of pathological conditions ranging from hepatic lipid accumulation (steatosis), to hepatocyte degeneration (ballooning), inflammation (steatohepatitis) and, eventually, cirrhosis and hepatocellular carcinoma.1,2Fatty liver disease may be the result of long-term excessive ethanol consumption (alcoholic liver disease) or of visceral obesity and metabolic syndrome without ethanol consumption, leading to non-alcoholic fatty liver disease (NAFLD) which can evolve to non-alcoholic steatohepatitis (NASH).3
Fatty liver disease, including non-alcoholic fatty liver (NAFLD) and steatohepatitis (NASH), has been associated with increased intestinal barrier permeability and translocation of bacteria or bacterial products into the blood circulation. In this study, we aimed to unravel the role of both intestinal barrier integrity and microbiota in NAFLD/NASH development. C57BL/6J mice were fed with high-fat diet (HFD) or methionine-choline-deficient diet for 1 week or longer to recapitulate aspects of NASH (steatosis, inflammation, insulin resistance). Genetic and pharmacological strategies were then used to modulate intestinal barrier integrity. We show that disruption of the intestinal epithelial barrier and gut vascular barrier (GVB) are early events in NASH pathogenesis. Mice fed HFD for only 1 week undergo a diet-induced dysbiosis that drives GVB damage and bacterial translocation into the liver. Fecal microbiota transplantation from HFD-fed mice into specific pathogen-free recipients induces GVB damage and epididymal adipose tissue enlargement. GVB disruption depends on interference with the WNT/β-catenin signaling pathway, as shown by genetic intervention driving β-catenin activation only in endothelial cells, preventing GVB disruption and NASH development. The bile acid analogue and farnesoid X receptor agonist obeticholic acid (OCA) drives β-catenin activation in endothelial cells. Accordingly, pharmacologic intervention with OCA protects against GVB disruption, both as a preventive and therapeutic agent. Importantly, we found upregulation of the GVB leakage marker in the colon of patients with NASH. We have identified a new player in NASH development, the GVB, whose damage leads to bacteria or bacterial product translocation into the blood circulation. Treatment aimed at restoring β-catenin activation in endothelial cells, such as administration of OCA, protects against GVB damage and NASH development. Several factors, likely acting in parallel, contribute to NASH development, including genetic predisposition, epigenetic changes, insulin resistance, abnormal lipid metabolism, oxidative stress, lipotoxicity, mitochondrial dysfunction, endoplasmic reticulum stress, hepatocyte apoptosis, activation of hepatic stellate cells, activation and recruitment of immune cells with production of inflammatory cytokines, altered adipokines, and gut dysbiosis.4,5
Fatty liver disease, including non-alcoholic fatty liver (NAFLD) and steatohepatitis (NASH), has been associated with increased intestinal barrier permeability and translocation of bacteria or bacterial products into the blood circulation. In this study, we aimed to unravel the role of both intestinal barrier integrity and microbiota in NAFLD/NASH development. C57BL/6J mice were fed with high-fat diet (HFD) or methionine-choline-deficient diet for 1 week or longer to recapitulate aspects of NASH (steatosis, inflammation, insulin resistance). Genetic and pharmacological strategies were then used to modulate intestinal barrier integrity. We show that disruption of the intestinal epithelial barrier and gut vascular barrier (GVB) are early events in NASH pathogenesis. Mice fed HFD for only 1 week undergo a diet-induced dysbiosis that drives GVB damage and bacterial translocation into the liver. Fecal microbiota transplantation from HFD-fed mice into specific pathogen-free recipients induces GVB damage and epididymal adipose tissue enlargement. GVB disruption depends on interference with the WNT/β-catenin signaling pathway, as shown by genetic intervention driving β-catenin activation only in endothelial cells, preventing GVB disruption and NASH development. The bile acid analogue and farnesoid X receptor agonist obeticholic acid (OCA) drives β-catenin activation in endothelial cells. Accordingly, pharmacologic intervention with OCA protects against GVB disruption, both as a preventive and therapeutic agent. Importantly, we found upregulation of the GVB leakage marker in the colon of patients with NASH. We have identified a new player in NASH development, the GVB, whose damage leads to bacteria or bacterial product translocation into the blood circulation. Treatment aimed at restoring β-catenin activation in endothelial cells, such as administration of OCA, protects against GVB damage and NASH development. The important role of the gut microbiota has been clearly established both in preclinical NAFLD/NASH models and in patients with NASH.6,7Bacterial overgrowth in the small intestine, as well as qualitative microbiome abnormalities can impair the barrier functions of the intestinal mucosa, leading to enhanced mucosa permeability and subsequent translocation of endotoxin to the bloodstream.8Patients with NASH present increased intestinal permeability and small intestinal bacterial overgrowth, which correlate with the severity of steatosis.9Similarly, mice deficient in Jam-A, an integral component of tight junctions, which control the paracellular route of solutes and prevent molecules such as lipopolysaccharide (LPS) from crossing the epithelium,10 are also more susceptible to NASH development.11Together, these data suggest that increased gut permeability represents an important mechanism in NASH pathogenesis, leading to the accumulation of endotoxin and bacterial components in the liver, and subsequently to the induction of inflammatory responses, through activation of pattern recognition receptors.
Fatty liver disease, including non-alcoholic fatty liver (NAFLD) and steatohepatitis (NASH), has been associated with increased intestinal barrier permeability and translocation of bacteria or bacterial products into the blood circulation. In this study, we aimed to unravel the role of both intestinal barrier integrity and microbiota in NAFLD/NASH development. C57BL/6J mice were fed with high-fat diet (HFD) or methionine-choline-deficient diet for 1 week or longer to recapitulate aspects of NASH (steatosis, inflammation, insulin resistance). Genetic and pharmacological strategies were then used to modulate intestinal barrier integrity. We show that disruption of the intestinal epithelial barrier and gut vascular barrier (GVB) are early events in NASH pathogenesis. Mice fed HFD for only 1 week undergo a diet-induced dysbiosis that drives GVB damage and bacterial translocation into the liver. Fecal microbiota transplantation from HFD-fed mice into specific pathogen-free recipients induces GVB damage and epididymal adipose tissue enlargement. GVB disruption depends on interference with the WNT/β-catenin signaling pathway, as shown by genetic intervention driving β-catenin activation only in endothelial cells, preventing GVB disruption and NASH development. The bile acid analogue and farnesoid X receptor agonist obeticholic acid (OCA) drives β-catenin activation in endothelial cells. Accordingly, pharmacologic intervention with OCA protects against GVB disruption, both as a preventive and therapeutic agent. Importantly, we found upregulation of the GVB leakage marker in the colon of patients with NASH. We have identified a new player in NASH development, the GVB, whose damage leads to bacteria or bacterial product translocation into the blood circulation. Treatment aimed at restoring β-catenin activation in endothelial cells, such as administration of OCA, protects against GVB damage and NASH development. The intestinal epithelial barrier (IEB) is composed of several elements, which are more or less accessible to bacteria and their derivatives.The first line of defense is provided by the mucus layer, thicker in the ileum, cecum and colon where microbes mainly reside.12The mucus physically separates the microbiota from the second physical barrier composed of epithelial cells, sealed by tight junctions.10We have described that just below the epithelium an additional cellular barrier, the gut vascular barrier (GVB), controls entry into the portal circulation and access to the liver.13,14Thus, if a molecule or a microorganism crosses the epithelial barrier, it will remain in the lamina propria, unless the GVB is also impaired.Indeed, for bacteria to reach the systemic circulation the GVB must be disrupted.Enteric pathogens such as Salmonella typhimurium, have established strategies to elude the GVB by interfering with the WNT/β-catenin signaling pathway.14In addition, the GVB is also disrupted in some pathological conditions such as in celiac disease (in patients with high serum levels of liver aminotransferases),13 and in patients with ankylosing spondylitis.15In the context of NASH, it is unclear if increased intestinal permeability due to disrupted IEB alone is sufficient to induce bacteria and LPS translocation and drive liver damage, or if GVB disruption is also required.
Fatty liver disease, including non-alcoholic fatty liver (NAFLD) and steatohepatitis (NASH), has been associated with increased intestinal barrier permeability and translocation of bacteria or bacterial products into the blood circulation. In this study, we aimed to unravel the role of both intestinal barrier integrity and microbiota in NAFLD/NASH development. C57BL/6J mice were fed with high-fat diet (HFD) or methionine-choline-deficient diet for 1 week or longer to recapitulate aspects of NASH (steatosis, inflammation, insulin resistance). Genetic and pharmacological strategies were then used to modulate intestinal barrier integrity. We show that disruption of the intestinal epithelial barrier and gut vascular barrier (GVB) are early events in NASH pathogenesis. Mice fed HFD for only 1 week undergo a diet-induced dysbiosis that drives GVB damage and bacterial translocation into the liver. Fecal microbiota transplantation from HFD-fed mice into specific pathogen-free recipients induces GVB damage and epididymal adipose tissue enlargement. GVB disruption depends on interference with the WNT/β-catenin signaling pathway, as shown by genetic intervention driving β-catenin activation only in endothelial cells, preventing GVB disruption and NASH development. The bile acid analogue and farnesoid X receptor agonist obeticholic acid (OCA) drives β-catenin activation in endothelial cells. Accordingly, pharmacologic intervention with OCA protects against GVB disruption, both as a preventive and therapeutic agent. Importantly, we found upregulation of the GVB leakage marker in the colon of patients with NASH. We have identified a new player in NASH development, the GVB, whose damage leads to bacteria or bacterial product translocation into the blood circulation. Treatment aimed at restoring β-catenin activation in endothelial cells, such as administration of OCA, protects against GVB damage and NASH development. Obeticholic acid (OCA), a derivative of the bile acid chenodeoxycholic acid and a potent agonist of the farnesoid X receptor (FXR)16 has been shown to play a role in controlling intestinal permeability,17 gut barrier dysfunction and bacterial translocation.18OCA ameliorates disease in patients with type II diabetes mellitus and NAFLD,19 and in patients with NASH.20However, the precise roles of bile acids and FXR in controlling IEB or GVB permeability are still unknown.
DNAJB1-PRKACA fusion is a specific driver event in fibrolamellar carcinoma (FLC), a rare subtype of hepatocellular carcinoma (HCC) occurring in adolescents and young adults. In older patients, molecular determinants of HCC with mixed histological features of HCC and FLC (mixed-FLC/HCC) remain to be discovered. A series of 151 liver tumors including 126 HCC, 15 FLC, and 10 mixed-FLC/HCC were analyzed by RNAseq and whole-genome- or whole-exome-sequencing. Western-blots were performed to validate genomics discoveries. Results were validated using the TCGA database. Most of the mixed-FLC/HCC RNAseq clustered in a robust subgroup of 17 tumors all showing mutation or translocation inactivating BAP1 that codes for the BRCA1 associated protein-1. Similar to FLC, BAP1-HCC were significantly enriched in female, tumor fibrosis and the lack of chronic liver disease when compared to non-BAP1-HCC. However, patients were older and with a poorer prognosis than FLC patients. BAP1 tumors were immune hot, showed progenitor features, did not show DNAJB1-PRKACA fusion and were almost all non-mutated for CTNNB1, TP53 and TERT promoter. In contrast, 80% of the BAP1 tumors showed a chromosome gain of PRKACA at 19p13, combined with a loss of PRKAR2A (coding for the inhibitory regulatory subunit of PKA) at 3p21, leading to a high PRKACA/PRKAR2A ratio at the mRNA and protein levels. We have characterized a subgroup of BAP1-driven HCC bearing fibrolamellar-like features and a dysregulation of the PKA pathway, which could be at the root of the clinical and histological similarities between BAP1 tumors and DNAJB1-PRKACA FLCs. Fibrolamellar carcinoma (FLC) is a rare subtype of hepatocellular carcinoma (HCC) mostly diagnosed in adolescents and young adults.It was originally defined by specific histological features of both the tumor cells and their stroma, with the presence of abundant fibrosis arranged in a lamellar fashion around deeply eosinophilic large neoplastic hepatocytes, frequent central scar and calcifications1,2.FLC defines a specific subgroup of HCC since they have peculiar clinical features compared to classical HCC, such as a young age at onset between 10 to 35 years old, a balanced sex ratio, an absence of underlying liver disease or risk factors and a better prognosis with 80% of survival at 5-years after resection3,4.Biologically, FLCs show a high number of mitochondria and progenitor features, suggesting that the tumor cells are blocked at a specific stage of differentiation with hepatocellular (HEPAR1), biliary (CK7) and CD68 co-expressed markers1,5.In 2014, Honeyman and colleagues discovered a specific 400 kb chromosome deletion at chromosome 19 in FLC leading to recurrent chimeric DNAJB1-PRKACA gene fusion6.DNAJB1 encodes HSP40, a member of the heat shock protein family, while PRKACA codes for the cAMP-dependent protein kinase (PKA) catalytic subunit alpha; the chimeric gene results in PRKACA catalytic domain overexpression and subsequent PKA activation.In addition, rare FLC without DNAJB1-PRKACA fusion were identified in patients with Carney disease due to PRKAR1A germline mutations leading also to PKA activation and similar phenotype7.In contrast, PKA activation is only rarely identified in HCC (<1% GNAS mutations) or in cholangiocarcinoma (around 6% of GNAS mutations and rare fusions of PRKACA, PRKACAB and PRKAR1B)8,9.
DNAJB1-PRKACA fusion is a specific driver event in fibrolamellar carcinoma (FLC), a rare subtype of hepatocellular carcinoma (HCC) occurring in adolescents and young adults. In older patients, molecular determinants of HCC with mixed histological features of HCC and FLC (mixed-FLC/HCC) remain to be discovered. A series of 151 liver tumors including 126 HCC, 15 FLC, and 10 mixed-FLC/HCC were analyzed by RNAseq and whole-genome- or whole-exome-sequencing. Western-blots were performed to validate genomics discoveries. Results were validated using the TCGA database. Most of the mixed-FLC/HCC RNAseq clustered in a robust subgroup of 17 tumors all showing mutation or translocation inactivating BAP1 that codes for the BRCA1 associated protein-1. Similar to FLC, BAP1-HCC were significantly enriched in female, tumor fibrosis and the lack of chronic liver disease when compared to non-BAP1-HCC. However, patients were older and with a poorer prognosis than FLC patients. BAP1 tumors were immune hot, showed progenitor features, did not show DNAJB1-PRKACA fusion and were almost all non-mutated for CTNNB1, TP53 and TERT promoter. In contrast, 80% of the BAP1 tumors showed a chromosome gain of PRKACA at 19p13, combined with a loss of PRKAR2A (coding for the inhibitory regulatory subunit of PKA) at 3p21, leading to a high PRKACA/PRKAR2A ratio at the mRNA and protein levels. We have characterized a subgroup of BAP1-driven HCC bearing fibrolamellar-like features and a dysregulation of the PKA pathway, which could be at the root of the clinical and histological similarities between BAP1 tumors and DNAJB1-PRKACA FLCs. Frequently, FLC diagnosis can be difficult and tumors with histological features of both HCC and FLC have been described as mixed-FLC/HCC10–12.In comparison with FLC, mixed-FLC/HCC patients were older, all above 35 years, and with a poor prognosis.Moreover, transcriptomic analyses showed a different profile of expression in the mixed-FLC and a lack of DNAJB1-PRKACA fusion13,14.Therefore, mixed-FLC/HCC should be better defined in the phenotypic/molecular diversity of the liver tumors, in particular to identify similarities and differences with FLC of the young and other HCC subtypes12.
There is growing evidence that liver graft ischemia-reperfusion (I/R) is a risk factor for hepatocellular carcinoma (HCC) recurrence, but the mechanisms involved are unclear. Herein, we tested the hypothesis that mesenteric congestion resulting from portal blood flow interruption induces endotoxin-mediated Toll-like receptor 4 (Tlr4) engagement, resulting in elevated liver cancer burden. We also assessed the role of remote ischemic preconditioning (RIPC) in this context. C57Bl/6j mice were exposed to standardized models of liver I/R injury and RIPC, induced by occluding the hepatic and femoral blood vessels. HCC was induced by injecting RIL-175 cells into the portal vein. We further evaluated the impact of the gut–liver axis (lipopolysaccharide (LPS)-Tlr4 pathway) in this context by studying mice with enhanced (lipopolysaccharide infusion) or defective (Tlr4−/− mice, gut sterilization, and Tlr4 antagonist) Tlr4 responses. Portal triad clamping provoked upstream mesenteric venous engorgement and increased bacterial translocation, resulting in aggravated tumor burden. RIPC prevented this mechanism by preserving intestinal integrity and reducing bacterial translocation, thereby mitigating HCC recurrence. These observations were linked to the LPS-Tlr4 pathway, as supported by the high and low tumor burden displayed by mice with enhanced or defective Tlr4 responses, respectively. Modulation of the gut–liver axis and the LPS-Tlr4 response by RIPC, gut sterilization, and Tlr4 antagonism represents a potential therapeutic target to prevent I/R lesions, and to alleviate HCC recurrence after liver transplantation and resection. Hepatocellular carcinoma (HCC) is the third most common cause of cancer-related death worldwide (www.who.int).Although surgical treatments, including liver resection and liver transplantation, are the best options for patients with curable HCC, cancer recurrence remains the main limitation to these strategies.1,2In addition to tumor biology, which is the main driving factor for HCC recurrence, preclinical and clinical evidence suggests that organ damage, such as ischemia/reperfusion (I/R) injury, favors the recurrence of HCC and colorectal metastases after liver surgery.3,4In this regard, experimental studies have demonstrated that liver I/R injury leads to sinusoidal microthrombi, neutrophil sequestration, and the release of proinflammatory and proregenerative molecules5 in the liver, facilitating the entrapment and proliferation of circulating cancer cells in the injured liver.
There is growing evidence that liver graft ischemia-reperfusion (I/R) is a risk factor for hepatocellular carcinoma (HCC) recurrence, but the mechanisms involved are unclear. Herein, we tested the hypothesis that mesenteric congestion resulting from portal blood flow interruption induces endotoxin-mediated Toll-like receptor 4 (Tlr4) engagement, resulting in elevated liver cancer burden. We also assessed the role of remote ischemic preconditioning (RIPC) in this context. C57Bl/6j mice were exposed to standardized models of liver I/R injury and RIPC, induced by occluding the hepatic and femoral blood vessels. HCC was induced by injecting RIL-175 cells into the portal vein. We further evaluated the impact of the gut–liver axis (lipopolysaccharide (LPS)-Tlr4 pathway) in this context by studying mice with enhanced (lipopolysaccharide infusion) or defective (Tlr4−/− mice, gut sterilization, and Tlr4 antagonist) Tlr4 responses. Portal triad clamping provoked upstream mesenteric venous engorgement and increased bacterial translocation, resulting in aggravated tumor burden. RIPC prevented this mechanism by preserving intestinal integrity and reducing bacterial translocation, thereby mitigating HCC recurrence. These observations were linked to the LPS-Tlr4 pathway, as supported by the high and low tumor burden displayed by mice with enhanced or defective Tlr4 responses, respectively. Modulation of the gut–liver axis and the LPS-Tlr4 response by RIPC, gut sterilization, and Tlr4 antagonism represents a potential therapeutic target to prevent I/R lesions, and to alleviate HCC recurrence after liver transplantation and resection. Liver I/R injury is commonly interpreted as a state of sterile inflammation, where liver tissue undergoing hypoxia initiates a cascade of events that eventually result in hepatocellular injury, alteration of liver function, and worsened oncological outcomes in the presence of cancer.6However, in addition to the downstream effects of liver blood inflow interruption, portal vein inflow obstruction also causes mesenteric congestion, bowel wall oedema, and venous ischemia, which can facilitate the passage of bacteria (or bacterial debris) from the intestinal lumen to the portal circulation, a process termed ‘bacterial translocation’.7Translocation of bacterial components in the portal circulation triggers inflammatory pathways and contributes to liver injury through the engagement of various pattern-recognizing receptors (PRRs), including Toll-like receptor 4 (Tlr4).8Tlr4 binding of circulating Gram-negative bacterial wall lipopolysaccharide (LPS, also known as endotoxin) leads to a cascade of the phosphorylation of mitogen-activated protein kinases (MAPKs), activation of nuclear factor-κB (NF-κB) signaling, and upregulation of various inflammatory cytokines, chemokines, and adhesion molecules.9Tlr4 is considered to be pivotal to liver I/R, given that it is engaged at the onset of I/R injury and that it induces the release (or the upregulation) of danger-associated molecular patterns (DAMPs), such as heat shock proteins and high-mobility group box protein 1, which are themselves ligands of Tlr4 (or facilitate Tlr4 signaling).10,11Tlr4 regulates a range of biological responses that reach beyond defence against invading pathogens, including the promotion of inflammation and carcinogenesis.12
It is unclear if a reduction in hepatic fat content (HFC) is a major mediator of the cardiometabolic benefit of lifestyle intervention, and whether it has prognostic significance beyond the loss of visceral adipose tissue (VAT). In the present sub-study, we hypothesized that HFC loss in response to dietary interventions induces specific beneficial effects independently of VAT changes. In an 18-month weight-loss trial, 278 participants with abdominal obesity/dyslipidemia were randomized to low-fat (LF) or Mediterranean/low-carbohydrate (MED/LC + 28 g walnuts/day) diets with/without moderate physical activity. HFC and abdominal fat-depots were measured using magnetic resonance imaging at baseline, after 6 (sub-study, n = 158) and 18 months. Of 278 participants (mean HFC 10.2% [range: 0.01%–50.4%]), the retention rate was 86.3%. The %HFC substantially decreased after 6 months (−6.6% absolute units [−41% relatively]) and 18 months (−4.0% absolute units [−29% relatively]; p <0.001 vs. baseline). Reductions of HFC were associated with decreases in VAT beyond weight loss. After controlling for VAT loss, decreased %HFC remained independently associated with reductions in serum gamma glutamyltransferase and alanine aminotransferase, circulating chemerin, and glycated hemoglobin (p <0.05). While the reduction in HFC was similar between physical activity groups, MED/LC induced a greater %HFC decrease (p = 0.036) and greater improvements in cardiometabolic risk parameters (p <0.05) than the LF diet, even after controlling for VAT changes. Yet, the greater improvements in cardiometabolic risk parameters induced by MED/LC were all markedly attenuated when controlling for HFC changes. %HFC is substantially reduced by diet-induced moderate weight loss and is more effectively reduced by the MED/LC diet than the LF diet, independently of VAT changes. The beneficial effects of the MED/LC diet on specific cardiometabolic parameters appear to be mediated more by decreases in %HFC than VAT loss. Beyond total body fat content, fat distribution, both within adipose tissue depots and in ectopic fat deposits, is increasingly being shown to determine obesity-related health impact.1,2Visceral adipose tissue (VAT), due to its unique anatomical location, releases free fatty acids (FFAs) and adipokines to the liver via the portal vein.Previous studies have demonstrated the inter-relationship between VAT and hepatic fat content (HFC), and indeed, increases in HFC were associated with similar metabolic abnormalities as observed for increases in VAT.3,4In addition, reductions in VAT and HFC are increasingly thought to mediate the beneficial cardiometabolic outcomes of weight loss.1,5Though closely associated with HFC, VAT and HFC may uniquely associate with specific effects and be linked independently with risk factors of cardiometabolic disease.6Interestingly, data from recent studies found that HFC was more strongly associated with obesity’s metabolic complications than VAT,7 including the deterioration of glucose tolerance,8 possibly by mediating the link between obesity and metabolic dysfunction.9,10Most recently, the decrease in HFC was associated with diabetes remission.11
It is unclear if a reduction in hepatic fat content (HFC) is a major mediator of the cardiometabolic benefit of lifestyle intervention, and whether it has prognostic significance beyond the loss of visceral adipose tissue (VAT). In the present sub-study, we hypothesized that HFC loss in response to dietary interventions induces specific beneficial effects independently of VAT changes. In an 18-month weight-loss trial, 278 participants with abdominal obesity/dyslipidemia were randomized to low-fat (LF) or Mediterranean/low-carbohydrate (MED/LC + 28 g walnuts/day) diets with/without moderate physical activity. HFC and abdominal fat-depots were measured using magnetic resonance imaging at baseline, after 6 (sub-study, n = 158) and 18 months. Of 278 participants (mean HFC 10.2% [range: 0.01%–50.4%]), the retention rate was 86.3%. The %HFC substantially decreased after 6 months (−6.6% absolute units [−41% relatively]) and 18 months (−4.0% absolute units [−29% relatively]; p <0.001 vs. baseline). Reductions of HFC were associated with decreases in VAT beyond weight loss. After controlling for VAT loss, decreased %HFC remained independently associated with reductions in serum gamma glutamyltransferase and alanine aminotransferase, circulating chemerin, and glycated hemoglobin (p <0.05). While the reduction in HFC was similar between physical activity groups, MED/LC induced a greater %HFC decrease (p = 0.036) and greater improvements in cardiometabolic risk parameters (p <0.05) than the LF diet, even after controlling for VAT changes. Yet, the greater improvements in cardiometabolic risk parameters induced by MED/LC were all markedly attenuated when controlling for HFC changes. %HFC is substantially reduced by diet-induced moderate weight loss and is more effectively reduced by the MED/LC diet than the LF diet, independently of VAT changes. The beneficial effects of the MED/LC diet on specific cardiometabolic parameters appear to be mediated more by decreases in %HFC than VAT loss. Diet plays an important role in the accumulation of HFC and VAT.12Several short-6,13 and long-term14,15 dietary interventions have suggested that Mediterranean and low-carb diets had favorable effects on VAT and HFC accumulation, but also on glycemic status and lipid biomarkers.Others found no differences between HFC changes induced by diets with different amounts of carbohydrate.16The effect of specific long-term lifestyle interventions on HFC and its association with the dynamics of cardiometabolic risk, beyond VAT loss, remain unclear.Notably, recent guidelines for decreasing HFC do not suggest a particular lifestyle strategy, but only endorse weight loss as a general recommendation.17
Chronic failure of mechanisms that promote effective regeneration of dead hepatocytes causes replacement of functional hepatic parenchyma with fibrous scar tissue, ultimately resulting in cirrhosis. Therefore, defining and optimizing mechanisms that orchestrate effective regeneration might prevent cirrhosis. We hypothesized that effective regeneration of injured livers requires hepatocytes to evade the growth-inhibitory actions of TGFβ, since TGFβ signaling inhibits mature hepatocyte growth but drives cirrhosis pathogenesis. Wild-type mice underwent 70% partial hepatectomy (PH); TGFβ expression and signaling were evaluated in intact tissue and primary hepatocytes before, during, and after the period of maximal hepatocyte proliferation that occurs from 24–72 h after PH. To determine the role of Yap1 in regulating TGFβ signaling in hepatocytes, studies were repeated after selectively deleting Yap1 from hepatocytes of Yap1flox/flox mice. TGFβ expression and hepatocyte nuclear accumulation of pSmad2 and Yap1 increased in parallel with hepatocyte proliferative activity after PH. Proliferative hepatocytes also upregulated Snai1, a pSmad2 target gene that promotes epithelial-to-mesenchymal transition (EMT), suppressed epithelial genes, induced myofibroblast markers, and produced collagen 1α1. Deleting Yap1 from hepatocytes blocked their nuclear accumulation of pSmad2 and EMT-like response, as well as their proliferation. Interactions between the TGFβ and Hippo-Yap signaling pathways stimulate hepatocytes to undergo an EMT-like response that is necessary for them to grow in a TGFβ-enriched microenvironment and regenerate injured livers. Adult liver has robust regenerative capacity as demonstrated by efficient restitution of fully functional liver mass within days to weeks after acute 70% partial hepatectomy (PH) in mice and humans, respectively.1Paradoxically, chronically dysregulated repair of even minor liver damage causes defective recovery of healthy liver parenchyma and ultimately results in cirrhosis and/or liver cancer.Preventing these bad outcomes of liver injury is necessary to reduce mortality from liver disease, a major cause of premature death in adulthood worldwide.2Success has been elusive because of relatively limited understanding of the mechanisms that promote effective reconstruction of this complex organ in adults.Old dogma posited that adult liver regeneration mainly involved replication of surviving mature hepatocytes.However, it is becoming clear that reconstruction of healthy liver parenchyma after both acute and chronic injury requires an orderly cascade of diverse repair responses that are precisely and coordinately regulated to assure that all resident liver cell populations are replaced and appropriately integrated to reconstitute diverse tissue-specific functions.PH is a useful model for teasing apart these regenerative mechanisms.3
Systemic corticosteroids may cause HBV reactivation, but the impact on patients with previous HBV exposure is poorly defined. We aimed to study the risk of HBsAg seroreversion and hepatitis flare in patients with previous HBV exposure. Patients who were negative for HBsAg and received corticosteroids between 2001–2010 were included. Patients who were positive for antibody to HBsAg (anti-HBs) and/or to HBcAg (anti-HBc) were defined as having previous HBV exposure. The primary endpoint was HBsAg seroreversion; the secondary endpoint was hepatitis flare (alanine aminotransferase >80 U/L) at 1 year. A total of 12,997 patients fulfilled the inclusion criteria: anti-HBs positive only (n = 10,561); anti-HBc positive only (n = 970); anti-HBs & anti-HBc positive (n = 830) and anti-HBs & anti-HBc negative (n = 636). HBsAg seroreversion occurred in 165 patients. Patients who were anti-HBc positive only had a higher risk of HBsAg seroreversion (1-year incidence 1.8%) than those negative for both anti-HBs & anti-HBc (0%; p = 0.014). Patients with previous HBV exposure had a similarly low risk of liver failure as unexposed individuals (1.1% vs. 0.9%). The risk of a hepatitis flare started to increase in those receiving corticosteroids at peak daily doses of 20–40 mg (adjusted hazard ratio [HR] 2.19, p = 0.048) or >40 mg (aHR 2.11, p = 0.015) prednisolone equivalents for <7 days, and was increased at treatment durations of 7–28 days and >28 days (aHR 2.02–3.85; p <0.001–0.012). In HBsAg-negative patients who were only anti-HBc positive, high peak daily doses of corticosteroids increased the risk of hepatitis flare, but not seroreversion. The rate of liver failure was low and similar in HBV exposed and unexposed individuals; there were no deaths, nor any requirement for liver transplantation. Chronic HBV infection remains a considerable global health problem despite vaccination.1Approximately 2 billion people of the world population have been infected; an estimated 257 million people are living with HBV infection (defined as HBsAg positive).2HBsAg seroclearance, which may occur either spontaneously or after antiviral treatment,3–5 is currently regarded as the functional cure of chronic hepatitis B (CHB).6–8Neither disappearance of HBsAg in patients with CHB nor recovery from a self-limiting acute hepatitis B guarantees lifelong protection.Some patients may suffer from occult hepatitis B infection (OBI), a status of undetectable serum HBsAg yet detectable serum and/or intrahepatic HBV DNA.9Past or resolved HBV infection may still lead to HCC, cirrhotic complications and liver-related death.3–5
Systemic corticosteroids may cause HBV reactivation, but the impact on patients with previous HBV exposure is poorly defined. We aimed to study the risk of HBsAg seroreversion and hepatitis flare in patients with previous HBV exposure. Patients who were negative for HBsAg and received corticosteroids between 2001–2010 were included. Patients who were positive for antibody to HBsAg (anti-HBs) and/or to HBcAg (anti-HBc) were defined as having previous HBV exposure. The primary endpoint was HBsAg seroreversion; the secondary endpoint was hepatitis flare (alanine aminotransferase >80 U/L) at 1 year. A total of 12,997 patients fulfilled the inclusion criteria: anti-HBs positive only (n = 10,561); anti-HBc positive only (n = 970); anti-HBs & anti-HBc positive (n = 830) and anti-HBs & anti-HBc negative (n = 636). HBsAg seroreversion occurred in 165 patients. Patients who were anti-HBc positive only had a higher risk of HBsAg seroreversion (1-year incidence 1.8%) than those negative for both anti-HBs & anti-HBc (0%; p = 0.014). Patients with previous HBV exposure had a similarly low risk of liver failure as unexposed individuals (1.1% vs. 0.9%). The risk of a hepatitis flare started to increase in those receiving corticosteroids at peak daily doses of 20–40 mg (adjusted hazard ratio [HR] 2.19, p = 0.048) or >40 mg (aHR 2.11, p = 0.015) prednisolone equivalents for <7 days, and was increased at treatment durations of 7–28 days and >28 days (aHR 2.02–3.85; p <0.001–0.012). In HBsAg-negative patients who were only anti-HBc positive, high peak daily doses of corticosteroids increased the risk of hepatitis flare, but not seroreversion. The rate of liver failure was low and similar in HBV exposed and unexposed individuals; there were no deaths, nor any requirement for liver transplantation. HBV reactivation during chemotherapy or immunosuppressive therapy is a well-known phenomenon and a major concern as it may be complicated by fulminant hepatic failure and death.10OBI reactivation may take place with increasing HBV DNA replication in patients during immunosuppression therapy.11Among all immunosuppressive agents, corticosteroids are the most widely used in a spectrum of acute and chronic immune-mediated diseases.12The degree of immunosuppression increases with dose and duration of treatment.A daily dose of prednisolone, or equivalent, above 20 mg for longer than 2 weeks is generally considered to induce clinically significant immunosuppression.13The latest American Gastroenterological Association Institute (AGA) guideline on the prevention and treatment of HBV reactivation during immunosuppressive drug therapy defined HBsAg-positive patients treated with moderate-dose (10–20 mg prednisolone daily, or equivalent) or high-dose (>20 mg prednisolone daily, or equivalent) corticosteroids for ≥4 weeks as being at high risk of HBV reactivation.14Nonetheless, our recent real-world cohort study showed that even a short course (less than 7 days) of high-dose corticosteroids might increase the risk of hepatitis flares.15
The degree of cholestasis is an important disease driver in alcoholic hepatitis, a severe clinical condition that needs new biomarkers and targeted therapies. We aimed to identify the largely unknown mechanisms and biomarkers linked to cholestasis in alcoholic hepatitis. Herein, we analyzed a well characterized cohort of patients with alcoholic hepatitis and correlated clinical and histological parameters and outcomes with serum bile acids and fibroblast growth factor 19 (FGF19), a major regulator of bile acid synthesis. We found that total and conjugated bile acids were significantly increased in patients with alcoholic hepatitis compared with controls. Serum FGF19 levels were strongly increased and gene expression of FGF19 was induced in biliary epithelial cells and ductular cells of patients with alcoholic hepatitis. De novo bile acid synthesis (CYP7A1 gene expression and C4 serum levels) was significantly decreased in patients with alcoholic hepatitis. Importantly, total and conjugated bile acids correlated positively with FGF19 and with disease severity (model for end-stage liver disease score). FGF19 correlated best with conjugated cholic acid, and model for end-stage liver disease score best with taurine-conjugated chenodeoxycholic acid. Univariate analysis demonstrated significant associations between FGF19 and bilirubin as well as gamma glutamyl transferase, and negative correlations between FGF19 and fibrosis stage as well as polymorphonuclear leukocyte infiltration, in all patients with alcoholic hepatitis. Serum FGF19 and bile acids are significantly increased in patients with alcoholic hepatitis, while de novo bile acid synthesis is suppressed. Modulation of bile acid metabolism or signaling could represent a promising target for treatment of alcoholic hepatitis in humans. Alcohol abuse is the most important cause of liver disease worldwide.1The most severe form of alcoholic liver disease is alcoholic hepatitis with mortality rates of 20–40% at 1–6 months, and a 90-day mortality rate of up to 75% in severe alcoholic hepatitis.2–4Corticosteroids are the only effective medical therapy but failure in many patients has been reported.2There is no cure for patients not responding to medical therapy, except for early liver transplantation that is offered in some centers to a highly selected group of patients.5
The degree of cholestasis is an important disease driver in alcoholic hepatitis, a severe clinical condition that needs new biomarkers and targeted therapies. We aimed to identify the largely unknown mechanisms and biomarkers linked to cholestasis in alcoholic hepatitis. Herein, we analyzed a well characterized cohort of patients with alcoholic hepatitis and correlated clinical and histological parameters and outcomes with serum bile acids and fibroblast growth factor 19 (FGF19), a major regulator of bile acid synthesis. We found that total and conjugated bile acids were significantly increased in patients with alcoholic hepatitis compared with controls. Serum FGF19 levels were strongly increased and gene expression of FGF19 was induced in biliary epithelial cells and ductular cells of patients with alcoholic hepatitis. De novo bile acid synthesis (CYP7A1 gene expression and C4 serum levels) was significantly decreased in patients with alcoholic hepatitis. Importantly, total and conjugated bile acids correlated positively with FGF19 and with disease severity (model for end-stage liver disease score). FGF19 correlated best with conjugated cholic acid, and model for end-stage liver disease score best with taurine-conjugated chenodeoxycholic acid. Univariate analysis demonstrated significant associations between FGF19 and bilirubin as well as gamma glutamyl transferase, and negative correlations between FGF19 and fibrosis stage as well as polymorphonuclear leukocyte infiltration, in all patients with alcoholic hepatitis. Serum FGF19 and bile acids are significantly increased in patients with alcoholic hepatitis, while de novo bile acid synthesis is suppressed. Modulation of bile acid metabolism or signaling could represent a promising target for treatment of alcoholic hepatitis in humans. Alcoholic hepatitis is a hepatocellular disease associated with cholestasis and accumulation of bile acids in the liver and the systemic circulation.6Accumulation of bile acids in the liver can result in hepatocellular damage followed by inflammation and fibrosis.7Early studies using high-performance liquid chromatography (HPLC) have identified elevated bile acid levels in patients with alcoholic hepatitis and a positive correlation with the alcoholic hepatitis score.8The liver controls bile acid homeostasis via a complicated network of pathways regulated by nuclear receptors.7One important regulator is fibroblast growth factor 19 (FGF19), a hormone that is secreted in the intestine and reaches the liver via the portal circulation.Binding of FGF19 to its receptor, fibroblast growth factor receptor 4 and β-Klotho, activates mitogen-activated protein kinase pathways, resulting in the downregulation of the rate-limiting enzyme of bile acid synthesis, cytochrome P450 family 7 subfamily A member 1 (CYP7A1), gene expression and inhibition of bile acid synthesis.9Although FGF19 is not detectable in the liver under normal conditions, previous studies suggest that FGF19 is expressed under cholestatic conditions, such as in patients with primary biliary cholangitis.Plasma/serum elevated FGF19 levels are elevated in patients with various hepatic disorders including non-cirrhotic and cirrhotic primary biliary cholangitis and in patients with extrahepatic cholestasis.9–11FGF19 mRNA expression was increased in the terminal ileum of actively drinking patients with cirrhosis when compared with patients with cirrhosis of non-alcoholic etiology.12
Cells of hematopoietic origin, including macrophages, are generally radiation sensitive, but a subset of Kupffer cells (KCs) is relatively radioresistant. Here, we focused on the identity of the radioresistant KCs in unmanipulated mice and the mechanism of radioresistance. We employed Emr1- and inducible CX3Cr1-based fate-mapping strategies combined with the RiboTag reporter to identify the total KCs and the embryo-derived KCs, respectively. The KC compartment was reconstituted with adult bone-marrow-derived KCs (bm-KCs) using clodronate depletion. Mice were lethally irradiated and transplanted with donor bone marrow, and the radioresistance of bone-marrow- or embryo-derived KCs was studied. Gene expression was analyzed using in situ mRNA isolation via RiboTag reporter mice, and the translatomes were compared among subsets. Here, we identified the radioresistant KCs as the long-lived subset that is derived from CX3CR1-expressing progenitor cells in fetal life, while adult bm-KCs do not resist irradiation. While both subsets upregulated the Cdkn1a gene, encoding p21-cip1/WAF1 protein, radioresistant embryo-derived KCs showed a greater increase in response to irradiation. In the absence of this molecule, the radioresistance of KCs was compromised. Replacement KCs, derived from adult hematopoietic stem cells, differed from radioresistant KCs in their expression of genes related to immunity and phagocytosis. Here, we show that, in the murine liver, a subset of KCs of embryonic origin resists lethal irradiation through Cdkn1a upregulation and is maintained for a long period, while bm-KCs do not survive lethal irradiation. Kupffer cells (KCs), the liver-resident macrophages, are important innate immune sensors that respond to liver stress, and may either stimulate or suppress immunity.1In contrast to most other leukocytes, tissue-resident macrophages in the brain (microglia) and epidermis (Langerhans cells) are highly radioresistant.2,3However, in the liver, only a subset of KCs resists lethal irradiation, while the other subset is replaced by donor bone-marrow-monocyte-derived KCs (bm-KCs).4,5This radioresistant KC subset is long lived, and they are not recruited to foci of inflammation, and thus, are termed sessile KCs.4,6Gene expression and epigenetic chromatin modification analysis have confirmed that bm-KCs become broadly similar to sessile KC post-recovery, but not identical.5,7While the sessile KC subset is identified post-irradiation, the identity of this subset in unmanipulated mice and why only a subset of KCs resists irradiation are not yet understood.
Cells of hematopoietic origin, including macrophages, are generally radiation sensitive, but a subset of Kupffer cells (KCs) is relatively radioresistant. Here, we focused on the identity of the radioresistant KCs in unmanipulated mice and the mechanism of radioresistance. We employed Emr1- and inducible CX3Cr1-based fate-mapping strategies combined with the RiboTag reporter to identify the total KCs and the embryo-derived KCs, respectively. The KC compartment was reconstituted with adult bone-marrow-derived KCs (bm-KCs) using clodronate depletion. Mice were lethally irradiated and transplanted with donor bone marrow, and the radioresistance of bone-marrow- or embryo-derived KCs was studied. Gene expression was analyzed using in situ mRNA isolation via RiboTag reporter mice, and the translatomes were compared among subsets. Here, we identified the radioresistant KCs as the long-lived subset that is derived from CX3CR1-expressing progenitor cells in fetal life, while adult bm-KCs do not resist irradiation. While both subsets upregulated the Cdkn1a gene, encoding p21-cip1/WAF1 protein, radioresistant embryo-derived KCs showed a greater increase in response to irradiation. In the absence of this molecule, the radioresistance of KCs was compromised. Replacement KCs, derived from adult hematopoietic stem cells, differed from radioresistant KCs in their expression of genes related to immunity and phagocytosis. Here, we show that, in the murine liver, a subset of KCs of embryonic origin resists lethal irradiation through Cdkn1a upregulation and is maintained for a long period, while bm-KCs do not survive lethal irradiation. Kupffer cells traditionally comprise all of the fully mature macrophages in the steady-state liver, identified by F4/80 staining in the mouse.A developing consensus asserts that KCs are a single population of long-lived cells that are seeded from embryonic precursors, and that any other macrophages found in the liver are transient, better-termed “liver macrophages.”In favor of this view, several lineage-tracing experiments concluded that all liver-resident macrophages are embryo derived.8–12Conversely, other evidence favors the concept that long-lived KCs are heterogeneous, with some derived from blood monocytes.On adoptive transfer into neonates, blood monocytes integrated into the KC niche, indicating that the liver macrophage compartment remained open.13Resident fractalkine receptor-1 (Cx3cr1)-hiF4/80+ subcapsular macrophages are continuously replenished by blood monocytes in the steady-state liver, further supporting the presence of bm-KCs in normal adult liver.14A single-cell transcriptome analysis reports two distinct KC clusters based on gene expression Marco, Clec4f, Clec1b versus Clec4e, Clec4d subsets.15While the relationships between subsets identified in these studies are yet to be resolved, a strong case can be made that the liver contains at least two subsets of macrophages: yolk-sac (YS) embryo-derived versus monocyte-derived cells.In this study, we explored the identity of the sessile KC subset in relation to ontogeny, evaluating their embryo versus monocyte origin and potential mechanisms of radioresistance.
In treated patients with chronic hepatitis B (CHB) who have achieved complete viral suppression, it is unclear if functional cure as indicated by hepatitis B surface antigen (HBsAg) seroclearance confers additional clinical benefit. We compared the risk of hepatocellular carcinoma (HCC) and hepatic events in nucleos(t)ide analogue (NA)-treated patients with and without HBsAg seroclearance. We performed a territory-wide retrospective cohort study on all patients with CHB who had received entecavir and/or tenofovir disoproxil fumarate (TDF) for at least 6 months between 2005 and 2016 from Hospital Authority, Hong Kong. Patients’ demographics, comorbidities, and laboratory parameters were analyzed. The primary outcome was HCC. The secondary outcomes were hepatic events including cirrhotic complications, liver transplantation, and liver-related mortality. A total of 20,263 entecavir/TDF-treated patients with CHB were identified; 17,499 (86.4%) patients had complete viral suppression; 376 (2.1%) achieved HBsAg seroclearance. At a median (interquartile range) follow-up of 4.8 (2.8–7.0) years, 603 (3.5%) and 121 (4.4%) patients with and without complete viral suppression developed HCC; 2 (0.5%) patients with HBsAg seroclearance developed HCC. Compared to complete viral suppression, lack of complete viral suppression was associated with a higher risk of HCC (7.8% vs. 5.6% at 8 years, Gray’s test, p <0.001) (adjusted hazard ratio [aHR] 1.69; 95% CI 1.36–2.09; p <0.001); patients who achieved functional cure had a lower risk of HCC (0.6% vs. 5.6% at 8 years, Gray’s test, p <0.001) (aHR 0.24; 95% CI 0.06–0.97; p = 0.045) but not hepatic events (aHR 0.99; 95% CI 0.30–3.26; p = 0.991). Patients who achieved HBsAg seroclearance on top of complete viral suppression with entecavir/TDF treatment may have a lower risk of HCC but not hepatic events. Chronic hepatitis B (CHB) is one of the leading causes of hepatocellular carcinoma (HCC), cirrhotic complications and liver-related death worldwide.1Antiviral therapy with potent nucleos(t)ide analogues (NA), namely entecavir and tenofovir disoproxil fumarate (TDF), reduces the risk of HCC and hepatic complications.2,3Recent data suggest that complete viral suppression is an important treatment goal because patients with low detectable serum hepatitis B virus (HBV) DNA levels still have a higher risk of developing HCC.4,5Complete viral suppression also leads to histological improvement and regression of liver fibrosis and cirrhosis.6,7
Immunotherapy for metastatic cancer can be complicated by the onset of hepatic immune-related adverse events (IRAEs). This study compared hepatic IRAEs associated with anti-programmed cell death protein 1 (PD-1)/PD ligand 1 (PD-L1) and anti-cytotoxic T lymphocyte antigen 4 (CTLA-4) monoclonal antibodies (mAbs). Among 536 patients treated with anti-PD-1/PD-L1 or CTLA-4 immunotherapies, 19 (3.5%) were referred to the liver unit for grade ≥3 hepatitis. Of these patients, nine had received anti-PD-1/PD-L1 and seven had received anti-CTLA-4 mAbs, in monotherapy or in combination with anti-PD-1. Liver investigations were undertaken in these 16 patients, including viral assays, autoimmune tests and liver biopsy, histological review, and immunostaining of liver specimens. In the 16 patients included in this study, median age was 63 (range 33-84) years, and nine (56%) were female. Time between therapy initiation and hepatitis was five (range, 1–49) weeks and median number of immunotherapy injections was two (range, 1–36). No patients developed hepatic failure. Histology related to anti-CTLA-4 mAbs demonstrated granulomatous hepatitis including fibrin ring granulomas and central vein endotheliitis. Histology related to anti-PD-1/PD-L1 mAbs was characterised by lobular hepatitis. The management of hepatic IRAEs was tailored according to the severity of both the biology and histology of liver injury: six patients improved spontaneously; seven received oral corticosteroids at 0.5–1 mg/kg/day; two were maintained on 0.2 mg/kg/day corticosteroids; and one patient required pulses and 2.5 mg/kg/day of corticosteroids, and the addition of a second immunosuppressive drug. In three patients, immunotherapy was reintroduced without recurrence of liver dysfunction. Acute hepatitis resulting from immunotherapy for metastatic cancer is rare (3.5%) and, in most cases, not severe. Histological assessment can distinguish between anti-PD-1/PD-L1 and anti-CTLA-4 mAb toxicity. The severity of liver injury is helpful for tailoring patient management, which does not require systematic corticosteroid administration. Immune-modulatory therapies have dramatically improved the survival of patients with metastatic tumours.1,2During the development of cancer, the immune system becomes naturally ‘tolerant’ towards cancer cells, which are seen as part of the ‘self’.This tolerance is maintained by immune checkpoint pathways that downregulate immune functions, permitting cancer cells to evade immune attacks.3,4Monoclonal antibodies (mAbs) directed against regulatory immune checkpoint molecules that inhibit T cell activation enhance antitumour immunity.5Ipilimumab, a human Ig-G1 mAb, blocks cytotoxic T lymphocyte antigen 4 (CTLA-4).6Pembrolizumab and nivolumab, humanized IgG4 kappa and human IgG4 mAbs, respectively, block the interaction between programmed cell death protein 1 (PD-1) and the two PD ligands, PD-L1 and PD-L2, by selectively binding the PD-1 receptor.7,8Durvalumab, a human IgG1 kappa mAb, targets PD-L1.9
Immunotherapy for metastatic cancer can be complicated by the onset of hepatic immune-related adverse events (IRAEs). This study compared hepatic IRAEs associated with anti-programmed cell death protein 1 (PD-1)/PD ligand 1 (PD-L1) and anti-cytotoxic T lymphocyte antigen 4 (CTLA-4) monoclonal antibodies (mAbs). Among 536 patients treated with anti-PD-1/PD-L1 or CTLA-4 immunotherapies, 19 (3.5%) were referred to the liver unit for grade ≥3 hepatitis. Of these patients, nine had received anti-PD-1/PD-L1 and seven had received anti-CTLA-4 mAbs, in monotherapy or in combination with anti-PD-1. Liver investigations were undertaken in these 16 patients, including viral assays, autoimmune tests and liver biopsy, histological review, and immunostaining of liver specimens. In the 16 patients included in this study, median age was 63 (range 33-84) years, and nine (56%) were female. Time between therapy initiation and hepatitis was five (range, 1–49) weeks and median number of immunotherapy injections was two (range, 1–36). No patients developed hepatic failure. Histology related to anti-CTLA-4 mAbs demonstrated granulomatous hepatitis including fibrin ring granulomas and central vein endotheliitis. Histology related to anti-PD-1/PD-L1 mAbs was characterised by lobular hepatitis. The management of hepatic IRAEs was tailored according to the severity of both the biology and histology of liver injury: six patients improved spontaneously; seven received oral corticosteroids at 0.5–1 mg/kg/day; two were maintained on 0.2 mg/kg/day corticosteroids; and one patient required pulses and 2.5 mg/kg/day of corticosteroids, and the addition of a second immunosuppressive drug. In three patients, immunotherapy was reintroduced without recurrence of liver dysfunction. Acute hepatitis resulting from immunotherapy for metastatic cancer is rare (3.5%) and, in most cases, not severe. Histological assessment can distinguish between anti-PD-1/PD-L1 and anti-CTLA-4 mAb toxicity. The severity of liver injury is helpful for tailoring patient management, which does not require systematic corticosteroid administration. By unbalancing the immune system, these new immunotherapies could result in immune-related adverse events (IRAEs), which mimic autoimmune conditions.10The incidence of immune-related acute hepatitis of all grades is estimated to affect between 4% and 9% of patients treated with anti-CTLA-4 mAbs, and 18% of patients treated with the combination of anti-PD-1 and anti-CTLA-4 mAbs.11,12Liver IRAEs occur more rarely with anti-PD-1 mAbs alone, with a reported incidence of 1–4% of patients.11,13
Myeloid cell leukemia 1 (MCL1), a prosurvival member of the BCL2 protein family, has a pivotal role in human cholangiocarcinoma (CCA) cell survival. We previously reported that fibroblast growth factor receptor (FGFR) signalling mediates MCL1-dependent survival of CCA cells in vitro and in vivo. However, the mode and mechanisms of cell death in this model were not delineated. Human CCA cell lines were treated with the pan-FGFR inhibitor LY2874455 and the mode of cell death examined by several complementary assays. Mitochondrial oxidative metabolism was examined using a XF24 extracellular flux analyser. The efficiency of FGFR inhibition in patient-derived xenografts (PDX) was also assessed. CCA cells expressed two species of MCL1, a full-length form localised to the outer mitochondrial membrane, and an N terminus-truncated species compartmentalised within the mitochondrial matrix. The pan-FGFR inhibitor LY2874455 induced non-apoptotic cell death in the CCA cell lines associated with cellular depletion of both MCL1 species. The cell death was accompanied by failure of mitochondrial oxidative metabolism and was most consistent with necrosis. Enforced expression of N terminus-truncated MCL1 targeted to the mitochondrial matrix, but not full-length MCL1 targeted to the outer mitochondrial membrane, rescued cell death and mitochondrial function. LY2874455 treatment of PDX-bearing mice was associated with tumour cell loss of MCL1 and cell necrosis. FGFR inhibition induces loss of matrix MCL1, resulting in cell necrosis. These observations support a heretofore unidentified, alternative MCL1 survival function, namely prevention of cell necrosis, and have implications for treatment of human CCA. Cholangiocarcinoma (CCA) is a lethal hepatobiliary malignancy with limited therapeutic options.1,2Advances in CCA therapy will require an understanding of oncogenic signalling networks that contribute to CCA pathogenesis and could be disrupted therapeutically.Similar to other malignancies, a cardinal feature of CCA is inhibition of cell death pathways that are engaged by oncogenic signalling pathways.3Members of the BCL2 gene family encode proteins that regulate the mitochondrial or intrinsic apoptotic pathway.4Of the BCL2 prosurvival members, myeloid cell leukemia 1 (MCL1) is most frequently amplified and overexpressed in CCA,5–7 and has a pivotal role in CCA cell survival.8Hence, targeting MCL1 is a strategy for the treatment of CCA and other malignancies.9,10
Myeloid cell leukemia 1 (MCL1), a prosurvival member of the BCL2 protein family, has a pivotal role in human cholangiocarcinoma (CCA) cell survival. We previously reported that fibroblast growth factor receptor (FGFR) signalling mediates MCL1-dependent survival of CCA cells in vitro and in vivo. However, the mode and mechanisms of cell death in this model were not delineated. Human CCA cell lines were treated with the pan-FGFR inhibitor LY2874455 and the mode of cell death examined by several complementary assays. Mitochondrial oxidative metabolism was examined using a XF24 extracellular flux analyser. The efficiency of FGFR inhibition in patient-derived xenografts (PDX) was also assessed. CCA cells expressed two species of MCL1, a full-length form localised to the outer mitochondrial membrane, and an N terminus-truncated species compartmentalised within the mitochondrial matrix. The pan-FGFR inhibitor LY2874455 induced non-apoptotic cell death in the CCA cell lines associated with cellular depletion of both MCL1 species. The cell death was accompanied by failure of mitochondrial oxidative metabolism and was most consistent with necrosis. Enforced expression of N terminus-truncated MCL1 targeted to the mitochondrial matrix, but not full-length MCL1 targeted to the outer mitochondrial membrane, rescued cell death and mitochondrial function. LY2874455 treatment of PDX-bearing mice was associated with tumour cell loss of MCL1 and cell necrosis. FGFR inhibition induces loss of matrix MCL1, resulting in cell necrosis. These observations support a heretofore unidentified, alternative MCL1 survival function, namely prevention of cell necrosis, and have implications for treatment of human CCA. Anti-apoptotic BCL2 proteins, such as MCL1, block cell death by binding pro-apoptotic BCL2 effectors, such as BAX, BAK, or BCL2 homology-3 (BH3)-only proteins.4By sequestering these proteins on the outer mitochondrial membrane, MCL1 effectively blocks mitochondrial outer membrane permeabilisation (MOMP), a requisite event for the intrinsic pathway of apoptosis.4However, in addition to residing on the outer mitochondrial membrane, MCL1 can also undergo proteolytic processing at its N terminus to generate a truncated species that translocates to, and localises within, the mitochondrial matrix.11–14Whereas full-length MCL1 on the outer mitochondrial membrane functions as an anti-apoptotic protein, N-terminal-truncated MCL1 within the mitochondrial matrix regulates mitochondrial oxidative metabolism and dynamics.14
Myeloid cell leukemia 1 (MCL1), a prosurvival member of the BCL2 protein family, has a pivotal role in human cholangiocarcinoma (CCA) cell survival. We previously reported that fibroblast growth factor receptor (FGFR) signalling mediates MCL1-dependent survival of CCA cells in vitro and in vivo. However, the mode and mechanisms of cell death in this model were not delineated. Human CCA cell lines were treated with the pan-FGFR inhibitor LY2874455 and the mode of cell death examined by several complementary assays. Mitochondrial oxidative metabolism was examined using a XF24 extracellular flux analyser. The efficiency of FGFR inhibition in patient-derived xenografts (PDX) was also assessed. CCA cells expressed two species of MCL1, a full-length form localised to the outer mitochondrial membrane, and an N terminus-truncated species compartmentalised within the mitochondrial matrix. The pan-FGFR inhibitor LY2874455 induced non-apoptotic cell death in the CCA cell lines associated with cellular depletion of both MCL1 species. The cell death was accompanied by failure of mitochondrial oxidative metabolism and was most consistent with necrosis. Enforced expression of N terminus-truncated MCL1 targeted to the mitochondrial matrix, but not full-length MCL1 targeted to the outer mitochondrial membrane, rescued cell death and mitochondrial function. LY2874455 treatment of PDX-bearing mice was associated with tumour cell loss of MCL1 and cell necrosis. FGFR inhibition induces loss of matrix MCL1, resulting in cell necrosis. These observations support a heretofore unidentified, alternative MCL1 survival function, namely prevention of cell necrosis, and have implications for treatment of human CCA. Recently, it was reported that pharmacological inhibition of fibroblast growth factor receptor (FGFR) signalling resulted in cell death that was accompanied by loss of MCL1 mRNA and protein, and was rescued by enforced MCL1 expression.15However, the cell death process was not characterised by morphological features of apoptosis, and biochemically was accompanied by reductions in cellular oxygen consumption and cellular ATP levels, observations more consistent with cell death by necrosis than by apoptosis.16The mode of cell death is therapeutically relevant, because it provides mechanistic insight for the development of combinatorial therapy.For example, necrosis is thought to be a more immunogenic cell death than is apoptosis, and combining immunotherapy with drugs inducing necrosis could be synergistic.
Acute-on-chronic liver failure (ACLF) is a syndrome of systemic inflammation and organ failures. Obesity, also characterized by chronic inflammation, is a risk factor among patients with cirrhosis for decompensation, infection, and mortality. Our aim was to test the hypothesis that obesity predisposes patients with decompensated cirrhosis to the development of ACLF. We examined the United Network for Organ Sharing (UNOS) database, from 2005–2016, characterizing patients at wait-listing as non-obese (body mass index [BMI] <30), obese class I-II (BMI 30–39.9) and obese class III (BMI ≥40). ACLF was determined based on the CANONIC study definition. We used Cox proportional hazards regression to assess the association between obesity and ACLF development at liver transplantation (LT). We confirmed our findings using the Nationwide Inpatient Sample (NIS), years 2009–2013, using validated diagnostic coding algorithms to identify obesity, hepatic decompensation and ACLF. Logistic regression evaluated the association between obesity and ACLF occurrence. Among 387,884 patient records of decompensated cirrhosis, 116,704 (30.1%) were identified as having ACLF in both databases. Multivariable modeling from the UNOS database revealed class III obesity to be an independent risk factor for ACLF at LT (hazard ratio 1.24; 95% CI 1.09–1.41; p <0.001). This finding was confirmed using the NIS (odds ratio 1.30; 95% CI 1.25–1.35; p <0.001). Regarding specific organ failures, analysis of both registries demonstrated patients with class I-II and class III obesity had a greater prevalence of renal failure. Class III obesity is a newly identified risk factor for ACLF development in patients with decompensated cirrhosis. Obese patients have a particularly high prevalence of renal failure as a component of ACLF. These findings have important implications regarding stratifying risk and preventing the occurrence of ACLF. Acute-on-chronic liver failure (ACLF) is a syndrome that occurs in patients with cirrhosis, characterized by acute hepatic decompensation, organ system failure, and 28-day mortality of greater than 15%.1The pathophysiology of ACLF has not been fully elucidated, but appears to be a consequence of a dysregulated inflammatory response, resulting in rapidly evolving organ failure and mortality.2–6The reported prevalence of ACLF among those hospitalized with decompensated cirrhosis approaches 30%1 and associated healthcare costs of ACLF are as high as $1.7 billion in the US.7Considering the high prevalence of this condition, along with the associated mortality and healthcare burden, identifying modifiable risk factors for ACLF is of high importance.
Acute-on-chronic liver failure (ACLF) is a syndrome of systemic inflammation and organ failures. Obesity, also characterized by chronic inflammation, is a risk factor among patients with cirrhosis for decompensation, infection, and mortality. Our aim was to test the hypothesis that obesity predisposes patients with decompensated cirrhosis to the development of ACLF. We examined the United Network for Organ Sharing (UNOS) database, from 2005–2016, characterizing patients at wait-listing as non-obese (body mass index [BMI] <30), obese class I-II (BMI 30–39.9) and obese class III (BMI ≥40). ACLF was determined based on the CANONIC study definition. We used Cox proportional hazards regression to assess the association between obesity and ACLF development at liver transplantation (LT). We confirmed our findings using the Nationwide Inpatient Sample (NIS), years 2009–2013, using validated diagnostic coding algorithms to identify obesity, hepatic decompensation and ACLF. Logistic regression evaluated the association between obesity and ACLF occurrence. Among 387,884 patient records of decompensated cirrhosis, 116,704 (30.1%) were identified as having ACLF in both databases. Multivariable modeling from the UNOS database revealed class III obesity to be an independent risk factor for ACLF at LT (hazard ratio 1.24; 95% CI 1.09–1.41; p <0.001). This finding was confirmed using the NIS (odds ratio 1.30; 95% CI 1.25–1.35; p <0.001). Regarding specific organ failures, analysis of both registries demonstrated patients with class I-II and class III obesity had a greater prevalence of renal failure. Class III obesity is a newly identified risk factor for ACLF development in patients with decompensated cirrhosis. Obese patients have a particularly high prevalence of renal failure as a component of ACLF. These findings have important implications regarding stratifying risk and preventing the occurrence of ACLF. Characterized by a chronic low-grade inflammatory state, obesity has been identified as an independent risk factor for liver decompensation and infection in cirrhosis.8,9Class III obesity, in particular, has been found to be a risk factor for mortality among those awaiting liver transplantation (LT), according to two recent United Network for Organ Sharing (UNOS) registry studies.10,11In a recent study by Piano et al., several predictors of ACLF development were identified, including ascites, anemia, hypotension and increasing model for end-stage liver disease (MELD) score.12However, this study did not include obese patients, as the upper limit of body mass index (BMI) in their study population was 29 kg/m2.Therefore, whether obesity similarly increases the risk of ACLF development, has yet to be determined.
Liver cancer is a common malignant neoplasm worldwide. The etiologies for liver cancer are diverse and the incidence trends of liver cancer caused by specific etiologies are rarely studied. We therefore aimed to determine the pattern of liver cancer incidence, as well as temporal trends. We collected detailed information on liver cancer etiology between 1990–2016, derived from the Global Burden of Disease study in 2016. Estimated annual percentage changes (EAPCs) in liver cancer age standardized incidence rate (ASR), by sex, region, and etiology, were calculated to quantify the temporal trends in liver cancer ASR. Globally, incident cases of liver cancer increased 114.0% from 471,000 in 1990 to 1,007,800 in 2016. The overall ASR increased by an average 0.34% (95% CI 0.22%–0.45%) per year in this period. The ASR of liver cancer due to hepatitis B, hepatitis C, and other causes increased between 1990 and 2016. The corresponding EAPCs were 0.22 (95% CI 0.08–0.36), 0.57 (95% CI 0.48–0.66), and 0.51 (95% CI 0.41–0.62), respectively. The ASR of liver cancer due to reported alcohol use remained stable (EAPC = 0.10, 95% CI −0.06–0.25). This increasing pattern was heterogeneous across regions and countries. The most pronounced increases were generally observed in countries with a high socio-demographic index, including the Netherlands, the UK, and the USA. Liver cancer remains a major public health concern globally, though control of hepatitis B and C virus infections has contributed to the decreasing incidence in some regions. We observed an unfavorable trend in countries with a high socio-demographic index, suggesting that current prevention strategies should be reoriented, and much more targeted and specific strategies should be established in some countries to forestall the increase in liver cancer. Liver cancer is a common lethal malignancy that afflicts in excess of 1 million people and caused 800,000 deaths globally in 2016.1It has been well documented that the incidence of liver cancer varies considerably across the world, with the highest incidence observed in East Asia.In contrast, the incidence in America is nearly 5- to 10-fold lower than the incidence observed in East Asia.2Recent decreases in the incidence of liver cancer have been reported in China and Japan.3–5However, newly diagnosed cases and the age standardized incidence rate of liver cancer have increased on a global level during the last few decades, albeit significant public health efforts have been made to counter this problem.2,6,7
Liver cancer is a common malignant neoplasm worldwide. The etiologies for liver cancer are diverse and the incidence trends of liver cancer caused by specific etiologies are rarely studied. We therefore aimed to determine the pattern of liver cancer incidence, as well as temporal trends. We collected detailed information on liver cancer etiology between 1990–2016, derived from the Global Burden of Disease study in 2016. Estimated annual percentage changes (EAPCs) in liver cancer age standardized incidence rate (ASR), by sex, region, and etiology, were calculated to quantify the temporal trends in liver cancer ASR. Globally, incident cases of liver cancer increased 114.0% from 471,000 in 1990 to 1,007,800 in 2016. The overall ASR increased by an average 0.34% (95% CI 0.22%–0.45%) per year in this period. The ASR of liver cancer due to hepatitis B, hepatitis C, and other causes increased between 1990 and 2016. The corresponding EAPCs were 0.22 (95% CI 0.08–0.36), 0.57 (95% CI 0.48–0.66), and 0.51 (95% CI 0.41–0.62), respectively. The ASR of liver cancer due to reported alcohol use remained stable (EAPC = 0.10, 95% CI −0.06–0.25). This increasing pattern was heterogeneous across regions and countries. The most pronounced increases were generally observed in countries with a high socio-demographic index, including the Netherlands, the UK, and the USA. Liver cancer remains a major public health concern globally, though control of hepatitis B and C virus infections has contributed to the decreasing incidence in some regions. We observed an unfavorable trend in countries with a high socio-demographic index, suggesting that current prevention strategies should be reoriented, and much more targeted and specific strategies should be established in some countries to forestall the increase in liver cancer. The underlying etiologies for liver cancer have been extensively investigated and widely recognized in previous epidemiological studies.8Therefore the heterogeneous incidence pattern of liver cancer was mainly determined by the prevalence of risk factors across different regions.For example, endemic hepatitis B virus (HBV) and development of chronic hepatitis B infection has been the main driver of liver cancer in China.9,10Whereas in South Korea and Japan, liver cancer is mainly caused by hepatitis C virus (HCV) infection, with HBV infection only accounting for approximately 15–20% of the total cases.11Knowing the pattern of liver cancer incidence as well as the temporal trends facilitates the initiation of more targeted prevention strategies, thereby promoting the precise prevention of liver cancer.
In vitro, cell function can be potently regulated by the mechanical properties of cells and of their microenvironment. Cells measure these features by developing forces via their actomyosin cytoskeleton, and respond accordingly by regulating intracellular pathways, including the transcriptional coactivators YAP/TAZ. Whether mechanical cues are relevant for in vivo regulation of adult organ homeostasis, and whether this occurs through YAP/TAZ, remains largely unaddressed. We developed Capzb conditional knockout mice and obtained primary fibroblasts to characterize the role of CAPZ in vitro. In vivo functional analyses were carried out by inducing Capzb inactivation in adult hepatocytes, manipulating YAP/Hippo activity by hydrodynamic tail vein injections, and treating mice with the ROCK inhibitor, fasudil. We found that the F-actin capping protein CAPZ restrains actomyosin contractility: Capzb inactivation alters stress fiber and focal adhesion dynamics leading to enhanced myosin activity, increased traction forces, and increased liver stiffness. In vitro, this rescues YAP from inhibition by a small cellular geometry; in vivo, it induces YAP activation in parallel to the Hippo pathway, causing extensive hepatocyte proliferation and leading to striking organ overgrowth. Moreover, Capzb is required for the maintenance of the differentiated hepatocyte state, for metabolic zonation, and for gluconeogenesis. In keeping with changes in tissue mechanics, inhibition of the contractility regulator ROCK, or deletion of the Yap1 mechanotransducer, reverse the phenotypes emerging in Capzb-null livers. These results indicate a previously unsuspected role for CAPZ in tuning the mechanical properties of cells and tissues, which is required in hepatocytes for the maintenance of the differentiated state and to regulate organ size. More generally, it indicates for the first time that mechanotransduction has a physiological role in maintaining liver homeostasis in mammals. Cell behavior is powerfully regulated by the mechanical properties of the microenvironment.For example, seminal studies indicated that extracellular matrix (ECM) stiffness and the resulting cell geometry can drive the choice between proliferation, cell death or differentiation, often dominating soluble cues and oncogenes.1–7The current model explaining these observations is that cells probe the physical properties of the microenvironment by exerting contractile forces on adhesion complexes generated by their actomyosin cytoskeleton.8–12In turn, actomyosin contractility regulates intracellular signaling pathways to regulate cell behavior.
In vitro, cell function can be potently regulated by the mechanical properties of cells and of their microenvironment. Cells measure these features by developing forces via their actomyosin cytoskeleton, and respond accordingly by regulating intracellular pathways, including the transcriptional coactivators YAP/TAZ. Whether mechanical cues are relevant for in vivo regulation of adult organ homeostasis, and whether this occurs through YAP/TAZ, remains largely unaddressed. We developed Capzb conditional knockout mice and obtained primary fibroblasts to characterize the role of CAPZ in vitro. In vivo functional analyses were carried out by inducing Capzb inactivation in adult hepatocytes, manipulating YAP/Hippo activity by hydrodynamic tail vein injections, and treating mice with the ROCK inhibitor, fasudil. We found that the F-actin capping protein CAPZ restrains actomyosin contractility: Capzb inactivation alters stress fiber and focal adhesion dynamics leading to enhanced myosin activity, increased traction forces, and increased liver stiffness. In vitro, this rescues YAP from inhibition by a small cellular geometry; in vivo, it induces YAP activation in parallel to the Hippo pathway, causing extensive hepatocyte proliferation and leading to striking organ overgrowth. Moreover, Capzb is required for the maintenance of the differentiated hepatocyte state, for metabolic zonation, and for gluconeogenesis. In keeping with changes in tissue mechanics, inhibition of the contractility regulator ROCK, or deletion of the Yap1 mechanotransducer, reverse the phenotypes emerging in Capzb-null livers. These results indicate a previously unsuspected role for CAPZ in tuning the mechanical properties of cells and tissues, which is required in hepatocytes for the maintenance of the differentiated state and to regulate organ size. More generally, it indicates for the first time that mechanotransduction has a physiological role in maintaining liver homeostasis in mammals. Several biochemical pathways respond to mechanical cues.Among them, Yes-associated protein 1 (YAP) and transcriptional coactivator with PDZ-binding motif (TAZ or WWTR1) are required mediators of multiple biological responses dictated in vitro by mechanical cues and actomyosin contractility.13–15YAP/TAZ function as transcriptional coactivators together with the TEAD family of transcription factors, and their activity is regulated by upstream inputs including the Hippo cascade, centered on the LATS1/2 kinases.16,17In vivo, the function of YAP and of YAP-regulatory inputs has been studied with great detail in the liver tissue, where YAP activation leads to hallmark phenotypes.18–29
Donation after circulatory death (DCD) in the UK has tripled in the last decade. However, outcomes following DCD liver transplantation are worse than for donation after brainstem death (DBD) liver transplants. This study examines whether a recipient should accept a “poorer quality” DCD organ or wait longer for a “better” DBD organ. Data were collected on 5,825 patients who were registered on the elective waiting list for a first adult liver-only transplant and 3,949 patients who received a liver-only transplant in the UK between 1 January 2008 and 31 December 2015. Survival following deceased donor liver transplantation performed between 2008 and 2015 was compared by Cox regression modelling to assess the impact on patient survival of accepting a DCD liver compared to deferring for a potential DBD transplant. A total of 953 (23%) of the 3,949 liver transplantations performed utilised DCD donors. Five-year post-transplant survival was worse following DCD than DBD transplantation (69.1% [DCD] vs. 78.3% [DBD]; p <0.0001: adjusted hazard ratio [HR] 1.65; 95% CI 1.40–1.94). Of the 5,798 patients registered on the transplant list, 1,325 (23%) died or were removed from the list without receiving a transplant. Patients who received DCD livers had a lower risk-adjusted hazard of death than those who remained on the waiting list for a potential DBD organ (adjusted HR 0.55; 95% CI 0.47–0.65). The greatest survival benefit was in those with the most advanced liver disease (adjusted HR 0.19; 95% CI 0.07–0.50). Although DCD liver transplantation leads to worse transplant outcomes than DBD transplantation, the individual’s survival is enhanced by accepting a DCD offer, particularly for patients with more severe liver disease. DCD liver transplantation improves overall survival for UK listed patients and should be encouraged. Rates of liver failure are increasing dramatically in the UK1 and over a million people worldwide die of cirrhosis every year.2Liver transplantation is the only effective treatment for end-stage liver disease and provides an average of 17–22 years of additional life.1,3,4Access to liver transplantation is limited by donor organ availability, and over the last decade, as the incidence of liver disease has increased,1 the number of patients on the liver transplant waiting list in the UK has roughly doubled.Consequently, after 2 years, about 13% of listed patients will no longer be eligible for liver transplantation because of death or deterioration in their condition.5,6These waiting list pressures have prompted a focus on the use of organs from donation after circulatory death (DCD) donors, with numbers of DCD donors increasing markedly over the last decade, such that they now almost match annual numbers of donation after brain death (DBD) donors in the UK.5Worldwide, only the Netherlands achieves similar numbers of DCD donors per million population.7
Donation after circulatory death (DCD) in the UK has tripled in the last decade. However, outcomes following DCD liver transplantation are worse than for donation after brainstem death (DBD) liver transplants. This study examines whether a recipient should accept a “poorer quality” DCD organ or wait longer for a “better” DBD organ. Data were collected on 5,825 patients who were registered on the elective waiting list for a first adult liver-only transplant and 3,949 patients who received a liver-only transplant in the UK between 1 January 2008 and 31 December 2015. Survival following deceased donor liver transplantation performed between 2008 and 2015 was compared by Cox regression modelling to assess the impact on patient survival of accepting a DCD liver compared to deferring for a potential DBD transplant. A total of 953 (23%) of the 3,949 liver transplantations performed utilised DCD donors. Five-year post-transplant survival was worse following DCD than DBD transplantation (69.1% [DCD] vs. 78.3% [DBD]; p <0.0001: adjusted hazard ratio [HR] 1.65; 95% CI 1.40–1.94). Of the 5,798 patients registered on the transplant list, 1,325 (23%) died or were removed from the list without receiving a transplant. Patients who received DCD livers had a lower risk-adjusted hazard of death than those who remained on the waiting list for a potential DBD organ (adjusted HR 0.55; 95% CI 0.47–0.65). The greatest survival benefit was in those with the most advanced liver disease (adjusted HR 0.19; 95% CI 0.07–0.50). Although DCD liver transplantation leads to worse transplant outcomes than DBD transplantation, the individual’s survival is enhanced by accepting a DCD offer, particularly for patients with more severe liver disease. DCD liver transplantation improves overall survival for UK listed patients and should be encouraged. While this increase in DCD donor activity has transformed UK transplant practice, DCD organs are generally regarded as suboptimal, because of the additional warm ischaemic ‘hit’ they are subject to during retrieval.Published series report higher incidences of primary non-function (PNF) and ischaemic cholangiopathy following DCD liver transplantation, resulting in inferior short and long-term outcomes.8–15These poorer outcomes for DCD transplantation have undoubtedly influenced the decision to select a particular liver for transplantation, and it is notable that in the UK, a much higher proportion of kidneys than livers are transplanted from potential DCD donors.5This is emblematic of a wider challenge posed to the transplant community: whether the increasing demand for transplantation merits increased utilisation of less optimal organs that are associated with poorer outcomes.Available evidence suggests that, despite the potential for such organs to increase liver transplant numbers substantially, decline in the quality of available organs often results in decreased utilisation rates.16This may reflect that the responsible clinician often finds it difficult to justify, for a particular individual, the use of a ‘marginal’ liver organ that is associated with higher morbidity and mortality than would be anticipated with a more optimal liver graft.Such a consideration overlooks, however, the potential for excess deaths while waiting for that more optimal organ.
Donation after circulatory death (DCD) in the UK has tripled in the last decade. However, outcomes following DCD liver transplantation are worse than for donation after brainstem death (DBD) liver transplants. This study examines whether a recipient should accept a “poorer quality” DCD organ or wait longer for a “better” DBD organ. Data were collected on 5,825 patients who were registered on the elective waiting list for a first adult liver-only transplant and 3,949 patients who received a liver-only transplant in the UK between 1 January 2008 and 31 December 2015. Survival following deceased donor liver transplantation performed between 2008 and 2015 was compared by Cox regression modelling to assess the impact on patient survival of accepting a DCD liver compared to deferring for a potential DBD transplant. A total of 953 (23%) of the 3,949 liver transplantations performed utilised DCD donors. Five-year post-transplant survival was worse following DCD than DBD transplantation (69.1% [DCD] vs. 78.3% [DBD]; p <0.0001: adjusted hazard ratio [HR] 1.65; 95% CI 1.40–1.94). Of the 5,798 patients registered on the transplant list, 1,325 (23%) died or were removed from the list without receiving a transplant. Patients who received DCD livers had a lower risk-adjusted hazard of death than those who remained on the waiting list for a potential DBD organ (adjusted HR 0.55; 95% CI 0.47–0.65). The greatest survival benefit was in those with the most advanced liver disease (adjusted HR 0.19; 95% CI 0.07–0.50). Although DCD liver transplantation leads to worse transplant outcomes than DBD transplantation, the individual’s survival is enhanced by accepting a DCD offer, particularly for patients with more severe liver disease. DCD liver transplantation improves overall survival for UK listed patients and should be encouraged. Thus, key to increased utilisation of marginal or DCD liver allografts is whether their use offers a survival advantage from the point of listing for the recipient population.
In patients with biliary atresia (BA), the rate of native liver survival (NLS) to adulthood has been reported as 14–44% worldwide. Complications related to portal hypertension (PHT) and cholangitis are common in adulthood. For those requiring liver transplantation (LT), the timing can be challenging. The aim of this study was to identify variables that could predict whether young people with BA would require LT when they are >16 years of age. This study was a single-centre retrospective analysis of 397 patients who underwent Kasai portoenterostomy (KP) between 1980–96 in the UK. After KP, 111/397 (28%) demonstrated NLS until 16 years of age. At final follow-up, 67 showed NLS when >16 years old (Group 1) and 22 required LT when >16 years old (Group 2). Laboratory, clinical and radiological parameters were collected for both groups at a median age of 16.06 years (13.6–17.4 years). The need for LT when >16 years old was associated with higher total bilirubin (hazard ratio 1.03, p = 0.019) and lower creatinine (hazard ratio 0.95, p = 0.040), at 16 years, on multivariate analysis. Receiver-operating characteristic curve analysis demonstrated that a total bilirubin level of ≥21 µmol/L at 16 years old (AUROC = 0.848) predicted the need for LT when >16 years old, with 85% sensitivity and 74% specificity. Cholangitis episode(s) during adolescence were associated with a 5-fold increased risk of needing LT when >16 years old. The presence of PHT or gastro-oesophageal varices in patients <16 years old was associated with a 7-fold and 8.6-fold increase in the risk of needing LT, respectively. BA in adulthood requires specialised management. Adult liver disease scoring models are not appropriate for this cohort. Bilirubin ≥21 µmol/L, PHT or gastro-oesophageal varices at 16 years, and cholangitis in adolescence, can predict the need for future LT in young people with BA. Low creatinine at 16 years also has potential prognostic value. Biliary atresia (BA) is an idiopathic neonatal cholangiopathy characterized by progressive inflammatory obliteration of the intrahepatic or extrahepatic bile ducts.1The surgical procedure, Kasai portoenterostomy (KP), aims to restore bile flow, and alleviate jaundice.Liver transplantation (LT) is performed in cases where KP is unable to salvage the native liver, with complications including jaundice, cholangitis, portal hypertension (PHT) and/or synthetic failure.2Five and 10-year UK native liver survival (NLS) rates in BA have been documented as 46% and 40%, respectively.3
In patients with biliary atresia (BA), the rate of native liver survival (NLS) to adulthood has been reported as 14–44% worldwide. Complications related to portal hypertension (PHT) and cholangitis are common in adulthood. For those requiring liver transplantation (LT), the timing can be challenging. The aim of this study was to identify variables that could predict whether young people with BA would require LT when they are >16 years of age. This study was a single-centre retrospective analysis of 397 patients who underwent Kasai portoenterostomy (KP) between 1980–96 in the UK. After KP, 111/397 (28%) demonstrated NLS until 16 years of age. At final follow-up, 67 showed NLS when >16 years old (Group 1) and 22 required LT when >16 years old (Group 2). Laboratory, clinical and radiological parameters were collected for both groups at a median age of 16.06 years (13.6–17.4 years). The need for LT when >16 years old was associated with higher total bilirubin (hazard ratio 1.03, p = 0.019) and lower creatinine (hazard ratio 0.95, p = 0.040), at 16 years, on multivariate analysis. Receiver-operating characteristic curve analysis demonstrated that a total bilirubin level of ≥21 µmol/L at 16 years old (AUROC = 0.848) predicted the need for LT when >16 years old, with 85% sensitivity and 74% specificity. Cholangitis episode(s) during adolescence were associated with a 5-fold increased risk of needing LT when >16 years old. The presence of PHT or gastro-oesophageal varices in patients <16 years old was associated with a 7-fold and 8.6-fold increase in the risk of needing LT, respectively. BA in adulthood requires specialised management. Adult liver disease scoring models are not appropriate for this cohort. Bilirubin ≥21 µmol/L, PHT or gastro-oesophageal varices at 16 years, and cholangitis in adolescence, can predict the need for future LT in young people with BA. Low creatinine at 16 years also has potential prognostic value. Twenty-year NLS rates have been documented as 14–44% worldwide.4–12However, most long-term native liver survivors develop complications in adulthood, including cholangitis and PHT,10,13 with reported LT rates as high as 22%.10Hence, it is important to incorporate the management of these young people with BA, into adult training programmes.14Timing of LT, and acceptance onto adult waiting lists can be difficult.We know that this unique cohort can progress differently compared to other causes of chronic liver disease, and due to mostly preserved synthetic liver function, these patients often do not fulfil minimal listing criteria.We, as paediatricians, should strive to optimally inform the transition/adult services about the health status of these patients, in order to improve their future management.
In Europe, hepatitis C virus (HCV) screening still targets people at high risk of infection. We aim to determine the cost-effectiveness of expanded HCV screening in France. A Markov model simulated chronic hepatitis C (CHC) prevalence, incidence of events, quality-adjusted life years (QALYs), costs and incremental cost-effectiveness ratio (ICER) in the French general population, aged 18 to 80 years, undiagnosed for CHC for different strategies: S1 = current strategy targeting the at risk population; S2 = S1 and all men between 18 and 59 years; S3 = S1 and all individuals between 40 and 59 years; S4 = S1 and all individuals between 40 and 80 years; S5 = all individuals between 18 and 80 years (universal screening). Once CHC was diagnosed, treatment was initiated either to patients with fibrosis stage ≥F2 or regardless of fibrosis. Data were extracted from published literature, a national prevalence survey, and a previously published mathematical model. ICER were interpreted based on one or three times French GDP per capita (€32,800). Universal screening led to the lowest prevalence of CHC and incidence of events, regardless of treatment initiation. When considering treatment initiation to patients with fibrosis ≥F2, targeting all people aged 40–80 was the only cost-effective strategy at both thresholds (€26,100/QALY). When we considered treatment for all, although universal screening of all individuals aged 18–80 is associated with the highest costs, it is more effective than targeting all people aged 40–80, and cost-effective at both thresholds (€31,100/QALY). In France, universal screening is the most effective screening strategy for HCV. Universal screening is cost-effective when treatment is initiated regardless of fibrosis stage. From an individual and especially from a societal perspective of HCV eradication, this strategy should be implemented. In Europe, recommendations for hepatitis C virus (HCV) screening still target only people at high risk of infection.1With this screening strategy, around 40% of infections are still not detected in France,2 equating to 75,000 individuals in 2014.3
In Europe, hepatitis C virus (HCV) screening still targets people at high risk of infection. We aim to determine the cost-effectiveness of expanded HCV screening in France. A Markov model simulated chronic hepatitis C (CHC) prevalence, incidence of events, quality-adjusted life years (QALYs), costs and incremental cost-effectiveness ratio (ICER) in the French general population, aged 18 to 80 years, undiagnosed for CHC for different strategies: S1 = current strategy targeting the at risk population; S2 = S1 and all men between 18 and 59 years; S3 = S1 and all individuals between 40 and 59 years; S4 = S1 and all individuals between 40 and 80 years; S5 = all individuals between 18 and 80 years (universal screening). Once CHC was diagnosed, treatment was initiated either to patients with fibrosis stage ≥F2 or regardless of fibrosis. Data were extracted from published literature, a national prevalence survey, and a previously published mathematical model. ICER were interpreted based on one or three times French GDP per capita (€32,800). Universal screening led to the lowest prevalence of CHC and incidence of events, regardless of treatment initiation. When considering treatment initiation to patients with fibrosis ≥F2, targeting all people aged 40–80 was the only cost-effective strategy at both thresholds (€26,100/QALY). When we considered treatment for all, although universal screening of all individuals aged 18–80 is associated with the highest costs, it is more effective than targeting all people aged 40–80, and cost-effective at both thresholds (€31,100/QALY). In France, universal screening is the most effective screening strategy for HCV. Universal screening is cost-effective when treatment is initiated regardless of fibrosis stage. From an individual and especially from a societal perspective of HCV eradication, this strategy should be implemented. The current situation supports a reassessment of the screening strategy for several reasons: (i) hepatitis C is diagnosed at an advanced stage of the disease in more than one patient out of 10, corresponding to a delay in diagnosis;4,5 (ii) recent availability of highly efficacious and well tolerated treatments (i.e.; direct-acting antiviral (DAA)-based regimens);6 (iii) in addition to standard serological tests, rapid screening tests now exist for viral hepatitis C, that can help to expand the availability of screening outside health facilities, providing new options for those who have no access to interventions implemented in healthcare facilities.7Since 2012, the US and Canada have advocated one-time testing in specific birth cohorts, corresponding to those with the highest prevalence of hepatitis C.8–10 It is however not clear that these strategies are applicable to other countries with a different HCV epidemiological profile.In addition, the cost-effectiveness analyses underlying these strategies were conducted with past treatments that were much less effective than present DAA.
In Europe, hepatitis C virus (HCV) screening still targets people at high risk of infection. We aim to determine the cost-effectiveness of expanded HCV screening in France. A Markov model simulated chronic hepatitis C (CHC) prevalence, incidence of events, quality-adjusted life years (QALYs), costs and incremental cost-effectiveness ratio (ICER) in the French general population, aged 18 to 80 years, undiagnosed for CHC for different strategies: S1 = current strategy targeting the at risk population; S2 = S1 and all men between 18 and 59 years; S3 = S1 and all individuals between 40 and 59 years; S4 = S1 and all individuals between 40 and 80 years; S5 = all individuals between 18 and 80 years (universal screening). Once CHC was diagnosed, treatment was initiated either to patients with fibrosis stage ≥F2 or regardless of fibrosis. Data were extracted from published literature, a national prevalence survey, and a previously published mathematical model. ICER were interpreted based on one or three times French GDP per capita (€32,800). Universal screening led to the lowest prevalence of CHC and incidence of events, regardless of treatment initiation. When considering treatment initiation to patients with fibrosis ≥F2, targeting all people aged 40–80 was the only cost-effective strategy at both thresholds (€26,100/QALY). When we considered treatment for all, although universal screening of all individuals aged 18–80 is associated with the highest costs, it is more effective than targeting all people aged 40–80, and cost-effective at both thresholds (€31,100/QALY). In France, universal screening is the most effective screening strategy for HCV. Universal screening is cost-effective when treatment is initiated regardless of fibrosis stage. From an individual and especially from a societal perspective of HCV eradication, this strategy should be implemented. Although France is one of the countries with the highest HCV screening level, the impact of treatment may be increased with improvement in HCV detection and therapeutic management.11Early detection can allow earlier introduction of antiviral treatment leading to a reduction in morbidity and mortality.12It can also allow a reduction in the cost of care, because an effective and early treatment may prevent progression to the costly complications, such as cirrhosis and/or hepatocellular carcinoma (HCC).13Finally, it can have a societal benefit because testing and treatment of hepatitis in particular in at-risk populations may avoid HCV transmission and help control the HCV epidemic.14Currently, in France, a group of experts, designated by the Ministry of Health to develop recommendations on HCV care and optimization of screening, recommended universal screening for hepatitis C for the purpose of elimination.15However, whether universal screening and the process of launching it will be funded by the Ministry of Health and the Haute Autorité de la Santé is not known.
Non-selective beta-blockers (NSBBs) are the mainstay of primary prophylaxis of esophageal variceal bleeding in patients with liver cirrhosis. We investigated whether non-invasive markers of portal hypertension correlate with hemodynamic responses to NSBBs in cirrhotic patients with esophageal varices. In this prospective cohort study, 106 cirrhotic patients with high-risk esophageal varices in the derivation cohort received carvedilol prophylaxis, and completed paired measurements of hepatic venous pressure gradient, liver stiffness (LS), and spleen stiffness (SS) at the beginning and end of dose titration. LS and SS were measured using acoustic radiation force impulse imaging. A prediction model for hemodynamic response was derived, and subject to an external validation in the validation cohort (63 patients). Hemodynamic response occurred in 59 patients (55.7%) in the derivation cohort, and in 33 patients (52.4%) in the validation cohort, respectively. Multivariate logistic regression analysis identified that ΔSS was the only significant predictor of hemodynamic response (odds ratio 0.039; 95% confidence interval 0.008–0.135; p <0.0001). The response prediction model (ModelΔSS = 0.0490–2.8345 × ΔSS; score = (exp[ModelΔSS])/(1 + exp[ModelΔSS]) showed good predictive performance (area under the receiver-operating characteristic curve [AUC] = 0.803) using 0.530 as the threshold value. The predictive performance of the ModelΔSS in the validation set improved using the same threshold value (AUC = 0.848). A new model based on dynamic changes in SS exhibited good performance in predicting hemodynamic response to NSBB prophylaxis in patients with high-risk esophageal varices. Gastroesophageal varices are significant challenges that are commonly faced by patients with cirrhosis.1Variceal hemorrhage occurs in 10–15% of patients annually depending on the size of varices, the presence of red wale marks, and the severity of liver disease.2In addition, acute variceal hemorrhage carries significant risks of morbidity and mortality, with reported 6-week mortality rates of 15–25%.3
Non-selective beta-blockers (NSBBs) are the mainstay of primary prophylaxis of esophageal variceal bleeding in patients with liver cirrhosis. We investigated whether non-invasive markers of portal hypertension correlate with hemodynamic responses to NSBBs in cirrhotic patients with esophageal varices. In this prospective cohort study, 106 cirrhotic patients with high-risk esophageal varices in the derivation cohort received carvedilol prophylaxis, and completed paired measurements of hepatic venous pressure gradient, liver stiffness (LS), and spleen stiffness (SS) at the beginning and end of dose titration. LS and SS were measured using acoustic radiation force impulse imaging. A prediction model for hemodynamic response was derived, and subject to an external validation in the validation cohort (63 patients). Hemodynamic response occurred in 59 patients (55.7%) in the derivation cohort, and in 33 patients (52.4%) in the validation cohort, respectively. Multivariate logistic regression analysis identified that ΔSS was the only significant predictor of hemodynamic response (odds ratio 0.039; 95% confidence interval 0.008–0.135; p <0.0001). The response prediction model (ModelΔSS = 0.0490–2.8345 × ΔSS; score = (exp[ModelΔSS])/(1 + exp[ModelΔSS]) showed good predictive performance (area under the receiver-operating characteristic curve [AUC] = 0.803) using 0.530 as the threshold value. The predictive performance of the ModelΔSS in the validation set improved using the same threshold value (AUC = 0.848). A new model based on dynamic changes in SS exhibited good performance in predicting hemodynamic response to NSBB prophylaxis in patients with high-risk esophageal varices. Primary prophylaxis for variceal hemorrhage using non-selective beta-blockers (NSBBs) has shown efficacy in patients with cirrhosis and high-risk varices.4Reducing the hepatic venous pressure gradient (HVPG) by NSBBs has been shown to be associated with a decreased risk of variceal hemorrhage and ascites, along with other benefits including amelioration of bacterial translocation and prevention of spontaneous bacterial peritonitis, resulting in positive effects on survival.5–9However, it is difficult to routinely measure HVPG in cirrhotic patients who receive prophylactic NSBBs in many institutions.
Non-selective beta-blockers (NSBBs) are the mainstay of primary prophylaxis of esophageal variceal bleeding in patients with liver cirrhosis. We investigated whether non-invasive markers of portal hypertension correlate with hemodynamic responses to NSBBs in cirrhotic patients with esophageal varices. In this prospective cohort study, 106 cirrhotic patients with high-risk esophageal varices in the derivation cohort received carvedilol prophylaxis, and completed paired measurements of hepatic venous pressure gradient, liver stiffness (LS), and spleen stiffness (SS) at the beginning and end of dose titration. LS and SS were measured using acoustic radiation force impulse imaging. A prediction model for hemodynamic response was derived, and subject to an external validation in the validation cohort (63 patients). Hemodynamic response occurred in 59 patients (55.7%) in the derivation cohort, and in 33 patients (52.4%) in the validation cohort, respectively. Multivariate logistic regression analysis identified that ΔSS was the only significant predictor of hemodynamic response (odds ratio 0.039; 95% confidence interval 0.008–0.135; p <0.0001). The response prediction model (ModelΔSS = 0.0490–2.8345 × ΔSS; score = (exp[ModelΔSS])/(1 + exp[ModelΔSS]) showed good predictive performance (area under the receiver-operating characteristic curve [AUC] = 0.803) using 0.530 as the threshold value. The predictive performance of the ModelΔSS in the validation set improved using the same threshold value (AUC = 0.848). A new model based on dynamic changes in SS exhibited good performance in predicting hemodynamic response to NSBB prophylaxis in patients with high-risk esophageal varices. Shortcomings of HVPG measurements, such as invasiveness and expensiveness, have led to numerous studies on non-invasive measurements of portal hypertension in the last decade.Measurements of liver stiffness (LS) using transient elastography (TE) have proven its accuracy for diagnosing clinically significant portal hypertension.10In addition, spleen stiffness (SS) measured via TE has recently been suggested as a promising method to assess portal hypertension.11,12However, measuring SS using TE requires additional ultrasound examination and its measurement feasibility largely depends on the size of the spleen, which limits its wide applicability.Therefore, newer ultrasound-based elastographic methods have emerged as tools for facilitating SS measurements, including point shear wave elastography such as acoustic radiation force impulse (ARFI) imaging, and 2-dimensional real-time shear wave elastography.These newer elastographic methods have shown similar accuracy as TE for predicting portal hypertension.13,14
Caspase 8 (CASP8) is the apical initiator caspase in death receptor-mediated apoptosis. Strong evidence for a link between death receptor signaling pathways and cholestasis has recently emerged. Herein, we investigated the role of CASP8-dependent and independent pathways during experimental cholestasis. Liver injury was characterized in a cohort of human sera (n = 28) and biopsies from patients with stage IV primary biliary cholangitis. In parallel, mice with either specific deletion of Casp8 in liver parenchymal cells (Casp8Δhepa) or hepatocytes (Casp8Δhep), and mice with constitutive Ripk3 (Ripk3−/−) deletion, were subjected to surgical ligation of the common bile duct (BDL) from 2 to 28 days. Floxed (Casp8fl/fl) and Ripk3+/+ mice were used as controls. Moreover, the pan-caspase inhibitor IDN-7314 was used, and cell death mechanisms were studied in primary isolated hepatocytes. Overexpression of activated caspase 3, CASP8 and RIPK3 was characteristic of liver explants from patients with primary biliary cholangitis. Twenty-eight days after BDL, Casp8Δhepa mice showed decreased necrotic foci, serum aminotransferase levels and apoptosis along with diminished compensatory proliferation and ductular reaction. These results correlated with a decreased inflammatory profile and ameliorated liver fibrogenesis. A similar phenotype was observed in Ripk3−/− mice. IDN-7314 treatment decreased CASP8 levels but failed to prevent BDL-induced cholestasis, independently of CASP8 in hepatocytes. These findings show that intervention against CASP8 in liver parenchymal cells – specifically in cholangiocytes – might be a beneficial option for treating obstructive cholestasis, while broad pan-caspase inhibition might trigger undesirable side effects. Cholestasis refers to a decrease in bile flow due to impaired secretion by hepatocytes or obstruction of the intra- or extrahepatic bile ducts.In humans, the most prominent disease is biliary obstruction of the common bile duct by gallstones.Other diseases include primary biliary cholangitis (PBC), primary sclerosing cholangitis or biliary atresia.Experimental surgical ligation of the common bile duct (BDL) is the standard experimental model of obstructive cholestatic injury.1BDL causes complete biliary obstruction and accumulation of toxic bile acids in liver and serum as observed in patients.Although this model has shed some light on the mechanisms leading to hepatocellular injury, treatment options especially for non-obstructive cholestatic liver injury need to be improved.
Caspase 8 (CASP8) is the apical initiator caspase in death receptor-mediated apoptosis. Strong evidence for a link between death receptor signaling pathways and cholestasis has recently emerged. Herein, we investigated the role of CASP8-dependent and independent pathways during experimental cholestasis. Liver injury was characterized in a cohort of human sera (n = 28) and biopsies from patients with stage IV primary biliary cholangitis. In parallel, mice with either specific deletion of Casp8 in liver parenchymal cells (Casp8Δhepa) or hepatocytes (Casp8Δhep), and mice with constitutive Ripk3 (Ripk3−/−) deletion, were subjected to surgical ligation of the common bile duct (BDL) from 2 to 28 days. Floxed (Casp8fl/fl) and Ripk3+/+ mice were used as controls. Moreover, the pan-caspase inhibitor IDN-7314 was used, and cell death mechanisms were studied in primary isolated hepatocytes. Overexpression of activated caspase 3, CASP8 and RIPK3 was characteristic of liver explants from patients with primary biliary cholangitis. Twenty-eight days after BDL, Casp8Δhepa mice showed decreased necrotic foci, serum aminotransferase levels and apoptosis along with diminished compensatory proliferation and ductular reaction. These results correlated with a decreased inflammatory profile and ameliorated liver fibrogenesis. A similar phenotype was observed in Ripk3−/− mice. IDN-7314 treatment decreased CASP8 levels but failed to prevent BDL-induced cholestasis, independently of CASP8 in hepatocytes. These findings show that intervention against CASP8 in liver parenchymal cells – specifically in cholangiocytes – might be a beneficial option for treating obstructive cholestasis, while broad pan-caspase inhibition might trigger undesirable side effects. Apoptosis and necroptosis are complementary pathways controlled by common signaling adaptors, kinases and proteases.2In fact, studies using FasR-, TNF-R1- or TRAIL-deficient mice have shown protection against cholestatic liver injury, strongly supporting a mechanistic role for death receptor family members in obstructive cholestasis.3
Caspase 8 (CASP8) is the apical initiator caspase in death receptor-mediated apoptosis. Strong evidence for a link between death receptor signaling pathways and cholestasis has recently emerged. Herein, we investigated the role of CASP8-dependent and independent pathways during experimental cholestasis. Liver injury was characterized in a cohort of human sera (n = 28) and biopsies from patients with stage IV primary biliary cholangitis. In parallel, mice with either specific deletion of Casp8 in liver parenchymal cells (Casp8Δhepa) or hepatocytes (Casp8Δhep), and mice with constitutive Ripk3 (Ripk3−/−) deletion, were subjected to surgical ligation of the common bile duct (BDL) from 2 to 28 days. Floxed (Casp8fl/fl) and Ripk3+/+ mice were used as controls. Moreover, the pan-caspase inhibitor IDN-7314 was used, and cell death mechanisms were studied in primary isolated hepatocytes. Overexpression of activated caspase 3, CASP8 and RIPK3 was characteristic of liver explants from patients with primary biliary cholangitis. Twenty-eight days after BDL, Casp8Δhepa mice showed decreased necrotic foci, serum aminotransferase levels and apoptosis along with diminished compensatory proliferation and ductular reaction. These results correlated with a decreased inflammatory profile and ameliorated liver fibrogenesis. A similar phenotype was observed in Ripk3−/− mice. IDN-7314 treatment decreased CASP8 levels but failed to prevent BDL-induced cholestasis, independently of CASP8 in hepatocytes. These findings show that intervention against CASP8 in liver parenchymal cells – specifically in cholangiocytes – might be a beneficial option for treating obstructive cholestasis, while broad pan-caspase inhibition might trigger undesirable side effects. Caspase 8 (CASP8), a cystein-aspartate protease, is the apical initiator caspase in death receptor-mediated apoptosis and constitutively expressed as a premature zymogen.After death receptor activation by its cognate ligand, the formation of a death-inducing signaling complex consisting of a death receptor, the adaptor protein Fas-associated protein with death domain (FADD), and pro-CASP8 is induced leading to activation of CASP8 by an autocatalytic process.4CASP8 can, in turn, activate downstream effector caspases such as caspase 3 (CASP3) or – as in hepatocytes – activate intrinsic pro-apoptotic pathways.5Alternatively, necroptosis is a caspase-independent form of cell death triggered by activating receptor interacting kinase 3 (RIPK3) which requires the pseudokinase MLKL.6
Hepatic encephalopathy (HE) is a syndrome of decreased vigilance and has been associated with impaired driving ability. The aim of this study was to evaluate the psychomotor vigilance task (PVT), which is used to assess both vigilance and driving ability, in a group of patients with cirrhosis and varying degrees of HE. A total of 145 patients (120 males, 59 ± 10 years, model for end-stage liver disease [MELD] score 13 ± 5) underwent the PVT; a subgroup of 117 completed a driving questionnaire and a subgroup of 106 underwent the psychometric hepatic encephalopathy score (PHES) and an electroencephalogram (EEG), based on which, plus a clinical evaluation, they were classed as being unimpaired (n = 51), or as having minimal (n = 35), or mild overt HE (n = 20). All patients were followed up for an average of 13 ± 5 months in relation to the occurrence of accidents and/or traffic offences, HE-related hospitalisations and death. Sixty-six healthy volunteers evenly distributed by sex, age and education served as a reference cohort for the PVT. Patients showed worse PVT performance compared with healthy volunteers, and PVT indices significantly correlated with MELD, ammonia levels, PHES and the EEG results. Significant associations were observed between neuropsychiatric performance/PVT indices and licence/driving status. PVT, PHES and EEG results all predicted HE-related hospitalisations and/or death over the follow-up period; none predicted accidents or traffic offences. However, individuals with the slowest reaction times and most lapses on the PVT were often not driving despite having a licence. When patients who had stopped driving for HE-related reasons (n = 6) were modelled as having an accident or fine over the subsequent 6 and 12 months, PVT was a predictor of accidents and traffic offences, even after correction for MELD and age. The PVT is worthy of further study for the purposes of both HE and driving ability assessment. Patients with hepatic encephalopathy (HE) exhibit psychomotor slowing and impairment of visuomotor coordination, inhibition and executive function, which can negatively impact on their fitness to drive.In 1995 Watanabe et al. found that 31% of patients with cirrhosis and 44% of patients with HE were unfit to drive based on their neuropsychological profiles.1Bajaj et al. recently conducted a cost-effectiveness analysis to assess the benefits of different strategies of minimal HE diagnosis and treatment for reducing accident-related costs, concluding that diagnosis by the inhibitory control test (ICT) and subsequent treatment with lactulose was the most cost-effective approach, with a significant, potential reduction in societal costs by prevented accidents.2In a standardised on-road driving test carried out by Wein et al., a professional driving instructor assessed driving performance in 14 patients with minimal HE and 34 unimpaired patients with cirrhosis.The number of times the instructor had to intervene to avoid accidents was nearly 10 times higher in patients with minimal HE compared with unimpaired patients and controls.3Kircheis et al. studied healthy controls and patients with cirrhosis with and without HE by a real driving test (multiple sensor and camera-equipped car), laboratory, in-car computerised psychometry, and a driving instructor's assessment.Patients with HE showed significantly worse performances compared with healthy controls and unimpaired patients.Moreover, they tended to overestimate their driving abilities.4In relation to this, Bajaj et al. were able to demonstrate some degree of improvement in self-assessment in patients with minimal HE who underwent driving simulation, including both testing and navigation tasks.5
Hepatic encephalopathy (HE) is a syndrome of decreased vigilance and has been associated with impaired driving ability. The aim of this study was to evaluate the psychomotor vigilance task (PVT), which is used to assess both vigilance and driving ability, in a group of patients with cirrhosis and varying degrees of HE. A total of 145 patients (120 males, 59 ± 10 years, model for end-stage liver disease [MELD] score 13 ± 5) underwent the PVT; a subgroup of 117 completed a driving questionnaire and a subgroup of 106 underwent the psychometric hepatic encephalopathy score (PHES) and an electroencephalogram (EEG), based on which, plus a clinical evaluation, they were classed as being unimpaired (n = 51), or as having minimal (n = 35), or mild overt HE (n = 20). All patients were followed up for an average of 13 ± 5 months in relation to the occurrence of accidents and/or traffic offences, HE-related hospitalisations and death. Sixty-six healthy volunteers evenly distributed by sex, age and education served as a reference cohort for the PVT. Patients showed worse PVT performance compared with healthy volunteers, and PVT indices significantly correlated with MELD, ammonia levels, PHES and the EEG results. Significant associations were observed between neuropsychiatric performance/PVT indices and licence/driving status. PVT, PHES and EEG results all predicted HE-related hospitalisations and/or death over the follow-up period; none predicted accidents or traffic offences. However, individuals with the slowest reaction times and most lapses on the PVT were often not driving despite having a licence. When patients who had stopped driving for HE-related reasons (n = 6) were modelled as having an accident or fine over the subsequent 6 and 12 months, PVT was a predictor of accidents and traffic offences, even after correction for MELD and age. The PVT is worthy of further study for the purposes of both HE and driving ability assessment. The strong relationship between HE and excessive daytime sleepiness has led to the interpretation of HE as a reduced vigilance syndrome,6 with both diagnostic and therapeutic implications.These include the possibility of diagnosing and quantifying HE using tools that were first developed for the evaluation of sleepiness.One such tool is the psychomotor vigilance task (PVT), a sustained-attention, visual reaction time task that has been widely used both in sleep medicine and the assessment of fitness to drive.7–18The PVT is a test of vigilance (i.e. the ability to maintain attention focused over a prolonged period of time).19The PVT is in fact one of the most widely used measures of vigilance, because of its simple use and the absence of bias related to familiarity with the task20 or to acquired skills, such as education or aptitude.Its sensitivity has been tested in several conditions of total or partial sleep deprivation, both in different categories of patients and in healthy individuals.21,22The PVT has also been used to assess sleepiness and predict sleepiness-related road traffic accidents in occupational clinics, particularly within the context of obstructive sleep apnoea.15By contrast, out of computerised tests used so for minimal/covert HE diagnosis: (i) The ICT is a computerised test of response inhibition and working memory.23It has been judged to have good validity but requires highly functional patients.(ii) The Stroop test evaluates psychomotor speed and cognitive flexibility by the interference between recognition reaction time to a coloured field and a written colour name;24,25 again, this is a complex task that is only applicable in highly functional patients.(iii) The SCAN Test is a computerised test that measures speed and accuracy to perform a digit recognition memory task of increasing complexity,26,27 mostly exploring working memory.(iv) The continuous reaction time (CRT) test relies on repeated registration of the motor reaction time (pressing a button) to auditory stimuli (through headphones).The most important test result is the CRT index, which measures the stability of the reaction times.Age and sex appear to exert limited influence and there are no learning and/or tiring effects.28,29This test, which has been used mostly in Denmark, is the only one that is similar to the PVT.
Non-alcoholic fatty liver disease (NAFLD) is a growing public health problem worldwide and has become an important field of biomedical inquiry. We aimed to determine whether European countries have mounted an adequate public health response to NAFLD and non-alcoholic steatohepatitis (NASH). In 2018 and 2019, NAFLD experts in 29 European countries completed an English-language survey on policies, guidelines, awareness, monitoring, diagnosis and clinical assessment in their country. The data were compiled, quality checked against existing official documents and reported descriptively. None of the 29 participating countries had written strategies or action plans for NAFLD. Two countries (7%) had mentions of NAFLD or NASH in related existing strategies (obesity and alcohol). Ten (34%) reported having national clinical guidelines specifically addressing NAFLD and, upon diagnosis, all included recommendations for the assessment of diabetes and liver cirrhosis. Eleven countries (38%) recommended screening for NAFLD in all patients with either diabetes, obesity and/or metabolic syndrome. Five countries (17%) had referral algorithms for follow-up and specialist referral in primary care, and 7 (24%) reported structured lifestyle programmes aimed at NAFLD. Seven (24%) had funded awareness campaigns that specifically included prevention of liver disease. Four countries (14%) reported having civil society groups which address NAFLD and 3 countries (10%) had national registries that include NAFLD. We found that a comprehensive public health response to NAFLD is lacking in the surveyed European countries. This includes policy in the form of a strategy, clinical guidelines, awareness campaigns, civil society involvement, and health systems organisation, including registries. Non-alcoholic fatty liver disease (NAFLD) is a growing challenge to global public health.It is defined as the increased accumulation of hepatic triglyceride (>5%) in the absence of excessive alcohol consumption or other causes of liver disease.The NAFLD spectrum encompasses steatosis (non-alcoholic fatty liver, NAFL) and non-alcoholic steatohepatitis (NASH), an inflammatory form of the condition marked by the presence of hepatocyte damage and progressive fibrosis that may lead to cirrhosis.1,2Although NAFLD may occur in patients with normal weight, it is closely associated with the presence of the metabolic syndrome, and therefore with obesity, type 2 diabetes mellitus, hypertension and dyslipidaemia.3The prevalence estimates of NAFLD vary widely according to the modality used to detect NAFLD and the geographical area.3,4Most of the larger studies on NAFLD prevalence are based on ultrasonography,5 which is insensitive to modest increases in hepatic lipid accumulation at levels <30%, and do not employ diagnostic tools recommended by current guidance (e.g. transient elastography, NAFLD fibrosis score, magnetic resonance imaging, or the gold standard, liver biopsy).1Nevertheless, a recent meta-analysis estimated the global prevalence of NAFLD to be 25%, with the highest estimates in the Middle East and South America (32% and 31%, respectively) and the lowest estimates in the African continent (14%); the estimates for Asia, the USA, and Europe were 27%, 24% and 23%, respectively.4
Non-alcoholic fatty liver disease (NAFLD) is a growing public health problem worldwide and has become an important field of biomedical inquiry. We aimed to determine whether European countries have mounted an adequate public health response to NAFLD and non-alcoholic steatohepatitis (NASH). In 2018 and 2019, NAFLD experts in 29 European countries completed an English-language survey on policies, guidelines, awareness, monitoring, diagnosis and clinical assessment in their country. The data were compiled, quality checked against existing official documents and reported descriptively. None of the 29 participating countries had written strategies or action plans for NAFLD. Two countries (7%) had mentions of NAFLD or NASH in related existing strategies (obesity and alcohol). Ten (34%) reported having national clinical guidelines specifically addressing NAFLD and, upon diagnosis, all included recommendations for the assessment of diabetes and liver cirrhosis. Eleven countries (38%) recommended screening for NAFLD in all patients with either diabetes, obesity and/or metabolic syndrome. Five countries (17%) had referral algorithms for follow-up and specialist referral in primary care, and 7 (24%) reported structured lifestyle programmes aimed at NAFLD. Seven (24%) had funded awareness campaigns that specifically included prevention of liver disease. Four countries (14%) reported having civil society groups which address NAFLD and 3 countries (10%) had national registries that include NAFLD. We found that a comprehensive public health response to NAFLD is lacking in the surveyed European countries. This includes policy in the form of a strategy, clinical guidelines, awareness campaigns, civil society involvement, and health systems organisation, including registries. NAFLD is a cause of significant morbidity and mortality, although it is not widely appreciated as being a major health threat.NAFLD-related cirrhosis can result in end-stage liver disease and hepatocellular carcinoma (HCC).6,7Recently, NAFLD became one of the main causes of liver transplantation in the United States.2,8,9While this is not yet the case in Europe, NAFLD is an increasingly common underlying cause of end-stage liver disease, which contributes substantially to hospital admissions.6,10This difference in prevalence may, at least in part, be due to an epidemiological lag in obesity rates in Europe coupled with the potential lack of recognition of the disease in European disease-coding data.11
Non-alcoholic fatty liver disease (NAFLD) is a growing public health problem worldwide and has become an important field of biomedical inquiry. We aimed to determine whether European countries have mounted an adequate public health response to NAFLD and non-alcoholic steatohepatitis (NASH). In 2018 and 2019, NAFLD experts in 29 European countries completed an English-language survey on policies, guidelines, awareness, monitoring, diagnosis and clinical assessment in their country. The data were compiled, quality checked against existing official documents and reported descriptively. None of the 29 participating countries had written strategies or action plans for NAFLD. Two countries (7%) had mentions of NAFLD or NASH in related existing strategies (obesity and alcohol). Ten (34%) reported having national clinical guidelines specifically addressing NAFLD and, upon diagnosis, all included recommendations for the assessment of diabetes and liver cirrhosis. Eleven countries (38%) recommended screening for NAFLD in all patients with either diabetes, obesity and/or metabolic syndrome. Five countries (17%) had referral algorithms for follow-up and specialist referral in primary care, and 7 (24%) reported structured lifestyle programmes aimed at NAFLD. Seven (24%) had funded awareness campaigns that specifically included prevention of liver disease. Four countries (14%) reported having civil society groups which address NAFLD and 3 countries (10%) had national registries that include NAFLD. We found that a comprehensive public health response to NAFLD is lacking in the surveyed European countries. This includes policy in the form of a strategy, clinical guidelines, awareness campaigns, civil society involvement, and health systems organisation, including registries. Modelling studies predict a steady increase in the incidence of NAFLD at the global level, accompanied by a proportionally larger increase in NASH cases, liver transplantation, HCC and mortality from liver and non-liver causes.4,12,13This, combined with the advent of highly efficacious antiviral agents against hepatitis C, has contributed to the trend towards NAFLD becoming the leading cause of liver transplantation in the near future.14The economic burden associated with the NAFLD epidemic is enormous and will continue to increase as societies become progressively affected by this global public health problem.15–17
α1-Antitrypsin deficiency (A1ATD) is an autosomal recessive disorder caused by mutations in the SERPINA1 gene. Individuals with the Z variant (Gly342Lys) retain polymerised protein in the endoplasmic reticulum (ER) of their hepatocytes, predisposing them to liver disease. The concomitant lack of circulating A1AT also causes lung emphysema. Greater insight into the mechanisms that link protein misfolding to liver injury will facilitate the design of novel therapies. Human-induced pluripotent stem cell (hiPSC)-derived hepatocytes provide a novel approach to interrogate the molecular mechanisms of A1ATD because of their patient-specific genetic architecture and reflection of human physiology. To that end, we utilised patient-specific hiPSC hepatocyte-like cells (ZZ-HLCs) derived from an A1ATD (ZZ) patient, which faithfully recapitulated key aspects of the disease at the molecular and cellular level. Subsequent functional and “omics” comparisons of these cells with their genetically corrected isogenic-line (RR-HLCs) and primary hepatocytes/human tissue enabled identification of new molecular markers and disease signatures. Our studies showed that abnormal A1AT polymer processing (immobilised ER components, reduced luminal protein mobility and disrupted ER cisternae) occurred heterogeneously within hepatocyte populations and was associated with disrupted mitochondrial structure, presence of the oncogenic protein AKR1B10 and two upregulated molecular clusters centred on members of inflammatory (IL-18 and Caspase-4) and unfolded protein response (Calnexin and Calreticulin) pathways. These results were validated in a second patient-specific hiPSC line. Our data identified novel pathways that potentially link the expression of Z A1AT polymers to liver disease. These findings could help pave the way towards identification of new therapeutic targets for the treatment of A1ATD. α1-Antitrypsin (A1AT) is a 52 kDa protein encoded by the SERPINA1 gene synthesised primarily by hepatocytes.1Secreted into the blood stream, it acts to control the function of neutrophil elastase, particularly in the lung.2A1AT also exerts anti-apoptotic and anti-inflammatory properties during inflammation and hepatic injury.Most people carry the wild-type M allele, while the rarer Z variant (found in 1–3% of the population), is associated with the most common and severe form of clinically significant A1AT deficiency (A1ATD).3The Z allele is caused by a Glu342Lys mutation in exon 5 of the SERPINA1 gene, leading to conformational instability within the protein.4Approximately 70% of synthesised Z A1AT is degraded by intracellular quality control mechanisms, 15% is secreted whilst the remaining 15% accumulates in hepatocytes as ordered polymers.These polymers are associated with neonatal hepatitis, cirrhosis and hepatocellular carcinoma.Furthermore, the significant reduction in circulating plasma A1AT levels leads to uncontrolled proteolytic activity within the lung and development of early onset panlobular emphysema.5
α1-Antitrypsin deficiency (A1ATD) is an autosomal recessive disorder caused by mutations in the SERPINA1 gene. Individuals with the Z variant (Gly342Lys) retain polymerised protein in the endoplasmic reticulum (ER) of their hepatocytes, predisposing them to liver disease. The concomitant lack of circulating A1AT also causes lung emphysema. Greater insight into the mechanisms that link protein misfolding to liver injury will facilitate the design of novel therapies. Human-induced pluripotent stem cell (hiPSC)-derived hepatocytes provide a novel approach to interrogate the molecular mechanisms of A1ATD because of their patient-specific genetic architecture and reflection of human physiology. To that end, we utilised patient-specific hiPSC hepatocyte-like cells (ZZ-HLCs) derived from an A1ATD (ZZ) patient, which faithfully recapitulated key aspects of the disease at the molecular and cellular level. Subsequent functional and “omics” comparisons of these cells with their genetically corrected isogenic-line (RR-HLCs) and primary hepatocytes/human tissue enabled identification of new molecular markers and disease signatures. Our studies showed that abnormal A1AT polymer processing (immobilised ER components, reduced luminal protein mobility and disrupted ER cisternae) occurred heterogeneously within hepatocyte populations and was associated with disrupted mitochondrial structure, presence of the oncogenic protein AKR1B10 and two upregulated molecular clusters centred on members of inflammatory (IL-18 and Caspase-4) and unfolded protein response (Calnexin and Calreticulin) pathways. These results were validated in a second patient-specific hiPSC line. Our data identified novel pathways that potentially link the expression of Z A1AT polymers to liver disease. These findings could help pave the way towards identification of new therapeutic targets for the treatment of A1ATD. Despite the recognition of A1ATD over 50 years ago,6 the detailed molecular mechanisms linking A1AT polymer accumulation to the development of liver disease remain poorly understood.This has been hampered by the availability of primary human hepatocytes expressing wild-type and mutant forms of A1AT capable of surviving in culture long-term.Although the use of animal models and artificial A1AT-expressing cell systems have contributed significantly to improved understanding of the disease processes,7–9 multiple gene copies, interference of endogenous animal antiproteases, lack of pathological features post-polymer accumulation, and the absence of endogenous promoters to activate gene expression represent challenges for translating new findings in these model systems to man.
α1-Antitrypsin deficiency (A1ATD) is an autosomal recessive disorder caused by mutations in the SERPINA1 gene. Individuals with the Z variant (Gly342Lys) retain polymerised protein in the endoplasmic reticulum (ER) of their hepatocytes, predisposing them to liver disease. The concomitant lack of circulating A1AT also causes lung emphysema. Greater insight into the mechanisms that link protein misfolding to liver injury will facilitate the design of novel therapies. Human-induced pluripotent stem cell (hiPSC)-derived hepatocytes provide a novel approach to interrogate the molecular mechanisms of A1ATD because of their patient-specific genetic architecture and reflection of human physiology. To that end, we utilised patient-specific hiPSC hepatocyte-like cells (ZZ-HLCs) derived from an A1ATD (ZZ) patient, which faithfully recapitulated key aspects of the disease at the molecular and cellular level. Subsequent functional and “omics” comparisons of these cells with their genetically corrected isogenic-line (RR-HLCs) and primary hepatocytes/human tissue enabled identification of new molecular markers and disease signatures. Our studies showed that abnormal A1AT polymer processing (immobilised ER components, reduced luminal protein mobility and disrupted ER cisternae) occurred heterogeneously within hepatocyte populations and was associated with disrupted mitochondrial structure, presence of the oncogenic protein AKR1B10 and two upregulated molecular clusters centred on members of inflammatory (IL-18 and Caspase-4) and unfolded protein response (Calnexin and Calreticulin) pathways. These results were validated in a second patient-specific hiPSC line. Our data identified novel pathways that potentially link the expression of Z A1AT polymers to liver disease. These findings could help pave the way towards identification of new therapeutic targets for the treatment of A1ATD. The advent of human-induced pluripotent stem cells (hiPSCs)10 has provided an exciting platform to address these obstacles.Indeed, PiZZ hiPSCs (ZZ-hiPSCs) derived from patients with A1ATD were shown to differentiate into hepatocyte-like cells (ZZ-HLCs) that recapitulate the modified post-translational processing and secretion kinetics of mutant A1AT.11,12Furthermore, patient-specific HLCs were genetically corrected to erase the disease signature (RR-HLCs)13 and so generated a perfect control cell line with which to perform mechanistic studies.
Glecaprevir plus pibrentasvir (G/P) is a pangenotypic, once-daily, ribavirin-free direct-acting antiviral (DAA) treatment for hepatitis C virus (HCV) infection. In nine phase II or III clinical trials, G/P therapy achieved rates of sustained virologic response 12 weeks after treatment (SVR12) of 93–100% across all six major HCV genotypes (GTs). An integrated efficacy analysis of 8- and 12-week G/P therapy in patients without cirrhosis with HCV GT 1–6 infection was performed. Data were pooled from nine phase II and III trials including patients with chronic HCV GT 1–6 infection without cirrhosis who received G/P (300 mg/120 mg) for either 8 or 12 weeks. Patients were treatment naïve or treatment experienced with peginterferon, ribavirin, and/or sofosbuvir; all patients infected with HCV GT 3 were treatment naïve. Efficacy was evaluated as the SVR12 rate. The analysis included 2,041 patients without cirrhosis. In the intent-to-treat population, 943/965 patients (98%) achieved SVR12 when treated for eight weeks, and 1,060/1,076 patients (99%) achieved SVR12 when treated for 12 weeks; the difference in rates was not significant (p = 0.2). A subgroup analysis demonstrated SVR12 rates > 95% across baseline factors traditionally associated with lower efficacy. G/P was well tolerated, with one DAA-related serious adverse event (<0.1%); grade 3 laboratory abnormalities were rare. G/P therapy for eight weeks in patients with chronic HCV GT 1–6 infection without cirrhosis achieved an overall SVR12 rate of 98% irrespective of baseline patient or viral characteristics; four additional weeks of treatment did not significantly increase the SVR12 rate, demonstrating that the optimal treatment duration in this population is eight weeks. Hepatitis C virus (HCV) infects an estimated 71–80 million individuals worldwide, and is a leading cause of cirrhosis, hepatocellular carcinoma, and liver-related deaths.1The era of direct-acting antiviral (DAA) agents for the treatment of HCV infection has resulted in a rapid and steady decline in the number of infected patients and an improvement in disease outcomes.As more patients with chronic HCV infection are being treated, it is projected that up to 75% of new patients will be HCV treatment naïve and without cirrhosis.2
Glecaprevir plus pibrentasvir (G/P) is a pangenotypic, once-daily, ribavirin-free direct-acting antiviral (DAA) treatment for hepatitis C virus (HCV) infection. In nine phase II or III clinical trials, G/P therapy achieved rates of sustained virologic response 12 weeks after treatment (SVR12) of 93–100% across all six major HCV genotypes (GTs). An integrated efficacy analysis of 8- and 12-week G/P therapy in patients without cirrhosis with HCV GT 1–6 infection was performed. Data were pooled from nine phase II and III trials including patients with chronic HCV GT 1–6 infection without cirrhosis who received G/P (300 mg/120 mg) for either 8 or 12 weeks. Patients were treatment naïve or treatment experienced with peginterferon, ribavirin, and/or sofosbuvir; all patients infected with HCV GT 3 were treatment naïve. Efficacy was evaluated as the SVR12 rate. The analysis included 2,041 patients without cirrhosis. In the intent-to-treat population, 943/965 patients (98%) achieved SVR12 when treated for eight weeks, and 1,060/1,076 patients (99%) achieved SVR12 when treated for 12 weeks; the difference in rates was not significant (p = 0.2). A subgroup analysis demonstrated SVR12 rates > 95% across baseline factors traditionally associated with lower efficacy. G/P was well tolerated, with one DAA-related serious adverse event (<0.1%); grade 3 laboratory abnormalities were rare. G/P therapy for eight weeks in patients with chronic HCV GT 1–6 infection without cirrhosis achieved an overall SVR12 rate of 98% irrespective of baseline patient or viral characteristics; four additional weeks of treatment did not significantly increase the SVR12 rate, demonstrating that the optimal treatment duration in this population is eight weeks. According to recent US and European guidelines, most approved regimens for patients with HCV infection without cirrhosis have treatment durations of 12 weeks or more.3,4Shorter treatment durations have been associated with improved adherence, which may benefit difficult-to-treat populations, such as prisoners, psychiatric patients, and injection drug users.5–8Until recently, there were only two options for a shorter, eight-week duration for patients with HCV genotype 1 infection without cirrhosis: sofosbuvir plus ledipasvir, and ombitasvir/paritaprevir/ritonavir plus dasabuvir.However, each regimen has restrictions: treatment with sofosbuvir plus ledipasvir for eight weeks is restricted by baseline HCV RNA level (<6 × 106 IU/ml), prior HCV treatment experience (treatment naïve), HIV-1 coinfection status (negative), and race (non-black);9 patients treated with ombitasvir/paritaprevir/ritonavir plus dasabuvir are eligible only for eight-week treatment in Europe, and treatment is restricted by viral subtype (genotype 1b), prior HCV treatment experience (treatment naïve), and fibrosis stage (F0–F2).10An eight-week, pangenotypic treatment option for treatment-naïve patients without cirrhosis that provides high response rates regardless of baseline patient or viral characteristics could simplify treatment decisions and potentially allow the treatment of more patients.
Glecaprevir plus pibrentasvir (G/P) is a pangenotypic, once-daily, ribavirin-free direct-acting antiviral (DAA) treatment for hepatitis C virus (HCV) infection. In nine phase II or III clinical trials, G/P therapy achieved rates of sustained virologic response 12 weeks after treatment (SVR12) of 93–100% across all six major HCV genotypes (GTs). An integrated efficacy analysis of 8- and 12-week G/P therapy in patients without cirrhosis with HCV GT 1–6 infection was performed. Data were pooled from nine phase II and III trials including patients with chronic HCV GT 1–6 infection without cirrhosis who received G/P (300 mg/120 mg) for either 8 or 12 weeks. Patients were treatment naïve or treatment experienced with peginterferon, ribavirin, and/or sofosbuvir; all patients infected with HCV GT 3 were treatment naïve. Efficacy was evaluated as the SVR12 rate. The analysis included 2,041 patients without cirrhosis. In the intent-to-treat population, 943/965 patients (98%) achieved SVR12 when treated for eight weeks, and 1,060/1,076 patients (99%) achieved SVR12 when treated for 12 weeks; the difference in rates was not significant (p = 0.2). A subgroup analysis demonstrated SVR12 rates > 95% across baseline factors traditionally associated with lower efficacy. G/P was well tolerated, with one DAA-related serious adverse event (<0.1%); grade 3 laboratory abnormalities were rare. G/P therapy for eight weeks in patients with chronic HCV GT 1–6 infection without cirrhosis achieved an overall SVR12 rate of 98% irrespective of baseline patient or viral characteristics; four additional weeks of treatment did not significantly increase the SVR12 rate, demonstrating that the optimal treatment duration in this population is eight weeks. The DAAs glecaprevir (GLE; an inhibitor of the NS3/4A protease, identified by AbbVie and Enanta) and pibrentasvir (PIB; an NS5A inhibitor) were coformulated for phase III studies to comprise the once-daily, pangenotypic regimen GLE/PIB (G/P), approved for the treatment of HCV genotype 1–6 infection.11,12Both DAAs demonstrate a high barrier to resistance and potent pangenotypic anti-HCV activity in vitro, with the half-maximal effective concentration of GLE and PIB ranging from 0.85 to 4.6 nM and from 1.4 to 5.0 pM, respectively, across HCV genotypes 1–6.13,14GLE and PIB are minimally metabolized in the liver, have negligible renal excretion, and exhibit a favourable drug–drug interaction profile with most commonly administered concomitant medications.15In phase II and III clinical trials, treatment of patients without cirrhosis and with compensated cirrhosis, including those with severe renal impairment, with the all-oral, interferon-, and ribavirin-free GLE and PIB combination regimen achieved rates of sustained virologic response (SVR; HCV RNA level below the lower limit of quantification) at post-treatment week 12 (SVR12) of 93–100% across all six major HCV genotypes.16–19
Primary sclerosing cholangitis (PSC) is an inflammatory, cholestatic and progressively fibrotic liver disease devoid of effective medical intervention. NGM282, an engineered, non-tumorigenic FGF19 analogue, potently regulates CYP7A1-mediated bile acid homeostasis. We assessed the activity and safety of NGM282 in patients with PSC. In this double-blind, placebo-controlled phase II trial, 62 patients who had PSC confirmed by cholangiography or biopsy and an elevated alkaline phosphatase (ALP) >1.5 × the upper limit of normal were randomly assigned 1:1:1 to receive NGM282 1 mg, 3 mg or placebo once daily for 12 weeks. The primary outcome was the change in ALP from baseline to week 12. Secondary and exploratory outcomes included changes in serum biomarkers of bile acid metabolism and fibrosis. Efficacy analysis was by intention-to-treat. At 12 weeks, there were no significant differences in the mean change from baseline in ALP between the NGM282 and placebo groups, and therefore, the primary endpoint was not met. However, NGM282 significantly reduced levels of 7alpha-hydroxy-4-cholesten-3-one (a marker of hepatic CYP7A1 activity, LS mean differences −6.2 ng/ml (95% CI −10.7 to −1.7; p = 0.008) and −9.4 ng/ml (−14.0 to −4.9; p <0.001) in the NGM282 1 mg and 3 mg groups, respectively, compared with placebo) and bile acids. Importantly, fibrosis biomarkers that predict transplant-free survival, including Enhanced Liver Fibrosis score and Pro-C3, were significantly improved following NGM282 treatment. Most adverse events were mild to moderate in severity, with gastrointestinal symptoms more frequent in the NGM282 treatment groups. In patients with PSC, NGM282 potently inhibited bile acid synthesis and decreased fibrosis markers, without significantly affecting ALP levels. Primary sclerosing cholangitis (PSC) is a chronic liver disease characterized by strictures of the biliary tree for which there is presently a dearth of effective medical treatment.1Of the histopathological hallmarks of PSC, periductal inflammation and “onion skin”-like fibrosis, referring to concentric layers of collagen fibers circumferential to the cholangiocyte lining of the bile ducts, characterize a progressive fibrosing cholangiopathy.2Patients frequently present with concurrent inflammatory bowel disease (IBD), and are at increased risk of developing hepatobiliary and colon cancers.More than 50% of patients need liver transplantation within 10–15 years of symptom development.1Biochemically, PSC is characterized by elevated serum liver tests, and alkaline phosphatase (ALP) levels associate with future risk of adverse events.
Primary sclerosing cholangitis (PSC) is an inflammatory, cholestatic and progressively fibrotic liver disease devoid of effective medical intervention. NGM282, an engineered, non-tumorigenic FGF19 analogue, potently regulates CYP7A1-mediated bile acid homeostasis. We assessed the activity and safety of NGM282 in patients with PSC. In this double-blind, placebo-controlled phase II trial, 62 patients who had PSC confirmed by cholangiography or biopsy and an elevated alkaline phosphatase (ALP) >1.5 × the upper limit of normal were randomly assigned 1:1:1 to receive NGM282 1 mg, 3 mg or placebo once daily for 12 weeks. The primary outcome was the change in ALP from baseline to week 12. Secondary and exploratory outcomes included changes in serum biomarkers of bile acid metabolism and fibrosis. Efficacy analysis was by intention-to-treat. At 12 weeks, there were no significant differences in the mean change from baseline in ALP between the NGM282 and placebo groups, and therefore, the primary endpoint was not met. However, NGM282 significantly reduced levels of 7alpha-hydroxy-4-cholesten-3-one (a marker of hepatic CYP7A1 activity, LS mean differences −6.2 ng/ml (95% CI −10.7 to −1.7; p = 0.008) and −9.4 ng/ml (−14.0 to −4.9; p <0.001) in the NGM282 1 mg and 3 mg groups, respectively, compared with placebo) and bile acids. Importantly, fibrosis biomarkers that predict transplant-free survival, including Enhanced Liver Fibrosis score and Pro-C3, were significantly improved following NGM282 treatment. Most adverse events were mild to moderate in severity, with gastrointestinal symptoms more frequent in the NGM282 treatment groups. In patients with PSC, NGM282 potently inhibited bile acid synthesis and decreased fibrosis markers, without significantly affecting ALP levels. There remains no single specific cause of PSC, and etio-pathogenesis is believed to encompass genetic, chemical, environmental (including microbiome factors) and immunologic pathways that selectively damage the biliary epithelium.1A prevalent “toxic bile” hypothesis posits that early pathogenesis of disease results from injury to the integrity of the biliary epithelium, leading to retention of bile acids and intrahepatic inflammation and fibrosis.Treatments that ameliorate bile acid toxicity, or that increase the efflux of bile acids, may slow the progression of PSC.3Ursodeoxycholic acid (UDCA), a hydrophilic bile acid, has been widely prescribed in PSC but without definitive evidence of its clinical benefit, seemingly having choleretic properties but not anti-fibrotic efficacy.
Primary sclerosing cholangitis (PSC) is an inflammatory, cholestatic and progressively fibrotic liver disease devoid of effective medical intervention. NGM282, an engineered, non-tumorigenic FGF19 analogue, potently regulates CYP7A1-mediated bile acid homeostasis. We assessed the activity and safety of NGM282 in patients with PSC. In this double-blind, placebo-controlled phase II trial, 62 patients who had PSC confirmed by cholangiography or biopsy and an elevated alkaline phosphatase (ALP) >1.5 × the upper limit of normal were randomly assigned 1:1:1 to receive NGM282 1 mg, 3 mg or placebo once daily for 12 weeks. The primary outcome was the change in ALP from baseline to week 12. Secondary and exploratory outcomes included changes in serum biomarkers of bile acid metabolism and fibrosis. Efficacy analysis was by intention-to-treat. At 12 weeks, there were no significant differences in the mean change from baseline in ALP between the NGM282 and placebo groups, and therefore, the primary endpoint was not met. However, NGM282 significantly reduced levels of 7alpha-hydroxy-4-cholesten-3-one (a marker of hepatic CYP7A1 activity, LS mean differences −6.2 ng/ml (95% CI −10.7 to −1.7; p = 0.008) and −9.4 ng/ml (−14.0 to −4.9; p <0.001) in the NGM282 1 mg and 3 mg groups, respectively, compared with placebo) and bile acids. Importantly, fibrosis biomarkers that predict transplant-free survival, including Enhanced Liver Fibrosis score and Pro-C3, were significantly improved following NGM282 treatment. Most adverse events were mild to moderate in severity, with gastrointestinal symptoms more frequent in the NGM282 treatment groups. In patients with PSC, NGM282 potently inhibited bile acid synthesis and decreased fibrosis markers, without significantly affecting ALP levels. Fibroblast growth factor 19 (FGF19), an endocrine gastrointestinal hormone, controls bile acid metabolism via actions on CYP7A1, the first and rate-limiting enzyme in the classic pathway of bile acid synthesis.4,5Circulating FGF19 concentration is increased in patients with PSC, further suggesting that FGF19 may represent an adaptive mechanism in PSC-related progressive liver diseases.6,7However, the therapeutic potential of FGF19 has been hindered by its hepatocarcinogenicity.8NGM282 (also known as M70), a non-tumorigenic analogue of FGF19, was designed to retain CYP7A1 suppression to reduce bile acid-associated biliary injury.9In NGM282, a 5-amino acid deletion (P24-S28) coupled with the substitution of 3 amino acids at critical positions (A30S, G31S, H33L) within the amino terminus, enable biased FGFR4 signaling so that NGM282 does not activate signal transducer and activator of transcription 3, a signaling pathway essential for FGF19-mediated hepatocarcinogenesis.10In animal models of PSC, treatment with NGM282 resulted in a rapid and robust reduction in ALP, alanine aminotransferase (ALT) and aspartate aminotransferase (AST) concentrations, as well as a clear improvement in histological features associated with PSC, including hepatic inflammation and “onion skin”-like periductal fibrosis.11NGM282 was safe and well tolerated in healthy volunteers and in patients with non-alcoholic steatohepatitis (NASH).12,13
Recently revised international guidelines for hepatocellular carcinoma (HCC) suggest that patients with inadequate ultrasonography be assessed by alternative imaging modalities. Given short scan time and the absence of contrast agent-associated risks, non-enhanced magnetic resonance imaging (MRI) has potential as a surveillance tool. This study compared the performance of non-enhanced MRI and ultrasonography for HCC surveillance in high-risk patients. We included 382 high-risk patients in a prospective cohort who underwent 1 to 3 rounds of paired gadoxetic acid-enhanced MRI and ultrasonography. Non-enhanced MRI, consisting of diffusion-weighted imaging (DWI) and T2-weighted imaging, was simulated and retrospectively analyzed, with results considered positive when lesion(s) ≥ 1cm showed diffusion restriction or mild–moderate T2 hyperintensity. Ultrasonography results were retrieved from patient records. HCC was diagnosed histologically and/or radiologically. Sensitivity, positive predictive value (PPV), specificity, and negative predictive value (NPV) were evaluated using generalized estimating equations. Forty-eight HCCs were diagnosed in 43 patients. Per-lesion and per-exam sensitivities of non-enhanced MRI were 77.1% and 79.1%, respectively, higher than ultrasonography (25.0% and 27.9%, respectively, P <0.001). Per-lesion and per-exam PPVs were higher for non-enhanced MRI (56.9% and 61.8%, respectively) than ultrasonography (16.7% and 17.7%, respectively). Specificities of non-enhanced MRI (97.9%) and ultrasonography (94.5%) differ significantly (P <0.001). NPV was higher for non-enhanced MRI (99.1%) than ultrasonography (96.9%). Estimated scan time of non-enhanced MRI was <6 minutes. Given the high performance, short scan time, and the lack of contrast agent-associated risks, non-enhanced MRI is a promising option for HCC surveillance in high-risk patients. International guidelines recommend that patients at high risk for hepatocellular carcinoma (HCC) undergo ultrasonography (US) every 6 months.1-3Recent studies, however, revealed that US had low sensitivity: only 63% in detecting early HCC and approximately 20% in detecting very early stage HCC.4-6Recently revised guidelines now suggest that selected patients with inadequate US examinations be assessed by alternative methods, such as computed tomography (CT) or magnetic resonance imaging (MRI).1-3Various HCC surveillance protocols using CT or MRI have been investigated.6-12Because of the repetitive nature of surveillance tests, the cumulative radiation hazard of periodic CT scans should not be neglected.13Gadoxetic acid-enhanced MRI has shown excellent performance in diagnosing HCC,14,15 suggesting that periodic MRI may be the best option for HCC surveillance.6However, the high cost and long imaging acquisition time of full-protocol gadoxetic acid-enhanced MRI can hamper its widespread use.Abbreviated MRI protocols, including hepatobiliary phase (HBP) imaging using gadoxetic acid without dynamic enhanced images, have therefore been tested.8,9,12Regardless of the inclusion of dynamic sequences, however, gadoxetic acid-enhanced MRI has various drawbacks associated with the use of gadolinium agent, including its long-term retention in human tissues.16,17
Protease inhibitors (PIs) are of central importance in the treatment of patients with chronic hepatitis C virus (HCV) infection. HCV NS3 protease (NS3P) position 80 displays polymorphisms associated with resistance to the PI simeprevir for HCV genotype 1a. We investigated the effects of position-80-substitutions on fitness and PI-resistance for HCV genotypes 1-6, and analyzed evolutionary mechanisms underlying viral escape mediated by pre-existing Q80K. The fitness of infectious NS3P recombinants of HCV genotypes 1-6, with engineered position-80-substitutions, was studied by comparison of viral spread kinetics in Huh-7.5 cells in culture. Median effective concentration (EC50) and fold resistance for PIs simeprevir, asunaprevir, paritaprevir, grazoprevir, glecaprevir and voxilaprevir were determined in short-term treatment assays. Viral escape was studied by long-term treatment of genotype 1a recombinants with simeprevir, grazoprevir, glecaprevir and voxilaprevir and of genotype 3a recombinants with glecaprevir and voxilaprevir, next generation sequencing, NS3P substitution linkage and haplotype analysis. Among tested PIs, only glecaprevir and voxilaprevir showed pan-genotypic activity against the original genotype 1-6 culture viruses. Variants with position-80-substitutions were all viable, but fitness depended on the specific substitution and the HCV isolate. Q80K conferred resistance to simeprevir across genotypes but had only minor effects on the activity of the remaining PIs. For genotype 1a, pre-existing Q80K mediated accelerated escape from simeprevir, grazoprevir and to a lesser extent glecaprevir, but not voxilaprevir. For genotype 3a, Q80K mediated accelerated escape from glecaprevir and voxilaprevir. Escape was mediated by rapid and genotype-, PI- and PI-concentration-dependent co-selection of clinically relevant resistance associated substitutions. Position-80-substitutions had relatively low fitness cost and the potential to promote HCV escape from clinically relevant PIs in vitro, despite having a minor impact on results in classical short-term resistance assays. Regimens based on direct-acting antivirals (DAAs) have revolutionized treatment of patients with chronic infection with hepatitis C virus (HCV), which globally has been estimated to cause 70–150 million chronic infections and at least 400,000 deaths annually.1,2Approved DAAs target the HCV protease (NS3P), NS5A and NS5B.3,4Protease inhibitors (PIs), available since 2011, constitute an important component of DAA-based combination therapies.3–5While the initially developed PIs telaprevir and boceprevir have been discontinued, simeprevir might be used for treatment of patients infected with genotype 1, and grazoprevir or paritaprevir for genotypes 1 and 4.6,7Asunaprevir, approved in Asia and the Middle East, is used only for subtype 1b.3The novel PIs glecaprevir and voxilaprevir are recommended for genotypes 1-6.6,7
Protease inhibitors (PIs) are of central importance in the treatment of patients with chronic hepatitis C virus (HCV) infection. HCV NS3 protease (NS3P) position 80 displays polymorphisms associated with resistance to the PI simeprevir for HCV genotype 1a. We investigated the effects of position-80-substitutions on fitness and PI-resistance for HCV genotypes 1-6, and analyzed evolutionary mechanisms underlying viral escape mediated by pre-existing Q80K. The fitness of infectious NS3P recombinants of HCV genotypes 1-6, with engineered position-80-substitutions, was studied by comparison of viral spread kinetics in Huh-7.5 cells in culture. Median effective concentration (EC50) and fold resistance for PIs simeprevir, asunaprevir, paritaprevir, grazoprevir, glecaprevir and voxilaprevir were determined in short-term treatment assays. Viral escape was studied by long-term treatment of genotype 1a recombinants with simeprevir, grazoprevir, glecaprevir and voxilaprevir and of genotype 3a recombinants with glecaprevir and voxilaprevir, next generation sequencing, NS3P substitution linkage and haplotype analysis. Among tested PIs, only glecaprevir and voxilaprevir showed pan-genotypic activity against the original genotype 1-6 culture viruses. Variants with position-80-substitutions were all viable, but fitness depended on the specific substitution and the HCV isolate. Q80K conferred resistance to simeprevir across genotypes but had only minor effects on the activity of the remaining PIs. For genotype 1a, pre-existing Q80K mediated accelerated escape from simeprevir, grazoprevir and to a lesser extent glecaprevir, but not voxilaprevir. For genotype 3a, Q80K mediated accelerated escape from glecaprevir and voxilaprevir. Escape was mediated by rapid and genotype-, PI- and PI-concentration-dependent co-selection of clinically relevant resistance associated substitutions. Position-80-substitutions had relatively low fitness cost and the potential to promote HCV escape from clinically relevant PIs in vitro, despite having a minor impact on results in classical short-term resistance assays. A subset of DAA-treated patients experience treatment failure, which is associated with selection of resistance-associated substitutions (RASs) that are either naturally occurring and pre-existing at baseline or rapidly acquired during treatment.3,4Several clinical studies have provided evidence that baseline RASs in DAA targets can compromise DAA treatment efficacy.4,5
Protease inhibitors (PIs) are of central importance in the treatment of patients with chronic hepatitis C virus (HCV) infection. HCV NS3 protease (NS3P) position 80 displays polymorphisms associated with resistance to the PI simeprevir for HCV genotype 1a. We investigated the effects of position-80-substitutions on fitness and PI-resistance for HCV genotypes 1-6, and analyzed evolutionary mechanisms underlying viral escape mediated by pre-existing Q80K. The fitness of infectious NS3P recombinants of HCV genotypes 1-6, with engineered position-80-substitutions, was studied by comparison of viral spread kinetics in Huh-7.5 cells in culture. Median effective concentration (EC50) and fold resistance for PIs simeprevir, asunaprevir, paritaprevir, grazoprevir, glecaprevir and voxilaprevir were determined in short-term treatment assays. Viral escape was studied by long-term treatment of genotype 1a recombinants with simeprevir, grazoprevir, glecaprevir and voxilaprevir and of genotype 3a recombinants with glecaprevir and voxilaprevir, next generation sequencing, NS3P substitution linkage and haplotype analysis. Among tested PIs, only glecaprevir and voxilaprevir showed pan-genotypic activity against the original genotype 1-6 culture viruses. Variants with position-80-substitutions were all viable, but fitness depended on the specific substitution and the HCV isolate. Q80K conferred resistance to simeprevir across genotypes but had only minor effects on the activity of the remaining PIs. For genotype 1a, pre-existing Q80K mediated accelerated escape from simeprevir, grazoprevir and to a lesser extent glecaprevir, but not voxilaprevir. For genotype 3a, Q80K mediated accelerated escape from glecaprevir and voxilaprevir. Escape was mediated by rapid and genotype-, PI- and PI-concentration-dependent co-selection of clinically relevant resistance associated substitutions. Position-80-substitutions had relatively low fitness cost and the potential to promote HCV escape from clinically relevant PIs in vitro, despite having a minor impact on results in classical short-term resistance assays. The prevalence of pre-existing RASs depends on their effect on viral fitness, influencing persistence and spread in human populations.Of described naturally occurring NS3P RASs, Q80K shows the highest prevalence: for genotype 1a-infected patients, 19%, 48% and 9% are carrying Q80K in Europe, North America, and South America, respectively.3,8Strong regional deviations in Q80K prevalence have been observed, with European prevalence rates ranging from 5% in Norway to 75% in Poland.8Q80K is rarely observed for genotype 1b and for other genotypes limited data are available.4For genotypes 1a and 1b, Q80K and the less prevalent Q80R caused resistance to simeprevir in vitro,9–11 and for 1a, baseline Q80K resulted in decreased efficacy of simeprevir-based treatment regimens.4,5For other genotypes, data on the effect of Q80K on PI efficacy are not available.4While PI RASs can confer extensive cross-resistance among different PIs, little is known about the impact of Q80K on the efficacy of PIs other than simeprevir.4Finally, molecular mechanisms underlying treatment failure mediated by baseline RASs have not been studied.
Mitochondrial dysfunction and subsequent metabolic deregulation are commonly observed in cancers including hepatocellular carcinoma (HCC). When mitochondrial function is impaired, reductive glutamine metabolism is a major cellular carbon source for de novo lipogenesis to support cancer cell growth. The underlying regulators of reductively metabolized glutamine in mitochondrial dysfunction are not completely understood in tumorigenesis including in HCC. We systematically investigated the role of oxoglutarate dehydrogenase-like (OGDHL), one of the rate-limiting components of the key mitochondrial multi-enzyme OGDH complex (OGDHC), in the regulation of lipid metabolism in hepatoma cells and explored the underlying molecular mechanisms. Lower expression of OGDHL was associated with advanced tumor stage, significantly worse survival and more frequent tumor recurrence in three independent cohorts totaling 681 postoperative HCC patients. Promoter hypermethylation and DNA copy deletion of OGDHL were independently correlated with reduced OGDHL expression in HCC specimens. Additionally, OGDHL overexpression significantly inhibited the growth of hepatoma cells as mouse xenografts while knockdown of OGDHL promoted proliferation in hepatoma cells. Mechanistically, OGDHL downregulation upregulated the α-ketoglutarate (αKG):citrate ratio by reducing OGDHC activity, which subsequently drove reductive carboxylation (RC) of glutamine-derived αKG for lipogenesis via retrograde TCA cycling in hepatoma cells. Notably, silencing of OGDHL activated the mTORC1 signaling pathway in an α-KG-dependent manner, which in turn transcriptionally induced expression of SCD1 and FASN, thus, enhancing de novo lipogenesis. Meanwhile, metabolic reprogramming in OGDHL-negative hepatoma cells provided an abundant supply of NADPH and GSH to support the cellular antioxidant system. The reduction of reductive glutamine metabolism through OGDHL overexpression or through use of glutaminase inhibitors sensitized tumor cells to sorafenib, a molecular-targeted therapy for HCC. Our findings established that silencing of OGDHL contributed to HCC development and survival by regulating glutamine metabolic pathways, and suggest OGDHL as a promising prognostic biomarker and therapeutic target for HCC. Hepatocellular carcinoma (HCC) is one of the most common malignancies worldwide, which has increased morbidity and mortality of patients with liver disorders [1].Therefore, novel causative molecular biomarkers need to be identified to improve clinical decision-making and therapy.Cancer cells undergo huge metabolic alterations to compete for nutrient-limited resources with surrounding normal cells in order to maintain rapid growth; therefore; understanding these changes is fundamental for identifying potential biomarkers and therapeutic targets in HCC [2].
Mitochondrial dysfunction and subsequent metabolic deregulation are commonly observed in cancers including hepatocellular carcinoma (HCC). When mitochondrial function is impaired, reductive glutamine metabolism is a major cellular carbon source for de novo lipogenesis to support cancer cell growth. The underlying regulators of reductively metabolized glutamine in mitochondrial dysfunction are not completely understood in tumorigenesis including in HCC. We systematically investigated the role of oxoglutarate dehydrogenase-like (OGDHL), one of the rate-limiting components of the key mitochondrial multi-enzyme OGDH complex (OGDHC), in the regulation of lipid metabolism in hepatoma cells and explored the underlying molecular mechanisms. Lower expression of OGDHL was associated with advanced tumor stage, significantly worse survival and more frequent tumor recurrence in three independent cohorts totaling 681 postoperative HCC patients. Promoter hypermethylation and DNA copy deletion of OGDHL were independently correlated with reduced OGDHL expression in HCC specimens. Additionally, OGDHL overexpression significantly inhibited the growth of hepatoma cells as mouse xenografts while knockdown of OGDHL promoted proliferation in hepatoma cells. Mechanistically, OGDHL downregulation upregulated the α-ketoglutarate (αKG):citrate ratio by reducing OGDHC activity, which subsequently drove reductive carboxylation (RC) of glutamine-derived αKG for lipogenesis via retrograde TCA cycling in hepatoma cells. Notably, silencing of OGDHL activated the mTORC1 signaling pathway in an α-KG-dependent manner, which in turn transcriptionally induced expression of SCD1 and FASN, thus, enhancing de novo lipogenesis. Meanwhile, metabolic reprogramming in OGDHL-negative hepatoma cells provided an abundant supply of NADPH and GSH to support the cellular antioxidant system. The reduction of reductive glutamine metabolism through OGDHL overexpression or through use of glutaminase inhibitors sensitized tumor cells to sorafenib, a molecular-targeted therapy for HCC. Our findings established that silencing of OGDHL contributed to HCC development and survival by regulating glutamine metabolic pathways, and suggest OGDHL as a promising prognostic biomarker and therapeutic target for HCC. Glutamine, the most abundant amino acid in blood, has been widely reported as a key source of carbon secondary only to glucose for energy production and anabolic processes [3].Recently, glutamine was shown to be vital for fueling mitochondrial metabolism to maintain rapid rates of cancer cell growth [4-6].Inhibiting mitochondrial glutamine metabolism had been found to exert strong anticancer effects [7].
Mitochondrial dysfunction and subsequent metabolic deregulation are commonly observed in cancers including hepatocellular carcinoma (HCC). When mitochondrial function is impaired, reductive glutamine metabolism is a major cellular carbon source for de novo lipogenesis to support cancer cell growth. The underlying regulators of reductively metabolized glutamine in mitochondrial dysfunction are not completely understood in tumorigenesis including in HCC. We systematically investigated the role of oxoglutarate dehydrogenase-like (OGDHL), one of the rate-limiting components of the key mitochondrial multi-enzyme OGDH complex (OGDHC), in the regulation of lipid metabolism in hepatoma cells and explored the underlying molecular mechanisms. Lower expression of OGDHL was associated with advanced tumor stage, significantly worse survival and more frequent tumor recurrence in three independent cohorts totaling 681 postoperative HCC patients. Promoter hypermethylation and DNA copy deletion of OGDHL were independently correlated with reduced OGDHL expression in HCC specimens. Additionally, OGDHL overexpression significantly inhibited the growth of hepatoma cells as mouse xenografts while knockdown of OGDHL promoted proliferation in hepatoma cells. Mechanistically, OGDHL downregulation upregulated the α-ketoglutarate (αKG):citrate ratio by reducing OGDHC activity, which subsequently drove reductive carboxylation (RC) of glutamine-derived αKG for lipogenesis via retrograde TCA cycling in hepatoma cells. Notably, silencing of OGDHL activated the mTORC1 signaling pathway in an α-KG-dependent manner, which in turn transcriptionally induced expression of SCD1 and FASN, thus, enhancing de novo lipogenesis. Meanwhile, metabolic reprogramming in OGDHL-negative hepatoma cells provided an abundant supply of NADPH and GSH to support the cellular antioxidant system. The reduction of reductive glutamine metabolism through OGDHL overexpression or through use of glutaminase inhibitors sensitized tumor cells to sorafenib, a molecular-targeted therapy for HCC. Our findings established that silencing of OGDHL contributed to HCC development and survival by regulating glutamine metabolic pathways, and suggest OGDHL as a promising prognostic biomarker and therapeutic target for HCC. Mitochondrial glutamine metabolism involves either oxidative or reductive metabolic pathways [8].During mitochondrial utilization of glutamine, α-ketoglutarate (αKG) is produced typically by glutaminase and glutamate dehydrogenase from a two-step conversion of glutamine.In turn, αKG is either oxidized to succinate by the oxoglutarate dehydrogenase complex (OGDHC) in the canonical forward mode of the tricarboxylic acid (TCA) cycle, or it can undergos reductive carboxylation (RC), thereby pushing the reverse TCA cycle towards citrate for further de novo lipogenesis [9-12].More recently, it has been shown that reductively metabolized glutamine may become a major carbon source for lipid biosynthesis during impaired mitochondrial function [11-12], and that this switch in carbon source selection was fundamental for sustaining rapid cell proliferation [10, 13].
Mitochondrial dysfunction and subsequent metabolic deregulation are commonly observed in cancers including hepatocellular carcinoma (HCC). When mitochondrial function is impaired, reductive glutamine metabolism is a major cellular carbon source for de novo lipogenesis to support cancer cell growth. The underlying regulators of reductively metabolized glutamine in mitochondrial dysfunction are not completely understood in tumorigenesis including in HCC. We systematically investigated the role of oxoglutarate dehydrogenase-like (OGDHL), one of the rate-limiting components of the key mitochondrial multi-enzyme OGDH complex (OGDHC), in the regulation of lipid metabolism in hepatoma cells and explored the underlying molecular mechanisms. Lower expression of OGDHL was associated with advanced tumor stage, significantly worse survival and more frequent tumor recurrence in three independent cohorts totaling 681 postoperative HCC patients. Promoter hypermethylation and DNA copy deletion of OGDHL were independently correlated with reduced OGDHL expression in HCC specimens. Additionally, OGDHL overexpression significantly inhibited the growth of hepatoma cells as mouse xenografts while knockdown of OGDHL promoted proliferation in hepatoma cells. Mechanistically, OGDHL downregulation upregulated the α-ketoglutarate (αKG):citrate ratio by reducing OGDHC activity, which subsequently drove reductive carboxylation (RC) of glutamine-derived αKG for lipogenesis via retrograde TCA cycling in hepatoma cells. Notably, silencing of OGDHL activated the mTORC1 signaling pathway in an α-KG-dependent manner, which in turn transcriptionally induced expression of SCD1 and FASN, thus, enhancing de novo lipogenesis. Meanwhile, metabolic reprogramming in OGDHL-negative hepatoma cells provided an abundant supply of NADPH and GSH to support the cellular antioxidant system. The reduction of reductive glutamine metabolism through OGDHL overexpression or through use of glutaminase inhibitors sensitized tumor cells to sorafenib, a molecular-targeted therapy for HCC. Our findings established that silencing of OGDHL contributed to HCC development and survival by regulating glutamine metabolic pathways, and suggest OGDHL as a promising prognostic biomarker and therapeutic target for HCC. Tumor cells exhibit severe defects in mitochondrial function induced by silencing or mutations of genes encoding key enzymes involved in the TCA cycle.These defects have been reported in fumarate hydratase in renal neoplasms and in subunits of the succinate dehydrogenase complex in gastrointestinal stromal tumors, paraganglioma and pheochromocytoma [14-18].All of these defects hampered both glucose and glutamine oxidative metabolism in the TCA cycle.Therefore, RC can substitute to support biosynthesis by enabling cells with mitochondrial dysfunction to maintain biosynthetic precursor pools that would normally be provided by oxidative metabolism [19].In this regard, the identification of alterations in genes encoding key enzymes participating in the TCA cycle has emphasized the importance of mitochondrial metabolic fluxes (such as RC of glutamine-derived αKG) in regulating both the metabolism and function of tumor cells with mitochondrial dysfunction [9].
Mitochondrial dysfunction and subsequent metabolic deregulation are commonly observed in cancers including hepatocellular carcinoma (HCC). When mitochondrial function is impaired, reductive glutamine metabolism is a major cellular carbon source for de novo lipogenesis to support cancer cell growth. The underlying regulators of reductively metabolized glutamine in mitochondrial dysfunction are not completely understood in tumorigenesis including in HCC. We systematically investigated the role of oxoglutarate dehydrogenase-like (OGDHL), one of the rate-limiting components of the key mitochondrial multi-enzyme OGDH complex (OGDHC), in the regulation of lipid metabolism in hepatoma cells and explored the underlying molecular mechanisms. Lower expression of OGDHL was associated with advanced tumor stage, significantly worse survival and more frequent tumor recurrence in three independent cohorts totaling 681 postoperative HCC patients. Promoter hypermethylation and DNA copy deletion of OGDHL were independently correlated with reduced OGDHL expression in HCC specimens. Additionally, OGDHL overexpression significantly inhibited the growth of hepatoma cells as mouse xenografts while knockdown of OGDHL promoted proliferation in hepatoma cells. Mechanistically, OGDHL downregulation upregulated the α-ketoglutarate (αKG):citrate ratio by reducing OGDHC activity, which subsequently drove reductive carboxylation (RC) of glutamine-derived αKG for lipogenesis via retrograde TCA cycling in hepatoma cells. Notably, silencing of OGDHL activated the mTORC1 signaling pathway in an α-KG-dependent manner, which in turn transcriptionally induced expression of SCD1 and FASN, thus, enhancing de novo lipogenesis. Meanwhile, metabolic reprogramming in OGDHL-negative hepatoma cells provided an abundant supply of NADPH and GSH to support the cellular antioxidant system. The reduction of reductive glutamine metabolism through OGDHL overexpression or through use of glutaminase inhibitors sensitized tumor cells to sorafenib, a molecular-targeted therapy for HCC. Our findings established that silencing of OGDHL contributed to HCC development and survival by regulating glutamine metabolic pathways, and suggest OGDHL as a promising prognostic biomarker and therapeutic target for HCC. The function of oxoglutarate dehydrogenase-like (OGDHL), one of the rate-limiting components of the OGDHC, has been reported in certain cancers.Some studies have previously demonstrated that OGDHL expression was silenced by cancer-specific promoter methylation in lung, breast, cervix, oesophagus, pancreas and colon cancers [20-21].Inactivation of OGDHL was shown to contribute to cervical tumor proliferation via activation of the AKT signaling pathway [22].Moreover, OGDHL downregulation resulted in suppression in the activity of the key mitochondrial multi-enzyme OGDHC [23], which was associated with RC of glutamine-derived αKG [24].However, neither has the role of OGDHL in altered reductive glutamine metabolism been evaluated in HCC, nor has research revealed whether silencing of OGDHL affects survival prognosis in HCC patients.
Currently, much effort is directed towards the development of new cell sources for clinical therapy using cell fate conversion by small molecules. Direct lineage reprogramming to a progenitor state has been reported in terminally differentiated rodent hepatocytes, yet remains a challenge in human hepatocytes. Human hepatocytes were isolated from healthy and diseased donor livers and reprogrammed into progenitor cells by 2 small molecules, A83-01 and CHIR99021 (AC), in the presence of EGF and HGF. The stemness properties of human chemically derived hepatic progenitors (hCdHs) were tested by standard in vitro and in vivo assays and transcriptome profiling. We developed a robust culture system for generating hCdHs with therapeutic potential. The use of HGF proved to be an essential determinant of the fate conversion process. Based on functional evidence, activation of the HGF/MET signal transduction system collaborated with A83-01 and CHIR99021 to allow a rapid expansion of progenitor cells through the activation of the ERK pathway. hCdHs expressed hepatic progenitor markers and could self-renew for at least 10 passages while retaining a normal karyotype and potential to differentiate into functional hepatocytes and biliary epithelial cells in vitro. Gene expression profiling using RNAseq confirmed the transcriptional reprogramming of hCdHs towards a progenitor state and the suppression of mature hepatocyte transcripts. Upon intrasplenic transplantation in several models of therapeutic liver repopulation, hCdHs effectively repopulated the damaged parenchyma. Our study is the first report of successful reprogramming of human hepatocytes to a population of proliferating bipotent cells with regenerative potential. hCdHs may provide a novel tool that permits expansion and genetic manipulation of patient-specific progenitors to study regeneration and the repair of diseased livers. Currently, liver transplantation represents the only approved standard of care for patients with end-stage liver diseases.1Experimental studies in rodents and clinical trials of hepatocyte transplantation have shown that direct infusion of mature hepatocytes may serve as an alternative to whole organ replacement in some cases.However, hepatocyte transplantation only results in a partial and relatively short-term correction of liver dysfunction, and has been hampered by numerous issues related to the shortage of donor tissue, limited numbers of cells suitable for transplantation, and a low efficiency of engraftment in the abnormal microenvironment of diseased livers.2–4In addition, human hepatocytes are difficult to maintain and expand in vitro because of the lack of adequate environmental signals.Typically, mature hepatocytes have low proliferative potential and easily become apoptotic in culture which reduces their therapeutic value.
Currently, much effort is directed towards the development of new cell sources for clinical therapy using cell fate conversion by small molecules. Direct lineage reprogramming to a progenitor state has been reported in terminally differentiated rodent hepatocytes, yet remains a challenge in human hepatocytes. Human hepatocytes were isolated from healthy and diseased donor livers and reprogrammed into progenitor cells by 2 small molecules, A83-01 and CHIR99021 (AC), in the presence of EGF and HGF. The stemness properties of human chemically derived hepatic progenitors (hCdHs) were tested by standard in vitro and in vivo assays and transcriptome profiling. We developed a robust culture system for generating hCdHs with therapeutic potential. The use of HGF proved to be an essential determinant of the fate conversion process. Based on functional evidence, activation of the HGF/MET signal transduction system collaborated with A83-01 and CHIR99021 to allow a rapid expansion of progenitor cells through the activation of the ERK pathway. hCdHs expressed hepatic progenitor markers and could self-renew for at least 10 passages while retaining a normal karyotype and potential to differentiate into functional hepatocytes and biliary epithelial cells in vitro. Gene expression profiling using RNAseq confirmed the transcriptional reprogramming of hCdHs towards a progenitor state and the suppression of mature hepatocyte transcripts. Upon intrasplenic transplantation in several models of therapeutic liver repopulation, hCdHs effectively repopulated the damaged parenchyma. Our study is the first report of successful reprogramming of human hepatocytes to a population of proliferating bipotent cells with regenerative potential. hCdHs may provide a novel tool that permits expansion and genetic manipulation of patient-specific progenitors to study regeneration and the repair of diseased livers. To facilitate the development of cell-based therapies for treating liver disease, over the last decade much effort has been directed towards the potential use of pluripotent stem cells capable of indefinite self-renewal, including embryonic stem cells (ESCs),5–8 induced pluripotent stem cells (iPSCs),9–11 mesenchymal stem cells (MSCs),12–14 and hepatic progenitor cells.15–18Despite the important advances in generating stem cell-derived hepatocyte-like cells from pluripotent cells, their clinical applications are impeded by their low efficiency of hepatic differentiation,19 the likelihood of immune rejection,20 high risk of cancer development,21,22 as well as low rate of proliferation and rapid loss of differentiation potency in culture.23In addition, the therapeutic use of iPSCs and ESCs could be compromised by the possibility of genetic transformation and/or ethical issues.24,25MSCs are generally reported not to form teratomas, but they have a small total cell yield and inefficient hepatic differentiation,26 which limits their potential use in clinic.More recently, a 3D organoid culture system for human liver has been established27 which allows for the generation of highly stable bipotent progenitor cells capable of bi-lineage differentiation both in vitro and in vivo, but is technically challenging due to a multi-step process of cell isolation, selection and long-term expansion.
Currently, much effort is directed towards the development of new cell sources for clinical therapy using cell fate conversion by small molecules. Direct lineage reprogramming to a progenitor state has been reported in terminally differentiated rodent hepatocytes, yet remains a challenge in human hepatocytes. Human hepatocytes were isolated from healthy and diseased donor livers and reprogrammed into progenitor cells by 2 small molecules, A83-01 and CHIR99021 (AC), in the presence of EGF and HGF. The stemness properties of human chemically derived hepatic progenitors (hCdHs) were tested by standard in vitro and in vivo assays and transcriptome profiling. We developed a robust culture system for generating hCdHs with therapeutic potential. The use of HGF proved to be an essential determinant of the fate conversion process. Based on functional evidence, activation of the HGF/MET signal transduction system collaborated with A83-01 and CHIR99021 to allow a rapid expansion of progenitor cells through the activation of the ERK pathway. hCdHs expressed hepatic progenitor markers and could self-renew for at least 10 passages while retaining a normal karyotype and potential to differentiate into functional hepatocytes and biliary epithelial cells in vitro. Gene expression profiling using RNAseq confirmed the transcriptional reprogramming of hCdHs towards a progenitor state and the suppression of mature hepatocyte transcripts. Upon intrasplenic transplantation in several models of therapeutic liver repopulation, hCdHs effectively repopulated the damaged parenchyma. Our study is the first report of successful reprogramming of human hepatocytes to a population of proliferating bipotent cells with regenerative potential. hCdHs may provide a novel tool that permits expansion and genetic manipulation of patient-specific progenitors to study regeneration and the repair of diseased livers. New technological advances in the direct reprogramming of somatic cells by a defined set of small molecules simplified and shortened the process of generating integration-free progenitor-type cells.28–30In particular, Katsuda et al. identified a combination of only three small molecules, Y-27632, A83-01, and CHIR99021, which was very effective in converting terminally differentiated rat and mouse but not human hepatocytes to bipotent progenitor cells.31
Cytokine-induced killer (CIK) cell-based immunotherapy is effective as an adjuvant therapy in early stage hepatocellular carcinoma (HCC) but lacks efficacy in advanced HCC. We aimed to investigate immune suppressor mechanisms in HCC, focusing on the role of myeloid-derived suppressor cells (MDSCs) in response to CIK therapy. MDSCs were quantified by flow cytometry and quantitative real-time PCR. Cytokines were detected by cytokine array. A lactate dehydrogenase cytotoxicity assay was performed in the presence or absence of MDSCs to study CIK function against HCC cells in vitro. An FDA-approved PDE5 inhibitor, tadalafil, was used to target MDSCs in vitro and in vivo. Two different murine HCC cell lines were tested in subcutaneous and orthotopic tumor models in C57BL/6 and BALB/c mice. The antitumor effects of human CIKs and MDSCs were also tested in vitro. Adoptive cell transfer of CIKs into tumor-bearing mice induced inflammatory mediators (e.g., CX3CL1, IL-13) in the tumor microenvironment and an increase of tumor-infiltrating MDSCs, leading to impaired antitumor activity in 2 different HCC models. MDSCs efficiently suppressed the cytotoxic activity of CIKs in vitro. In contrast, treatment with a PDE5 inhibitor reversed the MDSC suppressor function via ARG1 and iNOS blockade and systemic treatment with a PDE5 inhibitor prevented MDSC accumulation in the tumor microenvironment upon CIK cell therapy and increased its antitumor efficacy. Similar results were observed when human CIKs were tested in vitro in the presence of CD14+HLA-DR−/low MDSCs. Treatment of MDSCs with a PDE5 inhibitor suppressed MDSC suppressor function and enhanced CIK activity against human HCC cell lines in vitro. Our results suggest that targeting MDSCs is an efficient strategy to enhance the antitumor efficacy of CIKs for the treatment of patients with HCC. Hepatocellular carcinoma (HCC) is a common and fatal cancer, with an increasing incidence worldwide.1Though many curative therapies have been developed, the overall response to these therapies is inadequate and the long-term prognosis of patients with HCC remains poor because of its high recurrence rates.2A lot of data have shown that tumor progression is correlated with the accumulation of myeloid-derived suppressor cells (MDSCs) which induce local and possibly systemic immunosuppression.3Moreover, a greater prevalence of MDSCs has been correlated with early recurrence and was shown to be a predictor of poor prognosis in patients with HCC who underwent curative resection,4 radiotherapy,5 and hepatic arterial infusion chemotherapy.6MDSCs have been shown to suppress CD8+7–9 and CD4+ T10 cells as well as natural killer (NK)11 cells through diverse direct or indirect mechanisms.12
Cytokine-induced killer (CIK) cell-based immunotherapy is effective as an adjuvant therapy in early stage hepatocellular carcinoma (HCC) but lacks efficacy in advanced HCC. We aimed to investigate immune suppressor mechanisms in HCC, focusing on the role of myeloid-derived suppressor cells (MDSCs) in response to CIK therapy. MDSCs were quantified by flow cytometry and quantitative real-time PCR. Cytokines were detected by cytokine array. A lactate dehydrogenase cytotoxicity assay was performed in the presence or absence of MDSCs to study CIK function against HCC cells in vitro. An FDA-approved PDE5 inhibitor, tadalafil, was used to target MDSCs in vitro and in vivo. Two different murine HCC cell lines were tested in subcutaneous and orthotopic tumor models in C57BL/6 and BALB/c mice. The antitumor effects of human CIKs and MDSCs were also tested in vitro. Adoptive cell transfer of CIKs into tumor-bearing mice induced inflammatory mediators (e.g., CX3CL1, IL-13) in the tumor microenvironment and an increase of tumor-infiltrating MDSCs, leading to impaired antitumor activity in 2 different HCC models. MDSCs efficiently suppressed the cytotoxic activity of CIKs in vitro. In contrast, treatment with a PDE5 inhibitor reversed the MDSC suppressor function via ARG1 and iNOS blockade and systemic treatment with a PDE5 inhibitor prevented MDSC accumulation in the tumor microenvironment upon CIK cell therapy and increased its antitumor efficacy. Similar results were observed when human CIKs were tested in vitro in the presence of CD14+HLA-DR−/low MDSCs. Treatment of MDSCs with a PDE5 inhibitor suppressed MDSC suppressor function and enhanced CIK activity against human HCC cell lines in vitro. Our results suggest that targeting MDSCs is an efficient strategy to enhance the antitumor efficacy of CIKs for the treatment of patients with HCC. Cytokine-induced killer (CIK) cells are a mixed cell population of effector cells with diverse T cell receptor specificities that also possess non-major histocompatibility complex-restricted cytotoxic activity against tumor cells.CIK cells, which comprise cytotoxic T cells, NK cells, and NK-like T cells that express both NK- and T-cell markers are expanded ex vivo using recombinant IFN-γ, IL-2 and anti-CD3.13CIK cell-based immunotherapies have been widely studied and used in the treatment of patients with cancer, including HCC.14Currently, 90 registered clinical trials are listed on the ClinicalTrials.gov website (http://www.clinicaltrials.gov) when the following keywords are used in the search: cytokine-induced killer cells or CIK.15
Cytokine-induced killer (CIK) cell-based immunotherapy is effective as an adjuvant therapy in early stage hepatocellular carcinoma (HCC) but lacks efficacy in advanced HCC. We aimed to investigate immune suppressor mechanisms in HCC, focusing on the role of myeloid-derived suppressor cells (MDSCs) in response to CIK therapy. MDSCs were quantified by flow cytometry and quantitative real-time PCR. Cytokines were detected by cytokine array. A lactate dehydrogenase cytotoxicity assay was performed in the presence or absence of MDSCs to study CIK function against HCC cells in vitro. An FDA-approved PDE5 inhibitor, tadalafil, was used to target MDSCs in vitro and in vivo. Two different murine HCC cell lines were tested in subcutaneous and orthotopic tumor models in C57BL/6 and BALB/c mice. The antitumor effects of human CIKs and MDSCs were also tested in vitro. Adoptive cell transfer of CIKs into tumor-bearing mice induced inflammatory mediators (e.g., CX3CL1, IL-13) in the tumor microenvironment and an increase of tumor-infiltrating MDSCs, leading to impaired antitumor activity in 2 different HCC models. MDSCs efficiently suppressed the cytotoxic activity of CIKs in vitro. In contrast, treatment with a PDE5 inhibitor reversed the MDSC suppressor function via ARG1 and iNOS blockade and systemic treatment with a PDE5 inhibitor prevented MDSC accumulation in the tumor microenvironment upon CIK cell therapy and increased its antitumor efficacy. Similar results were observed when human CIKs were tested in vitro in the presence of CD14+HLA-DR−/low MDSCs. Treatment of MDSCs with a PDE5 inhibitor suppressed MDSC suppressor function and enhanced CIK activity against human HCC cell lines in vitro. Our results suggest that targeting MDSCs is an efficient strategy to enhance the antitumor efficacy of CIKs for the treatment of patients with HCC. A recent study determined that adjuvant immunotherapy using CIK cells appeared to reduce the recurrence of HCC and to improve overall survival.16Although adjuvant CIK cell-based immunotherapy is a promising treatment option for early stage HCC, it lacks efficacy in advanced HCC.17–19We hypothesized that CIK cells could trigger a counter-regulatory immunosuppressive mechanism through recruitment of MDSCs that might hinder CIK cell antitumor activity.
The development of non-invasive liver fibrosis tests may enable earlier identification of patients with non-alcoholic fatty liver disease (NAFLD) requiring referral to secondary care. We developed and evaluated a pathway for the management of patients with NAFLD, aimed at improving the detection of cases of advanced fibrosis and cirrhosis, and avoiding unnecessary referrals. This was a prospective longitudinal cohort study, with analyses performed before and after introduction of the pathway, and comparisons made to unexposed controls. We used a 2-step algorithm combining the use of Fibrosis-4 score followed by the ELF™ test if required. In total, 3,012 patients were analysed. Use of the pathway detected 5 times more cases of advanced fibrosis (Kleiner F3) and cirrhosis (odds ratio [OR] 5.18; 95% CI 2.97–9.04; p <0.0001), while reducing unnecessary referrals from primary care to secondary care by 81% (OR 0.193; 95% CI 0.111–0.337; p <0.0001). Although it was used for only 48% of referrals, significant benefits were observed in practices exposed to the pathway compared to those which were not, with unnecessary referrals falling by 77% (OR 0.23; 95% CI 0.658–0.082; p = 0.006) and a 4-fold improvement in detection of cases of advanced fibrosis and cirrhosis (OR 4.32; 95% CI 1.52–12.25; p = 0.006). Compared to referrals made before the introduction of the pathway, unnecessary referrals fell from 79/83 referrals (95.2%) to 107/152 (70.4%), representing an 88% reduction in unnecessary referrals when the pathway was followed (OR 0.12; 95% CI 0.042–0.349; p <0.0001). The use of non-invasive blood tests for liver fibrosis improves the detection of advanced fibrosis and cirrhosis, while reducing unnecessary referrals in patients with NAFLD. This strategy improves resource use and benefits patients. Non-alcoholic fatty liver disease (NAFLD) is the commonest cause of deranged liver blood tests (LFTs) in primary care in Europe and North America,1 with an estimated prevalence of 25–30% in the adult population.2Only a minority of people with NAFLD (5%) develop clinically significant liver disease,2 but the burden is such that NAFLD is predicted to be the leading indication for liver transplantation within a decade.3
The development of non-invasive liver fibrosis tests may enable earlier identification of patients with non-alcoholic fatty liver disease (NAFLD) requiring referral to secondary care. We developed and evaluated a pathway for the management of patients with NAFLD, aimed at improving the detection of cases of advanced fibrosis and cirrhosis, and avoiding unnecessary referrals. This was a prospective longitudinal cohort study, with analyses performed before and after introduction of the pathway, and comparisons made to unexposed controls. We used a 2-step algorithm combining the use of Fibrosis-4 score followed by the ELF™ test if required. In total, 3,012 patients were analysed. Use of the pathway detected 5 times more cases of advanced fibrosis (Kleiner F3) and cirrhosis (odds ratio [OR] 5.18; 95% CI 2.97–9.04; p <0.0001), while reducing unnecessary referrals from primary care to secondary care by 81% (OR 0.193; 95% CI 0.111–0.337; p <0.0001). Although it was used for only 48% of referrals, significant benefits were observed in practices exposed to the pathway compared to those which were not, with unnecessary referrals falling by 77% (OR 0.23; 95% CI 0.658–0.082; p = 0.006) and a 4-fold improvement in detection of cases of advanced fibrosis and cirrhosis (OR 4.32; 95% CI 1.52–12.25; p = 0.006). Compared to referrals made before the introduction of the pathway, unnecessary referrals fell from 79/83 referrals (95.2%) to 107/152 (70.4%), representing an 88% reduction in unnecessary referrals when the pathway was followed (OR 0.12; 95% CI 0.042–0.349; p <0.0001). The use of non-invasive blood tests for liver fibrosis improves the detection of advanced fibrosis and cirrhosis, while reducing unnecessary referrals in patients with NAFLD. This strategy improves resource use and benefits patients. The majority of patients with NAFLD are followed up in the community by general practitioners (GPs).Liver fibrosis severity is the key determinant of liver-related outcomes in NAFLD.4–6However, identifying patients with significant fibrosis who might benefit from early specialist intervention is challenging.As clinical assessment is a poor discriminator of fibrosis, such patients progress silently until cirrhosis leads to complications.Accurate fibrosis assessment in primary care is limited by a reliance on LFTs, which correlate poorly with fibrosis7,8 and limited access to discriminatory fibrosis tests.Thus, current management strategies are inefficient in identifying patients for specialist referral.Patients with mild disease are often referred for specialist review when the appropriate preventative interventions of lifestyle changes can be delivered effectively in primary care.9,10Conversely, patients with advanced fibrosis or cirrhosis who will benefit from specialist interventions including clinical trials and cirrhosis surveillance often remain undetected until they present with complications of cirrhosis, including hepatocellular carcinoma.This ineffective management contributes to the poor outcomes associated with liver disease and the increasing trends in NAFLD-related morbidity and mortality.
The development of non-invasive liver fibrosis tests may enable earlier identification of patients with non-alcoholic fatty liver disease (NAFLD) requiring referral to secondary care. We developed and evaluated a pathway for the management of patients with NAFLD, aimed at improving the detection of cases of advanced fibrosis and cirrhosis, and avoiding unnecessary referrals. This was a prospective longitudinal cohort study, with analyses performed before and after introduction of the pathway, and comparisons made to unexposed controls. We used a 2-step algorithm combining the use of Fibrosis-4 score followed by the ELF™ test if required. In total, 3,012 patients were analysed. Use of the pathway detected 5 times more cases of advanced fibrosis (Kleiner F3) and cirrhosis (odds ratio [OR] 5.18; 95% CI 2.97–9.04; p <0.0001), while reducing unnecessary referrals from primary care to secondary care by 81% (OR 0.193; 95% CI 0.111–0.337; p <0.0001). Although it was used for only 48% of referrals, significant benefits were observed in practices exposed to the pathway compared to those which were not, with unnecessary referrals falling by 77% (OR 0.23; 95% CI 0.658–0.082; p = 0.006) and a 4-fold improvement in detection of cases of advanced fibrosis and cirrhosis (OR 4.32; 95% CI 1.52–12.25; p = 0.006). Compared to referrals made before the introduction of the pathway, unnecessary referrals fell from 79/83 referrals (95.2%) to 107/152 (70.4%), representing an 88% reduction in unnecessary referrals when the pathway was followed (OR 0.12; 95% CI 0.042–0.349; p <0.0001). The use of non-invasive blood tests for liver fibrosis improves the detection of advanced fibrosis and cirrhosis, while reducing unnecessary referrals in patients with NAFLD. This strategy improves resource use and benefits patients. The evolution of non-invasive liver fibrosis tests has created the opportunity for GPs to use these tests in innovative pathways that permit earlier identification of patients with chronic liver disease and subsequent access to specialist care.11An example of this approach is outlined in the recent British Society for Gastroenterology guidance on the management of abnormal LFTs that recommends the use of non-invasive tests to stratify patients at risk of chronic liver disease.12
The development of non-invasive liver fibrosis tests may enable earlier identification of patients with non-alcoholic fatty liver disease (NAFLD) requiring referral to secondary care. We developed and evaluated a pathway for the management of patients with NAFLD, aimed at improving the detection of cases of advanced fibrosis and cirrhosis, and avoiding unnecessary referrals. This was a prospective longitudinal cohort study, with analyses performed before and after introduction of the pathway, and comparisons made to unexposed controls. We used a 2-step algorithm combining the use of Fibrosis-4 score followed by the ELF™ test if required. In total, 3,012 patients were analysed. Use of the pathway detected 5 times more cases of advanced fibrosis (Kleiner F3) and cirrhosis (odds ratio [OR] 5.18; 95% CI 2.97–9.04; p <0.0001), while reducing unnecessary referrals from primary care to secondary care by 81% (OR 0.193; 95% CI 0.111–0.337; p <0.0001). Although it was used for only 48% of referrals, significant benefits were observed in practices exposed to the pathway compared to those which were not, with unnecessary referrals falling by 77% (OR 0.23; 95% CI 0.658–0.082; p = 0.006) and a 4-fold improvement in detection of cases of advanced fibrosis and cirrhosis (OR 4.32; 95% CI 1.52–12.25; p = 0.006). Compared to referrals made before the introduction of the pathway, unnecessary referrals fell from 79/83 referrals (95.2%) to 107/152 (70.4%), representing an 88% reduction in unnecessary referrals when the pathway was followed (OR 0.12; 95% CI 0.042–0.349; p <0.0001). The use of non-invasive blood tests for liver fibrosis improves the detection of advanced fibrosis and cirrhosis, while reducing unnecessary referrals in patients with NAFLD. This strategy improves resource use and benefits patients. Whilst there is little evidence supporting the application of non-invasive tests in community settings, with only 1 study focusing on patients with NAFLD,13 guidelines recommend a 2-tier approach to detect the presence of advanced fibrosis in NAFLD using either Fibrosis-4 (FIB-4) or NAFLD Fibrosis score, as an inexpensive first screen, in a combined cut-off approach with indeterminate scores retested using more sensitive and specific tests, enhanced liver fibrosis (ELF™) or FibroScan®, that are more costly.14
The burden of hepatitis E virus (HEV) infection among patients with haematological malignancy has only been scarcely reported. Therefore, we aimed to describe this burden in patients with haematological malignancies, including those receiving allogeneic haematopoietic stem cell transplantation. We conducted a retrospective, multicentre cohort study across 11 European centres and collected clinical characteristics of 50 patients with haematological malignancy and RNA-positive, clinically overt hepatitis E between April 2014 and March 2017. The primary endpoint was HEV-associated mortality; the secondary endpoint was HEV-associated liver-related morbidity. The most frequent underlying haematological malignancies were aggressive non-Hodgkin lymphoma (NHL) (34%), indolent NHL (iNHL) (24%), and acute leukaemia (36%). Twenty-one (42%) patients had received allogeneic haematopoietic stem cell transplantation (alloHSCT). Death with ongoing hepatitis E occurred in 8 (16%) patients, including 1 patient with iNHL and 1 patient >100 days after alloHSCT in complete remission, and was associated with male sex (p = 0.040), cirrhosis (p = 0.006) and alloHSCT (p = 0.056). Blood-borne transmission of hepatitis E was demonstrated in 5 (10%) patients, and associated with liver-related mortality in 2 patients. Hepatitis E progressed to chronic hepatitis in 17 (34%) patients overall, and in 10 (47.6%) and 6 (50%) alloHSCT and iNHL patients, respectively. Hepatitis E was associated with acute or acute-on-chronic liver failure in 4 (8%) patients with 75% mortality. Ribavirin was administered to 24 (48%) patients, with an HEV clearance rate of 79.2%. Ribavirin treatment was associated with lower mortality (p = 0.037) and by trend with lower rates of chronicity (p = 0.407) when initiated <24 and <12 weeks after diagnosis of hepatitis E, respectively. Immunosuppressive treatment reductions were associated with mortality in 2 patients (28.6%). Hepatitis E is associated with mortality and liver-related morbidity in patients with haematological malignancy. Blood-borne transmission contributes to the burden. Ribavirin should be initiated early, whereas reduction of immunosuppressive treatment requires caution. The hepatitis E virus (HEV), the causative agent of hepatitis E, is a member of the Hepeviridae family that includes enterically-transmitted, small, non-enveloped positive-sense RNA viruses that can infect mammals (Orthohepevirus A, C and D), birds (Orthohepevirus B) and trout (Piscihepevirus).There are 4 major HEV genotypes (HEV-1 to HEV-4) that can infect humans.1HEV is predominately transmitted by contaminated water in low-income countries (mostly HEV-1 and HEV-2) and by contaminated meat and offal that originate from HEV-infected animals (HEV-3 and HEV-4) in high-income countries.2,3Blood-borne transmission of HEV via red blood cells, platelets and plasma has also been reported worldwide.4–7HEV is pandemic globally, including in industrialised countries.5
The burden of hepatitis E virus (HEV) infection among patients with haematological malignancy has only been scarcely reported. Therefore, we aimed to describe this burden in patients with haematological malignancies, including those receiving allogeneic haematopoietic stem cell transplantation. We conducted a retrospective, multicentre cohort study across 11 European centres and collected clinical characteristics of 50 patients with haematological malignancy and RNA-positive, clinically overt hepatitis E between April 2014 and March 2017. The primary endpoint was HEV-associated mortality; the secondary endpoint was HEV-associated liver-related morbidity. The most frequent underlying haematological malignancies were aggressive non-Hodgkin lymphoma (NHL) (34%), indolent NHL (iNHL) (24%), and acute leukaemia (36%). Twenty-one (42%) patients had received allogeneic haematopoietic stem cell transplantation (alloHSCT). Death with ongoing hepatitis E occurred in 8 (16%) patients, including 1 patient with iNHL and 1 patient >100 days after alloHSCT in complete remission, and was associated with male sex (p = 0.040), cirrhosis (p = 0.006) and alloHSCT (p = 0.056). Blood-borne transmission of hepatitis E was demonstrated in 5 (10%) patients, and associated with liver-related mortality in 2 patients. Hepatitis E progressed to chronic hepatitis in 17 (34%) patients overall, and in 10 (47.6%) and 6 (50%) alloHSCT and iNHL patients, respectively. Hepatitis E was associated with acute or acute-on-chronic liver failure in 4 (8%) patients with 75% mortality. Ribavirin was administered to 24 (48%) patients, with an HEV clearance rate of 79.2%. Ribavirin treatment was associated with lower mortality (p = 0.037) and by trend with lower rates of chronicity (p = 0.407) when initiated <24 and <12 weeks after diagnosis of hepatitis E, respectively. Immunosuppressive treatment reductions were associated with mortality in 2 patients (28.6%). Hepatitis E is associated with mortality and liver-related morbidity in patients with haematological malignancy. Blood-borne transmission contributes to the burden. Ribavirin should be initiated early, whereas reduction of immunosuppressive treatment requires caution. In the absence of comorbidity, HEV infection is usually a self-limiting illness lasting 1 to 3 months with spontaneous resolution.However, fulminant courses leading to life-threatening liver failure are possible and chronic hepatitis E can occur in immunocompromised hosts, including transplant recipients and lymphopenic hosts.8–10In the setting of chronic hepatitis E following transplantation, reduction of immunosuppressive drugs has been recommended as initial treatment, whereas ribavirin treatment serves as second line.11,12Yet, these recommendations are based on low grade of evidence, in particular for patients with haematological malignancy.12–14
The burden of hepatitis E virus (HEV) infection among patients with haematological malignancy has only been scarcely reported. Therefore, we aimed to describe this burden in patients with haematological malignancies, including those receiving allogeneic haematopoietic stem cell transplantation. We conducted a retrospective, multicentre cohort study across 11 European centres and collected clinical characteristics of 50 patients with haematological malignancy and RNA-positive, clinically overt hepatitis E between April 2014 and March 2017. The primary endpoint was HEV-associated mortality; the secondary endpoint was HEV-associated liver-related morbidity. The most frequent underlying haematological malignancies were aggressive non-Hodgkin lymphoma (NHL) (34%), indolent NHL (iNHL) (24%), and acute leukaemia (36%). Twenty-one (42%) patients had received allogeneic haematopoietic stem cell transplantation (alloHSCT). Death with ongoing hepatitis E occurred in 8 (16%) patients, including 1 patient with iNHL and 1 patient >100 days after alloHSCT in complete remission, and was associated with male sex (p = 0.040), cirrhosis (p = 0.006) and alloHSCT (p = 0.056). Blood-borne transmission of hepatitis E was demonstrated in 5 (10%) patients, and associated with liver-related mortality in 2 patients. Hepatitis E progressed to chronic hepatitis in 17 (34%) patients overall, and in 10 (47.6%) and 6 (50%) alloHSCT and iNHL patients, respectively. Hepatitis E was associated with acute or acute-on-chronic liver failure in 4 (8%) patients with 75% mortality. Ribavirin was administered to 24 (48%) patients, with an HEV clearance rate of 79.2%. Ribavirin treatment was associated with lower mortality (p = 0.037) and by trend with lower rates of chronicity (p = 0.407) when initiated <24 and <12 weeks after diagnosis of hepatitis E, respectively. Immunosuppressive treatment reductions were associated with mortality in 2 patients (28.6%). Hepatitis E is associated with mortality and liver-related morbidity in patients with haematological malignancy. Blood-borne transmission contributes to the burden. Ribavirin should be initiated early, whereas reduction of immunosuppressive treatment requires caution. The course of HEV infection among patients with haematological malignancy has been scarcely reported.HEV infection occurred in 8 (2.4%) patients in a retrospective cohort of 328 adult allogeneic haematopoietic stem cell transplant recipients from Rotterdam, the Netherlands, of whom 5 (62.5%) had HEV replication for more than 6 months and 4 (50%) died with ongoing HEV infection without a liver-related complication.8In an observational cohort of 26 haematological patients with HEV infection from Toulouse, France, 5 (37.5%) patients had HEV replication for more than 3 months, and 1 patient with non-alcoholic steatohepatitis developed a liver-related complication.15Otherwise, cases of liver failure and liver-related death have been reported occasionally in patients with haematological malignancy.16–19
To compare the overall survival (OS) and disease progression free survival (PFS) in patients with advanced hepatocellular carcinoma (Ad-HCC) who are undergoing hepatic arterial infusion (HAI) of oxaliplatin, fluorouracil/leucovorin (FOLFOX) treatment vs. sorafenib. This retrospective study was approved by the ethical review committee, and informed consent was obtained from all patients before treatment. HAI of FOLFOX (HAIF) was recommended as an alternative treatment option for patients who refused sorafenib. Of the 412 patients with Ad-HCC (376 men and 36 women) between Jan 2012 to Dec 2015, 232 patients were treated with sorafenib; 180 patients were given HAIF therapy. The median age was 51 years (range, 16–82 years). Propensity-score matched estimates were used to reduce bias when evaluating survival. Survival curves were calculated by performing the Kaplan-Meier method and compared by using the log-rank test and Cox regression models. The median PFS and OS in the HAIF group were significantly longer than those in the sorafenib group (PFS 7.1 vs. 3.3 months [RECIST]/7.4 vs. 3.6 months [mRECIST], respectively; OS 14.5 vs. 7.0 months; p <0.001 for each). In the propensity-score matched cohorts (147 pairs), both PFS and OS in the HAIF group were longer than those in the sorafenib group (p <0.001). At multivariate analysis, HAIF treatment was an independent factor for PFS (hazard ratio [HR] 0.389 [RECIST]/0.402 [mRECIST]; p <0.001 for each) and OS (HR 0.129; p <0.001). HAIF therapy may improve survival compared to sorafenib in patients with Ad-HCC. A prospective randomized trial is ongoing to confirm this finding. Hepatocellular carcinoma (HCC) is the fourth leading cause of cancer worldwide.1A total of 25%–70% of HCC is diagnosed at an advanced stage, with a median overall survival (OS) of only 4.2–7.9 months, because of limited treatment options.2,3To date, sorafenib is still the only treatment shown to extend OS for advanced HCC (Ad-HCC).4However limitations including, low response rates,2 modest survival advantages,3 high-level heterogeneity of individual response5 and insensitivity for populations with hepatitis B virus (HBV) infection,6 prohibit sorafenib’s widespread use in Ad-HCC.Thus, alternative therapies for Ad-HCC are urgently required.1,7,8
To compare the overall survival (OS) and disease progression free survival (PFS) in patients with advanced hepatocellular carcinoma (Ad-HCC) who are undergoing hepatic arterial infusion (HAI) of oxaliplatin, fluorouracil/leucovorin (FOLFOX) treatment vs. sorafenib. This retrospective study was approved by the ethical review committee, and informed consent was obtained from all patients before treatment. HAI of FOLFOX (HAIF) was recommended as an alternative treatment option for patients who refused sorafenib. Of the 412 patients with Ad-HCC (376 men and 36 women) between Jan 2012 to Dec 2015, 232 patients were treated with sorafenib; 180 patients were given HAIF therapy. The median age was 51 years (range, 16–82 years). Propensity-score matched estimates were used to reduce bias when evaluating survival. Survival curves were calculated by performing the Kaplan-Meier method and compared by using the log-rank test and Cox regression models. The median PFS and OS in the HAIF group were significantly longer than those in the sorafenib group (PFS 7.1 vs. 3.3 months [RECIST]/7.4 vs. 3.6 months [mRECIST], respectively; OS 14.5 vs. 7.0 months; p <0.001 for each). In the propensity-score matched cohorts (147 pairs), both PFS and OS in the HAIF group were longer than those in the sorafenib group (p <0.001). At multivariate analysis, HAIF treatment was an independent factor for PFS (hazard ratio [HR] 0.389 [RECIST]/0.402 [mRECIST]; p <0.001 for each) and OS (HR 0.129; p <0.001). HAIF therapy may improve survival compared to sorafenib in patients with Ad-HCC. A prospective randomized trial is ongoing to confirm this finding. Recently, several studies have explored hepatic arterial infusion (HAI) chemotherapy (HAIC) alone or accompanied with sorafenib in Ad-HCC and reported favorable results either in response rate or survival.9–11This therapy can directly deliver chemotherapeutic agents into tumor-associated hepatic arterial branches at increased local concentrations,12 providing stronger antitumor efficacy and lower systemic toxicity through a greater first-pass effect in the liver compared with systemic chemotherapy.13However, although effective, the response rate of HAIC is unstable and varies from 7 to 81% with median OS of 6–15.9 months.9,14This may be attributed to complicated combinations of various drugs, including cisplatin, fluorouracil, interferon and doxorubicin.9,14
Imaging characteristics for discriminating the malignant potential of intraductal papillary neoplasm of the bile duct (IPNB) still remain unclear. This study aimed to define the magnetic resonance (MR) imaging findings that help to differentiate IPNB with an associated invasive carcinoma from IPNB with intraepithelial neoplasia and to investigate their significance with respect to long-term outcomes in patients with surgically resected IPNB. This retrospective study included 120 patients with surgically resected IPNB who underwent preoperative MR imaging with MR cholangiography before surgery from January 2008 and December 2017 in two tertiary referral centers. Clinical and MR imaging features of IPNB with intraepithelial neoplasia (n = 34) and IPNB with an associated invasive carcinoma (n = 86) were compared. Regarding significant features for discriminating IPNB with or without an associated invasive carcinoma, recurrence-free survival (RFS) rates were evaluated. Significant MR imaging findings for differentiating IPNB with an associated invasive carcinoma from IPNB with intraepithelial neoplasia were intraductal visible mass, tumor size ≥2.5 cm, multiplicity of the tumor, bile duct wall thickening, and adjacent organ invasion (all p ≤0.002). The 1-, 3-, and 5-year RFS rates for surgically resected IPNB were 93.8%, 79.1%, and 70.0%, respectively. RFS rates were significantly lower in patients with each significant MR imaging finding of IPNB with an associated invasive carcinoma than in those without significant MR imaging findings (all p ≤0.039). MR imaging with MR cholangiography may be helpful in differentiating IPNB with an associated invasive carcinoma from IPNB with intraepithelial neoplasia. Significant MR imaging findings of IPNB with an associated invasive carcinoma have a negative impact on RFS. Intraductal papillary neoplasm of the bile duct (IPNB) is characterized by a papillary or villous biliary neoplasm covering delicate fibrovascular stalks and a histological spectrum ranging from benign disease to invasive malignancy.1,2According to the 2010 World Health Organization classification, IPNB can be classified into the following 3 histologic grades: IPNB with low- or intermediate-grade intraepithelial neoplasia, IPNB with high-grade intraepithelial neoplasia, and IPNB with an associated invasive carcinoma.IPNB with low- or intermediate- and high-grade intraepithelial neoplasia are regarded as premalignant and non-invasive IPNB, whereas IPNB with an associated invasive carcinoma is considered malignant and invasive IPNB.
Imaging characteristics for discriminating the malignant potential of intraductal papillary neoplasm of the bile duct (IPNB) still remain unclear. This study aimed to define the magnetic resonance (MR) imaging findings that help to differentiate IPNB with an associated invasive carcinoma from IPNB with intraepithelial neoplasia and to investigate their significance with respect to long-term outcomes in patients with surgically resected IPNB. This retrospective study included 120 patients with surgically resected IPNB who underwent preoperative MR imaging with MR cholangiography before surgery from January 2008 and December 2017 in two tertiary referral centers. Clinical and MR imaging features of IPNB with intraepithelial neoplasia (n = 34) and IPNB with an associated invasive carcinoma (n = 86) were compared. Regarding significant features for discriminating IPNB with or without an associated invasive carcinoma, recurrence-free survival (RFS) rates were evaluated. Significant MR imaging findings for differentiating IPNB with an associated invasive carcinoma from IPNB with intraepithelial neoplasia were intraductal visible mass, tumor size ≥2.5 cm, multiplicity of the tumor, bile duct wall thickening, and adjacent organ invasion (all p ≤0.002). The 1-, 3-, and 5-year RFS rates for surgically resected IPNB were 93.8%, 79.1%, and 70.0%, respectively. RFS rates were significantly lower in patients with each significant MR imaging finding of IPNB with an associated invasive carcinoma than in those without significant MR imaging findings (all p ≤0.039). MR imaging with MR cholangiography may be helpful in differentiating IPNB with an associated invasive carcinoma from IPNB with intraepithelial neoplasia. Significant MR imaging findings of IPNB with an associated invasive carcinoma have a negative impact on RFS. IPNB presents as an intraductal mass within the dilated intrahepatic or extrahepatic bile ducts on imaging studies.3–5However, small-sized papillary lesions may be hard to detect by conventional imaging studies, such as ultrasonography (US) or computed tomography (CT).Currently, magnetic resonance (MR) imaging with MR cholangiography is being commonly used as the imaging modality of choice for evaluating various biliary disorders.Given the superior contrast resolution, MR imaging with MR cholangiography has advantages in the detection and evaluation of small intraductal tumor and tumor multiplicity of IPNB.6,7
Imaging characteristics for discriminating the malignant potential of intraductal papillary neoplasm of the bile duct (IPNB) still remain unclear. This study aimed to define the magnetic resonance (MR) imaging findings that help to differentiate IPNB with an associated invasive carcinoma from IPNB with intraepithelial neoplasia and to investigate their significance with respect to long-term outcomes in patients with surgically resected IPNB. This retrospective study included 120 patients with surgically resected IPNB who underwent preoperative MR imaging with MR cholangiography before surgery from January 2008 and December 2017 in two tertiary referral centers. Clinical and MR imaging features of IPNB with intraepithelial neoplasia (n = 34) and IPNB with an associated invasive carcinoma (n = 86) were compared. Regarding significant features for discriminating IPNB with or without an associated invasive carcinoma, recurrence-free survival (RFS) rates were evaluated. Significant MR imaging findings for differentiating IPNB with an associated invasive carcinoma from IPNB with intraepithelial neoplasia were intraductal visible mass, tumor size ≥2.5 cm, multiplicity of the tumor, bile duct wall thickening, and adjacent organ invasion (all p ≤0.002). The 1-, 3-, and 5-year RFS rates for surgically resected IPNB were 93.8%, 79.1%, and 70.0%, respectively. RFS rates were significantly lower in patients with each significant MR imaging finding of IPNB with an associated invasive carcinoma than in those without significant MR imaging findings (all p ≤0.039). MR imaging with MR cholangiography may be helpful in differentiating IPNB with an associated invasive carcinoma from IPNB with intraepithelial neoplasia. Significant MR imaging findings of IPNB with an associated invasive carcinoma have a negative impact on RFS. IPNB has been considered a biliary counterpart of intraductal papillary mucinous neoplasms (IPMN) of the pancreas.8In IPMN of the pancreas, morphologic criteria based on imaging findings that indicate the presence of a malignant neoplasm, that is, “high-risk stigmata” or “worrisome features”, are well established by international consensus guidelines of 2012 and 2017.9,10Contrary to well-documented IPMN of the pancreas, imaging characteristics of IPNB for discriminating malignant potential are poorly understood.It may be helpful to know the imaging findings suggestive of the presence of malignancy and invasive carcinoma in IPNB, since early diagnosis and preoperative evaluation of tumor invasiveness is crucial for determining proper therapy and for better prognosis.To our knowledge, there is only one previous study regarding the MR imaging features of non-invasive and invasive IPNBs.11However, this study only reviewed 23 cases in a single institution; moreover, it did not evaluate the relation between MR imaging findings and long-term outcomes.
The development of hepatic models capable of long-term expansion with competent liver functionality is technically challenging in a personalized setting. Stem cell-based organoid technologies can provide an alternative source of patient-derived primary hepatocytes. However, self-renewing and functionally competent human pluripotent stem cell (PSC)-derived hepatic organoids have not been developed. We developed a novel method to efficiently and reproducibly generate functionally mature human hepatic organoids derived from PSCs, including human embryonic stem cells and induced PSCs. The maturity of the organoids was validated by a detailed transcriptome analysis and functional performance assays. The organoids were applied to screening platforms for the prediction of toxicity and the evaluation of drugs that target hepatic steatosis through real-time monitoring of cellular bioenergetics and high-content analyses. Our organoids were morphologically indistinguishable from adult liver tissue-derived epithelial organoids and exhibited self-renewal. With further maturation, their molecular features approximated those of liver tissue, although these features were lacking in 2D differentiated hepatocytes. Our organoids preserved mature liver properties, including serum protein production, drug metabolism and detoxifying functions, active mitochondrial bioenergetics, and regenerative and inflammatory responses. The organoids exhibited significant toxic responses to clinically relevant concentrations of drugs that had been withdrawn from the market due to hepatotoxicity and recapitulated human disease phenotypes such as hepatic steatosis. Our organoids exhibit self-renewal (expandable and further able to differentiate) while maintaining their mature hepatic characteristics over long-term culture. These organoids may provide a versatile and valuable platform for physiologically and pathologically relevant hepatic models in the context of personalized medicine. Human cell-based and personalized in vitro liver models are urgently needed for drug efficacy and toxicity tests in pre-clinical drug development.Although the liver is a representative organ with a native regenerative potential in vivo, primary human hepatocytes (PHHs), which are considered the gold standard for evaluating hepatic metabolism, are limited by their loss of proliferative capacity and long-term functionality in vitro.1To overcome the limitations of PHHs, various approaches have been developed, including genetic modification,2 3D culture combined with tissue engineering technologies,1,3–7 and defined medium compositions.8,9However, the development of alternative and sustainable cell sources to recapitulate the function of native liver remains challenging.
Diabetes occurring as a direct consequence of loss of liver function is usually characterized by non-diabetic fasting plasma glucose (FPG) and haemoglobin A1c (HbA1c) levels and should regress after orthotopic liver transplantation (OLT). This observational, longitudinal study investigated the relationship between the time-courses of changes in all 3 direct determinants of glucose regulation, i.e., β-cell function, insulin clearance and insulin sensitivity, and diabetes regression after OLT. Eighty cirrhotic patients with non-diabetic FPG and HbA1c levels underwent an extended oral glucose tolerance test (OGTT) before and 3, 6, 12 and 24 months after OLT. The OGTT data were analysed with a mathematical model to estimate derivative control (DC) and proportional control (PC) of β-cell function and insulin clearance (which determine insulin bioavailability), and with the Oral Glucose Insulin Sensitivity (OGIS)-2 h index to estimate insulin sensitivity. At baseline, 36 patients were diabetic (45%) and 44 were non-diabetic (55%). Over the 2-year follow-up, 23 diabetic patients (63.9%) regressed to non-diabetic glucose regulation, whereas 13 did not (36.1%); moreover, 4 non-diabetic individuals progressed to diabetes (9.1%), whereas 40 did not (90.9%). Both DC and PC increased in regressors (from month 3 and 24, respectively) and decreased in progressors, whereas they remained stable in non-regressors and only PC decreased in non-progressors. Insulin clearance increased in all groups, apart from progressors. Likewise, OGIS-2 h improved at month 3 in all groups, but thereafter it continued to improve only in regressors, whereas it returned to baseline values in the other groups. Increased insulin bioavailability driven by improved β-cell function plays a central role in favouring diabetes regression after OLT, in the presence of a sustained improvement of insulin sensitivity. Diabetes mellitus (DM) is a common feature in cirrhotic individuals, due to the bidirectional relationship between impaired glucose metabolism and chronic liver disease.1On the one hand, type 2 DM is a risk factor for non-alcoholic fatty liver disease (NAFLD)2 and, though not included in the most widely used prognostic tools,3 is a major predictor of adverse outcomes in cirrhotic individuals both before4 and after5 orthotopic liver transplantation (OLT).On the other hand, certain aetiological agents of liver disease, including HCV and NAFLD, may cause β-cell dysfunction and/or insulin resistance, thus favouring development of DM even prior to the onset of cirrhosis.1Moreover, DM may be a direct consequence of loss of liver function, which impairs insulin secretion and sensitivity via several, partly unrecognized, mechanisms.6This is the so-called hepatogenous DM, which is not considered a separate clinical entity, despite distinguishing pathophysiological and clinical features.7We have previously shown that, compared with non-DM cirrhotic individuals, those with hepatogenous DM are characterized by worse β-cell function, which deteriorates in parallel with severity of liver disease.8In addition, they present with fasting plasma glucose (FPG) and haemoglobin A1c (HbA1c) levels not in the DM range, due to impaired glucose metabolism and reduced lifespan of erythrocytes, respectively.6,7This “subclinical” presentation implies that, in cirrhotic patients, an oral glucose tolerance test (OGTT) is required for DM diagnosis6,9 and explains the differences in prevalence estimates of DM according to the method(s) of assessment.9
Diabetes occurring as a direct consequence of loss of liver function is usually characterized by non-diabetic fasting plasma glucose (FPG) and haemoglobin A1c (HbA1c) levels and should regress after orthotopic liver transplantation (OLT). This observational, longitudinal study investigated the relationship between the time-courses of changes in all 3 direct determinants of glucose regulation, i.e., β-cell function, insulin clearance and insulin sensitivity, and diabetes regression after OLT. Eighty cirrhotic patients with non-diabetic FPG and HbA1c levels underwent an extended oral glucose tolerance test (OGTT) before and 3, 6, 12 and 24 months after OLT. The OGTT data were analysed with a mathematical model to estimate derivative control (DC) and proportional control (PC) of β-cell function and insulin clearance (which determine insulin bioavailability), and with the Oral Glucose Insulin Sensitivity (OGIS)-2 h index to estimate insulin sensitivity. At baseline, 36 patients were diabetic (45%) and 44 were non-diabetic (55%). Over the 2-year follow-up, 23 diabetic patients (63.9%) regressed to non-diabetic glucose regulation, whereas 13 did not (36.1%); moreover, 4 non-diabetic individuals progressed to diabetes (9.1%), whereas 40 did not (90.9%). Both DC and PC increased in regressors (from month 3 and 24, respectively) and decreased in progressors, whereas they remained stable in non-regressors and only PC decreased in non-progressors. Insulin clearance increased in all groups, apart from progressors. Likewise, OGIS-2 h improved at month 3 in all groups, but thereafter it continued to improve only in regressors, whereas it returned to baseline values in the other groups. Increased insulin bioavailability driven by improved β-cell function plays a central role in favouring diabetes regression after OLT, in the presence of a sustained improvement of insulin sensitivity. By definition, hepatogenous DM should benefit from OLT, the first-choice treatment for end-stage liver disease, as restoration of liver function would remove the local and systemic factors detrimentally affecting insulin secretion and sensitivity, thereby leading to improvement or even regression of DM.9However, like other solid organ transplants, OLT is often associated with development of post-transplant DM,10 which is predicted by prior DM11 and is favoured by immunosuppressant treatment12 and changes in nutritional habits13.Previous studies in patients with overt DM provided conflicting results.Perseghin et al. reported that transplantation was successful in curing DM in two-thirds of cases14.Conversely, Lunati et al. showed that DM prevalence remained unchanged over a 1-year follow-up, with DM regressing or developing as new-onset DM in a few patients.13Likewise, even in cirrhotic patients with normal FPG (i.e. those likely suffering from hepatogenous DM), abnormalities of glucose tolerance persisted despite restoration of liver function.15
The impact of hepatitis B core antibody (anti-HBc) positive liver grafts on survival and the risk of de novo hepatitis B virus (HBV) infection after liver transplantation (LT) remain controversial. Therefore, we aimed to analyze this risk and the associated outcomes in a large cohort of patients. This was a retrospective study that included all adults who underwent LT at Queen Mary Hospital, Hong Kong, between 2000 and 2015. Data were retrieved from a prospectively collected database. Antiviral monotherapy prophylaxis was given for patients receiving grafts from anti-HBc positive donors. A total of 964 LTs were performed during the study period, with 416 (43.2%) anti-HBc positive and 548 (56.8%) anti-HBc negative donors. The median follow-up time was 7.8 years. Perioperative outcomes (hospital mortality, complications, primary nonfunction and delayed graft function) were similar between the 2 groups. The 1-, 5- and 10-year graft survival rates were comparable in anti-HBc positive (93.3%, 85.3% and 76.8%) and anti-HBc negative groups (92.5%, 82.9% and 78.4%, p = 0.944). The 1-, 5- and 10-year patient survival rates in anti-HBc positive group were 94.2%, 87% and 79% and were similar to the anti-HBc negative group (93.5%, 84% and 79.7%, p = 0.712). One-hundred and eight HBsAg negative recipients received anti-HBc positive grafts, of whom 64 received lamivudine and 44 entecavir monotherapy prophylaxis. The risk of de novo HBV was 3/108 (2.8%) and all occurred in the lamivudine era. There were 659 HBsAg-positive patients and 308 (46.7%) received anti-HBc positive grafts. The risk of HBV recurrence was similar between the 2 groups. Donor anti-HBc status did not impact on long-term patient and graft survival, or the risk of hepatocellular carcinoma recurrence after LT. De novo HBV was exceedingly rare especially with entecavir prophylaxis. Anti-HBc positive grafts did not impact on perioperative and long-term outcomes after transplant. Liver transplantation (LT) has become the standard of care for patients with end-stage liver disease and early non-resectable hepatocellular carcinoma (HCC).The growth in demand for LT has not been paralleled by a similar increase in organ supply.1Efforts have been made to promote organ donation, to develop surgical innovations such as living donor liver transplantation (LDLT), and to promote the use of extended criteria donor (ECD) organs.1
Acute-on-chronic liver failure (ACLF) is characterized by acute decompensation of cirrhosis, development of organ failure and high short-term mortality. Whether the outcome in patients admitted to the intensive care unit (ICU) with ACLF differs from other ICU populations is unknown. We compared the clinical course and host response in ICU patients with or without ACLF, matched for baseline severity of illness scores and characteristics. From the large prospective EPaNIC randomized control trial database (n = 4,640), 133 patients were identified with cirrhosis of whom 71 fulfilled the Chronic Liver Failure Consortium criteria for ACLF. These patients were matched for type and severity of illness and demographics to 71 septic and 71 medical ICU patients from the same database without chronic liver disease. Clinical, biochemical and outcome parameters were compared in this cohort study of 213 patients. In a subset of 100 patients, day 1 serum cytokines were quantified. The outcome of ACLF, when compared to septic or medical ICU patients, matched for baseline parameters of illness severity, was similar regarding length of ICU stay, development of new infections, organ failure and septic shock. ICU, hospital and 90-day mortality were similar between the groups. C-reactive protein and platelet levels were lower in patients with ACLF throughout the first week. Cytokines, including IL-10, IL-1β, IL-6, and IL-8, were similarly elevated in ACLF and septic ICU patients on day 1. However, TNF-α levels were higher in patients with ACLF. Patients with ACLF admitted to the ICU showed comparable clinical and ICU outcomes as ICU patients without chronic liver disease, but with similar baseline severity of illness characteristics. This suggests that ICU admission criteria should not be different in ACLF populations. Acute-on-chronic liver failure (ACLF) is a recently defined clinical entity characterized by acute deterioration of compensated or stable decompensated cirrhosis associated with organ failure and a high 28-day mortality.1–6ACLF as an entity differs in its clinical presentation and prognosis from alcoholic hepatitis and decompensated cirrhosis without organ failure.4,7Also, the inflammatory response is more pronounced in ACLF compared to compensated or decompensated cirrhosis with preserved organ function.4White blood cell counts, plasma pro-inflammatory cytokines and C-reactive protein (CRP) levels are higher in patients with ACLF than in cirrhotic patients without ACLF.8–10This excessive inflammatory response may be an important contributing factor in the development of organ failure and immune exhaustion which may predispose to the development of new infections especially in hospitalized individuals.1,11
Acute-on-chronic liver failure (ACLF) is characterized by acute decompensation of cirrhosis, development of organ failure and high short-term mortality. Whether the outcome in patients admitted to the intensive care unit (ICU) with ACLF differs from other ICU populations is unknown. We compared the clinical course and host response in ICU patients with or without ACLF, matched for baseline severity of illness scores and characteristics. From the large prospective EPaNIC randomized control trial database (n = 4,640), 133 patients were identified with cirrhosis of whom 71 fulfilled the Chronic Liver Failure Consortium criteria for ACLF. These patients were matched for type and severity of illness and demographics to 71 septic and 71 medical ICU patients from the same database without chronic liver disease. Clinical, biochemical and outcome parameters were compared in this cohort study of 213 patients. In a subset of 100 patients, day 1 serum cytokines were quantified. The outcome of ACLF, when compared to septic or medical ICU patients, matched for baseline parameters of illness severity, was similar regarding length of ICU stay, development of new infections, organ failure and septic shock. ICU, hospital and 90-day mortality were similar between the groups. C-reactive protein and platelet levels were lower in patients with ACLF throughout the first week. Cytokines, including IL-10, IL-1β, IL-6, and IL-8, were similarly elevated in ACLF and septic ICU patients on day 1. However, TNF-α levels were higher in patients with ACLF. Patients with ACLF admitted to the ICU showed comparable clinical and ICU outcomes as ICU patients without chronic liver disease, but with similar baseline severity of illness characteristics. This suggests that ICU admission criteria should not be different in ACLF populations. Cirrhotic patients with organ failure admitted to the intensive care unit (ICU) have a high ICU and hospital mortality, ranging from 36 to 89%, depending on the type and number of organ failures.1,12–15These high mortality rates commonly impact on the decision not to admit these patients to the ICU, especially when not regarded eligible candidates for a liver transplant (LT).However, it is still unknown whether the outcome of ACLF is in fact inferior, when compared to the outcome of patients without chronic liver disease admitted to the ICU with organ failure.
Chronic hepatitis B (CHB) affects over 2 million people in the US, with little reported on healthcare utilization and cost. We aimed to quantify annual CHB utilization and costs by disease severity and payer type. Using Commercial, Medicare, and Medicaid databases from 2004 to 2015 and ICD9 codes, we retrospectively identified adults with CHB, analyzing all-cause inpatient, outpatient, and pharmaceutical utilization and costs by disease severity. We compared healthcare utilization and costs between patients with CHB, without advanced liver disease, and matched non-CHB controls. All-cause inpatient, outpatient, and pharmaceutical utilization and costs were reported for each year and adjusted to 2015 dollars. Our sample consisted of 33,904 CHB cases and 86,072 non-CHB controls. All-cause inpatient admissions (average stay 6–10 days) were more frequent in advanced liver disease states. Across all payers, patients with decompensated cirrhosis had the highest emergency department utilization (1.6–2.8 annual visits) and highest mean annual costs. The largest all-cause cost components for Commercial and Medicaid were inpatient costs for all advanced liver disease groups (Commercial: 62%, 47%, 68%; Medicaid: 81%, 72%, 74%, respectively), and decompensated cirrhosis and hepatocellular carcinoma groups for Medicare (Medicare 49% and 48%). In addition, patients with compensated liver disease incurred costs 3 times higher than non-CHB controls. Patients with CHB, regardless of payer, who experienced decompensated cirrhosis, hepatocellular carcinoma, or a liver transplant incurred the highest annual costs and utilization of healthcare resources, but even patients with CHB and compensated liver disease incurred higher costs than those without CHB. All stakeholders in disease management need to combine efforts to prevent infection and advanced liver disease through improved vaccination rates, earlier diagnosis, and treatment. Chronic hepatitis B (CHB)-induced liver disease can progress to compensated cirrhosis, hepatic decompensation, hepatocellular carcinoma (HCC), and death; and such progression occurs over many years for most patients.CHB currently affects approximately 240 million individuals worldwide, including 2 million individuals within the US, and is associated with substantial healthcare utilization.1–4
Chronic hepatitis B (CHB) affects over 2 million people in the US, with little reported on healthcare utilization and cost. We aimed to quantify annual CHB utilization and costs by disease severity and payer type. Using Commercial, Medicare, and Medicaid databases from 2004 to 2015 and ICD9 codes, we retrospectively identified adults with CHB, analyzing all-cause inpatient, outpatient, and pharmaceutical utilization and costs by disease severity. We compared healthcare utilization and costs between patients with CHB, without advanced liver disease, and matched non-CHB controls. All-cause inpatient, outpatient, and pharmaceutical utilization and costs were reported for each year and adjusted to 2015 dollars. Our sample consisted of 33,904 CHB cases and 86,072 non-CHB controls. All-cause inpatient admissions (average stay 6–10 days) were more frequent in advanced liver disease states. Across all payers, patients with decompensated cirrhosis had the highest emergency department utilization (1.6–2.8 annual visits) and highest mean annual costs. The largest all-cause cost components for Commercial and Medicaid were inpatient costs for all advanced liver disease groups (Commercial: 62%, 47%, 68%; Medicaid: 81%, 72%, 74%, respectively), and decompensated cirrhosis and hepatocellular carcinoma groups for Medicare (Medicare 49% and 48%). In addition, patients with compensated liver disease incurred costs 3 times higher than non-CHB controls. Patients with CHB, regardless of payer, who experienced decompensated cirrhosis, hepatocellular carcinoma, or a liver transplant incurred the highest annual costs and utilization of healthcare resources, but even patients with CHB and compensated liver disease incurred higher costs than those without CHB. All stakeholders in disease management need to combine efforts to prevent infection and advanced liver disease through improved vaccination rates, earlier diagnosis, and treatment. Currently, there is no cure for CHB, though newer treatments with nucleos(t)ide-based medications demonstrate effective viral suppression, consequently slowing disease progression.HBV vaccination became available over 30 years ago and is now the primary method of HBV prevention.The original targeted populations for vaccination were people at high risk of blood-borne infections through sexual contact or in the provision of health care, but in the mid-1990s universal vaccination was recommended for all infants.5,6
Irisin, the cleaved extra-cellular fragment of the Fibronectin type III domain-containing protein 5 (FNDC5) is a myokine that is proposed to have favorable metabolic activity. We aimed to elucidate the currently undefined role of variants in the FNDC5 gene in non-alcoholic fatty liver disease (NAFLD). We prioritized single nucleotide polymorphisms in FNDC5 on the basis of their putative biological function and identified rs3480 in the 3′ untranslated region (3′UTR). We studied the association of rs3480 with liver disease severity and the metabolic profile of 987 Caucasian patients with NAFLD. Functional investigations were undertaken using luciferase reporter assays of the 3′UTR of human FNDC5, pyrosequencing for allele-specific expression of FNDC5 in liver, measurement of serum irisin, and bioinformatics analysis. The rs3480 (G) allele was associated with advanced steatosis (OR 1.29; 95% CI 1.08–1.55; p = 0.004), but not with other histological features. This effect was independent but additive to PNPLA3 and TM6SF2. The rs3480 polymorphism influenced FNDC5 mRNA stability and the binding of miR-135a-5P. Compared with controls, hepatic expression of this microRNA was upregulated while FNDC5 expression was downregulated. Elevated serum irisin was associated with reduced steatosis, and an improved metabolic profile. Carriage of the FNDC5 rs3480 minor (G) allele is associated with more severe steatosis in NAFLD through a microRNA-mediated mechanism controlling FNDC5 mRNA stability. Irisin is likely to have a favorable metabolic impact on NAFLD. Non-alcoholic fatty liver disease (NAFLD) is a principal liver disorder in Western countries and is on trajectory to become the leading cause of end-stage liver disease and liver transplantation.1–3Notably, NAFLD is part of a multisystem metabolic disturbance, as its impact is not limited to the liver but also affects extra-hepatic sites such as the cardiovascular system and kidneys.4In NAFLD, there is pathological hepatic accumulation of fat that over time can lead to inflammation and progress to cirrhosis, end-stage liver disease and hepatocellular carcinoma.The mechanisms underlying the accumulation of liver fat are complex and gene × environment interactions play a critical role.5–7
Irisin, the cleaved extra-cellular fragment of the Fibronectin type III domain-containing protein 5 (FNDC5) is a myokine that is proposed to have favorable metabolic activity. We aimed to elucidate the currently undefined role of variants in the FNDC5 gene in non-alcoholic fatty liver disease (NAFLD). We prioritized single nucleotide polymorphisms in FNDC5 on the basis of their putative biological function and identified rs3480 in the 3′ untranslated region (3′UTR). We studied the association of rs3480 with liver disease severity and the metabolic profile of 987 Caucasian patients with NAFLD. Functional investigations were undertaken using luciferase reporter assays of the 3′UTR of human FNDC5, pyrosequencing for allele-specific expression of FNDC5 in liver, measurement of serum irisin, and bioinformatics analysis. The rs3480 (G) allele was associated with advanced steatosis (OR 1.29; 95% CI 1.08–1.55; p = 0.004), but not with other histological features. This effect was independent but additive to PNPLA3 and TM6SF2. The rs3480 polymorphism influenced FNDC5 mRNA stability and the binding of miR-135a-5P. Compared with controls, hepatic expression of this microRNA was upregulated while FNDC5 expression was downregulated. Elevated serum irisin was associated with reduced steatosis, and an improved metabolic profile. Carriage of the FNDC5 rs3480 minor (G) allele is associated with more severe steatosis in NAFLD through a microRNA-mediated mechanism controlling FNDC5 mRNA stability. Irisin is likely to have a favorable metabolic impact on NAFLD. The heritable component of hepatic steatosis has been estimated at ∼50% based on a prospective twin study.8To date, the major inherited determinants of hepatic fat accumulation based on genome wide association studies are the patatin-like phospholipase domain-containing 3 (PNPLA3) I148M and the transmembrane 6 superfamily member 2 (TM6SF2) E167K gene variants.9,10.However, these polymorphisms explain only 10–20% of the heritability.6,11Thus, other as yet unidentified genetic and epigenetic variations likely exist to explain the missing heritability.
Irisin, the cleaved extra-cellular fragment of the Fibronectin type III domain-containing protein 5 (FNDC5) is a myokine that is proposed to have favorable metabolic activity. We aimed to elucidate the currently undefined role of variants in the FNDC5 gene in non-alcoholic fatty liver disease (NAFLD). We prioritized single nucleotide polymorphisms in FNDC5 on the basis of their putative biological function and identified rs3480 in the 3′ untranslated region (3′UTR). We studied the association of rs3480 with liver disease severity and the metabolic profile of 987 Caucasian patients with NAFLD. Functional investigations were undertaken using luciferase reporter assays of the 3′UTR of human FNDC5, pyrosequencing for allele-specific expression of FNDC5 in liver, measurement of serum irisin, and bioinformatics analysis. The rs3480 (G) allele was associated with advanced steatosis (OR 1.29; 95% CI 1.08–1.55; p = 0.004), but not with other histological features. This effect was independent but additive to PNPLA3 and TM6SF2. The rs3480 polymorphism influenced FNDC5 mRNA stability and the binding of miR-135a-5P. Compared with controls, hepatic expression of this microRNA was upregulated while FNDC5 expression was downregulated. Elevated serum irisin was associated with reduced steatosis, and an improved metabolic profile. Carriage of the FNDC5 rs3480 minor (G) allele is associated with more severe steatosis in NAFLD through a microRNA-mediated mechanism controlling FNDC5 mRNA stability. Irisin is likely to have a favorable metabolic impact on NAFLD. Adipose-derived hormones, collectively termed adipokines are established drivers/contributors to the pathogenesis of NAFLD.12–14More recently, the myokine irisin was isolated from muscle by Boström et al. 2012.15Irisin is a 12 kD, 112 amino acid fragment that is proteolytically processed from the fibronectin type III domain-containing protein 5 (FNDC5) and released into the circulation.16The functions of irisin and even its existence however have been a matter of debate,16,17 with some reports suggesting that the irisin polypeptide is a “myth”.17Human FNDC5 has an atypical ATA translation start codon rather than the ATG sequence.Hence, it has been argued that the human ATA codon represents a null mutation without irisin production and that reports measuring human irisin are an artefact from using an FNDC5 antibody with poor specificity.17However, recent work using quantitative mass spectrometry has confirmed the existence of circulating irisin.18The 3 published reports have shown inconsistent associations of NAFLD with irisin levels;19–21 in addition, the role of genetic variants in FNDC5 and the mechanisms involved have not been well defined.
Genetic hemochromatosis is mainly related to the homozygous p.Cys282Tyr (C282Y) mutation in the HFE gene, which causes hepcidin deficiency. Its low penetrance suggests the involvement of cofactors that modulate its expression. We aimed to describe the evolution of disease presentation and of non-genetic factors liable to impact hepcidin production in the long term. Clinical symptoms, markers of iron load, and risk factors according to the year of diagnosis were recorded over 30 years in a cohort of adult C282Y homozygotes. A total of 2,050 patients (1,460 probands [804 males and 656 females] and 542 relatives [244 males and 346 females]) were studied. Over time: (i) the proband-to-relative ratio remained roughly stable; (ii) the gender ratio tended towards equilibrium among probands; (iii) age at diagnosis did not change among males and increased among females; (iv) the frequency of diabetes and hepatic fibrosis steadily decreased while that of chronic fatigue and distal joint symptoms remained stable; (v) transferrin saturation, serum ferritin and the amount of iron removed decreased; and (vi) the prevalence of excessive alcohol consumption decreased while that of patients who were overweight increased. Tobacco smoking was associated with increased transferrin saturation. Genetic testing did not alter the age at diagnosis, which contrasts with the dramatic decrease in iron load in both genders. Tobacco smoking could be involved in the extent of iron loading. Besides HFE testing, which enables the diagnosis of minor forms of the disease, the reduction of alcohol consumption and the increased frequency of overweight patients may have played a role in the decreased long-term iron load, as these factors are likely to improve hepcidin production. In Caucasians, most cases of genetic iron overload result from the homozygous genotype for the HFE p.Cys282Tyr (C282Y) mutation, which leads to impaired production of hepcidin, the key regulator of systemic iron.1This results in increased iron efflux from cells, mainly from enterocytes and macrophages, and consequently in increased serum iron levels and transferrin saturation, leading to abnormal iron deposits in various parenchyma, especially the liver.1The clinical penetrance of the HFE C282Y homozygous genotype is fairly low, estimated at 30% among males and 1% among females.2Thus, C282Y homozygosity is a necessary, although insufficient, condition for developing clinical hemochromatosis.This suggests that genetic and environmental cofactors modulate its expression in terms of both iron load and organ damage.Genetic polymorphisms have been suggested as phenotypic modifiers but none has been found to be frequent enough to explain this low penetrance.3,4This suggests that non-genetic factors are likely to play a key role in disease presentation, especially those liable to interfere with hepcidin production, i.e. alcohol consumption, being overweight and tobacco smoking.
Although the majority of patients with non-alcoholic fatty liver disease (NAFLD) have only steatosis without progression, a sizeable fraction develop non-alcoholic steatohepatitis (NASH), which can lead to cirrhosis and hepatocellular carcinoma (HCC). Many established diet-induced mouse models for NASH require 24–52 weeks, which makes testing for drug response costly and time consuming. We have sought to establish a murine NASH model with rapid progression of extensive fibrosis and HCC by using a western diet (WD), which is high-fat, high-fructose and high-cholesterol, combined with low weekly dose of intraperitoneal carbon tetrachloride (CCl4), which serves as an accelerator. C57BL/6J mice were fed a normal chow diet ± CCl4 or WD ± CCl4 for 12 and 24 weeks. Addition of CCl4 exacerbated histological features of NASH, fibrosis, and tumor development induced by WD, which resulted in stage 3 fibrosis at 12 weeks and HCC development at 24 weeks. Furthermore, whole liver transcriptomic analysis indicated that dysregulated molecular pathways in WD/CCl4 mice and immunologic features were similar to those of human NASH. Our mouse NASH model exhibits rapid progression of advanced fibrosis and HCC, and mimics histological, immunological and transcriptomic features of human NASH, suggesting that it will be a useful experimental tool for preclinical drug testing. Non-alcoholic fatty liver disease (NAFLD) is a rising cause of chronic liver disease worldwide.Although the majority of patients with NAFLD have only steatosis without progression, a sizeable fraction develop non-alcoholic steatohepatitis (NASH), which can lead to cirrhosis, hepatocellular carcinoma (HCC), and increased liver-related mortality.1The prevalence of NAFLD in the US population is estimated at ∼24% (or ∼65 million) and up to a third of these individuals have NASH.2,3The prevalence of NAFLD is steadily increasing in parallel with the rising prevalence of obesity.NAFLD/NASH is already the third leading indication for liver transplantation, and the second leading cause of HCC requiring liver transplantation in the US,4 and it is likely to be the leading indication for transplantation by 2020.
Although the majority of patients with non-alcoholic fatty liver disease (NAFLD) have only steatosis without progression, a sizeable fraction develop non-alcoholic steatohepatitis (NASH), which can lead to cirrhosis and hepatocellular carcinoma (HCC). Many established diet-induced mouse models for NASH require 24–52 weeks, which makes testing for drug response costly and time consuming. We have sought to establish a murine NASH model with rapid progression of extensive fibrosis and HCC by using a western diet (WD), which is high-fat, high-fructose and high-cholesterol, combined with low weekly dose of intraperitoneal carbon tetrachloride (CCl4), which serves as an accelerator. C57BL/6J mice were fed a normal chow diet ± CCl4 or WD ± CCl4 for 12 and 24 weeks. Addition of CCl4 exacerbated histological features of NASH, fibrosis, and tumor development induced by WD, which resulted in stage 3 fibrosis at 12 weeks and HCC development at 24 weeks. Furthermore, whole liver transcriptomic analysis indicated that dysregulated molecular pathways in WD/CCl4 mice and immunologic features were similar to those of human NASH. Our mouse NASH model exhibits rapid progression of advanced fibrosis and HCC, and mimics histological, immunological and transcriptomic features of human NASH, suggesting that it will be a useful experimental tool for preclinical drug testing. Histologically, NASH is characterized by the presence of steatosis, inflammation, hepatocyte injury (ballooning), and/or fibrosis.1Key risk factors include diabetes, obesity, age, ethnicity, gender, and genetic polymorphisms that can affect natural history and disease progression.These divergent risk factors reflect the complex and heterogeneous nature of the disease.
Although the majority of patients with non-alcoholic fatty liver disease (NAFLD) have only steatosis without progression, a sizeable fraction develop non-alcoholic steatohepatitis (NASH), which can lead to cirrhosis and hepatocellular carcinoma (HCC). Many established diet-induced mouse models for NASH require 24–52 weeks, which makes testing for drug response costly and time consuming. We have sought to establish a murine NASH model with rapid progression of extensive fibrosis and HCC by using a western diet (WD), which is high-fat, high-fructose and high-cholesterol, combined with low weekly dose of intraperitoneal carbon tetrachloride (CCl4), which serves as an accelerator. C57BL/6J mice were fed a normal chow diet ± CCl4 or WD ± CCl4 for 12 and 24 weeks. Addition of CCl4 exacerbated histological features of NASH, fibrosis, and tumor development induced by WD, which resulted in stage 3 fibrosis at 12 weeks and HCC development at 24 weeks. Furthermore, whole liver transcriptomic analysis indicated that dysregulated molecular pathways in WD/CCl4 mice and immunologic features were similar to those of human NASH. Our mouse NASH model exhibits rapid progression of advanced fibrosis and HCC, and mimics histological, immunological and transcriptomic features of human NASH, suggesting that it will be a useful experimental tool for preclinical drug testing. Despite the growing public health impact of NASH, treatment options remain limited and there are no FDA-approved therapies.One obstacle to drug development has been the paucity of standardized and relevant animal models.Although there are several dietary and genetic models of NASH described, few, if any, replicate all the metabolic, histologic and genetic features of the human disease.5Mice are generally preferable because they are easy to handle and suitable for drug testing.Also, dietary models are preferred because genetic models often induce NASH by manipulating one specific molecule/pathway.For example, a methionine/choline-deficient (MCD) diet has traditionally been used to induce histological features of NASH.However, mice treated with MCD diet lose body weight, and do not develop insulin resistance and related co-morbidities.6,7A western diet (WD), which is high-fat, high-fructose and high-cholesterol, mimics fast food style diets that have been implicated in NASH pathogenesis in humans.8WD treatment induces obesity and insulin resistance in addition to NASH histology.However, the WD-based models do not fully progress to severe steatohepatitis and advanced fibrosis, even after long-term feeding for 25 to 52 weeks.8,9
Although the majority of patients with non-alcoholic fatty liver disease (NAFLD) have only steatosis without progression, a sizeable fraction develop non-alcoholic steatohepatitis (NASH), which can lead to cirrhosis and hepatocellular carcinoma (HCC). Many established diet-induced mouse models for NASH require 24–52 weeks, which makes testing for drug response costly and time consuming. We have sought to establish a murine NASH model with rapid progression of extensive fibrosis and HCC by using a western diet (WD), which is high-fat, high-fructose and high-cholesterol, combined with low weekly dose of intraperitoneal carbon tetrachloride (CCl4), which serves as an accelerator. C57BL/6J mice were fed a normal chow diet ± CCl4 or WD ± CCl4 for 12 and 24 weeks. Addition of CCl4 exacerbated histological features of NASH, fibrosis, and tumor development induced by WD, which resulted in stage 3 fibrosis at 12 weeks and HCC development at 24 weeks. Furthermore, whole liver transcriptomic analysis indicated that dysregulated molecular pathways in WD/CCl4 mice and immunologic features were similar to those of human NASH. Our mouse NASH model exhibits rapid progression of advanced fibrosis and HCC, and mimics histological, immunological and transcriptomic features of human NASH, suggesting that it will be a useful experimental tool for preclinical drug testing. Carbon tetrachloride (CCl4) has been widely used for decades to induce liver injury and fibrosis in mice.10A previous report has suggested that multiple administrations of CCl4 with a high-fat diet induce oxidative stress that triggers inflammation and apoptosis, leading to the development of fibrosis in mice.11Also, combined chronic treatment with CCl4 and choline-deficient L-amino-acid-defined-diet for up to nine months can induce NASH with fibrosis, and HCC.12However, these reports have lacked blinded quantitative assessment of the three features of the NAFLD activity score, which is used routinely in human studies (steatosis, lobular inflammation, and hepatocyte ballooning); moreover, the fibrosis stages have not been quantified or fully characterized.Importantly, the effects of chronic treatment with CCl4 combined with a WD have not been assessed as a potential model for NASH.
Liver macrosteatosis (MS) is a major predictor of graft dysfunction after transplantation. However, frozen section techniques to quantify steatosis are often unavailable in the context of procurements, and the findings of preoperative imaging techniques correlate poorly with those of permanent sections, so that the surgeon is ultimately responsible for the decision. Our aim was to assess the accuracy of a non-invasive pocket-sized micro-spectrometer (PSM) for the real-time estimation of MS. We prospectively evaluated a commercial PSM by scanning the liver capsule. A double pathological quantification of MS was performed on permanent sections. Initial calibration (training cohort) was performed on 35 livers (MS ≤60%) and an algorithm was created to correlate the estimated (PSM) and known (pathological) MS values. A second assessment (validation cohort) was then performed on 154 grafts. Our algorithm achieved a coefficient of determination R2 = 0.81. Its validation on the second cohort demonstrated a Lin’s concordance coefficient of 0.78. Accuracy reached 0.91%, with reproducibility of 86.3%. The sensitivity, specificity, positive and negative predictive values for MS ≥30% were 66.7%, 100%, 100% and 98%, respectively. The PSM could predict the absence (<30%)/presence (≥30%) of MS with a kappa coefficient of 0.79. Neither graft weight nor height, donor body mass index nor the CT-scan liver-to-spleen attenuation ratio could accurately predict MS. We demonstrated that a PSM can reliably and reproducibly assess mild-to-moderate MS. Its low cost and the immediacy of results may offer considerable added-value decision support for surgeons. This tool could avoid the detrimental and prolonged ischaemia caused by the pathological examination of (potentially) marginal grafts. This device now needs to be assessed in the context of a large-scale multicentre study. The current organ shortage has led most liver transplant teams to use marginal grafts that modify the benefit-risk ratio for recipients and impose a heavy responsibility on the surgical teams.1
Liver macrosteatosis (MS) is a major predictor of graft dysfunction after transplantation. However, frozen section techniques to quantify steatosis are often unavailable in the context of procurements, and the findings of preoperative imaging techniques correlate poorly with those of permanent sections, so that the surgeon is ultimately responsible for the decision. Our aim was to assess the accuracy of a non-invasive pocket-sized micro-spectrometer (PSM) for the real-time estimation of MS. We prospectively evaluated a commercial PSM by scanning the liver capsule. A double pathological quantification of MS was performed on permanent sections. Initial calibration (training cohort) was performed on 35 livers (MS ≤60%) and an algorithm was created to correlate the estimated (PSM) and known (pathological) MS values. A second assessment (validation cohort) was then performed on 154 grafts. Our algorithm achieved a coefficient of determination R2 = 0.81. Its validation on the second cohort demonstrated a Lin’s concordance coefficient of 0.78. Accuracy reached 0.91%, with reproducibility of 86.3%. The sensitivity, specificity, positive and negative predictive values for MS ≥30% were 66.7%, 100%, 100% and 98%, respectively. The PSM could predict the absence (<30%)/presence (≥30%) of MS with a kappa coefficient of 0.79. Neither graft weight nor height, donor body mass index nor the CT-scan liver-to-spleen attenuation ratio could accurately predict MS. We demonstrated that a PSM can reliably and reproducibly assess mild-to-moderate MS. Its low cost and the immediacy of results may offer considerable added-value decision support for surgeons. This tool could avoid the detrimental and prolonged ischaemia caused by the pathological examination of (potentially) marginal grafts. This device now needs to be assessed in the context of a large-scale multicentre study. Liver steatosis (LS) remains a major concern in liver transplantation (LT) because non-alcoholic fatty liver disease can affect up to 30% of individuals (potential donors) in western countries, as confirmed by the reported incidence of LS during procurement.2LS involves 2 types of steatosis: macrosteatosis (MS) characterised by a single, bulky fat vacuole in hepatocytes that displaces the nucleus to the edge of the cell, and microsteatosis when the cytoplasm of hepatocytes contains tiny lipid vesicles without nuclear dislocation.In almost all reports, only MS has negatively impacted outcomes after LT, while the low or negligible impact of microsteatosis is accepted.3–6If steatotic (MS) grafts are used, there is general consensus regarding a higher incidence of primary non-function and biliary complications, increased costs and longer stays in hospital, associated with poorer patient and graft survivals.7–11These grafts are more susceptible to cold ischaemia,12 which explains why there is a growing body of literature on the normothermic preservation of fatty livers in order to limit ischaemia-reperfusion disorders and induce “defatting”.1,13,14
Liver macrosteatosis (MS) is a major predictor of graft dysfunction after transplantation. However, frozen section techniques to quantify steatosis are often unavailable in the context of procurements, and the findings of preoperative imaging techniques correlate poorly with those of permanent sections, so that the surgeon is ultimately responsible for the decision. Our aim was to assess the accuracy of a non-invasive pocket-sized micro-spectrometer (PSM) for the real-time estimation of MS. We prospectively evaluated a commercial PSM by scanning the liver capsule. A double pathological quantification of MS was performed on permanent sections. Initial calibration (training cohort) was performed on 35 livers (MS ≤60%) and an algorithm was created to correlate the estimated (PSM) and known (pathological) MS values. A second assessment (validation cohort) was then performed on 154 grafts. Our algorithm achieved a coefficient of determination R2 = 0.81. Its validation on the second cohort demonstrated a Lin’s concordance coefficient of 0.78. Accuracy reached 0.91%, with reproducibility of 86.3%. The sensitivity, specificity, positive and negative predictive values for MS ≥30% were 66.7%, 100%, 100% and 98%, respectively. The PSM could predict the absence (<30%)/presence (≥30%) of MS with a kappa coefficient of 0.79. Neither graft weight nor height, donor body mass index nor the CT-scan liver-to-spleen attenuation ratio could accurately predict MS. We demonstrated that a PSM can reliably and reproducibly assess mild-to-moderate MS. Its low cost and the immediacy of results may offer considerable added-value decision support for surgeons. This tool could avoid the detrimental and prolonged ischaemia caused by the pathological examination of (potentially) marginal grafts. This device now needs to be assessed in the context of a large-scale multicentre study. The principal issue regarding LS in the LT setting is the diagnosis and quantification of MS. There is a global agreement that mild MS (<30%) causes little or no graft injury, while a moderate (30–60%) or high (>60%) degree of MS constitutes a significantly higher risk.7,9,15,16However, the preoperative diagnosis of MS remains a challenge.Many non-invasive techniques have been described but their accuracy remains a matter of debate:17 i) ultrasonography is able to detect the presence of severe steatosis but remains relatively inaccurate (non-quantitative) and operator-dependent procedure;18 ii) despite many liver attenuation indices published, a diagnosis of mild-to-moderate LS remains insufficient using a CT scan;19,20 iii) the magnetic resonance spectroscopy examinations are accurate but costly and unavailable before organ retrieval;21,22 iv) the use of percutaneous ultrasonic controlled attenuation parameter (CAP) is a promising technique but we are still awaiting cheap machines with reliable and consensual cut-off values for the distinction of moderate/high LS content.23
Liver macrosteatosis (MS) is a major predictor of graft dysfunction after transplantation. However, frozen section techniques to quantify steatosis are often unavailable in the context of procurements, and the findings of preoperative imaging techniques correlate poorly with those of permanent sections, so that the surgeon is ultimately responsible for the decision. Our aim was to assess the accuracy of a non-invasive pocket-sized micro-spectrometer (PSM) for the real-time estimation of MS. We prospectively evaluated a commercial PSM by scanning the liver capsule. A double pathological quantification of MS was performed on permanent sections. Initial calibration (training cohort) was performed on 35 livers (MS ≤60%) and an algorithm was created to correlate the estimated (PSM) and known (pathological) MS values. A second assessment (validation cohort) was then performed on 154 grafts. Our algorithm achieved a coefficient of determination R2 = 0.81. Its validation on the second cohort demonstrated a Lin’s concordance coefficient of 0.78. Accuracy reached 0.91%, with reproducibility of 86.3%. The sensitivity, specificity, positive and negative predictive values for MS ≥30% were 66.7%, 100%, 100% and 98%, respectively. The PSM could predict the absence (<30%)/presence (≥30%) of MS with a kappa coefficient of 0.79. Neither graft weight nor height, donor body mass index nor the CT-scan liver-to-spleen attenuation ratio could accurately predict MS. We demonstrated that a PSM can reliably and reproducibly assess mild-to-moderate MS. Its low cost and the immediacy of results may offer considerable added-value decision support for surgeons. This tool could avoid the detrimental and prolonged ischaemia caused by the pathological examination of (potentially) marginal grafts. This device now needs to be assessed in the context of a large-scale multicentre study. Practically, the final decision often relies on the macroscopic appearance of the graft, even though it is well known that an evaluation performed by the surgeon is poorly correlated to pathological estimations.24,25It must also be remembered that the results of frozen sections are not aligned with those of permanent sections, with MS being underestimated in 75% cases.26Moreover, a frozen section is not always technically feasible (organisational issues) as its accuracy depends on the hospital where the retrieval is performed (often peripheral), and it frequently results in a longer cold ischaemia time if the biopsy needs to be transferred from the hospital where the procurement takes place to the transplant centre.
Liver macrosteatosis (MS) is a major predictor of graft dysfunction after transplantation. However, frozen section techniques to quantify steatosis are often unavailable in the context of procurements, and the findings of preoperative imaging techniques correlate poorly with those of permanent sections, so that the surgeon is ultimately responsible for the decision. Our aim was to assess the accuracy of a non-invasive pocket-sized micro-spectrometer (PSM) for the real-time estimation of MS. We prospectively evaluated a commercial PSM by scanning the liver capsule. A double pathological quantification of MS was performed on permanent sections. Initial calibration (training cohort) was performed on 35 livers (MS ≤60%) and an algorithm was created to correlate the estimated (PSM) and known (pathological) MS values. A second assessment (validation cohort) was then performed on 154 grafts. Our algorithm achieved a coefficient of determination R2 = 0.81. Its validation on the second cohort demonstrated a Lin’s concordance coefficient of 0.78. Accuracy reached 0.91%, with reproducibility of 86.3%. The sensitivity, specificity, positive and negative predictive values for MS ≥30% were 66.7%, 100%, 100% and 98%, respectively. The PSM could predict the absence (<30%)/presence (≥30%) of MS with a kappa coefficient of 0.79. Neither graft weight nor height, donor body mass index nor the CT-scan liver-to-spleen attenuation ratio could accurately predict MS. We demonstrated that a PSM can reliably and reproducibly assess mild-to-moderate MS. Its low cost and the immediacy of results may offer considerable added-value decision support for surgeons. This tool could avoid the detrimental and prolonged ischaemia caused by the pathological examination of (potentially) marginal grafts. This device now needs to be assessed in the context of a large-scale multicentre study. Recent publications in the LT setting on infrared spectroscopy have produced some very promising results as it enabled an accurate quantification of LS.27–29Spectroscopy is based on determining the absorption of infrared light due to resonance with vibrational motions of functional molecular groups.Clinical studies have already demonstrated the feasibility and reliability of this concept.30,31However, the outstanding issue is that this technique requires expensive and non-transportable equipment.Until now, clinical experiments required contact between a probe and the liver (introduction of a needle into the organ), this being an invasive technique with theoretical complications.
The Karnofsky performance status (KPS) has been used for almost 70 years for clinical assessment of patients. Our objective was to determine whether KPS is an independent predictor of post-liver transplant (LT) survival after adjusting for known confounders. Adult patients listed with the United Network for Organ Sharing (UNOS) from 2006 to 2016 were grouped into low (10–40%, n = 15,103), intermediate (50–70%, n = 22,183) and high (80–100%, n = 13,131) KPS groups based on KPS scores at the time of LT, after excluding those on ventilators or life support. We determined the trends in KPS before and after LT, and survival probabilities based on KPS. There was a decline in KPS scores between listing and LT and there was significant improvement after LT. The graft and patient survival differences were significantly lower (p <0.0001) in those with low KPS. After adjusting for other confounders, the hazard ratios for graft failure were 1.17 (1.12–1.22, p <0.01) for the intermediate and 1.38 (1.31–1.46, p <0.01) for the low group. Similarly, hazard ratios for patient failure were 1.18 (1.13–1.24, p <0.01) for the intermediate and 1.43 (1.35–1.52, p <0.01) for the low group. Other independent negative predictors for graft and patient survival were older age, Black ethnicity, presence of hepatic encephalopathy and donor risk index. Those who did not show significant improvements in post-LT KPS scores had poorer outcomes in all three KPS groups, but it was most obvious in the low KPS group with one-year patient survival of 33%. The KPS, before and after LT, is an independent predictor of graft and patient survival after adjusting for other important predictors of survival. The Karnofsky performance status (KPS) has been used for almost 70 years in clinical practice as a subjective ‘eyeball’ assessment of the overall performance status of patients.The KPS scores, administered by the provider or support staff, assign scores to patients on a scale of 0–100%, in increments of 10, where 100% is normal activity and 0% is dead.1It is widely used in general oncology practice as a prognostic predictor and also for the selection of patients in clinical trials.4–10The inter-observer reliability, validity and reproducibility of KPS scores in multiple clinical settings have shown to be excellent.2,10,11Recently, the KPS was shown to be a useful tool for predicting survival in patients admitted to hospitals with complications of cirrhosis and was shown to be a predictor of transplant waitlist mortality.12,13
The Karnofsky performance status (KPS) has been used for almost 70 years for clinical assessment of patients. Our objective was to determine whether KPS is an independent predictor of post-liver transplant (LT) survival after adjusting for known confounders. Adult patients listed with the United Network for Organ Sharing (UNOS) from 2006 to 2016 were grouped into low (10–40%, n = 15,103), intermediate (50–70%, n = 22,183) and high (80–100%, n = 13,131) KPS groups based on KPS scores at the time of LT, after excluding those on ventilators or life support. We determined the trends in KPS before and after LT, and survival probabilities based on KPS. There was a decline in KPS scores between listing and LT and there was significant improvement after LT. The graft and patient survival differences were significantly lower (p <0.0001) in those with low KPS. After adjusting for other confounders, the hazard ratios for graft failure were 1.17 (1.12–1.22, p <0.01) for the intermediate and 1.38 (1.31–1.46, p <0.01) for the low group. Similarly, hazard ratios for patient failure were 1.18 (1.13–1.24, p <0.01) for the intermediate and 1.43 (1.35–1.52, p <0.01) for the low group. Other independent negative predictors for graft and patient survival were older age, Black ethnicity, presence of hepatic encephalopathy and donor risk index. Those who did not show significant improvements in post-LT KPS scores had poorer outcomes in all three KPS groups, but it was most obvious in the low KPS group with one-year patient survival of 33%. The KPS, before and after LT, is an independent predictor of graft and patient survival after adjusting for other important predictors of survival. The model for end-stage liver disease (MELD) scores is objective, but does not incorporate many variables that may predict outcomes before and after liver transplantation including malnutrition or morbid obesity, mobility and performance status.Despite its subjectivity, KPS scores reflect the overall assessment of a patient’s performance status that may include some of the subjective tools that are difficult to quantify in an objective manner.2Recently, there have been attempts to develop objective tools, such as frailty index, six-minute walk distance and sarcopenia, to assess patients with liver diseases, but their utility in epidemiological studies remain unknown.14–18
To improve outcomes of two-staged hepatectomies for large/multiple liver tumors, portal vein ligation (PVL) has been combined with parenchymal transection (associating liver partition and portal vein ligation for staged hepatectomy [coined ALPPS]) to greatly accelerate liver regeneration. In a novel ALPPS mouse model, we have reported paracrine Indian hedgehog (IHH) signaling from stellate cells as an early contributor to augmented regeneration. Here, we sought to identify upstream regulators of IHH. ALPPS in mice was compared against PVL and additional control surgeries. Potential IHH regulators were identified through in silico mining of transcriptomic data. c-Jun N-terminal kinase (JNK1 [Mapk8]) activity was reduced through SP600125 to evaluate its effects on IHH signaling. Recombinant IHH was injected after JNK1 diminution to substantiate their relationship during accelerated liver regeneration. Transcriptomic analysis linked Ihh to Mapk8. JNK1 upregulation after ALPPS was validated and preceded the IHH peak. On immunofluorescence, JNK1 and IHH co-localized in alpha-smooth muscle actin-positive non-parenchymal cells. Inhibition of JNK1 prior to ALPPS surgery reduced liver weight gain to PVL levels and was accompanied by downregulation of hepatocellular proliferation and the IHH-GLI1-CCND1 axis. In JNK1-inhibited mice, recombinant IHH restored ALPPS-like acceleration of regeneration and re-elevated JNK1 activity, suggesting the presence of a positive IHH-JNK1 feedback loop. JNK1-mediated induction of IHH paracrine signaling from hepatic stellate cells is essential for accelerated regeneration of parenchymal mass. The JNK1-IHH axis is a mechanism unique to ALPPS surgery and may point to therapeutic alternatives for patients with insufficient regenerative capacity. The unique ability of mammalian liver to regain mass after tissue loss has revolutionized the treatment and cure of many patients with liver tumors.However, there are limitations to effective regeneration, as hepatic failure may develop after extensive liver resection.This entity, known as the small-for-size syndrome (SFSS), results from an insufficient functional volume of the liver remnant and remains the most frequent cause of death due to liver surgery.1,2Two-staged hepatectomies were introduced to reduce the SFSS risk.Typically, the portal vein draining the part of the liver containing the tumor is occluded (step 1), causing growth of the contralateral liver part (defined as the functional liver remnant [FLR]); when the FLR has gained sufficient functional volume, step 2 (resection of the diseased part) is performed.2,3Nevertheless, in some cases regeneration is still insufficient, or the considerable time period between step 1 and 2 allows for further progression of the disease.4,5To additionally reduce the risk of SFSS in patients with large or multiple liver tumors, associating liver partition and portal vein ligation (PVL) for staged hepatectomy ([ALPPS] the combination of PVL with parenchymal transection) has been showcased as a procedure that induces accelerated liver regeneration and greatly reduces the interval between steps, allowing treatment of patients otherwise deemed unresectable.6,7This procedure, introduced about five years ago, has gained sustained acceptance with more than 1,000 cases included in an international registry (http://www.alpps.net/?q=registry).
To improve outcomes of two-staged hepatectomies for large/multiple liver tumors, portal vein ligation (PVL) has been combined with parenchymal transection (associating liver partition and portal vein ligation for staged hepatectomy [coined ALPPS]) to greatly accelerate liver regeneration. In a novel ALPPS mouse model, we have reported paracrine Indian hedgehog (IHH) signaling from stellate cells as an early contributor to augmented regeneration. Here, we sought to identify upstream regulators of IHH. ALPPS in mice was compared against PVL and additional control surgeries. Potential IHH regulators were identified through in silico mining of transcriptomic data. c-Jun N-terminal kinase (JNK1 [Mapk8]) activity was reduced through SP600125 to evaluate its effects on IHH signaling. Recombinant IHH was injected after JNK1 diminution to substantiate their relationship during accelerated liver regeneration. Transcriptomic analysis linked Ihh to Mapk8. JNK1 upregulation after ALPPS was validated and preceded the IHH peak. On immunofluorescence, JNK1 and IHH co-localized in alpha-smooth muscle actin-positive non-parenchymal cells. Inhibition of JNK1 prior to ALPPS surgery reduced liver weight gain to PVL levels and was accompanied by downregulation of hepatocellular proliferation and the IHH-GLI1-CCND1 axis. In JNK1-inhibited mice, recombinant IHH restored ALPPS-like acceleration of regeneration and re-elevated JNK1 activity, suggesting the presence of a positive IHH-JNK1 feedback loop. JNK1-mediated induction of IHH paracrine signaling from hepatic stellate cells is essential for accelerated regeneration of parenchymal mass. The JNK1-IHH axis is a mechanism unique to ALPPS surgery and may point to therapeutic alternatives for patients with insufficient regenerative capacity. Although there have been several clinical studies evaluating ALPPS in patients, reports on the molecular mechanisms underlying the regenerative acceleration following ALPPS step 1 remain scarce.Several components known to be necessary for liver regeneration after partial hepatectomy have likewise been associated with accelerated regeneration.In rodent models, IL-6 and TNFα – established mediators of liver regeneration – have been found upregulated in ALPPS at the mRNA and protein level.Similarly, protein expression of STAT3, RELA, and YAP1 – known transcription factors in the regenerating liver - were elevated after ALPPS step 1.8,9Furthermore, hypoxia has recently been shown to be an important driver of normal regeneration through HIF2α, with preliminary studies pointing to the importance of hypoxia in ALPPS as well.10,11In human ALPPS tissues, IL6 and TNF have been validated on the mRNA level.12However, none of these molecules seem to be unique to the ALPPS-induced acceleration of regeneration.
To improve outcomes of two-staged hepatectomies for large/multiple liver tumors, portal vein ligation (PVL) has been combined with parenchymal transection (associating liver partition and portal vein ligation for staged hepatectomy [coined ALPPS]) to greatly accelerate liver regeneration. In a novel ALPPS mouse model, we have reported paracrine Indian hedgehog (IHH) signaling from stellate cells as an early contributor to augmented regeneration. Here, we sought to identify upstream regulators of IHH. ALPPS in mice was compared against PVL and additional control surgeries. Potential IHH regulators were identified through in silico mining of transcriptomic data. c-Jun N-terminal kinase (JNK1 [Mapk8]) activity was reduced through SP600125 to evaluate its effects on IHH signaling. Recombinant IHH was injected after JNK1 diminution to substantiate their relationship during accelerated liver regeneration. Transcriptomic analysis linked Ihh to Mapk8. JNK1 upregulation after ALPPS was validated and preceded the IHH peak. On immunofluorescence, JNK1 and IHH co-localized in alpha-smooth muscle actin-positive non-parenchymal cells. Inhibition of JNK1 prior to ALPPS surgery reduced liver weight gain to PVL levels and was accompanied by downregulation of hepatocellular proliferation and the IHH-GLI1-CCND1 axis. In JNK1-inhibited mice, recombinant IHH restored ALPPS-like acceleration of regeneration and re-elevated JNK1 activity, suggesting the presence of a positive IHH-JNK1 feedback loop. JNK1-mediated induction of IHH paracrine signaling from hepatic stellate cells is essential for accelerated regeneration of parenchymal mass. The JNK1-IHH axis is a mechanism unique to ALPPS surgery and may point to therapeutic alternatives for patients with insufficient regenerative capacity. We have developed a mouse model of ALPPS and documented the release of serum factors that can accelerate regeneration in mice following PVL to levels seen after the complete ALPPS step 1 procedure.12These findings suggest that humoral factors must play essential roles in the early instigation of accelerated regeneration.With this mouse model, we established that the secretion of Indian Hedgehog (IHH) from hepatic stellate cells (HSCs) was necessary for the early activation and progression of accelerated regeneration.13Although hedgehog signaling has previously been reported to be required for regeneration after hepatectomy, the early elevations in serum IHH were unique to ALPPS, and not observed following hepatectomy or other surgeries.13
To improve outcomes of two-staged hepatectomies for large/multiple liver tumors, portal vein ligation (PVL) has been combined with parenchymal transection (associating liver partition and portal vein ligation for staged hepatectomy [coined ALPPS]) to greatly accelerate liver regeneration. In a novel ALPPS mouse model, we have reported paracrine Indian hedgehog (IHH) signaling from stellate cells as an early contributor to augmented regeneration. Here, we sought to identify upstream regulators of IHH. ALPPS in mice was compared against PVL and additional control surgeries. Potential IHH regulators were identified through in silico mining of transcriptomic data. c-Jun N-terminal kinase (JNK1 [Mapk8]) activity was reduced through SP600125 to evaluate its effects on IHH signaling. Recombinant IHH was injected after JNK1 diminution to substantiate their relationship during accelerated liver regeneration. Transcriptomic analysis linked Ihh to Mapk8. JNK1 upregulation after ALPPS was validated and preceded the IHH peak. On immunofluorescence, JNK1 and IHH co-localized in alpha-smooth muscle actin-positive non-parenchymal cells. Inhibition of JNK1 prior to ALPPS surgery reduced liver weight gain to PVL levels and was accompanied by downregulation of hepatocellular proliferation and the IHH-GLI1-CCND1 axis. In JNK1-inhibited mice, recombinant IHH restored ALPPS-like acceleration of regeneration and re-elevated JNK1 activity, suggesting the presence of a positive IHH-JNK1 feedback loop. JNK1-mediated induction of IHH paracrine signaling from hepatic stellate cells is essential for accelerated regeneration of parenchymal mass. The JNK1-IHH axis is a mechanism unique to ALPPS surgery and may point to therapeutic alternatives for patients with insufficient regenerative capacity. Hedgehog activity is barely detectable in healthy adult liver.Upon hepatectomy, however, hepatocytes begin to proliferate, and HSCs undergo differentiation into myofibroblastic and fibrogenic states.14,15Furthermore, there is increasing evidence that morphogenic signals – such as the hedgehog pathway – are mediators of the myofibroblastic HSC switch.When blocking or knocking out Smo, the receptor for hedgehog ligands, HSCs do not acquire myofibroblastic traits, and subsequent hepatocyte regeneration is inhibited.16–18
Osteoporotic fractures are a major cause of morbidity and reduced quality of life in patients with primary sclerosing cholangitis (PSC), a progressive bile duct disease of unknown origin. Although it is generally assumed that this pathology is a consequence of impaired calcium homeostasis and malabsorption, the cellular and molecular causes of PSC-associated osteoporosis are unknown. We determined bone mineral density by dual-X-ray absorptiometry and assessed bone microstructure by high-resolution peripheral quantitative computed tomography in patients with PSC. Laboratory markers of liver and bone metabolism were measured, and liver stiffness was assessed by FibroScan. We determined the frequency of Th17 cells by the ex vivo stimulation of peripheral blood mononuclear cells in a subgroup of 40 patients with PSC. To investigate the potential involvement of IL-17 in PSC-associated bone loss, we analyzed the skeletal phenotype of mice lacking Abcb4 and/or Il-17. Unlike in patients with primary biliary cholangitis, bone loss in patients with PSC was not associated with disease duration or liver fibrosis. However, we observed a significant negative correlation between the bone resorption biomarker deoxypyridinoline and bone mineral density in the PSC cohort, indicating increased bone resorption. Importantly, the frequency of Th17 cells in peripheral blood was positively correlated with the urinary deoxypyridinoline level and negatively correlated with bone mass. We observed that Abcb4-deficient mice displayed a low-bone-mass phenotype, which was corrected by an additional Il-17 deficiency or anti-IL-17 treatment, whereas the liver pathology was unaffected. Our findings demonstrate that an increased frequency of Th17 cells is associated with bone resorption in PSC. Whether antibody-based IL-17 blockade is beneficial against bone loss in patients with PSC should be addressed in future studies. Primary sclerosing cholangitis (PSC) is a severe idiopathic disease characterized by the progressive fibrosis of intrahepatic and extrahepatic bile ducts with a median age at onset of between 30–40 years.1–3In contrast to primary biliary cholangitis (PBC), which predominantly affects middle-aged or elderly women,2 approximately 60% of PSC patients additionally develop colitis.Genetic variations associated with PSC can involve genes related to the immune system (human leukocyte antigen) and pathways that mediate inflammation.These associations are clearly different from those of ulcerative colitis.3In addition to genetic risk factors, environmental influences also play important roles in the development of biliary inflammation and fibrosis.4In this context, changes in microbial flora in the gut and subsequent activation of the immune system may be an important driving factor.5In fact, recent studies have shown a clear difference in the gut microbiome between patients with PSC and healthy controls.6We have previously shown that the bile fluid of patients with PSC is frequently colonized with different pathogens.Moreover, patients with PSC displayed a higher Th17 cell frequency after the stimulation of peripheral blood mononuclear cells with these pathogens, suggesting an important role of Th17 cells in the pathogenesis of PSC.7However, the impact of microbial alterations on immune dysregulation and the perpetuation of biliary inflammation remain to be determined.
Osteoporotic fractures are a major cause of morbidity and reduced quality of life in patients with primary sclerosing cholangitis (PSC), a progressive bile duct disease of unknown origin. Although it is generally assumed that this pathology is a consequence of impaired calcium homeostasis and malabsorption, the cellular and molecular causes of PSC-associated osteoporosis are unknown. We determined bone mineral density by dual-X-ray absorptiometry and assessed bone microstructure by high-resolution peripheral quantitative computed tomography in patients with PSC. Laboratory markers of liver and bone metabolism were measured, and liver stiffness was assessed by FibroScan. We determined the frequency of Th17 cells by the ex vivo stimulation of peripheral blood mononuclear cells in a subgroup of 40 patients with PSC. To investigate the potential involvement of IL-17 in PSC-associated bone loss, we analyzed the skeletal phenotype of mice lacking Abcb4 and/or Il-17. Unlike in patients with primary biliary cholangitis, bone loss in patients with PSC was not associated with disease duration or liver fibrosis. However, we observed a significant negative correlation between the bone resorption biomarker deoxypyridinoline and bone mineral density in the PSC cohort, indicating increased bone resorption. Importantly, the frequency of Th17 cells in peripheral blood was positively correlated with the urinary deoxypyridinoline level and negatively correlated with bone mass. We observed that Abcb4-deficient mice displayed a low-bone-mass phenotype, which was corrected by an additional Il-17 deficiency or anti-IL-17 treatment, whereas the liver pathology was unaffected. Our findings demonstrate that an increased frequency of Th17 cells is associated with bone resorption in PSC. Whether antibody-based IL-17 blockade is beneficial against bone loss in patients with PSC should be addressed in future studies. One serious complication in patients with cholestatic liver diseases is osteoporosis.In contrast to the occurrence of osteoporosis accompanied by low bone formation in PBC, which has been reported repeatedly,8–10 this pattern is less established for PSC.However, in a previous study involving 237 patients with PSC, osteoporosis was found in 15% of patients.11These authors also reported that osteoporosis was associated with age, body mass index (BMI) and the presence of inflammatory bowel disease (IBD) in their PSC cohort.However, the respective study also had some limitations, as the patients did not undergo examination for the analysis of biomarkers of bone turnover, trabecular and cortical bone architecture, or distinct immune cell populations.It therefore remains to be established which bone remodeling cell type is primarily affected in PSC.Considering that liver transplantation may be required in most patients and that subsequent immunosuppression is an additional risk factor for osteoporosis, determination of the bone status and providing adequate treatment is crucial for preventing fractures in affected patients.
Primary biliary cholangitis (PBC) is an autoimmune-associated chronic liver disease triggered by environmental factors, such as exposure to xenobiotics, which leads to a loss of tolerance to the lipoic acid-conjugated regions of the mitochondrial pyruvate dehydrogenase complex, typically to the E2 component. We aimed to identify xenobiotics that might be involved in the environmental triggering of PBC. Urban landfill and control soil samples from a region with high PBC incidence were screened for xenobiotic activities using analytical, cell-based xenobiotic receptor activation assays and toxicity screens. A variety of potential xenobiotic classes were ubiquitously present, as identified by their interaction with xenobiotic receptors – aryl hydrocarbon receptor, androgen receptor and peroxisome proliferator activated receptor alpha – in cell-based screens. In contrast, xenoestrogens were present at higher levels in soil extracts from around an urban landfill. Furthermore, two landfill sampling sites contained a chemical(s) that inhibited mitochondrial oxidative phosphorylation and induced the apoptosis of a hepatic progenitor cell. The mitochondrial effect was also demonstrated in human liver cholangiocytes from three separate donors. The chemical was identified as the ionic liquid [3-methyl-1-octyl-1H-imidazol-3-ium]+ (M8OI) and the toxic effects were recapitulated using authentic pure chemical. A carboxylate-containing human hepatocyte metabolite of M8OI, bearing structural similarity to lipoic acid, was also enzymatically incorporated into the E2 component of the pyruvate dehydrogenase complex via the exogenous lipoylation pathway in vitro. These results identify, for the first time, a xenobiotic in the environment that may be related to and/or be a component of an environmental trigger for PBC. Therefore, further study in experimental animal models is warranted, to determine the risk of exposure to these ionic liquids. Liver disease constitutes the third most common cause of premature death in the UK, with an upward trend in mortality.1,2The major causes of liver disease such as obesity, viral infection and chronic alcohol consumption are preventable.However, the causes of rarer types of liver disease – such as primary biliary cholangitis (PBC) and primary sclerosing cholangitis (PSC) are unknown and as a consequence, prevention and/or treatments are limited.3A variety of factors have been linked to increased incidence of PBC and PSC in populations.In both cases, although there is a clear genetic pre-disposition,4,5 there is also evidence that environmental factors – such as exposure to foreign compounds (xenobiotics) – determine the likelihood of developing disease.6–9
Primary biliary cholangitis (PBC) is an autoimmune-associated chronic liver disease triggered by environmental factors, such as exposure to xenobiotics, which leads to a loss of tolerance to the lipoic acid-conjugated regions of the mitochondrial pyruvate dehydrogenase complex, typically to the E2 component. We aimed to identify xenobiotics that might be involved in the environmental triggering of PBC. Urban landfill and control soil samples from a region with high PBC incidence were screened for xenobiotic activities using analytical, cell-based xenobiotic receptor activation assays and toxicity screens. A variety of potential xenobiotic classes were ubiquitously present, as identified by their interaction with xenobiotic receptors – aryl hydrocarbon receptor, androgen receptor and peroxisome proliferator activated receptor alpha – in cell-based screens. In contrast, xenoestrogens were present at higher levels in soil extracts from around an urban landfill. Furthermore, two landfill sampling sites contained a chemical(s) that inhibited mitochondrial oxidative phosphorylation and induced the apoptosis of a hepatic progenitor cell. The mitochondrial effect was also demonstrated in human liver cholangiocytes from three separate donors. The chemical was identified as the ionic liquid [3-methyl-1-octyl-1H-imidazol-3-ium]+ (M8OI) and the toxic effects were recapitulated using authentic pure chemical. A carboxylate-containing human hepatocyte metabolite of M8OI, bearing structural similarity to lipoic acid, was also enzymatically incorporated into the E2 component of the pyruvate dehydrogenase complex via the exogenous lipoylation pathway in vitro. These results identify, for the first time, a xenobiotic in the environment that may be related to and/or be a component of an environmental trigger for PBC. Therefore, further study in experimental animal models is warranted, to determine the risk of exposure to these ionic liquids. In the majority of cases, PBC is associated with an immunological loss of tolerance to the lipoic acid-conjugated regions of the mitochondrial pyruvate dehydrogenase complex (PDC), also known as dihydrolipoamide-S-acetyl transferase, leading to the presence of high levels of diagnostic anti-mitochondrial antibodies (AMA) in patient sera.10,11Thus, one potential trigger for PBC may be exposure to chemicals that structurally and chemically mimic lipoic acid and may therefore be capable of being enzymatically incorporated into the E2 component of PDC (PDC-E2) in place of lipoic acid.10,11Specific disease pathology in the liver (since the antigen for AMA is present in most cells of the body) is thought to be due to selective exposure of the neo-antigen (e.g. via selective apoptosis of hepatic intrahepatic duct cells/cholangiocytes).12
Primary biliary cholangitis (PBC) is an autoimmune-associated chronic liver disease triggered by environmental factors, such as exposure to xenobiotics, which leads to a loss of tolerance to the lipoic acid-conjugated regions of the mitochondrial pyruvate dehydrogenase complex, typically to the E2 component. We aimed to identify xenobiotics that might be involved in the environmental triggering of PBC. Urban landfill and control soil samples from a region with high PBC incidence were screened for xenobiotic activities using analytical, cell-based xenobiotic receptor activation assays and toxicity screens. A variety of potential xenobiotic classes were ubiquitously present, as identified by their interaction with xenobiotic receptors – aryl hydrocarbon receptor, androgen receptor and peroxisome proliferator activated receptor alpha – in cell-based screens. In contrast, xenoestrogens were present at higher levels in soil extracts from around an urban landfill. Furthermore, two landfill sampling sites contained a chemical(s) that inhibited mitochondrial oxidative phosphorylation and induced the apoptosis of a hepatic progenitor cell. The mitochondrial effect was also demonstrated in human liver cholangiocytes from three separate donors. The chemical was identified as the ionic liquid [3-methyl-1-octyl-1H-imidazol-3-ium]+ (M8OI) and the toxic effects were recapitulated using authentic pure chemical. A carboxylate-containing human hepatocyte metabolite of M8OI, bearing structural similarity to lipoic acid, was also enzymatically incorporated into the E2 component of the pyruvate dehydrogenase complex via the exogenous lipoylation pathway in vitro. These results identify, for the first time, a xenobiotic in the environment that may be related to and/or be a component of an environmental trigger for PBC. Therefore, further study in experimental animal models is warranted, to determine the risk of exposure to these ionic liquids. A number of signaling pathways have been identified in cells that function to detect xenobiotics and modulate gene expression in order to facilitate their metabolism and excretion.Polycyclic aromatic hydrocarbons (PAHs) are environmental organic xenobiotics consisting of two or more clustered benzene rings, which are present in crude oil and coal tar and contaminate the environment through fossil fuel burning, incineration of waste and other industrial processes.13They are hydrophobic and often persist in the environment and tissues.Many PAHs are suspected or known to be toxic, genotoxic and carcinogenic.14The aryl hydrocarbon receptor (AHR) binds and is transcriptionally activated by many PAHs.15In the liver, this leads to the increased expression of a variety of genes associated with xenobiotic metabolism (e.g. the AHR locus).15However, the function of the AHR likely extends beyond xenobiotic metabolism alone as its endogenous functions appear to include roles in the immune system.16In the thymus, AHR activation can lead to thymocyte apoptosis and thymic atrophy.17
Primary biliary cholangitis (PBC) is an autoimmune-associated chronic liver disease triggered by environmental factors, such as exposure to xenobiotics, which leads to a loss of tolerance to the lipoic acid-conjugated regions of the mitochondrial pyruvate dehydrogenase complex, typically to the E2 component. We aimed to identify xenobiotics that might be involved in the environmental triggering of PBC. Urban landfill and control soil samples from a region with high PBC incidence were screened for xenobiotic activities using analytical, cell-based xenobiotic receptor activation assays and toxicity screens. A variety of potential xenobiotic classes were ubiquitously present, as identified by their interaction with xenobiotic receptors – aryl hydrocarbon receptor, androgen receptor and peroxisome proliferator activated receptor alpha – in cell-based screens. In contrast, xenoestrogens were present at higher levels in soil extracts from around an urban landfill. Furthermore, two landfill sampling sites contained a chemical(s) that inhibited mitochondrial oxidative phosphorylation and induced the apoptosis of a hepatic progenitor cell. The mitochondrial effect was also demonstrated in human liver cholangiocytes from three separate donors. The chemical was identified as the ionic liquid [3-methyl-1-octyl-1H-imidazol-3-ium]+ (M8OI) and the toxic effects were recapitulated using authentic pure chemical. A carboxylate-containing human hepatocyte metabolite of M8OI, bearing structural similarity to lipoic acid, was also enzymatically incorporated into the E2 component of the pyruvate dehydrogenase complex via the exogenous lipoylation pathway in vitro. These results identify, for the first time, a xenobiotic in the environment that may be related to and/or be a component of an environmental trigger for PBC. Therefore, further study in experimental animal models is warranted, to determine the risk of exposure to these ionic liquids. The peroxisome proliferator activated receptor alpha (PPARα) is a nuclear receptor that functions in the regulation of fatty acid oxidation.In addition to its activation by selected endogenous lipids, the receptor is also activated by fibrate drugs (its pharmacological target) and xenobiotics such as polyhalogenated chemicals e.g. perfluorooctane sulfonate.18The nuclear estrogen receptor alpha (ERα) appears to be a frequent target for a variety of natural (e.g. plant phytoestrogens) and xenobiotic man-made chemicals (e.g. pesticides).It has been proposed that xenoestrogens are responsible for a spectrum of adverse effects in wildlife and man that include malformations in the male genital tract; decreased sperm quality; neuroendocrinological, behavioral and metabolic effects and cancer.19–21
Fatty acid translocase CD36 (CD36) is a membrane protein with multiple immuno-metabolic functions. Palmitoylation has been suggested to regulate the distribution and functions of CD36, but little is known about its significance in non-alcoholic steatohepatitis (NASH). Human liver tissue samples were obtained from patients undergoing liver biopsy for diagnostic purposes. CD36 knockout mice were injected with lentiviral vectors expressing wild-type CD36 or CD36 with mutated palmitoylation sites. Liver histology, immunofluorescence, mRNA expression profile, subcellular distributions and functions of CD36 protein were assessed. The localization of CD36 on the plasma membrane of hepatocytes was markedly increased in patients with NASH compared to patients with normal liver and those with simple steatosis. Increased CD36 palmitoylation and increased localization of CD36 on the plasma membrane of hepatocytes were also observed in livers of mice with NASH. Furthermore, inhibition of CD36 palmitoylation protected mice from developing NASH. The absence of palmitoylation decreased CD36 protein hydrophobicity reducing its localization on the plasma membrane as well as in lipid raft of hepatocytes. Consequently, a lack of palmitoylation decreased fatty acid uptake and CD36/Fyn/Lyn complex in HepG2 cells. Inhibition of CD36 palmitoylation not only ameliorated intracellular lipid accumulation via activation of the AMPK pathway, but also inhibited the inflammatory response through the inhibition of the JNK signaling pathway. Our findings demonstrate the key role of palmitoylation in regulating CD36 distributions and its functions in NASH. Inhibition of CD36 palmitoylation may represent an effective therapeutic strategy in patients with NASH. Non-alcoholic fatty liver disease (NAFLD) describes a range of conditions caused by the accumulation of fat in hepatocytes.Fifteen percent to 30% of the general population in both the Western world and Asia suffer from NAFLD.1,2The prevalence is increased in type 2 diabetes mellitus (T2DM) (70%) and morbid obesity (90%).3Non-alcoholic steatohepatitis (NASH) is a subset of NAFLD characterized by excessive fat accumulation in hepatocytes (steatosis) associated with liver tissue inflammation.Unlike steatosis alone (simple steatosis [SS]), which is generally considered benign and reversible, NASH may progress to fibrosis, cirrhosis and hepatocellular carcinoma.4,5Approximately 2–3% of the global population is thought to have NASH, tending to rise rapidly along with the incidence of obesity, T2DM and the metabolic syndrome.6,7The pathogenesis of NASH appears complex and it is still poorly understood, hence there are no specific therapeutic strategies for NASH.
Fatty acid translocase CD36 (CD36) is a membrane protein with multiple immuno-metabolic functions. Palmitoylation has been suggested to regulate the distribution and functions of CD36, but little is known about its significance in non-alcoholic steatohepatitis (NASH). Human liver tissue samples were obtained from patients undergoing liver biopsy for diagnostic purposes. CD36 knockout mice were injected with lentiviral vectors expressing wild-type CD36 or CD36 with mutated palmitoylation sites. Liver histology, immunofluorescence, mRNA expression profile, subcellular distributions and functions of CD36 protein were assessed. The localization of CD36 on the plasma membrane of hepatocytes was markedly increased in patients with NASH compared to patients with normal liver and those with simple steatosis. Increased CD36 palmitoylation and increased localization of CD36 on the plasma membrane of hepatocytes were also observed in livers of mice with NASH. Furthermore, inhibition of CD36 palmitoylation protected mice from developing NASH. The absence of palmitoylation decreased CD36 protein hydrophobicity reducing its localization on the plasma membrane as well as in lipid raft of hepatocytes. Consequently, a lack of palmitoylation decreased fatty acid uptake and CD36/Fyn/Lyn complex in HepG2 cells. Inhibition of CD36 palmitoylation not only ameliorated intracellular lipid accumulation via activation of the AMPK pathway, but also inhibited the inflammatory response through the inhibition of the JNK signaling pathway. Our findings demonstrate the key role of palmitoylation in regulating CD36 distributions and its functions in NASH. Inhibition of CD36 palmitoylation may represent an effective therapeutic strategy in patients with NASH. Fatty acid translocase CD36 (CD36) is a widely expressed membrane glycoprotein, which plays an important role in facilitating the uptake and intracellular trafficking of long-chain fatty acid (LCFA).8,9Growing evidence indicates that the role played by CD36 extends far beyond the transport of fatty acids (FAs).Indeed, CD36 also impacts FA oxidation by influencing the activation of monophosphate-activated protein kinase (AMPK).10More importantly, CD36 is engaged in the regulation of chronic metabolic inflammation.Stewart et al. reported that CD36 ligands (oxidized low-density lipoprotein and amyloid-beta) are able to trigger inflammatory signaling through the assembly of a complex of CD36 and toll-like receptors 4/6.11,12CD36 deletion markedly reduced adipocyte inflammation in mice fed with a high-fat diet (HFD) by inhibiting the c-JUN N-terminal kinase (JNK) pathway.13Accordingly, CD36 has been recognized as a multifunctional immuno-metabolic receptor.14
Fatty acid translocase CD36 (CD36) is a membrane protein with multiple immuno-metabolic functions. Palmitoylation has been suggested to regulate the distribution and functions of CD36, but little is known about its significance in non-alcoholic steatohepatitis (NASH). Human liver tissue samples were obtained from patients undergoing liver biopsy for diagnostic purposes. CD36 knockout mice were injected with lentiviral vectors expressing wild-type CD36 or CD36 with mutated palmitoylation sites. Liver histology, immunofluorescence, mRNA expression profile, subcellular distributions and functions of CD36 protein were assessed. The localization of CD36 on the plasma membrane of hepatocytes was markedly increased in patients with NASH compared to patients with normal liver and those with simple steatosis. Increased CD36 palmitoylation and increased localization of CD36 on the plasma membrane of hepatocytes were also observed in livers of mice with NASH. Furthermore, inhibition of CD36 palmitoylation protected mice from developing NASH. The absence of palmitoylation decreased CD36 protein hydrophobicity reducing its localization on the plasma membrane as well as in lipid raft of hepatocytes. Consequently, a lack of palmitoylation decreased fatty acid uptake and CD36/Fyn/Lyn complex in HepG2 cells. Inhibition of CD36 palmitoylation not only ameliorated intracellular lipid accumulation via activation of the AMPK pathway, but also inhibited the inflammatory response through the inhibition of the JNK signaling pathway. Our findings demonstrate the key role of palmitoylation in regulating CD36 distributions and its functions in NASH. Inhibition of CD36 palmitoylation may represent an effective therapeutic strategy in patients with NASH. Hepatic CD36 expression is normally weak, but its expression is significantly increased in animal models and patients with steatosis.15,16Hepatic overexpression (OE) of CD36 results in steatosis in mice even in the absence of a HFD,17 and liver-specific knockout (KO) of CD36 reduces liver lipid content when mice were fed with HFD.18However, little is known about the role of CD36 in NASH.
Fatty acid translocase CD36 (CD36) is a membrane protein with multiple immuno-metabolic functions. Palmitoylation has been suggested to regulate the distribution and functions of CD36, but little is known about its significance in non-alcoholic steatohepatitis (NASH). Human liver tissue samples were obtained from patients undergoing liver biopsy for diagnostic purposes. CD36 knockout mice were injected with lentiviral vectors expressing wild-type CD36 or CD36 with mutated palmitoylation sites. Liver histology, immunofluorescence, mRNA expression profile, subcellular distributions and functions of CD36 protein were assessed. The localization of CD36 on the plasma membrane of hepatocytes was markedly increased in patients with NASH compared to patients with normal liver and those with simple steatosis. Increased CD36 palmitoylation and increased localization of CD36 on the plasma membrane of hepatocytes were also observed in livers of mice with NASH. Furthermore, inhibition of CD36 palmitoylation protected mice from developing NASH. The absence of palmitoylation decreased CD36 protein hydrophobicity reducing its localization on the plasma membrane as well as in lipid raft of hepatocytes. Consequently, a lack of palmitoylation decreased fatty acid uptake and CD36/Fyn/Lyn complex in HepG2 cells. Inhibition of CD36 palmitoylation not only ameliorated intracellular lipid accumulation via activation of the AMPK pathway, but also inhibited the inflammatory response through the inhibition of the JNK signaling pathway. Our findings demonstrate the key role of palmitoylation in regulating CD36 distributions and its functions in NASH. Inhibition of CD36 palmitoylation may represent an effective therapeutic strategy in patients with NASH. It is well-established that the functions of CD36 are largely dependent on its localization on the plasma membrane.19Insulin and the forkhead transcription factor FoxO1 induce CD36 translocation from intracellular deposits to the plasma membrane, facilitating FA uptake.20However, the mechanisms regulating CD36 localization on the plasma membrane remain poorly understood.
Fatty acid translocase CD36 (CD36) is a membrane protein with multiple immuno-metabolic functions. Palmitoylation has been suggested to regulate the distribution and functions of CD36, but little is known about its significance in non-alcoholic steatohepatitis (NASH). Human liver tissue samples were obtained from patients undergoing liver biopsy for diagnostic purposes. CD36 knockout mice were injected with lentiviral vectors expressing wild-type CD36 or CD36 with mutated palmitoylation sites. Liver histology, immunofluorescence, mRNA expression profile, subcellular distributions and functions of CD36 protein were assessed. The localization of CD36 on the plasma membrane of hepatocytes was markedly increased in patients with NASH compared to patients with normal liver and those with simple steatosis. Increased CD36 palmitoylation and increased localization of CD36 on the plasma membrane of hepatocytes were also observed in livers of mice with NASH. Furthermore, inhibition of CD36 palmitoylation protected mice from developing NASH. The absence of palmitoylation decreased CD36 protein hydrophobicity reducing its localization on the plasma membrane as well as in lipid raft of hepatocytes. Consequently, a lack of palmitoylation decreased fatty acid uptake and CD36/Fyn/Lyn complex in HepG2 cells. Inhibition of CD36 palmitoylation not only ameliorated intracellular lipid accumulation via activation of the AMPK pathway, but also inhibited the inflammatory response through the inhibition of the JNK signaling pathway. Our findings demonstrate the key role of palmitoylation in regulating CD36 distributions and its functions in NASH. Inhibition of CD36 palmitoylation may represent an effective therapeutic strategy in patients with NASH. Palmitoylation is the covalent attachment of palmitate to cystein residues of proteins.This post-translational modification increases the lipophilicity of the modified protein, thus regulating its subcellular distribution and function.21It has been confirmed that human CD36 is palmitoylated, cysteines residues (cys) 3, 7, 464, and 466 account for the entire palmitoylation of CD36.22Inhibition of palmitoylation causes ER accumulation of CD36 and decreases its incorporation into plasma membrane rafts, thus reducing the efficiency of uptake of oxidized low-density lipoprotein in melanoma cell lines.23
Endothelial dysfunction plays an essential role in liver injury, yet the phenotypic regulation of liver sinusoidal endothelial cells (LSECs) remains unknown. Autophagy is an endogenous protective system whose loss could undermine LSEC integrity and phenotype. The aim of our study was to investigate the role of autophagy in the regulation of endothelial dysfunction and the impact of its manipulation during liver injury. We analyzed primary isolated LSECs from Atg7control and Atg7endo mice as well as rats after CCl4 induced liver injury. Liver tissue and primary isolated stellate cells were used to analyze liver fibrosis. Autophagy flux, microvascular function, nitric oxide bioavailability, cellular superoxide content and the antioxidant response were evaluated in endothelial cells. Autophagy maintains LSEC homeostasis and is rapidly upregulated during capillarization in vitro and in vivo. Pharmacological and genetic downregulation of endothelial autophagy increases oxidative stress in vitro. During liver injury in vivo, the selective loss of endothelial autophagy leads to cellular dysfunction and reduced intrahepatic nitric oxide. The loss of autophagy also impairs LSECs ability to handle oxidative stress and aggravates fibrosis. Autophagy contributes to maintaining endothelial phenotype and protecting LSECs from oxidative stress during early phases of liver disease. Selectively potentiating autophagy in LSECs during early stages of liver disease may be an attractive approach to modify the disease course and prevent fibrosis progression. Chronic liver injury from any source leads to progressive fibrosis, yet treatments are elusive.A better understanding of the early changes that disrupt cellular homeostasis, and initiate and perpetuate fibrogenesis following liver injury is needed.Liver sinusoidal endothelial cells (LSECs) constitute the liver’s first barrier of defense because of their unique position lining the sinusoidal lumen.They are also the initial liver cell type to sense injury.Maintenance of the LSEC phenotype associated with cellular pores, or fenestrae, is critical to maintaining homeostasis in the whole liver parenchyma.Following hepatic damage, sinusoidal endothelial dysfunction may arise and it is characterized by the loss of both fenestrae (capillarization) and of its anti-fibrotic, anti-thrombotic and anti-vasodilatory properties, which are essential for the maintenance of liver integrity.1LSEC injury also plays an essential role in initiation and progression of liver injury.Indeed, signals derived from the sinusoidal endothelium during liver damage determine the outcome of pro-regenerative vs. pro-fibrotic processes.2,3Despite its primary role in maladaptive healing and liver fibrosis,4,5 the phenotypic regulation of endothelial dysfunction is not fully understood.
Endothelial dysfunction plays an essential role in liver injury, yet the phenotypic regulation of liver sinusoidal endothelial cells (LSECs) remains unknown. Autophagy is an endogenous protective system whose loss could undermine LSEC integrity and phenotype. The aim of our study was to investigate the role of autophagy in the regulation of endothelial dysfunction and the impact of its manipulation during liver injury. We analyzed primary isolated LSECs from Atg7control and Atg7endo mice as well as rats after CCl4 induced liver injury. Liver tissue and primary isolated stellate cells were used to analyze liver fibrosis. Autophagy flux, microvascular function, nitric oxide bioavailability, cellular superoxide content and the antioxidant response were evaluated in endothelial cells. Autophagy maintains LSEC homeostasis and is rapidly upregulated during capillarization in vitro and in vivo. Pharmacological and genetic downregulation of endothelial autophagy increases oxidative stress in vitro. During liver injury in vivo, the selective loss of endothelial autophagy leads to cellular dysfunction and reduced intrahepatic nitric oxide. The loss of autophagy also impairs LSECs ability to handle oxidative stress and aggravates fibrosis. Autophagy contributes to maintaining endothelial phenotype and protecting LSECs from oxidative stress during early phases of liver disease. Selectively potentiating autophagy in LSECs during early stages of liver disease may be an attractive approach to modify the disease course and prevent fibrosis progression. Autophagy is a major intracellular recycling system that maintains cellular homeostasis under basal conditions, and plays an integral role in regulating the cellular adaptive response during stress.6Although autophagy has been implicated in the regulation of other resident liver cells (hepatocytes,7 stellate cells8 and macrophages9,10) and cardiovascular endothelial cell biology and physiopathology,11 its role in regulating liver endothelial phenotype during acute liver injury remains largely unknown.
In liver transplantation, organ shortage leads to the use of marginal grafts that are more susceptible to ischemia–reperfusion (IR) injury. We identified nucleotide-binding oligomerization domain 1 (NOD1) as an important modulator of polymorphonuclear neutrophil (PMN)-induced liver injury, which occurs in IR. Herein, we aimed to elucidate the role of NOD1 in IR injury, particularly focusing on its effects on the endothelium and hepatocytes. Nod1 WT and KO mice were treated with NOD1 agonists and subjected to liver IR. Expression of adhesion molecules was analyzed in total liver, isolated hepatocytes and endothelial cells. Interactions between PMNs and hepatocytes were studied in an ex vivo co-culture model using electron microscopy and lactate dehydrogenase levels. We generated NOD1 antagonist-loaded nanoparticles (np ALINO). NOD1 agonist treatment increased liver injury, PMN tissue infiltration and upregulated ICAM-1 and VCAM-1 expression 20 hours after reperfusion. NOD1 agonist treatment without IR increased expression of adhesion molecules (ICAM-1, VCAM-1) in total liver and more particularly in WT hepatocytes, but not in Nod1 KO hepatocytes. This induction is dependent of p38 and ERK signaling pathways. Compared to untreated hepatocytes, a NOD1 agonist markedly increased hepatocyte lysis in co-culture with PMNs as shown by the increase of lactate dehydrogenase in supernatants. Interaction between hepatocytes and PMNs was confirmed by electron microscopy. In a mouse model of liver IR, treatment with np ALINO significantly reduced the area of necrosis, aminotransferase levels and ICAM-1 expression. NOD1 regulates liver IR injury through induction of adhesion molecules and modulation of hepatocyte-PMN interactions. NOD1 antagonist-loaded nanoparticles reduced liver IR injury and provide a potential approach to prevent IR, especially in the context of liver transplantation. In the field of liver transplantation, organ shortage is a major issue; especially in western countries where the living donor transplantation is still scare.This situation led transplant units to use of liver allografts following donation after cardiac death along with marginal and extended criteria donors.1These marginal grafts are known to be more susceptible to early graft dysfunction and retransplantation, increasing morbidity and mortality2–5 mostly due to liver ischemia–reperfusion (IR) injuries.6,7Liver IR is a biphasic phenomenon in which hypoxia-induced lesions are exacerbated, after oxygen delivery is restored, by shear stress and tissue infiltration of polymorphonuclear neutrophils (PMNs).8This process of liver injury also occurs during hemodynamic instability and hepatic resection.It is a frequent cause of acute liver dysfunction.2–5,9,10Thus, preventing IR and its consequences remains a clinical challenge.
In liver transplantation, organ shortage leads to the use of marginal grafts that are more susceptible to ischemia–reperfusion (IR) injury. We identified nucleotide-binding oligomerization domain 1 (NOD1) as an important modulator of polymorphonuclear neutrophil (PMN)-induced liver injury, which occurs in IR. Herein, we aimed to elucidate the role of NOD1 in IR injury, particularly focusing on its effects on the endothelium and hepatocytes. Nod1 WT and KO mice were treated with NOD1 agonists and subjected to liver IR. Expression of adhesion molecules was analyzed in total liver, isolated hepatocytes and endothelial cells. Interactions between PMNs and hepatocytes were studied in an ex vivo co-culture model using electron microscopy and lactate dehydrogenase levels. We generated NOD1 antagonist-loaded nanoparticles (np ALINO). NOD1 agonist treatment increased liver injury, PMN tissue infiltration and upregulated ICAM-1 and VCAM-1 expression 20 hours after reperfusion. NOD1 agonist treatment without IR increased expression of adhesion molecules (ICAM-1, VCAM-1) in total liver and more particularly in WT hepatocytes, but not in Nod1 KO hepatocytes. This induction is dependent of p38 and ERK signaling pathways. Compared to untreated hepatocytes, a NOD1 agonist markedly increased hepatocyte lysis in co-culture with PMNs as shown by the increase of lactate dehydrogenase in supernatants. Interaction between hepatocytes and PMNs was confirmed by electron microscopy. In a mouse model of liver IR, treatment with np ALINO significantly reduced the area of necrosis, aminotransferase levels and ICAM-1 expression. NOD1 regulates liver IR injury through induction of adhesion molecules and modulation of hepatocyte-PMN interactions. NOD1 antagonist-loaded nanoparticles reduced liver IR injury and provide a potential approach to prevent IR, especially in the context of liver transplantation. The pathophysiology of liver IR involves numerous cells (i.e. PMN, liver sinusoidal endothelial cells (LSECs) and hepatocytes) and is considered to be an experimental model of PMN-mediated hepatitis.3The process of injury occurs during the reperfusion phase with PMN infiltration11.The first step of this event is the interaction between PMNs and the endothelium, allowing PMNs to enter the liver parenchyma.The upregulation of adhesion molecules (i.e. ICAM-1, VCAM-1, E-selectin) on LSECs follows reperfusion, allowing PMN recruitment in the rolling process,12 through the binding of neutrophil adhesion molecules L-Selectin and Mac-1, an heterodimeric integrin (CDb/CD18).13The dual role and interactions between the endothelium and PMNs are accepted to be a cornerstone of the pathophysiology of liver IR.Thus, inhibiting adhesion molecule expression (such as ICAM, VCAM, E-selectin, CD44 and PECAM) or using blocking antibodies have been widely studied to reduce liver injury.14
In liver transplantation, organ shortage leads to the use of marginal grafts that are more susceptible to ischemia–reperfusion (IR) injury. We identified nucleotide-binding oligomerization domain 1 (NOD1) as an important modulator of polymorphonuclear neutrophil (PMN)-induced liver injury, which occurs in IR. Herein, we aimed to elucidate the role of NOD1 in IR injury, particularly focusing on its effects on the endothelium and hepatocytes. Nod1 WT and KO mice were treated with NOD1 agonists and subjected to liver IR. Expression of adhesion molecules was analyzed in total liver, isolated hepatocytes and endothelial cells. Interactions between PMNs and hepatocytes were studied in an ex vivo co-culture model using electron microscopy and lactate dehydrogenase levels. We generated NOD1 antagonist-loaded nanoparticles (np ALINO). NOD1 agonist treatment increased liver injury, PMN tissue infiltration and upregulated ICAM-1 and VCAM-1 expression 20 hours after reperfusion. NOD1 agonist treatment without IR increased expression of adhesion molecules (ICAM-1, VCAM-1) in total liver and more particularly in WT hepatocytes, but not in Nod1 KO hepatocytes. This induction is dependent of p38 and ERK signaling pathways. Compared to untreated hepatocytes, a NOD1 agonist markedly increased hepatocyte lysis in co-culture with PMNs as shown by the increase of lactate dehydrogenase in supernatants. Interaction between hepatocytes and PMNs was confirmed by electron microscopy. In a mouse model of liver IR, treatment with np ALINO significantly reduced the area of necrosis, aminotransferase levels and ICAM-1 expression. NOD1 regulates liver IR injury through induction of adhesion molecules and modulation of hepatocyte-PMN interactions. NOD1 antagonist-loaded nanoparticles reduced liver IR injury and provide a potential approach to prevent IR, especially in the context of liver transplantation. Contrary to the endothelium, the mechanisms driving the interaction between PMNs and hepatocytes during liver IR remain to be clarified.The expression of adhesion molecules by the hepatocyte has been suggested to be important for its interaction with PMNs during IR.15Intercellular adhesion molecule-1 (ICAM-1) has been shown to be expressed on the plasma membrane of hepatocytes under IR conditions.15However, it remains unclear whether the regulation of ICAM expression on the hepatocyte impacts the severity of IR lesions.
In liver transplantation, organ shortage leads to the use of marginal grafts that are more susceptible to ischemia–reperfusion (IR) injury. We identified nucleotide-binding oligomerization domain 1 (NOD1) as an important modulator of polymorphonuclear neutrophil (PMN)-induced liver injury, which occurs in IR. Herein, we aimed to elucidate the role of NOD1 in IR injury, particularly focusing on its effects on the endothelium and hepatocytes. Nod1 WT and KO mice were treated with NOD1 agonists and subjected to liver IR. Expression of adhesion molecules was analyzed in total liver, isolated hepatocytes and endothelial cells. Interactions between PMNs and hepatocytes were studied in an ex vivo co-culture model using electron microscopy and lactate dehydrogenase levels. We generated NOD1 antagonist-loaded nanoparticles (np ALINO). NOD1 agonist treatment increased liver injury, PMN tissue infiltration and upregulated ICAM-1 and VCAM-1 expression 20 hours after reperfusion. NOD1 agonist treatment without IR increased expression of adhesion molecules (ICAM-1, VCAM-1) in total liver and more particularly in WT hepatocytes, but not in Nod1 KO hepatocytes. This induction is dependent of p38 and ERK signaling pathways. Compared to untreated hepatocytes, a NOD1 agonist markedly increased hepatocyte lysis in co-culture with PMNs as shown by the increase of lactate dehydrogenase in supernatants. Interaction between hepatocytes and PMNs was confirmed by electron microscopy. In a mouse model of liver IR, treatment with np ALINO significantly reduced the area of necrosis, aminotransferase levels and ICAM-1 expression. NOD1 regulates liver IR injury through induction of adhesion molecules and modulation of hepatocyte-PMN interactions. NOD1 antagonist-loaded nanoparticles reduced liver IR injury and provide a potential approach to prevent IR, especially in the context of liver transplantation. There is evidence that damage-associated molecular patterns and pathogen-associated molecular patterns activate PMNs16,17 leading to IR-mediated liver injury.We previously showed that activation of the nucleotide-binding oligomerization domain 1 (NOD1), a cytosolic pattern recognition receptor, was responsible for activation of PMN function and migration.18Indeed, by activating mitogen-activated protein (MAP) kinases such as p38, NOD1 leads to PMN infiltration in the liver, facilitating injury in a model of liver IR.At the opposite, Nod1 knockout (KO) mice were protected against liver IR injury.18In addition, NOD1 is significantly expressed in the liver and in hepatocytes,19 thus the NOD1 pathway appears to be a promising target for liver IR injury regulation.
Living-donor liver transplantation (LDLT) can simultaneously cure hepatocellular carcinoma (HCC) and underlying liver cirrhosis, improving long-term results in patients with HCC. ABO-incompatible LDLT could expand the living-donor pool, reduce waiting times for deceased-donor liver transplantation, and improve long-term survival for some patients with HCC. We retrospectively reviewed the medical records of patients undergoing LDLT for HCC from November 2008 to December 2015 at a single institution in Korea. In total, 165 patients underwent ABO-incompatible and 753 patients underwent ABO-compatible LDLT for HCC. ABO-incompatible recipients underwent desensitization to overcome the ABO blood group barrier, including pretransplant plasma exchange and rituximab administration (300–375 mg/m2 /body surface area). We performed 1:1 propensity score matching and included 165 patients in each group. 82.4% of ABO-incompatible and 83.0% of -compatible LDLT groups had HCC within conventional Milan criteria, respectively, and 92.1% and 92.7% of patients in each group had a Child-Pugh score of A or B. ABO-incompatible and -compatible LDLT groups were followed up for 48.0 and 48.7 months, respectively, with both groups showing comparable recurrence-free survival rates (hazard ratio [HR] 1.14; 95% CI 0.68–1.90; p = 0.630) and overall patient-survival outcomes (HR 1.10; 95% CI 0.60–2.00; p = 0.763). These findings suggested that ABO-incompatible liver transplantation is a feasible option for patients with HCC, especially for those with compensated cirrhosis with HCC within conventional Milan criteria. Although several potentially curative treatments for hepatocellular carcinoma (HCC) are known, such as liver resection and local ablation, HCC is difficult to manage, because the tumor mostly develops on a background of cirrhosis, and the limited functional reserves of the liver making their use difficult.1In recent years, liver transplantation (LT) has generally been considered a feasible treatment capable of simultaneously curing HCC and the underlying liver cirrhosis, with living-donor liver transplantation (LDLT) performed on select patients with HCC as a practical alternative to deceased-donor liver transplantation (DDLT).2–4However, many patients with HCC who may benefit from long-term survival through LT still do not have a chance of transplantation due to a lack of organs, especially in Asia where HCC incidence combined with chronic HBV- and HCV-related liver diseases is high.5
Living-donor liver transplantation (LDLT) can simultaneously cure hepatocellular carcinoma (HCC) and underlying liver cirrhosis, improving long-term results in patients with HCC. ABO-incompatible LDLT could expand the living-donor pool, reduce waiting times for deceased-donor liver transplantation, and improve long-term survival for some patients with HCC. We retrospectively reviewed the medical records of patients undergoing LDLT for HCC from November 2008 to December 2015 at a single institution in Korea. In total, 165 patients underwent ABO-incompatible and 753 patients underwent ABO-compatible LDLT for HCC. ABO-incompatible recipients underwent desensitization to overcome the ABO blood group barrier, including pretransplant plasma exchange and rituximab administration (300–375 mg/m2 /body surface area). We performed 1:1 propensity score matching and included 165 patients in each group. 82.4% of ABO-incompatible and 83.0% of -compatible LDLT groups had HCC within conventional Milan criteria, respectively, and 92.1% and 92.7% of patients in each group had a Child-Pugh score of A or B. ABO-incompatible and -compatible LDLT groups were followed up for 48.0 and 48.7 months, respectively, with both groups showing comparable recurrence-free survival rates (hazard ratio [HR] 1.14; 95% CI 0.68–1.90; p = 0.630) and overall patient-survival outcomes (HR 1.10; 95% CI 0.60–2.00; p = 0.763). These findings suggested that ABO-incompatible liver transplantation is a feasible option for patients with HCC, especially for those with compensated cirrhosis with HCC within conventional Milan criteria. We have previously investigated the feasibility and clinical outcomes of ABO-incompatible (ABOi) LDLT, finding that the use of living ABOi liver donors is a very effective and safe method for expanding the donor pool for LDLT without an additional donor risk.6,7In practice and in countries with an extreme scarcity of deceased-donor organs, liver donations from living ABOi donors have been widely accepted as a viable option for patients with end-stage liver disease or HCC and for whom an ABO-compatible (ABOc) donor is unavailable.8,9
Porphyrias result from anomalies of heme biosynthetic enzymes and can lead to cirrhosis and hepatocellular cancer. In mice, these diseases can be modeled by administration of a diet containing 3,5-diethoxycarbonyl-1,4-dihydrocollidine (DDC), which causes accumulation of porphyrin intermediates, resulting in hepatobiliary injury. Wnt/β-catenin signaling has been shown to be a modulatable target in models of biliary injury; thus, we investigated its role in DDC-driven injury. β-Catenin (Ctnnb1) knockout (KO) mice, Wnt co-receptor KO mice, and littermate controls were fed a DDC diet for 2 weeks. β-Catenin was exogenously inhibited in hepatocytes by administering β-catenin dicer-substrate RNA (DsiRNA), conjugated to a lipid nanoparticle, to mice after DDC diet and then weekly for 4 weeks. In all experiments, serum and livers were collected; livers were analyzed by histology, western blotting, and real-time PCR. Porphyrin was measured by fluorescence, quantification of polarized light images, and liquid chromatography-mass spectrometry. DDC-fed mice lacking β-catenin or Wnt signaling had decreased liver injury compared to controls. Exogenous mice that underwent β-catenin suppression by DsiRNA during DDC feeding also showed less injury compared to control mice receiving lipid nanoparticles. Control livers contained extensive porphyrin deposits which were largely absent in mice lacking β-catenin signaling. Notably, we identified a network of key heme biosynthesis enzymes that are suppressed in the absence of β-catenin, preventing accumulation of toxic protoporphyrins. Additionally, mice lacking β-catenin exhibited fewer protein aggregates, improved proteasomal activity, and reduced induction of autophagy, all contributing to protection from injury. β-Catenin inhibition, through its pleiotropic effects on metabolism, cell stress, and autophagy, represents a novel therapeutic approach for patients with porphyria. Porphyrins are precursors of heme, an essential co-factor for hemoproteins like cytochrome-P450, hemoglobin, and peroxidases.Heme biosynthesis starts in mitochondria, where ALA-synthase (ALA-S) catalyzes the combination of glycine and succinyl Co-A to form δ-aminolevulinic acid (ALA).ALA leaves the mitochondria, and is sequentially converted in the cytosol to porphobilinogen, hydroxymethylbilane, uroporphyrin and then to coproporphyin, which re-enters mitochondria.In the penultimate step, protoporphyrin-IX (PP-IX) is generated, which is metallated by ferrochelatase (FC) to form the iron containing heme.1,2
Porphyrias result from anomalies of heme biosynthetic enzymes and can lead to cirrhosis and hepatocellular cancer. In mice, these diseases can be modeled by administration of a diet containing 3,5-diethoxycarbonyl-1,4-dihydrocollidine (DDC), which causes accumulation of porphyrin intermediates, resulting in hepatobiliary injury. Wnt/β-catenin signaling has been shown to be a modulatable target in models of biliary injury; thus, we investigated its role in DDC-driven injury. β-Catenin (Ctnnb1) knockout (KO) mice, Wnt co-receptor KO mice, and littermate controls were fed a DDC diet for 2 weeks. β-Catenin was exogenously inhibited in hepatocytes by administering β-catenin dicer-substrate RNA (DsiRNA), conjugated to a lipid nanoparticle, to mice after DDC diet and then weekly for 4 weeks. In all experiments, serum and livers were collected; livers were analyzed by histology, western blotting, and real-time PCR. Porphyrin was measured by fluorescence, quantification of polarized light images, and liquid chromatography-mass spectrometry. DDC-fed mice lacking β-catenin or Wnt signaling had decreased liver injury compared to controls. Exogenous mice that underwent β-catenin suppression by DsiRNA during DDC feeding also showed less injury compared to control mice receiving lipid nanoparticles. Control livers contained extensive porphyrin deposits which were largely absent in mice lacking β-catenin signaling. Notably, we identified a network of key heme biosynthesis enzymes that are suppressed in the absence of β-catenin, preventing accumulation of toxic protoporphyrins. Additionally, mice lacking β-catenin exhibited fewer protein aggregates, improved proteasomal activity, and reduced induction of autophagy, all contributing to protection from injury. β-Catenin inhibition, through its pleiotropic effects on metabolism, cell stress, and autophagy, represents a novel therapeutic approach for patients with porphyria. 3,5-Diethoxycarbonyl-1,4-dihydrocollidine (DDC) has been widely used to induce hepatic porphyria and Mallory-Denk bodies in mice.3,4DDC perturbs porphyrin metabolism at multiple steps.DDC N-methylates heme residues of some hepatic cytochrome-P450 enzymes, which subsequently demetallates and releases the tetrapyrrole moiety in the form of N-methyl protoporphyrin-IX (NMP).5NMP is a potent inhibitor of FC and blocks conversion of PP-IX to heme.3,6Thus, hepatic NMP accumulation causes build-up of tetrapyrrole precursors, including PP-IX.Additionally, ALA-S is under negative feedback regulation of heme.5Inhibition of FC by NMP, and the subsequent heme-deficient state, de-represses ALA-S expression,7 which in turn generates more ALA that feeds forward into the heme biosynthetic pathway.DDC also induces formation of Mallory-Denk bodies (MDB), which are hepatocellular inclusions found in diseases like alcoholic and non-alcoholic steatohepatitis and metabolic liver diseases.8,9Aggregates of keratin proteins K8 and K18 constitute a major component of MDB as they are early sensors of porphyrin-mediated liver injury.10Protein aggregation hinders clearance, causing proteasomal inhibition and induction of autophagy.11,12Because of crystallized porphyrin biliary plugs, DDC also results in ductular reaction, pericholangitis and periductal fibrosis, similar to that seen in patients with primary sclerosing cholangitis.13
The efficacy of fresh frozen plasma (FFP) transfusion in enhancing thrombin generation in patients with cirrhosis and impaired conventional coagulation tests has not been sufficiently explored. Thus, we aimed to assess the effect of FFP transfusion on thrombin generation in these patients. Fifty-three consecutive patients receiving a standard dose of FFP to treat bleeding and/or before invasive procedures – if international normalized ratio (INR)/prothrombin time (PT) ratio were ≥1.5 – were prospectively enrolled. The primary endpoint was the amelioration of endogenous thrombin potential (ETP) with thrombomodulin (ETP-TM) after transfusion, which corresponds to the total amount of generated thrombin. INR/PT ratio and activated partial thromboplastin time (aPTT) were also assessed before and after transfusion. FFP enhanced ETP-TM by 5.7%, from 973 (731–1,258) to 1,028 (885–1,343 nM × min; p = 0.019). Before transfusion, evidence of normal or high ETP-TM was found in 94% of patients, even in those with bacterial infections. Only 1 (1.9%) patient had ETP-TM values reverting to the normal range after transfusion. Notably, no patients with low ETP-TM had bleeding. The median decrease in ETP-TM was 8.3% and the mean was 12.8% in 18 (34%) patients after transfusion (from 1,225 [1,071–1,537] to 1,124 [812–1,370] nM × min; p ≤0.0001). Similar responses to FFP transfusion were observed in patients with compensated and acute decompensated cirrhosis, acute-on-chronic liver failure, infection or shock. FFP significantly ameliorated INR and aPTT values (p <0.0001), but in a minority of patients the values were reduced to less than the cut-off point of 1.5. FFP transfusion enhanced thrombin generation and ameliorated conventional coagulation tests to normal values in a limited number of patients, and slightly decreased thrombin generation in 34% of cases. Over the last decade there has been considerable progress in understanding the complex mechanisms behind the coagulopathy of cirrhosis.Landmark studies have demonstrated that there is a substantial balance between pro- and anticoagulant factors in cirrhosis.Despite preserving normal thrombin generation, this balance is relatively unstable and prone to tip towards hemorrhage or thrombosis, depending on the prevailing circumstantial risk factors to which patients are exposed.1,2In this process, factor VIII, an endothelial-released coagulation factor (typically increased in cirrhosis) to some extent counteracts the decline of the other procoagulant factors synthesized by the liver.As the levels of the naturally occurring anticoagulants are also decreased in cirrhosis, the anticoagulant effect of protein C, protein S and antithrombin on thrombin generation is attenuated when compared to normal individuals.3Indeed, it has been shown in vitro that plasma of patients with cirrhosis may generate adequate thrombin amounts when exposed to tissue factor and exogenous phospholipids, provided that plasmatic protein C is activated by its main physiological activator, thrombomodulin (TM).3–4
The efficacy of fresh frozen plasma (FFP) transfusion in enhancing thrombin generation in patients with cirrhosis and impaired conventional coagulation tests has not been sufficiently explored. Thus, we aimed to assess the effect of FFP transfusion on thrombin generation in these patients. Fifty-three consecutive patients receiving a standard dose of FFP to treat bleeding and/or before invasive procedures – if international normalized ratio (INR)/prothrombin time (PT) ratio were ≥1.5 – were prospectively enrolled. The primary endpoint was the amelioration of endogenous thrombin potential (ETP) with thrombomodulin (ETP-TM) after transfusion, which corresponds to the total amount of generated thrombin. INR/PT ratio and activated partial thromboplastin time (aPTT) were also assessed before and after transfusion. FFP enhanced ETP-TM by 5.7%, from 973 (731–1,258) to 1,028 (885–1,343 nM × min; p = 0.019). Before transfusion, evidence of normal or high ETP-TM was found in 94% of patients, even in those with bacterial infections. Only 1 (1.9%) patient had ETP-TM values reverting to the normal range after transfusion. Notably, no patients with low ETP-TM had bleeding. The median decrease in ETP-TM was 8.3% and the mean was 12.8% in 18 (34%) patients after transfusion (from 1,225 [1,071–1,537] to 1,124 [812–1,370] nM × min; p ≤0.0001). Similar responses to FFP transfusion were observed in patients with compensated and acute decompensated cirrhosis, acute-on-chronic liver failure, infection or shock. FFP significantly ameliorated INR and aPTT values (p <0.0001), but in a minority of patients the values were reduced to less than the cut-off point of 1.5. FFP transfusion enhanced thrombin generation and ameliorated conventional coagulation tests to normal values in a limited number of patients, and slightly decreased thrombin generation in 34% of cases. Despite abnormal results of conventional coagulation tests, patients with cirrhosis are not ‘auto-anticoagulated’, as was previously believed.Conversely, some are at risk of venous thromboembolism when compared to the general population of non-cirrhotic individuals, because the balance may shift toward the production of higher than normal amounts of thrombin and hypercoagulability.4–6Conventional coagulation tests such as the prothrombin time (PT) and activated partial thromboplastin time (aPTT) are based on clot formation as an endpoint, which occurs when approximately 5% of thrombin is generated.7Therefore these tests are suitable for assessing the procoagulant arm of coagulation, but fail to fully assess its anticoagulant counterpart.4Furthermore, TM is not added to these tests to mimic the endothelial activation of the protein C pathway.Therefore, PT and aPTT are not reliable indicators of in vivo thrombin generation, so they are not suitable for investigating coagulopathy in cirrhosis and guiding transfusion policy.
The efficacy of fresh frozen plasma (FFP) transfusion in enhancing thrombin generation in patients with cirrhosis and impaired conventional coagulation tests has not been sufficiently explored. Thus, we aimed to assess the effect of FFP transfusion on thrombin generation in these patients. Fifty-three consecutive patients receiving a standard dose of FFP to treat bleeding and/or before invasive procedures – if international normalized ratio (INR)/prothrombin time (PT) ratio were ≥1.5 – were prospectively enrolled. The primary endpoint was the amelioration of endogenous thrombin potential (ETP) with thrombomodulin (ETP-TM) after transfusion, which corresponds to the total amount of generated thrombin. INR/PT ratio and activated partial thromboplastin time (aPTT) were also assessed before and after transfusion. FFP enhanced ETP-TM by 5.7%, from 973 (731–1,258) to 1,028 (885–1,343 nM × min; p = 0.019). Before transfusion, evidence of normal or high ETP-TM was found in 94% of patients, even in those with bacterial infections. Only 1 (1.9%) patient had ETP-TM values reverting to the normal range after transfusion. Notably, no patients with low ETP-TM had bleeding. The median decrease in ETP-TM was 8.3% and the mean was 12.8% in 18 (34%) patients after transfusion (from 1,225 [1,071–1,537] to 1,124 [812–1,370] nM × min; p ≤0.0001). Similar responses to FFP transfusion were observed in patients with compensated and acute decompensated cirrhosis, acute-on-chronic liver failure, infection or shock. FFP significantly ameliorated INR and aPTT values (p <0.0001), but in a minority of patients the values were reduced to less than the cut-off point of 1.5. FFP transfusion enhanced thrombin generation and ameliorated conventional coagulation tests to normal values in a limited number of patients, and slightly decreased thrombin generation in 34% of cases. However, it is still common practice to attempt to correct the deficiency of coagulation factors, suggested by PT/international normalized ratio (INR) and aPTT, with fresh frozen plasma (FFP) transfusions before invasive procedures or to control ongoing bleeding events.This practice is illustrated by a nationwide British study that collected data from 85 hospitals over a 28-day period, which showed that 30% of the patients with cirrhosis were transfused with at least one blood component during their admission to the hospital, and that FFP was prescribed in nearly 30% of cases.8Similar figures were observed in a survey in the United States, showing that patients with cirrhosis consumed a disproportionate 32.4% of the units of FFP in a tertiary hospital, despite accounting for less than 8% of total hospital admissions.9
The efficacy of fresh frozen plasma (FFP) transfusion in enhancing thrombin generation in patients with cirrhosis and impaired conventional coagulation tests has not been sufficiently explored. Thus, we aimed to assess the effect of FFP transfusion on thrombin generation in these patients. Fifty-three consecutive patients receiving a standard dose of FFP to treat bleeding and/or before invasive procedures – if international normalized ratio (INR)/prothrombin time (PT) ratio were ≥1.5 – were prospectively enrolled. The primary endpoint was the amelioration of endogenous thrombin potential (ETP) with thrombomodulin (ETP-TM) after transfusion, which corresponds to the total amount of generated thrombin. INR/PT ratio and activated partial thromboplastin time (aPTT) were also assessed before and after transfusion. FFP enhanced ETP-TM by 5.7%, from 973 (731–1,258) to 1,028 (885–1,343 nM × min; p = 0.019). Before transfusion, evidence of normal or high ETP-TM was found in 94% of patients, even in those with bacterial infections. Only 1 (1.9%) patient had ETP-TM values reverting to the normal range after transfusion. Notably, no patients with low ETP-TM had bleeding. The median decrease in ETP-TM was 8.3% and the mean was 12.8% in 18 (34%) patients after transfusion (from 1,225 [1,071–1,537] to 1,124 [812–1,370] nM × min; p ≤0.0001). Similar responses to FFP transfusion were observed in patients with compensated and acute decompensated cirrhosis, acute-on-chronic liver failure, infection or shock. FFP significantly ameliorated INR and aPTT values (p <0.0001), but in a minority of patients the values were reduced to less than the cut-off point of 1.5. FFP transfusion enhanced thrombin generation and ameliorated conventional coagulation tests to normal values in a limited number of patients, and slightly decreased thrombin generation in 34% of cases. FFP has been used for many years on the assumption that patients with cirrhosis are at high risk of bleeding because they have frequent episodes of digestive hemorrhage and prolonged PT and aPTT.However, scarce data are available on the effect of FFP transfusions on thrombin generation to support biological plausibility and indiscriminate use.There are few published studies, and most provide insufficient evidence to make strong recommendations in this regard.Data from an in vitro study simulating the effect of transfusion by mixing samples of pooled normal plasma with plasma from patients with cirrhosis suggested that thrombin generation does not appreciably change after the addition of normal plasma, despite PT and aPTT shortening suggesting otherwise.10Although the prohemostatic effect was small, a different in vitro study showed a difference in response to FFP transfusion in patients with acute-on-chronic liver failure (ACLF) compared to patients with acute decompensation of cirrhosis, compensated cirrhosis and healthy controls.11This gives grounds to hypothesize that a very limited number of patients would benefit from FFP transfusion in real life settings, because thrombin generation is already expected to be preserved before transfusion in most patients with cirrhosis.
Chronic infection with hepatitis B virus (HBV) in children is a serious health problem worldwide. How to treat children with immune-tolerant chronic hepatitis B infection, commonly characterized by hepatitis B e antigen (HBeAg) positivity, high viral load, normal or mildly elevated alanine aminotransferase and no or minimal inflammation in liver histology, remains unresolved. This trial aims to study the benefits of antiviral therapy in children with these characteristics. This is a pilot open-label randomized controlled study. From May 2014 to April 2015, 69 treatment-naive chronically HBV-infected children, aged 1 to 16 years, who had immune-tolerant characteristics were recruited to this trial and randomly assigned, in a 2:1 ratio, to treatment group and control group. Patients in the treatment group received either interferon-α (IFN) monotherapy or consecutively received IFN monotherapy, combination therapy of IFN and lamivudine (LAM), and LAM therapy alone. All patients were observed until week 96. At baseline, epidemiological, biochemical, serological, virological and histological indices were consistent across the treatment and control groups. Of the 46 patients in the treatment group, 73.91% had undetectable serum HBV DNA, 32.61% achieved HBeAg seroconversion and 21.74% lost hepatitis B surface antigen (HBsAg) at the endpoint. No LAM resistance emerged at week 96. In the control group, only one (4.35%) patient underwent spontaneous HBeAg seroconversion and had undetectable serum HBV DNA during observation, and moreover, none developed HBsAg clearance. For all patients, no serious adverse events were observed. Antiviral treatment with a sequential combination of IFN and LAM resulted in a significant improvement in the rates of undetectable serum HBV DNA, HBeAg seroconversion and HBsAg loss in children with chronic HBV infection and immune-tolerant characteristics. Despite extensive vaccination programs, chronic infection with hepatitis B virus (HBV) in children remains a serious health problem worldwide.1–5Even though a benign course of chronic HBV infection during childhood has been described, 3–5% and 0.01–0.03% of chronic carriers develop cirrhosis or hepatocellular carcinoma (HCC) before adulthood.6Considering the whole lifetime, the risk of HCC rises to 9–24% and the incidence of cirrhosis to 2–3% per year.7In children, chronic hepatitis B (CHB) is characterized by its natural course, including immune-tolerant phase, immune-active state and inactive carrier.Ideally, a child with CHB should be treated early to prevent the development of cirrhosis and HCC.Nevertheless, to date, no therapeutic interventions have been recommended by the guidelines for the pediatric immune-tolerant CHB, because of the scarcity of available data.6,8,9Low therapeutic coverage exacerbated by restrictive treatment guidelines may facilitate disease progression in many patients.Thus, earlier treatment than recommended by current guidelines should be considered.10
All known hepatitis B virus (HBV) genotypes occur in humans and hominoid Old World non-human primates (NHPs). The divergent woolly monkey HBV (WMHBV) forms another orthohepadnavirus species. The evolutionary origins of HBV are unclear. We analysed sera from 124 Brazilian monkeys collected during 2012–2016 for hepadnaviruses using molecular and serological tools, and conducted evolutionary analyses. We identified a novel orthohepadnavirus species in capuchin monkeys (capuchin monkey hepatitis B virus [CMHBV]). We found CMHBV-specific antibodies in five animals and high CMHBV concentrations in one animal. Non-inflammatory, probably chronic infection was consistent with an intact preCore domain, low genetic variability, core deletions in deep sequencing, and no elevated liver enzymes. Cross-reactivity of antisera against surface antigens suggested antigenic relatedness of HBV, CMHBV, and WMHBV. Infection-determining CMHBV surface peptides bound to the human HBV receptor (human sodium taurocholate co-transporting polypeptide), but preferentially interacted with the capuchin monkey receptor homologue. CMHBV and WMHBV pseudotypes infected human hepatoma cells via the human sodium taurocholate co-transporting polypeptide, and were poorly neutralised by HBV vaccine-derived antibodies, suggesting that cross-species infections may be possible. Ancestral state reconstructions and sequence distance comparisons associated HBV with humans, whereas primate hepadnaviruses as a whole were projected to NHP ancestors. Co-phylogenetic analyses yielded evidence for co-speciation of hepadnaviruses and New World NHP. Bayesian hypothesis testing yielded strong support for an association of the HBV stem lineage with hominoid ancestors. Neither CMHBV nor WMHBV was likely the ancestor of the divergent human HBV genotypes F/H found in American natives. Our data suggest ancestral co-speciation of hepadnaviruses and NHP, and an Old World origin of the divergent HBV genotypes F/H. The identification of a novel primate hepadnavirus offers new perspectives for urgently needed animal models of chronic hepatitis B. The hepatitis B virus (HBV) is one of the most important human pathogens, causing at least 680,000 deaths each year globally caused by chronic infection resulting in liver cirrhosis and hepatocellular carcinoma.1HBV is the prototype species of the genus Orthohepadnavirus in the family Hepadnaviridae.In humans, HBV comprises 10 genotypes named A–J.2 Additional HBV genotypes infect Old World non-human primates (NHPs), including chimpanzees, gorillas, orangutans, and gibbons.3Infection of humans with HBV genotypes from NHPs has not been described yet.By contrast, NHPs can carry human HBV genotypes and HBV genotypes from other NHP species, illustrating the potential of primate HBV to cross the species barrier.3Different from other major blood-borne viruses, such as HIV, there is no evidence for an evolutionary origin of human HBV from viruses carried by Old World NHPs.4Similarly, the evolutionary origins of the divergent human HBV genotypes F and H associated with American natives inhabiting Alaska and Latin America are unknown.5
All known hepatitis B virus (HBV) genotypes occur in humans and hominoid Old World non-human primates (NHPs). The divergent woolly monkey HBV (WMHBV) forms another orthohepadnavirus species. The evolutionary origins of HBV are unclear. We analysed sera from 124 Brazilian monkeys collected during 2012–2016 for hepadnaviruses using molecular and serological tools, and conducted evolutionary analyses. We identified a novel orthohepadnavirus species in capuchin monkeys (capuchin monkey hepatitis B virus [CMHBV]). We found CMHBV-specific antibodies in five animals and high CMHBV concentrations in one animal. Non-inflammatory, probably chronic infection was consistent with an intact preCore domain, low genetic variability, core deletions in deep sequencing, and no elevated liver enzymes. Cross-reactivity of antisera against surface antigens suggested antigenic relatedness of HBV, CMHBV, and WMHBV. Infection-determining CMHBV surface peptides bound to the human HBV receptor (human sodium taurocholate co-transporting polypeptide), but preferentially interacted with the capuchin monkey receptor homologue. CMHBV and WMHBV pseudotypes infected human hepatoma cells via the human sodium taurocholate co-transporting polypeptide, and were poorly neutralised by HBV vaccine-derived antibodies, suggesting that cross-species infections may be possible. Ancestral state reconstructions and sequence distance comparisons associated HBV with humans, whereas primate hepadnaviruses as a whole were projected to NHP ancestors. Co-phylogenetic analyses yielded evidence for co-speciation of hepadnaviruses and New World NHP. Bayesian hypothesis testing yielded strong support for an association of the HBV stem lineage with hominoid ancestors. Neither CMHBV nor WMHBV was likely the ancestor of the divergent human HBV genotypes F/H found in American natives. Our data suggest ancestral co-speciation of hepadnaviruses and NHP, and an Old World origin of the divergent HBV genotypes F/H. The identification of a novel primate hepadnavirus offers new perspectives for urgently needed animal models of chronic hepatitis B. In the absence of known animal reservoirs for human HBV strains, the eradication of hepatitis B through universal vaccination and antiviral treatment might be possible.6We recently described a New World bat hepadnavirus that could infect human hepatocytes and was not neutralised by hepatitis B vaccine-induced antibodies.7Whether this bat virus may infect humans remains open, because direct contact between humans and bats (e.g. from hunting of bats as bushmeat) is rare in the New World.8By contrast, contact between indigenous American populations and NHP is more intense, including consumption of NHP, their keeping as pets, and the encroachment of NHP to human dwellings because of destruction of their natural habitats.9Additionally, the genetic relatedness of humans and NHP facilitates cross-species infections.10
All known hepatitis B virus (HBV) genotypes occur in humans and hominoid Old World non-human primates (NHPs). The divergent woolly monkey HBV (WMHBV) forms another orthohepadnavirus species. The evolutionary origins of HBV are unclear. We analysed sera from 124 Brazilian monkeys collected during 2012–2016 for hepadnaviruses using molecular and serological tools, and conducted evolutionary analyses. We identified a novel orthohepadnavirus species in capuchin monkeys (capuchin monkey hepatitis B virus [CMHBV]). We found CMHBV-specific antibodies in five animals and high CMHBV concentrations in one animal. Non-inflammatory, probably chronic infection was consistent with an intact preCore domain, low genetic variability, core deletions in deep sequencing, and no elevated liver enzymes. Cross-reactivity of antisera against surface antigens suggested antigenic relatedness of HBV, CMHBV, and WMHBV. Infection-determining CMHBV surface peptides bound to the human HBV receptor (human sodium taurocholate co-transporting polypeptide), but preferentially interacted with the capuchin monkey receptor homologue. CMHBV and WMHBV pseudotypes infected human hepatoma cells via the human sodium taurocholate co-transporting polypeptide, and were poorly neutralised by HBV vaccine-derived antibodies, suggesting that cross-species infections may be possible. Ancestral state reconstructions and sequence distance comparisons associated HBV with humans, whereas primate hepadnaviruses as a whole were projected to NHP ancestors. Co-phylogenetic analyses yielded evidence for co-speciation of hepadnaviruses and New World NHP. Bayesian hypothesis testing yielded strong support for an association of the HBV stem lineage with hominoid ancestors. Neither CMHBV nor WMHBV was likely the ancestor of the divergent human HBV genotypes F/H found in American natives. Our data suggest ancestral co-speciation of hepadnaviruses and NHP, and an Old World origin of the divergent HBV genotypes F/H. The identification of a novel primate hepadnavirus offers new perspectives for urgently needed animal models of chronic hepatitis B. The only other known primate hepadnavirus species beyond HBV, termed woolly monkey HBV (WMHBV), was described in 1998 from captive woolly monkeys (Lagothrix lagotricha).11Although the WMHBV forms a phylogenetic sister species to HBV, its description in a confined setting challenged definitive assertions on the role of New World NHPs for the evolutionary origins of HBV.Sampling of NHPs is difficult for ethical and technical reasons, and in consequence, South American NHPs are among the most understudied primate populations in terms of the infectious agents they may carry.12There are only four studies on HBV in New World NHPs that collectively analysed only about 100 animals.3
Sorafenib is associated with multiple adverse events (AEs), potentially causing its permanent interruption. It is unknown how physicians’ experience has impacted on the management of these AEs and consequently on clinical outcomes. We aimed to assess whether AE management changed over time and if these modifications impacted on treatment duration and overall survival (OS). We analysed the prospectively collected data of 338 consecutive patients who started sorafenib between January 2008 and December 2017 in 3 tertiary care centres in Italy. Patients were divided according to the starting date: Group A (2008–2012; n = 154), and Group B (2013–2017, n = 184). Baseline and follow-up data were compared. In the OS analysis, patients who received second-line treatments were censored when starting the new therapy. Baseline characteristics, AEs, and radiological response were consistent across groups. Patients in Group B received a lower median daily dose (425 vs. 568 mg/day, p <0.001) due to more frequent dose modifications. However, treatment duration was longer (5.8 vs. 4.1 months, p = 0.021) with a trend toward a higher cumulative dose in Group B. Notably, the OS was also higher (12.0 vs. 11.0 months, p = 0.003) with a sharp increase in the 2-year survival rate (28.1 vs. 18.4%, p = 0.003) in Group B. Multivariate time-dependent Cox regression analysis confirmed later period of treatment (2013–2017) as an independent predictor of survival (HR 0.728; 95% CI 0.581–0.937; p = 0.013). Unconsidered confounders were unlikely to affect these results at the sensitivity analysis. Experience in the management of sorafenib-related AEs prolongs treatment duration and survival. This factor should be considered in the design of future randomised clinical trials including a sorafenib treatment arm, as an underestimate of sample size may derive. Sorafenib is a multitarget tyrosine kinase inhibitor (TKI) currently used for the treatment of hepatocellular carcinoma (HCC) not amenable to surgery or locoregional treatments.1Sorafenib significantly prolongs patients’ overall survival (OS), but its use is associated with different adverse events (AEs), mainly dermatological, gastrointestinal and cardiovascular.2,3The management of these AEs can require dose reductions and temporary interruptions.In a sizeable proportion of patients, however, these modifications are not able to avoid intolerable or severe AEs, resulting in permanent drug discontinuation.1It might seem common sense that the experience accumulated in the prescription of sorafenib could lead to improved management of its related AEs.However, the exact impact of the operators' experience on the prescribing patterns of sorafenib has rarely been investigated.4Similarly, it is not known whether this phenomenon can lead to an increase in OS.The latter point is of crucial importance, as dermatological AEs have been demonstrated to have a favourable prognostic impact.5,6Thus, avoiding a definitive suspension of sorafenib in these patients would be extremely beneficial.Moreover, data from observational studies7,8 and randomised clinical trials (RCTs)9 indirectly seem to suggest that the OS of patients treated with sorafenib is progressively increasing.However, the reasons for this phenomenon have not been fully elucidated.A very recent monocentric study10 suggested that the management of sorafenib AEs has improved over time, but rigorous multicentric studies that consider time-dependent variables, addressing the possible confounding factor of second-line treatments, and providing a confirmation of the survival benefit through multivariable regressions models are still lacking.
Acetaminophen (APAP)-induced acute liver failure is associated with substantial alterations in the hemostatic system. In mice, platelets accumulate in the liver after APAP overdose and appear to promote liver injury. Interestingly, patients with acute liver injury have highly elevated levels of the platelet-adhesive protein von Willebrand factor (VWF), but a mechanistic connection between VWF and progression of liver injury has not been established. We tested the hypothesis that VWF contributes directly to experimental APAP-induced acute liver injury. Wild-type mice and VWF-deficient (Vwf−/−) mice were given a hepatotoxic dose of APAP (300 mg/kg, i.p.) or vehicle (saline). VWF plasma levels were measured by ELISA, and liver necrosis or hepatocyte proliferation was measured by immunohistochemistry. Platelet and VWF deposition were measured by immunofluorescence. In wild-type mice, VWF plasma levels, high molecular weight (HMW) VWF multimers, and VWF activity decreased 24 h after APAP challenge. These changes coupled to robust hepatic VWF and platelet deposition, although VWF deficiency had minimal effect on peak hepatic platelet accumulation or liver injury. VWF plasma levels were elevated 48 h after APAP challenge, but with relative reductions in HMW multimers and VWF activity. Whereas hepatic platelet aggregates persisted in livers of APAP-challenged wild-type mice, platelets were nearly absent in Vwf−/− mice 48 h after APAP challenge. The absence of platelet aggregates was linked to dramatically accelerated repair of the injured liver. Complementing observations in Vwf−/− mice, blocking VWF or the platelet integrin αIIbβ3 during development of injury significantly reduced hepatic platelet aggregation and accelerated liver repair in APAP-challenged wild-type mice. These studies are the first to suggest a mechanistic link between VWF, hepatic platelet accumulation, and liver repair. Targeting VWF might provide a novel therapeutic approach to improve repair of the APAP-injured liver. Acetaminophen (paracetamol, APAP) overdose is a leading cause of drug-induced acute liver injury and acute liver failure (ALF) in the Western world.1Accumulating evidence from experimental and clinical studies suggests that the hemostatic system contributes to the progression of acute liver injury after APAP overdose.2–4Although not associated with clinically significant bleeding,5 substantial alterations in the hemostatic system are evident in patients with APAP-induced liver failure, including a reduced platelet count.6,7The ALF study group has demonstrated in a large cohort of ALF patients that a much more profound thrombocytopenia is associated with poor outcome (i.e. death or the need for a liver transplant).8Platelets have been proposed to drive disease progression through the formation of microthrombi within the liver microvasculature.9APAP-induced liver damage in mice causes persistent thrombocytopenia, which is coupled to accumulation of platelets in the injured liver.3Notably, platelets have been shown to promote APAP-induced liver injury in mice.3However, the mechanisms driving platelet accumulation in the APAP-injured liver remain to be elucidated.
Acetaminophen (APAP)-induced acute liver failure is associated with substantial alterations in the hemostatic system. In mice, platelets accumulate in the liver after APAP overdose and appear to promote liver injury. Interestingly, patients with acute liver injury have highly elevated levels of the platelet-adhesive protein von Willebrand factor (VWF), but a mechanistic connection between VWF and progression of liver injury has not been established. We tested the hypothesis that VWF contributes directly to experimental APAP-induced acute liver injury. Wild-type mice and VWF-deficient (Vwf−/−) mice were given a hepatotoxic dose of APAP (300 mg/kg, i.p.) or vehicle (saline). VWF plasma levels were measured by ELISA, and liver necrosis or hepatocyte proliferation was measured by immunohistochemistry. Platelet and VWF deposition were measured by immunofluorescence. In wild-type mice, VWF plasma levels, high molecular weight (HMW) VWF multimers, and VWF activity decreased 24 h after APAP challenge. These changes coupled to robust hepatic VWF and platelet deposition, although VWF deficiency had minimal effect on peak hepatic platelet accumulation or liver injury. VWF plasma levels were elevated 48 h after APAP challenge, but with relative reductions in HMW multimers and VWF activity. Whereas hepatic platelet aggregates persisted in livers of APAP-challenged wild-type mice, platelets were nearly absent in Vwf−/− mice 48 h after APAP challenge. The absence of platelet aggregates was linked to dramatically accelerated repair of the injured liver. Complementing observations in Vwf−/− mice, blocking VWF or the platelet integrin αIIbβ3 during development of injury significantly reduced hepatic platelet aggregation and accelerated liver repair in APAP-challenged wild-type mice. These studies are the first to suggest a mechanistic link between VWF, hepatic platelet accumulation, and liver repair. Targeting VWF might provide a novel therapeutic approach to improve repair of the APAP-injured liver. The platelet-adhesive glycoprotein von Willebrand factor (VWF) is a key component of the hemostatic system.VWF synthesis is restricted to endothelial cells and megakaryocytes.10Following vascular damage, VWF binds to subendothelial collagens, which activate VWF to serve as an adhesion molecule for platelets, thereby initiating platelet plug formation.11VWF is a multimeric protein and its reactivity towards platelets depends on the size of its multimers.The high molecular weight (HMW) multimers are the most effective in supporting platelet adhesion.10,12Multimeric size of VWF is regulated by ADAMTS13 (a disintegrin and metalloproteinase with a thrombospondin type 1 motif, member 13), which proteolytically cleaves the large multimers into smaller, less active multimers.12VWF size regulation is important for normal hemostatic function, as demonstrated by patients with a congenital or acquired ADAMTS13 deficiency, who suffer from severe thrombotic episodes.13
NGM282, an engineered analogue of the gut hormone FGF19, improves hepatic steatosis and fibrosis biomarkers in patients with non-alcoholic steatohepatitis (NASH). However, NGM282 increases serum cholesterol levels by inhibiting CYP7A1, which encodes the rate-limiting enzyme in the conversion of cholesterol to bile acids. Herein, we investigate whether administration of a statin can manage the cholesterol increase seen in patients with NASH receiving treatment with NGM282. In this phase II, open-label, multicenter study, patients with biopsy-confirmed NASH were treated with subcutaneous NGM282 once daily for 12 weeks. After 2 weeks, rosuvastatin was added in stepwise, biweekly incremental doses to a maximum of 40 mg daily. Both drugs were continued until the end of treatment at week 12. We evaluated plasma lipids, lipoprotein particles and liver fat content. In 66 patients who received NGM282 0.3 mg (n = 23), NGM282 1 mg (n = 21), or NGM282 3 mg (n = 22), circulating cholesterol increased from baseline at week 2. Initiation of rosuvastatin resulted in rapid decline in plasma levels of total cholesterol and low-density lipoprotein cholesterol. At week 12, reductions from baseline in total cholesterol levels of up to 18% (p <0.001), low-density lipoprotein cholesterol of up to 28% (p <0.001), triglycerides of up to 34% (p <0.001) and an increase in high-density lipoprotein cholesterol of up to 16% (p <0.001), with similar changes in lipoprotein particles, were observed in these patients. Robust decreases from baseline in 7alpha-hydroxy-4-cholesten-3-one (p <0.001) and liver fat content (p <0.001) were also observed. Rosuvastatin was safe and well-tolerated when co-administered with NGM282 in patients with NASH. In this multicenter study, NGM282-associated elevation of cholesterol was effectively managed with rosuvastatin. Co-administration of rosuvastatin with NGM282 may be a reasonable strategy to optimize the cardiovascular risk profile in patients with NASH. Non-alcoholic steatohepatitis (NASH), a severe form of non-alcoholic fatty liver disease (NAFLD), represents a large and growing public health concern that is increasingly contributing to the rising prevalence of cirrhosis and hepatocellular carcinoma globally.1,2Currently, there is no approved drug for NASH, which is projected to be the leading indication for liver transplantation in the next decade.3The pathogenesis of NASH is complex, and it is hypothesized that toxic lipid species or intermediates may inflict hepatocyte injury.4In recent years, bile acids have emerged as important molecules that act at both hepatic and extrahepatic tissues to modulate metabolic, inflammation and fibrogenesis pathways.5While crucial for the emulsification and absorption of dietary fat, bile acids can cause cell death and liver injury when amassed within hepatocytes.Indeed, patients with NASH have elevated hepatic and circulating concentrations of bile acids.6,7
NGM282, an engineered analogue of the gut hormone FGF19, improves hepatic steatosis and fibrosis biomarkers in patients with non-alcoholic steatohepatitis (NASH). However, NGM282 increases serum cholesterol levels by inhibiting CYP7A1, which encodes the rate-limiting enzyme in the conversion of cholesterol to bile acids. Herein, we investigate whether administration of a statin can manage the cholesterol increase seen in patients with NASH receiving treatment with NGM282. In this phase II, open-label, multicenter study, patients with biopsy-confirmed NASH were treated with subcutaneous NGM282 once daily for 12 weeks. After 2 weeks, rosuvastatin was added in stepwise, biweekly incremental doses to a maximum of 40 mg daily. Both drugs were continued until the end of treatment at week 12. We evaluated plasma lipids, lipoprotein particles and liver fat content. In 66 patients who received NGM282 0.3 mg (n = 23), NGM282 1 mg (n = 21), or NGM282 3 mg (n = 22), circulating cholesterol increased from baseline at week 2. Initiation of rosuvastatin resulted in rapid decline in plasma levels of total cholesterol and low-density lipoprotein cholesterol. At week 12, reductions from baseline in total cholesterol levels of up to 18% (p <0.001), low-density lipoprotein cholesterol of up to 28% (p <0.001), triglycerides of up to 34% (p <0.001) and an increase in high-density lipoprotein cholesterol of up to 16% (p <0.001), with similar changes in lipoprotein particles, were observed in these patients. Robust decreases from baseline in 7alpha-hydroxy-4-cholesten-3-one (p <0.001) and liver fat content (p <0.001) were also observed. Rosuvastatin was safe and well-tolerated when co-administered with NGM282 in patients with NASH. In this multicenter study, NGM282-associated elevation of cholesterol was effectively managed with rosuvastatin. Co-administration of rosuvastatin with NGM282 may be a reasonable strategy to optimize the cardiovascular risk profile in patients with NASH. Fibroblast growth factor 19 (FGF19), an endocrine hormone produced in the ileum, acts on the liver to suppress bile acid synthesis from cholesterol, while also inhibiting insulin-induced hepatic lipogenesis.8,9FGF19 regulates bile acid metabolism via suppression of CYP7A1, the first and rate-limiting enzyme in the classic pathway for the conversion of cholesterol to bile acids.NGM282 is a non-tumorigenic analogue of FGF19 that retains the ability to suppress CYP7A1.10,11In a double-blind, randomized, placebo-controlled study previously reported, NGM282 3 mg and 6 mg produced rapid and sustained improvements in liver fat content over 12 weeks in patients with biopsy-proven NASH.12NGM282 also significantly improved liver aminotransferases, serum fibrosis biomarkers, and histological features of NASH.12,13However, a significant increase in low-density lipoprotein cholesterol (LDL-C) was observed after 12 weeks of treatment with NGM282.Given that patients with NASH develop atherogenic dyslipidemia, which is associated with an increased risk of atherosclerotic cardiovascular disease (ASCVD),14–16 it is important to explore whether co-administration of a 3-hydroxy-3-methylglutaryl-coenzyme A (HMG-CoA) reductase inhibitor can mitigate the effect of NGM282 on LDL-C and favorably impact the atherogenic lipoprotein profile.
Hepatitis C virus (HCV) infection contributes to the development of autoimmune disorders such as cryoglobulinaemia vasculitis (CV). However, it remains unclear why only some individuals with HCV develop HCV-associated CV (HCV-CV). HCV-CV is characterized by the expansion of anergic CD19+CD27+CD21low/− atypical memory B cells (AtMs). Herein, we report the mechanisms by which AtMs participate in HCV-associated autoimmunity. The phenotype and function of peripheral AtMs were studied by multicolour flow cytometry and co-culture assays with effector T cells and regulatory T cells in 20 patients with HCV-CV, 10 chronically HCV-infected patients without CV and 8 healthy donors. We performed gene expression profile analysis of AtMs stimulated or not by TLR9. Immunoglobulin gene repertoire and antibody reactivity profiles of AtM-expressing IgM antibodies were analysed following single B cell FACS sorting and expression-cloning of monoclonal antibodies. The Tbet+CD11c+CD27+CD21− AtM population is expanded in patients with HCV-CV compared to HCV controls without CV. TLR9 activation of AtMs induces a specific transcriptional signature centred on TNFα overexpression, and an enhanced secretion of TNFα and rheumatoid factor-type IgMs in patients with HCV-CV. AtMs stimulated through TLR9 promote type 1 effector T cell activation and reduce the proliferation of CD4+CD25hiCD127−/lowFoxP3+ regulatory T cells. AtM expansions display intraclonal diversity with immunoglobulin features of antigen-driven maturation. AtM-derived IgM monoclonal antibodies do not react against ubiquitous autoantigens or HCV antigens including NS3 and E2 proteins. Rather, AtM-derived antibodies possess rheumatoid factor activity and target unique epitopes on the human IgG-Fc region. Our data strongly suggest a central role for TLR9 activation of AtMs in driving HCV-CV autoimmunity through rheumatoid factor production and type 1 T cell responses. Chronic HCV infection is associated with extrahepatic complications that are largely immunologically driven.Among those, cryoglobulinaemia and its clinical sequelae hold the strongest association.Cryoglobulins are readily detectable in 40–60% of HCV-infected patients,1–3 whereas cryoglobulinaemia vasculitis (CV) develops in only 5–10% of the cases.4,5The presence of autoantibodies and T cells in vascular infiltrates as well as the observation that specific HLA alleles confer susceptibility to CV in HCV-infected patients support the autoimmune nature of this virus-associated pathology.6,7CV pathophysiology depends on the interaction between HCV and lymphocytes that directly modulate B- and T cell function, which ultimately leads to the polyclonal activation and expansion of B cells producing rheumatoid factors (RFs).1,8We previously reported abnormal immune responses mediated by T cells in patients with HCV-associated cryoglobulinaemia vasculitis (HCV-CV), with a quantitative defect in regulatory T cells (Tregs),6 and a Th1 polarization.9,10HCV infection has been associated with lymphoproliferations, which likely result from an indirect process following the chronic antigenic stimulation of a limited pool of pre-existing autoreactive B cells.It has been proposed that persistently high levels of HCV-containing immune complexes stimulate the proliferation of RF-bearing B cells, but the precise antigen(s) and stimulatory mechanisms have remained elusive.Our group and others previously identified a clonal expansion of CD27+IgM+CD21low/− memory B cells, referred to as activated or atypical memory B cells (AtMs), in HCV-CV.11,12AtMs are clonal or clonally related, and mainly express the VH1-69 IgH gene, which is also highly prevalent in HCV-associated lymphoproliferations.11–13These clonal cells express reduced levels of the complement receptor 2, CD21, which mirrors an anergic state.14,15AtMs are prone to undergoing apoptosis.11,13They do not proliferate upon BCR stimulation but respond to the TLR9 agonist CpG by expressing activator and proliferative markers.11,13Anergy is a well-known regulatory mechanism for maintaining immune tolerance of autoreactive cells.14,15Indeed, AtMs in HCV-CV produced somatically mutated RF autoantibodies,16 and are not removed from the B cell repertoire.However, it remains unclear why only some HCV-infected individuals develop CV, and why anergic mechanisms fail to prevent the development of non-Hodgkin’s lymphoma in some patients with HCV-CV.
As a nicotinamide adenine dinucleotide-dependent deacetylase and a key epigenetic regulator, sirtuin 6 (SIRT6) has been implicated in the regulation of metabolism, DNA repair, and inflammation. However, the role of SIRT6 in alcohol-related liver disease (ALD) remains unclear. The aim of this study was to investigate the function and mechanism of SIRT6 in ALD pathogenesis. We developed and characterized Sirt6 knockout (KO) and transgenic mouse models that were treated with either control or ethanol diet. Hepatic steatosis, inflammation, and oxidative stress were analyzed using biochemical and histological methods. Gene regulation was analyzed by luciferase reporter and chromatin immunoprecipitation assays. The Sirt6 KO mice developed severe liver injury characterized by a remarkable increase of oxidative stress and inflammation, whereas the Sirt6 transgenic mice were protected from ALD via normalization of hepatic lipids, inflammatory response, and oxidative stress. Our molecular analysis has identified a number of novel Sirt6-regulated genes that are involved in antioxidative stress, including metallothionein 1 and 2 (Mt1 and Mt2). Mt1/2 genes were downregulated in the livers of Sirt6 KO mice and patients with alcoholic hepatitis. Overexpression of Mt1 in the liver of Sirt6 KO mice improved ALD by reducing hepatic oxidative stress and inflammation. We also identified a critical link between SIRT6 and metal regulatory transcription factor 1 (Mtf1) via a physical interaction and functional coactivation. Mt1/2 promoter reporter assays showed a strong synergistic effect of SIRT6 on the transcriptional activity of Mtf1. Our data suggest that SIRT6 plays a critical protective role against ALD and it may serve as a potential therapeutic target for ALD. Chronic and excessive alcohol consumption causes nearly half of liver cirrhosis-associated mortality in the United States, but there remains no effective treatment for the underlying liver disorder.1An early stage of alcohol-related liver disease (ALD), which is featured as simple hepatic steatosis, is reversible; however, chronic and excessive alcohol consumption can lead to progressive steatohepatitis (ASH) and fibrosis, and in some cases, the disease further progresses to cirrhosis and even hepatocellular carcinoma.2,3Alcohol-induced hepatic steatosis initially manifests as lipid droplet accumulation in the liver, but as the liver tissue gets injured by the lipid overload, circulating and resident immune cells including Kupffer cells and infiltrated macrophages and neutrophils respond to the liver injury by producing inflammatory cytokines, such as tumor necrosis factor-α (TNF-α) and interleukin (IL)-1β.Chronic alcohol drinking and drinking patterns such as binges can lead to repeated liver injury, inflammation, and oxidative stress.2,4Therefore, there is a clinical need for the better understanding of ALD pathogenesis and identification of therapeutic targets.
The prevalence of anti-hepatitis C virus antibody in Punjab, India is 3.6%, with 728,000 people estimated to have viremic chronic hepatitis C (CHC). The Mukh-Mantri Punjab Hepatitis C Relief Fund, launched on 18th June 2016, provides no-cost generic direct-acting antivirals (DAAs) with sofosbuvir + ledipasvir ± ribavirin or sofosbuvir + daclatasvir ± ribavirin with the goal of eliminating CHC from Punjab. We assessed the safety and efficacy of decentralized treatment of CHC in a public health care setting. Primary care providers from 3 university and 22 district hospitals were trained to provide algorithm-based DAA treatment and supervised by telehealth clinics conducted fortnightly. The diagnosis of cirrhosis was based on clinical and radiological evidence, including aspartate aminotransferase-to-platelet ratio index (APRI ≥2.0) and FIB-4 score (>3.25), or on liver stiffness measurement ≥12.5 kPa on Fibroscan®. We enrolled 48,088 individuals with CHC (63.8% male; mean age 42.1 years; 80.5% rural; 14.8% compensated cirrhosis; 69.9% genotype [GT] 3) between 18th June 2016 to 31st July 2018. While 36,250 (75.4%) patients completed treatment, 5,497 (11.4%) had treatment interruptions and 6,341 (13.2%) patients are currently ongoing treatment. Sustained virological response at 12 weeks after treatment completion (SVR12) was achieved in 91.6% of patients per protocol, 67.6% in intention-to-treat (ITT) analysis, where all interruptions were treated as failures, and 91.2% in a modified ITT analysis where all patients with successful SVR12 in the interruptions arm were included as cured. SVR12 rates in patients with and without cirrhosis and GT3 versus non-GT3 were comparable. The SVR12 rate was 84.4% in patients who had treatment interruptions. Decentralized care of patients with CHC using generic all-oral DAA regimens is safe and effective regardless of genotype or presence of cirrhosis. Of the 28 million people living in Punjab (2011 Census), 3.6% are estimated to be positive for anti-HCV antibody and 2.6% (728,000) are estimated to test positive for HCV RNA.1,2A majority of these people may progress to cirrhosis and its complications including variceal bleeding, liver failure and hepatocellular carcinoma (HCC), or death.3,4Successful treatment of chronic hepatitis C (CHC) is associated with 62–84% reduction in all-cause mortality, 68–79% reduction in risk of HCC and 90% reduction in need for liver transplantation.Treating CHC saves lives, is cost-effective and cost-saving in the long-term by reducing the overall cost of public health expenditure for treatment of liver disease.5With the advent of generic direct-acting antivirals (DAAs), the treatment is less expensive and equally effective (cure rates >90%).6–8The Mukh-Mantri Punjab Hepatitis C Relief Fund program, provides free medical treatment for people with CHC with the goal of eliminating hepatitis C from Punjab.9The Punjab Model is an innovative interactive model of decentralized services that uses telementoring and algorithm-based treatment with generic drugs.The Punjab Model trains and supports primary care providers (PCPs) to learn about emerging treatment options, adverse effects and treatment adherence, so that they can manage CHC using the existing health care infrastructure.10,11
The prevalence of anti-hepatitis C virus antibody in Punjab, India is 3.6%, with 728,000 people estimated to have viremic chronic hepatitis C (CHC). The Mukh-Mantri Punjab Hepatitis C Relief Fund, launched on 18th June 2016, provides no-cost generic direct-acting antivirals (DAAs) with sofosbuvir + ledipasvir ± ribavirin or sofosbuvir + daclatasvir ± ribavirin with the goal of eliminating CHC from Punjab. We assessed the safety and efficacy of decentralized treatment of CHC in a public health care setting. Primary care providers from 3 university and 22 district hospitals were trained to provide algorithm-based DAA treatment and supervised by telehealth clinics conducted fortnightly. The diagnosis of cirrhosis was based on clinical and radiological evidence, including aspartate aminotransferase-to-platelet ratio index (APRI ≥2.0) and FIB-4 score (>3.25), or on liver stiffness measurement ≥12.5 kPa on Fibroscan®. We enrolled 48,088 individuals with CHC (63.8% male; mean age 42.1 years; 80.5% rural; 14.8% compensated cirrhosis; 69.9% genotype [GT] 3) between 18th June 2016 to 31st July 2018. While 36,250 (75.4%) patients completed treatment, 5,497 (11.4%) had treatment interruptions and 6,341 (13.2%) patients are currently ongoing treatment. Sustained virological response at 12 weeks after treatment completion (SVR12) was achieved in 91.6% of patients per protocol, 67.6% in intention-to-treat (ITT) analysis, where all interruptions were treated as failures, and 91.2% in a modified ITT analysis where all patients with successful SVR12 in the interruptions arm were included as cured. SVR12 rates in patients with and without cirrhosis and GT3 versus non-GT3 were comparable. The SVR12 rate was 84.4% in patients who had treatment interruptions. Decentralized care of patients with CHC using generic all-oral DAA regimens is safe and effective regardless of genotype or presence of cirrhosis. Decentralized services were used to treat patients with CHC through the involvement of PCPs, pharmacists and other team members in district hospitals.The primary hub in this model is the Postgraduate Institute of Medical Education and Research, Chandigarh, India, and 22 district hospitals and 3 university hospitals serve as spokes.12
Controlled attenuation parameter (CAP) is a novel non-invasive measure of hepatic steatosis, but it has not been evaluated in alcoholic liver disease. Therefore, we aimed to validate CAP for the assessment of biopsy-verified alcoholic steatosis and to study the effect of alcohol detoxification on CAP. This was a cross-sectional biopsy-controlled diagnostic study in four European liver centres. Consecutive alcohol-overusing patients underwent concomitant CAP, regular ultrasound, and liver biopsy. In addition, we measured CAP before and after admission for detoxification in a separate single-centre cohort. A total of 562 patients were included in the study: 269 patients in the diagnostic cohort with steatosis scores S0, S1, S2, and S3 = 77 (28%), 94 (35%), 64 (24%), and 34 (13%), respectively. CAP diagnosed any steatosis and moderate steatosis with fair accuracy (area under the receiver operating characteristic curve [AUC] ≥S1 = 0.77; 0.71–0.83 and AUC ≥S2 = 0.78; 0.72–0.83), and severe steatosis with good accuracy (AUC S3 = 0.82; 0.75–0.88). CAP was superior to bright liver echo pattern by regular ultrasound. CAP above 290 dB/m ruled in any steatosis with 88% specificity and 92% positive predictive value, while CAP below 220 dB/m ruled out steatosis with 90% sensitivity, but 62% negative predictive value. In the 293 patients who were admitted 6.3 days (interquartile range 4–6) for detoxification, CAP decreased by 32 ± 47 dB/m (p <0.001). Body mass index predicted higher CAP in both cohorts, irrespective of drinking pattern. Obese patients with body mass index ≥30 kg/m2 had a significantly higher CAP, which did not decrease significantly during detoxification. CAP has a good diagnostic accuracy for diagnosing severe alcoholic liver steatosis and can be used to rule in any steatosis. In non-obese but not in obese, patients, CAP rapidly declines after alcohol withdrawal. Alcohol is a key risk factor for liver-related and overall mortality, contributing to 3.3 million annual deaths worldwide.1Simple steatosis is the most common liver manifestation of harmful drinking, but is considered a benign condition by many, since the disturbed lipid metabolism normalises with abstinence.2,3The high prevalence of steatosis in most liver diseases has even cast doubt on the role of hepatic fat on fibrosis progression, with steatosis being discussed as a bystander rather than a causative factor.4,5This notion may, however, be too credulous, as 7% of patients with simple alcoholic steatosis have been shown to progress to cirrhosis within five years.6Additionally, in non-alcoholic fatty-liver disease, steatosis is an independent risk factor for the development of type 2 diabetes and cardiovascular disease.7Finally, harmful drinking is often accompanied by similar unfavourable health behaviours, such as overeating, smoking, and a sedentary lifestyle.Thus, components of the metabolic syndrome (MetS) are common in alcoholic patients, where they may aggravate steatosis and act in synergy with alcohol to progress fibrosis.8Consequently, reliable non-invasive tools to diagnose and monitor hepatic steatosis in patients with alcoholic liver disease (ALD) are needed.
Controlled attenuation parameter (CAP) is a novel non-invasive measure of hepatic steatosis, but it has not been evaluated in alcoholic liver disease. Therefore, we aimed to validate CAP for the assessment of biopsy-verified alcoholic steatosis and to study the effect of alcohol detoxification on CAP. This was a cross-sectional biopsy-controlled diagnostic study in four European liver centres. Consecutive alcohol-overusing patients underwent concomitant CAP, regular ultrasound, and liver biopsy. In addition, we measured CAP before and after admission for detoxification in a separate single-centre cohort. A total of 562 patients were included in the study: 269 patients in the diagnostic cohort with steatosis scores S0, S1, S2, and S3 = 77 (28%), 94 (35%), 64 (24%), and 34 (13%), respectively. CAP diagnosed any steatosis and moderate steatosis with fair accuracy (area under the receiver operating characteristic curve [AUC] ≥S1 = 0.77; 0.71–0.83 and AUC ≥S2 = 0.78; 0.72–0.83), and severe steatosis with good accuracy (AUC S3 = 0.82; 0.75–0.88). CAP was superior to bright liver echo pattern by regular ultrasound. CAP above 290 dB/m ruled in any steatosis with 88% specificity and 92% positive predictive value, while CAP below 220 dB/m ruled out steatosis with 90% sensitivity, but 62% negative predictive value. In the 293 patients who were admitted 6.3 days (interquartile range 4–6) for detoxification, CAP decreased by 32 ± 47 dB/m (p <0.001). Body mass index predicted higher CAP in both cohorts, irrespective of drinking pattern. Obese patients with body mass index ≥30 kg/m2 had a significantly higher CAP, which did not decrease significantly during detoxification. CAP has a good diagnostic accuracy for diagnosing severe alcoholic liver steatosis and can be used to rule in any steatosis. In non-obese but not in obese, patients, CAP rapidly declines after alcohol withdrawal. Controlled attenuation parameter (CAP) is a novel method for the non-invasive assessment of steatosis, which measures the increased attenuation of ultrasound waves when travelling through steatotic hepatic tissue, compared to normal liver.9The CAP software is incorporated into the FibroScan® (Echosens, Paris, France) equipment, allowing for combined transient elastography (TE) and CAP.An individual patient data meta-analysis recently showed that CAP diagnosed moderate and severe steatosis with diagnostic accuracies above 0.85 in mixed-aetiology liver-disease patients.However, the analysis did not include ALD patients.10To date, no sufficiently large biopsy-verified study exists to analyse the performance of CAP in patients with ALD.
Hepatic recruitment of monocyte-derived macrophages (MoMFs) contributes to the inflammatory response in non-alcoholic steatohepatitis (NASH). However, how hepatocyte lipotoxicity promotes MoMF inflammation is unclear. Here we demonstrate that lipotoxic hepatocyte-derived extracellular vesicles (LPC-EVs) are enriched with active integrin β1 (ITGβ1), which promotes monocyte adhesion and liver inflammation in murine NASH. Hepatocytes were treated with either vehicle or the toxic lipid mediator lysophosphatidylcholine (LPC); EVs were isolated from the conditioned media and subjected to proteomic analysis. C57BL/6J mice were fed a diet rich in fat, fructose, and cholesterol (FFC) to induce NASH. Mice were treated with anti-ITGβ1 neutralizing antibody (ITGβ1Ab) or control IgG isotype. Ingenuity® Pathway Analysis of the LPC-EV proteome indicated that ITG signaling is an overrepresented canonical pathway. Immunogold electron microscopy and nanoscale flow cytometry confirmed that LPC-EVs were enriched with activated ITGβ1. Furthermore, we showed that LPC treatment in hepatocytes activates ITGβ1 and mediates its endocytic trafficking and sorting into EVs. LPC-EVs enhanced monocyte adhesion to liver sinusoidal cells, as observed by shear stress adhesion assay. This adhesion was attenuated in the presence of ITGβ1Ab. FFC-fed, ITGβ1Ab-treated mice displayed reduced inflammation, defined by decreased hepatic infiltration and activation of proinflammatory MoMFs, as assessed by immunohistochemistry, mRNA expression, and flow cytometry. Likewise, mass cytometry by time-of-flight on intrahepatic leukocytes showed that ITGβ1Ab reduced levels of infiltrating proinflammatory monocytes. Furthermore, ITGβ1Ab treatment significantly ameliorated liver injury and fibrosis. Lipotoxic EVs mediate monocyte adhesion to LSECs mainly through an ITGβ1-dependent mechanism. ITGβ1Ab ameliorates diet-induced NASH in mice by reducing MoMF-driven inflammation, suggesting that blocking ITGβ1 is a potential anti-inflammatory therapeutic strategy in human NASH. With the worldwide increase in obesity, non-alcoholic fatty liver disease (NAFLD) is currently the most common chronic liver disease.1A subset of patients with NAFLD develop a more severe inflammatory form termed non-alcoholic steatohepatitis (NASH) which can progress to end-stage liver disease.NASH is currently the leading cause of liver-related mortality in many western countries.2Therefore, there is an unmet need for mechanism-based therapeutic strategies that reverse established NASH and control the progression of the disease.
Hepatic recruitment of monocyte-derived macrophages (MoMFs) contributes to the inflammatory response in non-alcoholic steatohepatitis (NASH). However, how hepatocyte lipotoxicity promotes MoMF inflammation is unclear. Here we demonstrate that lipotoxic hepatocyte-derived extracellular vesicles (LPC-EVs) are enriched with active integrin β1 (ITGβ1), which promotes monocyte adhesion and liver inflammation in murine NASH. Hepatocytes were treated with either vehicle or the toxic lipid mediator lysophosphatidylcholine (LPC); EVs were isolated from the conditioned media and subjected to proteomic analysis. C57BL/6J mice were fed a diet rich in fat, fructose, and cholesterol (FFC) to induce NASH. Mice were treated with anti-ITGβ1 neutralizing antibody (ITGβ1Ab) or control IgG isotype. Ingenuity® Pathway Analysis of the LPC-EV proteome indicated that ITG signaling is an overrepresented canonical pathway. Immunogold electron microscopy and nanoscale flow cytometry confirmed that LPC-EVs were enriched with activated ITGβ1. Furthermore, we showed that LPC treatment in hepatocytes activates ITGβ1 and mediates its endocytic trafficking and sorting into EVs. LPC-EVs enhanced monocyte adhesion to liver sinusoidal cells, as observed by shear stress adhesion assay. This adhesion was attenuated in the presence of ITGβ1Ab. FFC-fed, ITGβ1Ab-treated mice displayed reduced inflammation, defined by decreased hepatic infiltration and activation of proinflammatory MoMFs, as assessed by immunohistochemistry, mRNA expression, and flow cytometry. Likewise, mass cytometry by time-of-flight on intrahepatic leukocytes showed that ITGβ1Ab reduced levels of infiltrating proinflammatory monocytes. Furthermore, ITGβ1Ab treatment significantly ameliorated liver injury and fibrosis. Lipotoxic EVs mediate monocyte adhesion to LSECs mainly through an ITGβ1-dependent mechanism. ITGβ1Ab ameliorates diet-induced NASH in mice by reducing MoMF-driven inflammation, suggesting that blocking ITGβ1 is a potential anti-inflammatory therapeutic strategy in human NASH. Current concepts suggest that excess circulating free fatty acids mediate hepatocyte lipotoxicity in NASH.3Moreover, patients with NASH are at risk of end-stage liver disease, mainly secondary to the unrelenting sterile inflammatory response triggered by hepatocyte lipotoxicity.This inflammatory response is mediated, in part, by the recruited monocytes that differentiate into macrophages, so-called monocyte-derived macrophages (MoMFs).4Although targeting monocyte infiltration in NASH via the dual CC chemokine receptor types 2 and 5 (CCR2/5) antagonist improved fibrosis, it was insufficient to resolve human steatohepatitis.5Therefore, key additional signals regulating the trafficking and retention of circulating monocytes in the NASH liver remain undefined.
Hepatic recruitment of monocyte-derived macrophages (MoMFs) contributes to the inflammatory response in non-alcoholic steatohepatitis (NASH). However, how hepatocyte lipotoxicity promotes MoMF inflammation is unclear. Here we demonstrate that lipotoxic hepatocyte-derived extracellular vesicles (LPC-EVs) are enriched with active integrin β1 (ITGβ1), which promotes monocyte adhesion and liver inflammation in murine NASH. Hepatocytes were treated with either vehicle or the toxic lipid mediator lysophosphatidylcholine (LPC); EVs were isolated from the conditioned media and subjected to proteomic analysis. C57BL/6J mice were fed a diet rich in fat, fructose, and cholesterol (FFC) to induce NASH. Mice were treated with anti-ITGβ1 neutralizing antibody (ITGβ1Ab) or control IgG isotype. Ingenuity® Pathway Analysis of the LPC-EV proteome indicated that ITG signaling is an overrepresented canonical pathway. Immunogold electron microscopy and nanoscale flow cytometry confirmed that LPC-EVs were enriched with activated ITGβ1. Furthermore, we showed that LPC treatment in hepatocytes activates ITGβ1 and mediates its endocytic trafficking and sorting into EVs. LPC-EVs enhanced monocyte adhesion to liver sinusoidal cells, as observed by shear stress adhesion assay. This adhesion was attenuated in the presence of ITGβ1Ab. FFC-fed, ITGβ1Ab-treated mice displayed reduced inflammation, defined by decreased hepatic infiltration and activation of proinflammatory MoMFs, as assessed by immunohistochemistry, mRNA expression, and flow cytometry. Likewise, mass cytometry by time-of-flight on intrahepatic leukocytes showed that ITGβ1Ab reduced levels of infiltrating proinflammatory monocytes. Furthermore, ITGβ1Ab treatment significantly ameliorated liver injury and fibrosis. Lipotoxic EVs mediate monocyte adhesion to LSECs mainly through an ITGβ1-dependent mechanism. ITGβ1Ab ameliorates diet-induced NASH in mice by reducing MoMF-driven inflammation, suggesting that blocking ITGβ1 is a potential anti-inflammatory therapeutic strategy in human NASH. Hepatocytes release diverse types of membrane-bound, nanometer-sized extracellular vesicles (EVs) into the extracellular milieu under physiological conditions.EVs are efficient messengers, with superior stability and bioavailability of their signature cargos implicated in inflammatory responses.6–8Interestingly, many of the EV cargos are selectively transferred through intracellular trafficking pathways and packaged into EVs, reflecting the pathophysiological context of the parent cell.9EVs released from lipotoxic hepatocytes are involved in MoMF chemotaxis and the hepatic inflammatory response,6–8 however, the role of lipotoxic hepatocyte-derived EVs (lipotoxic EVs) in promoting circulating monocyte liver-specific homing through regulating their adhesion to liver sinusoidal endothelial cells (LSECs) in the NASH liver microenvironment has not been explored.
Hepatic recruitment of monocyte-derived macrophages (MoMFs) contributes to the inflammatory response in non-alcoholic steatohepatitis (NASH). However, how hepatocyte lipotoxicity promotes MoMF inflammation is unclear. Here we demonstrate that lipotoxic hepatocyte-derived extracellular vesicles (LPC-EVs) are enriched with active integrin β1 (ITGβ1), which promotes monocyte adhesion and liver inflammation in murine NASH. Hepatocytes were treated with either vehicle or the toxic lipid mediator lysophosphatidylcholine (LPC); EVs were isolated from the conditioned media and subjected to proteomic analysis. C57BL/6J mice were fed a diet rich in fat, fructose, and cholesterol (FFC) to induce NASH. Mice were treated with anti-ITGβ1 neutralizing antibody (ITGβ1Ab) or control IgG isotype. Ingenuity® Pathway Analysis of the LPC-EV proteome indicated that ITG signaling is an overrepresented canonical pathway. Immunogold electron microscopy and nanoscale flow cytometry confirmed that LPC-EVs were enriched with activated ITGβ1. Furthermore, we showed that LPC treatment in hepatocytes activates ITGβ1 and mediates its endocytic trafficking and sorting into EVs. LPC-EVs enhanced monocyte adhesion to liver sinusoidal cells, as observed by shear stress adhesion assay. This adhesion was attenuated in the presence of ITGβ1Ab. FFC-fed, ITGβ1Ab-treated mice displayed reduced inflammation, defined by decreased hepatic infiltration and activation of proinflammatory MoMFs, as assessed by immunohistochemistry, mRNA expression, and flow cytometry. Likewise, mass cytometry by time-of-flight on intrahepatic leukocytes showed that ITGβ1Ab reduced levels of infiltrating proinflammatory monocytes. Furthermore, ITGβ1Ab treatment significantly ameliorated liver injury and fibrosis. Lipotoxic EVs mediate monocyte adhesion to LSECs mainly through an ITGβ1-dependent mechanism. ITGβ1Ab ameliorates diet-induced NASH in mice by reducing MoMF-driven inflammation, suggesting that blocking ITGβ1 is a potential anti-inflammatory therapeutic strategy in human NASH. LSECs are highly specialized endothelial cells that serve as a platform for various immune cells, including monocytes, to lodge in the liver.10As monocyte receptor-LSEC ligand interactions are not unique to the liver, the question remains whether hepatocyte-specific recruitment processes via EVs exists in NASH.A single prior study reported that adoptive transfer of EVs isolated from the serum of high fat diet-fed mice into chow-fed mice resulted in myeloid cell activation and accumulation in the liver.11However, the mechanism by which EVs mediate the hepatic accumulation of myeloid cells was not explored.
Hepatic recruitment of monocyte-derived macrophages (MoMFs) contributes to the inflammatory response in non-alcoholic steatohepatitis (NASH). However, how hepatocyte lipotoxicity promotes MoMF inflammation is unclear. Here we demonstrate that lipotoxic hepatocyte-derived extracellular vesicles (LPC-EVs) are enriched with active integrin β1 (ITGβ1), which promotes monocyte adhesion and liver inflammation in murine NASH. Hepatocytes were treated with either vehicle or the toxic lipid mediator lysophosphatidylcholine (LPC); EVs were isolated from the conditioned media and subjected to proteomic analysis. C57BL/6J mice were fed a diet rich in fat, fructose, and cholesterol (FFC) to induce NASH. Mice were treated with anti-ITGβ1 neutralizing antibody (ITGβ1Ab) or control IgG isotype. Ingenuity® Pathway Analysis of the LPC-EV proteome indicated that ITG signaling is an overrepresented canonical pathway. Immunogold electron microscopy and nanoscale flow cytometry confirmed that LPC-EVs were enriched with activated ITGβ1. Furthermore, we showed that LPC treatment in hepatocytes activates ITGβ1 and mediates its endocytic trafficking and sorting into EVs. LPC-EVs enhanced monocyte adhesion to liver sinusoidal cells, as observed by shear stress adhesion assay. This adhesion was attenuated in the presence of ITGβ1Ab. FFC-fed, ITGβ1Ab-treated mice displayed reduced inflammation, defined by decreased hepatic infiltration and activation of proinflammatory MoMFs, as assessed by immunohistochemistry, mRNA expression, and flow cytometry. Likewise, mass cytometry by time-of-flight on intrahepatic leukocytes showed that ITGβ1Ab reduced levels of infiltrating proinflammatory monocytes. Furthermore, ITGβ1Ab treatment significantly ameliorated liver injury and fibrosis. Lipotoxic EVs mediate monocyte adhesion to LSECs mainly through an ITGβ1-dependent mechanism. ITGβ1Ab ameliorates diet-induced NASH in mice by reducing MoMF-driven inflammation, suggesting that blocking ITGβ1 is a potential anti-inflammatory therapeutic strategy in human NASH. Integrins (ITGs) provide the central mechanism for cells in multicellular organisms to interact with and sense their extracellular environment.12Integrins are heterodimeric cell surface transmembrane proteins consisting of 24 non-covalently associated α and β subunits which mediate cell-cell and cell-matrix interaction.13ITGα9 and β1 exist as heterodimers, and our data indicate that they are particularly enriched in EVs derived from lipotoxic hepatocytes; hence we will use ITGβ1 for simplicity in this manuscript to refer to ITGα9β1.Vascular cell adhesion molecule 1 (VCAM-1) is expressed on the surface of LSECs and is a known ligand for ITGα9β1.14Interestingly, ITG can adopt a closed conformation that has a low affinity for ligand (inactive) or an extended open conformation that has a high affinity for ligand (active).15Binding of intracellular proteins such as Talin to the dephosphorylated cytoplasmic tail of ITGβ1 regulates its activation and promotes ligand binding.15Kinase p38 has been implicated in ITGβ activation via an inside-out, ligand-independent signaling in different disease models.16,17We have previously demonstrated that lipotoxic treatment in hepatocytes induces a mitogen-activated protein kinase (MAPK) signaling cascade leading to the activated phosphorylation of p38.6,18,19Moreover, ITGs undergo constant endocytic trafficking and recycling that regulate ITG-mediated cell adhesion and migration.20,21This process of ITG trafficking suggests that in lipotoxic hepatocytes, ITGβ1 traffics through the endocytic-multivesicular body (MVB) pathway to be released in EVs.
The World Health Organization (WHO) established targets to eliminate hepatitis C virus (HCV) infection as a public health threat by 2030. Evidence that HCV treatment can lower viraemic prevalence among people who inject drugs (PWID) is limited. Broad accessibility of direct-acting antiviral (DAA) therapy in Australia, since March 2016, provides an opportunity to assess the efficacy of these treatments at a population level in a real-world setting. Data from Australia’s annual bio-behavioural surveillance examined treatment uptake and estimated viraemic prevalence among PWID attending needle syringe programs nationally between 2015 and 2017. Multivariate logistic regression identified variables independently associated with HCV treatment among those considered eligible (anti-HCV positive excluding HCV RNA negative with no self-reported history of HCV treatment) in 2017. Annual samples ranged from 1,995–2,380 PWID. Anti-HCV prevalence declined from 57% (2015) to 49% (2017, χ2 p trend <0.001), with 40–56% of anti-HCV positive respondents providing sufficient sample for HCV RNA testing. Between 2015 and 2017, treatment uptake among those eligible increased from 10% to 41% (χ2 p trend <0.001) and viraemic prevalence among the overall sample declined from 43% to 25% (χ2 p trend <0.001). In multivariable analysis, older age (≥50 years adjusted odds ratio [aOR] 1.82; 95% CI 1.09–3.06;p = 0.023 and 44–49 years aOR 1.75; 95% CI 1.03–3.00;p = 0.038 vs. ≤37 years) and history of opioid substitution therapy (aOR 2.06; 95% CI 1.30–3.26; p = 0.002) were independently associated with treatment. This study confirms PWID are willing to initiate treatment when HCV DAA therapy is available and provides population-level evidence of a decline in viraemic prevalence among people most at risk of ongoing HCV transmission. Scaled up surveillance and monitoring are required to evaluate progress toward WHO HCV elimination goals. The treatment landscape for chronic hepatitis C virus (HCV) infection has transformed over the past five years, with the development of all-oral direct-acting antiviral (DAA) regimens with minimal toxicity and cure rates above 95%.1In keeping with the optimism surrounding these new treatments, the World Health Organisation (WHO) recently developed targets with a goal to eliminate viral hepatitis as a public health threat by 2030, including 80% of the eligible chronic HCV population treated, 65% reduction in liver-related mortality and 80% reduction in HCV incidence by 2030.2
The World Health Organization (WHO) established targets to eliminate hepatitis C virus (HCV) infection as a public health threat by 2030. Evidence that HCV treatment can lower viraemic prevalence among people who inject drugs (PWID) is limited. Broad accessibility of direct-acting antiviral (DAA) therapy in Australia, since March 2016, provides an opportunity to assess the efficacy of these treatments at a population level in a real-world setting. Data from Australia’s annual bio-behavioural surveillance examined treatment uptake and estimated viraemic prevalence among PWID attending needle syringe programs nationally between 2015 and 2017. Multivariate logistic regression identified variables independently associated with HCV treatment among those considered eligible (anti-HCV positive excluding HCV RNA negative with no self-reported history of HCV treatment) in 2017. Annual samples ranged from 1,995–2,380 PWID. Anti-HCV prevalence declined from 57% (2015) to 49% (2017, χ2 p trend <0.001), with 40–56% of anti-HCV positive respondents providing sufficient sample for HCV RNA testing. Between 2015 and 2017, treatment uptake among those eligible increased from 10% to 41% (χ2 p trend <0.001) and viraemic prevalence among the overall sample declined from 43% to 25% (χ2 p trend <0.001). In multivariable analysis, older age (≥50 years adjusted odds ratio [aOR] 1.82; 95% CI 1.09–3.06;p = 0.023 and 44–49 years aOR 1.75; 95% CI 1.03–3.00;p = 0.038 vs. ≤37 years) and history of opioid substitution therapy (aOR 2.06; 95% CI 1.30–3.26; p = 0.002) were independently associated with treatment. This study confirms PWID are willing to initiate treatment when HCV DAA therapy is available and provides population-level evidence of a decline in viraemic prevalence among people most at risk of ongoing HCV transmission. Scaled up surveillance and monitoring are required to evaluate progress toward WHO HCV elimination goals. The WHO goal to eliminate viral hepatitis (HCV and hepatitis B) as a public health threat, is a clear response to the rising burden of liver disease globally, with viral hepatitis ranked as the seventh leading cause of death worldwide in 2013 and ∼ 700,000 deaths attributed to HCV annually.3An estimated 71 million people are living with chronic HCV infection,4 with people who inject drugs (PWID) the predominant affected population in high income countries and a major population in many low and middle-income countries.5PWID have been recognised as a priority population for DAA therapy,6,7 given high HCV prevalence,8 the rising burden of liver disease,5 the potential for treatment as prevention benefits9 and associated enhanced cost-effectiveness.10Despite these recommendations, restrictions that exclude people with ongoing substance use limit access to DAA therapy by PWID in many settings.11,12Other restrictions, based on the extent of liver disease or a requirement for specialist prescribing also impact uptake by PWID even in settings without substance use restrictions, as the large majority of PWID have early liver disease13 and often have poor access to specialist services.14
The World Health Organization (WHO) established targets to eliminate hepatitis C virus (HCV) infection as a public health threat by 2030. Evidence that HCV treatment can lower viraemic prevalence among people who inject drugs (PWID) is limited. Broad accessibility of direct-acting antiviral (DAA) therapy in Australia, since March 2016, provides an opportunity to assess the efficacy of these treatments at a population level in a real-world setting. Data from Australia’s annual bio-behavioural surveillance examined treatment uptake and estimated viraemic prevalence among PWID attending needle syringe programs nationally between 2015 and 2017. Multivariate logistic regression identified variables independently associated with HCV treatment among those considered eligible (anti-HCV positive excluding HCV RNA negative with no self-reported history of HCV treatment) in 2017. Annual samples ranged from 1,995–2,380 PWID. Anti-HCV prevalence declined from 57% (2015) to 49% (2017, χ2 p trend <0.001), with 40–56% of anti-HCV positive respondents providing sufficient sample for HCV RNA testing. Between 2015 and 2017, treatment uptake among those eligible increased from 10% to 41% (χ2 p trend <0.001) and viraemic prevalence among the overall sample declined from 43% to 25% (χ2 p trend <0.001). In multivariable analysis, older age (≥50 years adjusted odds ratio [aOR] 1.82; 95% CI 1.09–3.06;p = 0.023 and 44–49 years aOR 1.75; 95% CI 1.03–3.00;p = 0.038 vs. ≤37 years) and history of opioid substitution therapy (aOR 2.06; 95% CI 1.30–3.26; p = 0.002) were independently associated with treatment. This study confirms PWID are willing to initiate treatment when HCV DAA therapy is available and provides population-level evidence of a decline in viraemic prevalence among people most at risk of ongoing HCV transmission. Scaled up surveillance and monitoring are required to evaluate progress toward WHO HCV elimination goals. Australia implemented unrestricted subsidised access to DAA therapy in March 2016, ensuring that all adults with chronic HCV were eligible for DAA therapy irrespective of liver disease stage and ongoing drug use.Further, all medical practitioners can prescribe DAA therapy, although low caseload non-specialist prescribers are required to consult a specialist prior to prescribing.Over the initial 16-month period (March 2016 to June 2017), 43,360 people initiated DAA therapy in Australia, equivalent to 19% of the estimated 227,000 Australians living with chronic HCV in 2015.15There is evidence that initial DAA therapy targeted those with more advanced liver disease, with an estimated 70% of people living with HCV-related cirrhosis initiating DAA therapy in 2014–2016, including through pharmaceutical industry compassionate access prior to Government subsidy.15
We undertook a cross-sectional study of children/adolescents with and without non-alcoholic fatty liver disease (NAFLD) to compare the prevalence of prediabetes and diabetes, and to examine the role of abnormal glucose tolerance as a predictor of liver disease severity. We recruited a cohort of 599 Caucasian children/adolescents with biopsy-proven NAFLD, and 118 children/adolescents without NAFLD, who were selected to be similar for age, sex, body mass index and waist circumference to those with NAFLD. The diagnosis of prediabetes and diabetes was based on either hemoglobin A1c, fasting plasma glucose or 2 h post-load glucose concentrations. Children/adolescents with NAFLD had a significantly higher prevalence of abnormal glucose tolerance (prediabetes or diabetes) than those without NAFLD (20.6% vs. 11%, p = 0.02). In particular, 124 (20.6%) children/adolescents with NAFLD had abnormal glucose tolerance, with 19.8% (n = 119) satisfying the diagnostic criteria for prediabetes and 0.8% (n = 5) satisfying the criteria for diabetes. The combined presence of prediabetes and diabetes was associated with a nearly 2.2-fold increased risk of non-alcoholic steatohepatitis (NASH; unadjusted odds ratio 2.19; 95% CI 1.47–3.29; p <0.001). However, this association was attenuated (but remained significant) after adjustment for age, sex, waist circumference (adjusted odds ratio 1.69, 95% CI 1.06–2.69, p = 0.032), and the PNPLA3 rs738409 polymorphism. Both this PNPLA3 polymorphism and waist circumference were strongly associated with NASH. Abnormal glucose tolerance (especially prediabetes) is highly prevalent among children/adolescents with biopsy-proven NAFLD. These children also have a higher risk of NASH, though central adiposity is the factor that is most strongly associated with NASH. With the growing prevalence of childhood obesity, pediatric non-alcoholic fatty liver disease (NAFLD) has emerged as the most common chronic liver disease in children and adolescents in Western countries.1–3NAFLD is a spectrum of progressive liver disease that encompasses simple steatosis, non-alcoholic steatohepatitis (NASH), advanced fibrosis and, ultimately, cirrhosis.3,4Compelling evidence indicates that NAFLD also has serious health consequences outside of the liver and is strongly associated with an increased risk of cardiovascular disease and abnormal glucose tolerance (prediabetes and type 2 diabetes).5–7
We undertook a cross-sectional study of children/adolescents with and without non-alcoholic fatty liver disease (NAFLD) to compare the prevalence of prediabetes and diabetes, and to examine the role of abnormal glucose tolerance as a predictor of liver disease severity. We recruited a cohort of 599 Caucasian children/adolescents with biopsy-proven NAFLD, and 118 children/adolescents without NAFLD, who were selected to be similar for age, sex, body mass index and waist circumference to those with NAFLD. The diagnosis of prediabetes and diabetes was based on either hemoglobin A1c, fasting plasma glucose or 2 h post-load glucose concentrations. Children/adolescents with NAFLD had a significantly higher prevalence of abnormal glucose tolerance (prediabetes or diabetes) than those without NAFLD (20.6% vs. 11%, p = 0.02). In particular, 124 (20.6%) children/adolescents with NAFLD had abnormal glucose tolerance, with 19.8% (n = 119) satisfying the diagnostic criteria for prediabetes and 0.8% (n = 5) satisfying the criteria for diabetes. The combined presence of prediabetes and diabetes was associated with a nearly 2.2-fold increased risk of non-alcoholic steatohepatitis (NASH; unadjusted odds ratio 2.19; 95% CI 1.47–3.29; p <0.001). However, this association was attenuated (but remained significant) after adjustment for age, sex, waist circumference (adjusted odds ratio 1.69, 95% CI 1.06–2.69, p = 0.032), and the PNPLA3 rs738409 polymorphism. Both this PNPLA3 polymorphism and waist circumference were strongly associated with NASH. Abnormal glucose tolerance (especially prediabetes) is highly prevalent among children/adolescents with biopsy-proven NAFLD. These children also have a higher risk of NASH, though central adiposity is the factor that is most strongly associated with NASH. Prediabetes and type 2 diabetes are 2 common conditions in adults with NAFLD.3,6–8Furthermore, the presence of established diabetes among adults with NAFLD is an important risk factor for the development of advanced fibrosis and cirrhosis, and is also a significant predictor of liver-related mortality.3,6,8To date, the impact of prediabetes and diabetes in children and adolescents with NAFLD has been less well defined.Although insulin resistance occurs almost universally in children/adolescents with NAFLD,1,2 the prevalence of both prediabetes and diabetes in children/adolescents with biopsy-proven NAFLD is uncertain and, previously, the sample sizes of published studies have usually been too small to support a stable estimate of the prevalence of these 2 conditions in the pediatric NAFLD population.9–11It is also uncertain whether abnormal glucose tolerance is a risk factor for NASH in children and adolescents.Recently, in a multi-ethnic cohort study from the US, of 675 obese children/adolescents with biopsy-confirmed NAFLD enrolled in the NASH Clinical Research Network, the authors reported that the prevalence of prediabetes and type 2 diabetes was 23.4% and 6.5%, respectively.12Moreover, children/adolescents with prediabetes or diabetes also had a greater prevalence of NASH compared to their counterparts with normal glucose tolerance.12
Macrophages contribute to liver disease, but their role in cholestatic liver injury, including primary sclerosing cholangitis (PSC), is unclear. We tested the hypothesis that macrophages contribute to the pathogenesis of, and are therapeutic targets for, PSC. Immune cell profile, hepatic macrophage number, localization and polarization, fibrosis, and serum markers of liver injury and cholestasis were measured in an acute (intrabiliary injection of the inhibitor of apoptosis antagonist BV6) and chronic (Mdr2−/− mice) mouse model of sclerosing cholangitis (SC). Selected observations were confirmed in liver specimens from patients with PSC. Because of the known role of the CCR2/CCL2 axis in monocyte/macrophage chemotaxis, therapeutic effects of the CCR2/5 antagonist cenicriviroc (CVC), or genetic deletion of CCR2 (Ccr2−/− mice) were determined in BV6-injected mice. We found increased peribiliary pro-inflammatory (M1-like) and alternatively-activated (M2-like) monocyte-derived macrophages in PSC compared to normal livers. In both SC models, genetic profiling of liver immune cells identified a predominance of monocytes/macrophages; immunohistochemistry confirmed peribiliary monocyte-derived macrophage recruitment (M1>M2-polarized), which paralleled injury onset and was reversed upon resolution in acute SC mice. PSC, senescent and BV6-treated human cholangiocytes released monocyte chemoattractants (CCL2, IL-8) and macrophage-activating factors in vitro. Pharmacological inhibition of monocyte recruitment by CVC treatment or CCR2 genetic deletion attenuated macrophage accumulation, liver injury and fibrosis in acute SC. Peribiliary recruited macrophages are a feature of both PSC and acute and chronic murine SC models. Pharmacologic and genetic inhibition of peribiliary macrophage recruitment decreases liver injury and fibrosis in mouse SC. These observations suggest monocyte-derived macrophages contribute to the development of SC in mice and in PSC pathogenesis, and support their potential as a therapeutic target. Primary sclerosing cholangitis (PSC) is a progressive cholestatic liver disease, characterized by chronic inflammation of the biliary epithelium and fibrous obliterative cholangitis of the intra- and extra-hepatic bile ducts.Its etiology and pathogenic mechanisms are still largely unknown, which contributes to the lack of effective therapies aside from liver transplantation.Patients with PSC are at high risk for the development of cirrhosis and its sequela of portal hypertension, end-stage liver disease, and cholangiocarcinoma.1Animal models contribute to our knowledge of disease pathogenesis and malignant transformation of the biliary epithelia, and provide insight regarding new therapeutic strategies.However, given the multifactorial nature of PSC, a single animal model recapitulating all the aspects of the disease may be difficult to achieve.Therefore, the study of multiple models may represent the best approach to investigate the molecular mechanisms of disease pathogenesis.
Macrophages contribute to liver disease, but their role in cholestatic liver injury, including primary sclerosing cholangitis (PSC), is unclear. We tested the hypothesis that macrophages contribute to the pathogenesis of, and are therapeutic targets for, PSC. Immune cell profile, hepatic macrophage number, localization and polarization, fibrosis, and serum markers of liver injury and cholestasis were measured in an acute (intrabiliary injection of the inhibitor of apoptosis antagonist BV6) and chronic (Mdr2−/− mice) mouse model of sclerosing cholangitis (SC). Selected observations were confirmed in liver specimens from patients with PSC. Because of the known role of the CCR2/CCL2 axis in monocyte/macrophage chemotaxis, therapeutic effects of the CCR2/5 antagonist cenicriviroc (CVC), or genetic deletion of CCR2 (Ccr2−/− mice) were determined in BV6-injected mice. We found increased peribiliary pro-inflammatory (M1-like) and alternatively-activated (M2-like) monocyte-derived macrophages in PSC compared to normal livers. In both SC models, genetic profiling of liver immune cells identified a predominance of monocytes/macrophages; immunohistochemistry confirmed peribiliary monocyte-derived macrophage recruitment (M1>M2-polarized), which paralleled injury onset and was reversed upon resolution in acute SC mice. PSC, senescent and BV6-treated human cholangiocytes released monocyte chemoattractants (CCL2, IL-8) and macrophage-activating factors in vitro. Pharmacological inhibition of monocyte recruitment by CVC treatment or CCR2 genetic deletion attenuated macrophage accumulation, liver injury and fibrosis in acute SC. Peribiliary recruited macrophages are a feature of both PSC and acute and chronic murine SC models. Pharmacologic and genetic inhibition of peribiliary macrophage recruitment decreases liver injury and fibrosis in mouse SC. These observations suggest monocyte-derived macrophages contribute to the development of SC in mice and in PSC pathogenesis, and support their potential as a therapeutic target. Mice with targeted disruption of Mdr2 (Abcb4) (Mdr2−/−) develop chronic and progressive hepatic lesions closely resembling PSC.2,3Although no significant association has been found between MDR3 variants (the human orthologue of rodent Mdr2) and PSC susceptibility,4 the Mdr2−/− mouse has been extensively used as an appropriate animal model to study the pathogenesis and progression of sclerosing cholangitis and for the development of anti-fibrotic therapies.We have recently established an acute mouse model of sclerosing cholangitis that develops following the intrabiliary instillation of a single dose of the inhibitor of apoptosis antagonist BV6.5These mice display acute cholestatic injury resembling human sclerosing cholangitis, with histological features consistent with a fibrous cholangiopathy of the interlobular bile ducts, portal and periportal inflammation and fibrosis, elevated serum markers of cholestasis and cholangiographic evidence of intrahepatic biliary strictures and dilatations.These models complement each other for understanding biliary injury with periductular fibrosis.
Macrophages contribute to liver disease, but their role in cholestatic liver injury, including primary sclerosing cholangitis (PSC), is unclear. We tested the hypothesis that macrophages contribute to the pathogenesis of, and are therapeutic targets for, PSC. Immune cell profile, hepatic macrophage number, localization and polarization, fibrosis, and serum markers of liver injury and cholestasis were measured in an acute (intrabiliary injection of the inhibitor of apoptosis antagonist BV6) and chronic (Mdr2−/− mice) mouse model of sclerosing cholangitis (SC). Selected observations were confirmed in liver specimens from patients with PSC. Because of the known role of the CCR2/CCL2 axis in monocyte/macrophage chemotaxis, therapeutic effects of the CCR2/5 antagonist cenicriviroc (CVC), or genetic deletion of CCR2 (Ccr2−/− mice) were determined in BV6-injected mice. We found increased peribiliary pro-inflammatory (M1-like) and alternatively-activated (M2-like) monocyte-derived macrophages in PSC compared to normal livers. In both SC models, genetic profiling of liver immune cells identified a predominance of monocytes/macrophages; immunohistochemistry confirmed peribiliary monocyte-derived macrophage recruitment (M1>M2-polarized), which paralleled injury onset and was reversed upon resolution in acute SC mice. PSC, senescent and BV6-treated human cholangiocytes released monocyte chemoattractants (CCL2, IL-8) and macrophage-activating factors in vitro. Pharmacological inhibition of monocyte recruitment by CVC treatment or CCR2 genetic deletion attenuated macrophage accumulation, liver injury and fibrosis in acute SC. Peribiliary recruited macrophages are a feature of both PSC and acute and chronic murine SC models. Pharmacologic and genetic inhibition of peribiliary macrophage recruitment decreases liver injury and fibrosis in mouse SC. These observations suggest monocyte-derived macrophages contribute to the development of SC in mice and in PSC pathogenesis, and support their potential as a therapeutic target. Macrophages are key regulators of the inflammatory responses.Macrophages are ontologically and functionally diverse.Tissue resident macrophages, such as hepatic Kupffer cells (KC), derive from the yolk sac, whereas recruited macrophages originate from bone marrow-derived monocytes.Their functional diversity relates to their ability to polarize into different functional phenotypes which regulate both the initial inflammatory response and the late healing response.Although several subsets of macrophage phenotypes have been proposed (i.e., M1-like or pro-inflammatory macrophages; and M2-like or restorative, reparative macrophages), it has not been established whether they are phenotypically distinct populations or, more likely, they derive from the same population with a continuum of phenotypes between the two subtypes regulated by evolving cues in the tissue microenvironment.6Macrophages are among the first immune cells responding to liver injury and can regulate the fibrogenic response via multiple mechanisms, including phagocytosis of dead cells and secretion of cytokines, chemokines and growth factors, production of matrix metalloproteases (MMPs) and tissue inhibitors of MMP (TIMPs).7Following liver injury, pro-inflammatory monocyte-derived macrophages are recruited to the injury site in response to the release of chemokines, in particular CCL2 (also called monocyte chemoattractant protein 1 [MCP-1]), produced by fibroblasts, resident macrophages, activated cholangiocytes and endothelial cells as part of the inflammatory response.The initial pro-inflammatory response is followed by a resolution phase carried out by macrophages that facilitate tissue healing and remodeling by breaking down extracellular matrix and promoting cell proliferation and hepatic stellate cell (HSC) apoptosis.7
Hepatocellular carcinoma (HCC) is a common cancer worldwide and remains a major clinical challenge. Ketoconazole, a traditional antifungal agent, has attracted considerable attention as a therapeutic option for cancer treatment. However, its mechanism of action is still not clearly defined. We aimed to evaluate the effect of ketoconazole on HCC and investigate the underlying mechanisms. We examined the antitumor effect of ketoconazole on HCC cells, cell line-derived xenografts, and a patient-derived xenograft (PDX) model. Ketoconazole-induced mitophagy was quantified by immunofluorescence, immunoblotting and transmission electron microscopy analysis. We used mitophagy inhibitors to study the role of mitophagy on HCC cell death induced by ketoconazole. The role of cyclooxygenase-2 (COX-2 [encoded by PTGS2]) on ketoconazole-induced mitophagy was evaluated using gain- and loss-of-function methods. The synergistic effect of ketoconazole with sorafenib on HCC was measured in vivo and in vitro. Ketoconazole stimulated apoptosis in HCC cells by triggering mitophagy in vitro and in vivo. Mechanistically, ketoconazole downregulated COX-2, which led to PINK1 accumulation and subsequent mitochondrial translocation of Parkin (PRKN), and thereby promoted mitophagy-mediated mitochondrial dysfunction. Inhibiting mitophagy alleviated ketoconazole-induced mitochondrial dysfunction and apoptosis, supporting a causal role for mitophagy in the antitumor effect of ketoconazole. In the HCC PDX model, ketoconazole demonstrated a marked antitumor effect characterized by COX-2 downregulation, mitophagy activation, and apoptosis induction. Moreover, ketoconazole acted synergistically with sorafenib to suppress HCC xenograft growth in vivo. Our results demonstrate a novel link between ketoconazole and mitophagy machinery, providing preclinical proof of concept for the use of ketoconazole in HCC treatment. Hepatocellular carcinoma (HCC) is the second leading cause of cancer-related death worldwide.1Among all the treatment options for HCC, surgical resection remains the main approach, with the best long-term survival for early-onset disease.2However, the majority of patients with HCC are diagnosed at late stages when curative strategies are not applicable.Thus, the options become extremely limited for those patients who are referred to systemic therapy because of the low response rates of chemotherapeutic agents in HCC treatment.3,4Despite the current use of an oral multikinase inhibitor, sorafenib, in the standard care for advanced HCC, multiple clinical trials on sorafenib revealed limited survival benefits in patients with HCC, which is normally attributed to primary and acquired drug resistance.5As such, new therapeutic agents are urgently needed to improve outcomes in patients with HCC.
Hepatocellular carcinoma (HCC) is a common cancer worldwide and remains a major clinical challenge. Ketoconazole, a traditional antifungal agent, has attracted considerable attention as a therapeutic option for cancer treatment. However, its mechanism of action is still not clearly defined. We aimed to evaluate the effect of ketoconazole on HCC and investigate the underlying mechanisms. We examined the antitumor effect of ketoconazole on HCC cells, cell line-derived xenografts, and a patient-derived xenograft (PDX) model. Ketoconazole-induced mitophagy was quantified by immunofluorescence, immunoblotting and transmission electron microscopy analysis. We used mitophagy inhibitors to study the role of mitophagy on HCC cell death induced by ketoconazole. The role of cyclooxygenase-2 (COX-2 [encoded by PTGS2]) on ketoconazole-induced mitophagy was evaluated using gain- and loss-of-function methods. The synergistic effect of ketoconazole with sorafenib on HCC was measured in vivo and in vitro. Ketoconazole stimulated apoptosis in HCC cells by triggering mitophagy in vitro and in vivo. Mechanistically, ketoconazole downregulated COX-2, which led to PINK1 accumulation and subsequent mitochondrial translocation of Parkin (PRKN), and thereby promoted mitophagy-mediated mitochondrial dysfunction. Inhibiting mitophagy alleviated ketoconazole-induced mitochondrial dysfunction and apoptosis, supporting a causal role for mitophagy in the antitumor effect of ketoconazole. In the HCC PDX model, ketoconazole demonstrated a marked antitumor effect characterized by COX-2 downregulation, mitophagy activation, and apoptosis induction. Moreover, ketoconazole acted synergistically with sorafenib to suppress HCC xenograft growth in vivo. Our results demonstrate a novel link between ketoconazole and mitophagy machinery, providing preclinical proof of concept for the use of ketoconazole in HCC treatment. Autophagy is a catabolic process by which intracellular cargo components, such as excess or defective proteins and organelles, are engulfed in double-membrane autophagosomes and subsequently degraded in autolysosomes.6The importance of autophagy in cancer treatment is highlighted by recent findings of the association of autophagy with tumor progression and drug response.7,8In some forms of autophagy, superfluous or damaged organelles and protein aggregates may be removed in a cargo-specific manner.Mitophagy, a well-studied type of cargo-specific autophagy, has commonly been recognized as a form of basal quality control to remove damaged mitochondria.9However, there has been evidence showing that in certain context, mitophagy is preferentially engaged to decrease the mass of functional mitochondria, contributing to cell demise.10,11In the scenario of HCC treatment, it has been perceived that mitophagy may play a double-faceted role in tumor cell survival depending on different cellular context.12,13Therefore, dissection of the mechanism underlying the dual role of mitophagy is crucial for exploiting mitophagy as a therapeutic target in HCC treatment.
Hepatocellular carcinoma (HCC) is a common cancer worldwide and remains a major clinical challenge. Ketoconazole, a traditional antifungal agent, has attracted considerable attention as a therapeutic option for cancer treatment. However, its mechanism of action is still not clearly defined. We aimed to evaluate the effect of ketoconazole on HCC and investigate the underlying mechanisms. We examined the antitumor effect of ketoconazole on HCC cells, cell line-derived xenografts, and a patient-derived xenograft (PDX) model. Ketoconazole-induced mitophagy was quantified by immunofluorescence, immunoblotting and transmission electron microscopy analysis. We used mitophagy inhibitors to study the role of mitophagy on HCC cell death induced by ketoconazole. The role of cyclooxygenase-2 (COX-2 [encoded by PTGS2]) on ketoconazole-induced mitophagy was evaluated using gain- and loss-of-function methods. The synergistic effect of ketoconazole with sorafenib on HCC was measured in vivo and in vitro. Ketoconazole stimulated apoptosis in HCC cells by triggering mitophagy in vitro and in vivo. Mechanistically, ketoconazole downregulated COX-2, which led to PINK1 accumulation and subsequent mitochondrial translocation of Parkin (PRKN), and thereby promoted mitophagy-mediated mitochondrial dysfunction. Inhibiting mitophagy alleviated ketoconazole-induced mitochondrial dysfunction and apoptosis, supporting a causal role for mitophagy in the antitumor effect of ketoconazole. In the HCC PDX model, ketoconazole demonstrated a marked antitumor effect characterized by COX-2 downregulation, mitophagy activation, and apoptosis induction. Moreover, ketoconazole acted synergistically with sorafenib to suppress HCC xenograft growth in vivo. Our results demonstrate a novel link between ketoconazole and mitophagy machinery, providing preclinical proof of concept for the use of ketoconazole in HCC treatment. Ketoconazole (KET) is a broad-spectrum antifungal agent derived from imidazole and is primarily used to treat fungal infections such as candidiasis.14,15The systemic use of oral ketoconazole for fungal infections has been discontinued due to potential risk of liver injury.16Nowadays growing evidence suggests that ketoconazole alone or in combination with other agents possesses considerable antitumor properties.Initially shown for prostate cancer, these findings have been extended to other cancers, including breast, colon, and bladder cancers.17–19Moreover, the oral use of ketoconazole has been proven to be fairly well tolerated in cancer patients with no obviously increased toxicity in several clinical studies.20–22However, to date, limited data exist regarding the efficacy of ketoconazole in the treatment of HCC.These observations, in conjunction with the reported potential hepatotoxicity, underscore the need to evaluate the rational utility of ketoconazole in HCC treatment and to investigate the mechanisms involved.
Shp2 is an SH2-tyrosine phosphatase acting downstream of receptor tyrosine kinases (RTKs). Most recent data demonstrated a liver tumor-suppressing role for Shp2, as ablating Shp2 in hepatocytes aggravated hepatocellular carcinoma (HCC) induced by chemical carcinogens or Pten loss. We further investigated the effect of Shp2 deficiency on liver tumorigenesis driven by classical oncoproteins c-Met (receptor for HGF), β-catenin and PIK3CA. We performed hydrodynamic tail vein injection of two pairs of plasmids expressing c-Met and ΔN90-β-catenin (MET/CAT), or c-Met and PIK3CAH1047R (MET/PIK), into WT and Shp2hep−/− mice. We compared liver tumor loads and investigated the pathogenesis and molecular mechanisms involved using multidisciplinary approaches. Despite the induction of oxidative and metabolic stresses, Shp2 deletion in hepatocytes suppressed hepatocarcinogenesis driven by overexpression of oncoproteins MET/CAT or MET/PIK. Shp2 loss inhibited proliferative signaling from c-Met, Wnt/β-catenin, Ras/Erk and PI3K/Akt pathways, but triggered cell senescence following exogenous expression of the oncogenes. Shp2, acting downstream of RTKs, is positively required for hepatocyte-intrinsic tumorigenic signaling from these oncoproteins, even if Shp2 deficiency induces a tumor-promoting hepatic microenvironment. These data suggest a new and more effective therapeutic strategy for HCCs driven by oncogenic RTKs and other upstream molecules, by inhibiting Shp2 and also suppressing any tumor-enhancing stromal factors produced because of Shp2 inhibition. Primary liver cancer, mainly hepatocellular carcinoma (HCC), is a highly malignant disease.While the overall cancer mortality and incidences are decreasing, liver cancer incidences are increasing rapidly in the United States.1The lack of effective therapeutic drugs is evidently caused by poor understanding of the complicated mechanisms of hepatocarcinogenesis.
Shp2 is an SH2-tyrosine phosphatase acting downstream of receptor tyrosine kinases (RTKs). Most recent data demonstrated a liver tumor-suppressing role for Shp2, as ablating Shp2 in hepatocytes aggravated hepatocellular carcinoma (HCC) induced by chemical carcinogens or Pten loss. We further investigated the effect of Shp2 deficiency on liver tumorigenesis driven by classical oncoproteins c-Met (receptor for HGF), β-catenin and PIK3CA. We performed hydrodynamic tail vein injection of two pairs of plasmids expressing c-Met and ΔN90-β-catenin (MET/CAT), or c-Met and PIK3CAH1047R (MET/PIK), into WT and Shp2hep−/− mice. We compared liver tumor loads and investigated the pathogenesis and molecular mechanisms involved using multidisciplinary approaches. Despite the induction of oxidative and metabolic stresses, Shp2 deletion in hepatocytes suppressed hepatocarcinogenesis driven by overexpression of oncoproteins MET/CAT or MET/PIK. Shp2 loss inhibited proliferative signaling from c-Met, Wnt/β-catenin, Ras/Erk and PI3K/Akt pathways, but triggered cell senescence following exogenous expression of the oncogenes. Shp2, acting downstream of RTKs, is positively required for hepatocyte-intrinsic tumorigenic signaling from these oncoproteins, even if Shp2 deficiency induces a tumor-promoting hepatic microenvironment. These data suggest a new and more effective therapeutic strategy for HCCs driven by oncogenic RTKs and other upstream molecules, by inhibiting Shp2 and also suppressing any tumor-enhancing stromal factors produced because of Shp2 inhibition. Ptpn11 encodes a cytoplasmic tyrosine phosphatase Shp2 that can dock directly on ligand-activated receptor tyrosine kinases (RTKs) through its two SH2 domains,2 which immediately implicated a putative role of Shp2 in dephosphorylation and inactivation of RTKs.However, genetic and biochemical data have disclosed a positive effect of this phosphatase in augmenting cytoplasmic signaling through the Erk pathway, proximal to RTKs.2,3Consistently, Ptpn11/Shp2 has been identified as the first proto-oncogene that encodes a tyrosine phosphatase, with dominant active mutations detected in leukemia patients, with or without Noonan Syndrome.4,5In contrast, our most recent experiments demonstrated a tumor-inhibitory role for Shp2 in liver cancer, because ablating Shp2 in hepatocytes triggered hepatocellular adenoma (HCA) in aged mice and also enhanced HCC development induced by diethylnitrosamine (DEN).6Simultaneous deletion of Shp2 and Pten in hepatocytes dramatically accelerated and enhanced non-alcoholic steatohepatitis (NASH) and liver tumorigenesis,7 indicating concerted tumor-inhibitory activities of Shp2 and Pten in guarding hepatic homeostasis and functions.Consistent with the animal data, concomitant Pten and Shp2 deficiencies were detected in HCC patients with poor prognosis.7These results indicate an anti-oncogenic role of Shp2 in the liver, in contrast to its pro-leukemogenic effect in the hematopoietic system.Similarly, several other groups reported that deleting some classical oncoproteins, such as c-MET, EGFR, β-catenin, Ikkβ and Jnk1/2 and Akt1/2, ironically exacerbated DEN-induced HCC development, although the underlying mechanisms are not fully understood.8–10
Understanding the real-world effectiveness of all-oral hepatitis C virus (HCV) regimens informs treatment decisions. We evaluated the effectiveness of daclatasvir + sofosbuvir ± ribavirin (DCV + SOF ± RBV) and velpatasvir/sofosbuvir (VEL/SOF) ± RBV in patients with genotype 2 and genotype 3 infection treated in routine practice. This observational analysis was carried out in an intent-to-treat cohort of patients with HCV genotype 2 and genotype 3. Sustained virologic response (SVR) analysis was performed in 5,400 patients initiated on DCV + SOF ± RBV or VEL/SOF ± RBV at any Department of Veterans Affairs facility. For genotype 2, SVR rates did not differ between DCV + SOF (94.5%) and VEL/SOF (94.4%) or between DCV + SOF + RBV (88.1%) and VEL/SOF + RBV (89.5%). For genotype 3, SVR rates did not differ between DCV + SOF (90.8%) and VEL/SOF (92.0%) or between DCV + SOF + RBV (88.1%) and VEL/SOF + RBV (86.4%). In multivariate models of patients with genotype 2 and 3 infection, the treatment regimen was not a significant predictor of the odds of SVR. For genotype 3, significant predictors of reduced odds of SVR were prior HCV treatment-experience (odds ratio [OR] 0.51, 95% CI 0.36–0.72; p <0.001), FIB-4 >3.25 (OR 0.60; 95% CI 0.43–0.84; p = 0.002) and a history of decompensated liver disease (OR 0.68; 95% CI 0.47–0.98; p = 0.04). For patients with genotype 2 and 3, treated with VEL/SOF ± RBV, 89% and 85% received 12-weeks of treatment, respectively. For DCV + SOF ± RBV, 56% and 20% of patients with HCV genotype 2 received 12-weeks and 24-weeks of treatment, respectively; while 53% and 23% of patients with HCV genotype 3 received 12-weeks and 24-weeks, with most direct-acting antiviral experienced patients receiving 24-weeks. In patients infected with HCV genotype 2 and 3, DCV + SOF ± RBV and VEL/SOF ± RBV produced similar SVR rates within each genotype, and the regimen did not have a significant impact on the odds of SVR. For patients with genotype 3, prior treatment-experience and advanced liver disease were significant predictors of reduced odds of SVR regardless of regimen. Approximately 71.1 million people are estimated to be chronically infected with hepatitis C virus (HCV) worldwide, with 2.9 million infected in the United States.1Genotype 2 accounts for 11% of chronic HCV infections both worldwide and in the United States, while genotype 3 accounts for 18% of HCV infections worldwide and 9% in the United States.1,2
Understanding the real-world effectiveness of all-oral hepatitis C virus (HCV) regimens informs treatment decisions. We evaluated the effectiveness of daclatasvir + sofosbuvir ± ribavirin (DCV + SOF ± RBV) and velpatasvir/sofosbuvir (VEL/SOF) ± RBV in patients with genotype 2 and genotype 3 infection treated in routine practice. This observational analysis was carried out in an intent-to-treat cohort of patients with HCV genotype 2 and genotype 3. Sustained virologic response (SVR) analysis was performed in 5,400 patients initiated on DCV + SOF ± RBV or VEL/SOF ± RBV at any Department of Veterans Affairs facility. For genotype 2, SVR rates did not differ between DCV + SOF (94.5%) and VEL/SOF (94.4%) or between DCV + SOF + RBV (88.1%) and VEL/SOF + RBV (89.5%). For genotype 3, SVR rates did not differ between DCV + SOF (90.8%) and VEL/SOF (92.0%) or between DCV + SOF + RBV (88.1%) and VEL/SOF + RBV (86.4%). In multivariate models of patients with genotype 2 and 3 infection, the treatment regimen was not a significant predictor of the odds of SVR. For genotype 3, significant predictors of reduced odds of SVR were prior HCV treatment-experience (odds ratio [OR] 0.51, 95% CI 0.36–0.72; p <0.001), FIB-4 >3.25 (OR 0.60; 95% CI 0.43–0.84; p = 0.002) and a history of decompensated liver disease (OR 0.68; 95% CI 0.47–0.98; p = 0.04). For patients with genotype 2 and 3, treated with VEL/SOF ± RBV, 89% and 85% received 12-weeks of treatment, respectively. For DCV + SOF ± RBV, 56% and 20% of patients with HCV genotype 2 received 12-weeks and 24-weeks of treatment, respectively; while 53% and 23% of patients with HCV genotype 3 received 12-weeks and 24-weeks, with most direct-acting antiviral experienced patients receiving 24-weeks. In patients infected with HCV genotype 2 and 3, DCV + SOF ± RBV and VEL/SOF ± RBV produced similar SVR rates within each genotype, and the regimen did not have a significant impact on the odds of SVR. For patients with genotype 3, prior treatment-experience and advanced liver disease were significant predictors of reduced odds of SVR regardless of regimen. Daclatasvir (DCV), an NS5A inhibitor, has been evaluated in combination with sofosbuvir (SOF), an NS5B polymerase inhibitor, in phase III and early access clinical trials in patients with HCV genotype 2 and genotype 3.Sustained virologic response (SVR) rates of 92%–100% were obtained with 12 or 24-week regimens in treatment-naïve and peginterferon/ribavirin (RBV)-experienced patients with HCV genotype 2.3In genotype 3, SVR rates varied based on patient characteristics, duration, and RBV use, ranging from 86% to 97% with 12 weeks of DCV + SOF ± RBV in treatment-naïve and peginterferon, protease inhibitor, or SOF-experienced patients without cirrhosis and from 70% to 89% with 12 or 24 weeks in patients with cirrhosis.4–7The co-formulated NS5A and NS5B inhibitor velpatasvir (VEL)/SOF was approved approximately one year after DCV for genotype 2 and genotype 3.In phase III clinical trials of patients with HCV genotype 2, SVR was achieved in 99%–100% of treatment-naïve and interferon-experienced patients with and without cirrhosis.8–10In a mix of treatment-naïve and interferon-experienced patients with genotype 3, with and without cirrhosis, 12 weeks of VEL/SOF resulted in SVRs of 89% to 97%, and VEL/SOF + RBV resulted in an SVR rate of 85% in patients with decompensated cirrhosis.9,11
Understanding the real-world effectiveness of all-oral hepatitis C virus (HCV) regimens informs treatment decisions. We evaluated the effectiveness of daclatasvir + sofosbuvir ± ribavirin (DCV + SOF ± RBV) and velpatasvir/sofosbuvir (VEL/SOF) ± RBV in patients with genotype 2 and genotype 3 infection treated in routine practice. This observational analysis was carried out in an intent-to-treat cohort of patients with HCV genotype 2 and genotype 3. Sustained virologic response (SVR) analysis was performed in 5,400 patients initiated on DCV + SOF ± RBV or VEL/SOF ± RBV at any Department of Veterans Affairs facility. For genotype 2, SVR rates did not differ between DCV + SOF (94.5%) and VEL/SOF (94.4%) or between DCV + SOF + RBV (88.1%) and VEL/SOF + RBV (89.5%). For genotype 3, SVR rates did not differ between DCV + SOF (90.8%) and VEL/SOF (92.0%) or between DCV + SOF + RBV (88.1%) and VEL/SOF + RBV (86.4%). In multivariate models of patients with genotype 2 and 3 infection, the treatment regimen was not a significant predictor of the odds of SVR. For genotype 3, significant predictors of reduced odds of SVR were prior HCV treatment-experience (odds ratio [OR] 0.51, 95% CI 0.36–0.72; p <0.001), FIB-4 >3.25 (OR 0.60; 95% CI 0.43–0.84; p = 0.002) and a history of decompensated liver disease (OR 0.68; 95% CI 0.47–0.98; p = 0.04). For patients with genotype 2 and 3, treated with VEL/SOF ± RBV, 89% and 85% received 12-weeks of treatment, respectively. For DCV + SOF ± RBV, 56% and 20% of patients with HCV genotype 2 received 12-weeks and 24-weeks of treatment, respectively; while 53% and 23% of patients with HCV genotype 3 received 12-weeks and 24-weeks, with most direct-acting antiviral experienced patients receiving 24-weeks. In patients infected with HCV genotype 2 and 3, DCV + SOF ± RBV and VEL/SOF ± RBV produced similar SVR rates within each genotype, and the regimen did not have a significant impact on the odds of SVR. For patients with genotype 3, prior treatment-experience and advanced liver disease were significant predictors of reduced odds of SVR regardless of regimen. Several expert-developed recommendations for HCV management exist, including those from the American Association for the Study of Liver Diseases (AASLD) and the Infectious Disease Society of America (IDSA), the European Association for the Study of the Liver (EASL), and the Department of Veterans Affairs (VA).12–14Given rapid advances in HCV treatment, these HCV guidelines have changed as new data have emerged.12,15In real-world practice, treatment does not always follow guidelines and providers make individualized decisions to use regimens where evidence is evolving.
Affordable point-of-care tests for hepatitis C (HCV) viraemia are needed to improve access to treatment in low- and middle-income countries. Our aims were to determine the target limit of detection (LOD) necessary to diagnose the majority of people with HCV eligible for treatment, and identify characteristics associated with low-level viraemia (LLV) (defined as the lowest 3% of the distribution of HCV RNA) to understand those at risk of being misdiagnosed. We established a multi-country cross-sectional dataset of first available quantitative HCV RNA measurements linked to demographic and clinical data. We excluded individuals on HCV treatment. We analysed the distribution of HCV RNA and determined critical thresholds for detection of HCV viraemia. We then performed logistic regression to evaluate factors associated with LLV, and derived relative sensitivities for significant covariates. The dataset included 66,640 individuals with HCV viraemia from across the world. The LOD for the 95th and 99th percentiles were 3,311 IU/ml and 214 IU/ml. The LOD for the 97th percentile was 1,318 IU/ml (95% CI 1,298.4–1,322.3). Factors associated with LLV, defined as HCV RNA <1,318 IU/ml, were younger age 18–30 vs. 51–64 years (odds ratios [OR] 2.56; 95% CI 2.19–2.99), female vs. male sex (OR 1.32; 95% CI 1.18–1.49), and advanced fibrosis stage F4 vs. F0-1 (OR 1.44; 95% CI 1.21–1.69). Only the younger age group had a decreased relative sensitivity below 95%, at 93.3%. In this global dataset, a test with an LOD of 1,318 IU/ml would identify 97% of viraemic HCV infections among almost all populations. This LOD will help guide manufacturers in the development of affordable point-of-care diagnostics to expand HCV testing and linkage to care in low- and middle-income countries. Globally, viral hepatitis is responsible for 1.34 million deaths1,2 and more than 50 million of the estimated 70 million cases of chronic hepatitis C virus (HCV) occur in low and middle-income countries (LMICs).3The World Health Organization (WHO) defined goals towards the elimination of viral hepatitis as a public health threat, with a 90% reduction in new infections, and a 65% reduction in mortality by 2030.1,4Achievement of these targets requires scale-up of access to affordable testing and treatment alongside interventions for HCV prevention (harm reduction and safe blood donation and injections).5Progress in treatment scale-up is encouraging with more than 3 million treated with direct-acting antivirals since 2015, however, testing coverage and diagnosis rates are still less than 10% in LMICs.6
Affordable point-of-care tests for hepatitis C (HCV) viraemia are needed to improve access to treatment in low- and middle-income countries. Our aims were to determine the target limit of detection (LOD) necessary to diagnose the majority of people with HCV eligible for treatment, and identify characteristics associated with low-level viraemia (LLV) (defined as the lowest 3% of the distribution of HCV RNA) to understand those at risk of being misdiagnosed. We established a multi-country cross-sectional dataset of first available quantitative HCV RNA measurements linked to demographic and clinical data. We excluded individuals on HCV treatment. We analysed the distribution of HCV RNA and determined critical thresholds for detection of HCV viraemia. We then performed logistic regression to evaluate factors associated with LLV, and derived relative sensitivities for significant covariates. The dataset included 66,640 individuals with HCV viraemia from across the world. The LOD for the 95th and 99th percentiles were 3,311 IU/ml and 214 IU/ml. The LOD for the 97th percentile was 1,318 IU/ml (95% CI 1,298.4–1,322.3). Factors associated with LLV, defined as HCV RNA <1,318 IU/ml, were younger age 18–30 vs. 51–64 years (odds ratios [OR] 2.56; 95% CI 2.19–2.99), female vs. male sex (OR 1.32; 95% CI 1.18–1.49), and advanced fibrosis stage F4 vs. F0-1 (OR 1.44; 95% CI 1.21–1.69). Only the younger age group had a decreased relative sensitivity below 95%, at 93.3%. In this global dataset, a test with an LOD of 1,318 IU/ml would identify 97% of viraemic HCV infections among almost all populations. This LOD will help guide manufacturers in the development of affordable point-of-care diagnostics to expand HCV testing and linkage to care in low- and middle-income countries. Approximately 15–45% of people infected with HCV will spontaneously clear the virus7,8 and therefore confirmation of HCV viraemia is necessary to identify those needing treatment.The standard diagnostic algorithm recommended by the WHO includes an initial HCV antibody test followed by confirmatory testing for viraemia with either a nucleic acid test (NAT) for HCV RNA or core antigen (HCVcAg) where RNA tests are not available.9–11High proportions of those with positive antibody fail to have confirmatory testing and are never linked to treatment.12,13Further, available tests for viraemia are expensive, and require advanced laboratory facilities, electricity, water, and refrigerated reagents.Few LMICs have testing policies or the requisite laboratory infrastructure in place13–15
Affordable point-of-care tests for hepatitis C (HCV) viraemia are needed to improve access to treatment in low- and middle-income countries. Our aims were to determine the target limit of detection (LOD) necessary to diagnose the majority of people with HCV eligible for treatment, and identify characteristics associated with low-level viraemia (LLV) (defined as the lowest 3% of the distribution of HCV RNA) to understand those at risk of being misdiagnosed. We established a multi-country cross-sectional dataset of first available quantitative HCV RNA measurements linked to demographic and clinical data. We excluded individuals on HCV treatment. We analysed the distribution of HCV RNA and determined critical thresholds for detection of HCV viraemia. We then performed logistic regression to evaluate factors associated with LLV, and derived relative sensitivities for significant covariates. The dataset included 66,640 individuals with HCV viraemia from across the world. The LOD for the 95th and 99th percentiles were 3,311 IU/ml and 214 IU/ml. The LOD for the 97th percentile was 1,318 IU/ml (95% CI 1,298.4–1,322.3). Factors associated with LLV, defined as HCV RNA <1,318 IU/ml, were younger age 18–30 vs. 51–64 years (odds ratios [OR] 2.56; 95% CI 2.19–2.99), female vs. male sex (OR 1.32; 95% CI 1.18–1.49), and advanced fibrosis stage F4 vs. F0-1 (OR 1.44; 95% CI 1.21–1.69). Only the younger age group had a decreased relative sensitivity below 95%, at 93.3%. In this global dataset, a test with an LOD of 1,318 IU/ml would identify 97% of viraemic HCV infections among almost all populations. This LOD will help guide manufacturers in the development of affordable point-of-care diagnostics to expand HCV testing and linkage to care in low- and middle-income countries. Innovations in testing technology, and research to inform optimal implementation strategies for HCV in LMICs are needed.9,16,15A rapid, affordable, easy-to-use test for confirmation of HCV viraemia at the point-of-care (POC) that can be deployed on a large scale has the potential to improve outcomes across the diagnosis and care continuum, particularly in high HCV prevalence settings.16–18
Affordable point-of-care tests for hepatitis C (HCV) viraemia are needed to improve access to treatment in low- and middle-income countries. Our aims were to determine the target limit of detection (LOD) necessary to diagnose the majority of people with HCV eligible for treatment, and identify characteristics associated with low-level viraemia (LLV) (defined as the lowest 3% of the distribution of HCV RNA) to understand those at risk of being misdiagnosed. We established a multi-country cross-sectional dataset of first available quantitative HCV RNA measurements linked to demographic and clinical data. We excluded individuals on HCV treatment. We analysed the distribution of HCV RNA and determined critical thresholds for detection of HCV viraemia. We then performed logistic regression to evaluate factors associated with LLV, and derived relative sensitivities for significant covariates. The dataset included 66,640 individuals with HCV viraemia from across the world. The LOD for the 95th and 99th percentiles were 3,311 IU/ml and 214 IU/ml. The LOD for the 97th percentile was 1,318 IU/ml (95% CI 1,298.4–1,322.3). Factors associated with LLV, defined as HCV RNA <1,318 IU/ml, were younger age 18–30 vs. 51–64 years (odds ratios [OR] 2.56; 95% CI 2.19–2.99), female vs. male sex (OR 1.32; 95% CI 1.18–1.49), and advanced fibrosis stage F4 vs. F0-1 (OR 1.44; 95% CI 1.21–1.69). Only the younger age group had a decreased relative sensitivity below 95%, at 93.3%. In this global dataset, a test with an LOD of 1,318 IU/ml would identify 97% of viraemic HCV infections among almost all populations. This LOD will help guide manufacturers in the development of affordable point-of-care diagnostics to expand HCV testing and linkage to care in low- and middle-income countries. Presently, there are no data to determine a limit of detection (LOD) for WHO prequalification criteria for a POC HCV viraemia test.Thus, POC tests are held to the same standards as laboratory-based NATs.The laboratory-based Abbott RealTime HCV viral load test, for example, is able to detect and measure HCV RNA down to 12 international units per milliliter (IU/ml) with >99% sensitivity; similarly, the Roche COBAS®TaqMan® HCV Test reports an LOD of 15 IU/ml.19The laboratory-based Abbott ARCHITECT HCVcAg test has an LOD corresponding to 3,000 IU/ml with 93.4% sensitivity.20Requiring POC assays to achieve the same prequalification criteria as laboratory assays may limit the ability to expand HCV testing and treatment in LMICs.The POC Genedrive® HCV assay, however, acquired European in vitro diagnostics approval this year with an LOD of 2,362 IU/ml,21 but is not yet WHO prequalified.Additionally, Cepheid Xpert® HCV Viral Load finger-stick assay detects as little as 40 IU/ml and can utilise consolidated near-patient Xpert platforms or the POC Omni version.22
Excessive alcohol consumption is one of the major causes of hepatocellular carcinoma (HCC). Approximately 30–40% of the Asian population are deficient for aldehyde dehydrogenase 2 (ALDH2), a key enzyme that detoxifies the ethanol metabolite acetaldehyde. However, how ALDH2 deficiency affects alcohol-related HCC remains unclear. ALDH2 polymorphisms were studied in 646 patients with viral hepatitis B (HBV) infection, who did or did not drink alcohol. A new model of HCC induced by chronic carbon tetrachloride (CCl4) and alcohol administration was developed and studied in 3 lines of Aldh2-deficient mice: including Aldh2 global knockout (KO) mice, Aldh2*1/*2 knock-in mutant mice, and liver-specific Aldh2 KO mice. We demonstrated that ALDH2 deficiency was not associated with liver disease progression but was associated with an increased risk of HCC development in cirrhotic patients with HBV who consumed excessive alcohol. The mechanisms underlying HCC development associated with cirrhosis and alcohol consumption were studied in Aldh2-deficient mice. We found that all 3 lines of Aldh2-deficient mice were more susceptible to CCl4 plus alcohol-associated liver fibrosis and HCC development. Furthermore, our results from in vivo and in vitro mechanistic studies revealed that after CCl4 plus ethanol exposure, Aldh2-deficient hepatocytes produced a large amount of harmful oxidized mitochondrial DNA via extracellular vesicles, which were then transferred into neighboring HCC cells and together with acetaldehyde activated multiple oncogenic pathways (JNK, STAT3, BCL-2, and TAZ), thereby promoting HCC. ALDH2 deficiency is associated with an increased risk of alcohol-related HCC development from fibrosis in patients and in mice. Mechanistic studies reveal a novel mechanism that Aldh2-deficient hepatocytes promote alcohol-associated HCC by transferring harmful oxidized mitochondrial DNA-enriched extracellular vesicles into HCC and subsequently activating multiple oncogenic pathways in HCC. Hepatocellular carcinoma (HCC) is a leading cause of cancer-related death worldwide.Chronic alcohol abuse is a major cause of HCC development; its metabolite, acetaldehyde is believed to play an important role in inducing HCC.1Mitochondrial aldehyde dehydrogenase (ALDH2) is a major enzyme for acetaldehyde elimination.The Glu487Lys polymorphism (also named rs671, with the glutamate corresponding to *1 allele, and lysine corresponding to *2 allele) at codon 487 in the ALDH2 gene causes the substitution of glutamate (Glu) by lysine (Lys), which exists in approximately 40% of east Asian populations.2Such a polymorphism (Glu to Lys, or G to A, or *1 to *2) disrupts ALDH2 activity, causing high blood acetaldehyde concentration and “alcohol flush reactions” after alcohol consumption.1Alcoholics with heterozygous ALDH2*1/*2 or homozygous ALDH2*2/*2 polymorphism have ALDH2 deficiency and have an increased risk of developing digestive tract cancers,3 however, the association of this polymorphism with HCC development and how acetaldehyde affects HCC still remain obscure.
Excessive alcohol consumption is one of the major causes of hepatocellular carcinoma (HCC). Approximately 30–40% of the Asian population are deficient for aldehyde dehydrogenase 2 (ALDH2), a key enzyme that detoxifies the ethanol metabolite acetaldehyde. However, how ALDH2 deficiency affects alcohol-related HCC remains unclear. ALDH2 polymorphisms were studied in 646 patients with viral hepatitis B (HBV) infection, who did or did not drink alcohol. A new model of HCC induced by chronic carbon tetrachloride (CCl4) and alcohol administration was developed and studied in 3 lines of Aldh2-deficient mice: including Aldh2 global knockout (KO) mice, Aldh2*1/*2 knock-in mutant mice, and liver-specific Aldh2 KO mice. We demonstrated that ALDH2 deficiency was not associated with liver disease progression but was associated with an increased risk of HCC development in cirrhotic patients with HBV who consumed excessive alcohol. The mechanisms underlying HCC development associated with cirrhosis and alcohol consumption were studied in Aldh2-deficient mice. We found that all 3 lines of Aldh2-deficient mice were more susceptible to CCl4 plus alcohol-associated liver fibrosis and HCC development. Furthermore, our results from in vivo and in vitro mechanistic studies revealed that after CCl4 plus ethanol exposure, Aldh2-deficient hepatocytes produced a large amount of harmful oxidized mitochondrial DNA via extracellular vesicles, which were then transferred into neighboring HCC cells and together with acetaldehyde activated multiple oncogenic pathways (JNK, STAT3, BCL-2, and TAZ), thereby promoting HCC. ALDH2 deficiency is associated with an increased risk of alcohol-related HCC development from fibrosis in patients and in mice. Mechanistic studies reveal a novel mechanism that Aldh2-deficient hepatocytes promote alcohol-associated HCC by transferring harmful oxidized mitochondrial DNA-enriched extracellular vesicles into HCC and subsequently activating multiple oncogenic pathways in HCC. A case‐control study of a small number of patients (78 cases) suggests that the frequency of the ALDH2*2/*2 allele correlated with an increased risk of HCC among heavy drinkers,4 but it is not clear whether this ALDH2 polymorphism is associated with HCC caused by etiologies other than alcohol.In the current study, we examined 646 patients with viral hepatitis B (HBV) infection and found that ALDH2 deficiency was not associated with an increased risk of HCC development in patients with HBV, without alcohol consumption, but was a risk factor for HCC in cirrhotic patients with HBV who consumed excessive alcohol, suggesting that cirrhosis, alcohol consumption, and ALDH2 deficiency synergistically promote HCC.To model this clinical condition, we developed a mouse model of HCC induced by chronic CCl4 administration (fibrosis) and alcohol feeding, and tested this model in 3 lines of Aldh2-deficient mice: including Aldh2 global knockout (KO), Aldh2*1/*2 knock-in, and liver-specific Aldh2 KO (Aldh2Hep−/−) mice.Our data revealed that all of these Aldh2-deficient mice were more susceptible to CCl4 plus alcohol-associated HCC development than their wild-type (WT) counterparts.
Excessive alcohol consumption is one of the major causes of hepatocellular carcinoma (HCC). Approximately 30–40% of the Asian population are deficient for aldehyde dehydrogenase 2 (ALDH2), a key enzyme that detoxifies the ethanol metabolite acetaldehyde. However, how ALDH2 deficiency affects alcohol-related HCC remains unclear. ALDH2 polymorphisms were studied in 646 patients with viral hepatitis B (HBV) infection, who did or did not drink alcohol. A new model of HCC induced by chronic carbon tetrachloride (CCl4) and alcohol administration was developed and studied in 3 lines of Aldh2-deficient mice: including Aldh2 global knockout (KO) mice, Aldh2*1/*2 knock-in mutant mice, and liver-specific Aldh2 KO mice. We demonstrated that ALDH2 deficiency was not associated with liver disease progression but was associated with an increased risk of HCC development in cirrhotic patients with HBV who consumed excessive alcohol. The mechanisms underlying HCC development associated with cirrhosis and alcohol consumption were studied in Aldh2-deficient mice. We found that all 3 lines of Aldh2-deficient mice were more susceptible to CCl4 plus alcohol-associated liver fibrosis and HCC development. Furthermore, our results from in vivo and in vitro mechanistic studies revealed that after CCl4 plus ethanol exposure, Aldh2-deficient hepatocytes produced a large amount of harmful oxidized mitochondrial DNA via extracellular vesicles, which were then transferred into neighboring HCC cells and together with acetaldehyde activated multiple oncogenic pathways (JNK, STAT3, BCL-2, and TAZ), thereby promoting HCC. ALDH2 deficiency is associated with an increased risk of alcohol-related HCC development from fibrosis in patients and in mice. Mechanistic studies reveal a novel mechanism that Aldh2-deficient hepatocytes promote alcohol-associated HCC by transferring harmful oxidized mitochondrial DNA-enriched extracellular vesicles into HCC and subsequently activating multiple oncogenic pathways in HCC. ALDH2 deficiency is known to induce excessive acetaldehyde accumulation and oxidative stress during alcohol consumption, leading to mitochondrial DNA damage and base modifications such as oxidation of deoxyguanosine to 8-hydroxy-2′-deoxyguanosine (8-OhDG) leading to mitochondrial dysfunction.5Consequently, mitochondrial dysfunction-mediated DNA damage further accelerates cellular senescence, and contributes to aging-associated phenotypes and pathologies in various types of diseases and cancer including HCC.6Herein, we provide evidence suggesting that extracellular vesicles (EVs) can transfer this damaged DNA from hepatocytes into HCC cells, thereby promoting HCC progression, suggesting a cross-talk between damaged hepatocytes and HCC cells via the transfer of oxidized mitochondrial DNA (mtDNA)-enriched EVs that promote HCC.
There have been calls to integrate HCV testing into existing services, including harm reduction and HIV prevention and treatment, but there are few empirical trials to date. We evaluated the impact of integrating HCV testing/education into integrated care centers (ICCs) delivering HIV services to people who inject drugs (PWID) across India, using a cluster-randomized trial. We compared ICCs with usual care in the PWID stratum (12 sites) of a 22-site cluster-randomized trial. In 6 sites, ICCs delivering HIV testing, harm reduction, other preventive services and linkage to HIV treatment were scaled from opioid agonist therapy centers and operated for 2 years. On-site rapid HCV antibody testing was integrated after 1 year. To assess impact, we conducted baseline and evaluation surveys using respondent-driven sampling (RDS) across the 12 sites (n = 11,993 recruited at baseline; n = 11,721 recruited at evaluation). The primary outcome was population-level self-reported HCV testing history. At evaluation, HCV antibody prevalence ranged from 7.2–76.6%. Across 6 ICCs, 5,263 ICC clients underwent HCV testing, of whom 2,278 were newly diagnosed. At evaluation, PWID in ICC clusters were 4-fold more likely to report being tested for HCV than in usual care clusters, adjusting for baseline testing (adjusted prevalence ratio [aPR] 3.69; 95% CI 1.34–10.2). PWID in ICC clusters were also 7-fold more likely to be aware of their HCV status (aPR 7.11; 95% CI 1.14–44.3) and significantly more likely to initiate treatment (aPR 9.86; 95% CI 1.52–63.8). We provide among the first empirical data supporting the integration of HCV testing into HIV/harm reduction services. To achieve elimination targets, programs will need to scale-up such venues to deliver comprehensive HCV services. An estimated 71 million people are chronically infected with HCV.1The availability of safe, short duration, curative therapies2–4 prompted the World Health Organization (WHO) to release targets for HCV elimination – 80% reduction in incidence and 65% reduction in mortality by 2030.5Achieving these targets requires 80% of all people with active infection to be treated.Thus, it is essential that major inroads are made in people who inject drugs (PWID) in low- and middle-income countries (LMICs).In these settings, awareness of HCV status is well below 10% and most have not even received the most basic HCV education.6
There have been calls to integrate HCV testing into existing services, including harm reduction and HIV prevention and treatment, but there are few empirical trials to date. We evaluated the impact of integrating HCV testing/education into integrated care centers (ICCs) delivering HIV services to people who inject drugs (PWID) across India, using a cluster-randomized trial. We compared ICCs with usual care in the PWID stratum (12 sites) of a 22-site cluster-randomized trial. In 6 sites, ICCs delivering HIV testing, harm reduction, other preventive services and linkage to HIV treatment were scaled from opioid agonist therapy centers and operated for 2 years. On-site rapid HCV antibody testing was integrated after 1 year. To assess impact, we conducted baseline and evaluation surveys using respondent-driven sampling (RDS) across the 12 sites (n = 11,993 recruited at baseline; n = 11,721 recruited at evaluation). The primary outcome was population-level self-reported HCV testing history. At evaluation, HCV antibody prevalence ranged from 7.2–76.6%. Across 6 ICCs, 5,263 ICC clients underwent HCV testing, of whom 2,278 were newly diagnosed. At evaluation, PWID in ICC clusters were 4-fold more likely to report being tested for HCV than in usual care clusters, adjusting for baseline testing (adjusted prevalence ratio [aPR] 3.69; 95% CI 1.34–10.2). PWID in ICC clusters were also 7-fold more likely to be aware of their HCV status (aPR 7.11; 95% CI 1.14–44.3) and significantly more likely to initiate treatment (aPR 9.86; 95% CI 1.52–63.8). We provide among the first empirical data supporting the integration of HCV testing into HIV/harm reduction services. To achieve elimination targets, programs will need to scale-up such venues to deliver comprehensive HCV services. In 2015, it was estimated that there were 15.6 million PWID globally, of whom 8.2 million were exposed to HCV7 and 6.1 million had active HCV infection requiring treatment.8In India, data from a 2013 cross-sectional serosurvey demonstrated that 1 in 3 PWID were infected with HCV.9Despite this high burden, at the time fewer than 6% of HCV-infected PWID were aware of their status.Moreover, the majority had not been tested because they had never heard of HCV, highlighting a compelling need for testing and educational programs.Calls have been made to integrate HCV testing into existing services, including harm reduction and HIV prevention and treatment, particularly for drug-using populations, but there are few empirical trials to date.10–12
Acute-on-chronic liver failure (ACLF) is a clinical syndrome defined by liver failure on preexisting chronic liver disease and is often associated with bacterial infection with high short-term mortality. Experimental models that fully reproduce ACLF and effective pharmacological therapies are lacking. To mimic ACLF conditions, we developed a severe liver injury model by combining chronic injury (chronic carbon tetrachloride [CCl4] injection), acute hepatic insult (injection of a double dose CCl4), and bacterial infection (intraperitoneal injection of bacteria). Serum and liver samples from patients with ACLF or acute drug-induced liver injury (DILI) were used. Liver injury and regeneration were assessed to ascertain for potential benefits of interleukin-22 (IL-22Fc) administration. This severe liver injury model developed acute-on-chronic liver injury, bacterial infection, multi-organ injury, and high mortality, recapitulating some features of clinical ACLF. Liver regeneration in this model was severely impaired due to the shift from the activation of pro-regenerative IL-6/STAT3 to anti-regenerative IFN-γ/STAT1 pathway. The impaired IL-6/STAT3 activation was due to Kupffer cell inability to produce IL-6; whereas the enhanced STAT1 activation was due to strong innate immune response and subsequent production of IFN-γ. Compared to DILI patients, ACLF patients had higher levels of IFN-γ but lower liver regeneration. IL-22Fc treatment improved survival of the ACLF mice by reversing the STAT1/STAT3 pathway imbalance and enhancing expression of many anti-bacterial genes in a manner involving the anti-apoptotic protein BCL2. Acute-on-chronic liver injury or bacterial infection is associated with impaired liver regeneration due to a shift from the pro-regenerative to anti-regenerative pathways, IL-22Fc therapy reverses this shift and attenuates bacterial infection, thus IL-22Fc may have therapeutic potential for ACLF treatment. Acute-on-chronic liver failure (ACLF) is generally accepted as a clinical syndrome characterized by an acute hepatic insult and rapid deterioration of liver function in patients with pre-existing chronic liver disease in combination with multi-organ failure with high short-term mortality.1-5The main etiologies of the pre-existing chronic liver disease are alcoholism and chronic hepatitis B virus (HBV) infection, while the most frequently documented acute insults that induce acute injury in ACLF include excessive alcohol drinking, HBV activation, drug-induced liver injury (DILI) etc.1-6 Bacterial infections were detected in up to 2/3 ACLF patients and contributed to the poor outcome of ACLF.7-9Although the poor outcome is closely associated with bacterial infection,7,8 whether bacterial infection is a consequence of ACLF or a trigger as an acute insult to induce ACLF is still a question of debate.7-9Collectively, the outcome of ACLF likely depends on two major aspects: the recovery of multi-organ injury and the control of bacterial infection.
Acute-on-chronic liver failure (ACLF) is a clinical syndrome defined by liver failure on preexisting chronic liver disease and is often associated with bacterial infection with high short-term mortality. Experimental models that fully reproduce ACLF and effective pharmacological therapies are lacking. To mimic ACLF conditions, we developed a severe liver injury model by combining chronic injury (chronic carbon tetrachloride [CCl4] injection), acute hepatic insult (injection of a double dose CCl4), and bacterial infection (intraperitoneal injection of bacteria). Serum and liver samples from patients with ACLF or acute drug-induced liver injury (DILI) were used. Liver injury and regeneration were assessed to ascertain for potential benefits of interleukin-22 (IL-22Fc) administration. This severe liver injury model developed acute-on-chronic liver injury, bacterial infection, multi-organ injury, and high mortality, recapitulating some features of clinical ACLF. Liver regeneration in this model was severely impaired due to the shift from the activation of pro-regenerative IL-6/STAT3 to anti-regenerative IFN-γ/STAT1 pathway. The impaired IL-6/STAT3 activation was due to Kupffer cell inability to produce IL-6; whereas the enhanced STAT1 activation was due to strong innate immune response and subsequent production of IFN-γ. Compared to DILI patients, ACLF patients had higher levels of IFN-γ but lower liver regeneration. IL-22Fc treatment improved survival of the ACLF mice by reversing the STAT1/STAT3 pathway imbalance and enhancing expression of many anti-bacterial genes in a manner involving the anti-apoptotic protein BCL2. Acute-on-chronic liver injury or bacterial infection is associated with impaired liver regeneration due to a shift from the pro-regenerative to anti-regenerative pathways, IL-22Fc therapy reverses this shift and attenuates bacterial infection, thus IL-22Fc may have therapeutic potential for ACLF treatment. Despite several experimental ACLF models being reported,10-14 none of them simulates the whole pathological process of this disease.Several existing ACLF models have been developed via the combination of chronic and acute liver injury.10-14Chronic injury is most commonly induced by injection of carbon tetrachloride (CCl4) or via bile duct ligation surgery; whereas acute injury is induced by injection of D-galactosamine/lipopolysaccharide (LPS).10-14Although these models include chronic and acute liver injury resulting in considerable mortality, the mean survival period is very short after the acute insult, which renders its application for preclinical interventional studies.In addition, in these models no viable bacterial infection is present while LPS injection does not fully mimic bacterial infection.In the current study, we developed a mouse model of ACLF by chronically administrating CCl4 (chronic injury) and followed by an acute injection of a higher dose of CCl4 (acute injury) and Klebsiella pneumoniae (K.P.) or cecal ligation and puncture (CLP) (bacterial infection).This model recapitulates the three major stages (chronic and acute liver injury and bacterial infection) of ACLF with an appropriate survival period for subsequent studies.Furthermore, we studied liver regeneration and explored the therapeutic potential of interleukin-22Fc (IL-22Fc) in this model.IL-22Fc is a recombinant fusion protein consisting of two human IL-22 molecules linked to an immunoglobulin constant region (IgG2-Fc) with an extended half-life, and is currently being examined in clinical trials for the treatment of severe alcoholic hepatitis.15
In phase III studies, the fixed dose combination of sofosbuvir/velpatasvir/voxilaprevir (SOF/VEL/VOX) administered for 12 weeks led to a sustained virologic response at 12 weeks (SVR12) in 96% of NS5A inhibitor-experienced patients, and an SVR12 rate of 98% in DAA-experienced patients who had not previously received an NS5A inhibitor. Herein, we evaluate the relationship between the presence of detectable resistance-associated substitutions (RASs) at baseline and treatment outcome, and whether RASs were selected for in cases of virologic failure. NS3, NS5A, and NS5B deep sequencing analyses were performed at baseline for all patients and at the time of virologic failure. Results are reported using a 15% cut-off. A total of 82.7% of NS5A inhibitor-experienced patients (205/248) had baseline NS3 and/or NS5A RASs; 79% had baseline NS5A RASs. SVR12 rates were similar in patients with or without NS3 and/or NS5A RASs, and with or without VOX- or VEL-specific RASs. RASs at NS5A position Y93 were present in 37.3% of patients and 95% achieved SVR12. All patients with ≥2 NS5A RASs achieved SVR12. Baseline NS3 and/or NS5A RASs were present in 46.6% (83/178) of non-NS5A inhibitor DAA-experienced patients, all of whom achieved SVR12. All patients with baseline NS5B nucleoside inhibitor RASs, including two patients with S282T, achieved SVR12. Treatment-selected resistance was seen in one of seven patients who relapsed. Baseline RASs had no impact on virologic response in DAA-experienced patients following treatment with SOF/VEL/VOX for 12 weeks. Selection of viral resistance with virologic relapse was uncommon. Chronic hepatitis C virus infection (HCV) is a global health problem causing death and morbidity.1Recent studies have shown that the global prevalence of HCV is estimated to be 1% in 2015, corresponding to 71.1 million individuals with chronic HCV infection.2,3The disease burden of HCV infection is due to progression of chronic liver disease, which can lead to cirrhosis, liver failure, hepatocellular carcinoma (HCC), and death.Globally, 27% of all cases of cirrhosis and 25% of all HCC is attributable to HCV infection.4With the introduction of direct-acting antiviral agents (DAAs), highly effective regimens with sustained virologic response (SVR) rates of >90% are now available for most patients with HCV infection.5–7Despite high SVR rates following DAA treatment in both clinical trials and “real-world” cohorts, there is a growing population of patients who fail DAA-based therapies and have limited approved retreatment options.8–13These patients, who represent the majority of recent treatment failures, are of particular concern because the resistance-associated substitutions (RASs) that are selected by NS5A inhibitors maintain viral fitness long after the end of the failed treatment.14,15It has been shown that patients with genotype 1 (GT1) HCV infection who failed 8 or 12 weeks of treatment with the fixed dose combination of ledipasvir/sofosbuvir (LDV/SOF) and were retreated with LDV/SOF for 24 weeks had an SVR rate of 71%.16Patients who failed treatment with SOF and VEL, and were retreated with SOF/VEL+ribavirin for 24 weeks had an overall SVR rate of 91%, with an SVR rate of 76% for a subset of patients with GT3 HCV infection.17
In phase III studies, the fixed dose combination of sofosbuvir/velpatasvir/voxilaprevir (SOF/VEL/VOX) administered for 12 weeks led to a sustained virologic response at 12 weeks (SVR12) in 96% of NS5A inhibitor-experienced patients, and an SVR12 rate of 98% in DAA-experienced patients who had not previously received an NS5A inhibitor. Herein, we evaluate the relationship between the presence of detectable resistance-associated substitutions (RASs) at baseline and treatment outcome, and whether RASs were selected for in cases of virologic failure. NS3, NS5A, and NS5B deep sequencing analyses were performed at baseline for all patients and at the time of virologic failure. Results are reported using a 15% cut-off. A total of 82.7% of NS5A inhibitor-experienced patients (205/248) had baseline NS3 and/or NS5A RASs; 79% had baseline NS5A RASs. SVR12 rates were similar in patients with or without NS3 and/or NS5A RASs, and with or without VOX- or VEL-specific RASs. RASs at NS5A position Y93 were present in 37.3% of patients and 95% achieved SVR12. All patients with ≥2 NS5A RASs achieved SVR12. Baseline NS3 and/or NS5A RASs were present in 46.6% (83/178) of non-NS5A inhibitor DAA-experienced patients, all of whom achieved SVR12. All patients with baseline NS5B nucleoside inhibitor RASs, including two patients with S282T, achieved SVR12. Treatment-selected resistance was seen in one of seven patients who relapsed. Baseline RASs had no impact on virologic response in DAA-experienced patients following treatment with SOF/VEL/VOX for 12 weeks. Selection of viral resistance with virologic relapse was uncommon. The fixed dose combination of sofosbuvir/velpatasvir/voxilaprevir (SOF/VEL/VOX) was developed to address the medical need for retreatment options for DAA-experienced patients.Resistance to SOF is conferred by the S282T substitution in NS5B.18S282T was first described as a major RAS for other nucleotide inhibitors (NIs).19A comprehensive analysis of all substitutions in NS5B among SOF-treated patients in the phase II and III studies identified two additional treatment-selected substitutions, L159F and V321A, using deep sequencing.20Velpatasvir, both in vitro and in vivo, selected HCV replicon variants with mutations at positions 24, 28, 31, 32, 92, and 93 in NS5A across GTs 1–6.In the GT1a replicon, the NS5A RASs Q30L/R/H, Y93F demonstrated no resistance (≤2.5-fold change) to VEL and Q30K/E, L31I/M/V, P32L, H58D, and Y93C/S) demonstrated low- to mid-level resistance (2.5- to 100-fold change); high levels of resistance (>100-fold change) were observed in Y93H/N/R/W and double mutants.In the GT1b replicon, all NS5A RASs conferred <10-fold change to VEL, except A92K.The majority of the GT2a, GT3a, and GT4a single mutants displayed no or low levels of resistance to VEL; however, Y93H in GT3a conferred 723.5-fold reduced susceptibility to VEL.21
In phase III studies, the fixed dose combination of sofosbuvir/velpatasvir/voxilaprevir (SOF/VEL/VOX) administered for 12 weeks led to a sustained virologic response at 12 weeks (SVR12) in 96% of NS5A inhibitor-experienced patients, and an SVR12 rate of 98% in DAA-experienced patients who had not previously received an NS5A inhibitor. Herein, we evaluate the relationship between the presence of detectable resistance-associated substitutions (RASs) at baseline and treatment outcome, and whether RASs were selected for in cases of virologic failure. NS3, NS5A, and NS5B deep sequencing analyses were performed at baseline for all patients and at the time of virologic failure. Results are reported using a 15% cut-off. A total of 82.7% of NS5A inhibitor-experienced patients (205/248) had baseline NS3 and/or NS5A RASs; 79% had baseline NS5A RASs. SVR12 rates were similar in patients with or without NS3 and/or NS5A RASs, and with or without VOX- or VEL-specific RASs. RASs at NS5A position Y93 were present in 37.3% of patients and 95% achieved SVR12. All patients with ≥2 NS5A RASs achieved SVR12. Baseline NS3 and/or NS5A RASs were present in 46.6% (83/178) of non-NS5A inhibitor DAA-experienced patients, all of whom achieved SVR12. All patients with baseline NS5B nucleoside inhibitor RASs, including two patients with S282T, achieved SVR12. Treatment-selected resistance was seen in one of seven patients who relapsed. Baseline RASs had no impact on virologic response in DAA-experienced patients following treatment with SOF/VEL/VOX for 12 weeks. Selection of viral resistance with virologic relapse was uncommon. Amino acid changes in NS3 positions R155 and A156 were signature mutations for the first-generation NS3/4A protease inhibitors (PIs), telaprevir and boceprevir.Second-generation PIs including the macrocyclic PIs simeprevir and asunaprevir, have similar resistance profiles to the first-generation PIs and in addition select mutations at position D168.22–24The third-generation PI grazoprevir has decreased interactions with R155 wherein it retains activity against R155K.25,26Voxilaprevir (VOX; GS-9857) is a pan-genotypic HCV NS3/4A PI with potent antiviral activity across all HCV GTs and an improved resistance profile compared with first and second-generation PIs.27Voxilaprevir demonstrated improved activity against clinically significant GT1 RASs including Q80K, R155K, D168A and D168E.27
In phase III studies, the fixed dose combination of sofosbuvir/velpatasvir/voxilaprevir (SOF/VEL/VOX) administered for 12 weeks led to a sustained virologic response at 12 weeks (SVR12) in 96% of NS5A inhibitor-experienced patients, and an SVR12 rate of 98% in DAA-experienced patients who had not previously received an NS5A inhibitor. Herein, we evaluate the relationship between the presence of detectable resistance-associated substitutions (RASs) at baseline and treatment outcome, and whether RASs were selected for in cases of virologic failure. NS3, NS5A, and NS5B deep sequencing analyses were performed at baseline for all patients and at the time of virologic failure. Results are reported using a 15% cut-off. A total of 82.7% of NS5A inhibitor-experienced patients (205/248) had baseline NS3 and/or NS5A RASs; 79% had baseline NS5A RASs. SVR12 rates were similar in patients with or without NS3 and/or NS5A RASs, and with or without VOX- or VEL-specific RASs. RASs at NS5A position Y93 were present in 37.3% of patients and 95% achieved SVR12. All patients with ≥2 NS5A RASs achieved SVR12. Baseline NS3 and/or NS5A RASs were present in 46.6% (83/178) of non-NS5A inhibitor DAA-experienced patients, all of whom achieved SVR12. All patients with baseline NS5B nucleoside inhibitor RASs, including two patients with S282T, achieved SVR12. Treatment-selected resistance was seen in one of seven patients who relapsed. Baseline RASs had no impact on virologic response in DAA-experienced patients following treatment with SOF/VEL/VOX for 12 weeks. Selection of viral resistance with virologic relapse was uncommon. In a phase Ib study of VOX monotherapy, most patients (74%) did not develop NS3 selected RASs, and no RASs were selected in patients with GT2 or 4.A156T and A156V were the most common selected substitutions that were no longer detectable at 12 weeks post-treatment in subjects with GT1a or 1b.28Previously, it has been shown that most patients treated with other macrocyclic NS3/4A PIs, including vaniprevir, danoprevir, paritaprevir, and simeprevir selected substitutions at positions R155 and D168 in GT1a and GT1b patients, respectively.29–32These variants were rarely selected by VOX, consistent with the no or low-level resistance in replicon assays.
In phase III studies, the fixed dose combination of sofosbuvir/velpatasvir/voxilaprevir (SOF/VEL/VOX) administered for 12 weeks led to a sustained virologic response at 12 weeks (SVR12) in 96% of NS5A inhibitor-experienced patients, and an SVR12 rate of 98% in DAA-experienced patients who had not previously received an NS5A inhibitor. Herein, we evaluate the relationship between the presence of detectable resistance-associated substitutions (RASs) at baseline and treatment outcome, and whether RASs were selected for in cases of virologic failure. NS3, NS5A, and NS5B deep sequencing analyses were performed at baseline for all patients and at the time of virologic failure. Results are reported using a 15% cut-off. A total of 82.7% of NS5A inhibitor-experienced patients (205/248) had baseline NS3 and/or NS5A RASs; 79% had baseline NS5A RASs. SVR12 rates were similar in patients with or without NS3 and/or NS5A RASs, and with or without VOX- or VEL-specific RASs. RASs at NS5A position Y93 were present in 37.3% of patients and 95% achieved SVR12. All patients with ≥2 NS5A RASs achieved SVR12. Baseline NS3 and/or NS5A RASs were present in 46.6% (83/178) of non-NS5A inhibitor DAA-experienced patients, all of whom achieved SVR12. All patients with baseline NS5B nucleoside inhibitor RASs, including two patients with S282T, achieved SVR12. Treatment-selected resistance was seen in one of seven patients who relapsed. Baseline RASs had no impact on virologic response in DAA-experienced patients following treatment with SOF/VEL/VOX for 12 weeks. Selection of viral resistance with virologic relapse was uncommon. In phase III studies SOF/VEL/VOX administered for 12 weeks to DAA-experienced patients led to an SVR12 rate of 96% in NS5A inhibitor-experienced patients in POLARIS-1, and an SVR12 rate of 98% in DAA-experienced patients who had not previously received an NS5A inhibitor in POLARIS-4.33Recently, SOF/VEL/VOX has received regulatory approval as a salvage regimen for DAA-experienced patients in the US and EU.34,35
Glycogen storage disease type Ia (GSDIa) is a rare genetic disease associated with glycogen accumulation in hepatocytes and steatosis. With age, most adult patients with GSDIa develop hepatocellular adenomas (HCA), which can progress to hepatocellular carcinomas (HCC). In this study, we characterized metabolic reprogramming and cellular defense alterations during tumorigenesis in the liver of hepatocyte-specific G6pc deficient (L.G6pc−/−) mice, which develop all the hepatic hallmarks of GSDIa. Liver metabolism and cellular defenses were assessed at pretumoral (four months) and tumoral (nine months) stages in L.G6pc−/− mice fed a high fat/high sucrose (HF/HS) diet. In response to HF/HS diet, hepatocarcinogenesis was highly accelerated since 85% of L.G6pc−/− mice developed multiple hepatic tumors after nine months, with 70% classified as HCA and 30% as HCC. Tumor development was associated with high expression of malignancy markers of HCC, i.e. alpha-fetoprotein, glypican 3 and β-catenin. In addition, L.G6pc−/− livers exhibited loss of tumor suppressors. Interestingly, L.G6pc−/− steatosis exhibited a low-inflammatory state and was less pronounced than in wild-type livers. This was associated with an absence of epithelial-mesenchymal transition and fibrosis, while HCA/HCC showed a partial epithelial-mesenchymal transition in the absence of TGF-β1 increase. In HCA/HCC, glycolysis was characterized by a marked expression of PK-M2, decreased mitochondrial OXPHOS and a decrease of pyruvate entry in the mitochondria, confirming a “Warburg-like” phenotype. These metabolic alterations led to a decrease in antioxidant defenses and autophagy and chronic endoplasmic reticulum stress in L.G6pc−/− livers and tumors. Interestingly, autophagy was reactivated in HCA/HCC. The metabolic remodeling in L.G6pc −/− liver generates a preneoplastic status and leads to a loss of cellular defenses and tumor suppressors that facilitates tumor development in GSDI. Hepatocellular carcinoma (HCC) is one of the most common primary malignancies of the liver.1Many studies have demonstrated a strong link between hepatic tumorigenesis and non-alcoholic fatty liver diseases (NAFLDs), including obesity, diabetes and other genetic metabolic liver diseases, such as glycogen storage disease type I (GSDI).2–4NAFLD can go from simple steatosis characterized by hepatic fat accumulation, to the more aggressive form called non-alcoholic steatohepatitis (NASH), associated with inflammation and fibrosis.While it is considered that NASH/NAFLD can lead to liver cirrhosis, predisposing the liver to HCC, studies have reported that HCC can also develop in the absence of cirrhosis.Moreover, hepatocellular adenomas (HCA), which are rare benign tumors, can later transform into HCC in the absence of cirrhosis.5,6
Glycogen storage disease type Ia (GSDIa) is a rare genetic disease associated with glycogen accumulation in hepatocytes and steatosis. With age, most adult patients with GSDIa develop hepatocellular adenomas (HCA), which can progress to hepatocellular carcinomas (HCC). In this study, we characterized metabolic reprogramming and cellular defense alterations during tumorigenesis in the liver of hepatocyte-specific G6pc deficient (L.G6pc−/−) mice, which develop all the hepatic hallmarks of GSDIa. Liver metabolism and cellular defenses were assessed at pretumoral (four months) and tumoral (nine months) stages in L.G6pc−/− mice fed a high fat/high sucrose (HF/HS) diet. In response to HF/HS diet, hepatocarcinogenesis was highly accelerated since 85% of L.G6pc−/− mice developed multiple hepatic tumors after nine months, with 70% classified as HCA and 30% as HCC. Tumor development was associated with high expression of malignancy markers of HCC, i.e. alpha-fetoprotein, glypican 3 and β-catenin. In addition, L.G6pc−/− livers exhibited loss of tumor suppressors. Interestingly, L.G6pc−/− steatosis exhibited a low-inflammatory state and was less pronounced than in wild-type livers. This was associated with an absence of epithelial-mesenchymal transition and fibrosis, while HCA/HCC showed a partial epithelial-mesenchymal transition in the absence of TGF-β1 increase. In HCA/HCC, glycolysis was characterized by a marked expression of PK-M2, decreased mitochondrial OXPHOS and a decrease of pyruvate entry in the mitochondria, confirming a “Warburg-like” phenotype. These metabolic alterations led to a decrease in antioxidant defenses and autophagy and chronic endoplasmic reticulum stress in L.G6pc−/− livers and tumors. Interestingly, autophagy was reactivated in HCA/HCC. The metabolic remodeling in L.G6pc −/− liver generates a preneoplastic status and leads to a loss of cellular defenses and tumor suppressors that facilitates tumor development in GSDI. GSDI is due to a deficiency in glucose-6 phosphatase (G6Pase) activity.7,8The mutations in G6PC, encoding the catalytic subunit of G6Pase,9,10 are responsible for GSD type Ia (GSDIa), while mutations in SLC37A4 encoding the transport subunit of the G6Pase, are responsible for GSD type Ib (GSDIb).G6Pase is an enzyme expressed only in the liver, kidneys and intestine, converting glucose-6 phosphate (G6P) into glucose, ensuring normal glycemia.11,12The deficiency of G6Pase results in hypoglycemia, associated with hypercholesterolemia, hypertriglyceridemia, lactic acidosis and hyperuricemia in patients with GSDI and animal models.7,8,13Furthermore, increased accumulation and intracellular flow through G6P lead to glycogen and lipid accumulation in the hepatocytes, resulting in hepatomegaly and hepatic steatosis.7,8,13As mentioned above, most adult GSDI patients develop HCA, which can transform into HCC, with approximately a 10% incidence of transformation, in the absence of liver fibrosis and cirrhosis.14,15In a French cohort of patients with GSDI, about 50% of HCA were diagnosed as inflammatory, nearly 30% of HCA showed β-catenin mutations and 20% of HCA were unclassified.14
Glycogen storage disease type Ia (GSDIa) is a rare genetic disease associated with glycogen accumulation in hepatocytes and steatosis. With age, most adult patients with GSDIa develop hepatocellular adenomas (HCA), which can progress to hepatocellular carcinomas (HCC). In this study, we characterized metabolic reprogramming and cellular defense alterations during tumorigenesis in the liver of hepatocyte-specific G6pc deficient (L.G6pc−/−) mice, which develop all the hepatic hallmarks of GSDIa. Liver metabolism and cellular defenses were assessed at pretumoral (four months) and tumoral (nine months) stages in L.G6pc−/− mice fed a high fat/high sucrose (HF/HS) diet. In response to HF/HS diet, hepatocarcinogenesis was highly accelerated since 85% of L.G6pc−/− mice developed multiple hepatic tumors after nine months, with 70% classified as HCA and 30% as HCC. Tumor development was associated with high expression of malignancy markers of HCC, i.e. alpha-fetoprotein, glypican 3 and β-catenin. In addition, L.G6pc−/− livers exhibited loss of tumor suppressors. Interestingly, L.G6pc−/− steatosis exhibited a low-inflammatory state and was less pronounced than in wild-type livers. This was associated with an absence of epithelial-mesenchymal transition and fibrosis, while HCA/HCC showed a partial epithelial-mesenchymal transition in the absence of TGF-β1 increase. In HCA/HCC, glycolysis was characterized by a marked expression of PK-M2, decreased mitochondrial OXPHOS and a decrease of pyruvate entry in the mitochondria, confirming a “Warburg-like” phenotype. These metabolic alterations led to a decrease in antioxidant defenses and autophagy and chronic endoplasmic reticulum stress in L.G6pc−/− livers and tumors. Interestingly, autophagy was reactivated in HCA/HCC. The metabolic remodeling in L.G6pc −/− liver generates a preneoplastic status and leads to a loss of cellular defenses and tumor suppressors that facilitates tumor development in GSDI. While the molecular mechanisms involved in GSDI tumorigenesis remain unclear, there are several pathways suspected to be involved in this process.16Since GSDI can be considered as a NAFLD-like condition, lipid-related risks could lead to tumorigenesis.Glycogen and lipid storages in massive quantities can induce inflammation, apoptosis and even necrosis.17–19Impaired metabolism can induce abnormal responses in hepatocytes, such as autophagy dysregulation and activation of endoplasmic reticulum (ER) stress pathways.Indeed, recent studies showed a decrease in autophagic flux in GSDI livers.20,21All these mechanisms could promote tumorigenesis in the case of GSDI.
Glycogen storage disease type Ia (GSDIa) is a rare genetic disease associated with glycogen accumulation in hepatocytes and steatosis. With age, most adult patients with GSDIa develop hepatocellular adenomas (HCA), which can progress to hepatocellular carcinomas (HCC). In this study, we characterized metabolic reprogramming and cellular defense alterations during tumorigenesis in the liver of hepatocyte-specific G6pc deficient (L.G6pc−/−) mice, which develop all the hepatic hallmarks of GSDIa. Liver metabolism and cellular defenses were assessed at pretumoral (four months) and tumoral (nine months) stages in L.G6pc−/− mice fed a high fat/high sucrose (HF/HS) diet. In response to HF/HS diet, hepatocarcinogenesis was highly accelerated since 85% of L.G6pc−/− mice developed multiple hepatic tumors after nine months, with 70% classified as HCA and 30% as HCC. Tumor development was associated with high expression of malignancy markers of HCC, i.e. alpha-fetoprotein, glypican 3 and β-catenin. In addition, L.G6pc−/− livers exhibited loss of tumor suppressors. Interestingly, L.G6pc−/− steatosis exhibited a low-inflammatory state and was less pronounced than in wild-type livers. This was associated with an absence of epithelial-mesenchymal transition and fibrosis, while HCA/HCC showed a partial epithelial-mesenchymal transition in the absence of TGF-β1 increase. In HCA/HCC, glycolysis was characterized by a marked expression of PK-M2, decreased mitochondrial OXPHOS and a decrease of pyruvate entry in the mitochondria, confirming a “Warburg-like” phenotype. These metabolic alterations led to a decrease in antioxidant defenses and autophagy and chronic endoplasmic reticulum stress in L.G6pc−/− livers and tumors. Interestingly, autophagy was reactivated in HCA/HCC. The metabolic remodeling in L.G6pc −/− liver generates a preneoplastic status and leads to a loss of cellular defenses and tumor suppressors that facilitates tumor development in GSDI. In order to investigate the pathways involved in tumorigenesis, our laboratory has developed a GSDIa mouse model, in which G6pc is deleted specifically in the liver.These L.G6pc−/− mice exhibit the same hepatic and plasmatic complications observed in patients with GSDIa, including HCA and HCC development.13,22
Genetic variability in the hepatitis B virus X gene (HBx) is frequently observed and is associated with hepatocellular carcinoma (HCC) progression. However, a genotype classification based on the full-length HBx sequence and the impact of genotypes on hepatitis B virus (HBV)-related HCC prognosis remain unclear. We therefore aimed to perform this genotype classification and assess its clinical impact. We classified the genotypes of the full-length HBx gene through sequencing and a cluster analysis of HBx DNA from a cohort of patients with HBV-related HCC, which served as the primary cohort (n = 284). Two independent HBV-related HCC cohorts, a validation cohort (n = 171) and a serum cohort (n = 168), were used to verify the results. Protein microarray assay analysis was performed to explore the underlying mechanism. In the primary cohort, the HBx DNA was classified into 3 genotypes: HBx-EHBH1, HBx-EHBH2, and HBx-EHBH3. HBx-EHBH2 (HBx-E2) indicated better recurrence-free survival and overall survival for patients with HCC. HBx-E2 was significantly correlated with the absence of liver cirrhosis, a small tumor size, a solitary tumor, complete encapsulation and Barcelona Clinic Liver Cancer (BCLC) stage A-0 tumors. Additionally, HBx-E2 served as a significant prognostic factor for patients with BCLC stage B HCC after hepatectomy. Mechanistically, HBx-E2 is unable to promote proliferation in HCC cells and normal hepatocytes. It also fails to activate the Janus kinase 1 (JAK1)/signal transducer and activator of transcription 3 (STAT3)/STAT5 pathway. Our study identifies a novel HBx genotype that is unable to promote the proliferation of HCC cells and suggests a potential marker to preoperatively predict the prognosis of patients with BCLC stage B, HBV-associated, HCC. Chronic hepatitis B virus (HBV) infection is the dominant risk factor for the development of hepatocellular carcinoma (HCC) in the Asia-Pacific region due to inflammation, cirrhosis and direct viral oncogenic factors.1The hepatitis B virus X gene (HBx) gene, 1 of 4 open reading frames of HBV, encodes a 17 kDa protein and has been shown to be strongly linked to the development of HCC.2As a transactivator, the HBx protein activates various viral and cellular promoters and enhancers via protein–protein interactions and affects various signal-transduction pathways, such as the Janus kinase (JAK)/signal transducer and activator of transcription (STAT), Wnt/β-catenin, nuclear factor-κB (NF-κB) and protein kinase B/Akt pathways.3,4Furthermore, HBx has been shown to modulate a wide range of cellular functions, including proliferation, the cell cycle, apoptosis, autophagy, metastasis, and metabolism, which lead to the development of HCC.5
Genetic variability in the hepatitis B virus X gene (HBx) is frequently observed and is associated with hepatocellular carcinoma (HCC) progression. However, a genotype classification based on the full-length HBx sequence and the impact of genotypes on hepatitis B virus (HBV)-related HCC prognosis remain unclear. We therefore aimed to perform this genotype classification and assess its clinical impact. We classified the genotypes of the full-length HBx gene through sequencing and a cluster analysis of HBx DNA from a cohort of patients with HBV-related HCC, which served as the primary cohort (n = 284). Two independent HBV-related HCC cohorts, a validation cohort (n = 171) and a serum cohort (n = 168), were used to verify the results. Protein microarray assay analysis was performed to explore the underlying mechanism. In the primary cohort, the HBx DNA was classified into 3 genotypes: HBx-EHBH1, HBx-EHBH2, and HBx-EHBH3. HBx-EHBH2 (HBx-E2) indicated better recurrence-free survival and overall survival for patients with HCC. HBx-E2 was significantly correlated with the absence of liver cirrhosis, a small tumor size, a solitary tumor, complete encapsulation and Barcelona Clinic Liver Cancer (BCLC) stage A-0 tumors. Additionally, HBx-E2 served as a significant prognostic factor for patients with BCLC stage B HCC after hepatectomy. Mechanistically, HBx-E2 is unable to promote proliferation in HCC cells and normal hepatocytes. It also fails to activate the Janus kinase 1 (JAK1)/signal transducer and activator of transcription 3 (STAT3)/STAT5 pathway. Our study identifies a novel HBx genotype that is unable to promote the proliferation of HCC cells and suggests a potential marker to preoperatively predict the prognosis of patients with BCLC stage B, HBV-associated, HCC. HBx shows high genetic variability, which is due to an inaccurate reverse transcriptase and a lack of proofreading activity, in HBV.6According to the results of genetic analyses of the HBx DNA, 2 main types of HBx genetic variations have been detected in patients with HBV-related liver diseases.First, some point mutations in the HBx gene, including nucleotides 1630, 1721, 1762, and 1764, and particularly double substitutions (A1762T and G1764A), are more frequent in patients with advanced liver diseases and HCC.7,8Second, distal C-terminally truncated HBx mutants are selected in tumor tissues and play a role in hepatocarcinogenesis.9,10HBx variants have been shown to play an important role in HCC progression.The A1762T/G1764A mutation increases the risk of HCC and is independently predictive of postoperative survival in patients with HCC.11–12C-terminally truncated HBx promotes the development and progression of cancer by increasing cell proliferation, invasiveness and metastasis.13,14The overall activity of HBx mutants appears to increase the expression of hypoxia-inducible factor-1α (HIF-1α), which is associated with poor outcomes for patients with HCC.15To study the variability of the HBx gene, it is helpful to further clarify the role of HBx in the progression of HCC and to identify the key locus that affects HBx functions.
Genetic variability in the hepatitis B virus X gene (HBx) is frequently observed and is associated with hepatocellular carcinoma (HCC) progression. However, a genotype classification based on the full-length HBx sequence and the impact of genotypes on hepatitis B virus (HBV)-related HCC prognosis remain unclear. We therefore aimed to perform this genotype classification and assess its clinical impact. We classified the genotypes of the full-length HBx gene through sequencing and a cluster analysis of HBx DNA from a cohort of patients with HBV-related HCC, which served as the primary cohort (n = 284). Two independent HBV-related HCC cohorts, a validation cohort (n = 171) and a serum cohort (n = 168), were used to verify the results. Protein microarray assay analysis was performed to explore the underlying mechanism. In the primary cohort, the HBx DNA was classified into 3 genotypes: HBx-EHBH1, HBx-EHBH2, and HBx-EHBH3. HBx-EHBH2 (HBx-E2) indicated better recurrence-free survival and overall survival for patients with HCC. HBx-E2 was significantly correlated with the absence of liver cirrhosis, a small tumor size, a solitary tumor, complete encapsulation and Barcelona Clinic Liver Cancer (BCLC) stage A-0 tumors. Additionally, HBx-E2 served as a significant prognostic factor for patients with BCLC stage B HCC after hepatectomy. Mechanistically, HBx-E2 is unable to promote proliferation in HCC cells and normal hepatocytes. It also fails to activate the Janus kinase 1 (JAK1)/signal transducer and activator of transcription 3 (STAT3)/STAT5 pathway. Our study identifies a novel HBx genotype that is unable to promote the proliferation of HCC cells and suggests a potential marker to preoperatively predict the prognosis of patients with BCLC stage B, HBV-associated, HCC. Previous studies focusing on HBx gene variants generally concentrated on 2 mutation types.These studies failed to recognize the full-length HBx DNA sequence when classifying the genotype.They also failed to explore differences among genotypes and the biological effects resulting from multiple site variants.The primary amino acid sequence of the HBx protein is organized into a negative regulatory domain and a transactivation domain.Furthermore, 6 (A–F) regions in HBx exhibit different modular organizations and functions.16,17Interactions between the transactivation domain of HBx with different target molecules have been mapped to different regions.18Thus, the functional alterations in HBx resulting from variations at multiple sites in different regions between various genotypes are quite complicated and remain unclear.
Dysregulation of the Keap1-Nrf2 pathway has been observed in experimental and human tumors, suggesting possible roles of the pathway in cancer development. Herein, we examined whether Nrf2 (Nfe2l2) activation occurs at early steps of rat hepatocarcinogenesis, to assess critical contributions of Nrf2 to the onset of hepatocellular carcinoma (HCC). We used wild-type (WT) and Nrf2 knockout (Nrf2KO) rats treated with a single injection of diethylnitrosamine (DENA) followed by choline-devoid methionine-deficient (CMD) diet. This experimental model causes massive fatty liver and steatohepatitis with fibrosis and enables identification of early stages of hepatocarcinogenesis. We found that Nrf2 activation takes place in early preneoplastic lesions identified by the marker glutathione S-transferase placental form (GSTP). Nrf2 missense mutations, known to disrupt the Keap1-Nrf2 binding, were present in 65.7% of GSTP-positive foci. Nrf2KO rats were used to directly investigate whether Nrf2 is critical for initiation and/or clonal expansion of DENA-damaged hepatocytes. While Nrf2 genetic inactivation did not alter DENA-induced initiation, it led to increased liver injury and chronic compensatory hepatocyte regeneration when rats were fed a CMD diet. However, in spite of such a permissive environment, the livers of Nrf2KO rats did not display any preneoplastic lesion unlike those of WT rats. These results demonstrate that, in a model of hepatocarcinogenesis resembling human non-alcoholic fatty liver disease: i) Nrf2 is activated at early steps of the tumorigenic process and ii) Nrf2 is mandatory for the clonal expansion of initiated cells, indicating that Nrf2 is critical in the onset of HCC. Hepatocellular carcinoma (HCC) is the second largest cause of cancer-related deaths worldwide.1Unfortunately, our knowledge of the genetic/epigenetic alterations implicated in HCC initiation and progression is still fragmented.The driving genetic alterations to which tumor cells are addicted and that can be targeted have not been identified.In this scenario, HCC is probably one of the tumor types where a more complete understanding of the underlying genetic alterations could have a major impact on the development of new treatment strategies.NRF2, also known as NFE2L2, is of particular interest as the role of the KEAP1-NRF2 pathway in cancer development has provided conflicting results.2–4NRF2 is a master transcriptional activator of genes encoding enzymes that protect cells from oxidative stress and xenobiotics, as well as various drug efflux pump-members of the multidrug resistance protein family.5NRF2 is negatively regulated and targeted to proteasomal degradation by KEAP1.6–9It was reported in a number of studies that point mutations in the KEAP1 or NRF2 genes are often present in primary tumors.10–16As to human HCC, two recent studies using whole-exome sequencing have revealed mutations of either NRF2 (6.4%) or KEAP1 (8%),17,18 suggesting that the dysregulation of this pathway may play a relevant role in a subset of human HCC.Notably, increased levels of NRF2 mRNA have been found associated with poor prognosis in HCC.19
Dysregulation of the Keap1-Nrf2 pathway has been observed in experimental and human tumors, suggesting possible roles of the pathway in cancer development. Herein, we examined whether Nrf2 (Nfe2l2) activation occurs at early steps of rat hepatocarcinogenesis, to assess critical contributions of Nrf2 to the onset of hepatocellular carcinoma (HCC). We used wild-type (WT) and Nrf2 knockout (Nrf2KO) rats treated with a single injection of diethylnitrosamine (DENA) followed by choline-devoid methionine-deficient (CMD) diet. This experimental model causes massive fatty liver and steatohepatitis with fibrosis and enables identification of early stages of hepatocarcinogenesis. We found that Nrf2 activation takes place in early preneoplastic lesions identified by the marker glutathione S-transferase placental form (GSTP). Nrf2 missense mutations, known to disrupt the Keap1-Nrf2 binding, were present in 65.7% of GSTP-positive foci. Nrf2KO rats were used to directly investigate whether Nrf2 is critical for initiation and/or clonal expansion of DENA-damaged hepatocytes. While Nrf2 genetic inactivation did not alter DENA-induced initiation, it led to increased liver injury and chronic compensatory hepatocyte regeneration when rats were fed a CMD diet. However, in spite of such a permissive environment, the livers of Nrf2KO rats did not display any preneoplastic lesion unlike those of WT rats. These results demonstrate that, in a model of hepatocarcinogenesis resembling human non-alcoholic fatty liver disease: i) Nrf2 is activated at early steps of the tumorigenic process and ii) Nrf2 is mandatory for the clonal expansion of initiated cells, indicating that Nrf2 is critical in the onset of HCC. Increasing lines of evidence suggest that human HCC might arise as a consequence of chronic non-alcoholic fatty liver and non-alcoholic steatohepatitis because of increased oxidative stress.20–22Indeed, increased levels of reactive oxygen species (ROS), lipid peroxidation products and decreased levels of antioxidant molecules have been observed in patients with non-alcoholic fatty liver disease.23Conversely, in dietary mice models, Nrf2-deficiency favors non-alcoholic steatohepatitis development.24–27
Antibiotic resistance has been increasingly reported in patients with decompensated cirrhosis in single-center studies. Prospective investigations reporting broad epidemiological data are scarce. We aimed to analyze epidemiological changes in bacterial infections in patients with decompensated cirrhosis. This was a prospective evaluation of 2 series of patients hospitalized with decompensated cirrhosis. The Canonic series included 1,146 patients from Northern, Southern and Western Europe in 2011. Data on epidemiology, clinical characteristics of bacterial infections, microbiology and empirical antibiotic schedules were assessed. A second series of 883 patients from Eastern, Southern and Western Europe was investigated between 2017–2018. A total of 455 patients developed 520 infections (39.7%) in the first series, with spontaneous bacterial peritonitis, urinary tract infections and pneumonia the most frequent infections. Nosocomial episodes predominated in this series. Nearly half of the infections were culture-positive, of which 29.2% were caused by multidrug-resistant organisms (MDROs). MDR strains were more frequently isolated in Northern and Western Europe. Extended-spectrum beta-lactamase-producing Enterobacteriaceae were the most frequent MDROs isolated in this series, although prevalence and type differed markedly among countries and centers. Antibiotic resistance was associated with poor prognosis and failure of antibiotic strategies, based on third-generation cephalosporins or quinolones. Nosocomial infection (odds ratio [OR] 2.74; p < 0.001), intensive care unit admission (OR 2.09; p = 0.02), and recent hospitalization (OR 1.93; p = 0.04) were identified as independent predictors of MDR infection. The prevalence of MDROs in the second series (392 infections/284 patients) was 23%; 38% in culture-positive infections. A mild increase in the rate of carbapenem-resistant Enterobacteriaceae was observed in this series. MDR bacterial infections constitute a prevalent, growing and complex healthcare problem in patients with decompensated cirrhosis and acute-on-chronic liver failure across Europe, negatively impacting on prognosis. Strategies aimed at preventing the spread of antibiotic resistance in cirrhosis should be urgently evaluated. Bacterial infections constitute a frequent complication in patients with decompensated cirrhosis and are the most frequent trigger of acute-on-chronic liver failure (ACLF) in Western countries.1–5Patients with cirrhosis and acute decompensation (AD) are prone to developing spontaneous and secondary bacterial infections, a risk that is magnified in patients with ACLF.1,5,6Bacterial infection has a critical relevance in the clinical course of decompensated cirrhosis, increasing the rate of short-term mortality by 2–4 fold.7,8Recent data also show that bacterial infections are severe and associated with intense systemic inflammation, poor clinical course and high mortality in patients with ACLF.6
Antibiotic resistance has been increasingly reported in patients with decompensated cirrhosis in single-center studies. Prospective investigations reporting broad epidemiological data are scarce. We aimed to analyze epidemiological changes in bacterial infections in patients with decompensated cirrhosis. This was a prospective evaluation of 2 series of patients hospitalized with decompensated cirrhosis. The Canonic series included 1,146 patients from Northern, Southern and Western Europe in 2011. Data on epidemiology, clinical characteristics of bacterial infections, microbiology and empirical antibiotic schedules were assessed. A second series of 883 patients from Eastern, Southern and Western Europe was investigated between 2017–2018. A total of 455 patients developed 520 infections (39.7%) in the first series, with spontaneous bacterial peritonitis, urinary tract infections and pneumonia the most frequent infections. Nosocomial episodes predominated in this series. Nearly half of the infections were culture-positive, of which 29.2% were caused by multidrug-resistant organisms (MDROs). MDR strains were more frequently isolated in Northern and Western Europe. Extended-spectrum beta-lactamase-producing Enterobacteriaceae were the most frequent MDROs isolated in this series, although prevalence and type differed markedly among countries and centers. Antibiotic resistance was associated with poor prognosis and failure of antibiotic strategies, based on third-generation cephalosporins or quinolones. Nosocomial infection (odds ratio [OR] 2.74; p < 0.001), intensive care unit admission (OR 2.09; p = 0.02), and recent hospitalization (OR 1.93; p = 0.04) were identified as independent predictors of MDR infection. The prevalence of MDROs in the second series (392 infections/284 patients) was 23%; 38% in culture-positive infections. A mild increase in the rate of carbapenem-resistant Enterobacteriaceae was observed in this series. MDR bacterial infections constitute a prevalent, growing and complex healthcare problem in patients with decompensated cirrhosis and acute-on-chronic liver failure across Europe, negatively impacting on prognosis. Strategies aimed at preventing the spread of antibiotic resistance in cirrhosis should be urgently evaluated. Early diagnosis and adequate empirical antibiotic therapy of bacterial infections is key in the management of cirrhotic patients.1,9However, the epidemiology of bacterial infections is now much more complex than in the past.9The efficacy of classical empirical antibiotic strategies based on the administration of third-generation cephalosporins has markedly decreased in the last decade because of the emergence of multidrug-resistant (MDR) bacteria.9–13Resistance to antibiotics in pathogenic bacteria is currently a major global public health problem,14 and is particularly serious in patients with decompensated cirrhosis.These patients frequently accumulate several risk factors for MDR organisms (MDROs) including recurrent hospitalizations, invasive procedures and repeated exposures to prophylactic or therapeutic antibiotics.9Antibiotic overuse and failure of control measures to prevent the spread of MDROs in the healthcare setting have magnified antimicrobial resistance in cirrhosis.Therefore, the characterization of these epidemiological changes and the identification of the MDROs that infect our cirrhotic patients are of major clinical relevance.The great majority of the epidemiological data on antibiotic resistance in cirrhosis derives from single-center studies2,4,10–13,15–20 or from multicenter studies performed in specific countries21 or assessing specific infections.22However, at present no study has explored the epidemiology of MDROs in large geographical, multinational regions in patients with cirrhosis and all types of infection.These studies are essential to understand the global impact of antibiotic resistance.
Knowledge about the regulation of anti-HBV humoral immunity during natural HBV infection is limited. We recently utilized dual fluorochrome-conjugated HBsAg to demonstrate, in patients with chronic HBV (CHB) infection, the functional impairment of their HBsAg-specific B cells. However, the features of their HBcAg-specific B cells are unknown. Here we developed a method to directly visualize, select and characterize HBcAg-specific B cells in parallel with HBsAg-specific B cells. Fluorochrome-conjugated HBcAg reagents were synthesized and utilized to directly detect ex vivo HBcAg-specific B cells in 36 patients with CHB. The frequency, phenotype, functional maturation and transcriptomic profile of HBcAg-specific B cells was studied by flow cytometry, in vitro maturation assays and NanoString-based detection of expression of immune genes, which we compared with HBsAg-specific B cells and total B cells. HBcAg-specific B cells are present at a higher frequency than HBsAg-specific B cells in patients with CHB and, unlike HBsAg-specific B cells, they mature efficiently into antibody-secreting cells in vitro. Their phenotypic and transcriptomic profiles show that HBcAg-specific B cells are preferentially IgG+ memory B cells. However, despite their phenotypic and functional differences, HBcAg- and HBsAg-specific B cells from patients with CHB share an mRNA expression pattern that differs from global memory B cells and is characterized by high expression of genes indicative of cross-presentation and innate immune activity. During chronic HBV infection, a direct relation exists between serological detection of anti-HBs and anti-HBc antibodies, and the quantity and function of their respective specific B cells. However, the transcriptomic analysis performed in HBsAg- and HBcAg-specific B cells suggests additional roles of HBV-specific B cells beyond the production of antibodies. Antibodies and T cells cooperate to control pathogens and present functional alterations during persistent infections.This scenario is also present in the infection with HBV, a hepatotropic DNA virus that establishes chronic infection in the human liver and can cause pathologies (liver cirrhosis and cancer) that have increased decisively in the last decades.1
Knowledge about the regulation of anti-HBV humoral immunity during natural HBV infection is limited. We recently utilized dual fluorochrome-conjugated HBsAg to demonstrate, in patients with chronic HBV (CHB) infection, the functional impairment of their HBsAg-specific B cells. However, the features of their HBcAg-specific B cells are unknown. Here we developed a method to directly visualize, select and characterize HBcAg-specific B cells in parallel with HBsAg-specific B cells. Fluorochrome-conjugated HBcAg reagents were synthesized and utilized to directly detect ex vivo HBcAg-specific B cells in 36 patients with CHB. The frequency, phenotype, functional maturation and transcriptomic profile of HBcAg-specific B cells was studied by flow cytometry, in vitro maturation assays and NanoString-based detection of expression of immune genes, which we compared with HBsAg-specific B cells and total B cells. HBcAg-specific B cells are present at a higher frequency than HBsAg-specific B cells in patients with CHB and, unlike HBsAg-specific B cells, they mature efficiently into antibody-secreting cells in vitro. Their phenotypic and transcriptomic profiles show that HBcAg-specific B cells are preferentially IgG+ memory B cells. However, despite their phenotypic and functional differences, HBcAg- and HBsAg-specific B cells from patients with CHB share an mRNA expression pattern that differs from global memory B cells and is characterized by high expression of genes indicative of cross-presentation and innate immune activity. During chronic HBV infection, a direct relation exists between serological detection of anti-HBs and anti-HBc antibodies, and the quantity and function of their respective specific B cells. However, the transcriptomic analysis performed in HBsAg- and HBcAg-specific B cells suggests additional roles of HBV-specific B cells beyond the production of antibodies. The 2 arms of adaptive immunity are however differently regulated in chronic HBV (CHB) infection.Despite a recently demonstrated phenotypic heterogeneity,2,3 T cells specific to the different HBV proteins (envelope, nucleocapsid, polymerase and X) are all present at limited frequencies and display functional defects in patients with CHB.2In contrast, the HBV-specific humoral response is not homogeneously suppressed.Antibodies specific for the major component of the virus envelope (defined as anti-HBs) are only detectable in individuals with resolved HBV infection and are produced in very low quantities in CHB infection.3–5In contrast, antibodies specific for the protein component of the nucleocapsid (HBcAg) are not only clearly detectable both in self-limited and CHB infection, but are more abundant in the latter.6
Knowledge about the regulation of anti-HBV humoral immunity during natural HBV infection is limited. We recently utilized dual fluorochrome-conjugated HBsAg to demonstrate, in patients with chronic HBV (CHB) infection, the functional impairment of their HBsAg-specific B cells. However, the features of their HBcAg-specific B cells are unknown. Here we developed a method to directly visualize, select and characterize HBcAg-specific B cells in parallel with HBsAg-specific B cells. Fluorochrome-conjugated HBcAg reagents were synthesized and utilized to directly detect ex vivo HBcAg-specific B cells in 36 patients with CHB. The frequency, phenotype, functional maturation and transcriptomic profile of HBcAg-specific B cells was studied by flow cytometry, in vitro maturation assays and NanoString-based detection of expression of immune genes, which we compared with HBsAg-specific B cells and total B cells. HBcAg-specific B cells are present at a higher frequency than HBsAg-specific B cells in patients with CHB and, unlike HBsAg-specific B cells, they mature efficiently into antibody-secreting cells in vitro. Their phenotypic and transcriptomic profiles show that HBcAg-specific B cells are preferentially IgG+ memory B cells. However, despite their phenotypic and functional differences, HBcAg- and HBsAg-specific B cells from patients with CHB share an mRNA expression pattern that differs from global memory B cells and is characterized by high expression of genes indicative of cross-presentation and innate immune activity. During chronic HBV infection, a direct relation exists between serological detection of anti-HBs and anti-HBc antibodies, and the quantity and function of their respective specific B cells. However, the transcriptomic analysis performed in HBsAg- and HBcAg-specific B cells suggests additional roles of HBV-specific B cells beyond the production of antibodies. In addition, while HBV-specific T cells, irrespective of their specificities, always correlate with HBV control and protection,7,8 only anti-HBs antibodies have neutralizing activity while anti-HBc antibodies do not appear to possess, except in some selected cases,9 pathological or protective functions.10The reason why the antibody responses to these 2 major components of HBV are differentially regulated remains unknown.
Knowledge about the regulation of anti-HBV humoral immunity during natural HBV infection is limited. We recently utilized dual fluorochrome-conjugated HBsAg to demonstrate, in patients with chronic HBV (CHB) infection, the functional impairment of their HBsAg-specific B cells. However, the features of their HBcAg-specific B cells are unknown. Here we developed a method to directly visualize, select and characterize HBcAg-specific B cells in parallel with HBsAg-specific B cells. Fluorochrome-conjugated HBcAg reagents were synthesized and utilized to directly detect ex vivo HBcAg-specific B cells in 36 patients with CHB. The frequency, phenotype, functional maturation and transcriptomic profile of HBcAg-specific B cells was studied by flow cytometry, in vitro maturation assays and NanoString-based detection of expression of immune genes, which we compared with HBsAg-specific B cells and total B cells. HBcAg-specific B cells are present at a higher frequency than HBsAg-specific B cells in patients with CHB and, unlike HBsAg-specific B cells, they mature efficiently into antibody-secreting cells in vitro. Their phenotypic and transcriptomic profiles show that HBcAg-specific B cells are preferentially IgG+ memory B cells. However, despite their phenotypic and functional differences, HBcAg- and HBsAg-specific B cells from patients with CHB share an mRNA expression pattern that differs from global memory B cells and is characterized by high expression of genes indicative of cross-presentation and innate immune activity. During chronic HBV infection, a direct relation exists between serological detection of anti-HBs and anti-HBc antibodies, and the quantity and function of their respective specific B cells. However, the transcriptomic analysis performed in HBsAg- and HBcAg-specific B cells suggests additional roles of HBV-specific B cells beyond the production of antibodies. The constant production of anti-HBc in patients with CHB11 has been justified by the demonstration, in mice, that the development of anti-HBc secreting plasma cells is a T cell independent-phenomenon,12 but the reason why persistent antigen presence has a different effect on HBsAg- and HBcAg-specific B cell responses is unknown.
Knowledge about the regulation of anti-HBV humoral immunity during natural HBV infection is limited. We recently utilized dual fluorochrome-conjugated HBsAg to demonstrate, in patients with chronic HBV (CHB) infection, the functional impairment of their HBsAg-specific B cells. However, the features of their HBcAg-specific B cells are unknown. Here we developed a method to directly visualize, select and characterize HBcAg-specific B cells in parallel with HBsAg-specific B cells. Fluorochrome-conjugated HBcAg reagents were synthesized and utilized to directly detect ex vivo HBcAg-specific B cells in 36 patients with CHB. The frequency, phenotype, functional maturation and transcriptomic profile of HBcAg-specific B cells was studied by flow cytometry, in vitro maturation assays and NanoString-based detection of expression of immune genes, which we compared with HBsAg-specific B cells and total B cells. HBcAg-specific B cells are present at a higher frequency than HBsAg-specific B cells in patients with CHB and, unlike HBsAg-specific B cells, they mature efficiently into antibody-secreting cells in vitro. Their phenotypic and transcriptomic profiles show that HBcAg-specific B cells are preferentially IgG+ memory B cells. However, despite their phenotypic and functional differences, HBcAg- and HBsAg-specific B cells from patients with CHB share an mRNA expression pattern that differs from global memory B cells and is characterized by high expression of genes indicative of cross-presentation and innate immune activity. During chronic HBV infection, a direct relation exists between serological detection of anti-HBs and anti-HBc antibodies, and the quantity and function of their respective specific B cells. However, the transcriptomic analysis performed in HBsAg- and HBcAg-specific B cells suggests additional roles of HBV-specific B cells beyond the production of antibodies. We and others have recently demonstrated that fluorochrome-labeled HBsAg can be used to visualize HBsAg-specific B cells, in HBsAg-vaccinated individuals and HBV-infected patients.4,5Labeled HBsAg binds to the specific variable region of the B cell receptor (BCR) present on the surface of HBsAg-specific B cells and has allowed us4 and others5 to demonstrate that HBsAg-specific B cells in patients with CHB are not only detectable at low frequency, but present maturation defects that can be caused by continuous antigenic exposure4 or by liver-localized B cell priming.5
Knowledge about the regulation of anti-HBV humoral immunity during natural HBV infection is limited. We recently utilized dual fluorochrome-conjugated HBsAg to demonstrate, in patients with chronic HBV (CHB) infection, the functional impairment of their HBsAg-specific B cells. However, the features of their HBcAg-specific B cells are unknown. Here we developed a method to directly visualize, select and characterize HBcAg-specific B cells in parallel with HBsAg-specific B cells. Fluorochrome-conjugated HBcAg reagents were synthesized and utilized to directly detect ex vivo HBcAg-specific B cells in 36 patients with CHB. The frequency, phenotype, functional maturation and transcriptomic profile of HBcAg-specific B cells was studied by flow cytometry, in vitro maturation assays and NanoString-based detection of expression of immune genes, which we compared with HBsAg-specific B cells and total B cells. HBcAg-specific B cells are present at a higher frequency than HBsAg-specific B cells in patients with CHB and, unlike HBsAg-specific B cells, they mature efficiently into antibody-secreting cells in vitro. Their phenotypic and transcriptomic profiles show that HBcAg-specific B cells are preferentially IgG+ memory B cells. However, despite their phenotypic and functional differences, HBcAg- and HBsAg-specific B cells from patients with CHB share an mRNA expression pattern that differs from global memory B cells and is characterized by high expression of genes indicative of cross-presentation and innate immune activity. During chronic HBV infection, a direct relation exists between serological detection of anti-HBs and anti-HBc antibodies, and the quantity and function of their respective specific B cells. However, the transcriptomic analysis performed in HBsAg- and HBcAg-specific B cells suggests additional roles of HBV-specific B cells beyond the production of antibodies. The development of a similar reagent for the study of HBcAg-specific B cells, however, poses methodological problems.The 183 amino-acid long protein defined as HBcAg constitutes the nucleocapsid of HBV.It is comprised of 180 to 240 subunits clustered as dimers, producing spikes that are suggested to contain the major binding sites13 for anti-HBc-specific antibodies.However, HBcAg can also bind to a conserved linear motif present in the framework region 1 complementary determining region (FR1-CDR1) junction of the variable heavy 1 (VH1) and probably VH7 chain of different immunoglobulins, thus acting as a B cell superantigen.14,15This low-affinity interaction, demonstrated for example with immunoglobulin receptors of naïve B cells,15,16 might hamper the ability of fluorochrome-conjugated HBcAg to bind specifically to HBcAg-specific BCRs that characterize memory B cells producing anti-HBc.
Advanced liver fibrosis is an important diagnostic target in non-alcoholic fatty liver disease (NAFLD) as it defines the subgroup of patients with impaired prognosis. The non-invasive diagnosis of advanced fibrosis is currently limited by the suboptimal positive predictive value and the grey zone (representing indeterminate diagnosis) of fibrosis tests. Here, we aimed to determine the best combination of non-invasive tests for the diagnosis of advanced fibrosis in NAFLD. A total of 938 patients with biopsy-proven NAFLD were randomized 2:1 into derivation and validation sets. All patients underwent liver stiffness measurement with vibration controlled transient elastography (VCTE) and blood fibrosis tests (NAFLD fibrosis score, Fibrosis-4 [FIB4], Fibrotest, Hepascore, FibroMeter). FibroMeterVCTE, which combines VCTE results and FibroMeter markers in a single test, was also calculated in all patients. For the diagnosis of advanced fibrosis, VCTE was significantly more accurate than the blood tests (area under the receiver operating characteristic curve [AUROC]: 0.840 ± 0.013, p ≤0.005). FibroMeter was the most accurate blood test (AUROC: 0.793 ± 0.015, p ≤0.017). The combinatory test FibroMeterVCTE outperformed VCTE and blood tests (AUROC: 0.866 ± 0.012, p ≤0.005). The sequential combination of FIB4 then FibroMeterVCTE (FIB4-FMVCTE algorithm) or VCTE then FibroMeterVCTE (VCTE-FMVCTE algorithm) provided an excellent diagnostic accuracy of 90% for advanced fibrosis, with liver biopsy only required to confirm the diagnosis in 20% of cases. The FIB4-FMVCTE and VCTE-FMVCTE algorithms were significantly more accurate than the pragmatic algorithms currently proposed. The sequential combination of fibrosis tests in the FIB4-FMVCTE and VCTE-FMVCTE algorithms provides a highly accurate solution for the diagnosis of advanced fibrosis in NAFLD. These algorithms should now be validated for the diagnosis of advanced liver fibrosis in diabetology or primary care settings. Non-alcoholic fatty liver disease (NAFLD), the liver manifestation of the metabolic syndrome linked to obesity and insulin resistance, affects 25% of the general population both in western and developing countries.1As in the other causes of chronic liver disease, liver fibrosis is the main determinant of prognosis in NAFLD.2The risk of liver-related mortality increases from fibrosis stage 2 and is exponentially higher when transitioning to stage F3 (bridging fibrosis) then F4 (cirrhosis).2Therefore, as recommended by international guidelines, patients with NAFLD should be assessed for the presence of advanced F3/4 fibrosis, because of its prognostic implications.3,4
Advanced liver fibrosis is an important diagnostic target in non-alcoholic fatty liver disease (NAFLD) as it defines the subgroup of patients with impaired prognosis. The non-invasive diagnosis of advanced fibrosis is currently limited by the suboptimal positive predictive value and the grey zone (representing indeterminate diagnosis) of fibrosis tests. Here, we aimed to determine the best combination of non-invasive tests for the diagnosis of advanced fibrosis in NAFLD. A total of 938 patients with biopsy-proven NAFLD were randomized 2:1 into derivation and validation sets. All patients underwent liver stiffness measurement with vibration controlled transient elastography (VCTE) and blood fibrosis tests (NAFLD fibrosis score, Fibrosis-4 [FIB4], Fibrotest, Hepascore, FibroMeter). FibroMeterVCTE, which combines VCTE results and FibroMeter markers in a single test, was also calculated in all patients. For the diagnosis of advanced fibrosis, VCTE was significantly more accurate than the blood tests (area under the receiver operating characteristic curve [AUROC]: 0.840 ± 0.013, p ≤0.005). FibroMeter was the most accurate blood test (AUROC: 0.793 ± 0.015, p ≤0.017). The combinatory test FibroMeterVCTE outperformed VCTE and blood tests (AUROC: 0.866 ± 0.012, p ≤0.005). The sequential combination of FIB4 then FibroMeterVCTE (FIB4-FMVCTE algorithm) or VCTE then FibroMeterVCTE (VCTE-FMVCTE algorithm) provided an excellent diagnostic accuracy of 90% for advanced fibrosis, with liver biopsy only required to confirm the diagnosis in 20% of cases. The FIB4-FMVCTE and VCTE-FMVCTE algorithms were significantly more accurate than the pragmatic algorithms currently proposed. The sequential combination of fibrosis tests in the FIB4-FMVCTE and VCTE-FMVCTE algorithms provides a highly accurate solution for the diagnosis of advanced fibrosis in NAFLD. These algorithms should now be validated for the diagnosis of advanced liver fibrosis in diabetology or primary care settings. Only a small number of patients with NAFLD develop advanced liver fibrosis and it is a challenge for physicians to identify them within the large NAFLD population.5Non-invasive tests, mainly blood tests and elastography devices, are now available to facilitate the evaluation of liver fibrosis in chronic liver diseases.A recent meta-analysis showed that non-invasive fibrosis tests can accurately diagnose advanced fibrosis in NAFLD, with an area under the receiver operating characteristic curve (AUROC) around 0.80–0.85.6These tests have excellent negative predictive values to confidently exclude advanced fibrosis, but also have high rates of false positive results, limiting their ability to affirm the diagnosis.6In addition, non-invasive fibrosis tests are usually used with 2 diagnostic thresholds framing a grey zone where the diagnosis remains undetermined.Several studies, mainly performed in chronic viral hepatitis, have shown that combining non-invasive fibrosis tests helps to reduce this grey zone and furthermore increases the positive predictive value of the diagnosis.7–9For example, in the setting of chronic hepatitis C, we have developed the FibroMeterVCTE3G (FMVCTE), which is a combination of the result of transient elastography with the biomarkers of the blood test FibroMeterV2G (FM).10This concept of combining tests remains poorly evaluated in NAFLD.A stepwise algorithm (simple blood test first-line, specialized blood test or elastography second-line) has recently been proposed and is now presented in the slide deck of the guidelines of the European Association for the Study of the Liver (EASL).11,12However, the development of this algorithm was based on a pragmatic approach and literature results, and its diagnostic accuracy has never been evaluated.
Liver fibrosis regression but also progression may occur in patients with autoimmune hepatitis (AIH) under treatment. There is a need for non-invasive surrogate markers for fibrosis development in AIH to better guide immunosuppressive treatment. The aims of the study were to assess the impact of complete biochemical remission defined as normalisation of aminotransferases and IgG on histological activity and fibrosis development, and the value of repeat transient elastography (TE) measurement for monitoring disease progression in AIH. A total of 131 liver biopsies from 60 patients with AIH and more than 900 TE from 125 patients with AIH, 130 with primary biliary cholangitis (PBC) and 100 with primary sclerosing cholangitis (PSC), were evaluated. Time intervals between TE were at least 12 months. Patients with AIH were treated for at least six months at first TE. In contrast to PBC and PSC, a decrease of liver stiffness (LS) was observed in the whole group of patients with AIH (−6.2%/year; 95% CI −12.6% to −0.2%; p = 0.04). The largest decrease of LS was observed in patients with severe fibrosis at baseline (F4: −11.7%/year; 95% CI −19% to −3.5%; p = 0.006). Complete biochemical remission was strongly linked to regression of LS (“remission”: −7.5%/year vs. “no remission”: +1.7%/year, p <0.001). Similarly, complete biochemical remission predicted low histological disease activity and was the only independent predictor for histological fibrosis regression (relative risk 3.66; 95% CI 1.54–10.2; p = 0.001). Patients with F3/F4-fibrosis, who remained in biochemical remission showed a considerable decrease of fibrosis stage (3.7 ± 0.5 to 1.8 ± 1.7; p = 0.007) on histological follow-up. This study demonstrates that complete biochemical remission is a reliable predictor of a good prognosis in AIH and leads to fibrosis regression that can be monitored by TE. Autoimmune hepatitis (AIH) is a chronic disease, which harbours a significant risk of fibrosis progression when treatment is insufficient.1,2Some patients may progress despite immunosuppressive treatment.3,4Therefore, AIH patients require close, life-long follow-up.In daily practice, treatment monitoring is mainly guided by biochemical surrogate markers for hepatic inflammation.Besides serum aminotransferases, it has been demonstrated that selectively elevated IgG levels also indicate ongoing inflammatory activity in AIH.5Therefore, complete biochemical remission has been defined as repeatedly normal serum aminotransferase and IgG levels.Although the prognostic relevance of this definition is uncertain, current treatment guidelines have incorporated this definition as the goal of immunosuppressive treatment in AIH.6–8Since complete biochemical remission is hard to achieve in some patients and requires high doses of immunosuppressants in others, it is important to validate the prognostic significance of this definition.
Liver fibrosis regression but also progression may occur in patients with autoimmune hepatitis (AIH) under treatment. There is a need for non-invasive surrogate markers for fibrosis development in AIH to better guide immunosuppressive treatment. The aims of the study were to assess the impact of complete biochemical remission defined as normalisation of aminotransferases and IgG on histological activity and fibrosis development, and the value of repeat transient elastography (TE) measurement for monitoring disease progression in AIH. A total of 131 liver biopsies from 60 patients with AIH and more than 900 TE from 125 patients with AIH, 130 with primary biliary cholangitis (PBC) and 100 with primary sclerosing cholangitis (PSC), were evaluated. Time intervals between TE were at least 12 months. Patients with AIH were treated for at least six months at first TE. In contrast to PBC and PSC, a decrease of liver stiffness (LS) was observed in the whole group of patients with AIH (−6.2%/year; 95% CI −12.6% to −0.2%; p = 0.04). The largest decrease of LS was observed in patients with severe fibrosis at baseline (F4: −11.7%/year; 95% CI −19% to −3.5%; p = 0.006). Complete biochemical remission was strongly linked to regression of LS (“remission”: −7.5%/year vs. “no remission”: +1.7%/year, p <0.001). Similarly, complete biochemical remission predicted low histological disease activity and was the only independent predictor for histological fibrosis regression (relative risk 3.66; 95% CI 1.54–10.2; p = 0.001). Patients with F3/F4-fibrosis, who remained in biochemical remission showed a considerable decrease of fibrosis stage (3.7 ± 0.5 to 1.8 ± 1.7; p = 0.007) on histological follow-up. This study demonstrates that complete biochemical remission is a reliable predictor of a good prognosis in AIH and leads to fibrosis regression that can be monitored by TE. Monitoring treatment response and disease progression by biochemical markers may have limitations, since a proportion of patients may display relevant inflammatory activity on liver histology despite normal biochemical markers.4,5In addition, AIH is characterised by a fluctuating disease course, which may mask repeated flares and disease progression if laboratory values are controlled infrequently.Therefore, although not generally recommended by current guidelines, many experts perform follow-up liver biopsies to assess inflammatory activity and disease progression.However, liver biopsies are invasive procedures bearing a residual risk of severe complications and are of limited acceptance among patients.
Liver fibrosis regression but also progression may occur in patients with autoimmune hepatitis (AIH) under treatment. There is a need for non-invasive surrogate markers for fibrosis development in AIH to better guide immunosuppressive treatment. The aims of the study were to assess the impact of complete biochemical remission defined as normalisation of aminotransferases and IgG on histological activity and fibrosis development, and the value of repeat transient elastography (TE) measurement for monitoring disease progression in AIH. A total of 131 liver biopsies from 60 patients with AIH and more than 900 TE from 125 patients with AIH, 130 with primary biliary cholangitis (PBC) and 100 with primary sclerosing cholangitis (PSC), were evaluated. Time intervals between TE were at least 12 months. Patients with AIH were treated for at least six months at first TE. In contrast to PBC and PSC, a decrease of liver stiffness (LS) was observed in the whole group of patients with AIH (−6.2%/year; 95% CI −12.6% to −0.2%; p = 0.04). The largest decrease of LS was observed in patients with severe fibrosis at baseline (F4: −11.7%/year; 95% CI −19% to −3.5%; p = 0.006). Complete biochemical remission was strongly linked to regression of LS (“remission”: −7.5%/year vs. “no remission”: +1.7%/year, p <0.001). Similarly, complete biochemical remission predicted low histological disease activity and was the only independent predictor for histological fibrosis regression (relative risk 3.66; 95% CI 1.54–10.2; p = 0.001). Patients with F3/F4-fibrosis, who remained in biochemical remission showed a considerable decrease of fibrosis stage (3.7 ± 0.5 to 1.8 ± 1.7; p = 0.007) on histological follow-up. This study demonstrates that complete biochemical remission is a reliable predictor of a good prognosis in AIH and leads to fibrosis regression that can be monitored by TE. Therefore, non-invasive surrogate markers of disease progression are urgently needed to better guide immunosuppressive treatment.We have recently validated liver stiffness (LS) measurement by transient elastography (TE) as a non-invasive tool to assess liver fibrosis in treated AIH.9TE is currently the only validated non-invasive method to estimate liver fibrosis in AIH with a reliable accuracy and reproducibility.9,10In the cholestatic autoimmune liver diseases, primary biliary cholangitis (PBC) and primary sclerosing cholangitis (PSC), it could be demonstrated that changes in LS are strongly linked with disease progression and outcome.11–13However, there are no data assessing progression rates of LS in AIH.
Treatment of liver cancer remains challenging because of a paucity of drugs that target critical dependencies. Sorafenib is a multikinase inhibitor that is approved as the standard therapy for patients with advanced hepatocellular carcinoma, but it only provides limited survival benefit. In this study we aimed to identify potential combination therapies to improve the clinical response to sorafenib. To investigate the cause of the limited therapeutic effect of sorafenib, we performed a CRISPR-Cas9 based synthetic lethality screen to search for kinases whose knockout synergizes with sorafenib. Synergistic effects of sorafenib and selumetinib on cell apoptosis and phospho-ERK (p-ERK) were analyzed by caspase-3/7 apoptosis assay and western blot, respectively. p-ERK was measured by immunochemical analysis using a tissue microarray containing 78 liver cancer specimens. The in vivo effects of the combination were also measured in two xenograft models. We found that suppression of ERK2 (MAPK1) sensitizes several liver cancer cell lines to sorafenib. Drugs inhibiting the MEK (MEK1/2 [MAP2K1/2]) or ERK (ERK1/2 [MAPK1/3]) kinases reverse unresponsiveness to sorafenib in vitro and in vivo in a subset of liver cancer cell lines characterized by high levels of active p-ERK, through synergistic inhibition of ERK kinase activity. Our data provide a combination strategy for treating liver cancer and suggest that tumors with high basal p-ERK levels, which are seen in approximately 30% of liver cancers, are most likely to benefit from such combinatorial treatment. Liver cancer is one of the most frequent malignancies and the second leading cause of cancer-related deaths worldwide.1In the last decade, our understanding of the genetic landscape of hepatocellular carcinoma (HCC) has improved significantly through large-scale sequencing studies.These studies have indicated that several signaling pathways are involved in HCC initiation and progression, including telomere maintenance, WNT-β-catenin pathway, cell cycle regulators, epigenetic regulators, AKT/mTOR and mitogen-activated protein kinase (MAPK) pathways.2However, the most frequent mutations in HCC are currently undruggable.Sorafenib is a multikinase inhibitor that is approved as the standard therapy for advanced HCC patients, but only provides 2.8 months survival benefit.3The survival advantage is even more limited for Asia-Pacific patients (2.3 months).4Regorafenib is a treatment strategy shown to provide overall survival benefit in patients with HCC that has progressed on sorafenib treatment.5However, chemically, regorafenib and sorafenib differ by just one atom.6The regorafenib registration trial design involved a high degree of patient selection.Even in such optimal settings, only a modest clinical benefit was achieved.Lenvatinib is another multikinase inhibitor, which provided impressive antitumor activity in phase II trials in patients with advanced unresectable HCC.7However, there is only 1.3 months additional survival benefit for patients compared to sorafenib in the phase III trials.These clinical data point to the need to reconsider the current therapeutic strategy.A recurrent problem in clinical studies in liver cancer is the paucity of biomarkers linked to drug response.8
Treatment of liver cancer remains challenging because of a paucity of drugs that target critical dependencies. Sorafenib is a multikinase inhibitor that is approved as the standard therapy for patients with advanced hepatocellular carcinoma, but it only provides limited survival benefit. In this study we aimed to identify potential combination therapies to improve the clinical response to sorafenib. To investigate the cause of the limited therapeutic effect of sorafenib, we performed a CRISPR-Cas9 based synthetic lethality screen to search for kinases whose knockout synergizes with sorafenib. Synergistic effects of sorafenib and selumetinib on cell apoptosis and phospho-ERK (p-ERK) were analyzed by caspase-3/7 apoptosis assay and western blot, respectively. p-ERK was measured by immunochemical analysis using a tissue microarray containing 78 liver cancer specimens. The in vivo effects of the combination were also measured in two xenograft models. We found that suppression of ERK2 (MAPK1) sensitizes several liver cancer cell lines to sorafenib. Drugs inhibiting the MEK (MEK1/2 [MAP2K1/2]) or ERK (ERK1/2 [MAPK1/3]) kinases reverse unresponsiveness to sorafenib in vitro and in vivo in a subset of liver cancer cell lines characterized by high levels of active p-ERK, through synergistic inhibition of ERK kinase activity. Our data provide a combination strategy for treating liver cancer and suggest that tumors with high basal p-ERK levels, which are seen in approximately 30% of liver cancers, are most likely to benefit from such combinatorial treatment. Combination therapies can help fight therapy failure and improve response to approved drugs.We have successfully employed functional genetic screens to find powerful combinations of cancer drugs by exploiting the concept of ‘synthetic lethality’.This has resulted in clinical testing of several combination treatment strategies, including the use of BRAF inhibitor combined with epidermal growth factor receptor (EGFR) inhibitor in BRAF(V600E) mutant colon cancer9 (NCT01719380, NCT01791309, NCT02164916, NCT02928224) and MEK (MEK1/2 [MAP2K1/2]) inhibitor combined with EGFR/ERBB2 inhibitor in KRAS mutant cancer10 (NCT02039336, NCT02230553, NCT02450656).Some of these studies have already revealed promising clinical activity in these difficult-to-treat patient populations.11For liver cancer, the concept of ‘synthetic lethality’ has also been validated by the combination of sorafenib and MAPK14 inhibition, which was identified by an in vivo RNAi screen.12
Treatment of liver cancer remains challenging because of a paucity of drugs that target critical dependencies. Sorafenib is a multikinase inhibitor that is approved as the standard therapy for patients with advanced hepatocellular carcinoma, but it only provides limited survival benefit. In this study we aimed to identify potential combination therapies to improve the clinical response to sorafenib. To investigate the cause of the limited therapeutic effect of sorafenib, we performed a CRISPR-Cas9 based synthetic lethality screen to search for kinases whose knockout synergizes with sorafenib. Synergistic effects of sorafenib and selumetinib on cell apoptosis and phospho-ERK (p-ERK) were analyzed by caspase-3/7 apoptosis assay and western blot, respectively. p-ERK was measured by immunochemical analysis using a tissue microarray containing 78 liver cancer specimens. The in vivo effects of the combination were also measured in two xenograft models. We found that suppression of ERK2 (MAPK1) sensitizes several liver cancer cell lines to sorafenib. Drugs inhibiting the MEK (MEK1/2 [MAP2K1/2]) or ERK (ERK1/2 [MAPK1/3]) kinases reverse unresponsiveness to sorafenib in vitro and in vivo in a subset of liver cancer cell lines characterized by high levels of active p-ERK, through synergistic inhibition of ERK kinase activity. Our data provide a combination strategy for treating liver cancer and suggest that tumors with high basal p-ERK levels, which are seen in approximately 30% of liver cancers, are most likely to benefit from such combinatorial treatment. CRISPR-Cas9 is a powerful gene-editing technology that has been widely employed for genome-scale functional screens.13Compared to the shRNA-based system, CRISPR technology provides advantages in low noise, minimal off-target effects and high consistent activity.14Taking advantage of the CRISPR-Cas9 based functional screening system, we developed a platform that can be used to identify kinases whose inhibition increases the therapeutic efficacy of cancer drugs.
CD100 is constitutively expressed on T cells and can be cleaved from the cell surface by matrix metalloproteases (MMPs) to become soluble CD100 (sCD100). Both membrane-bound CD100 (mCD100) and sCD100 have important immune regulatory functions that promote immune cell activation and responses. This study investigated the expression and role of mCD100 and sCD100 in regulating antiviral immune responses during HBV infection. mCD100 expression on T cells, sCD100 levels in the serum, and MMP expression in the liver and serum were analysed in patients with chronic HBV (CHB) and in HBV-replicating mice. The ability of sCD100 to mediate antigen-presenting cell maturation, HBV-specific T cell activation, and HBV clearance were analysed in HBV-replicating mice and patients with CHB. Patients with CHB had higher mCD100 expression on T cells and lower serum sCD100 levels compared with healthy controls. Therapeutic sCD100 treatment resulted in the activation of DCs and liver sinusoidal endothelial cells, enhanced HBV-specific CD8 T cell responses, and accelerated HBV clearance, whereas blockade of its receptor CD72 attenuated the intrahepatic anti-HBV CD8 T cell response. Together with MMP9, MMP2 mediated mCD100 shedding from the T cell surface. Patients with CHB had significantly lower serum MMP2 levels, which positively correlated with serum sCD100 levels, compared with healthy controls. Inhibition of MMP2/9 activity resulted in an attenuated anti-HBV T cell response and delayed HBV clearance in mice. MMP2/9-mediated sCD100 release has an important role in regulating intrahepatic anti-HBV CD8 T cell responses, thus mediating subsequent viral clearance during HBV infection. Chronic HBV infection continues to be a major public health burden worldwide.The persistence of HBV infection increases the risk of end-stage liver diseases, such as liver cirrhosis and hepatocellular carcinoma.1Exposure to HBV in neonates usually leads to viral persistence, whereas most infected adults spontaneously clear the virus.2The clearance of HBV relies largely on a potent and diverse T cell immune response, which usually becomes dysregulated in chronic HBV infection.3–5However, the mechanism by which a favourable anti-HBV T cell response is generated in an infected individual remains largely unknown.6
CD100 is constitutively expressed on T cells and can be cleaved from the cell surface by matrix metalloproteases (MMPs) to become soluble CD100 (sCD100). Both membrane-bound CD100 (mCD100) and sCD100 have important immune regulatory functions that promote immune cell activation and responses. This study investigated the expression and role of mCD100 and sCD100 in regulating antiviral immune responses during HBV infection. mCD100 expression on T cells, sCD100 levels in the serum, and MMP expression in the liver and serum were analysed in patients with chronic HBV (CHB) and in HBV-replicating mice. The ability of sCD100 to mediate antigen-presenting cell maturation, HBV-specific T cell activation, and HBV clearance were analysed in HBV-replicating mice and patients with CHB. Patients with CHB had higher mCD100 expression on T cells and lower serum sCD100 levels compared with healthy controls. Therapeutic sCD100 treatment resulted in the activation of DCs and liver sinusoidal endothelial cells, enhanced HBV-specific CD8 T cell responses, and accelerated HBV clearance, whereas blockade of its receptor CD72 attenuated the intrahepatic anti-HBV CD8 T cell response. Together with MMP9, MMP2 mediated mCD100 shedding from the T cell surface. Patients with CHB had significantly lower serum MMP2 levels, which positively correlated with serum sCD100 levels, compared with healthy controls. Inhibition of MMP2/9 activity resulted in an attenuated anti-HBV T cell response and delayed HBV clearance in mice. MMP2/9-mediated sCD100 release has an important role in regulating intrahepatic anti-HBV CD8 T cell responses, thus mediating subsequent viral clearance during HBV infection. The semaphorin family, which includes more than 30 secreted or transmembrane proteins, was initially found to be essential for proper neuronal development.7However, several semaphorins are also expressed by immune cells, among which semaphorin 4D (also called CD100) has attracted particular attention because of its extensive functions in the immune system.8In lymphoid tissues, membrane CD100 (mCD100) is expressed abundantly on the surface of resting T cells and weakly on antigen-presenting cells (APCs), such as B cells and dendritic cells (DCs).9The expression of mCD100 on immune cells increases upon cell activation, which subsequently induces shedding of the CD100 extracellular domain via proteolytic cleavage by certain matrix metalloproteinases (MMPs).This gives rise to a 120-kDa soluble form of CD100 (sCD100).10–12CD100 functions as a ligand and binds to different receptors, including CD72 on lymphoid tissues and plexin-B1/B2 on non-lymphoid tissues.13–15Through binding with CD72, which is expressed on most professional APCs, CD100 exerts essential functions in DC maturation, B cell proliferation, and antibody production, and, subsequently, in T cell priming and activation during the course of the T cell–APC interaction.8DCs from CD100-deficient mice display poor immunogenicity and are defected in costimulatory molecule expression and IL-12 production upon CD40 stimulation.16Treatment with exogenous sCD100 restored the normal functions of CD100-deficient DCs and further enhanced the functionality of wild-type DCs.16Although CD100-deficient T cells showed normal proliferative responses to anti-CD3 or concanavalin A stimulation in vitro, the in vivo generation of antigen-specific T cells was impaired in CD100-deficient mice.17This defective T cell priming was rescued by the administration of recombinant sCD100 protein.17In line with these findings, numerous studies have demonstrated the involvement of CD100 in inflammation and infectious diseases.10For example, CD100-deficient mice were protected from developing experimental autoimmune encephalomyelitis (EAE)16 and glomerulonephritis,18 whereas serum sCD100 levels positively correlated with autoantibody levels in mice that developed systemic autoimmune disease.19Levels of sCD100 were elevated in both serum and synovial fluid from patients with rheumatoid arthritis (RA) and were positively correlated with disease activity markers.20Altered mCD100/sCD00 expression has also been reported in viral infections.Serum sCD100 levels were increased and positively correlated with alanine aminotransferase (ALT) levels in patients with acute HCV infection.21Similarly, during the acute phase of haemorrhagic fever with renal syndrome (HFRS) caused by Hantan virus infection, plasma sCD100 levels were elevated and positively correlated with disease severity.22In patients with HIV, decreased mCD100 expression on CD4 and CD8 T cells in peripheral blood has been observed, and the loss of mCD100 expression on T cells was related to the immune activation status and viral loads.23
Hepatocellular carcinoma (HCC) staging according to the Barcelona Clinical Liver Cancer (BCLC) classification is based on conventional imaging. The aim of our study was to assess the impact of dual-tracer 18F-fluorocholine and 18F-fluorodeoxyglucose positron emission tomography/computed tomography (PET/CT) on tumor staging and treatment allocation. A total of 192 dual-tracer PET/CT scans (18F-fluorocholine and 18F-fluorodeoxyglucose PET/CT) were performed in 177 patients with HCC. BCLC staging and treatment proposal were retrospectively collected based on conventional imaging, along with any new lesions detected, and changes in BCLC classification or treatment allocation based on dual-tracer PET/CT. Patients were primarily men (87.5%) with cirrhosis (71%) due to alcohol ± non-alcoholic steatohepatitis (26%), viral infection (62%) or unknown causes (12%). Among 122 patients with PET/CT performed for staging, BCLC stage based on conventional imaging was 0/A in 61 patients (50%), B in 32 patients (26%) and C in 29 patients (24%). Dual-tracer PET/CT detected new lesions in 26 patients (21%), upgraded BCLC staging in 14 (11%) and modified treatment strategy in 17 (14%). In addition, dual-tracer PET/CT modified the final treatment in 4/9 (44%) patients with unexplained elevation of alpha-fetoprotein (AFP), 10/25 patients (40%) with doubtful lesions on conventional imaging and 3/36 patients (8%) waiting for liver transplantation without active HCC after tumor response following bridging therapy. When used for HCC staging, dual-tracer PET/CT enabled BCLC upgrading and treatment modification in 11% and 14% of patients, respectively. Dual-tracer PET/CT might also be useful in specific situations (an unexplained rise in AFP, doubtful lesions or pre-transplant evaluation of patients without active HCC). Hepatocellular carcinoma (HCC) is one of the leading causes of death from cancer worldwide, developing on cirrhosis in 95% of cases.1In this setting, the diagnosis of HCC relies on non-invasive conventional imaging criteria (computed tomography [CT] and/or magnetic resonance imaging [MRI]) and/or histology.2–4Conventional imaging (liver CT and/or MRI and lung CT) enables HCC staging according to the Barcelona Clinical Liver Cancer (BCLC) system, linking each stage to therapeutic modalities.1,5Accurate staging with reliable imaging methods is therefore crucial to determine the best treatment strategy.6
Hepatocellular carcinoma (HCC) staging according to the Barcelona Clinical Liver Cancer (BCLC) classification is based on conventional imaging. The aim of our study was to assess the impact of dual-tracer 18F-fluorocholine and 18F-fluorodeoxyglucose positron emission tomography/computed tomography (PET/CT) on tumor staging and treatment allocation. A total of 192 dual-tracer PET/CT scans (18F-fluorocholine and 18F-fluorodeoxyglucose PET/CT) were performed in 177 patients with HCC. BCLC staging and treatment proposal were retrospectively collected based on conventional imaging, along with any new lesions detected, and changes in BCLC classification or treatment allocation based on dual-tracer PET/CT. Patients were primarily men (87.5%) with cirrhosis (71%) due to alcohol ± non-alcoholic steatohepatitis (26%), viral infection (62%) or unknown causes (12%). Among 122 patients with PET/CT performed for staging, BCLC stage based on conventional imaging was 0/A in 61 patients (50%), B in 32 patients (26%) and C in 29 patients (24%). Dual-tracer PET/CT detected new lesions in 26 patients (21%), upgraded BCLC staging in 14 (11%) and modified treatment strategy in 17 (14%). In addition, dual-tracer PET/CT modified the final treatment in 4/9 (44%) patients with unexplained elevation of alpha-fetoprotein (AFP), 10/25 patients (40%) with doubtful lesions on conventional imaging and 3/36 patients (8%) waiting for liver transplantation without active HCC after tumor response following bridging therapy. When used for HCC staging, dual-tracer PET/CT enabled BCLC upgrading and treatment modification in 11% and 14% of patients, respectively. Dual-tracer PET/CT might also be useful in specific situations (an unexplained rise in AFP, doubtful lesions or pre-transplant evaluation of patients without active HCC). Positron emission tomography-CT (PET/CT) using 18F-fluorodeoxyglucose (18F-FDG), a marker of glycolysis in cells, is used for staging and assessment of treatment response in several cancers.718F-FDG PET/CT has been proposed to detect poorly differentiated HCC,7,8 but is limited by its inability to detect well-differentiated HCC; consequently, it is not currently recommended for HCC staging.7,9In contrast, PET/CT using 18F-fluorocholine (18F-FCH), a precursor of phospholipid synthesis, could be a useful marker of well-differentiated HCC.10,11Therefore, dual-tracer PET/CT, using both 18F-FDG and 18F-FCH, is likely to detect both well- and poorly differentiated HCC, and might be valuable for tumor staging before treatment.11–13However, to date, the role of 18F-FCH in HCC staging has been assessed only in small studies that included a limited number of patients.Thus, the added value of dual-tracer PET/CT compared to conventional imaging remains unknown.14–18
Obeticholic acid (OCA), a farnesoid X receptor agonist, increases total and low-density lipoprotein cholesterol (LDL-C) in patients with non-alcoholic steatohepatitis. In the present study, we aimed to evaluate the impact of OCA therapy on lipoprotein sub-particles. This study included 196 patients (99 OCA group and 97 placebo group) who were enrolled in the FLINT trial and had samples available for lipid analysis and liver biopsies at enrollment and end-of-treatment (EOT) at 72 weeks. Very low-density lipoprotein (VLDL), low-density lipoprotein (LDL), and high-density lipoprotein (HDL) particles were evaluated at baseline, 12 and 72 weeks after randomization, and 24 weeks following EOT. Baseline lipoprotein profiles were similar among OCA and placebo groups. OCA did not affect total VLDL particle concentrations, but OCA vs. placebo treatment was associated with decreased large VLDL particle concentration at 12 weeks (baseline-adjusted mean: 6.8 vs. 8.9 nmol/L; p = 0.002), mirrored by an increase in less atherogenic, small VLDL particle concentration (33.9 vs. 28.0 nmol/L; p = 0.02). After 12 weeks, total LDL particle concentration was higher in the OCA group than the placebo group (1,667 vs. 1,329 nmol/L; p <0.0001), characterized by corresponding increases in both less atherogenic, large-buoyant LDL (475 vs. 308 nmol/L; p ≤0.001) and more atherogenic small-dense LDL particles (1,015 vs. 872 nmol/L; p = 0.002). The changes in LDL particle concentrations were similar between treatment groups (OCA and placebo) 24 weeks following EOT due to improvement in the OCA cohort. Compared to placebo, a reduction in total HDL particle concentration, particularly large and medium HDL particles, was noted in the OCA-treated patients, but this resolved after drug discontinuation. OCA therapy is associated with increases in small VLDL particles, large and small LDL particles, and a reduction in HDL particles at 12 weeks. These lipoprotein concentrations reverted to baseline values 24 weeks after drug discontinuation. Non-alcoholic fatty liver disease (NAFLD) is the most common etiology of chronic liver disease, affecting nearly a third of the United States population.1Non-alcoholic steatohepatitis (NASH) is the clinically aggressive variant of NAFLD that is characterized histologically by hepatic steatosis along with necro-inflammatory activity.1In the absence of approved pharmacological therapy, NASH is increasingly being targeted for drug development efforts.3Obeticholic acid (OCA) is a farnesoid X receptor (FXR) agonist that was recently shown to improve liver histology in patients with NASH in a randomized, placebo-controlled clinical trial.4FXR is a bile acid-binding transcription factor belonging to the super family of nuclear receptors.Because of its central role in inflammation, glucose and lipid metabolism, FXR agonism is an attractive therapeutic target in NASH.
Obeticholic acid (OCA), a farnesoid X receptor agonist, increases total and low-density lipoprotein cholesterol (LDL-C) in patients with non-alcoholic steatohepatitis. In the present study, we aimed to evaluate the impact of OCA therapy on lipoprotein sub-particles. This study included 196 patients (99 OCA group and 97 placebo group) who were enrolled in the FLINT trial and had samples available for lipid analysis and liver biopsies at enrollment and end-of-treatment (EOT) at 72 weeks. Very low-density lipoprotein (VLDL), low-density lipoprotein (LDL), and high-density lipoprotein (HDL) particles were evaluated at baseline, 12 and 72 weeks after randomization, and 24 weeks following EOT. Baseline lipoprotein profiles were similar among OCA and placebo groups. OCA did not affect total VLDL particle concentrations, but OCA vs. placebo treatment was associated with decreased large VLDL particle concentration at 12 weeks (baseline-adjusted mean: 6.8 vs. 8.9 nmol/L; p = 0.002), mirrored by an increase in less atherogenic, small VLDL particle concentration (33.9 vs. 28.0 nmol/L; p = 0.02). After 12 weeks, total LDL particle concentration was higher in the OCA group than the placebo group (1,667 vs. 1,329 nmol/L; p <0.0001), characterized by corresponding increases in both less atherogenic, large-buoyant LDL (475 vs. 308 nmol/L; p ≤0.001) and more atherogenic small-dense LDL particles (1,015 vs. 872 nmol/L; p = 0.002). The changes in LDL particle concentrations were similar between treatment groups (OCA and placebo) 24 weeks following EOT due to improvement in the OCA cohort. Compared to placebo, a reduction in total HDL particle concentration, particularly large and medium HDL particles, was noted in the OCA-treated patients, but this resolved after drug discontinuation. OCA therapy is associated with increases in small VLDL particles, large and small LDL particles, and a reduction in HDL particles at 12 weeks. These lipoprotein concentrations reverted to baseline values 24 weeks after drug discontinuation. Mechanistically, FXR activation reduces triglyceride-rich lipoproteins via repression of hepatic sterol regulatory element binding protein 1c (SREBP1c), microsomal triglyceride transfer protein (MTTP), and apolipoprotein B (apoB) gene expression.5–7However, it also reduces cholesterol conversion to bile acids, which is a major mechanism of cholesterol disposal.FXR agonism via OCA in the Farnesoid X Receptor Ligand Obeticholic Acid in NASH Treatment (FLINT) trial led to an increase in low-density lipoprotein cholesterol (LDL-C) and reduction in high-density lipoprotein cholesterol (HDL-C).4,8Since cardiovascular disease (CVD) is common among patients with NAFLD and the leading cause of mortality among patients with NAFLD,9–11 this rise in serum LDL-C requires a deeper understanding.
Robust data on hepatocellular carcinoma (HCC) incidence among HIV/hepatitis B virus (HBV)-coinfected individuals on antiretroviral therapy (ART) are needed to inform HCC screening strategies. We aimed to evaluate the incidence and risk factors of HCC among HIV/HBV-coinfected individuals on tenofovir disoproxil fumarate (TDF)-containing ART in a large multi-cohort study. We included all HIV-infected adults with a positive hepatitis B surface antigen test followed in 4 prospective European cohorts. The primary outcome was the occurrence of HCC. Demographic and clinical information was retrieved from routinely collected data, and liver cirrhosis was defined according to results from liver biopsy or non-invasive measurements. Multivariable Poisson regression was used to assess HCC risk factors. A total of 3,625 HIV/HBV-coinfected patients were included, of whom 72% had started TDF-containing ART. Over 32,673 patient-years (py), 60 individuals (1.7%) developed an HCC. The incidence of HCC remained stable over time among individuals on TDF, whereas it increased steadily among those not on TDF. Among individuals on TDF, the incidence of HCC was 5.9 per 1,000 py (95% CI 3.60–9.10) in cirrhotics and 1.17 per 1,000 py (0.56–2.14) among non-cirrhotics. Age at initiation of TDF (adjusted incidence rate ratio per 10-year increase: 2.2, 95% CI 1.6–3.0) and the presence of liver cirrhosis (4.5, 2.3–8.9) were predictors of HCC. Among non-cirrhotic individuals, the incidence of HCC was only above the commonly used screening threshold of 2 cases per 1,000 py in patients aged >45 years old at TDF initiation. Whereas the incidence of HCC was high in cirrhotic HIV/HBV-coinfected individuals, it remained below the HCC screening threshold in patients without cirrhosis who started TDF aged <46 years old. Hepatitis B virus (HBV) infection is the most important cause of liver cirrhosis and hepatocellular carcinoma (HCC) worldwide.1In high-income settings, between 5 and 10% of HIV-infected individuals are coinfected with HBV, which is a major cause of severe morbidity and mortality in this population.2HIV infection accelerates the progression of HBV-related liver disease and mortality is higher among HIV/HBV-coinfected individuals compared to HBV-monoinfected ones.3While the incidence of HBV-related HCC is estimated to range between 0.1 and 0.4% per year among non-cirrhotics and to be above 3% per year among cirrhotics, it is uncertain if the risk of developing HCC is different among HIV/HBV-coinfected individuals.4In fact, many factors which have a profound impact on HBV-related HCC incidence, including age, sex, liver cirrhosis, HBV viral load, hepatitis B e antigen (HBeAg) and hepatitis delta virus (HDV)-coinfection, are distributed differently among HIV-infected individuals compared to HIV-uninfected ones.5–8
Robust data on hepatocellular carcinoma (HCC) incidence among HIV/hepatitis B virus (HBV)-coinfected individuals on antiretroviral therapy (ART) are needed to inform HCC screening strategies. We aimed to evaluate the incidence and risk factors of HCC among HIV/HBV-coinfected individuals on tenofovir disoproxil fumarate (TDF)-containing ART in a large multi-cohort study. We included all HIV-infected adults with a positive hepatitis B surface antigen test followed in 4 prospective European cohorts. The primary outcome was the occurrence of HCC. Demographic and clinical information was retrieved from routinely collected data, and liver cirrhosis was defined according to results from liver biopsy or non-invasive measurements. Multivariable Poisson regression was used to assess HCC risk factors. A total of 3,625 HIV/HBV-coinfected patients were included, of whom 72% had started TDF-containing ART. Over 32,673 patient-years (py), 60 individuals (1.7%) developed an HCC. The incidence of HCC remained stable over time among individuals on TDF, whereas it increased steadily among those not on TDF. Among individuals on TDF, the incidence of HCC was 5.9 per 1,000 py (95% CI 3.60–9.10) in cirrhotics and 1.17 per 1,000 py (0.56–2.14) among non-cirrhotics. Age at initiation of TDF (adjusted incidence rate ratio per 10-year increase: 2.2, 95% CI 1.6–3.0) and the presence of liver cirrhosis (4.5, 2.3–8.9) were predictors of HCC. Among non-cirrhotic individuals, the incidence of HCC was only above the commonly used screening threshold of 2 cases per 1,000 py in patients aged >45 years old at TDF initiation. Whereas the incidence of HCC was high in cirrhotic HIV/HBV-coinfected individuals, it remained below the HCC screening threshold in patients without cirrhosis who started TDF aged <46 years old. International guidelines recommend the initiation of tenofovir-containing antiretroviral therapy (ART) in all HIV-infected individuals who are positive for hepatitis B surface antigen (HBsAg).Tenofovir disoproxil fumarate (TDF) is very successful in suppressing HBV replication, independent of HIV infection, although treatment response seems to be slightly delayed in HIV/HBV-coinfected individuals.8–10Importantly, no clinically relevant HBV-related resistance to TDF has been described to date.11The use of the potent nucleos(t)ide analogs (NAs) TDF and entecavir (ETV) has been associated with a reduced incidence of HCC among HBV-infected patients in Europe and Asia.12,13Among 325 cirrhotic HBV-infected patients treated with TDF or ETV, the HCC incidence rate within the first 5 years (3.22% per year) was higher than that beyond 5 years of therapy (1.57% per year), suggesting a potential decrease in HCC risk with long-term suppression of HBV replication.12Furthermore, recent data from South Korea suggest that HCC incidence is further decreased, albeit not eliminated, in the event of HBsAg loss during NA therapy, also defined as the functional cure of HBV infection.14
Chronic alcohol (EtOH) consumption is a leading risk factor for development of hepatocellular carcinoma (HCC), which is associated with marked increase of hepatic expression of pro-inflammatory IL-17A and its receptor IL-17RA. Genetic deletion and pharmacological blocking was used to characterize the role of IL-17A/IL-17RA signaling in the pathogenesis of HCC. We demonstrate that global deletion of IL-17RA gene suppressed HCC in alcohol-fed DEN-challenged IL-17RA-/- and Mup-uPA/IL-17RA-/- mice compared to wild type mice. When the cell-specific role of IL-17RA signaling was examined, development of HCC was decreased in both alcohol-fed IL-17RAΔMΦ and IL-17RAΔHep mice devoid of IL-17RA in myeloid cells and hepatocytes, but not in IL-17RAΔHSCs mice (deficient of IL-17RA in hepatic stellate cells (HSCs)). Deletion of IL-17RA in myeloid cells ameliorated tumorigenesis via suppression of pro-tumorigenic/inflammatory and pro-fibrogenic responses in alcohol-fed IL-17RAΔMΦ mice. Remarkably, despite a normal inflammatory response, alcohol-fed IL-17RAΔHep mice developed the fewest tumors (compared to IL-17RAΔMΦ mice), with reduced steatosis and fibrosis. Steatotic IL-17RA-deficient hepatocytes downregulated expression of Cxcl1 and other chemokines, exhibited a striking defect in TNF-TNFR1-dependent Caspase-2-SREBP-1/2-DHCR7-mediated cholesterol synthesis, and upregulated production of anti-oxidant Vitamin D3. Pharmacological blocking of IL-17A/Th-17 cells using anti-IL-12/IL-23 Ab suppressed progression of HCC (by 70%) in alcohol-fed mice, indicating that targeting IL-17 signaling might provide novel strategies for treatment of alcohol-induced HCC. Overall, IL-17A is as a tumor promoting cytokine, which critically regulates alcohol-induced hepatic steatosis, inflammation, fibrosis, and HCC. Hepatocellular carcinoma (HCC) develops in response to chronic injury, such as HBV/HCV infections, and metabolic diseases including non-alcoholic steatohepatitis (NASH), and alcoholic liver disease (ALD).With the introduction of new therapies, the incidence of HBV/HCV-related HCC has declined1, while NASH- and ALD-induced HCC are rapidly rising, both of which typically progress from hepatic steatosis to steatohepatitis, fibrosis, cirrhosis, and cancer2.ALD remains a major risk factor of hepatic cirrhosis and aggressive HCC2.Consistent with the global epidemic of NASH, alcohol-induced HCC often occurs in patients with BMI > 253.Chronic alcohol consumption is believed to increase development of HCC in patients with NASH4, therefore, we used an experimental model of ALD and Western Diet which reflects the typical American patient with ALD.
Chronic alcohol (EtOH) consumption is a leading risk factor for development of hepatocellular carcinoma (HCC), which is associated with marked increase of hepatic expression of pro-inflammatory IL-17A and its receptor IL-17RA. Genetic deletion and pharmacological blocking was used to characterize the role of IL-17A/IL-17RA signaling in the pathogenesis of HCC. We demonstrate that global deletion of IL-17RA gene suppressed HCC in alcohol-fed DEN-challenged IL-17RA-/- and Mup-uPA/IL-17RA-/- mice compared to wild type mice. When the cell-specific role of IL-17RA signaling was examined, development of HCC was decreased in both alcohol-fed IL-17RAΔMΦ and IL-17RAΔHep mice devoid of IL-17RA in myeloid cells and hepatocytes, but not in IL-17RAΔHSCs mice (deficient of IL-17RA in hepatic stellate cells (HSCs)). Deletion of IL-17RA in myeloid cells ameliorated tumorigenesis via suppression of pro-tumorigenic/inflammatory and pro-fibrogenic responses in alcohol-fed IL-17RAΔMΦ mice. Remarkably, despite a normal inflammatory response, alcohol-fed IL-17RAΔHep mice developed the fewest tumors (compared to IL-17RAΔMΦ mice), with reduced steatosis and fibrosis. Steatotic IL-17RA-deficient hepatocytes downregulated expression of Cxcl1 and other chemokines, exhibited a striking defect in TNF-TNFR1-dependent Caspase-2-SREBP-1/2-DHCR7-mediated cholesterol synthesis, and upregulated production of anti-oxidant Vitamin D3. Pharmacological blocking of IL-17A/Th-17 cells using anti-IL-12/IL-23 Ab suppressed progression of HCC (by 70%) in alcohol-fed mice, indicating that targeting IL-17 signaling might provide novel strategies for treatment of alcohol-induced HCC. Overall, IL-17A is as a tumor promoting cytokine, which critically regulates alcohol-induced hepatic steatosis, inflammation, fibrosis, and HCC. Alcohol is metabolized in hepatocytes, leading to production of toxic metabolites (acetaldehyde and acetate), fatty acid and cholesterol synthesis2, and inhibition of DNA damage repair, causing accumulation of somatic mutations that drive HCC5-7.
Chronic alcohol (EtOH) consumption is a leading risk factor for development of hepatocellular carcinoma (HCC), which is associated with marked increase of hepatic expression of pro-inflammatory IL-17A and its receptor IL-17RA. Genetic deletion and pharmacological blocking was used to characterize the role of IL-17A/IL-17RA signaling in the pathogenesis of HCC. We demonstrate that global deletion of IL-17RA gene suppressed HCC in alcohol-fed DEN-challenged IL-17RA-/- and Mup-uPA/IL-17RA-/- mice compared to wild type mice. When the cell-specific role of IL-17RA signaling was examined, development of HCC was decreased in both alcohol-fed IL-17RAΔMΦ and IL-17RAΔHep mice devoid of IL-17RA in myeloid cells and hepatocytes, but not in IL-17RAΔHSCs mice (deficient of IL-17RA in hepatic stellate cells (HSCs)). Deletion of IL-17RA in myeloid cells ameliorated tumorigenesis via suppression of pro-tumorigenic/inflammatory and pro-fibrogenic responses in alcohol-fed IL-17RAΔMΦ mice. Remarkably, despite a normal inflammatory response, alcohol-fed IL-17RAΔHep mice developed the fewest tumors (compared to IL-17RAΔMΦ mice), with reduced steatosis and fibrosis. Steatotic IL-17RA-deficient hepatocytes downregulated expression of Cxcl1 and other chemokines, exhibited a striking defect in TNF-TNFR1-dependent Caspase-2-SREBP-1/2-DHCR7-mediated cholesterol synthesis, and upregulated production of anti-oxidant Vitamin D3. Pharmacological blocking of IL-17A/Th-17 cells using anti-IL-12/IL-23 Ab suppressed progression of HCC (by 70%) in alcohol-fed mice, indicating that targeting IL-17 signaling might provide novel strategies for treatment of alcohol-induced HCC. Overall, IL-17A is as a tumor promoting cytokine, which critically regulates alcohol-induced hepatic steatosis, inflammation, fibrosis, and HCC. Many cancers including HCC result from chronic inflammation.Previous studies have outlined the critical role of IL-17A and IL-17RA in the early development of colorectal adenomas8,9, lung fibrosis10, and toxic and NASH liver fibrosis11,12.IL-17A, a member of the IL-17 family of cytokines, was implicated in the pathogenesis of NASH- and ALD-induced fibrosis and HCC13.IL-6, TGF-β1, and IL-23 promote differentiation of IL-17A-producing Th17 cells from naïve Th0 cells14.In fibrotic liver, IL-17A is produced mainly by CD4+ Th17 cells11, and signals through its cognate receptor IL-17RA.Deletion of IL-17RA in Kupffer cells/macrophages was shown to prevent development of HCC in Diethylnitrosamine (DEN)-challenged mice with NASH12.
Chronic alcohol (EtOH) consumption is a leading risk factor for development of hepatocellular carcinoma (HCC), which is associated with marked increase of hepatic expression of pro-inflammatory IL-17A and its receptor IL-17RA. Genetic deletion and pharmacological blocking was used to characterize the role of IL-17A/IL-17RA signaling in the pathogenesis of HCC. We demonstrate that global deletion of IL-17RA gene suppressed HCC in alcohol-fed DEN-challenged IL-17RA-/- and Mup-uPA/IL-17RA-/- mice compared to wild type mice. When the cell-specific role of IL-17RA signaling was examined, development of HCC was decreased in both alcohol-fed IL-17RAΔMΦ and IL-17RAΔHep mice devoid of IL-17RA in myeloid cells and hepatocytes, but not in IL-17RAΔHSCs mice (deficient of IL-17RA in hepatic stellate cells (HSCs)). Deletion of IL-17RA in myeloid cells ameliorated tumorigenesis via suppression of pro-tumorigenic/inflammatory and pro-fibrogenic responses in alcohol-fed IL-17RAΔMΦ mice. Remarkably, despite a normal inflammatory response, alcohol-fed IL-17RAΔHep mice developed the fewest tumors (compared to IL-17RAΔMΦ mice), with reduced steatosis and fibrosis. Steatotic IL-17RA-deficient hepatocytes downregulated expression of Cxcl1 and other chemokines, exhibited a striking defect in TNF-TNFR1-dependent Caspase-2-SREBP-1/2-DHCR7-mediated cholesterol synthesis, and upregulated production of anti-oxidant Vitamin D3. Pharmacological blocking of IL-17A/Th-17 cells using anti-IL-12/IL-23 Ab suppressed progression of HCC (by 70%) in alcohol-fed mice, indicating that targeting IL-17 signaling might provide novel strategies for treatment of alcohol-induced HCC. Overall, IL-17A is as a tumor promoting cytokine, which critically regulates alcohol-induced hepatic steatosis, inflammation, fibrosis, and HCC. Meanwhile, the role of IL-17 signaling in steatotic hepatocytes and NASH- and ASH-induced HCC has not been previously explored.Hepatic lipogenesis is driven majorly by transcription factors SREBP 1 and 2 (sterol regulatory element binding proteins 1 and 2) that control fatty acid and cholesterol biosynthesis15.A novel Caspase-2-S1P-SREBP pathway has been recently identified in experimental models and patients with NASH and was shown to be activated in response to TNF signaling16.Similar to TNF/TNFR1-dependent activation of Caspase-2-S1P-SREBP signaling cascade in patients with NASH, here we demonstrate that caspase-2-S1P-SREBP mechanism is induced in patients with ASH, and activation of this pathway is regulated by IL-17 signaling.
Lactation lowers blood glucose and triglycerides, and increases insulin sensitivity. We hypothesized that a longer duration of lactation would be associated with lower prevalence of non-alcoholic fatty liver disease (NAFLD), which is the leading cause of chronic liver disease in the United States. Participants from the Coronary Artery Risk Development in Young Adults cohort study who delivered ≥ 1 child post-baseline (Y0: 1985–1986), and underwent CT quantification of hepatic steatosis 25 years following cohort entry (Y25: 2010–2011) were included (n = 844). The duration of lactation was summed for all post-baseline births, and NAFLD at Y25 was assessed by central review of CT images and defined by liver attenuation ≤ 40 Hounsfield Units after exclusion of other causes of hepatic steatosis. Unadjusted and multivariable logistic regression analyses were performed using an a priori set of confounding variables; age, race, education, and baseline body mass index. Of 844 women who delivered after baseline (48% black, 52% white, mean age 49 years at Y25 exam), 32% reported lactation duration of 0 to 1 month, 25% reported >1 to 6 months, 43% reported more than 6 months, while 54 (6%) had NAFLD. Longer lactation duration was inversely associated with NAFLD in unadjusted logistic regression. For women who reported >6 months lactation compared to those reporting 0–1 month, the odds ratio for NAFLD was 0.48 (95% CI 0.25–0.94; p = 0.03) and the association remained after adjustment for confounders (adjusted odds ratio 0.46; 95% CI 0.22–0.97; p = 0.04). A longer duration of lactation, particularly greater than 6 months, is associated with lower odds of NAFLD in mid-life and may represent a modifiable risk factor for NAFLD. Non-alcoholic fatty liver disease (NAFLD) is an increasingly common cause of cirrhosis and hepatocellular carcinoma and is on trajectory to become the most frequent indication for liver transplantation in the United States,1,2 however therapeutic options are limited.While NAFLD is recognized as the hepatic manifestation of metabolic dysfunction, it also portends an increased risk for incident diabetes mellitus (DM) and metabolic syndrome suggesting a complex bidirectional relationship.3,4
Accurate evaluation of renal function in patients with liver cirrhosis is critical for clinical management. However, there are still discrepancies between the measured glomerular filtration rate (mGFR) and creatinine-based estimated GFR (eGFR). In this study, we compared the performance of 2 common eGFR measurements with mGFR and evaluated the impact of low muscle mass on overestimation of renal function in patients with cirrhosis. This study included 779 consecutive cirrhotic patients who underwent 51Cr-ethylenediamine tetra acetic acid (EDTA) (as a mGFR) and abdominal computed tomography (CT). The eGFR was calculated using creatinine or cystatin C. Muscle mass was assessed in terms of the total skeletal muscle at L3 level using CT. Modification of diet in renal disease (MDRD)-eGFR was overestimated in 47% of patients. A multivariate analysis showed that female sex (adjusted odds ratio [aOR] 4.91), Child B and C vs. A (aOR 1.69 and 1.84) and skeletal muscle mass (aOR 0.89) were independent risk factors associated with overestimation. Interestingly, the effect of skeletal muscle mass on overestimation varied based on sex. Decreased muscle mass significantly enhanced the risk of overestimation of MDRD-eGFR in male patients, but not in female patients. Cystatin C-based eGFR showed a better correlation with mGFR than MDRD-eGFR; it was also better at predicting overall survival and the incidence of acute kidney injury than MDRD-eGFR. The risk factors associated with overestimation included female sex, impaired liver function, and decreased muscle mass in males. In particular, eGFR in male patients with sarcopenia should be carefully interpreted. Creatinine-based eGFR was overestimated more often than cystatin C-based eGFR, with overestimation of eGFR closely related to poor prognostic performance. Accurate evaluation of renal function is a prerequisite for the management of patients with liver cirrhosis.1It facilitates diagnostic and therapeutic assessment, prognostic evaluation and indication for liver transplantation.2It is well known that cirrhosis is often accompanied by decreased renal function, resulting in poor outcomes even in stage I acute kidney injury (AKI).3,4
Accurate evaluation of renal function in patients with liver cirrhosis is critical for clinical management. However, there are still discrepancies between the measured glomerular filtration rate (mGFR) and creatinine-based estimated GFR (eGFR). In this study, we compared the performance of 2 common eGFR measurements with mGFR and evaluated the impact of low muscle mass on overestimation of renal function in patients with cirrhosis. This study included 779 consecutive cirrhotic patients who underwent 51Cr-ethylenediamine tetra acetic acid (EDTA) (as a mGFR) and abdominal computed tomography (CT). The eGFR was calculated using creatinine or cystatin C. Muscle mass was assessed in terms of the total skeletal muscle at L3 level using CT. Modification of diet in renal disease (MDRD)-eGFR was overestimated in 47% of patients. A multivariate analysis showed that female sex (adjusted odds ratio [aOR] 4.91), Child B and C vs. A (aOR 1.69 and 1.84) and skeletal muscle mass (aOR 0.89) were independent risk factors associated with overestimation. Interestingly, the effect of skeletal muscle mass on overestimation varied based on sex. Decreased muscle mass significantly enhanced the risk of overestimation of MDRD-eGFR in male patients, but not in female patients. Cystatin C-based eGFR showed a better correlation with mGFR than MDRD-eGFR; it was also better at predicting overall survival and the incidence of acute kidney injury than MDRD-eGFR. The risk factors associated with overestimation included female sex, impaired liver function, and decreased muscle mass in males. In particular, eGFR in male patients with sarcopenia should be carefully interpreted. Creatinine-based eGFR was overestimated more often than cystatin C-based eGFR, with overestimation of eGFR closely related to poor prognostic performance. Creatinine (Cr), the most frequently used parameter for the evaluation of renal function, is a very powerful prognostic biomarker in cirrhotic patients, and an important element of the model for end-stage liver disease (MELD) scoring system.5However, renal function estimated by the Cr-based formula is relatively inaccurate considering endogenous synthesis and metabolism.2The actual renal function is likely to be overestimated rather than undervalued, especially in patients with liver cirrhosis because of poor nutritional status and reduced muscle mass.6Since Cr is produced in liver and stored in muscle following phosphorylation, its serum concentration is inevitably associated with liver function and muscle mass.It is also known to be affected by age, gender, and race, which are reflected in Cr-based formulas.7,8In fact, it has been reported that Cr-based equations (e.g. MDRD-4 equation) overestimate the true renal function by less than 30% to as much as 50%, especially, in patients with poor liver function or low glomerular filtration rate (GFR).9–11
Patients with advanced liver fibrosis remain at risk of cirrhosis-related outcomes and those with severe comorbidities may not benefit from hepatitis C (HCV) eradication. We aimed to collect data on all-cause mortality and relevant clinical events within the first two years of direct-acting antiviral therapy, whilst determining the prognostic capability of a comorbidity-based model. This was a prospective non-interventional study, from the beginning of direct-acting antiviral therapy to the event of interest (mortality) or up to two years of follow-up, including 14 Spanish University Hospitals. Patients with HCV infection, irrespective of liver fibrosis stage, who received direct-acting antiviral therapy were used to build an estimation and a validation cohort. Comorbidity was assessed according to Charlson comorbidity and CirCom indexes. A total of 3.4% (65/1,891) of individuals died within the first year, while 5.4% (102/1,891) died during the study. After adjusting for cirrhosis, platelet count, alanine aminotransferase and sex, the following factors were independently associated with one-year mortality: Charlson index (hazard ratio [HR] 1.55; 95% CI 1.29–1.86; p = 0.0001), bilirubin (HR 1.39; 95% CI 1.11–1.75; p = 0.004), age (HR 1.06 95% CI 1.02–1.11; p = 0.005), international normalized ratio (HR 3.49; 95% CI 1.36–8.97; p = 0.010), and albumin (HR 0.18; 95% CI 0.09–0.37; p = 0.0001). HepCom score showed a good calibration and discrimination (C-statistics 0.90), and was superior to the other prognostic scores (model for end-stage liver disease 0.81, Child-Pugh 0.72, CirCom 0.68) regarding one- and two-year mortality. HepCom score identified low- (≤5.7 points: 2%–3%) and high-risk (≥25 points: 56%–59%) mortality groups, both in the estimation and validation cohorts. The distribution of clinical events was similar between groups. The HepCom score, a combination of Charlson comorbidity index, age, and liver function (international normalized ratio, albumin, and bilirubin) enables detection of a group at high risk of one- and two-year mortality, and relevant clinical events, after starting direct-acting antiviral therapy. The advent of new direct-acting antivirals (DAAs) has dramatically changed the landscape of hepatitis C (HCV) treatment, by increasing efficacy1 and safety.2The cure of infection creates a rupture in the natural history of the HCV disease.Indeed, patients achieving sustained virologic response (SVR) showed decreased rates of all-cause mortality, hepatocellular carcinoma (HCC) and decompensated cirrhosis, as well as less need for liver transplantation.3However, patients with advanced liver fibrosis remain at risk of cirrhosis-related outcomes and patients with severe comorbidities may not benefit from viral clearance.In this setting of excellent results, these factors could be disappointing for physicians, patients, and relatives.
The role of hepatitis B virus (HBV)-specific CD4 T cells in patients with chronic HBV infection is not clear. Thus, we aimed to elucidate this in patients with chronic infection, and those with hepatitis B flares. Through intracellular IFN-γ and TNF-α staining, HBV-specific CD4 T cells were analyzed in 68 patients with chronic HBV infection and alanine aminotransferase (ALT) <2x the upper limit of normal (ULN), and 28 patients with a hepatitis B flare. HBV-specific HLA-DRB1*0803/HLA-DRB1*1202-restricted CD4 T cell epitopes were identified. TNF-α producing cells were the dominant population in patients’ HBV-specific CD4 T cells. In patients with ALT <2xULN, both the frequency and the dominance of HBV-specific IFN-γ producing CD4 T cells increased sequentially in patients with elevated levels of viral clearance: HBV e antigen (HBeAg) positive, HBeAg negative, and HBV surface antigen (HBsAg) negative. In patients with a hepatitis B flare, the frequency of HBV core-specific TNF-α producing CD4 T cells was positively correlated with patients’ ALT and total bilirubin levels, and the frequency of those cells changed in parallel with the severity of liver damage. Patients with HBeAg/HBsAg loss after flare showed higher frequency and dominance of HBV-specific IFN-γ producing CD4 T cells, compared to patients without HBeAg/HBsAg loss. Both the frequency and the dominance of HBV S-specific IFN-γ producing CD4 T cells were positively correlated with the decrease of HBsAg during flare. A differentiation process from TNF-α producing cells to IFN-γ producing cells in HBV-specific CD4 T cells was observed during flare. Eight and 9 HBV-derived peptides/pairs were identified as HLA-DRB1*0803 restricted epitopes and HLA-DRB1*1202 restricted epitopes, respectively. HBV-specific TNF-α producing CD4 T cells are associated with liver damage, while HBV-specific IFN-γ producing CD4 T cells are associated with viral clearance in patients with chronic HBV infection. In chronic hepatitis B virus (HBV) infection, HBV-specific T cells plays a pivotal role in viral clearance and liver damage.Studies on a chimpanzee model of HBV acute infection have demonstrated an irreplaceable role of CD8 T cells in HBV clearance and disease pathogenesis.1In patients with chronic HBV infection, HBV-specific CD8 T cells usually show functional exhaustion or even clonal deletion,2–7 which is deemed a major reason for the HBV persistence in those patients.
The role of hepatitis B virus (HBV)-specific CD4 T cells in patients with chronic HBV infection is not clear. Thus, we aimed to elucidate this in patients with chronic infection, and those with hepatitis B flares. Through intracellular IFN-γ and TNF-α staining, HBV-specific CD4 T cells were analyzed in 68 patients with chronic HBV infection and alanine aminotransferase (ALT) <2x the upper limit of normal (ULN), and 28 patients with a hepatitis B flare. HBV-specific HLA-DRB1*0803/HLA-DRB1*1202-restricted CD4 T cell epitopes were identified. TNF-α producing cells were the dominant population in patients’ HBV-specific CD4 T cells. In patients with ALT <2xULN, both the frequency and the dominance of HBV-specific IFN-γ producing CD4 T cells increased sequentially in patients with elevated levels of viral clearance: HBV e antigen (HBeAg) positive, HBeAg negative, and HBV surface antigen (HBsAg) negative. In patients with a hepatitis B flare, the frequency of HBV core-specific TNF-α producing CD4 T cells was positively correlated with patients’ ALT and total bilirubin levels, and the frequency of those cells changed in parallel with the severity of liver damage. Patients with HBeAg/HBsAg loss after flare showed higher frequency and dominance of HBV-specific IFN-γ producing CD4 T cells, compared to patients without HBeAg/HBsAg loss. Both the frequency and the dominance of HBV S-specific IFN-γ producing CD4 T cells were positively correlated with the decrease of HBsAg during flare. A differentiation process from TNF-α producing cells to IFN-γ producing cells in HBV-specific CD4 T cells was observed during flare. Eight and 9 HBV-derived peptides/pairs were identified as HLA-DRB1*0803 restricted epitopes and HLA-DRB1*1202 restricted epitopes, respectively. HBV-specific TNF-α producing CD4 T cells are associated with liver damage, while HBV-specific IFN-γ producing CD4 T cells are associated with viral clearance in patients with chronic HBV infection. CD4 T cells, as the main regulating cell population, also play an important role in HBV clearance.A mice model of HBV genome plasmid hydrodynamic transfection indicated a critical role of CD4 T cells in induction of CD8 T cell-mediated HBV clearance.8Strong HBV-specific CD4 T cell responses have been observed in patients with self-limited acute HBV infection.9,10A lack of HBV-specific CD4 T cell responses has been associated with HBV persistence in HBV/hepatitis C virus coinfection patients despite detectable HBV-specific CD8 T cells in those patients.11Most of the studies on HBV-specific CD4 T cells have focused on the magnitude of response, and usually found a low frequency of HBV-specific CD4 T cells in patients with chronic HBV infection, resembling the T cell exhaustion in HBV-specific CD8 T cells.3–7As a versatile cell population, detailed functional dissection of HBV-specific CD4 T cells has received less attention in previous studies.The observation of an increased frequency of circulating HBV-specific follicular T helper (Tfh)-like cells in patients with HBV e antigen (HBeAg) seroconversion has directly associated a functional subset of HBV-specific CD4 T cells with HBV viral control.12Our previous study has identified HLA-DR as the major locus for susceptibility to HBV-related acute-on-chronic liver failure, indicating a role of CD4 T cells in the immune-pathogenesis of HBV-related acute-on-chronic liver failure.13Compared to HBV-specific CD8 T cell epitopes, fewer HBV-specific CD4 T cell epitopes have been identified,14,15 which hinders detailed functional analysis of HBV-specific CD4 T cells.
The role of hepatitis B virus (HBV)-specific CD4 T cells in patients with chronic HBV infection is not clear. Thus, we aimed to elucidate this in patients with chronic infection, and those with hepatitis B flares. Through intracellular IFN-γ and TNF-α staining, HBV-specific CD4 T cells were analyzed in 68 patients with chronic HBV infection and alanine aminotransferase (ALT) <2x the upper limit of normal (ULN), and 28 patients with a hepatitis B flare. HBV-specific HLA-DRB1*0803/HLA-DRB1*1202-restricted CD4 T cell epitopes were identified. TNF-α producing cells were the dominant population in patients’ HBV-specific CD4 T cells. In patients with ALT <2xULN, both the frequency and the dominance of HBV-specific IFN-γ producing CD4 T cells increased sequentially in patients with elevated levels of viral clearance: HBV e antigen (HBeAg) positive, HBeAg negative, and HBV surface antigen (HBsAg) negative. In patients with a hepatitis B flare, the frequency of HBV core-specific TNF-α producing CD4 T cells was positively correlated with patients’ ALT and total bilirubin levels, and the frequency of those cells changed in parallel with the severity of liver damage. Patients with HBeAg/HBsAg loss after flare showed higher frequency and dominance of HBV-specific IFN-γ producing CD4 T cells, compared to patients without HBeAg/HBsAg loss. Both the frequency and the dominance of HBV S-specific IFN-γ producing CD4 T cells were positively correlated with the decrease of HBsAg during flare. A differentiation process from TNF-α producing cells to IFN-γ producing cells in HBV-specific CD4 T cells was observed during flare. Eight and 9 HBV-derived peptides/pairs were identified as HLA-DRB1*0803 restricted epitopes and HLA-DRB1*1202 restricted epitopes, respectively. HBV-specific TNF-α producing CD4 T cells are associated with liver damage, while HBV-specific IFN-γ producing CD4 T cells are associated with viral clearance in patients with chronic HBV infection. Persistent exposure to pathogen antigen has been indicated as the main cause of T cell exhaustion in mice with chronic lymphocytic choriomeningitis virus (LCMV) infection.16A lot of studies have correlated the HBV-specific T cell response with serum HBV antigen in patients with chronic HBV infection,3–7 while the discovery of preserved T cell function in young adults in the immune-tolerant phase argues against current knowledge about HBV-specific T cell exhaustion.17Another scenario not matching current knowledge of HBV-specific T cell exhaustion is hepatitis B flare.Patients with a hepatitis B flare show both high HBV viral load and rising HBV-specific T cell responses.18Some of these patients would undergo HBeAg or even HBV surface antigen (HBsAg) seroclearance after the flare, which indicates that the rising HBV-specific T cells in those patients is effective at HBV clearance.19Those discrepancies indicate that some mechanisms, besides T cell exhaustion, might be involved in the regulation of HBV-specific T cell responses.
Biliary tract cancers (BTCs) are clinically and pathologically heterogeneous and respond poorly to treatment. Genomic profiling can offer a clearer understanding of their carcinogenesis, classification and treatment strategy. We performed large-scale genome sequencing analyses on BTCs to investigate their somatic and germline driver events and characterize their genomic landscape. We analyzed 412 BTC samples from Japanese and Italian populations, 107 by whole-exome sequencing (WES), 39 by whole-genome sequencing (WGS), and a further 266 samples by targeted sequencing. The subtypes were 136 intrahepatic cholangiocarcinomas (ICCs), 101 distal cholangiocarcinomas (DCCs), 109 peri-hilar type cholangiocarcinomas (PHCs), and 66 gallbladder or cystic duct cancers (GBCs/CDCs). We identified somatic alterations and searched for driver genes in BTCs, finding pathogenic germline variants of cancer-predisposing genes. We predicted cell-of-origin for BTCs by combining somatic mutation patterns and epigenetic features. We identified 32 significantly and commonly mutated genes including TP53, KRAS, SMAD4, NF1, ARID1A, PBRM1, and ATR, some of which negatively affected patient prognosis. A novel deletion of MUC17 at 7q22.1 affected patient prognosis. Cell-of-origin predictions using WGS and epigenetic features suggest hepatocyte-origin of hepatitis-related ICCs. Deleterious germline mutations of cancer-predisposing genes such as BRCA1, BRCA2, RAD51D, MLH1, or MSH2 were detected in 11% (16/146) of BTC patients. BTCs have distinct genetic features including somatic events and germline predisposition. These findings could be useful to establish treatment and diagnostic strategies for BTCs based on genetic information. Biliary tract cancer (BTC) or cholangiocarcinoma (CC) is a rare cancer worldwide, but prevalent in some areas, where a specific risk factor of environmental exposure is involved in BTC development, such as chronic cholangitis,1,2 liver fluke infection in Thailand,1,2 viral hepatitis,1,2 aflatoxin exposure in Chile,3 or other chemical exposures.2,4According to its anatomical location, BTCs are mainly classified as intrahepatic cholangiocarcinoma (ICC), extrahepatic bile duct cancer, or gallbladder cancer.The extrahepatic form is composed of peri-hilar type cholangiocarcinoma (PHC or Klatskin tumor) and distal cholangiocarcinoma (DCC), while gallbladder cancer (GBC) also includes cystic duct carcinoma (CDC).There is some debate about the cellular origin of ICC.BTC cells are presumed to originate from cholangiocytes, but the presence of mixed tumor types in ICC and HCC (hepatocellular carcinoma) with intermediate characteristics between ICC and HCC suggests that a subset of intrahepatic tumors could share a common hepatic progenitor cell origin.5Regardless of its location or pathology, BTCs are very aggressive with high metastatic and invasive potential and are difficult to completely resect by surgery because of their anatomical location and spread along the bile ducts.Standard of practice for advanced CC is cisplatin or gemcitabine, but the response to these chemotherapies is poor, and consequently they show poor prognosis with only 5–10% five-year survival.1
Little is known about outcomes of liver transplantation for patients with non-alcoholic steatohepatitis (NASH). We aimed to determine the frequency and outcomes of liver transplantation for patients with NASH in Europe and identify prognostic factors. We analysed data from patients transplanted for end-stage liver disease between January 2002 and December 2016 using the European Liver Transplant Registry database. We compared data between patients with NASH versus other aetiologies. The principle endpoints were patient and overall allograft survival. Among 68,950 adults undergoing first liver transplantation, 4.0% were transplanted for NASH – an increase from 1.2% in 2002 to 8.4% in 2016. A greater proportion of patients transplanted for NASH (39.1%) had hepatocellular carcinoma (HCC) than non-NASH patients (28.9%, p <0.001). NASH was not significantly associated with survival of patients (hazard ratio [HR] 1.02, p = 0.713) or grafts (HR 0.99; p = 0.815) after accounting for available recipient and donor variables. Infection (24.0%) and cardio/cerebrovascular complications (5.3%) were the commonest causes of death in patients with NASH without HCC. Increasing recipient age (61–65 years: HR 2.07, p <0.001; >65: HR 1.72, p = 0.017), elevated model for end-stage liver disease score (>23: HR 1.48, p = 0.048) and low (<18.5 kg/m2: HR 4.29, p = 0.048) or high (>40 kg/m2: HR 1.96, p = 0.012) recipient body mass index independently predicted death in patients transplanted for NASH without HCC. Data must be interpreted in the context of absent recognised confounders, such as pre-morbid metabolic risk factors. The number and proportion of liver transplants performed for NASH in Europe has increased from 2002 through 2016. HCC was more common in patients transplanted with NASH. Survival of patients and grafts in patients with NASH is comparable to that of other disease indications. The prevalence of non-alcoholic fatty liver disease (NAFLD) has increased dramatically, in parallel with the worldwide increase in obesity and diabetes.1,2Approximately a quarter of the European adult population have NAFLD, representing an increase of 10% since 2005.3
Little is known about outcomes of liver transplantation for patients with non-alcoholic steatohepatitis (NASH). We aimed to determine the frequency and outcomes of liver transplantation for patients with NASH in Europe and identify prognostic factors. We analysed data from patients transplanted for end-stage liver disease between January 2002 and December 2016 using the European Liver Transplant Registry database. We compared data between patients with NASH versus other aetiologies. The principle endpoints were patient and overall allograft survival. Among 68,950 adults undergoing first liver transplantation, 4.0% were transplanted for NASH – an increase from 1.2% in 2002 to 8.4% in 2016. A greater proportion of patients transplanted for NASH (39.1%) had hepatocellular carcinoma (HCC) than non-NASH patients (28.9%, p <0.001). NASH was not significantly associated with survival of patients (hazard ratio [HR] 1.02, p = 0.713) or grafts (HR 0.99; p = 0.815) after accounting for available recipient and donor variables. Infection (24.0%) and cardio/cerebrovascular complications (5.3%) were the commonest causes of death in patients with NASH without HCC. Increasing recipient age (61–65 years: HR 2.07, p <0.001; >65: HR 1.72, p = 0.017), elevated model for end-stage liver disease score (>23: HR 1.48, p = 0.048) and low (<18.5 kg/m2: HR 4.29, p = 0.048) or high (>40 kg/m2: HR 1.96, p = 0.012) recipient body mass index independently predicted death in patients transplanted for NASH without HCC. Data must be interpreted in the context of absent recognised confounders, such as pre-morbid metabolic risk factors. The number and proportion of liver transplants performed for NASH in Europe has increased from 2002 through 2016. HCC was more common in patients transplanted with NASH. Survival of patients and grafts in patients with NASH is comparable to that of other disease indications. Non-alcoholic steatohepatitis (NASH) and any associated fibrosis, confer a greater risk of liver-related morbidity and mortality amongst patients with NAFLD.4NASH is an increasingly common indication for liver transplantation (LT), and is now second only to alcohol-related liver disease (ALD) in the US.5Similarly, NASH accounts for an increasing proportion of patients undergoing LT in the UK (4% in 1995; 12% in 2013).6However, pan-European data to describe the burden of NASH on transplantation services are lacking.
Little is known about outcomes of liver transplantation for patients with non-alcoholic steatohepatitis (NASH). We aimed to determine the frequency and outcomes of liver transplantation for patients with NASH in Europe and identify prognostic factors. We analysed data from patients transplanted for end-stage liver disease between January 2002 and December 2016 using the European Liver Transplant Registry database. We compared data between patients with NASH versus other aetiologies. The principle endpoints were patient and overall allograft survival. Among 68,950 adults undergoing first liver transplantation, 4.0% were transplanted for NASH – an increase from 1.2% in 2002 to 8.4% in 2016. A greater proportion of patients transplanted for NASH (39.1%) had hepatocellular carcinoma (HCC) than non-NASH patients (28.9%, p <0.001). NASH was not significantly associated with survival of patients (hazard ratio [HR] 1.02, p = 0.713) or grafts (HR 0.99; p = 0.815) after accounting for available recipient and donor variables. Infection (24.0%) and cardio/cerebrovascular complications (5.3%) were the commonest causes of death in patients with NASH without HCC. Increasing recipient age (61–65 years: HR 2.07, p <0.001; >65: HR 1.72, p = 0.017), elevated model for end-stage liver disease score (>23: HR 1.48, p = 0.048) and low (<18.5 kg/m2: HR 4.29, p = 0.048) or high (>40 kg/m2: HR 1.96, p = 0.012) recipient body mass index independently predicted death in patients transplanted for NASH without HCC. Data must be interpreted in the context of absent recognised confounders, such as pre-morbid metabolic risk factors. The number and proportion of liver transplants performed for NASH in Europe has increased from 2002 through 2016. HCC was more common in patients transplanted with NASH. Survival of patients and grafts in patients with NASH is comparable to that of other disease indications. Given the frequent co-existence of obesity, diabetes and related comorbidities, patients with NASH requiring LT are considered to be at a higher risk.7In contrast to the US,8–10 European reports of post-transplant outcomes of NASH have been limited to single-centre datasets.11In the absence of well-validated contraindications it remains a challenge to effectively risk-stratify patients with NASH being considered for LT.7
Tenofovir disoproxil fumarate (TDF) monotherapy has displayed non-inferior efficacy to TDF plus entecavir (ETV) combination therapy in patients with hepatitis B virus (HBV) resistant to ETV and/or adefovir (ADV). Nonetheless, the virologic response rate was suboptimal in patients receiving up to 144 weeks of TDF monotherapy. We aimed to assess the efficacy and safety of TDF monotherapy given for up to 240 weeks. One trial enrolled patients with ETV resistance without ADV resistance (n = 90), and another trial included patients with ADV resistance (n = 102). Most patients (91.2%) also had lamivudine resistance. Patients were randomized 1:1 to receive TDF monotherapy or TDF + ETV combination therapy for 48 weeks, and then TDF monotherapy until week 240. We compared efficacy between the studies and safety in the pooled population at 240 weeks. At week 240, the proportion of patients with serum HBV DNA <15 IU/ml was not significantly different between the ETV and ADV resistance groups in the full analysis set (84.4% vs. 73.5%; p = 0.07), which was significantly different by on-treatment analysis (92.7% vs. 79.8%; p = 0.02). Virologic blips associated with poor medication adherence occurred in 7 patients throughout the 240 weeks. None developed additional HBV resistance mutations. Among the 170 HBV e antigen (HBeAg)-positive patients at baseline, 12 (7.1%) achieved HBeAg seroconversion at week 240. None achieved HBV surface antigen seroclearance. Significant decreases from baseline were observed at week 240 in the estimated glomerular filtration rate (−3.21 ml/min/1.73 m2 by the CKD-EPI equation, p <0.001) and bone mineral density (g/cm2) at the femur (−2.48%, p <0.001). Up to 240 weeks of TDF monotherapy provided an increasing virologic response rate in heavily pretreated patients with HBV resistant to ETV and/or ADV. However, it was associated with poor serological responses and decreasing renal function and bone mineral density. (ClinicalTrials.gov No, NCT01639066 and NCT01639092). Persistently high serum hepatitis B virus (HBV) DNA levels are an independent risk factor for disease progression to cirrhosis and hepatocellular carcinoma (HCC) in patients with chronic hepatitis B (CHB).1–3Multiple studies have shown that long-term treatment with nucleos(t)ide analogue (NUC) therapy reduces the risk of mortality and HCC through inhibition of HBV replication.3–6Nevertheless, functional cure of chronic HBV infection (HBV surface antigen [HBsAg] seroclearance) is very rarely achievable, necessitating almost life-long NUC therapy in most patients.7,8
Tenofovir disoproxil fumarate (TDF) monotherapy has displayed non-inferior efficacy to TDF plus entecavir (ETV) combination therapy in patients with hepatitis B virus (HBV) resistant to ETV and/or adefovir (ADV). Nonetheless, the virologic response rate was suboptimal in patients receiving up to 144 weeks of TDF monotherapy. We aimed to assess the efficacy and safety of TDF monotherapy given for up to 240 weeks. One trial enrolled patients with ETV resistance without ADV resistance (n = 90), and another trial included patients with ADV resistance (n = 102). Most patients (91.2%) also had lamivudine resistance. Patients were randomized 1:1 to receive TDF monotherapy or TDF + ETV combination therapy for 48 weeks, and then TDF monotherapy until week 240. We compared efficacy between the studies and safety in the pooled population at 240 weeks. At week 240, the proportion of patients with serum HBV DNA <15 IU/ml was not significantly different between the ETV and ADV resistance groups in the full analysis set (84.4% vs. 73.5%; p = 0.07), which was significantly different by on-treatment analysis (92.7% vs. 79.8%; p = 0.02). Virologic blips associated with poor medication adherence occurred in 7 patients throughout the 240 weeks. None developed additional HBV resistance mutations. Among the 170 HBV e antigen (HBeAg)-positive patients at baseline, 12 (7.1%) achieved HBeAg seroconversion at week 240. None achieved HBV surface antigen seroclearance. Significant decreases from baseline were observed at week 240 in the estimated glomerular filtration rate (−3.21 ml/min/1.73 m2 by the CKD-EPI equation, p <0.001) and bone mineral density (g/cm2) at the femur (−2.48%, p <0.001). Up to 240 weeks of TDF monotherapy provided an increasing virologic response rate in heavily pretreated patients with HBV resistant to ETV and/or ADV. However, it was associated with poor serological responses and decreasing renal function and bone mineral density. (ClinicalTrials.gov No, NCT01639066 and NCT01639092). Worldwide, many patients with CHB have been exposed to low-potency, low genetic barrier NUCs, i.e., lamivudine (LAM) and adefovir dipivoxil (ADV), and developed multiple resistance mutations of HBV.9Treatment options are severely limited for these patients who have HBV resistant to multiple NUCs, such as LAM, ADV, and entecavir (ETV).
Tenofovir disoproxil fumarate (TDF) monotherapy has displayed non-inferior efficacy to TDF plus entecavir (ETV) combination therapy in patients with hepatitis B virus (HBV) resistant to ETV and/or adefovir (ADV). Nonetheless, the virologic response rate was suboptimal in patients receiving up to 144 weeks of TDF monotherapy. We aimed to assess the efficacy and safety of TDF monotherapy given for up to 240 weeks. One trial enrolled patients with ETV resistance without ADV resistance (n = 90), and another trial included patients with ADV resistance (n = 102). Most patients (91.2%) also had lamivudine resistance. Patients were randomized 1:1 to receive TDF monotherapy or TDF + ETV combination therapy for 48 weeks, and then TDF monotherapy until week 240. We compared efficacy between the studies and safety in the pooled population at 240 weeks. At week 240, the proportion of patients with serum HBV DNA <15 IU/ml was not significantly different between the ETV and ADV resistance groups in the full analysis set (84.4% vs. 73.5%; p = 0.07), which was significantly different by on-treatment analysis (92.7% vs. 79.8%; p = 0.02). Virologic blips associated with poor medication adherence occurred in 7 patients throughout the 240 weeks. None developed additional HBV resistance mutations. Among the 170 HBV e antigen (HBeAg)-positive patients at baseline, 12 (7.1%) achieved HBeAg seroconversion at week 240. None achieved HBV surface antigen seroclearance. Significant decreases from baseline were observed at week 240 in the estimated glomerular filtration rate (−3.21 ml/min/1.73 m2 by the CKD-EPI equation, p <0.001) and bone mineral density (g/cm2) at the femur (−2.48%, p <0.001). Up to 240 weeks of TDF monotherapy provided an increasing virologic response rate in heavily pretreated patients with HBV resistant to ETV and/or ADV. However, it was associated with poor serological responses and decreasing renal function and bone mineral density. (ClinicalTrials.gov No, NCT01639066 and NCT01639092). Recently, in 2 separate randomized trials for 48 weeks, we have demonstrated that tenofovir disoproxil fumarate (TDF) monotherapy provides a virologic response rate non-inferior to that of combination therapy with TDF and ETV in patients who acquired multiple HBV mutations resistant to ETV and/or ADV through previous exposure to multiple NUCs.10,11However, the virologic response rate was not optimal with up to 144 weeks of TDF therapy in patients resistant to ETV (82.2%) or ADV (67.7%),12 which raised concerns about the efficacy and safety of long-term TDF monotherapy, especially in those with ADV resistance.Moreover, long-term TDF use may be linked to renal toxicity and reductions in bone mineral density (BMD) in patients with CHB.13–15
Acetaminophen-protein adducts are specific biomarkers of toxic acetaminophen (paracetamol) metabolite exposure. In patients with hepatotoxicity (alanine aminotransferase [ALT] >1,000 U/L), an adduct concentration ≥1.0 nmol/ml is sensitive and specific for identifying cases secondary to acetaminophen. Our aim was to characterise acetaminophen-protein adduct concentrations in patients following acetaminophen overdose and determine if they predict toxicity. We performed a multicentre prospective observational study, recruiting patients 14 years of age or older with acetaminophen overdose regardless of intent or formulation. Three serum samples were obtained within the first 24 h of presentation and analysed for acetaminophen-protein adducts. Acetaminophen-protein adduct concentrations were compared to ALT and other indicators of toxicity. Of the 240 patients who participated, 204 (85%) presented following acute ingestions, with a median ingested dose of 20 g (IQR 10–40), and 228 (95%) were treated with intravenous acetylcysteine at a median time of 6 h (IQR 3.5–10.5) post-ingestion. Thirty-six (15%) patients developed hepatotoxicity, of whom 22 had an ALT ≤1,000 U/L at the time of initial acetaminophen-protein adduct measurement. Those who developed hepatotoxicity had a higher initial acetaminophen-protein adduct concentration compared to those who did not, 1.63 nmol/ml (IQR 0.76–2.02, n = 22) vs. 0.26 nmol/ml (IQR 0.15–0.41; n = 204; p <0.0001), respectively. The AUROC for hepatotoxicity was 0.98 (95% CI 0.96–1.00; n = 226; p <0.0001) with acetaminophen-protein adduct concentration and 0.89 (95% CI 0.82–0.96; n = 219; p <0.0001) with ALT. An acetaminophen-protein adduct concentration of 0.58 nmol/ml was 100% sensitive and 91% specific for identifying patients with an initial ALT ≤1,000 U/L who would develop hepatotoxicity. Adding acetaminophen-protein adduct concentrations to risk prediction models improved prediction of hepatotoxicity to a level similar to that obtained by more complex models. Acetaminophen-protein adduct concentration on presentation predicted which patients with acetaminophen overdose subsequently developed hepatotoxicity, regardless of time of ingestion. An adduct threshold of 0.58 nmol/L was required for optimal prediction. Acetaminophen (APAP), also called paracetamol, is one of the most common medications resulting in hospital presentations and admissions following deliberate self-poisoning and accidental overdose worldwide.1Its major toxic effect is acute liver injury (ALI) and it is the most common cause of acute liver failure (ALF) in North America, Europe and Australia.2–4The American Association of Poison Control Centres, which annually provides over 2.1 million tele-consults for the US and associated territories, received over 100,000 calls related to APAP exposure in 2017.5
Acetaminophen-protein adducts are specific biomarkers of toxic acetaminophen (paracetamol) metabolite exposure. In patients with hepatotoxicity (alanine aminotransferase [ALT] >1,000 U/L), an adduct concentration ≥1.0 nmol/ml is sensitive and specific for identifying cases secondary to acetaminophen. Our aim was to characterise acetaminophen-protein adduct concentrations in patients following acetaminophen overdose and determine if they predict toxicity. We performed a multicentre prospective observational study, recruiting patients 14 years of age or older with acetaminophen overdose regardless of intent or formulation. Three serum samples were obtained within the first 24 h of presentation and analysed for acetaminophen-protein adducts. Acetaminophen-protein adduct concentrations were compared to ALT and other indicators of toxicity. Of the 240 patients who participated, 204 (85%) presented following acute ingestions, with a median ingested dose of 20 g (IQR 10–40), and 228 (95%) were treated with intravenous acetylcysteine at a median time of 6 h (IQR 3.5–10.5) post-ingestion. Thirty-six (15%) patients developed hepatotoxicity, of whom 22 had an ALT ≤1,000 U/L at the time of initial acetaminophen-protein adduct measurement. Those who developed hepatotoxicity had a higher initial acetaminophen-protein adduct concentration compared to those who did not, 1.63 nmol/ml (IQR 0.76–2.02, n = 22) vs. 0.26 nmol/ml (IQR 0.15–0.41; n = 204; p <0.0001), respectively. The AUROC for hepatotoxicity was 0.98 (95% CI 0.96–1.00; n = 226; p <0.0001) with acetaminophen-protein adduct concentration and 0.89 (95% CI 0.82–0.96; n = 219; p <0.0001) with ALT. An acetaminophen-protein adduct concentration of 0.58 nmol/ml was 100% sensitive and 91% specific for identifying patients with an initial ALT ≤1,000 U/L who would develop hepatotoxicity. Adding acetaminophen-protein adduct concentrations to risk prediction models improved prediction of hepatotoxicity to a level similar to that obtained by more complex models. Acetaminophen-protein adduct concentration on presentation predicted which patients with acetaminophen overdose subsequently developed hepatotoxicity, regardless of time of ingestion. An adduct threshold of 0.58 nmol/L was required for optimal prediction. The mainstay of treatment of APAP poisoning is acetylcysteine.Indications for treatment are determined by reported dose and time of ingestion, APAP concentration and hepatic aminotransferase values.6,7This approach has various limitations including the need for an accurate time of exposure measurement following an acute ingestion to be able to plot APAP concentration on the Rumack-Matthew nomogram.APAP concentrations have a low specificity and low positive predictive value in determining who will develop ALI.Furthermore, the aminotransferases, utilised as the traditional biomarkers of ALI, can remain within the normal range for up to 24 h post-exposure and are not specific for APAP-induced ALI.4
Acetaminophen-protein adducts are specific biomarkers of toxic acetaminophen (paracetamol) metabolite exposure. In patients with hepatotoxicity (alanine aminotransferase [ALT] >1,000 U/L), an adduct concentration ≥1.0 nmol/ml is sensitive and specific for identifying cases secondary to acetaminophen. Our aim was to characterise acetaminophen-protein adduct concentrations in patients following acetaminophen overdose and determine if they predict toxicity. We performed a multicentre prospective observational study, recruiting patients 14 years of age or older with acetaminophen overdose regardless of intent or formulation. Three serum samples were obtained within the first 24 h of presentation and analysed for acetaminophen-protein adducts. Acetaminophen-protein adduct concentrations were compared to ALT and other indicators of toxicity. Of the 240 patients who participated, 204 (85%) presented following acute ingestions, with a median ingested dose of 20 g (IQR 10–40), and 228 (95%) were treated with intravenous acetylcysteine at a median time of 6 h (IQR 3.5–10.5) post-ingestion. Thirty-six (15%) patients developed hepatotoxicity, of whom 22 had an ALT ≤1,000 U/L at the time of initial acetaminophen-protein adduct measurement. Those who developed hepatotoxicity had a higher initial acetaminophen-protein adduct concentration compared to those who did not, 1.63 nmol/ml (IQR 0.76–2.02, n = 22) vs. 0.26 nmol/ml (IQR 0.15–0.41; n = 204; p <0.0001), respectively. The AUROC for hepatotoxicity was 0.98 (95% CI 0.96–1.00; n = 226; p <0.0001) with acetaminophen-protein adduct concentration and 0.89 (95% CI 0.82–0.96; n = 219; p <0.0001) with ALT. An acetaminophen-protein adduct concentration of 0.58 nmol/ml was 100% sensitive and 91% specific for identifying patients with an initial ALT ≤1,000 U/L who would develop hepatotoxicity. Adding acetaminophen-protein adduct concentrations to risk prediction models improved prediction of hepatotoxicity to a level similar to that obtained by more complex models. Acetaminophen-protein adduct concentration on presentation predicted which patients with acetaminophen overdose subsequently developed hepatotoxicity, regardless of time of ingestion. An adduct threshold of 0.58 nmol/L was required for optimal prediction. Acetylcysteine is a highly efficacious treatment and has dramatically decreased the rates of ALI and mortality from APAP overdose.However, the literature has reported cases of ALI that occurred despite early antidote treatment, particularly in large ingestions, or rarely in patients despite APAP concentrations below the Rumack-Matthew nomogram treatment line.8–11
Acetaminophen-protein adducts are specific biomarkers of toxic acetaminophen (paracetamol) metabolite exposure. In patients with hepatotoxicity (alanine aminotransferase [ALT] >1,000 U/L), an adduct concentration ≥1.0 nmol/ml is sensitive and specific for identifying cases secondary to acetaminophen. Our aim was to characterise acetaminophen-protein adduct concentrations in patients following acetaminophen overdose and determine if they predict toxicity. We performed a multicentre prospective observational study, recruiting patients 14 years of age or older with acetaminophen overdose regardless of intent or formulation. Three serum samples were obtained within the first 24 h of presentation and analysed for acetaminophen-protein adducts. Acetaminophen-protein adduct concentrations were compared to ALT and other indicators of toxicity. Of the 240 patients who participated, 204 (85%) presented following acute ingestions, with a median ingested dose of 20 g (IQR 10–40), and 228 (95%) were treated with intravenous acetylcysteine at a median time of 6 h (IQR 3.5–10.5) post-ingestion. Thirty-six (15%) patients developed hepatotoxicity, of whom 22 had an ALT ≤1,000 U/L at the time of initial acetaminophen-protein adduct measurement. Those who developed hepatotoxicity had a higher initial acetaminophen-protein adduct concentration compared to those who did not, 1.63 nmol/ml (IQR 0.76–2.02, n = 22) vs. 0.26 nmol/ml (IQR 0.15–0.41; n = 204; p <0.0001), respectively. The AUROC for hepatotoxicity was 0.98 (95% CI 0.96–1.00; n = 226; p <0.0001) with acetaminophen-protein adduct concentration and 0.89 (95% CI 0.82–0.96; n = 219; p <0.0001) with ALT. An acetaminophen-protein adduct concentration of 0.58 nmol/ml was 100% sensitive and 91% specific for identifying patients with an initial ALT ≤1,000 U/L who would develop hepatotoxicity. Adding acetaminophen-protein adduct concentrations to risk prediction models improved prediction of hepatotoxicity to a level similar to that obtained by more complex models. Acetaminophen-protein adduct concentration on presentation predicted which patients with acetaminophen overdose subsequently developed hepatotoxicity, regardless of time of ingestion. An adduct threshold of 0.58 nmol/L was required for optimal prediction. Hence, there has been increasing interest in novel biomarkers that may be able to stratify patients at presentation based on their risk of liver injury and/or biomarkers that are specific for APAP toxicity.Novel biomarkers include those that are liver specific (e.g. microRNA-122 [miR-122]), or reflect mechanisms of toxicity, such as the involvement of activated immune cells (e.g. high mobility group box-1 [HMGB1], keratin-18); mitochondrial dysfunction (e.g. glutamate dehydrogenase) or oxidative metabolism (e.g. APAP-protein adducts [APAP-protein adducts]).12–14
Acetaminophen-protein adducts are specific biomarkers of toxic acetaminophen (paracetamol) metabolite exposure. In patients with hepatotoxicity (alanine aminotransferase [ALT] >1,000 U/L), an adduct concentration ≥1.0 nmol/ml is sensitive and specific for identifying cases secondary to acetaminophen. Our aim was to characterise acetaminophen-protein adduct concentrations in patients following acetaminophen overdose and determine if they predict toxicity. We performed a multicentre prospective observational study, recruiting patients 14 years of age or older with acetaminophen overdose regardless of intent or formulation. Three serum samples were obtained within the first 24 h of presentation and analysed for acetaminophen-protein adducts. Acetaminophen-protein adduct concentrations were compared to ALT and other indicators of toxicity. Of the 240 patients who participated, 204 (85%) presented following acute ingestions, with a median ingested dose of 20 g (IQR 10–40), and 228 (95%) were treated with intravenous acetylcysteine at a median time of 6 h (IQR 3.5–10.5) post-ingestion. Thirty-six (15%) patients developed hepatotoxicity, of whom 22 had an ALT ≤1,000 U/L at the time of initial acetaminophen-protein adduct measurement. Those who developed hepatotoxicity had a higher initial acetaminophen-protein adduct concentration compared to those who did not, 1.63 nmol/ml (IQR 0.76–2.02, n = 22) vs. 0.26 nmol/ml (IQR 0.15–0.41; n = 204; p <0.0001), respectively. The AUROC for hepatotoxicity was 0.98 (95% CI 0.96–1.00; n = 226; p <0.0001) with acetaminophen-protein adduct concentration and 0.89 (95% CI 0.82–0.96; n = 219; p <0.0001) with ALT. An acetaminophen-protein adduct concentration of 0.58 nmol/ml was 100% sensitive and 91% specific for identifying patients with an initial ALT ≤1,000 U/L who would develop hepatotoxicity. Adding acetaminophen-protein adduct concentrations to risk prediction models improved prediction of hepatotoxicity to a level similar to that obtained by more complex models. Acetaminophen-protein adduct concentration on presentation predicted which patients with acetaminophen overdose subsequently developed hepatotoxicity, regardless of time of ingestion. An adduct threshold of 0.58 nmol/L was required for optimal prediction. APAP is primarily metabolised into non-toxic metabolites, but a small percentage is metabolised via cytochrome P450 to produce the reactive metabolite, N-acetyl-p-benzoquinone imine (NAPQI).15NAPQI is detoxified by irreversible glutathione-dependent conjugation reactions to mercapturic acid and cysteine conjugates.16In overdose, the increased formation of NAPQI depletes hepatic glutathione and NAPQI covalently binds to critical cellular proteins.17It has been hypothesised that covalent binding reduces the activity of critical proteins, although additional mechanisms, such as mitochondrial injury and oxygen and nitrogen stress, have been linked to hepatic cell death.APAP-protein adducts represent NAPQI covalently bound to cysteine groups on proteins that are released into blood during hepatocyte lysis.18
Around 10–20% of patients with non-alcoholic fatty liver disease (NAFLD) are non-obese. The benefit of weight reduction in such patients is unclear. We aim to study the efficacy of lifestyle intervention in non-obese patients with NAFLD and to identify factors that predict treatment response. A total of 154 community NAFLD patients were randomised to a 12-month lifestyle intervention programme involving regular exercise, or to standard care. The primary outcome was remission of NAFLD at Month 12 by proton-magnetic resonance spectroscopy. After the programme, the patients were prospectively followed until Year 6. The Asian body mass index (BMI) cut-off of 25 kg/m2 was used to define non-obese NAFLD. Patients were assigned to the intervention (n = 77) and control (n = 77) groups (39 and 38 in each group had baseline BMI <25 and ≥25 kg/m2, respectively). More patients in the intervention group achieved the primary outcome than the control group regardless of baseline BMI (non-obese: 67% vs. 18%, p <0.001; obese: 61% vs. 21%, p <0.001). Lifestyle intervention, lower baseline intrahepatic triglyceride, and reduction in body weight and waist circumference were independent factors associated with remission of NAFLD in non-obese patients. Half of non-obese patients achieved remission of NAFLD with 3–5% weight reduction; the same could only be achieved in obese patients with 7–10% weight reduction. By Year 6, non-obese patients in the intervention group remained more likely to maintain weight reduction and alanine aminotransferase normalisation than the control group. Lifestyle intervention is effective in treating NAFLD in both non-obese and obese patients. Weight reduction predicts remission of NAFLD in non-obese patients, but a modest weight reduction may be sufficient in this population. Non-alcoholic fatty liver disease (NAFLD) is currently the most common chronic liver disease and is one of the leading causes of end-stage liver disease and hepatocellular carcinoma worldwide.1,2Although NAFLD is strongly associated with metabolic syndrome and obesity,3 around 10–20% of patients with NAFLD have a relatively normal body mass index (BMI), a condition often described as non-obese or lean NAFLD.4Studies based on liver histology or non-invasive tests of fibrosis suggest that these non-obese patients may also harbour non-alcoholic steatohepatitis (NASH) and advanced fibrosis.5–8
Around 10–20% of patients with non-alcoholic fatty liver disease (NAFLD) are non-obese. The benefit of weight reduction in such patients is unclear. We aim to study the efficacy of lifestyle intervention in non-obese patients with NAFLD and to identify factors that predict treatment response. A total of 154 community NAFLD patients were randomised to a 12-month lifestyle intervention programme involving regular exercise, or to standard care. The primary outcome was remission of NAFLD at Month 12 by proton-magnetic resonance spectroscopy. After the programme, the patients were prospectively followed until Year 6. The Asian body mass index (BMI) cut-off of 25 kg/m2 was used to define non-obese NAFLD. Patients were assigned to the intervention (n = 77) and control (n = 77) groups (39 and 38 in each group had baseline BMI <25 and ≥25 kg/m2, respectively). More patients in the intervention group achieved the primary outcome than the control group regardless of baseline BMI (non-obese: 67% vs. 18%, p <0.001; obese: 61% vs. 21%, p <0.001). Lifestyle intervention, lower baseline intrahepatic triglyceride, and reduction in body weight and waist circumference were independent factors associated with remission of NAFLD in non-obese patients. Half of non-obese patients achieved remission of NAFLD with 3–5% weight reduction; the same could only be achieved in obese patients with 7–10% weight reduction. By Year 6, non-obese patients in the intervention group remained more likely to maintain weight reduction and alanine aminotransferase normalisation than the control group. Lifestyle intervention is effective in treating NAFLD in both non-obese and obese patients. Weight reduction predicts remission of NAFLD in non-obese patients, but a modest weight reduction may be sufficient in this population. Similar to the management of other metabolic disorders, lifestyle modification is the cornerstone for the management of NAFLD.A number of diets including low-carbohydrate diet, low-fat diet, low-glycaemic index diet and the Mediterranean diet have been shown to improve liver enzyme levels, liver fat, and histology in patients with NAFLD.9–17Likewise, beneficial effects have been observed for both aerobic exercise and resistance training.18,19In particular, a reduction of 10% or more in body weight results in the remission of NAFLD and NASH in the majority and improvement in liver fibrosis in almost half of patients.20,21
Embedded into a complex signaling network that coordinates glucose uptake, usage and production, the nuclear bile acid receptor FXR is expressed in several glucose-processing organs including the liver. Hepatic gluconeogenesis is controlled through allosteric regulation of gluconeogenic enzymes and by glucagon/cAMP-dependent transcriptional regulatory pathways. We aimed to elucidate the role of FXR in the regulation of fasting hepatic gluconeogenesis. The role of FXR in hepatic gluconeogenesis was assessed in vivo and in mouse primary hepatocytes. Gene expression patterns in response to glucagon and FXR agonists were characterized by quantitative reverse transcription PCR and microarray analysis. FXR phosphorylation by protein kinase A was determined by mass spectrometry. The interaction of FOXA2 with FXR was identified by cistromic approaches and in vitro protein-protein interaction assays. The functional impact of the crosstalk between FXR, the PKA and FOXA2 signaling pathways was assessed by site-directed mutagenesis, transactivation assays and restoration of FXR expression in FXR-deficient hepatocytes in which gene expression and glucose production were assessed. FXR positively regulates hepatic glucose production through two regulatory arms, the first one involving protein kinase A-mediated phosphorylation of FXR, which allowed for the synergistic activation of gluconeogenic genes by glucagon, agonist-activated FXR and CREB. The second arm involves the inhibition of FXR’s ability to induce the anti-gluconeogenic nuclear receptor SHP by the glucagon-activated FOXA2 transcription factor, which physically interacts with FXR. Additionally, knockdown of Foxa2 did not alter glucagon-induced and FXR agonist enhanced expression of gluconeogenic genes, suggesting that the PKA and FOXA2 pathways regulate distinct subsets of FXR responsive genes. Thus, hepatic glucose production is regulated during physiological fasting by FXR, which integrates the glucagon/cAMP signal and the FOXA2 signal, by being post-translationally modified, and by engaging in protein-protein interactions, respectively. Glucose supply to tissues is maintained through a complex regulatory network mostly driven by the pancreatic hormones insulin and glucagon which control glucose use, storage and synthesis.Through glycogenolysis and gluconeogenesis, the liver contributes to ∼70–80% of glucose production during an overnight fast,1 the remaining 30% coming from intestinal and kidney gluconeogenesis in physiological conditions.2,3
Embedded into a complex signaling network that coordinates glucose uptake, usage and production, the nuclear bile acid receptor FXR is expressed in several glucose-processing organs including the liver. Hepatic gluconeogenesis is controlled through allosteric regulation of gluconeogenic enzymes and by glucagon/cAMP-dependent transcriptional regulatory pathways. We aimed to elucidate the role of FXR in the regulation of fasting hepatic gluconeogenesis. The role of FXR in hepatic gluconeogenesis was assessed in vivo and in mouse primary hepatocytes. Gene expression patterns in response to glucagon and FXR agonists were characterized by quantitative reverse transcription PCR and microarray analysis. FXR phosphorylation by protein kinase A was determined by mass spectrometry. The interaction of FOXA2 with FXR was identified by cistromic approaches and in vitro protein-protein interaction assays. The functional impact of the crosstalk between FXR, the PKA and FOXA2 signaling pathways was assessed by site-directed mutagenesis, transactivation assays and restoration of FXR expression in FXR-deficient hepatocytes in which gene expression and glucose production were assessed. FXR positively regulates hepatic glucose production through two regulatory arms, the first one involving protein kinase A-mediated phosphorylation of FXR, which allowed for the synergistic activation of gluconeogenic genes by glucagon, agonist-activated FXR and CREB. The second arm involves the inhibition of FXR’s ability to induce the anti-gluconeogenic nuclear receptor SHP by the glucagon-activated FOXA2 transcription factor, which physically interacts with FXR. Additionally, knockdown of Foxa2 did not alter glucagon-induced and FXR agonist enhanced expression of gluconeogenic genes, suggesting that the PKA and FOXA2 pathways regulate distinct subsets of FXR responsive genes. Thus, hepatic glucose production is regulated during physiological fasting by FXR, which integrates the glucagon/cAMP signal and the FOXA2 signal, by being post-translationally modified, and by engaging in protein-protein interactions, respectively. Glucagon-induced gluconeogenesis is the only source of glucose when glycogen stores are exhausted during fasting.Gluconeogenic substrates (lactate, alanine, and pyruvate) are funneled to mitochondria to generate oxaloacetate (OAA) through biotin-dependent pyruvate carboxylase.Cytosolic OAA is decarboxylated and phosphorylated to yield phosphoenolpyruvate (PEP), the primary building block of glucose, through phosphoenolpyruvate carboxylase (PEPCK), a rate-limiting enzyme of gluconeogenesis.Glycerol from triglyceride breakdown also contributes by varying extents to gluconeogenesis, feeding into the gluconeogenic pathway as glyceraldehyde-3-phosphate to generate fructose 1,6-biphosphate, the substrate of fructose 1,6-bisphosphatase (FBP1) which irreversibly yields fructose 6-phosphate (F6P).4The regulation of hepatic glucose production (HGP) is achieved through a sophisticated signaling network involving post-translational protein modifications, allosteric regulation and transcription factor activation and repression5,6 which essentially control the gene expression of three rate-limiting enzymes, glucose-6-phosphatase (G6pc), Fbp1 and PEPCK (Pck1)7 in a glucagon/cAMP-dependent manner.8–11
Assembly of infectious hepatitis C virus (HCV) particles is known to involve host lipoproteins, giving rise to unique lipo-viro-particles (LVPs), but proteome studies now suggest that additional cellular proteins are associated with HCV virions or other particles containing the viral envelope glycoprotein E2. Many of these host cell proteins are common markers of exosomes, most notably the intracellular adaptor protein syntenin, which is required for exosome biogenesis. We aimed to elucidate the role of syntenin/E2 in HCV infection. Using cell culture-derived HCV, we studied the biogenesis and function of E2-coated exosomes in both hepatoma cells and primary human hepatocytes (PHHs). Knockout of syntenin had a negligible impact on HCV replication and virus production, whereas ectopic expression of syntenin at physiological levels reduced intracellular E2 abundance, while concomitantly increasing the secretion of E2-coated exosomes. Importantly, cells expressing syntenin and HCV structural proteins efficiently released exosomes containing E2 but lacking the core protein. Furthermore, infectivity of HCV released from syntenin-expressing hepatoma cells and PHHs was more resistant to neutralization by E2-specific antibodies and chronic-phase patient serum. We also found that high E2/syntenin levels in sera correlate with lower serum neutralization capability. E2- and syntenin-containing exosomes are a major type of particle released from cells expressing high levels of syntenin. Efficient production of E2-coated exosomes renders HCV infectivity less susceptible to antibody neutralization in hepatoma cells and PHHs. Around 71 million people worldwide are infected with the hepatitis C virus (HCV).1Up to 80% of infected individuals are unable to clear the virus.Persistently infected individuals have a high risk of developing liver cirrhosis and hepatocellular carcinoma.Direct-acting antivirals have profoundly increased treatment efficiency, but a prophylactic vaccine required for the global control of new HCV infections is not available.
Assembly of infectious hepatitis C virus (HCV) particles is known to involve host lipoproteins, giving rise to unique lipo-viro-particles (LVPs), but proteome studies now suggest that additional cellular proteins are associated with HCV virions or other particles containing the viral envelope glycoprotein E2. Many of these host cell proteins are common markers of exosomes, most notably the intracellular adaptor protein syntenin, which is required for exosome biogenesis. We aimed to elucidate the role of syntenin/E2 in HCV infection. Using cell culture-derived HCV, we studied the biogenesis and function of E2-coated exosomes in both hepatoma cells and primary human hepatocytes (PHHs). Knockout of syntenin had a negligible impact on HCV replication and virus production, whereas ectopic expression of syntenin at physiological levels reduced intracellular E2 abundance, while concomitantly increasing the secretion of E2-coated exosomes. Importantly, cells expressing syntenin and HCV structural proteins efficiently released exosomes containing E2 but lacking the core protein. Furthermore, infectivity of HCV released from syntenin-expressing hepatoma cells and PHHs was more resistant to neutralization by E2-specific antibodies and chronic-phase patient serum. We also found that high E2/syntenin levels in sera correlate with lower serum neutralization capability. E2- and syntenin-containing exosomes are a major type of particle released from cells expressing high levels of syntenin. Efficient production of E2-coated exosomes renders HCV infectivity less susceptible to antibody neutralization in hepatoma cells and PHHs. A hallmark of infectious HCV particles is their intimate association with components of very low density lipoproteins and low density lipoproteins, giving rise to hybrid virions, designated lipo-viro-particles (LVPs), which are characterized by heterogeneous buoyant densities.2–5Core protein, the RNA genome and the heterodimeric E1/E2 envelope glycoprotein complex are major components of infectious LVPs.6–8In addition, the non-exchangeable apolipotein (apo) B and several exchangeable apolipoproteins (apoE, AI, CI and CIII) were found to be incorporated into LVPs.8–15Apart from infectious LVPs, non-infectious lipoprotein-like particles have been reported.16,17These particles contain E1/E2 complexes and apoE and apoB but lack the core protein.In addition, the proteome of E2-affinity purified HCV particles identified multiple cellular proteins specifically associated with E2-containing particles.18Interestingly, many of these proteins are common markers of exosomes.In particular, syntenin (also known as syndecan binding protein; SDCBP), the major intracellular adaptor protein for exosome biogenesis and cargo loading19–21 was co-purified with E2-containing particles.Gain and loss of function studies established that syntenin works as a key regulator, together with syndecan and Alix, to control the formation of intraluminal vesicles (ILV) and the release of heat shock protein 70 (HSP70) as well as tetraspanin-containing exosomes from multivesicular bodies (MVBs).These results suggest that in addition to being part of infectious HCV, LVPs and lipoprotein-like subviral particles, E2 might be a component of exosomes.Although E2 and exosome-like vesicle association has been implied through electron microscopy (EM) analysis,7,22,23 the major determinant of E2-exosome production and its function were not known.
Aberrant oncogenic mRNA translation and protein O-linked β-N-acetylglucosaminylation (O-GlcNAcylation) are general features during tumorigenesis. Nevertheless, whether and how these two pathways are interlinked remain unknown. Our previous study indicated that ribosomal receptor for activated C-kinase 1 (RACK1) promoted chemoresistance and growth in hepatocellular carcinoma (HCC). The aim of this study is to examine the role of RACK1 O-GlcNAcylation in oncogene translation and HCC carcinogenesis. The site(s) of RACK1 for O-GlcNAcylation was mapped by mass spectrometry analysis. HCC cell lines were employed to examine the effects of RACK1 O-GlcNAcylation on the translation of oncogenic factors and behaviors of tumor cells in vitro. Transgenic knock-in mice were used to detect the role of RACK1 O-GlcNAcylation in modulating HCC tumorigenesis in vivo. The correlation of RACK1 O-GlcNAcylation with tumor progression and relapse were analyzed in clinical HCC samples. We found that ribosomal RACK1 was highly modified by O-GlcNAc at Ser122. O-GlcNAcylation of RACK1 enhanced its protein stability, ribosome binding and interaction with PKCβII (PRKCB), leading to increased eukaryotic translation initiation factor 4E phosphorylation and translation of potent oncogenes in HCC cells. Genetic ablation of RACK1 O-GlcNAcylation at Ser122 dramatically suppressed tumorigenesis, angiogenesis, and metastasis in vitro and in diethylnitrosamine (DEN)-induced HCC mouse model. Increased RACK1 O-GlcNAcylation was also observed in HCC patient samples and correlated with tumor development and recurrence after chemotherapy. These findings demonstrate that RACK1 acts as key mediator linking O-GlcNAc metabolism to cap-dependent translation during HCC tumorigenesis. Targeting RACK1 O-GlcNAcylation provides promising options for HCC treatment. Hepatocellular carcinoma (HCC) is the sixth-most frequent and the second-most lethal cancer worldwide, with a rising incidence in developing and industrialized countries.1Until now, surgery has remained the most effective treatment with curative potential.Nevertheless, most patients are still diagnosed at an advanced/late stage when surgery is no longer applicable, and display symptoms of intrahepatic and extrahepatic metastasis.2Few effective treatment options exist for patients with advanced HCC, with a five-year survival rate of only 30%–40%.3A high rate of postsurgical metastasis and relapse remains a major challenge in HCC, owing to the fact that this disease is highly resistant to conventional chemotherapy and radiation.4Therefore, there is an urgent need to better understand the mechanisms contributing to the pathogenesis of HCC, to identify possible preventive strategies and therapeutic targets.
Aberrant oncogenic mRNA translation and protein O-linked β-N-acetylglucosaminylation (O-GlcNAcylation) are general features during tumorigenesis. Nevertheless, whether and how these two pathways are interlinked remain unknown. Our previous study indicated that ribosomal receptor for activated C-kinase 1 (RACK1) promoted chemoresistance and growth in hepatocellular carcinoma (HCC). The aim of this study is to examine the role of RACK1 O-GlcNAcylation in oncogene translation and HCC carcinogenesis. The site(s) of RACK1 for O-GlcNAcylation was mapped by mass spectrometry analysis. HCC cell lines were employed to examine the effects of RACK1 O-GlcNAcylation on the translation of oncogenic factors and behaviors of tumor cells in vitro. Transgenic knock-in mice were used to detect the role of RACK1 O-GlcNAcylation in modulating HCC tumorigenesis in vivo. The correlation of RACK1 O-GlcNAcylation with tumor progression and relapse were analyzed in clinical HCC samples. We found that ribosomal RACK1 was highly modified by O-GlcNAc at Ser122. O-GlcNAcylation of RACK1 enhanced its protein stability, ribosome binding and interaction with PKCβII (PRKCB), leading to increased eukaryotic translation initiation factor 4E phosphorylation and translation of potent oncogenes in HCC cells. Genetic ablation of RACK1 O-GlcNAcylation at Ser122 dramatically suppressed tumorigenesis, angiogenesis, and metastasis in vitro and in diethylnitrosamine (DEN)-induced HCC mouse model. Increased RACK1 O-GlcNAcylation was also observed in HCC patient samples and correlated with tumor development and recurrence after chemotherapy. These findings demonstrate that RACK1 acts as key mediator linking O-GlcNAc metabolism to cap-dependent translation during HCC tumorigenesis. Targeting RACK1 O-GlcNAcylation provides promising options for HCC treatment. Aberrant protein biosynthesis is a general feature during tumor transformation.5,6Historically, increased cancer cell proliferation has been shown to require accelerated protein synthesis and ribosome biogenesis.7Mutations that deregulate mRNA translation are also common events in human cancers.8Recently, translation reprogramming is reported to be an evolutionarily conserved driver of phenotypic plasticity and therapeutic resistance in melanoma.9Thus, deregulation of translation is regarded as an important step in tumorigenesis and cancer progression, directing both global control of protein synthesis and selective translation of specific mRNAs that promote tumor cell survival, angiogenesis, transformation, invasion and metastasis.5Small molecules that target mRNA translation have shown anti-tumor effects in cell and mouse models.10,11
Aberrant oncogenic mRNA translation and protein O-linked β-N-acetylglucosaminylation (O-GlcNAcylation) are general features during tumorigenesis. Nevertheless, whether and how these two pathways are interlinked remain unknown. Our previous study indicated that ribosomal receptor for activated C-kinase 1 (RACK1) promoted chemoresistance and growth in hepatocellular carcinoma (HCC). The aim of this study is to examine the role of RACK1 O-GlcNAcylation in oncogene translation and HCC carcinogenesis. The site(s) of RACK1 for O-GlcNAcylation was mapped by mass spectrometry analysis. HCC cell lines were employed to examine the effects of RACK1 O-GlcNAcylation on the translation of oncogenic factors and behaviors of tumor cells in vitro. Transgenic knock-in mice were used to detect the role of RACK1 O-GlcNAcylation in modulating HCC tumorigenesis in vivo. The correlation of RACK1 O-GlcNAcylation with tumor progression and relapse were analyzed in clinical HCC samples. We found that ribosomal RACK1 was highly modified by O-GlcNAc at Ser122. O-GlcNAcylation of RACK1 enhanced its protein stability, ribosome binding and interaction with PKCβII (PRKCB), leading to increased eukaryotic translation initiation factor 4E phosphorylation and translation of potent oncogenes in HCC cells. Genetic ablation of RACK1 O-GlcNAcylation at Ser122 dramatically suppressed tumorigenesis, angiogenesis, and metastasis in vitro and in diethylnitrosamine (DEN)-induced HCC mouse model. Increased RACK1 O-GlcNAcylation was also observed in HCC patient samples and correlated with tumor development and recurrence after chemotherapy. These findings demonstrate that RACK1 acts as key mediator linking O-GlcNAc metabolism to cap-dependent translation during HCC tumorigenesis. Targeting RACK1 O-GlcNAcylation provides promising options for HCC treatment. O-linked β-N-acetylglucosamine (O-GlcNAc) is a dynamic and inducible post-translational modification on serine and/or threonine residues of nuclear and cytosolic proteins.12Catalyzed by O-GlcNAc-transferase (OGT) and removed by O-GlcNAcase (OGA), this dynamic modification is dependent on environmental glucose concentration.12O-linked β-N-acetylglucosaminylation (O-GlcNAcylation) regulates the activities of a wide panel of proteins involved in almost all aspects of cell biology.12,13It also has become increasingly realized that O‑GlcNAcylation has important roles in cancer-relevant processes, such as cell signaling, transcription, epigenetics, metabolism and cytoskeletal regulation, while comprehension of the underlying mechanism remains at its beginnings.13Interestingly, recent research indicates that O-GlcNAc cycling enzymes associate with translational machinery and modify ribosomal proteins.14,15Nevertheless, little is currently known about the specific roles of aberrant O-GlcNAcylation in translational control of cancer cells.
The lipid-binding protein, SEC14L2, is crucial for the efficient viral replication of clinical hepatitis C virus (HCV) isolates in cell culture. Given the role of SEC14L2 in HCV replication, we aimed to study a large number of HCV positive sera carrying genotypes 1–4, to identify viral factors associated with efficient replication in culture. Additionally, we investigated whether 13 single nucleotide polymorphisms (SNPs) of SEC14L2 have an impact on RNA replication of naturally occurring HCV isolates. We generated Huh-7.5 cell lines overexpressing SEC14L2 or 13 coding SNPs and tested 73 different HCV positive sera for in vitro replication. Furthermore, we genotyped a cohort of 262 patients with chronic HCV for the common SNP (rs757660) and investigated its effect on the clinical phenotype. HCV isolates from genotype 1, 2, 3 and 4 replicate in Huh-7.5 cells overexpressing SEC14L2. Interestingly, only subgenomic replicons from genotypes 1 and 3 showed enhanced replication whereas genotypes 2 and 4 remained unaffected. Furthermore, replication was independent of viral load. Importantly, all tested SNPs supported HCV RNA replication in vitro, while 1 SNP was associated with decreased SEC1L2 expression and viral RNA. All SNPs exhibited comparable cellular cholesterol and vitamin E abundance in naïve Huh-7.5 cells. This large screen of natural HCV isolates of 4 genotypes underscores the relevance of SEC14L2 as an in vitro HCV host factor. Additionally, SEC14L2 variants appear to recapitulate the wild-type enhancement of HCV replication. Variant rs191341134 showed a decreased effect due to lowered stability, whereas variant rs757660, a high prevalence mutant, showed a similar phenotype to the wild-type. Hepatitis C virus (HCV) is a positive stranded RNA Hepacivirus in the Flaviviridae family.Today, HCV remains an important element in the aetiology of chronic liver disease and according to the World Health Organization there are estimated to be 71 million chronically infected patients worldwide.1Since the discovery of the virus in 1989, the development of tools such as a subgenomic replicon system, cell culture adapted variants and highly permissive hepatoma cell lines have enabled researchers to study different aspects of the HCV life cycle.This paved the way for the generation of effective antiviral therapies against the virus.2,3Nevertheless, replication of non-cell culture adapted variants remained a difficult achievement and later facets of the viral life cycle are yet to be fully described.This status quo was challenged in 2015 when pan-genotype (GT) replication of patient-derived isolates was shown by Saeed et al.They were able to show that the overexpression of the lipid-binding protein SEC14L2 in a Huh-7.5 hepatoma cell line allowed efficient replication of non-cell culture adapted HCV isolates, through a vitamin E mediated mechanism of lipid peroxidation resistance.4Curiously vitamin E supplementation was not sufficient to allow replicon colony formation in the absence of SEC14L2 which would suggest an alternative mechanism may be at play.The context of lipid peroxidation as a regulatory mechanism of HCV had already been suggested by Yamane et al.Yamane and co-authors showed that sphingosine kinase-2 partly regulated lipid peroxidation and the data implied a significant role of this regulation upon HCV replication.The EC50 of tested HCV antivirals was shown to increase in the presence of vitamin E or the sphingosine kinase inhibitor.5Further data on the subject of non-cell culture adapted virus replication were made available in 2016 when a mechanism for viral dependence on host phosphatidylinositol 4-kinase IIIα (PI4KA) was reported to be a determinant factor in hepatoma cell lines.Harak et al. were able to show that adaptive mutations with loss of function in NS5A-NS5B were necessary for efficient replication in hepatoma cell lines.The difference in the expression of PI4KA, and consequently, the abundance of PiP4, were pointed out as being detrimental for viral replication as they were present in excess when comparing model hepatoma cell lines with primary human hepatocytes.6Together these new data provided an important understanding of the role of host factors and lipid peroxidation in the life cycle of HCV, chief among them, SEC14L2.
The lipid-binding protein, SEC14L2, is crucial for the efficient viral replication of clinical hepatitis C virus (HCV) isolates in cell culture. Given the role of SEC14L2 in HCV replication, we aimed to study a large number of HCV positive sera carrying genotypes 1–4, to identify viral factors associated with efficient replication in culture. Additionally, we investigated whether 13 single nucleotide polymorphisms (SNPs) of SEC14L2 have an impact on RNA replication of naturally occurring HCV isolates. We generated Huh-7.5 cell lines overexpressing SEC14L2 or 13 coding SNPs and tested 73 different HCV positive sera for in vitro replication. Furthermore, we genotyped a cohort of 262 patients with chronic HCV for the common SNP (rs757660) and investigated its effect on the clinical phenotype. HCV isolates from genotype 1, 2, 3 and 4 replicate in Huh-7.5 cells overexpressing SEC14L2. Interestingly, only subgenomic replicons from genotypes 1 and 3 showed enhanced replication whereas genotypes 2 and 4 remained unaffected. Furthermore, replication was independent of viral load. Importantly, all tested SNPs supported HCV RNA replication in vitro, while 1 SNP was associated with decreased SEC1L2 expression and viral RNA. All SNPs exhibited comparable cellular cholesterol and vitamin E abundance in naïve Huh-7.5 cells. This large screen of natural HCV isolates of 4 genotypes underscores the relevance of SEC14L2 as an in vitro HCV host factor. Additionally, SEC14L2 variants appear to recapitulate the wild-type enhancement of HCV replication. Variant rs191341134 showed a decreased effect due to lowered stability, whereas variant rs757660, a high prevalence mutant, showed a similar phenotype to the wild-type. SEC14L2 is a human homologue of a lipid-binding protein first described in mice in 1977 as supernatant protein factor.7This protein is closely related to yeast sec14p proteins and it is characterized by a CRAL-TRIO domain.It has been reported to function as a putative transporter of lipophilic molecules between cellular compartments and to be intimately connected to the availability of vitamin E.8,9 It is characterized by high expression levels in the brain, prostate and the liver.10Several authors have attributed a role in vitamin E distribution in the cytosol and in the abundance of vitamin E in serum to SEC14L2/hTAP.11–13
The lipid-binding protein, SEC14L2, is crucial for the efficient viral replication of clinical hepatitis C virus (HCV) isolates in cell culture. Given the role of SEC14L2 in HCV replication, we aimed to study a large number of HCV positive sera carrying genotypes 1–4, to identify viral factors associated with efficient replication in culture. Additionally, we investigated whether 13 single nucleotide polymorphisms (SNPs) of SEC14L2 have an impact on RNA replication of naturally occurring HCV isolates. We generated Huh-7.5 cell lines overexpressing SEC14L2 or 13 coding SNPs and tested 73 different HCV positive sera for in vitro replication. Furthermore, we genotyped a cohort of 262 patients with chronic HCV for the common SNP (rs757660) and investigated its effect on the clinical phenotype. HCV isolates from genotype 1, 2, 3 and 4 replicate in Huh-7.5 cells overexpressing SEC14L2. Interestingly, only subgenomic replicons from genotypes 1 and 3 showed enhanced replication whereas genotypes 2 and 4 remained unaffected. Furthermore, replication was independent of viral load. Importantly, all tested SNPs supported HCV RNA replication in vitro, while 1 SNP was associated with decreased SEC1L2 expression and viral RNA. All SNPs exhibited comparable cellular cholesterol and vitamin E abundance in naïve Huh-7.5 cells. This large screen of natural HCV isolates of 4 genotypes underscores the relevance of SEC14L2 as an in vitro HCV host factor. Additionally, SEC14L2 variants appear to recapitulate the wild-type enhancement of HCV replication. Variant rs191341134 showed a decreased effect due to lowered stability, whereas variant rs757660, a high prevalence mutant, showed a similar phenotype to the wild-type. A common coding single nucleotide polymorphism (SNP) in SEC14L2 (rs757660) has been associated with alpha-tocopherol levels in the serum and the risk of prostate cancer.13but the impact of this SEC14L2 variant on HCV has not been addressed so far.At least 13 validated coding non-synonymous SNPs with a minor allele frequency (MAF) of at least 0.0002, each resulting in the exchange of a single amino acid in the SEC14L2 protein have been deposited in the National Center for Biotechnology Information dbSNP database (www.ncbi.nlm.nih.gov/snp) (Table 1).If these SNPs have a clinical phenotype is presently unknown.
As many as 70% of individuals with chronic hepatitis C (CHC) are managed solely in primary care. The aims of this study were to determine the prevalence of elevated liver stiffness measurement (LSM) in a cohort of community managed patients with CHC and to evaluate predictors of advanced liver disease and liver-related events. A prospective cohort of adult patients with CHC were recruited from 21 primary care practices throughout Victoria, Australia. Inclusion criteria included the presence of CHC for >6 months, no recent (<18 months) specialist input and no history of hepatocellular carcinoma. Clinical assessment, LSM and phlebotomy were carried out in primary care. A hospital cohort was recruited for comparison. Participants were followed longitudinally and monitored for liver-related events. Over 26 months, 780 community patients were recruited and included in the analysis. The median LSM was 6.9 kPa in the community, with 16.5% of patients at risk of advanced fibrosis (LSM ≥12.5 kPa); of these 8.5% had no laboratory features of advanced liver disease. The proportion at risk of cirrhosis was no different between the community and hospital cohorts (p = 0.169). At-risk alcohol consumption, advancing age, elevated body mass index and alanine aminotransferase were independent predictors of elevated LSM. Over a median follow-up of 15.2 months, liver-related events occurred in 9.3% of those with an LSM ≥12.5 kPa. An LSM of 24 kPa had the highest predictive power for liver-related events (hazard ratio 152; p <0.001). The prevalence of advanced fibrosis, as determined by LSM, in primary care managed CHC is significant and comparable to a hospital cohort. Furthermore, this study supports the use of LSM as a community screening tool in a CHC population and indicates a possible role in predicting liver-related events. Chronic hepatitis C (CHC) is a major public health issue with an estimated global prevalence greater than 71 million cases and over 1.75 million individuals infected annually.1Together with chronic hepatitis B (CHB), CHC is accountable for over 1.34 million deaths worldwide annually, being the second highest cause of death from communicable diseases and ranked seventh for all-cause mortality.2The rapidly increasing rates of hepatocellular carcinoma (HCC) in the developed world have largely been attributed to increased rates of CHC infection since 1975.3Furthermore, early identification of cirrhosis is associated with improved patient outcomes and HCC survival.4–6
As many as 70% of individuals with chronic hepatitis C (CHC) are managed solely in primary care. The aims of this study were to determine the prevalence of elevated liver stiffness measurement (LSM) in a cohort of community managed patients with CHC and to evaluate predictors of advanced liver disease and liver-related events. A prospective cohort of adult patients with CHC were recruited from 21 primary care practices throughout Victoria, Australia. Inclusion criteria included the presence of CHC for >6 months, no recent (<18 months) specialist input and no history of hepatocellular carcinoma. Clinical assessment, LSM and phlebotomy were carried out in primary care. A hospital cohort was recruited for comparison. Participants were followed longitudinally and monitored for liver-related events. Over 26 months, 780 community patients were recruited and included in the analysis. The median LSM was 6.9 kPa in the community, with 16.5% of patients at risk of advanced fibrosis (LSM ≥12.5 kPa); of these 8.5% had no laboratory features of advanced liver disease. The proportion at risk of cirrhosis was no different between the community and hospital cohorts (p = 0.169). At-risk alcohol consumption, advancing age, elevated body mass index and alanine aminotransferase were independent predictors of elevated LSM. Over a median follow-up of 15.2 months, liver-related events occurred in 9.3% of those with an LSM ≥12.5 kPa. An LSM of 24 kPa had the highest predictive power for liver-related events (hazard ratio 152; p <0.001). The prevalence of advanced fibrosis, as determined by LSM, in primary care managed CHC is significant and comparable to a hospital cohort. Furthermore, this study supports the use of LSM as a community screening tool in a CHC population and indicates a possible role in predicting liver-related events. In Australia an estimated 230,000 individuals are living with CHC.One-quarter or 55,000 of this population reside in the state of Victoria (over a geographical area of 237,000 km2).7Prior estimates in this population suggest that 7–8% of these individuals have cirrhosis.3,8While 75% of patients with CHC have been identified following the adoption of screening guidelines,9 50–70% of these patients are managed solely in primary care.10Despite free universal access, there is a relative lack of engagement in specialist care.The reason for this is multifactorial and includes distance, long waiting list times, a perception of toxic treatment regimens and stigma from health service providers.
As many as 70% of individuals with chronic hepatitis C (CHC) are managed solely in primary care. The aims of this study were to determine the prevalence of elevated liver stiffness measurement (LSM) in a cohort of community managed patients with CHC and to evaluate predictors of advanced liver disease and liver-related events. A prospective cohort of adult patients with CHC were recruited from 21 primary care practices throughout Victoria, Australia. Inclusion criteria included the presence of CHC for >6 months, no recent (<18 months) specialist input and no history of hepatocellular carcinoma. Clinical assessment, LSM and phlebotomy were carried out in primary care. A hospital cohort was recruited for comparison. Participants were followed longitudinally and monitored for liver-related events. Over 26 months, 780 community patients were recruited and included in the analysis. The median LSM was 6.9 kPa in the community, with 16.5% of patients at risk of advanced fibrosis (LSM ≥12.5 kPa); of these 8.5% had no laboratory features of advanced liver disease. The proportion at risk of cirrhosis was no different between the community and hospital cohorts (p = 0.169). At-risk alcohol consumption, advancing age, elevated body mass index and alanine aminotransferase were independent predictors of elevated LSM. Over a median follow-up of 15.2 months, liver-related events occurred in 9.3% of those with an LSM ≥12.5 kPa. An LSM of 24 kPa had the highest predictive power for liver-related events (hazard ratio 152; p <0.001). The prevalence of advanced fibrosis, as determined by LSM, in primary care managed CHC is significant and comparable to a hospital cohort. Furthermore, this study supports the use of LSM as a community screening tool in a CHC population and indicates a possible role in predicting liver-related events. In order to reach the World Health Organization’s goal of CHC elimination,11 the engagement of non-specialist physicians in CHC treatment and management is paramount.Australia has already demonstrated a rapid uptake of direct-acting antiviral (DAA) therapy through the unique expanded role of primary care, with 19% of treatment initiated by primary care physicians.12With increased numbers of patients with CHC treated in the community setting, it is imperative to identify those individuals most at risk of liver-related morbidity and mortality and institute appropriate interventions.As many patients with chronic liver disease remain asymptomatic until the development of clinically significant portal hypertension, it is a challenge for primary care providers to identify those requiring further specialist input and HCC screening.
Sorafenib is first-line standard of care for patients with advanced hepatocellular carcinoma (HCC), yet it confers limited survival benefit. Therefore, we aimed to compare clinical outcomes of sorafenib combined with concurrent conventional transarterial chemoembolization (cTACE) vs. sorafenib alone in patients with advanced HCC. In this investigator-initiated, multicenter, phase III trial, patients were randomized to receive sorafenib alone (Arm S, n = 169) or in combination with cTACE on demand (Arm C, n = 170). Sorafenib was started within 3 days and cTACE within 7–21 days of randomization. The primary endpoint was overall survival (OS). For Arms C and S, the median OS was 12.8 vs. 10.8 months (hazard ratio [HR] 0.91; 90% CI 0.69–1.21; p = 0.290); median time to progression, 5.3 vs. 3.5 months (HR 0.67; 90% CI 0.53–0.85; p = 0.003); median progression-free survival, 5.2 vs. 3.6 months (HR 0.73; 90% CI 0.59–0.91; p = 0.01); and tumor response rate, 60.6% vs. 47.3% (p = 0.005). For Arms C and S, serious (grade ≥3) adverse events occurred in 33.3% vs. 19.8% (p = 0.006) of patients and included increased alanine aminotransferase levels (20.3% vs. 3.6%), hyperbilirubinemia (11.8% vs. 3.0%), ascites (11.8% vs. 4.2%), thrombocytopenia (7.2% vs. 1.2%), anorexia (7.2% vs. 1.2%), and hand-foot skin reaction (10.5% vs. 11.4%). A post hoc subgroup analysis compared OS in Arm C patients (46.4%) receiving ≥2 cTACE sessions to Arm S patients (18.6 vs. 10.8 months; HR 0.58; 95% CI 0.40–0.82; p = 0.006). Compared with sorafenib alone, sorafenib combined with cTACE did not improve OS in patients with advanced HCC. However, sorafenib combined with cTACE significantly improved time to progression, progression-free survival, and tumor response rate. Sorafenib alone remains the first-line standard of care for patients with advanced HCC. Worldwide, primary liver cancer, including hepatocellular carcinoma (HCC), is the fifth most common cancer and the second leading cause of cancer-related mortality.1HCC prognosis remains poor because of the underlying chronic liver disease; late diagnosis, often at advanced stages of disease; and frequent recurrence/progression after treatment.2,3
Sorafenib is first-line standard of care for patients with advanced hepatocellular carcinoma (HCC), yet it confers limited survival benefit. Therefore, we aimed to compare clinical outcomes of sorafenib combined with concurrent conventional transarterial chemoembolization (cTACE) vs. sorafenib alone in patients with advanced HCC. In this investigator-initiated, multicenter, phase III trial, patients were randomized to receive sorafenib alone (Arm S, n = 169) or in combination with cTACE on demand (Arm C, n = 170). Sorafenib was started within 3 days and cTACE within 7–21 days of randomization. The primary endpoint was overall survival (OS). For Arms C and S, the median OS was 12.8 vs. 10.8 months (hazard ratio [HR] 0.91; 90% CI 0.69–1.21; p = 0.290); median time to progression, 5.3 vs. 3.5 months (HR 0.67; 90% CI 0.53–0.85; p = 0.003); median progression-free survival, 5.2 vs. 3.6 months (HR 0.73; 90% CI 0.59–0.91; p = 0.01); and tumor response rate, 60.6% vs. 47.3% (p = 0.005). For Arms C and S, serious (grade ≥3) adverse events occurred in 33.3% vs. 19.8% (p = 0.006) of patients and included increased alanine aminotransferase levels (20.3% vs. 3.6%), hyperbilirubinemia (11.8% vs. 3.0%), ascites (11.8% vs. 4.2%), thrombocytopenia (7.2% vs. 1.2%), anorexia (7.2% vs. 1.2%), and hand-foot skin reaction (10.5% vs. 11.4%). A post hoc subgroup analysis compared OS in Arm C patients (46.4%) receiving ≥2 cTACE sessions to Arm S patients (18.6 vs. 10.8 months; HR 0.58; 95% CI 0.40–0.82; p = 0.006). Compared with sorafenib alone, sorafenib combined with cTACE did not improve OS in patients with advanced HCC. However, sorafenib combined with cTACE significantly improved time to progression, progression-free survival, and tumor response rate. Sorafenib alone remains the first-line standard of care for patients with advanced HCC. Because there is no global consensus on the definition of advanced HCC, it encompasses a heterogenous group,4 generally indicated in cases with macrovascular invasion and extrahepatic spread or progression on curative treatments.In patients with advanced HCC, sorafenib, the first approved oral multi-tyrosine kinase inhibitor, is the standard first-line therapy;4–9 however, outcomes of most patients remain unsatisfactory.To augment or improve the modest effects of sorafenib, several trials have studied the effects of combining it with other systemic therapies10–12 or locoregional treatments, including radiofrequency ablation,13 transarterial chemoembolization (TACE),14,15 and radiotherapy.16Conventional TACE (cTACE) is an effective treatment for unresectable HCC, and most guidelines for HCC management recommend cTACE for intermediate stage or multifocal HCC.4,7–9In many countries, including South Korea, most patients with HCC present with unresectable disease, and TACE is most frequently performed across all disease stages, including advanced stage.2Contrary to Western guidelines, Asian guidelines recommend cTACE as an alternative treatment for advanced HCC according to low-level evidence.8,17
Sorafenib is first-line standard of care for patients with advanced hepatocellular carcinoma (HCC), yet it confers limited survival benefit. Therefore, we aimed to compare clinical outcomes of sorafenib combined with concurrent conventional transarterial chemoembolization (cTACE) vs. sorafenib alone in patients with advanced HCC. In this investigator-initiated, multicenter, phase III trial, patients were randomized to receive sorafenib alone (Arm S, n = 169) or in combination with cTACE on demand (Arm C, n = 170). Sorafenib was started within 3 days and cTACE within 7–21 days of randomization. The primary endpoint was overall survival (OS). For Arms C and S, the median OS was 12.8 vs. 10.8 months (hazard ratio [HR] 0.91; 90% CI 0.69–1.21; p = 0.290); median time to progression, 5.3 vs. 3.5 months (HR 0.67; 90% CI 0.53–0.85; p = 0.003); median progression-free survival, 5.2 vs. 3.6 months (HR 0.73; 90% CI 0.59–0.91; p = 0.01); and tumor response rate, 60.6% vs. 47.3% (p = 0.005). For Arms C and S, serious (grade ≥3) adverse events occurred in 33.3% vs. 19.8% (p = 0.006) of patients and included increased alanine aminotransferase levels (20.3% vs. 3.6%), hyperbilirubinemia (11.8% vs. 3.0%), ascites (11.8% vs. 4.2%), thrombocytopenia (7.2% vs. 1.2%), anorexia (7.2% vs. 1.2%), and hand-foot skin reaction (10.5% vs. 11.4%). A post hoc subgroup analysis compared OS in Arm C patients (46.4%) receiving ≥2 cTACE sessions to Arm S patients (18.6 vs. 10.8 months; HR 0.58; 95% CI 0.40–0.82; p = 0.006). Compared with sorafenib alone, sorafenib combined with cTACE did not improve OS in patients with advanced HCC. However, sorafenib combined with cTACE significantly improved time to progression, progression-free survival, and tumor response rate. Sorafenib alone remains the first-line standard of care for patients with advanced HCC. In a previous phase II study of patients with unresectable or advanced HCC, sorafenib combined with concurrent cTACE (SOR+T) demonstrated manageable safety and tended to improve outcomes.15TACE, inducing the upregulation of angiogenic factors by ischemic liver injury,18 plus an anti-angiogenic agent (sorafenib) may complementarily inhibit angiogenic factors and tumor growth.19
The clinical efficacy of ursodeoxycholic acid (UDCA) in primary biliary cholangitis (PBC) remains subject to debate as definitive randomized controlled trials are lacking. We aimed to determine whether UDCA prolongs liver transplant (LT)-free survival in patients with PBC. This international cohort study included patients from the Global PBC Study Group database, originating from 8 countries in Europe and North America. Both UDCA-treated and untreated patients were included. LT and death were assessed as a combined endpoint through Cox regression analyses, with inverse probability treatment weighting (IPTW). In the 3,902 patients included, the mean (SD) age was 54.3 (11.9) years, 3,552 patients (94.0%) were female, 3,529 patients (90.4%) were treated with UDCA and 373 patients (9.6%) were not treated. The median (interquartile range) follow-up was 7.8 (4.1–12.1) years. In total, 721 UDCA-treated patients and 145 untreated patients died or underwent LT. After IPTW, the 10-year cumulative LT-free survival was 79.7% (95% CI 78.1–81.2) among UDCA-treated patients and 60.7% (95% CI 58.2–63.4) among untreated patients (p <0.001). UDCA was associated with a statistically significant reduced risk of LT or death (hazard ratio 0.46, 95% CI 0.40–0.52; p <0.001). The hazard ratio remained statistically significant in all stages of disease. Patients classified as inadequate biochemical responders after 1 year of UDCA had a lower risk of LT or death than patients who were not treated (adjusted hazard ratio 0.56; 95% CI 0.45–0.69; p <0.001). The use of UDCA improves LT-free survival among patients with PBC, regardless of the disease stage and the observed biochemical response. These findings support UDCA as the current universal standard of care in PBC. Primary biliary cholangitis (PBC) is a chronic and usually slowly progressive liver disease with autoimmune features, histologically characterized by destruction of the small intrahepatic bile ducts.1,2The disease is primarily diagnosed based on an otherwise unexplained chronic elevation of serum alkaline phosphatase levels and the presence of anti-mitochondrial antibodies.Early identification of individuals with PBC is clinically challenging as symptoms are frequently absent.Identifying and managing patients with PBC is important, however, as the disease may silently progress towards cirrhosis and the survival of affected patients is substantially impaired.3
The clinical efficacy of ursodeoxycholic acid (UDCA) in primary biliary cholangitis (PBC) remains subject to debate as definitive randomized controlled trials are lacking. We aimed to determine whether UDCA prolongs liver transplant (LT)-free survival in patients with PBC. This international cohort study included patients from the Global PBC Study Group database, originating from 8 countries in Europe and North America. Both UDCA-treated and untreated patients were included. LT and death were assessed as a combined endpoint through Cox regression analyses, with inverse probability treatment weighting (IPTW). In the 3,902 patients included, the mean (SD) age was 54.3 (11.9) years, 3,552 patients (94.0%) were female, 3,529 patients (90.4%) were treated with UDCA and 373 patients (9.6%) were not treated. The median (interquartile range) follow-up was 7.8 (4.1–12.1) years. In total, 721 UDCA-treated patients and 145 untreated patients died or underwent LT. After IPTW, the 10-year cumulative LT-free survival was 79.7% (95% CI 78.1–81.2) among UDCA-treated patients and 60.7% (95% CI 58.2–63.4) among untreated patients (p <0.001). UDCA was associated with a statistically significant reduced risk of LT or death (hazard ratio 0.46, 95% CI 0.40–0.52; p <0.001). The hazard ratio remained statistically significant in all stages of disease. Patients classified as inadequate biochemical responders after 1 year of UDCA had a lower risk of LT or death than patients who were not treated (adjusted hazard ratio 0.56; 95% CI 0.45–0.69; p <0.001). The use of UDCA improves LT-free survival among patients with PBC, regardless of the disease stage and the observed biochemical response. These findings support UDCA as the current universal standard of care in PBC. Ursodeoxycholic acid (UDCA) is a choleretic and hydrophilic endogenous bile acid that is considered a safe and well-tolerated drug.4–6Based on the cumulative experience obtained with this drug over the past decades, UDCA is recommended as the standard treatment for PBC.4,6Long-term cohort studies have suggested an association between UDCA and improved liver transplant (LT)-free survival, but this was only based on the comparison of observed versus predicted LT-free survival according to the Mayo Risk Score, which estimates the prognosis when patients are left untreated.7–9Numerous randomized controlled trials (RCTs) have been performed as well, but all failed to show a difference in LT-free survival between placebo and UDCA-treated groups.10–19As did other more extensive meta-analyses, the Cochrane hepatobiliary group recently concluded once again that there is no demonstrated benefit of UDCA on LT and/or mortality.17–20Such positioning statements, in absence of definitive trials, have fueled the ongoing discussion about the therapeutic potential of UDCA.21–24This might explain the observation in a well-executed national PBC registry that, until recently, as many as 20% of patients remained untreated.25In another recent US-based cohort study the percentage of UDCA untreated patients was even as high as 30%.26However, the meta-analyses are based on inadequate RCTs that were limited by a small number of patients, insufficient dosages of UDCA, and short follow-up.Therefore, the evaluation of the clinical efficacy of UDCA in PBC should not be based on these RCTs alone.Nonetheless, there is an understandable reluctance to initiate new long-term, placebo-controlled RCTs in which many patients would be denied UDCA therapy, because of minimal safety concerns of UDCA and practical implications.
As hepatitis B virus (HBV) spreads through the infected liver it is simultaneously secreted into the blood. HBV-susceptible in vitro infection models do not efficiently amplify viral progeny or support cell-to-cell spread. We sought to establish a cell culture system for the amplification of infectious HBV from clinical specimens. An HBV-susceptible sodium-taurocholate cotransporting polypeptide-overexpressing HepG2 cell clone (HepG2-NTCPsec+) producing high titers of infectious progeny was selected. Secreted HBV progeny were characterized by native gel electrophoresis and electron microscopy. Comparative RNA-seq transcriptomics was performed to quantify the expression of host proviral and restriction factors. Viral spread routes were evaluated using HBV entry- or replication inhibitors, visualization of viral cell-to-cell spread in reporter cells, and nearest neighbor infection determination. Amplification kinetics of HBV genotypes B-D were analyzed. Infected HepG2-NTCPsec+ secreted high levels of large HBV surface protein-enveloped infectious HBV progeny with typical appearance under electron microscopy. RNA-seq transcriptomics revealed that HBV does not induce significant gene expression changes in HepG2-NTCPsec+, however, transcription factors favoring HBV amplification were more strongly expressed than in less permissive HepG2-NTCPsec−. Upon inoculation with HBV-containing patient sera, rates of infected cells increased from 10% initially to 70% by viral spread to adjacent cells, and viral progeny and antigens were efficiently secreted. HepG2-NTCPsec+ supported up to 1,300-fold net amplification of HBV genomes depending on the source of virus. Viral spread and amplification were abolished by entry and replication inhibitors; viral rebound was observed after inhibitor discontinuation. The novel HepG2-NTCPsec+ cells efficiently support the complete HBV life cycle, long-term viral spread and amplification of HBV derived from patients or cell culture, resembling relevant features of HBV-infected patients. Despite vaccination, chronic hepatitis B (CHB) has remained among the most widespread, life-shortening infectious diseases.Two billion people worldwide have been infected with the hepatitis B virus (HBV), including 257 million chronic carriers.Up to 30% of chronically HBV-infected adults will develop liver cirrhosis or hepatocellular carcinoma, accounting for 887,000 deaths annually.1Reverse transcriptase inhibitors and interferon-alpha can control viral replication and prevent CHB progression.However, these therapies do not act on HBV genomes in host cell nuclei that persist as covalently closed circular DNA (cccDNA).Thus, current treatment regimens are not curative, requiring lifelong therapy.
As hepatitis B virus (HBV) spreads through the infected liver it is simultaneously secreted into the blood. HBV-susceptible in vitro infection models do not efficiently amplify viral progeny or support cell-to-cell spread. We sought to establish a cell culture system for the amplification of infectious HBV from clinical specimens. An HBV-susceptible sodium-taurocholate cotransporting polypeptide-overexpressing HepG2 cell clone (HepG2-NTCPsec+) producing high titers of infectious progeny was selected. Secreted HBV progeny were characterized by native gel electrophoresis and electron microscopy. Comparative RNA-seq transcriptomics was performed to quantify the expression of host proviral and restriction factors. Viral spread routes were evaluated using HBV entry- or replication inhibitors, visualization of viral cell-to-cell spread in reporter cells, and nearest neighbor infection determination. Amplification kinetics of HBV genotypes B-D were analyzed. Infected HepG2-NTCPsec+ secreted high levels of large HBV surface protein-enveloped infectious HBV progeny with typical appearance under electron microscopy. RNA-seq transcriptomics revealed that HBV does not induce significant gene expression changes in HepG2-NTCPsec+, however, transcription factors favoring HBV amplification were more strongly expressed than in less permissive HepG2-NTCPsec−. Upon inoculation with HBV-containing patient sera, rates of infected cells increased from 10% initially to 70% by viral spread to adjacent cells, and viral progeny and antigens were efficiently secreted. HepG2-NTCPsec+ supported up to 1,300-fold net amplification of HBV genomes depending on the source of virus. Viral spread and amplification were abolished by entry and replication inhibitors; viral rebound was observed after inhibitor discontinuation. The novel HepG2-NTCPsec+ cells efficiently support the complete HBV life cycle, long-term viral spread and amplification of HBV derived from patients or cell culture, resembling relevant features of HBV-infected patients. HBV has a restricted host range and a narrow tissue tropism which has hampered the study of the HBV life cycle and the discovery of new therapeutic approaches.Only highly differentiated hepatocyte-derived cells from human or tree shrew (Tupaia) can be used for in vitro HBV infection studies.Hepatocytes derived from common laboratory animals like mice, rats, dogs, macaques or pigs do not support HBV infection.2,3The current gold standard for in vitro infection studies utilizes primary human hepatocytes (PHHs) and patient-derived HBV.However, PHH drawbacks include high cost, limited availability, difficult handling, donor variability, and dedifferentiation leading to rapid loss of HBV susceptibility and replication.4The hepatoma HepaRG cell line requires laborious re-differentiation that results in only moderate HBV susceptibility.5Novel models, i.e. self-assembling co-cultures (SACC) of PHHs with non-parenchymal liver cells, as well as a 3D microfluidic PHH system, can support persistent infection with cell culture and purified patient-derived HBV strains for 5-6 weeks, but no amplification and spread of HBV has been observed to date, and both models depend on human liver tissue.6,7Other promising systems are human hepatocyte-like cells (HLC) generated from induced pluripotent stem cells or embryonic stem cells which are susceptible to HBV and support limited viral production and spread.8,9
As hepatitis B virus (HBV) spreads through the infected liver it is simultaneously secreted into the blood. HBV-susceptible in vitro infection models do not efficiently amplify viral progeny or support cell-to-cell spread. We sought to establish a cell culture system for the amplification of infectious HBV from clinical specimens. An HBV-susceptible sodium-taurocholate cotransporting polypeptide-overexpressing HepG2 cell clone (HepG2-NTCPsec+) producing high titers of infectious progeny was selected. Secreted HBV progeny were characterized by native gel electrophoresis and electron microscopy. Comparative RNA-seq transcriptomics was performed to quantify the expression of host proviral and restriction factors. Viral spread routes were evaluated using HBV entry- or replication inhibitors, visualization of viral cell-to-cell spread in reporter cells, and nearest neighbor infection determination. Amplification kinetics of HBV genotypes B-D were analyzed. Infected HepG2-NTCPsec+ secreted high levels of large HBV surface protein-enveloped infectious HBV progeny with typical appearance under electron microscopy. RNA-seq transcriptomics revealed that HBV does not induce significant gene expression changes in HepG2-NTCPsec+, however, transcription factors favoring HBV amplification were more strongly expressed than in less permissive HepG2-NTCPsec−. Upon inoculation with HBV-containing patient sera, rates of infected cells increased from 10% initially to 70% by viral spread to adjacent cells, and viral progeny and antigens were efficiently secreted. HepG2-NTCPsec+ supported up to 1,300-fold net amplification of HBV genomes depending on the source of virus. Viral spread and amplification were abolished by entry and replication inhibitors; viral rebound was observed after inhibitor discontinuation. The novel HepG2-NTCPsec+ cells efficiently support the complete HBV life cycle, long-term viral spread and amplification of HBV derived from patients or cell culture, resembling relevant features of HBV-infected patients. A major breakthrough was the discovery of the sodium-taurocholate cotransporting polypeptide (NTCP) as the differentiation-dependent, hepatocyte- and species-specific cell membrane HBV receptor.10Although hepatoma cell lines, such as HepG2 and Huh-7, replicate HBV after transfection with HBV DNA, they are not susceptible to HBV infection.However, they can be rendered susceptible by NTCP overexpression.Efficient infection of these and other cells requires treatment with polyethylene glycol (PEG) during HBV attachment and/or entry to enhance glycosaminoglycan-dependent binding.11Dimethylsulfoxide (DMSO) is used to slow down cell proliferation, extend lifespan, increase differentiation status, and enhance susceptibility and HBV replication.However, following HBV infection and episomal cccDNA formation, these cells fail to amplify the viral genomes, hampering viral progeny production, and spread in cell culture.
Clinical evidence has indicated a close link between non-alcoholic fatty liver disease (NAFLD) and cardiovascular disease (CVD). However, the underlying mechanism remains to be elucidated. This study aimed to explore a potential role of hepatocyte-derived extracellular vesicles (EVs) in endothelial inflammation and atherogenesis in the context of NAFLD. EVs were isolated, quantified and characterized from steatotic hepatocytes. An endothelial cell-specific PCR array was used to screen the functional properties of EVs. Profiling of global microRNA expression was conducted in EVs. The expression level and biological function of microRNA-1 (miR-1) was determined by quantitative PCR, immunoblot and reporter gene assays, respectively. The in vivo effect of miR-1 on atherogenesis was investigated in apolipoprotein E (ApoE)-deficient mice administered with a miR-1-specific inhibitor, antagomiR-1. Steatotic hepatocytes released more EVs, which had significantly altered miRNA expression profiles compared to the EVs released by control hepatocytes. Endothelial cells co-cultured with steatotic hepatocytes, or treated with their EVs or miR-1, expressed significantly more proinflammatory molecules, as well as exhibiting increased NF-κB activity and reduced Kruppel-like factor 4 (KLF4) expression. EV-induced endothelial inflammation was prevented by either downregulation or inhibition of miR-1. While miR-1 treatment suppressed KLF4 expression and reporter gene activity, overexpression of KLF4 dramatically abolished the miR-1-induced endothelial inflammation. Moreover, not only did the miR-1 inhibitor reduce endothelial inflammation in vitro, but it also attenuated atherogenesis in ApoE-deficient mice. Steatotic hepatocyte-derived EVs promote endothelial inflammation and facilitate atherogenesis by miR-1 delivery, KLF4 suppression and NF-κB activation. The findings illustrate an important role of hepatocyte-derived EVs in distant communications between the liver and vasculature, suggesting a new mechanism underlying the link between NAFLD and CVD. Non-alcoholic fatty liver disease (NAFLD) has become a major public health concern worldwide.The disease currently affects 20–35% of the general population in Western countries, and 10% of patients can progress from benign steatosis to more severe conditions, including steatohepatitis, cirrhosis, and liver failure.1,2Moreover, multiple lines of evidence have revealed a strong association between NAFLD and several markers of subclinical atherosclerosis independent of traditional risk factors.3–5Thus, not only is the adverse effect of NAFLD confined to the progression of deteriorating liver function, but it also confers an independent risk for the development of atherosclerosis and other related cardiovascular diseases (CVDs).
Clinical evidence has indicated a close link between non-alcoholic fatty liver disease (NAFLD) and cardiovascular disease (CVD). However, the underlying mechanism remains to be elucidated. This study aimed to explore a potential role of hepatocyte-derived extracellular vesicles (EVs) in endothelial inflammation and atherogenesis in the context of NAFLD. EVs were isolated, quantified and characterized from steatotic hepatocytes. An endothelial cell-specific PCR array was used to screen the functional properties of EVs. Profiling of global microRNA expression was conducted in EVs. The expression level and biological function of microRNA-1 (miR-1) was determined by quantitative PCR, immunoblot and reporter gene assays, respectively. The in vivo effect of miR-1 on atherogenesis was investigated in apolipoprotein E (ApoE)-deficient mice administered with a miR-1-specific inhibitor, antagomiR-1. Steatotic hepatocytes released more EVs, which had significantly altered miRNA expression profiles compared to the EVs released by control hepatocytes. Endothelial cells co-cultured with steatotic hepatocytes, or treated with their EVs or miR-1, expressed significantly more proinflammatory molecules, as well as exhibiting increased NF-κB activity and reduced Kruppel-like factor 4 (KLF4) expression. EV-induced endothelial inflammation was prevented by either downregulation or inhibition of miR-1. While miR-1 treatment suppressed KLF4 expression and reporter gene activity, overexpression of KLF4 dramatically abolished the miR-1-induced endothelial inflammation. Moreover, not only did the miR-1 inhibitor reduce endothelial inflammation in vitro, but it also attenuated atherogenesis in ApoE-deficient mice. Steatotic hepatocyte-derived EVs promote endothelial inflammation and facilitate atherogenesis by miR-1 delivery, KLF4 suppression and NF-κB activation. The findings illustrate an important role of hepatocyte-derived EVs in distant communications between the liver and vasculature, suggesting a new mechanism underlying the link between NAFLD and CVD. Atherosclerosis is regarded as a chronic inflammatory metabolic disease caused by aberrant accumulation of lipids and infiltration of inflammatory cells at the vascular endothelium.6It is believed that the first step in the initiation of atherosclerosis is endothelial injury and dysfunction.6,7Both clinical evidence and experimental data have demonstrated a close link between NAFLD, endothelium dysfunction and CVD,4,5,8 whereas the underlying mechanism remains to be elucidated.
Clinical evidence has indicated a close link between non-alcoholic fatty liver disease (NAFLD) and cardiovascular disease (CVD). However, the underlying mechanism remains to be elucidated. This study aimed to explore a potential role of hepatocyte-derived extracellular vesicles (EVs) in endothelial inflammation and atherogenesis in the context of NAFLD. EVs were isolated, quantified and characterized from steatotic hepatocytes. An endothelial cell-specific PCR array was used to screen the functional properties of EVs. Profiling of global microRNA expression was conducted in EVs. The expression level and biological function of microRNA-1 (miR-1) was determined by quantitative PCR, immunoblot and reporter gene assays, respectively. The in vivo effect of miR-1 on atherogenesis was investigated in apolipoprotein E (ApoE)-deficient mice administered with a miR-1-specific inhibitor, antagomiR-1. Steatotic hepatocytes released more EVs, which had significantly altered miRNA expression profiles compared to the EVs released by control hepatocytes. Endothelial cells co-cultured with steatotic hepatocytes, or treated with their EVs or miR-1, expressed significantly more proinflammatory molecules, as well as exhibiting increased NF-κB activity and reduced Kruppel-like factor 4 (KLF4) expression. EV-induced endothelial inflammation was prevented by either downregulation or inhibition of miR-1. While miR-1 treatment suppressed KLF4 expression and reporter gene activity, overexpression of KLF4 dramatically abolished the miR-1-induced endothelial inflammation. Moreover, not only did the miR-1 inhibitor reduce endothelial inflammation in vitro, but it also attenuated atherogenesis in ApoE-deficient mice. Steatotic hepatocyte-derived EVs promote endothelial inflammation and facilitate atherogenesis by miR-1 delivery, KLF4 suppression and NF-κB activation. The findings illustrate an important role of hepatocyte-derived EVs in distant communications between the liver and vasculature, suggesting a new mechanism underlying the link between NAFLD and CVD. The link between NAFLD and CVD indicates the importance of intercellular communication in the pathogenic process of the diseases.It is now well recognized that cells communicate not only via direct contact and soluble factors, but also by membrane-derived nanometer-sized vesicles, namely, extracellular vesicles (EVs).9,10EVs carry numerous donor cell-derived molecules, including proteins, lipids and nucleic acids.By transferring these bioactive contents into target cells, EVs play essential roles in intercellular communications.9,11The EVs are released under either physiologic or pathologic conditions, including liver diseases, and exert a wide range of effects on target cells.9,12Among the various molecular cargos, miRNAs are regarded as essential to the function of EVs.9,13
HCV infection is associated with several extrahepatic manifestations (EHMs). We evaluated the impact of sustained virological response (SVR) on the risk of 7 EHMs that contribute to the burden of extrahepatic disease: type 2 diabetes mellitus, chronic kidney disease or end-stage renal disease, stroke, ischemic heart disease, major adverse cardiac events, mood and anxiety disorders, and rheumatoid arthritis. A longitudinal cohort study was conducted using data from the British Columbia Hepatitis Testers Cohort, which included ~1.3 million individuals screened for HCV. We identified all HCV-infected individuals who were treated with interferon-based therapies between 1999 and 2014. SVR was defined as a negative HCV RNA test ≥24 weeks post-treatment or after end-of-treatment, if unavailable. We computed adjusted subdistribution hazard ratios (asHR) for the effect of SVR on each EHM using competing risk proportional hazard models. Subgroup analyses by birth cohort, sex, injection drug exposure and genotype were also performed. Overall, 10,264 HCV-infected individuals were treated with interferon, of whom 6,023 (59%) achieved SVR. Compared to those that failed treatment, EHM risk was significantly reduced among patients with SVR for type 2 diabetes mellitus (asHR 0.65; 95% CI 0.55–0.77), chronic kidney disease or end-stage renal disease (asHR 0.53; 95% CI 0.43–0.65), ischemic or hemorrhagic stroke (asHR 0.73; 95% CI 0.49–1.09), and mood and anxiety disorders (asHR 0.82; 95% CI 0.71–0.95), but not for ischemic heart disease (asHR 1.23; 95% CI 1.03–1.47), major adverse cardiac events (asHR 0.93; 95% CI 0.79–1.11) or rheumatoid arthritis (asHR 1.09; 95% CI 0.73–1.64). SVR was associated with a reduction in the risk of several EHMs. Increased uptake of antiviral therapy may reduce the growing burden of EHMs in this population. Chronic HCV infection, which affects more than 71 million people worldwide, is an important source of morbidity and mortality.1Although liver-related sequelae have been well characterized, extrahepatic manifestations (EHM) associated with HCV have received considerably less attention.2–4These EHMs include metabolic,5 cardiovascular,6 renal,7 autoimmune,8 lymphoproliferative,9 and neurologic conditions which are estimated to be present in as many as 31% of HCV-infected individuals.3,10,11Direct medical costs associated with EHMs are substantial and range between $72 million and $443 million US dollars per year.3Increasing rates of healthcare resource utilization associated with EHMs have also been reported12,13 and, with a largely aging HCV-infected population, there is a greater need to assess the broader HCV-related disease burden and interventions to reduce future morbidity and mortality.
HCV infection is associated with several extrahepatic manifestations (EHMs). We evaluated the impact of sustained virological response (SVR) on the risk of 7 EHMs that contribute to the burden of extrahepatic disease: type 2 diabetes mellitus, chronic kidney disease or end-stage renal disease, stroke, ischemic heart disease, major adverse cardiac events, mood and anxiety disorders, and rheumatoid arthritis. A longitudinal cohort study was conducted using data from the British Columbia Hepatitis Testers Cohort, which included ~1.3 million individuals screened for HCV. We identified all HCV-infected individuals who were treated with interferon-based therapies between 1999 and 2014. SVR was defined as a negative HCV RNA test ≥24 weeks post-treatment or after end-of-treatment, if unavailable. We computed adjusted subdistribution hazard ratios (asHR) for the effect of SVR on each EHM using competing risk proportional hazard models. Subgroup analyses by birth cohort, sex, injection drug exposure and genotype were also performed. Overall, 10,264 HCV-infected individuals were treated with interferon, of whom 6,023 (59%) achieved SVR. Compared to those that failed treatment, EHM risk was significantly reduced among patients with SVR for type 2 diabetes mellitus (asHR 0.65; 95% CI 0.55–0.77), chronic kidney disease or end-stage renal disease (asHR 0.53; 95% CI 0.43–0.65), ischemic or hemorrhagic stroke (asHR 0.73; 95% CI 0.49–1.09), and mood and anxiety disorders (asHR 0.82; 95% CI 0.71–0.95), but not for ischemic heart disease (asHR 1.23; 95% CI 1.03–1.47), major adverse cardiac events (asHR 0.93; 95% CI 0.79–1.11) or rheumatoid arthritis (asHR 1.09; 95% CI 0.73–1.64). SVR was associated with a reduction in the risk of several EHMs. Increased uptake of antiviral therapy may reduce the growing burden of EHMs in this population. A sustained virological response (SVR) to HCV treatment has been associated with marked reductions in all-cause and liver-related mortality;14 however, there are limited data regarding the effect of SVR on the risk of developing EHMs.15Initial studies from Asia during the interferon era reported reduced rates of type 2 diabetes mellitus (T2DM),16 hemorrhagic and ischemic stroke,17,18 chronic kidney disease (CKD) or end-stage renal disease (ESRD),18,19 and acute coronary syndrome18 associated with SVR; however, these studies were limited by the small number of clinical events and incomplete linkages to assess HCV cure.Furthermore, this evidence may not be generalizable to HCV-infected populations in North America, who are both younger and more likely to have pre-existing chronic conditions.20More recently, a large study from the United States Veterans Affairs clinical cohort has reported a protective association between SVR and subclinical disease, but not for chronic conditions, such as coronary heart disease or stroke, though another study on the same population recently reported a protective effect for cardiovascular diseases compared to untreated individuals.21–25As >95% of veterans are male and may experience different risk profiles, these findings may not be generalizable.Additional studies in European patients with specific comorbid conditions, such as HIV coinfection26 and advanced liver disease,8,27 have reported reduced rates of certain EHMs among those who achieved SVR.Given the limitations of the existing literature, there is a need for comprehensive assessment of the effect of SVR on various EHMs among the general population.
Cholangiocarcinoma (CCA) carries a poor prognosis, is increasing in incidence and its causes are poorly understood. Although some risk factors are known, they vary globally and collectively account for a minority of cases. The aim of this study was to perform a comprehensive meta-analysis of risk factors for intrahepatic (iCCA) and extrahepatic cholangiocarcinoma (eCCA), from Eastern and Western world studies. A literature search of case-control studies was performed to identify potential risk factors for iCCA and eCCA. Pooled odds ratios (ORs) with 95% CIs and heterogeneity were calculated. Funnel plots were used to assess publication bias, and meta-regression was used to select risk factors for comparison between Eastern and Western studies. A total of 13 risk factors were selected from 25 case-control studies in 7 geographically diverse countries. The strongest risk factors for both iCCA and eCCA were biliary cysts and stones, cirrhosis, hepatitis B and hepatitis C. Choledochal cysts conferred the greatest risk of both iCCA and eCCA with pooled ORs of 26.71 (95% CI 15.80–45.16) and 34.94 (24.36–50.12), respectively. No significant associations were found between hypertension and obesity for either iCCA or eCCA. Comparing Eastern and Western populations, there was a difference for the association of hepatitis B with iCCA (coefficient = −0.15195; 95% CI −0.278 to −0.025; p = 0.022). This is the most comprehensive meta-analysis of CCA risk factors to date. Some risk factors, such as diabetes, although less strong, are increasing globally and may be contributing to rising rates of this cancer. Cholangiocarcinoma (CCA) is an exceptionally aggressive cancer arising from the biliary duct epithelium.CCAs represent approximately 3 to 5% of all malignancies of the gastrointestinal system.CCAs are classically sub-divided into 3 groups depending on the anatomical site of origin: intrahepatic CCA (iCCA), perihilar CCA (pCCA) and distal CCA (dCCA).iCCAs classically arise above the second-order bile ducts, whereas the anatomical point of distinction between perihilar cholangiocarcinomas (pCCAs) and distal cholangiocarcinomas (dCCAs) is the cystic duct.1–5pCCA account for approximately 50–60% of all CCAs, dCCA 20–30%; and iCCA approximately 10–20%.iCCAs comprise about 10% of all primary liver cancers, making them the second most common primary hepatic malignancy after hepatocellular carcinoma.4,6CCA typically presents late with non-specific symptoms.This is compounded by the lack of knowledge of risk factors in most cases and inaccurate screening tools, making the diagnosis of early, resectable disease uncommon.Beyond this stage, CCA is one of the most fatal cancers with a 5-year survival of approximately 5%.6
Cholangiocarcinoma (CCA) carries a poor prognosis, is increasing in incidence and its causes are poorly understood. Although some risk factors are known, they vary globally and collectively account for a minority of cases. The aim of this study was to perform a comprehensive meta-analysis of risk factors for intrahepatic (iCCA) and extrahepatic cholangiocarcinoma (eCCA), from Eastern and Western world studies. A literature search of case-control studies was performed to identify potential risk factors for iCCA and eCCA. Pooled odds ratios (ORs) with 95% CIs and heterogeneity were calculated. Funnel plots were used to assess publication bias, and meta-regression was used to select risk factors for comparison between Eastern and Western studies. A total of 13 risk factors were selected from 25 case-control studies in 7 geographically diverse countries. The strongest risk factors for both iCCA and eCCA were biliary cysts and stones, cirrhosis, hepatitis B and hepatitis C. Choledochal cysts conferred the greatest risk of both iCCA and eCCA with pooled ORs of 26.71 (95% CI 15.80–45.16) and 34.94 (24.36–50.12), respectively. No significant associations were found between hypertension and obesity for either iCCA or eCCA. Comparing Eastern and Western populations, there was a difference for the association of hepatitis B with iCCA (coefficient = −0.15195; 95% CI −0.278 to −0.025; p = 0.022). This is the most comprehensive meta-analysis of CCA risk factors to date. Some risk factors, such as diabetes, although less strong, are increasing globally and may be contributing to rising rates of this cancer. The incidence of CCA varies globally and particularly so between East and West.Northeast Thailand reports the highest age standardised incidence rates of CCA of 113/100,000 in men and 50/100,000 in women, which are approximately 100-fold greater than European and North American rates of around 1–2/100,000.6However, several studies report a rising incidence of iCCA in recent decades across diverse geographical regions, in contrast to a stable or declining incidence of eCCA.5,7–12The reasons for these trends are unclear, but the ongoing overall increase in the incidence of CCA makes it imperative to understand the disease’s aetiology and risk factors.
Before antiviral therapy, kidney transplant recipients infected with hepatitis B virus (HBV) or hepatitis C virus (HCV) had poor outcomes. Since the 90s, nucleos(t)ide analogues have been widely used in HBV-infected patients, while interferon-based therapy was rarely used in HCV-infected patients. The aim of this study was to assess the impact of HBV and HCV on patient and graft survival, according to viral replication status. Data from January 1993 to December 2010 were extracted from the French national database CRISTAL. A total of 31,433 kidney transplant recipients were included, of whom 575, 1,060 and 29,798 had chronic hepatitis B, C, or were not infected, respectively. Ten-year survival was lower in HCV-infected (71.3%) than in HBV-infected (81.2%, p = 0.0004) or non-infected kidney transplant recipients (82.7%, p <0.0001). Ten-year kidney graft survival was lower in HCV-infected (50.6%) than in HBV-infected (62.3%, p <0.0001) or non-infected kidney transplant recipients (64.7%, p <0.0001). A random analysis of the medical records of 184 patients with HBV and 504 patients with HCV showed a control of viral replication in 94% and 35% of cases, respectively. Ten-year patient and graft survival in patients with detectable HCV RNA was lower than in their matching controls. Conversely, patients with HCV and undetectable HCV RNA had higher 10-year survival than their matched controls without significant differences in graft survival. Chronic HBV infection does not impact 10-year patient and kidney graft survival thanks to control of viral replication with nucleos(t)ide analogues. In kidney transplant recipients infected with HCV, patients with detectable RNA had worse outcomes, whereas the outcomes of those with undetectable RNA were at least as good as non-infected patients. Thus, direct-acting antivirals should be systematically offered to HCV-infected patients. Kidney transplantation is the best treatment for patients with end-stage renal disease (ESRD) because of a significant survival benefit conferred compared to patients who remain on haemodialysis.1Although the prevalence of hepatitis B virus (HBV) and hepatitis C virus (HCV) infection in patients with ESRD has significantly declined over time, it remains at least 4-times higher than in the general population.2,3Chronic HCV or HBV infection can result in chronic liver disease, cirrhosis, and hepatocellular carcinoma4–6 and increase the risk of chronic kidney disease (CKD).7–10Before the use of antiviral therapy, HBV and HCV infection were associated with a poor outcome in kidney transplant recipients (KTRs).11–14This poorer outcome has been reported in untreated patients.11–14With the development of new treatments against viral hepatitis, an update of data in large cohorts of KTRs with long-term follow-up is warranted.
Before antiviral therapy, kidney transplant recipients infected with hepatitis B virus (HBV) or hepatitis C virus (HCV) had poor outcomes. Since the 90s, nucleos(t)ide analogues have been widely used in HBV-infected patients, while interferon-based therapy was rarely used in HCV-infected patients. The aim of this study was to assess the impact of HBV and HCV on patient and graft survival, according to viral replication status. Data from January 1993 to December 2010 were extracted from the French national database CRISTAL. A total of 31,433 kidney transplant recipients were included, of whom 575, 1,060 and 29,798 had chronic hepatitis B, C, or were not infected, respectively. Ten-year survival was lower in HCV-infected (71.3%) than in HBV-infected (81.2%, p = 0.0004) or non-infected kidney transplant recipients (82.7%, p <0.0001). Ten-year kidney graft survival was lower in HCV-infected (50.6%) than in HBV-infected (62.3%, p <0.0001) or non-infected kidney transplant recipients (64.7%, p <0.0001). A random analysis of the medical records of 184 patients with HBV and 504 patients with HCV showed a control of viral replication in 94% and 35% of cases, respectively. Ten-year patient and graft survival in patients with detectable HCV RNA was lower than in their matching controls. Conversely, patients with HCV and undetectable HCV RNA had higher 10-year survival than their matched controls without significant differences in graft survival. Chronic HBV infection does not impact 10-year patient and kidney graft survival thanks to control of viral replication with nucleos(t)ide analogues. In kidney transplant recipients infected with HCV, patients with detectable RNA had worse outcomes, whereas the outcomes of those with undetectable RNA were at least as good as non-infected patients. Thus, direct-acting antivirals should be systematically offered to HCV-infected patients. HBV treatment has improved in the last 2 decades.15–17The probability of undetectability HBV DNA upon therapy has increased from the use of first generation (lamivudine or adefovir)18–20 treatment to the second generation analogues (tenofovir or entecavir).21,22Most KTRs infected with HBV have received this safe long-term treatment with analogues before and continue long after kidney transplant.However, the impact of viral replication on kidney transplant outcomes remains somewhat controversial.23Additional studies in HBV-infected KTRs are needed with longer follow-up, larger sample populations, and prospective designs to provide accurate evaluation.
Before antiviral therapy, kidney transplant recipients infected with hepatitis B virus (HBV) or hepatitis C virus (HCV) had poor outcomes. Since the 90s, nucleos(t)ide analogues have been widely used in HBV-infected patients, while interferon-based therapy was rarely used in HCV-infected patients. The aim of this study was to assess the impact of HBV and HCV on patient and graft survival, according to viral replication status. Data from January 1993 to December 2010 were extracted from the French national database CRISTAL. A total of 31,433 kidney transplant recipients were included, of whom 575, 1,060 and 29,798 had chronic hepatitis B, C, or were not infected, respectively. Ten-year survival was lower in HCV-infected (71.3%) than in HBV-infected (81.2%, p = 0.0004) or non-infected kidney transplant recipients (82.7%, p <0.0001). Ten-year kidney graft survival was lower in HCV-infected (50.6%) than in HBV-infected (62.3%, p <0.0001) or non-infected kidney transplant recipients (64.7%, p <0.0001). A random analysis of the medical records of 184 patients with HBV and 504 patients with HCV showed a control of viral replication in 94% and 35% of cases, respectively. Ten-year patient and graft survival in patients with detectable HCV RNA was lower than in their matching controls. Conversely, patients with HCV and undetectable HCV RNA had higher 10-year survival than their matched controls without significant differences in graft survival. Chronic HBV infection does not impact 10-year patient and kidney graft survival thanks to control of viral replication with nucleos(t)ide analogues. In kidney transplant recipients infected with HCV, patients with detectable RNA had worse outcomes, whereas the outcomes of those with undetectable RNA were at least as good as non-infected patients. Thus, direct-acting antivirals should be systematically offered to HCV-infected patients. During the same period, interferon-based regimens were contraindicated in KTRs with HCV due to the high risk of transplant rejection.KDIGO recommendations were to treat patients on haemodialysis,24 but interferon-based regimens were associated with a low efficacy25 and poor tolerance in patients with ESRD.25–28Since 2017, the updated KDIGO and international guidelines recommend extensive use of the new direct-acting antivirals (DAAs) because of their good efficacy and safety.29,30
T cells are central mediators of liver inflammation and represent potential treatment targets in cholestatic liver disease. Whereas emerging evidence shows that bile acids (BAs) affect T cell function, the role of T cells for the regulation of BA metabolism is unknown. In order to understand this interplay, we investigated the influence of T cells on BA metabolism in a novel mouse model of cholangitis. Mdr2−/− mice were crossed with transgenic K14-OVAp mice, which express an MHC class I restricted ovalbumin peptide on biliary epithelial cells (Mdr2−/−xK14-OVAp). T cell-mediated cholangitis was induced by the adoptive transfer of antigen-specific CD8+ T cells. BA levels were quantified using a targeted liquid chromatography-mass spectrometry-based approach. T cell-induced cholangitis resulted in reduced levels of unconjugated BAs in the liver and significantly increased serum and hepatic levels of conjugated BAs. Genes responsible for BA synthesis and uptake were downregulated and expression of the bile salt export pump was increased. The transferred antigen-specific CD8+ T cells alone were able to induce these changes, as demonstrated using Mdr2−/−xK14-OVAp recipient mice on the Rag1−/− background. Mechanistically, we showed by depletion experiments that alterations in BA metabolism were partly mediated by the proinflammatory cytokines TNF and IFN-γ in an FXR-dependent manner, a process that in vitro required cell contact between T cells and hepatocytes. Whereas it is known that BA metabolism is dysregulated in sepsis and related conditions, we have shown that T cells are able to control the synthesis and metabolism of BAs, a process which depends on TNF and IFN-γ. Understanding the effect of lymphocytes on BA metabolism will help in the design of combined treatment strategies for cholestatic liver diseases. In primary biliary cholangitis (PBC) and primary sclerosing cholangitis (PSC), T cells are likely involved in targeting biliary epithelial cells.1Chronic bile duct inflammation leads to cholestasis, biliary fibrosis and potential malignant transformation in the case of PSC.2Adaptive immune responses are considered to contribute to early as well as late stages of disease.3Numerous genetic risk loci for PBC and PSC were identified that encode for genes involved in adaptive immune responses4–6 and portal infiltrates that were predominantly composed of CD4+ and CD8+ T cells have been described in the livers of patients with PBC and PSC.7–9Moreover, autoantigens within the pyruvate dehydrogenase complex (PDC-E2) are targeted by lymphocytes in PBC.10In PSC, impaired T cell homeostasis including an imbalance of IL-17 producing T cells and regulatory T cells has been described.11–16Since current treatment options for these diseases are limited, T cells are an interesting target for novel combination therapies.
T cells are central mediators of liver inflammation and represent potential treatment targets in cholestatic liver disease. Whereas emerging evidence shows that bile acids (BAs) affect T cell function, the role of T cells for the regulation of BA metabolism is unknown. In order to understand this interplay, we investigated the influence of T cells on BA metabolism in a novel mouse model of cholangitis. Mdr2−/− mice were crossed with transgenic K14-OVAp mice, which express an MHC class I restricted ovalbumin peptide on biliary epithelial cells (Mdr2−/−xK14-OVAp). T cell-mediated cholangitis was induced by the adoptive transfer of antigen-specific CD8+ T cells. BA levels were quantified using a targeted liquid chromatography-mass spectrometry-based approach. T cell-induced cholangitis resulted in reduced levels of unconjugated BAs in the liver and significantly increased serum and hepatic levels of conjugated BAs. Genes responsible for BA synthesis and uptake were downregulated and expression of the bile salt export pump was increased. The transferred antigen-specific CD8+ T cells alone were able to induce these changes, as demonstrated using Mdr2−/−xK14-OVAp recipient mice on the Rag1−/− background. Mechanistically, we showed by depletion experiments that alterations in BA metabolism were partly mediated by the proinflammatory cytokines TNF and IFN-γ in an FXR-dependent manner, a process that in vitro required cell contact between T cells and hepatocytes. Whereas it is known that BA metabolism is dysregulated in sepsis and related conditions, we have shown that T cells are able to control the synthesis and metabolism of BAs, a process which depends on TNF and IFN-γ. Understanding the effect of lymphocytes on BA metabolism will help in the design of combined treatment strategies for cholestatic liver diseases. In cholestasis, bile acids (BAs) accumulate in serum and liver and may fuel disease progression via nuclear or membrane bound BA receptors or via direct cytotoxic effects such as the induction of apoptosis or oxidative stress.17–19The synthesis of BAs takes place in hepatocytes by several enzymatic steps using 2 different routes.The classical BA synthesis pathway is controlled by the enzyme cholesterol 7-alpha-hydroxylase (CYP7A1), whereas the alternative BA synthesis pathway is initiated by sterol 27-hydroxylase (CYP27A1) followed by the action of 25-hydroxycholesterol 7-alpha-hydroxylase (CYP7B1).In principle, both pathways produce the same BA species that are conjugated with glycine or taurine, and secreted via the bile.The transcription factors liver X receptor (LXR) and farnesoid X receptor (FXR) control BA synthesis and homeostasis.FXR is activated by excess intestinal or hepatic BAs, leading to downregulation of CYP7A1 and hepatic basolateral BA transporter expression, and induction of bile salt export pump (BSEP) expression.20,21
Since the first account of the myth of Prometheus, the amazing regenerative capacity of the liver has fascinated researchers because of its enormous medical potential. Liver regeneration is promoted by multiple types of liver cells, including hepatocytes and liver non-parenchymal cells (NPCs), through complex intercellular signaling. However, the mechanism of liver organogenesis, especially the role of adult hepatocytes at ectopic sites, remains unknown. In this study, we demonstrate that hepatocytes alone spurred liver organogenesis to form an organ-sized complex 3D liver that exhibited native liver architecture and functions in the kidneys of mice. Isolated hepatocytes were transplanted under the kidney capsule of monocrotaline (MCT) and partial hepatectomy (PHx)-treated mice. To determine the origin of NPCs in neo-livers, hepatocytes were transplanted into MCT/PHx-treated green fluorescent protein transgenic mice or wild-type mice transplanted with bone marrow cells isolated from green fluorescent protein-mice. Hepatocytes engrafted at the subrenal space of mice underwent continuous growth in response to a chronic hepatic injury in the native liver. More than 1.5 years later, whole organ-sized liver tissues with greater mass than those of the injured native liver had formed. Most remarkably, we revealed that at least three types of NPCs with similar phenotypic features to the liver NPCs were recruited from the host tissues including bone marrow. The neo-livers in the kidney exhibited liver-specific functions and architectures, including sinusoidal vascular systems, zonal heterogeneity, and emergence of bile duct cells. Furthermore, the neo-livers successfully rescued the mice with lethal liver injury. Our data clearly show that adult hepatocytes play a leading role as organizer cells in liver organogenesis at ectopic sites via NPC recruitment. The liver is a complex organ that plays central roles in metabolism, protein/enzyme synthesis, and blood detoxification, and is the sole solid organ in the body that possesses high regenerative capability.Adult hepatocytes have great growth potential in vivo, similar to that of adult stem cells.1Following the loss of liver mass, compensatory regenerative responses are accelerated until the original liver mass is restored, and this process is promoted by cytokines, growth factors, and hormones secreted from hepatocytes and liver non-parenchymal cells (NPCs), and other organs via complex interactions.2,3Transplantation of a small number of hepatocytes to the liver has been shown to lead to complete or near-complete repopulation of the diseased livers of animal models such as urokinase-type plasminogen activator (uPA)-transgenic (Tg) or fumarylacetoacetate hydrolase (Fah)-deficient mice,4–6 ultimately reversing the lethality of these mouse models.
Since the first account of the myth of Prometheus, the amazing regenerative capacity of the liver has fascinated researchers because of its enormous medical potential. Liver regeneration is promoted by multiple types of liver cells, including hepatocytes and liver non-parenchymal cells (NPCs), through complex intercellular signaling. However, the mechanism of liver organogenesis, especially the role of adult hepatocytes at ectopic sites, remains unknown. In this study, we demonstrate that hepatocytes alone spurred liver organogenesis to form an organ-sized complex 3D liver that exhibited native liver architecture and functions in the kidneys of mice. Isolated hepatocytes were transplanted under the kidney capsule of monocrotaline (MCT) and partial hepatectomy (PHx)-treated mice. To determine the origin of NPCs in neo-livers, hepatocytes were transplanted into MCT/PHx-treated green fluorescent protein transgenic mice or wild-type mice transplanted with bone marrow cells isolated from green fluorescent protein-mice. Hepatocytes engrafted at the subrenal space of mice underwent continuous growth in response to a chronic hepatic injury in the native liver. More than 1.5 years later, whole organ-sized liver tissues with greater mass than those of the injured native liver had formed. Most remarkably, we revealed that at least three types of NPCs with similar phenotypic features to the liver NPCs were recruited from the host tissues including bone marrow. The neo-livers in the kidney exhibited liver-specific functions and architectures, including sinusoidal vascular systems, zonal heterogeneity, and emergence of bile duct cells. Furthermore, the neo-livers successfully rescued the mice with lethal liver injury. Our data clearly show that adult hepatocytes play a leading role as organizer cells in liver organogenesis at ectopic sites via NPC recruitment. Previous studies have reported that hepatocytes transplanted at ectopic sites such as the spleen, subrenal capsule, and lymph nodes proliferated and formed liver-like tissues in animal models with liver injury.7–9The propagated hepatocytes at these sites were functional and had therapeutic potency for liver diseases.Approaches using engineered tissues from isolated hepatocytes have also been empirically investigated to create de novo functional liver systems.10For example, hepatocyte sheets prepared using temperature-responsive dishes, have been successfully transplanted into subcutaneous sites, following prevascularization by basic fibroblast growth factor (bFGF)-releasing devices to promote the sheet engraftment and the formation of liver-like tissues.11These studies clearly verified the usefulness and potential of the de novo creation of ectopic liver systems.However, the mechanism of liver organogenesis from adult hepatocytes at ectopic sites remains largely unknown.Until now, morphology and function of the generated liver-like tissues at ectopic sites, including the emergence of NPCs, have not been investigated in detail.
Cardiovascular disease is the principle cause of death in patients with elevated liver fat unrelated to alcohol consumption, more so than liver-related morbidity and mortality. The aim of this study was to evaluate the relationship between liver fat and cardiac and autonomic function, as well as to assess how impairment in cardiac and autonomic function is influenced by metabolic risk factors. Cardiovascular and autonomic function were assessed in 96 sedentary individuals: i) non-alcoholic fatty liver disease (NAFLD) (n = 46, hepatic steatosis >5% by magnetic resonance spectroscopy), ii) Hepatic steatosis and alcohol (dual aetiology fatty liver disease [DAFLD]) (n = 16, hepatic steatosis >5%, consuming >20 g/day of alcohol) and iii) CONTROL (n = 34, no cardiac, liver or metabolic disorders, <20 g/day of alcohol). Patients with NAFLD and DAFLD had significantly impaired cardiac and autonomic function when compared with controls. Diastolic variability and systolic variability (LF/HF-sBP [n/1]; 2.3 (1.7) and 2.3 (1.5) vs. 3.4 (1.5), p <0.01) were impaired in patients with NAFLD and DAFLD when compared to controls, with DAFLD individuals showing a decrease in diastolic variability relative to NAFLD patients. Hepatic steatosis and fasting glucose were negatively correlated with stroke volume index. Fibrosis stage was significantly negatively associated with mean blood pressure (r = −0.47, p = 0.02), diastolic variability (r = −0.58, p ≤0.01) and systolic variability (r = −0.42, p = 0.04). Hepatic steatosis was independently associated with cardiac function (p ≤0.01); TNF-α (p ≤0.05) and CK-18 (p ≤0.05) were independently associated with autonomic function. Cardiac and autonomic impairments appear to be dependent on level of liver fat, metabolic dysfunction, inflammation and fibrosis staging, and to a lesser extent alcohol intake. Interventions should be sought to moderate the excess cardiovascular risk in patients with NAFLD or DAFLD. Current clinical care in chronic liver disease divides fatty liver disease into non-alcoholic fatty liver disease (NAFLD) or alcoholic fatty liver disease (ALD), predominantly based on alcohol intake.1In line with the increase in obesity NAFLD has become the leading cause of liver disease in developed countries,2,3 closely followed by ALD, which together account for the 2 most common liver diseases worldwide.4
Cardiovascular disease is the principle cause of death in patients with elevated liver fat unrelated to alcohol consumption, more so than liver-related morbidity and mortality. The aim of this study was to evaluate the relationship between liver fat and cardiac and autonomic function, as well as to assess how impairment in cardiac and autonomic function is influenced by metabolic risk factors. Cardiovascular and autonomic function were assessed in 96 sedentary individuals: i) non-alcoholic fatty liver disease (NAFLD) (n = 46, hepatic steatosis >5% by magnetic resonance spectroscopy), ii) Hepatic steatosis and alcohol (dual aetiology fatty liver disease [DAFLD]) (n = 16, hepatic steatosis >5%, consuming >20 g/day of alcohol) and iii) CONTROL (n = 34, no cardiac, liver or metabolic disorders, <20 g/day of alcohol). Patients with NAFLD and DAFLD had significantly impaired cardiac and autonomic function when compared with controls. Diastolic variability and systolic variability (LF/HF-sBP [n/1]; 2.3 (1.7) and 2.3 (1.5) vs. 3.4 (1.5), p <0.01) were impaired in patients with NAFLD and DAFLD when compared to controls, with DAFLD individuals showing a decrease in diastolic variability relative to NAFLD patients. Hepatic steatosis and fasting glucose were negatively correlated with stroke volume index. Fibrosis stage was significantly negatively associated with mean blood pressure (r = −0.47, p = 0.02), diastolic variability (r = −0.58, p ≤0.01) and systolic variability (r = −0.42, p = 0.04). Hepatic steatosis was independently associated with cardiac function (p ≤0.01); TNF-α (p ≤0.05) and CK-18 (p ≤0.05) were independently associated with autonomic function. Cardiac and autonomic impairments appear to be dependent on level of liver fat, metabolic dysfunction, inflammation and fibrosis staging, and to a lesser extent alcohol intake. Interventions should be sought to moderate the excess cardiovascular risk in patients with NAFLD or DAFLD. NAFLD and ALD encompass a spectrum of clinical features, with similar pathophysiology and histological features ranging from steatosis, steatohepatitis, fibrosis and cirrhosis, with more advanced forms of the disease causing considerable liver mortality and morbidity.5The clinical and economic burden of NAFLD and ALD is not only dependent on liver-related mortality, but is also due to extrahepatic diseases (type 2 diabetes, kidney disease) and increased risk of cardiovascular disease (CVD).6,7
Cardiovascular disease is the principle cause of death in patients with elevated liver fat unrelated to alcohol consumption, more so than liver-related morbidity and mortality. The aim of this study was to evaluate the relationship between liver fat and cardiac and autonomic function, as well as to assess how impairment in cardiac and autonomic function is influenced by metabolic risk factors. Cardiovascular and autonomic function were assessed in 96 sedentary individuals: i) non-alcoholic fatty liver disease (NAFLD) (n = 46, hepatic steatosis >5% by magnetic resonance spectroscopy), ii) Hepatic steatosis and alcohol (dual aetiology fatty liver disease [DAFLD]) (n = 16, hepatic steatosis >5%, consuming >20 g/day of alcohol) and iii) CONTROL (n = 34, no cardiac, liver or metabolic disorders, <20 g/day of alcohol). Patients with NAFLD and DAFLD had significantly impaired cardiac and autonomic function when compared with controls. Diastolic variability and systolic variability (LF/HF-sBP [n/1]; 2.3 (1.7) and 2.3 (1.5) vs. 3.4 (1.5), p <0.01) were impaired in patients with NAFLD and DAFLD when compared to controls, with DAFLD individuals showing a decrease in diastolic variability relative to NAFLD patients. Hepatic steatosis and fasting glucose were negatively correlated with stroke volume index. Fibrosis stage was significantly negatively associated with mean blood pressure (r = −0.47, p = 0.02), diastolic variability (r = −0.58, p ≤0.01) and systolic variability (r = −0.42, p = 0.04). Hepatic steatosis was independently associated with cardiac function (p ≤0.01); TNF-α (p ≤0.05) and CK-18 (p ≤0.05) were independently associated with autonomic function. Cardiac and autonomic impairments appear to be dependent on level of liver fat, metabolic dysfunction, inflammation and fibrosis staging, and to a lesser extent alcohol intake. Interventions should be sought to moderate the excess cardiovascular risk in patients with NAFLD or DAFLD. CVD rather than liver-related death, is a more common outcome in patients with NAFLD,8 however, liver-related deaths are more strongly associated with ALD.The risk of CVD in NAFLD increases in a dose dependent manner in line with the severity of NAFLD (especially, fibrosis stage)9,10 and alcohol consumption in ALD.11Over 20 retrospective and prospective studies have investigated the relationship between NAFLD and CVD, with the majority of NAFLD cases showing increased CVD morbidity and mortality, independent of metabolic phenotypes.12Furthermore, the relationship between liver disease and CVD, extends to fibrosis stage.In comparison with a matched reference population, patients with NAFLD and fibrosis were more likely to die from CVD and liver-related disease, specifically those with stage 3 or 4 fibrosis.13
Cardiovascular disease is the principle cause of death in patients with elevated liver fat unrelated to alcohol consumption, more so than liver-related morbidity and mortality. The aim of this study was to evaluate the relationship between liver fat and cardiac and autonomic function, as well as to assess how impairment in cardiac and autonomic function is influenced by metabolic risk factors. Cardiovascular and autonomic function were assessed in 96 sedentary individuals: i) non-alcoholic fatty liver disease (NAFLD) (n = 46, hepatic steatosis >5% by magnetic resonance spectroscopy), ii) Hepatic steatosis and alcohol (dual aetiology fatty liver disease [DAFLD]) (n = 16, hepatic steatosis >5%, consuming >20 g/day of alcohol) and iii) CONTROL (n = 34, no cardiac, liver or metabolic disorders, <20 g/day of alcohol). Patients with NAFLD and DAFLD had significantly impaired cardiac and autonomic function when compared with controls. Diastolic variability and systolic variability (LF/HF-sBP [n/1]; 2.3 (1.7) and 2.3 (1.5) vs. 3.4 (1.5), p <0.01) were impaired in patients with NAFLD and DAFLD when compared to controls, with DAFLD individuals showing a decrease in diastolic variability relative to NAFLD patients. Hepatic steatosis and fasting glucose were negatively correlated with stroke volume index. Fibrosis stage was significantly negatively associated with mean blood pressure (r = −0.47, p = 0.02), diastolic variability (r = −0.58, p ≤0.01) and systolic variability (r = −0.42, p = 0.04). Hepatic steatosis was independently associated with cardiac function (p ≤0.01); TNF-α (p ≤0.05) and CK-18 (p ≤0.05) were independently associated with autonomic function. Cardiac and autonomic impairments appear to be dependent on level of liver fat, metabolic dysfunction, inflammation and fibrosis staging, and to a lesser extent alcohol intake. Interventions should be sought to moderate the excess cardiovascular risk in patients with NAFLD or DAFLD. Although the relationship between liver disease and CVD is well defined, the underpinning mechanisms are less well understood.The presence of subclinical atherosclerosis has been shown to be linked with disease severity in NAFLD.14Furthermore, chronic low grade inflammation is a characteristic of metabolic disorders, and may contribute to extrahepatic complications.15In particular, higher levels of circulating inflammatory mediators associated with non-alcoholic steatohepatitis (NASH) may also play a pathogenic role in CVD,16 activate sites in the brain17 and increase sympathetic outflow.18NASH and fibrosis severity have also been linked with epicardial fat, morphological and functional cardiac alterations and inflammation.19–23Heavy alcohol consumption also increases the risk of cardiomyopathy, hypertension, arrhythmias, stroke and cardiac ischemia and altered autonomic function.11,24Furthermore, increased alcohol intake has shown to lead to weight gain and obesity, further exacerbating the risk factors and overlap between NAFLD and ALD.25
Intrahepatic cholangiocarcinoma (ICC) is the second-most lethal primary liver cancer. Little is known about intratumoral heterogeneity (ITH) and its impact on ICC progression. We aimed to investigate the ITH of ICC in the hope of helping to develop new therapeutic strategies. We obtained 69 spatially distinct regions from six operable ICCs. Patient-derived primary cancer cells (PDPCs) were established for each region, followed by whole-exome sequencing (WES) and multi-level validation. We observed widespread ITH for both somatic mutations and clonal architecture, shaped by multiple mechanisms, like clonal “illusion”, parallel evolution and chromosome instability. A median of 60.3% of mutations were heterogeneous, among which 85% of the driver mutations were located on the branches of tumor phylogenetic trees. Many truncal and clonal driver mutations occurred in tumor suppressor genes, such as TP53, SMARCB1 and PBRM1 that are involved in DNA repair and chromatin-remodeling. Genome doubling occurred in most cases (5/6) after the accumulation of truncal mutations and was shared by all intratumoral sub-regions. In all cases, ongoing chromosomal instability is evident throughout the evolutionary trajectory of ICC. The recurrence of ICC1239 provided evidence to support the polyclonal metastatic seeding in ICC. The change of mutation landscape and internal diversity among subclones during metastasis, such as the loss of chemoresistance mediator, can be used for new treatment strategies. Targeted therapy against truncal alterations, such as IDH1, JAK1, and KRAS mutations and EGFR amplification, was developed in 5/6 patients. Integrated investigations of spatial ITH and clonal evolution may provide an important molecular foundation for enhanced understanding of tumorigenesis and progression in ICC. Intrahepatic cholangiocarcinoma (ICC) is the second-most frequent type of primary liver cancer with an increasing global incidence during the past decades.Although hepatectomy is a curative treatment method for early ICC, five-year survival rates after resection varies around 30–35%.1For those in the late stage that only receive cisplatin-based chemotherapy, the median overall survival is as low as 11.7 months.2The lack of commonly accepted effective medical treatments highlights the importance of understanding ICC pathogenesis, to develop new treatment strategies.3
Intrahepatic cholangiocarcinoma (ICC) is the second-most lethal primary liver cancer. Little is known about intratumoral heterogeneity (ITH) and its impact on ICC progression. We aimed to investigate the ITH of ICC in the hope of helping to develop new therapeutic strategies. We obtained 69 spatially distinct regions from six operable ICCs. Patient-derived primary cancer cells (PDPCs) were established for each region, followed by whole-exome sequencing (WES) and multi-level validation. We observed widespread ITH for both somatic mutations and clonal architecture, shaped by multiple mechanisms, like clonal “illusion”, parallel evolution and chromosome instability. A median of 60.3% of mutations were heterogeneous, among which 85% of the driver mutations were located on the branches of tumor phylogenetic trees. Many truncal and clonal driver mutations occurred in tumor suppressor genes, such as TP53, SMARCB1 and PBRM1 that are involved in DNA repair and chromatin-remodeling. Genome doubling occurred in most cases (5/6) after the accumulation of truncal mutations and was shared by all intratumoral sub-regions. In all cases, ongoing chromosomal instability is evident throughout the evolutionary trajectory of ICC. The recurrence of ICC1239 provided evidence to support the polyclonal metastatic seeding in ICC. The change of mutation landscape and internal diversity among subclones during metastasis, such as the loss of chemoresistance mediator, can be used for new treatment strategies. Targeted therapy against truncal alterations, such as IDH1, JAK1, and KRAS mutations and EGFR amplification, was developed in 5/6 patients. Integrated investigations of spatial ITH and clonal evolution may provide an important molecular foundation for enhanced understanding of tumorigenesis and progression in ICC. Recently, genomic sequencing studies have provided insights into the genetic landscape of ICC.4–6The driver alterations identified in ICC may represent potential candidates for personalized targeted therapy.However, genomic alterations identified in most of these studies were obtained by single sampling in individual cases, and little is known about the spatial intratumoral heterogeneity (ITH) or temporal clonal evolutionary processes in ICC.In fact, anti-tumor drug development may require not only an understanding of genetic alterations but also an appreciation of the clonal status of driver alterations and evolutionary processes.During tumor progression, accumulating somatic mutations bestow a selective advantage on subclonal expansion, through which the fittest subclone will dominate.In turn, the presence of subclonal mutations may affect the efficacy of the targeted therapy.For example, in BRAFV600-mutant melanoma, the subclonal heterogeneity of PI3K-PTEN-AKT signaling is involved in the resistance to BRAF inhibitors.7Therefore, therapeutics targeting clonal somatic events or a combination of multiple target sites seem to be more effective.
Intrahepatic cholangiocarcinoma (ICC) is the second-most lethal primary liver cancer. Little is known about intratumoral heterogeneity (ITH) and its impact on ICC progression. We aimed to investigate the ITH of ICC in the hope of helping to develop new therapeutic strategies. We obtained 69 spatially distinct regions from six operable ICCs. Patient-derived primary cancer cells (PDPCs) were established for each region, followed by whole-exome sequencing (WES) and multi-level validation. We observed widespread ITH for both somatic mutations and clonal architecture, shaped by multiple mechanisms, like clonal “illusion”, parallel evolution and chromosome instability. A median of 60.3% of mutations were heterogeneous, among which 85% of the driver mutations were located on the branches of tumor phylogenetic trees. Many truncal and clonal driver mutations occurred in tumor suppressor genes, such as TP53, SMARCB1 and PBRM1 that are involved in DNA repair and chromatin-remodeling. Genome doubling occurred in most cases (5/6) after the accumulation of truncal mutations and was shared by all intratumoral sub-regions. In all cases, ongoing chromosomal instability is evident throughout the evolutionary trajectory of ICC. The recurrence of ICC1239 provided evidence to support the polyclonal metastatic seeding in ICC. The change of mutation landscape and internal diversity among subclones during metastasis, such as the loss of chemoresistance mediator, can be used for new treatment strategies. Targeted therapy against truncal alterations, such as IDH1, JAK1, and KRAS mutations and EGFR amplification, was developed in 5/6 patients. Integrated investigations of spatial ITH and clonal evolution may provide an important molecular foundation for enhanced understanding of tumorigenesis and progression in ICC. Studies adopting multiregional whole-exome sequencing (M-WES) have uncovered the complex ITH and evolutionary pattern in several types of cancer, including prostate cancer, hepatocellular carcinoma and non-small cell lung cancer.8–11Since ICC contains a large proportion of non-tumor components,12 sequencing tumor sampling directly may be confounded by low tumor purity, resulting in unexpected impacts on the accuracy of the mutational landscape and clonal structure.As a countermeasure, patient-derived primary cancer cells (PDPCs) obtained by single sampling were developed for their high purity and cell population representativeness.13Previous studies on hepatocellular carcinoma and lung cancer have shown that PDPCs can enable an accurate assessment of intratumor genetic heterogeneity and represent a potential powerful tool for drug screening.13–15
In the sera of infected patients, hepatitis C virus (HCV) particles display heterogeneous forms with low-buoyant densities (<1.08), underscoring their lipidation via association with apoB-containing lipoproteins, which was proposed to occur during assembly or secretion from infected hepatocytes. However, the mechanisms inducing this association remain poorly-defined and most cell culture grown HCV (HCVcc) particles exhibit higher density (>1.08) and poor/no association with apoB. We aimed to elucidate the mechanisms of lipidation and to produce HCVcc particles resembling those in infected sera. We produced HCVcc particles of Jc1 or H77 strains from Huh-7.5 hepatoma cells cultured in standard conditions (10%-fetal calf serum) vs. in serum-free or human serum conditions before comparing their density profiles to patient-derived virus. We also characterized wild-type and Jc1/H77 hypervariable region 1 (HVR1)-swapped mutant HCVcc particles produced in serum-free media and incubated with different serum types or with purified lipoproteins. Compared to serum-free or fetal calf serum conditions, production with human serum redistributed most HCVcc infectious particles to low density (<1.08) or very-low density (<1.04) ranges. In addition, short-time incubation with human serum was sufficient to shift HCVcc physical particles to low-density fractions, in time- and dose-dependent manners, which increased their specific infectivity, promoted apoB-association and induced neutralization-resistance. Moreover, compared to Jc1, we detected higher levels of H77 HCVcc infectious particles in very-low-density fractions, which could unambiguously be attributed to strain-specific features of the HVR1 sequence. Finally, all 3 lipoprotein classes, i.e., very-low-density, low-density and high-density lipoproteins, could synergistically induce low-density shift of HCV particles; yet, this required additional non-lipid serum factor(s) that include albumin. The association of HCV particles with lipids may occur in the extracellular milieu. The lipidation level depends on serum composition as well as on HVR1-specific properties. These simple culture conditions allow production of infectious HCV particles resembling those of chronically-infected patients. Hepatitis C virus (HCV) infection is a major cause of chronic liver diseases worldwide.Although direct-acting antivirals (DAAs) can now cure most patients, there remain major challenges in basic, translational and clinical research.1As DAAs are only curative, the development of a protective vaccine remains an important goal; yet, this requires deeper knowledge of the HCV particle’s structure.Indeed, the HCV virion has unusually heterogenous morphology, size and properties.2Immunocapture of its surface proteins revealed particles of 50–80 nm without symmetrical arrangement.3–6HCV particles harbor 2 envelope glycoproteins, E1 and E2, inserted on a membranous envelope that surrounds a nucleocapsid, composed of a core protein multimer and RNA+ viral genome; yet, the organization of the virion surface remains elusive and there is currently no clear model of the HCV particle’s topology.
In the sera of infected patients, hepatitis C virus (HCV) particles display heterogeneous forms with low-buoyant densities (<1.08), underscoring their lipidation via association with apoB-containing lipoproteins, which was proposed to occur during assembly or secretion from infected hepatocytes. However, the mechanisms inducing this association remain poorly-defined and most cell culture grown HCV (HCVcc) particles exhibit higher density (>1.08) and poor/no association with apoB. We aimed to elucidate the mechanisms of lipidation and to produce HCVcc particles resembling those in infected sera. We produced HCVcc particles of Jc1 or H77 strains from Huh-7.5 hepatoma cells cultured in standard conditions (10%-fetal calf serum) vs. in serum-free or human serum conditions before comparing their density profiles to patient-derived virus. We also characterized wild-type and Jc1/H77 hypervariable region 1 (HVR1)-swapped mutant HCVcc particles produced in serum-free media and incubated with different serum types or with purified lipoproteins. Compared to serum-free or fetal calf serum conditions, production with human serum redistributed most HCVcc infectious particles to low density (<1.08) or very-low density (<1.04) ranges. In addition, short-time incubation with human serum was sufficient to shift HCVcc physical particles to low-density fractions, in time- and dose-dependent manners, which increased their specific infectivity, promoted apoB-association and induced neutralization-resistance. Moreover, compared to Jc1, we detected higher levels of H77 HCVcc infectious particles in very-low-density fractions, which could unambiguously be attributed to strain-specific features of the HVR1 sequence. Finally, all 3 lipoprotein classes, i.e., very-low-density, low-density and high-density lipoproteins, could synergistically induce low-density shift of HCV particles; yet, this required additional non-lipid serum factor(s) that include albumin. The association of HCV particles with lipids may occur in the extracellular milieu. The lipidation level depends on serum composition as well as on HVR1-specific properties. These simple culture conditions allow production of infectious HCV particles resembling those of chronically-infected patients. A remarkable feature of HCV is the particularly low-buoyant density of virions, coined lipo-viro-particles (LVP).6In sera from infected patients and experimentally-infected chimpanzees, most HCV particles are found in densities between 1.00–1.10,6–8 owing to their particular lipid composition, distinct from other enveloped viruses.Indeed, patient-derived HCV particles contain neutral lipids such as triglycerides or cholesteryl esters and are associated with circulating apolipoproteins such as apoC-I, apoE and apoB;6,7,9,10 the latter being the major structural and non-exchangeable component of low-density lipoprotein (LDL) and very-low-density lipoprotein (VLDL).Importantly, injection of HCV particles into experimentally-infected animals revealed that the low-density inoculum is the most infectious.11
In the sera of infected patients, hepatitis C virus (HCV) particles display heterogeneous forms with low-buoyant densities (<1.08), underscoring their lipidation via association with apoB-containing lipoproteins, which was proposed to occur during assembly or secretion from infected hepatocytes. However, the mechanisms inducing this association remain poorly-defined and most cell culture grown HCV (HCVcc) particles exhibit higher density (>1.08) and poor/no association with apoB. We aimed to elucidate the mechanisms of lipidation and to produce HCVcc particles resembling those in infected sera. We produced HCVcc particles of Jc1 or H77 strains from Huh-7.5 hepatoma cells cultured in standard conditions (10%-fetal calf serum) vs. in serum-free or human serum conditions before comparing their density profiles to patient-derived virus. We also characterized wild-type and Jc1/H77 hypervariable region 1 (HVR1)-swapped mutant HCVcc particles produced in serum-free media and incubated with different serum types or with purified lipoproteins. Compared to serum-free or fetal calf serum conditions, production with human serum redistributed most HCVcc infectious particles to low density (<1.08) or very-low density (<1.04) ranges. In addition, short-time incubation with human serum was sufficient to shift HCVcc physical particles to low-density fractions, in time- and dose-dependent manners, which increased their specific infectivity, promoted apoB-association and induced neutralization-resistance. Moreover, compared to Jc1, we detected higher levels of H77 HCVcc infectious particles in very-low-density fractions, which could unambiguously be attributed to strain-specific features of the HVR1 sequence. Finally, all 3 lipoprotein classes, i.e., very-low-density, low-density and high-density lipoproteins, could synergistically induce low-density shift of HCV particles; yet, this required additional non-lipid serum factor(s) that include albumin. The association of HCV particles with lipids may occur in the extracellular milieu. The lipidation level depends on serum composition as well as on HVR1-specific properties. These simple culture conditions allow production of infectious HCV particles resembling those of chronically-infected patients. A major bottleneck of HCV characterization is the lack of in vitro-infectivity of patient-derived particles,1,12 which has prevented researchers from unraveling the properties of virus/lipid interaction.To overcome this, several experimental cellular models have been designed, including cell culture-grown HCV (HCVcc) particles that are usually produced from Huh-7.5 hepatoma cells.8,13While HCVcc particles have a heterogeneous density profile, from 1.00 to >1.17, their specific infectivity in low-density fractions is poor,5,8,14,15 which reflects incomplete virion lipidation.Indeed, while HCVcc particles are associated with apoC-I16 and apoE,17 their association with apoB is variably detected.4,5,12,17–19
In the sera of infected patients, hepatitis C virus (HCV) particles display heterogeneous forms with low-buoyant densities (<1.08), underscoring their lipidation via association with apoB-containing lipoproteins, which was proposed to occur during assembly or secretion from infected hepatocytes. However, the mechanisms inducing this association remain poorly-defined and most cell culture grown HCV (HCVcc) particles exhibit higher density (>1.08) and poor/no association with apoB. We aimed to elucidate the mechanisms of lipidation and to produce HCVcc particles resembling those in infected sera. We produced HCVcc particles of Jc1 or H77 strains from Huh-7.5 hepatoma cells cultured in standard conditions (10%-fetal calf serum) vs. in serum-free or human serum conditions before comparing their density profiles to patient-derived virus. We also characterized wild-type and Jc1/H77 hypervariable region 1 (HVR1)-swapped mutant HCVcc particles produced in serum-free media and incubated with different serum types or with purified lipoproteins. Compared to serum-free or fetal calf serum conditions, production with human serum redistributed most HCVcc infectious particles to low density (<1.08) or very-low density (<1.04) ranges. In addition, short-time incubation with human serum was sufficient to shift HCVcc physical particles to low-density fractions, in time- and dose-dependent manners, which increased their specific infectivity, promoted apoB-association and induced neutralization-resistance. Moreover, compared to Jc1, we detected higher levels of H77 HCVcc infectious particles in very-low-density fractions, which could unambiguously be attributed to strain-specific features of the HVR1 sequence. Finally, all 3 lipoprotein classes, i.e., very-low-density, low-density and high-density lipoproteins, could synergistically induce low-density shift of HCV particles; yet, this required additional non-lipid serum factor(s) that include albumin. The association of HCV particles with lipids may occur in the extracellular milieu. The lipidation level depends on serum composition as well as on HVR1-specific properties. These simple culture conditions allow production of infectious HCV particles resembling those of chronically-infected patients. The lipidation difference between HCVcc and patient-derived particles has been attributed to a defective lipoprotein-metabolism pathway in Huh-7 cells, preventing the formation of fully lipidated VLDLs.12,20–22HCVcc particles grown in primary human hepatocytes (PHH), which produce normal VLDLs, have higher specific infectivity owing to the viral RNA peak that coincides with the fractions of highest infectivity at densities of 1.10–1.11,20 which are above those of patient-derived HCV.Likewise, HCV particles grown in PHH-xenograft mouse models that release normal human apoB and apoE levels display coincident infectivity and viral RNA peaks at 1.06–1.11,8,21 which, again, are higher than for patient-derived HCV.
In the sera of infected patients, hepatitis C virus (HCV) particles display heterogeneous forms with low-buoyant densities (<1.08), underscoring their lipidation via association with apoB-containing lipoproteins, which was proposed to occur during assembly or secretion from infected hepatocytes. However, the mechanisms inducing this association remain poorly-defined and most cell culture grown HCV (HCVcc) particles exhibit higher density (>1.08) and poor/no association with apoB. We aimed to elucidate the mechanisms of lipidation and to produce HCVcc particles resembling those in infected sera. We produced HCVcc particles of Jc1 or H77 strains from Huh-7.5 hepatoma cells cultured in standard conditions (10%-fetal calf serum) vs. in serum-free or human serum conditions before comparing their density profiles to patient-derived virus. We also characterized wild-type and Jc1/H77 hypervariable region 1 (HVR1)-swapped mutant HCVcc particles produced in serum-free media and incubated with different serum types or with purified lipoproteins. Compared to serum-free or fetal calf serum conditions, production with human serum redistributed most HCVcc infectious particles to low density (<1.08) or very-low density (<1.04) ranges. In addition, short-time incubation with human serum was sufficient to shift HCVcc physical particles to low-density fractions, in time- and dose-dependent manners, which increased their specific infectivity, promoted apoB-association and induced neutralization-resistance. Moreover, compared to Jc1, we detected higher levels of H77 HCVcc infectious particles in very-low-density fractions, which could unambiguously be attributed to strain-specific features of the HVR1 sequence. Finally, all 3 lipoprotein classes, i.e., very-low-density, low-density and high-density lipoproteins, could synergistically induce low-density shift of HCV particles; yet, this required additional non-lipid serum factor(s) that include albumin. The association of HCV particles with lipids may occur in the extracellular milieu. The lipidation level depends on serum composition as well as on HVR1-specific properties. These simple culture conditions allow production of infectious HCV particles resembling those of chronically-infected patients. While HCVcc studies have tremendously advanced the knowledge of HCV and host-virus interactions, culminating in new DAA regimens that cure most patients, many aspects of HCV biology remain ill-defined because of the lack of models fully mimicking the conformation of authentic HCV particles.Furthermore, as lipoprotein association is thought to shape their surface and induce neutralization resistance,4,6,7,14,23 elucidating their structure would improve the design of rational vaccine candidates.
Advanced hepatocellular carcinoma (HCC) is a lethal malignancy with limited treatment options. Sorafenib is the only FDA-approved first-line targeted drug for advanced HCC, but its effect on patient survival is limited. Further, patients ultimately present with disease progression. A better understanding of the causes of sorafenib resistance, enhancing the efficacy of sorafenib and finding a reliable predictive biomarker are crucial to achieve efficient control of HCC. The functional effects of ANXA3 in conferring sorafenib resistance to HCC cells were analyzed in apoptotic and tumorigenicity assays. The role of ANXA3/PKCδ-mediated p38 signaling, and subsequently altered autophagic and apoptotic events, was assessed by immunoprecipitation, immunoblotting, immunofluorescence and transmission electron microscopy assays. The prognostic value of ANXA3 in predicting response to sorafenib was evaluated by immunohistochemistry. The therapeutic value of targeting ANXA3 to combat HCC with anti-ANXA3 monoclonal antibody alone or in combination with sorafenib/regorafenib was investigated ex vivo and in vivo. ANXA3 conferred HCC cells with resistance to sorafenib. ANXA3 was found enriched in sorafenib-resistant HCC cells and patient-derived xenografts. Mechanistically, overexpression of ANXA3 in sorafenib-resistant HCC cells suppressed PKCδ/p38 associated apoptosis and activated autophagy for cell survival. Clinically, ANXA3 expression correlated positively with the autophagic marker LC3B in HCC and was associated with a worse overall survival in patients who went on to receive sorafenib treatment. Anti-ANXA3 monoclonal antibody therapy combined with sorafenib/regorafenib impaired tumor growth in vivo and significantly increased survival. Anti-ANXA3 therapy in combination with sorafenib/regorafenib represents a novel therapeutic strategy for HCC treatment. ANXA3 represents a useful predictive biomarker to stratify patients with HCC for sorafenib treatment. Hepatocellular carcinoma (HCC) is a very aggressive disease and represents the third leading cause of cancer-related mortality worldwide.1Although treatment of HCC has greatly improved over the last decade, most patients with HCC are presented with an underlying chronic liver disease and diagnosed only at advanced stages, when they are no longer eligible for curative therapies such as liver resection or transplantation.2In these cases, the multi-target tyrosine kinase inhibitor sorafenib is the only FDA-approved first-line systematic therapy, expanding patient median survival from 7.9 to 10.7 months.3–5Despite initial response, only rarely do sorafenib-treated tumors regress completely, and the therapeutic effects of the drug are often temporary.Unfortunately, most patients develop disease progression, and in the case of HCC, radiological progression under sorafenib occurs after 4–5 months of treatment.4Regorafenib has recently been approved by the FDA as a second-line treatment for patients with advanced HCC, where regorafenib has provided some survival benefit in those progressing on sorafenib treatment.6As sorafenib targets several signaling pathways, acquisition of resistance might involve different mechanisms, including the activation of compensatory signaling cascades, rather than specific DNA aberrations, as was described with BCR-ABL and imatinib7 and BRAF mutations in melanoma resistant to vemurafenib.8Further, a targeted anti-cancer drug should ideally have a biomarker to predict its clinical response.Single kinase inhibitors, such as tarceva (EGFR inhibitor) or crizotinib (ALK inhibitor), have as predictive markers EGFR mutation and ALK translocation, respectively.9,10However, such markers are still not available for sorafenib because it targets multiple kinases, complicating the mechanism of action.As the precise molecular mechanisms underlying resistance to sorafenib are still barely understood,11 there is an urgent need to characterize drivers of resistance to identify new biomarkers that can predict sorafenib treatment outcome and therapeutic strategy that can improve the efficacy of sorafenib.
Advanced hepatocellular carcinoma (HCC) is a lethal malignancy with limited treatment options. Sorafenib is the only FDA-approved first-line targeted drug for advanced HCC, but its effect on patient survival is limited. Further, patients ultimately present with disease progression. A better understanding of the causes of sorafenib resistance, enhancing the efficacy of sorafenib and finding a reliable predictive biomarker are crucial to achieve efficient control of HCC. The functional effects of ANXA3 in conferring sorafenib resistance to HCC cells were analyzed in apoptotic and tumorigenicity assays. The role of ANXA3/PKCδ-mediated p38 signaling, and subsequently altered autophagic and apoptotic events, was assessed by immunoprecipitation, immunoblotting, immunofluorescence and transmission electron microscopy assays. The prognostic value of ANXA3 in predicting response to sorafenib was evaluated by immunohistochemistry. The therapeutic value of targeting ANXA3 to combat HCC with anti-ANXA3 monoclonal antibody alone or in combination with sorafenib/regorafenib was investigated ex vivo and in vivo. ANXA3 conferred HCC cells with resistance to sorafenib. ANXA3 was found enriched in sorafenib-resistant HCC cells and patient-derived xenografts. Mechanistically, overexpression of ANXA3 in sorafenib-resistant HCC cells suppressed PKCδ/p38 associated apoptosis and activated autophagy for cell survival. Clinically, ANXA3 expression correlated positively with the autophagic marker LC3B in HCC and was associated with a worse overall survival in patients who went on to receive sorafenib treatment. Anti-ANXA3 monoclonal antibody therapy combined with sorafenib/regorafenib impaired tumor growth in vivo and significantly increased survival. Anti-ANXA3 therapy in combination with sorafenib/regorafenib represents a novel therapeutic strategy for HCC treatment. ANXA3 represents a useful predictive biomarker to stratify patients with HCC for sorafenib treatment. Our group has previously reported on the critical role of endogenous and secretory annexin A3 (ANXA3) in promoting aggressive cancer and stem cell-like properties in HCC.12Clinically, ANXA3 expression in HCC patient sera and tissues closely associated with aggressive clinical features and when used either alone or in combination with alpha-fetoprotein (AFP), was found to represent a more specific and sensitive biomarker for HCC detection than AFP alone.12Consistent with our findings, ANXA3 was also reported to be overexpressed in 5-fluorouracil resistant HCC cells,13 to play a role in promoting resistance to chemotherapy in HCC,14 and to be a possible target for immunotherapy of liver cancer stem-like cells,15 In light of these interesting findings, our group subsequently went on to develop a monoclonal antibody specific against ANXA3 (anti-ANXA3 mAb) and showed in vivo that the use of this antibody alone or in combination with cisplatin could efficiently lead to a reduced ability of HCC cells to initiate tumor growth and self-renewal, concomitant with a decrease in liver cancer stem cell proportions including those of CD133, CD24 and EpCAM.12A number of reports have revealed that EpCAM+ and label-retaining HCC cells are more resistant to sorafenib treatment16,17 and that sorafenib-resistant HCC cells display enhanced cancer stemness features.18,19These results, together with our previous findings where a link between ANXA3 and cancer stem cells is demonstrated, led us to hypothesize that ANXA3 may also contribute to sorafenib resistance in HCC.
Although gadoxetate disodium-enhanced magnetic resonance imaging (MRI) shows higher sensitivity for diagnosing hepatocellular carcinoma (HCC), its arterial-phase images may be unsatisfactory because of weak arterial enhancement. We investigated the clinical effectiveness of arterial subtraction images from gadoxetate disodium-enhanced MRI for diagnosing early-stage HCC using the Liver Imaging Reporting and Data System (LI-RADS) v2018. In 258 patients at risk of HCC who underwent gadoxetate disodium-enhanced MRI in 2016, a total of 372 hepatic nodules (273 HCCs, 18 other malignancies, and 81 benign nodules) of 3.0 cm or smaller were retrospectively analyzed. Final diagnosis was assessed histopathologically or clinically (marginal recurrence after treatment or change in lesion size on follow-up imaging). The detection rate for arterial hyperenhancement was compared between ordinary arterial-phase and arterial subtraction images, and the benefit of arterial subtraction images in diagnosing HCC using LI-RADS was assessed. Arterial subtraction images had a significantly higher detection rate for arterial hyperenhancement than ordinary arterial-phase images, both for all hepatic nodules (72.3% vs. 62.4%, p <0.001) and HCCs (91.9% vs. 80.6%, p <0.001). Compared with ordinary arterial-phase images, arterial subtraction images significantly increased the sensitivity of LI-RADS category 5 for diagnosis of HCC (64.1% [173/270] vs. 55.9% [151/270], p <0.001), without significantly decreasing specificity (92.9% [91/98] vs. 94.9% [93/98], p = 0.155). For histopathologically confirmed lesions, arterial subtraction images significantly increased sensitivity to 68.8% (128/186) from the 61.3% (114/186) of ordinary arterial-phase images (p <0.001), with a minimal decrease in specificity to 84.8% (39/46) from 89.1% (41/46) (p = 0.151). Arterial subtraction images of gadoxetate disodium-enhanced MRI can significantly improve the sensitivity of early-stage HCC diagnosis using LI-RADS, without a significant decrease in specificity. Hepatocellular carcinoma (HCC) is the most common primary hepatic malignancy and the third most frequent cause of cancer-related deaths.1,2Although the prognosis of patients with advanced HCC remains poor, patients with early-stage HCC are eligible for curative treatments such as surgical resection, local ablation, and liver transplantation.3,4Therefore, accurate imaging diagnosis of early-stage HCC is important.
Although gadoxetate disodium-enhanced magnetic resonance imaging (MRI) shows higher sensitivity for diagnosing hepatocellular carcinoma (HCC), its arterial-phase images may be unsatisfactory because of weak arterial enhancement. We investigated the clinical effectiveness of arterial subtraction images from gadoxetate disodium-enhanced MRI for diagnosing early-stage HCC using the Liver Imaging Reporting and Data System (LI-RADS) v2018. In 258 patients at risk of HCC who underwent gadoxetate disodium-enhanced MRI in 2016, a total of 372 hepatic nodules (273 HCCs, 18 other malignancies, and 81 benign nodules) of 3.0 cm or smaller were retrospectively analyzed. Final diagnosis was assessed histopathologically or clinically (marginal recurrence after treatment or change in lesion size on follow-up imaging). The detection rate for arterial hyperenhancement was compared between ordinary arterial-phase and arterial subtraction images, and the benefit of arterial subtraction images in diagnosing HCC using LI-RADS was assessed. Arterial subtraction images had a significantly higher detection rate for arterial hyperenhancement than ordinary arterial-phase images, both for all hepatic nodules (72.3% vs. 62.4%, p <0.001) and HCCs (91.9% vs. 80.6%, p <0.001). Compared with ordinary arterial-phase images, arterial subtraction images significantly increased the sensitivity of LI-RADS category 5 for diagnosis of HCC (64.1% [173/270] vs. 55.9% [151/270], p <0.001), without significantly decreasing specificity (92.9% [91/98] vs. 94.9% [93/98], p = 0.155). For histopathologically confirmed lesions, arterial subtraction images significantly increased sensitivity to 68.8% (128/186) from the 61.3% (114/186) of ordinary arterial-phase images (p <0.001), with a minimal decrease in specificity to 84.8% (39/46) from 89.1% (41/46) (p = 0.151). Arterial subtraction images of gadoxetate disodium-enhanced MRI can significantly improve the sensitivity of early-stage HCC diagnosis using LI-RADS, without a significant decrease in specificity. To standardize the imaging diagnosis of HCC in at-risk patients, the Liver Imaging Reporting and Data System (LI-RADS) was developed in 2011,5 updated in 2014, 2017 and 2018, and unified with the American Association for the Study of Liver Disease (AASLD) 2018 HCC clinical practice guidance.6LI-RADS categorizes each hepatic observation according to its likelihood of benignity and HCC (i.e., LR-1 to LR-5).Major features in the LI-RADS v2018 include arterial-phase hyperenhancement, enhancing capsule, non-peripheral washout, and threshold growth.Although all of these major features are significantly associated with HCC, it is of the utmost importance to detect arterial-phase hyperenhancement for the diagnosis of HCC and evaluation of treatment response.7–9
Although gadoxetate disodium-enhanced magnetic resonance imaging (MRI) shows higher sensitivity for diagnosing hepatocellular carcinoma (HCC), its arterial-phase images may be unsatisfactory because of weak arterial enhancement. We investigated the clinical effectiveness of arterial subtraction images from gadoxetate disodium-enhanced MRI for diagnosing early-stage HCC using the Liver Imaging Reporting and Data System (LI-RADS) v2018. In 258 patients at risk of HCC who underwent gadoxetate disodium-enhanced MRI in 2016, a total of 372 hepatic nodules (273 HCCs, 18 other malignancies, and 81 benign nodules) of 3.0 cm or smaller were retrospectively analyzed. Final diagnosis was assessed histopathologically or clinically (marginal recurrence after treatment or change in lesion size on follow-up imaging). The detection rate for arterial hyperenhancement was compared between ordinary arterial-phase and arterial subtraction images, and the benefit of arterial subtraction images in diagnosing HCC using LI-RADS was assessed. Arterial subtraction images had a significantly higher detection rate for arterial hyperenhancement than ordinary arterial-phase images, both for all hepatic nodules (72.3% vs. 62.4%, p <0.001) and HCCs (91.9% vs. 80.6%, p <0.001). Compared with ordinary arterial-phase images, arterial subtraction images significantly increased the sensitivity of LI-RADS category 5 for diagnosis of HCC (64.1% [173/270] vs. 55.9% [151/270], p <0.001), without significantly decreasing specificity (92.9% [91/98] vs. 94.9% [93/98], p = 0.155). For histopathologically confirmed lesions, arterial subtraction images significantly increased sensitivity to 68.8% (128/186) from the 61.3% (114/186) of ordinary arterial-phase images (p <0.001), with a minimal decrease in specificity to 84.8% (39/46) from 89.1% (41/46) (p = 0.151). Arterial subtraction images of gadoxetate disodium-enhanced MRI can significantly improve the sensitivity of early-stage HCC diagnosis using LI-RADS, without a significant decrease in specificity. Some early-stage HCCs (17.3–31.6%) may not show arterial-phase hyperenhancement, and this results in a low sensitivity for the diagnosis of HCC.10–16Moreover, as arterial-phase enhancement on gadoxetate disodium (Eovist/Primovist; Bayer HealthCare, Berlin, Germany)-enhanced magnetic resonance imaging (MRI) is weaker than that on extracellular contrast-enhanced MRI, the detection rate for arterial-phase hyperenhancement of early-stage HCCs may be reduced on gadoxetate disodium-enhanced MRI in comparison with extracellular contrast agents.17To overcome the weak arterial enhancement of gadoxetate disodium, the LI-RADS v2018 suggests (as a technical recommendation) the use of subtraction images between the unenhanced and arterial phases, which many commercially available MRI systems with recent technical advances in image registration techniques allow.6Although a few previous studies have reported the added value of the arterial subtraction images for diagnosing HCCs on gadoxetate disodium-enhanced liver MRI,15,16 the clinical impact of arterial subtraction images on gadoxetate disodium-enhanced MRI has not been investigated using the LI-RADS.
The role of 18F-fluorodeoxyglucose positron emission tomography (18FDG-PET) in the diagnosis and staging of patients with biliary tract cancers (BTCs) remains controversial, so we aimed to provide robust information on the utility of 18FDG-PET in the diagnosis and management of BTC. This systematic review and meta-analysis explored the diagnostic test accuracy of 18FDG-PET as a diagnostic tool for diagnosis of primary tumour, lymph node invasion, distant metastases and relapsed disease. Subgroup analysis by study quality and BTC subtype were performed. Changes in management based on 18FDG-PET and impact of maximum standardised uptake values (SUVmax) on prognosis were also assessed. A random effects model was used for meta-analyses. A total of 2,125 patients were included from 47 eligible studies. The sensitivity (Se) and specificity (Sp) of 18FDG-PET for the diagnosis of primary tumour were 91.7% (95% CI 89.8–93.2) and 51.3% (95% CI 46.4–56.2), respectively, with an area under the curve (AUC) of 0.8668. For lymph node invasion, Se was 88.4% (95% CI 82.6–92.8) and Sp was 69.1% (95% CI 63.8–74.1); AUC 0.8519. For distant metastases, Se was 85.4% (95% CI 79.5–90.2) and Sp was 89.7% (95% CI 86.0–92.7); AUC 0.9253. For relapse, Se was 90.1% (95% CI 84.4–94.3) and Sp was 83.5% (95% CI 74.4–90.4); AUC 0.9592. No diagnostic threshold effect was identified. Meta-regression did not identify significant sources of heterogeneity. Sensitivity analysis revealed no change in results when analyses were limited to studies with low risk of bias/concern. The pooled proportion of change in management was 15% (95% CI 11–20); the majority (78%) due to disease upstaging. Baseline high SUVmax was associated with worse survival (pooled hazard ratio of 1.79; 95% CI 1.37–2.33; p <0.001). There is evidence to support the incorporation of 18FDG-PET into the current standard of care for the staging (lymph node and distant metastases) and identification of relapse in patients with BTC to guide treatment selection; especially if the identification of occult sites of disease would change management, or if diagnosis of relapse remains unclear following standard of care imaging. The role for diagnosis of the primary tumour remains controversial due to low sensitivity and 18FDG-PET should not be considered as a replacement for pathological confirmation in this setting. Introduction to biliary tract cancer
The role of 18F-fluorodeoxyglucose positron emission tomography (18FDG-PET) in the diagnosis and staging of patients with biliary tract cancers (BTCs) remains controversial, so we aimed to provide robust information on the utility of 18FDG-PET in the diagnosis and management of BTC. This systematic review and meta-analysis explored the diagnostic test accuracy of 18FDG-PET as a diagnostic tool for diagnosis of primary tumour, lymph node invasion, distant metastases and relapsed disease. Subgroup analysis by study quality and BTC subtype were performed. Changes in management based on 18FDG-PET and impact of maximum standardised uptake values (SUVmax) on prognosis were also assessed. A random effects model was used for meta-analyses. A total of 2,125 patients were included from 47 eligible studies. The sensitivity (Se) and specificity (Sp) of 18FDG-PET for the diagnosis of primary tumour were 91.7% (95% CI 89.8–93.2) and 51.3% (95% CI 46.4–56.2), respectively, with an area under the curve (AUC) of 0.8668. For lymph node invasion, Se was 88.4% (95% CI 82.6–92.8) and Sp was 69.1% (95% CI 63.8–74.1); AUC 0.8519. For distant metastases, Se was 85.4% (95% CI 79.5–90.2) and Sp was 89.7% (95% CI 86.0–92.7); AUC 0.9253. For relapse, Se was 90.1% (95% CI 84.4–94.3) and Sp was 83.5% (95% CI 74.4–90.4); AUC 0.9592. No diagnostic threshold effect was identified. Meta-regression did not identify significant sources of heterogeneity. Sensitivity analysis revealed no change in results when analyses were limited to studies with low risk of bias/concern. The pooled proportion of change in management was 15% (95% CI 11–20); the majority (78%) due to disease upstaging. Baseline high SUVmax was associated with worse survival (pooled hazard ratio of 1.79; 95% CI 1.37–2.33; p <0.001). There is evidence to support the incorporation of 18FDG-PET into the current standard of care for the staging (lymph node and distant metastases) and identification of relapse in patients with BTC to guide treatment selection; especially if the identification of occult sites of disease would change management, or if diagnosis of relapse remains unclear following standard of care imaging. The role for diagnosis of the primary tumour remains controversial due to low sensitivity and 18FDG-PET should not be considered as a replacement for pathological confirmation in this setting. Biliary tract cancers (BTCs) (including cholangiocarcinoma and cancers of the ampulla of Vater and gallbladder) are considered low-incidence malignancies, accounting for approximately 0.7% of all malignant tumours in adults.However, data from the past 25 years suggest that, predominantly due to a rise in diagnosis of intrahepatic cholangiocarcinoma, both incidence and mortality are increasing.1,2
Natural killer (NK) cells are known to exert strong antiviral activity. Killer cell lectin-like receptor subfamily G member 1 (KLRG1) is expressed by terminally differentiated NK cells and KLRG1-expressing lymphocytes are known to expand following chronic viral infections. We aimed to elucidate the previously unknown role of KLRG1 in the pathogenesis of chronic hepatitis B (CHB). KLRG1+ NK cells were taken from the blood and liver of healthy individuals and patients with CHB. The phenotype and function of these cells was assessed using flow cytometry and in vitro stimulation. Patients with CHB had a higher frequency of KLRG1+ NK cells compared to healthy controls (blood 13.4 vs. 2.3%, p <0.0001 and liver 23.4 vs. 2.6%, p <0.01). KLRG1+ NK cells were less responsive to K562 and cytokine stimulation, but demonstrated enhanced cytotoxicity (9.0 vs. 4.8%, p <0.05) and IFN-γ release (8.0 vs. 1.5%, p <0.05) via antibody dependent cellular cytotoxicity compared to their KLRG1− counterparts. KLRG1+ NK cells possessed a mature phenotype, demonstrating stronger cytolytic activity and IFN-γ secretion against hepatic stellate cells (HSCs) than KLRG1− NK cells. Moreover, KLRG1+ NK cells more effectively induced primary HSC apoptosis in a TRAIL-dependent manner. Increased KLRG1+ NK cell frequency in the liver and blood was associated with lower fibrosis stage (F0/F1) in patients with CHB. Finally, the expression of CD44, degranulation and IFN-γ production were all increased in KLRG1+ NK cells following stimulation with osteopontin, the CD44 ligand, suggesting that HSC-derived osteopontin may cause KLRG1+ NK cell activation. KLRG1+ NK cells likely play an antifibrotic role during the natural course of CHB infection. Harnessing this antifibrotic function may provide a novel therapeutic approach to treat liver fibrosis in patients with CHB. Natural killer (NK) cells are a primary effector population responsible for the innate immune response to viral infection, representing approximately 5–15% of blood and 40–60% of hepatic lymphocytes.1,2While NK cells play a role in controlling hepatitis B virus (HBV) infection,3–9 they act as a double-edged sword, contributing to liver injury through sustained activation and tissue damage.10–13
Natural killer (NK) cells are known to exert strong antiviral activity. Killer cell lectin-like receptor subfamily G member 1 (KLRG1) is expressed by terminally differentiated NK cells and KLRG1-expressing lymphocytes are known to expand following chronic viral infections. We aimed to elucidate the previously unknown role of KLRG1 in the pathogenesis of chronic hepatitis B (CHB). KLRG1+ NK cells were taken from the blood and liver of healthy individuals and patients with CHB. The phenotype and function of these cells was assessed using flow cytometry and in vitro stimulation. Patients with CHB had a higher frequency of KLRG1+ NK cells compared to healthy controls (blood 13.4 vs. 2.3%, p <0.0001 and liver 23.4 vs. 2.6%, p <0.01). KLRG1+ NK cells were less responsive to K562 and cytokine stimulation, but demonstrated enhanced cytotoxicity (9.0 vs. 4.8%, p <0.05) and IFN-γ release (8.0 vs. 1.5%, p <0.05) via antibody dependent cellular cytotoxicity compared to their KLRG1− counterparts. KLRG1+ NK cells possessed a mature phenotype, demonstrating stronger cytolytic activity and IFN-γ secretion against hepatic stellate cells (HSCs) than KLRG1− NK cells. Moreover, KLRG1+ NK cells more effectively induced primary HSC apoptosis in a TRAIL-dependent manner. Increased KLRG1+ NK cell frequency in the liver and blood was associated with lower fibrosis stage (F0/F1) in patients with CHB. Finally, the expression of CD44, degranulation and IFN-γ production were all increased in KLRG1+ NK cells following stimulation with osteopontin, the CD44 ligand, suggesting that HSC-derived osteopontin may cause KLRG1+ NK cell activation. KLRG1+ NK cells likely play an antifibrotic role during the natural course of CHB infection. Harnessing this antifibrotic function may provide a novel therapeutic approach to treat liver fibrosis in patients with CHB. In humans, NK cells are historically divided into 2 subsets according to the expression of CD56: CD56dim NK cells represent approximately 90% of circulating NK cells and demonstrate strong cytolytic capacity, whereas CD56bright NK cells constitute the remaining 10% of NK cells and are strong producers of cytokines.14,15With shorter telomeres and later appearance following haematopoietic stem cell transplantation,16,17 CD56dim cells are considered further differentiated than CD56bright NK cells.They can nonetheless continue to differentiate, losing NKG2A and acquiring CD16, killer immunoglobulin-like receptors (KIRs), CD57, and killer cell lectin-like receptor G1 (KLRG1) surface markers that appear sequentially during NK cell maturation.18–22
Natural killer (NK) cells are known to exert strong antiviral activity. Killer cell lectin-like receptor subfamily G member 1 (KLRG1) is expressed by terminally differentiated NK cells and KLRG1-expressing lymphocytes are known to expand following chronic viral infections. We aimed to elucidate the previously unknown role of KLRG1 in the pathogenesis of chronic hepatitis B (CHB). KLRG1+ NK cells were taken from the blood and liver of healthy individuals and patients with CHB. The phenotype and function of these cells was assessed using flow cytometry and in vitro stimulation. Patients with CHB had a higher frequency of KLRG1+ NK cells compared to healthy controls (blood 13.4 vs. 2.3%, p <0.0001 and liver 23.4 vs. 2.6%, p <0.01). KLRG1+ NK cells were less responsive to K562 and cytokine stimulation, but demonstrated enhanced cytotoxicity (9.0 vs. 4.8%, p <0.05) and IFN-γ release (8.0 vs. 1.5%, p <0.05) via antibody dependent cellular cytotoxicity compared to their KLRG1− counterparts. KLRG1+ NK cells possessed a mature phenotype, demonstrating stronger cytolytic activity and IFN-γ secretion against hepatic stellate cells (HSCs) than KLRG1− NK cells. Moreover, KLRG1+ NK cells more effectively induced primary HSC apoptosis in a TRAIL-dependent manner. Increased KLRG1+ NK cell frequency in the liver and blood was associated with lower fibrosis stage (F0/F1) in patients with CHB. Finally, the expression of CD44, degranulation and IFN-γ production were all increased in KLRG1+ NK cells following stimulation with osteopontin, the CD44 ligand, suggesting that HSC-derived osteopontin may cause KLRG1+ NK cell activation. KLRG1+ NK cells likely play an antifibrotic role during the natural course of CHB infection. Harnessing this antifibrotic function may provide a novel therapeutic approach to treat liver fibrosis in patients with CHB. KLRG1 is an inhibitory receptor of the C-type lectin-like family.23Approximately 20% of human CD4+ T cells and 40% of CD8+ T cells express KLRG1, most of which are effector or memory T cells.24KLRG1 expressing cells possess poor proliferative capacity indicative of senescence, but still maintain their ability to exert effector functions.24–26During persistent viral infections such as human cytomegalovirus (HCMV),27,28 human immunodeficiency virus (HIV),29 and Epstein-Barr virus infection,26,30,31 the proportion of KLRG1+ T cells increases in peripheral blood.The interaction of KLRG1 with its ligand, E-cadherin, results in inhibition of cytotoxic activity, suggesting that KLRG1 may raise the activation threshold of T cells.32–35As a result, KLRG1 may limit the antiviral activity of T cells, thus contributing to viral persistence.36Unlike T cells, the role of KLRG1 on NK cells remains uncertain, particularly in chronic HBV infection.
Hepatocyte polarity is essential for the development of bile canaliculi and for safely transporting bile and waste products from the liver. Functional studies of autologous mutated proteins in the context of the polarized hepatocyte have been challenging because of the lack of appropriate cell models. The aims of this study were to obtain a patient-specific hepatocyte model that recapitulated hepatocyte polarity and to employ this model to study endogenous mutant proteins in liver diseases that involve hepatocyte polarity. Urine cell-derived pluripotent stem cells, taken from a patient with a homozygous mutation in ATP7B and a patient with a heterozygous mutation, were differentiated towards hepatocyte-like cells (hiHeps). HiHeps were also derived from a patient with MEDNIK syndrome. Polarized hiHeps that formed in vivo-like bile canaliculi could be generated from embryonic and patient urine cell-derived pluripotent stem cells. HiHeps recapitulated polarized protein trafficking processes, exemplified by the Cu2+-induced redistribution of the copper transporter protein ATP7B to the bile canalicular domain. We demonstrated that, in contrast to the current dogma, the most frequent yet enigmatic Wilson disease-causing ATP7B-H1069Q mutation per se did not preclude trafficking of ATP7B to the trans-Golgi Network. Instead, it prevented its Cu2+-induced polarized redistribution to the bile canalicular domain, which could not be reversed by pharmacological folding chaperones. Finally, we demonstrate that hiHeps from a patient with MEDNIK syndrome, suffering from liver copper overload of unclear etiology, showed no defect in the Cu2+-induced redistribution of ATP7B to the bile canaliculi. Functional cell polarity can be achieved in patient pluripotent stem cell-derived hiHeps, enabling, for the first time, the study of the endogenous mutant proteins, patient-specific pathogenesis and drug responses for diseases where hepatocyte polarity is a key factor. Hepatocytes are polarized cells, exemplified by the segregation of their plasma membranes into basolateral/sinusoidal and apical/canalicular domains.1,2Hepatocyte polarity is essential for many hepatocyte-specific functions.1Not surprisingly therefore, loss of hepatocyte polarity is correlated with liver diseases.2In inherited liver diseases, mutations in specific genes can cause a defect in the targeting, expression and/or function of proteins that display a steady-state residence at either sinusoidal or canalicular surface domains.2
Hepatocyte polarity is essential for the development of bile canaliculi and for safely transporting bile and waste products from the liver. Functional studies of autologous mutated proteins in the context of the polarized hepatocyte have been challenging because of the lack of appropriate cell models. The aims of this study were to obtain a patient-specific hepatocyte model that recapitulated hepatocyte polarity and to employ this model to study endogenous mutant proteins in liver diseases that involve hepatocyte polarity. Urine cell-derived pluripotent stem cells, taken from a patient with a homozygous mutation in ATP7B and a patient with a heterozygous mutation, were differentiated towards hepatocyte-like cells (hiHeps). HiHeps were also derived from a patient with MEDNIK syndrome. Polarized hiHeps that formed in vivo-like bile canaliculi could be generated from embryonic and patient urine cell-derived pluripotent stem cells. HiHeps recapitulated polarized protein trafficking processes, exemplified by the Cu2+-induced redistribution of the copper transporter protein ATP7B to the bile canalicular domain. We demonstrated that, in contrast to the current dogma, the most frequent yet enigmatic Wilson disease-causing ATP7B-H1069Q mutation per se did not preclude trafficking of ATP7B to the trans-Golgi Network. Instead, it prevented its Cu2+-induced polarized redistribution to the bile canalicular domain, which could not be reversed by pharmacological folding chaperones. Finally, we demonstrate that hiHeps from a patient with MEDNIK syndrome, suffering from liver copper overload of unclear etiology, showed no defect in the Cu2+-induced redistribution of ATP7B to the bile canaliculi. Functional cell polarity can be achieved in patient pluripotent stem cell-derived hiHeps, enabling, for the first time, the study of the endogenous mutant proteins, patient-specific pathogenesis and drug responses for diseases where hepatocyte polarity is a key factor. Some proteins only adopt a polarized distribution in hepatocytes under specific circumstances.For example, the copper transporter protein ATP7B normally resides in the trans-Golgi network (TGN) but when intracellular copper levels become too high ATP7B displays a polarized translocation to the bile canalicular domain where excess copper is excreted.3,4Mutations in the ATP7B gene in patients with Wilson disease (WD)5 lead to excess copper deposition and damage to the liver and brain.6Some mutations have been reported to impair the intracellular trafficking of the mutant ATP7B protein.7,8However, the impact of several mutations, including the frequent H1069Q mutation in the Caucasian WD population,9 on the polarized trafficking of the mutant ATP7B to the bile canalicular domain in response to excess copper is unknown.Similarly, the impact of mutations in the regulatory gene AP1S1 on the trafficking of ATP7B in hepatocytes of patients with MEDNIK syndrome, a disease with parallels to WD,10 has been postulated but not experimentally addressed.
Hepatocyte polarity is essential for the development of bile canaliculi and for safely transporting bile and waste products from the liver. Functional studies of autologous mutated proteins in the context of the polarized hepatocyte have been challenging because of the lack of appropriate cell models. The aims of this study were to obtain a patient-specific hepatocyte model that recapitulated hepatocyte polarity and to employ this model to study endogenous mutant proteins in liver diseases that involve hepatocyte polarity. Urine cell-derived pluripotent stem cells, taken from a patient with a homozygous mutation in ATP7B and a patient with a heterozygous mutation, were differentiated towards hepatocyte-like cells (hiHeps). HiHeps were also derived from a patient with MEDNIK syndrome. Polarized hiHeps that formed in vivo-like bile canaliculi could be generated from embryonic and patient urine cell-derived pluripotent stem cells. HiHeps recapitulated polarized protein trafficking processes, exemplified by the Cu2+-induced redistribution of the copper transporter protein ATP7B to the bile canalicular domain. We demonstrated that, in contrast to the current dogma, the most frequent yet enigmatic Wilson disease-causing ATP7B-H1069Q mutation per se did not preclude trafficking of ATP7B to the trans-Golgi Network. Instead, it prevented its Cu2+-induced polarized redistribution to the bile canalicular domain, which could not be reversed by pharmacological folding chaperones. Finally, we demonstrate that hiHeps from a patient with MEDNIK syndrome, suffering from liver copper overload of unclear etiology, showed no defect in the Cu2+-induced redistribution of ATP7B to the bile canaliculi. Functional cell polarity can be achieved in patient pluripotent stem cell-derived hiHeps, enabling, for the first time, the study of the endogenous mutant proteins, patient-specific pathogenesis and drug responses for diseases where hepatocyte polarity is a key factor. Patient biopsies provide information about the steady-state distribution of mutant proteins, but not about the regulated dynamics of mutant proteins, such as the polarized trafficking of ATP7B mutants in copper-exposed cells.Studies of mutant proteins including WD ATP7B mutants required their heterologous overexpression11–14 in cancer cell lines, a strategy with several drawbacks.15Therefore, there is an urgent need for a cell culture system that recapitulates the trafficking of endogenously expressed mutant proteins in polarized human hepatocytes.A patient’s own cells are the ideal source to study endogenous mutant proteins.The ability to generate induced pluripotent stem cells (iPSCs) from a patient’s cells and to differentiate these to hepatocytes has been a major step towards the study of endogenous mutant proteins, including ATP7B, in physiologically relevant cell types.16–24However, the potential of iPSC-derived hepatocytes to polarize and form bile canaliculi, and their potential to study the polarized trafficking of endogenously expressed mutant protein, is not known.
GS-9620, an oral agonist of toll-like receptor 7, is in clinical development for the treatment of chronic hepatitis B (CHB). GS-9620 was previously shown to induce prolonged suppression of serum viral DNA and antigens in the chimpanzee and woodchuck models of CHB. Herein, we investigated the immunomodulatory mechanisms underlying these antiviral effects. Archived liver biopsies and paired peripheral blood mononuclear cell samples from a previous chimpanzee study were analyzed by RNA sequencing, quantitative reverse transcription PCR, immunohistochemistry (IHC) and in situ hybridization (ISH). GS-9620 treatment of CHB chimpanzees induced an intrahepatic transcriptional profile significantly enriched with genes associated with hepatitis B virus (HBV) clearance in acutely infected chimpanzees. Type I and II interferon, CD8+ T cell and B cell transcriptional signatures were associated with treatment response, together with evidence of hepatocyte death and liver regeneration. IHC and ISH confirmed an increase in intrahepatic CD8+ T cell and B cell numbers during treatment, and revealed that GS-9620 transiently induced aggregates predominantly comprised of CD8+ T cells and B cells in portal regions. There were no follicular dendritic cells or IgG-positive cells in these lymphoid aggregates and very few CD11b+ myeloid cells. There was no change in intrahepatic natural killer cell number during GS-9620 treatment. The antiviral response to GS-9620 treatment in CHB chimpanzees was associated with an intrahepatic interferon response and formation of lymphoid aggregates in the liver. Our data indicate these intrahepatic structures are not fully differentiated follicles containing germinal center reactions. However, the temporal correlation between development of these T and B cell aggregates and the antiviral response to treatment suggests they play a role in promoting an effective immune response against HBV. Approximately 240 million individuals live with chronic hepatitis B (CHB), and over half a million people are estimated to die each year because of CHB-associated liver diseases such as cirrhosis and hepatocellular carcinoma.Various nucleos(t)ide inhibitors and interferon-α (IFN-α) are currently licensed for the treatment of CHB, but while these therapies reduce viremia and improve long-term outcome, they rarely lead to functional cure.The rarity of complete treatment response, together with the difficulty in obtaining liver biopsy specimens from patients with CHB, has hindered characterization of the immune determinants of viral control.Accordingly, animal models have been extensively used to identify potential mechanisms of hepatitis B virus (HBV) clearance.Most notably, characterization of acute hepadnavirus infection in woodchucks and chimpanzees, together with studies in transgenic and hydrodynamic mouse models have identified important determinants of HBV clearance.1–4However, these studies were performed in models of self-limiting natural infection and/or in models that lack viral spread and the authentic HBV genome, and therefore it remains to be determined whether the identified mechanisms can be therapeutically induced to achieve functional cure of CHB in patients.Fortunately, the development of novel therapies provides new possibilities for studying the immune determinants of viral clearance during chronic infection.
GS-9620, an oral agonist of toll-like receptor 7, is in clinical development for the treatment of chronic hepatitis B (CHB). GS-9620 was previously shown to induce prolonged suppression of serum viral DNA and antigens in the chimpanzee and woodchuck models of CHB. Herein, we investigated the immunomodulatory mechanisms underlying these antiviral effects. Archived liver biopsies and paired peripheral blood mononuclear cell samples from a previous chimpanzee study were analyzed by RNA sequencing, quantitative reverse transcription PCR, immunohistochemistry (IHC) and in situ hybridization (ISH). GS-9620 treatment of CHB chimpanzees induced an intrahepatic transcriptional profile significantly enriched with genes associated with hepatitis B virus (HBV) clearance in acutely infected chimpanzees. Type I and II interferon, CD8+ T cell and B cell transcriptional signatures were associated with treatment response, together with evidence of hepatocyte death and liver regeneration. IHC and ISH confirmed an increase in intrahepatic CD8+ T cell and B cell numbers during treatment, and revealed that GS-9620 transiently induced aggregates predominantly comprised of CD8+ T cells and B cells in portal regions. There were no follicular dendritic cells or IgG-positive cells in these lymphoid aggregates and very few CD11b+ myeloid cells. There was no change in intrahepatic natural killer cell number during GS-9620 treatment. The antiviral response to GS-9620 treatment in CHB chimpanzees was associated with an intrahepatic interferon response and formation of lymphoid aggregates in the liver. Our data indicate these intrahepatic structures are not fully differentiated follicles containing germinal center reactions. However, the temporal correlation between development of these T and B cell aggregates and the antiviral response to treatment suggests they play a role in promoting an effective immune response against HBV. GS-9620 is a potent, orally active small molecule agonist of toll-like receptor 7 (TLR7) in clinical development for the treatment of CHB.5In humans, TLR7 is expressed predominantly by plasmacytoid dendritic cells (pDCs) and B cells.Activation of pDCs by TLR7 agonists induces IFN-α and various other cytokines.IFN-α is a pleiotropic cytokine that has both direct antiviral and immunomodulatory functions, including activation of natural killer (NK) cells, B cells and cytotoxic CD8+ T cells (CTLs).6Direct activation of TLR7 in B cells can also contribute to the development of effective antiviral antibody responses.7GS-9620 was previously shown to induce prolonged suppression of serum viral DNA and antigens in the woodchuck and chimpanzee models of CHB.8,9However, these preclinical studies did not comprehensively characterize the intrahepatic cellular and molecular characteristics of the antiviral response to GS-9620.Defining the immunological basis for response is an important goal since mechanistic understanding of GS-9620 activity could drive rational design of novel immunotherapeutic strategies.
GS-9620, an oral agonist of toll-like receptor 7, is in clinical development for the treatment of chronic hepatitis B (CHB). GS-9620 was previously shown to induce prolonged suppression of serum viral DNA and antigens in the chimpanzee and woodchuck models of CHB. Herein, we investigated the immunomodulatory mechanisms underlying these antiviral effects. Archived liver biopsies and paired peripheral blood mononuclear cell samples from a previous chimpanzee study were analyzed by RNA sequencing, quantitative reverse transcription PCR, immunohistochemistry (IHC) and in situ hybridization (ISH). GS-9620 treatment of CHB chimpanzees induced an intrahepatic transcriptional profile significantly enriched with genes associated with hepatitis B virus (HBV) clearance in acutely infected chimpanzees. Type I and II interferon, CD8+ T cell and B cell transcriptional signatures were associated with treatment response, together with evidence of hepatocyte death and liver regeneration. IHC and ISH confirmed an increase in intrahepatic CD8+ T cell and B cell numbers during treatment, and revealed that GS-9620 transiently induced aggregates predominantly comprised of CD8+ T cells and B cells in portal regions. There were no follicular dendritic cells or IgG-positive cells in these lymphoid aggregates and very few CD11b+ myeloid cells. There was no change in intrahepatic natural killer cell number during GS-9620 treatment. The antiviral response to GS-9620 treatment in CHB chimpanzees was associated with an intrahepatic interferon response and formation of lymphoid aggregates in the liver. Our data indicate these intrahepatic structures are not fully differentiated follicles containing germinal center reactions. However, the temporal correlation between development of these T and B cell aggregates and the antiviral response to treatment suggests they play a role in promoting an effective immune response against HBV. The chimpanzee is the only immunocompetent model of natural HBV infection.Chimpanzees also share high genetic similarity with humans, and have a comparable immune system.In addition, the cross-reactivity of human-specific reagents with the chimpanzee permits more extensive immune characterization than is possible in the woodchuck model.This is an important distinction because a number of studies have illustrated the challenge in determining the relative contribution of intrahepatic NK and CD8+ T cells to antiviral response in the woodchuck model.8,10Comprehensive characterization of the immune determinants of antiviral response to GS-9620 treatment in CHB chimpanzees is therefore of translational value and is also practicable.
Carcinogen-induced mouse models of liver cancer are used extensively to study the pathogenesis of the disease and are critical for validating candidate therapeutics. These models can recapitulate molecular and histological features of human disease. However, it is not known if the genomic alterations driving these mouse tumour genomes are comparable to those found in human tumours. Herein, we provide a detailed genomic characterisation of tumours from a commonly used mouse model of hepatocellular carcinoma (HCC). We analysed whole exome sequences of liver tumours arising in mice exposed to diethylnitrosamine (DEN). Mutational signatures were compared between liver tumours from DEN-treated and untreated mice, and human HCCs. DEN-initiated tumours had a high, uniform number of somatic single nucleotide variants (SNVs), with few insertions, deletions or copy number alterations, consistent with the known genotoxic action of DEN. Exposure of hepatocytes to DEN left a reproducible mutational imprint in resulting tumour exomes which we could computationally reconstruct using six known COSMIC mutational signatures. The tumours carried a high diversity of low-incidence, non-synonymous point mutations in many oncogenes and tumour suppressors, reflecting the stochastic introduction of SNVs into the hepatocyte genome by the carcinogen. We identified four recurrently mutated genes that were putative oncogenic drivers of HCC in this model. Every neoplasm carried activating hotspot mutations either in codon 61 of Hras, in codon 584 of Braf or in codon 254 of Egfr. Truncating mutations of Apc occurred in 21% of neoplasms, which were exclusively carcinomas supporting a role for deregulation of Wnt/β-catenin signalling in cancer progression. Our study provides detailed insight into the mutational landscape of tumours arising in a commonly used carcinogen model of HCC, facilitating the future use of this model to better understand the human disease. Hepatocellular carcinoma (HCC) is the predominant form of primary liver cancer, which is currently the sixth most frequently diagnosed human cancer.Liver cancer is the second most common cause of cancer death globally and its incidence is increasing in countries with historically low rates.1,2HCC typically develops in the context of end-stage liver disease, resulting from chronic inflammation, fibrosis and cirrhosis, and is almost exclusively caused by environmental risk factors, such as chronic hepatitis virus infection, aflatoxin B exposure, chronic alcohol consumption, and metabolic syndrome.3This diversity of aetiologies appears to be reflected in the molecular heterogeneity of the disease.Over the last few years, next generation sequencing analyses of hundreds of human liver tumours have identified several oncogenic pathways and a wide range of putative driver gene mutations underlying hepatocarcinogenesis.4–9
Carcinogen-induced mouse models of liver cancer are used extensively to study the pathogenesis of the disease and are critical for validating candidate therapeutics. These models can recapitulate molecular and histological features of human disease. However, it is not known if the genomic alterations driving these mouse tumour genomes are comparable to those found in human tumours. Herein, we provide a detailed genomic characterisation of tumours from a commonly used mouse model of hepatocellular carcinoma (HCC). We analysed whole exome sequences of liver tumours arising in mice exposed to diethylnitrosamine (DEN). Mutational signatures were compared between liver tumours from DEN-treated and untreated mice, and human HCCs. DEN-initiated tumours had a high, uniform number of somatic single nucleotide variants (SNVs), with few insertions, deletions or copy number alterations, consistent with the known genotoxic action of DEN. Exposure of hepatocytes to DEN left a reproducible mutational imprint in resulting tumour exomes which we could computationally reconstruct using six known COSMIC mutational signatures. The tumours carried a high diversity of low-incidence, non-synonymous point mutations in many oncogenes and tumour suppressors, reflecting the stochastic introduction of SNVs into the hepatocyte genome by the carcinogen. We identified four recurrently mutated genes that were putative oncogenic drivers of HCC in this model. Every neoplasm carried activating hotspot mutations either in codon 61 of Hras, in codon 584 of Braf or in codon 254 of Egfr. Truncating mutations of Apc occurred in 21% of neoplasms, which were exclusively carcinomas supporting a role for deregulation of Wnt/β-catenin signalling in cancer progression. Our study provides detailed insight into the mutational landscape of tumours arising in a commonly used carcinogen model of HCC, facilitating the future use of this model to better understand the human disease. There are an increasing number of experimental mouse models used in HCC research to study the disease pathogenesis and to assess novel therapeutics.10For several decades, carcinogen-induced tumours have been used in preclinical research, and the most widely used chemical to induce liver cancer in mice is diethylnitrosamine (DEN).When injected into juvenile mice, DEN targets the liver where it is metabolically activated by centrilobular hepatocytes into alkylating agents that can form mutagenic DNA adducts.11The introduction of oncogenic mutations into hepatocytes that are actively proliferating during normal post-natal development can then result in dysplastic lesions which progress to carcinoma.Mouse tumours induced by DEN alone frequently harbour initiating activating mutations in either Hras or Braf proto-oncogenes.12,13In a related model in which tumours are induced using DEN as an initiator followed by phenobarbital as a tumour promoter, chromosomal instability and activating mutations in β-catenin have been implicated in tumour progression.14There is also evidence that inflammation is a contributing factor to DEN-induced hepatocarcinogenesis.As well as acting as a genotoxin, DEN is also hepatotoxic causing necrotic cell death.This damage triggers an inflammatory response resulting in elevated expression of mitogens, such as interleukin-6, which promote compensatory proliferation of surviving hepatocytes.15
In recent years, circular RNAs (circRNAs) have been shown to have critical regulatory roles in cancer biology. However, the contributions of circRNAs to hepatocellular carcinoma (HCC) remain largely unknown. cSMARCA5 (a circRNA derived from exons 15 and 16 of the SMARCA5 gene, hsa_circ_0001445) was identified by RNA-sequencing and validated by quantitative reverse transcription PCR. The role of cSMARCA5 in HCC progression was assessed both in vitro and in vivo. circRNAs in vivo precipitation, luciferase reporter assay, biotin-coupled microRNA capture and fluorescence in situ hybridization were conducted to evaluate the interaction between cSMARCA5 and miR-17-3p/miR-181b-5p. The expression of cSMARCA5 was lower in HCC tissues, because of the regulation of DExH-Box Helicase 9, an abundant nuclear RNA helicase. The downregulation of cSMARCA5 in HCC was significantly correlated with aggressive characteristics and served as an independent risk factor for overall survival and recurrence-free survival in patients with HCC after hepatectomy. Our in vivo and in vitro data indicated that cSMARCA5 inhibits the proliferation and migration of HCC cells. Mechanistically, we found that cSMARCA5 could promote the expression of TIMP3, a well-known tumor suppressor, by sponging miR-17-3p and miR-181b-5p. These results reveal an important role of cSMARCA5 in the growth and metastasis of HCC and provide a fresh perspective on circRNAs in HCC progression. Hepatocellular carcinoma (HCC) is one of the most common malignancies worldwide.1Although it has long been highly prevalent in Asia and Africa, it was relatively less common in the Western world.However, over the past three decades HCC incidence has doubled in the United Kingdom and tripled in the United States.2,3Largely because of the propensity for metastasis, the five-year survival rate of patients with HCC remains poor, and approximately 600,000 patients die each year.2,4Identifying prognostic biomarkers and treatment targets for metastatic HCC is of paramount importance.
In recent years, circular RNAs (circRNAs) have been shown to have critical regulatory roles in cancer biology. However, the contributions of circRNAs to hepatocellular carcinoma (HCC) remain largely unknown. cSMARCA5 (a circRNA derived from exons 15 and 16 of the SMARCA5 gene, hsa_circ_0001445) was identified by RNA-sequencing and validated by quantitative reverse transcription PCR. The role of cSMARCA5 in HCC progression was assessed both in vitro and in vivo. circRNAs in vivo precipitation, luciferase reporter assay, biotin-coupled microRNA capture and fluorescence in situ hybridization were conducted to evaluate the interaction between cSMARCA5 and miR-17-3p/miR-181b-5p. The expression of cSMARCA5 was lower in HCC tissues, because of the regulation of DExH-Box Helicase 9, an abundant nuclear RNA helicase. The downregulation of cSMARCA5 in HCC was significantly correlated with aggressive characteristics and served as an independent risk factor for overall survival and recurrence-free survival in patients with HCC after hepatectomy. Our in vivo and in vitro data indicated that cSMARCA5 inhibits the proliferation and migration of HCC cells. Mechanistically, we found that cSMARCA5 could promote the expression of TIMP3, a well-known tumor suppressor, by sponging miR-17-3p and miR-181b-5p. These results reveal an important role of cSMARCA5 in the growth and metastasis of HCC and provide a fresh perspective on circRNAs in HCC progression. Circular RNAs (termed circRNAs) are “covalently closed, single-stranded transcripts comprising many RNA species”.5In the 1990s, a few circRNAs were identified in humans and rodents and found to be produced in the back-splicing of precursor mRNA (pre-mRNA).5–7However, these circRNAs were present at low levels and considered by-products of splicing.Recently, high-throughput sequencing and novel computational approaches have shown that the expression of circRNAs is widespread.8,9Furthermore, using biological experiments, many circRNAs have been proved to be to be highly expressed in a tissue-specific or cell type-specific manner.10,11Moreover, hundreds of circRNAs are regulated during the human epithelial–mesenchymal transition (EMT).12These findings demonstrate that the highly expressed circRNAs are not simply by-products of splicing, but suggestive of functionality.In fact, many studies have proved that circRNAs can function as sponges for microRNAs (miRNAs)13–20 or bind to proteins,21,22 and altered circRNA levels can result in aberrant expression of gene products that may contribute to cancer biology.15–18,21However, only preliminary studies on the role of circRNAs in HCC have been performed,18,23,24 and the overall pathophysiological contributions of circRNAs to HCC remain largely unknown.
Direct-acting antiviral therapies (DAA) are an important tool for hepatitis C virus (HCV) elimination. However, reinfection among people who inject drugs (PWID) may hamper elimination targets. Therefore, we estimated HCV reinfection rates among DAA-treated individuals, including PWID. We analyzed data from the British Columbia Hepatitis Testers Cohort which included ∼1.7 million individuals screened for HCV in British Columbia, Canada. We followed HCV-infected individuals treated with DAAs who achieved a sustained virologic response (SVR) and had ≥1 subsequent HCV RNA measurement to April 22nd, 2018. Reinfection was defined as a positive RNA measurement after SVR. PWID were identified using a validated algorithm and classified based on recent (<3 years) or former (≥3 years before SVR) use. Crude reinfection rates per 100 person-years (PYs) were calculated. Poisson regression was used to model adjusted incidence rate ratios (IRRs) and 95% CIs. Of 4,114 individuals who met the inclusion criteria, most were male (n = 2,692, 65%), born before 1965 (n = 3,411, 83%) and were either recent (n = 875, 21%) or former PWID (n = 1,793, 44%). Opioid-agonist therapy (OAT) was received by 19% of PWID. We identified 40 reinfections during 2,767 PYs. Reinfection rates were higher among recent (3.1/100 PYs; IRR 6.7; 95% CI 1.9–23.5) and former PWID (1.4/100 PYs; IRR 3.7; 95% CI 1.1–12.9) than non-PWID (0.3/100 PYs). Among recent PWID, reinfection rates were higher among individuals born after 1975 (10.2/100 PYs) and those co-infected with HIV (5.7/100 PYs). Only one PWID receiving daily OAT developed reinfection. Population-level reinfection rates remain elevated after DAA therapy among PWID because of ongoing exposure risk. Engagement of PWID in harm-reduction and support services is needed to prevent reinfections. Chronic hepatitis C virus (HCV) is associated with substantial morbidity and mortality, with >71 million people infected worldwide in 2015.1In 2013, an estimated 700,000 deaths globally were attributed to HCV-related liver disease sequelae, namely cirrhosis and hepatocellular carcinoma.2Mortality is expected to increase as many individuals infected with HCV decades ago are aging and at high risk of chronic liver disease.3,4With the introduction of highly effective and well-tolerated, all-oral direct-acting antiviral (DAA) therapies to treat chronic HCV infection,5,6 there has been renewed optimism regarding the ability to reduce the HCV disease burden and improve treatment outcomes in traditionally difficult to treat or cure populations.This includes those co-infected with HIV,7 as well as people who inject drugs (PWID).8
Direct-acting antiviral therapies (DAA) are an important tool for hepatitis C virus (HCV) elimination. However, reinfection among people who inject drugs (PWID) may hamper elimination targets. Therefore, we estimated HCV reinfection rates among DAA-treated individuals, including PWID. We analyzed data from the British Columbia Hepatitis Testers Cohort which included ∼1.7 million individuals screened for HCV in British Columbia, Canada. We followed HCV-infected individuals treated with DAAs who achieved a sustained virologic response (SVR) and had ≥1 subsequent HCV RNA measurement to April 22nd, 2018. Reinfection was defined as a positive RNA measurement after SVR. PWID were identified using a validated algorithm and classified based on recent (<3 years) or former (≥3 years before SVR) use. Crude reinfection rates per 100 person-years (PYs) were calculated. Poisson regression was used to model adjusted incidence rate ratios (IRRs) and 95% CIs. Of 4,114 individuals who met the inclusion criteria, most were male (n = 2,692, 65%), born before 1965 (n = 3,411, 83%) and were either recent (n = 875, 21%) or former PWID (n = 1,793, 44%). Opioid-agonist therapy (OAT) was received by 19% of PWID. We identified 40 reinfections during 2,767 PYs. Reinfection rates were higher among recent (3.1/100 PYs; IRR 6.7; 95% CI 1.9–23.5) and former PWID (1.4/100 PYs; IRR 3.7; 95% CI 1.1–12.9) than non-PWID (0.3/100 PYs). Among recent PWID, reinfection rates were higher among individuals born after 1975 (10.2/100 PYs) and those co-infected with HIV (5.7/100 PYs). Only one PWID receiving daily OAT developed reinfection. Population-level reinfection rates remain elevated after DAA therapy among PWID because of ongoing exposure risk. Engagement of PWID in harm-reduction and support services is needed to prevent reinfections. However, as HCV infection does not result in sterilizing immunity, individuals with ongoing risk activities remain vulnerable to reinfection following cure of their initial infection.9Thus, reinfection after successful HCV treatment is an important public health issue and may impact efforts to control HCV transmission.10,11PWID have been shown to be adherent to DAA treatments and can be effectively cured.12,13However, concerns about reinfection risk impact on clinical decision-making regarding treatment of PWID14,15 who remain at risk of ongoing transmission.Although, reinfection is a major concern in the DAA era, estimates of reinfection rates have been limited to the interferon era where treatment was largely restricted to motivated patients who would most likely benefit from therapy and reinfection rates were low.16–20As DAAs have mitigated concerns related to compliance and adherence among PWID with ongoing injection behaviors, it has been postulated that reinfection rates with DAAs may be higher than during the interferon era.21However, population-level data on reinfection following successful DAA treatment is scarce.11
Cirrhosis, the prevalence of which is increasing, is a risk factor for osteoporosis and fractures. However, little is known of the actual risk of hip fractures in patients with alcoholic cirrhosis. Using linked primary and secondary care data from the English and Danish nationwide registries, we quantified the hip fracture risk in two national cohorts of patients with alcoholic cirrhosis. We followed 3,706 English and 17,779 Danish patients with a diagnosis of alcoholic cirrhosis, and we identified matched controls from the general populations. We estimated hazard ratios (HR) of hip fracture for patients vs. controls, adjusted for age, sex and comorbidity. The five-year hip fracture risk was raised both in England (2.9% vs. 0.8% for controls) and Denmark (4.6% vs. 0.9% for controls). With confounder adjustment, patients with cirrhosis had fivefold (adjusted HR 5.5; 95% CI 4.3–6.9), and 8.5-fold (adjusted HR 8.5; 95% CI 7.8–9.3) increased rates of hip fracture, in England and Denmark, respectively. This association between alcoholic cirrhosis and risk of hip fracture showed significant interaction with age (p <0.001), being stronger in younger age groups (under 45 years, HR 17.9 and 16.6 for English and Danish patients, respectively) than in patients over 75 years (HR 2.1 and 2.9, respectively). In patients with alcoholic cirrhosis, 30-day mortality following a hip fracture was 11.1% in England and 10.0% in Denmark, giving age-adjusted post-fracture mortality rate ratios of 2.8 (95% CI 1.9–3.9) and 2.0 (95% CI 1.5–2.7), respectively. Patients with alcoholic cirrhosis have a markedly increased risk of hip fracture and post-hip fracture mortality compared with the general population. These findings support the need for more effort towards fracture prevention in this population, to benefit individuals and reduce the societal burden. An important and often studied complication of chronic liver disease is osteoporosis.1–5Osteoporosis may be asymptomatic, but increases the risk that minor accidents result in bone fractures, particularly of the distal radius and proximal femur.1,2Hip fractures in particular have a significant impact on health, productivity and life expectancy,6,7 and the 30-day mortality is estimated to be up to 10%.8,9Specifically in patients with alcoholic cirrhosis, the risk of these fractures may be further increased by the direct effect of high levels of alcohol use,10 or minimal hepatic encephalopathy.11However, owing to a dearth of studies that have actually quantified the incidence of fractures in chronic liver disease, the absolute risk of hip fractures in people with alcoholic cirrhosis is unknown.
Previous prognostic scores for transarterial chemoembolization (TACE) were mainly derived from real-world settings, which are beyond guideline recommendations. A robust model for outcome prediction and risk stratification of recommended TACE candidates is lacking. We aimed to develop an easy-to-use tool specifically for these patients. Between January 2010 and May 2016, 1,604 treatment-naïve patients with unresectable hepatocellular carcinoma (HCC), Child-Pugh A5-B7 and performance status 0 undergoing TACE were included from 24 tertiary centres. Patients were randomly divided into training (n = 807) and validation (n = 797) cohorts. A prognostic model was developed and subsequently validated. Predictive performance and discrimination were further evaluated and compared with other prognostic models. The final presentation of the model was “linear predictor = largest tumour diameter (cm) + tumour number”, which consistently outperformed other currently available models in both training and validation datasets as well as in different subgroups. The thirtieth percentile and the third quartile of the linear predictor, namely 6 and 12, were further selected as cut-off values, leading to the “six-and-twelve” score which could divide patients into 3 strata with the sum of tumour size and number ≤6, >6 but ≤12, and >12 presenting significantly different median survival of 49.1 (95% CI 43.7–59.4) months, 32.0 (95% CI 29.9–37.5) months, and 15.8 (95% CI 14.1–17.7) months, respectively. The six-and-twelve score may prove an easy-to-use tool to stratify recommended TACE candidates (Barcelona Clinic Liver Cancer stage-A/B) and predict individual survival with favourable performance and discrimination. Moreover, the score could stratify these patients in clinical practice as well as help design clinical trials with comparable criteria involving these patients. Further external validation of the score is required. According to the guidelines of the American Association for the Study of Liver Disease (AASLD) and the European Association for the Study of Liver (EASL), transarterial chemoembolization (TACE) is currently the only recommended treatment option for patients with intermediate stage hepatocellular carcinoma (HCC),1–3 with well-preserved liver function and performance status.4–6These patients, as well as those at early stage but considered unresectable due to tumour size, location, patient age, and suggestions of stage migration are considered “recommended” or “ideal” TACE candidates, i.e., the best target population for TACE, and have frequently been set as the target population in pivotal studies (Table S1A-1B).7–12However, this population is rather heterogeneous with a variable median overall survival of 13–43 months,13 rendering it crucial to develop a risk stratification tool.14Indeed, the necessity of stratifying risk in these patients has also been underlined by the most recent guidelines.14More importantly, a pre-procedure prognostic model providing survival estimates after TACE as a reference may enable outcome comparisons with other treatments and thus could aid clinical decision making.15
Previous prognostic scores for transarterial chemoembolization (TACE) were mainly derived from real-world settings, which are beyond guideline recommendations. A robust model for outcome prediction and risk stratification of recommended TACE candidates is lacking. We aimed to develop an easy-to-use tool specifically for these patients. Between January 2010 and May 2016, 1,604 treatment-naïve patients with unresectable hepatocellular carcinoma (HCC), Child-Pugh A5-B7 and performance status 0 undergoing TACE were included from 24 tertiary centres. Patients were randomly divided into training (n = 807) and validation (n = 797) cohorts. A prognostic model was developed and subsequently validated. Predictive performance and discrimination were further evaluated and compared with other prognostic models. The final presentation of the model was “linear predictor = largest tumour diameter (cm) + tumour number”, which consistently outperformed other currently available models in both training and validation datasets as well as in different subgroups. The thirtieth percentile and the third quartile of the linear predictor, namely 6 and 12, were further selected as cut-off values, leading to the “six-and-twelve” score which could divide patients into 3 strata with the sum of tumour size and number ≤6, >6 but ≤12, and >12 presenting significantly different median survival of 49.1 (95% CI 43.7–59.4) months, 32.0 (95% CI 29.9–37.5) months, and 15.8 (95% CI 14.1–17.7) months, respectively. The six-and-twelve score may prove an easy-to-use tool to stratify recommended TACE candidates (Barcelona Clinic Liver Cancer stage-A/B) and predict individual survival with favourable performance and discrimination. Moreover, the score could stratify these patients in clinical practice as well as help design clinical trials with comparable criteria involving these patients. Further external validation of the score is required. Nevertheless, existing models such as hepatoma arterial-embolization prognostic (HAP) score are mostly derived from patients receiving TACE in a broader setting beyond guideline recommendations rather than the best target population for TACE (Table S1C)16 and although their performance and discrimination have been validated in other settings, it remains unknown whether a consistent result could be observed, especially in these recommended TACE candidates.A model established especially for this population is therefore urgently needed and validation of previous models is crucial.
Previous prognostic scores for transarterial chemoembolization (TACE) were mainly derived from real-world settings, which are beyond guideline recommendations. A robust model for outcome prediction and risk stratification of recommended TACE candidates is lacking. We aimed to develop an easy-to-use tool specifically for these patients. Between January 2010 and May 2016, 1,604 treatment-naïve patients with unresectable hepatocellular carcinoma (HCC), Child-Pugh A5-B7 and performance status 0 undergoing TACE were included from 24 tertiary centres. Patients were randomly divided into training (n = 807) and validation (n = 797) cohorts. A prognostic model was developed and subsequently validated. Predictive performance and discrimination were further evaluated and compared with other prognostic models. The final presentation of the model was “linear predictor = largest tumour diameter (cm) + tumour number”, which consistently outperformed other currently available models in both training and validation datasets as well as in different subgroups. The thirtieth percentile and the third quartile of the linear predictor, namely 6 and 12, were further selected as cut-off values, leading to the “six-and-twelve” score which could divide patients into 3 strata with the sum of tumour size and number ≤6, >6 but ≤12, and >12 presenting significantly different median survival of 49.1 (95% CI 43.7–59.4) months, 32.0 (95% CI 29.9–37.5) months, and 15.8 (95% CI 14.1–17.7) months, respectively. The six-and-twelve score may prove an easy-to-use tool to stratify recommended TACE candidates (Barcelona Clinic Liver Cancer stage-A/B) and predict individual survival with favourable performance and discrimination. Moreover, the score could stratify these patients in clinical practice as well as help design clinical trials with comparable criteria involving these patients. Further external validation of the score is required. Fundamental and informative prognostic indicators for developing a model specific to patients with HCC include tumour burden profiles, liver function, performance status, and biomarkers such as alpha-fetoprotein (AFP).Among these parameters, tumour burden profiles seemed particularly important, not only due to their well-perceived negative correlation with survival17,18 and response rates,19,20 but also because of the disparate tumour load varying from a solitary tumour to multiple tumours with different diameters in these ideal TACE candidates,21–23 which might contribute significantly to the heterogeneity of the population.Meanwhile, liver function and performance status have also been established as significant predictors in prognostic models for patients with HCC.24
Previous prognostic scores for transarterial chemoembolization (TACE) were mainly derived from real-world settings, which are beyond guideline recommendations. A robust model for outcome prediction and risk stratification of recommended TACE candidates is lacking. We aimed to develop an easy-to-use tool specifically for these patients. Between January 2010 and May 2016, 1,604 treatment-naïve patients with unresectable hepatocellular carcinoma (HCC), Child-Pugh A5-B7 and performance status 0 undergoing TACE were included from 24 tertiary centres. Patients were randomly divided into training (n = 807) and validation (n = 797) cohorts. A prognostic model was developed and subsequently validated. Predictive performance and discrimination were further evaluated and compared with other prognostic models. The final presentation of the model was “linear predictor = largest tumour diameter (cm) + tumour number”, which consistently outperformed other currently available models in both training and validation datasets as well as in different subgroups. The thirtieth percentile and the third quartile of the linear predictor, namely 6 and 12, were further selected as cut-off values, leading to the “six-and-twelve” score which could divide patients into 3 strata with the sum of tumour size and number ≤6, >6 but ≤12, and >12 presenting significantly different median survival of 49.1 (95% CI 43.7–59.4) months, 32.0 (95% CI 29.9–37.5) months, and 15.8 (95% CI 14.1–17.7) months, respectively. The six-and-twelve score may prove an easy-to-use tool to stratify recommended TACE candidates (Barcelona Clinic Liver Cancer stage-A/B) and predict individual survival with favourable performance and discrimination. Moreover, the score could stratify these patients in clinical practice as well as help design clinical trials with comparable criteria involving these patients. Further external validation of the score is required. On the other hand, individualized prediction has been perceived as another requisite for an optimal prognostic model.Well-recognized categorized scores and models such as “four-and-seven” criteria, HAP score, and Barcelona Clinical Liver Cancer (BCLC) intermediate stage sub-classification can be useful for stratifying the level of risk but may not be sufficiently informative for individual outcome evaluation, calling for an individualized model displaying continuous data.
Unlike other hepatitis viruses that have infected primates for millions of years, hepatitis A virus (HAV) likely entered human populations only 10–12 thousand years ago after jumping from a rodent host. The phylogeny of modern hepatoviruses that infect rodents and bats suggest that multiple similar host shifts have occurred in the past. The factors determining such shifts are unknown, but the capacity to overcome innate antiviral responses in a foreign species is likely key. We assessed the capacity of diverse hepatovirus 3ABC proteases to cleave mitochondrial antiviral signaling protein (MAVS) and disrupt antiviral signaling in HEK293 and human hepatocyte-derived cell lines. We also applied maximum-likelihood and Bayesian algorithms to identify sites of diversifying selection in MAVS orthologs from 75 chiropteran, rodent and primate species. 3ABC proteases from bat, but not rodent hepatoviruses efficiently cleaved human MAVS at Glu463/Gly464, disrupting virus activation of the interferon-β promoter, whereas human HAV 3ABC cleaved at Gln427/Val428. In contrast, MAVS orthologs from rodents and bats were resistant to cleavage by 3ABC proteases of cognate hepatoviruses and in several cases human HAV. A search for diversifying selection among MAVS orthologs from all 3 orders revealed 90 of ∼540 residues to be under positive selection, including residues in chiropteran MAVS that align with the site of cleavage of human MAVS by bat 3ABC proteases. 3ABC protease cleavage of MAVS is a conserved attribute of hepatoviruses, acting broadly across different mammalian species and associated with evidence of diversifying selection at cleavage sites in rodent and bat MAVS orthologs. The capacity of hepatoviruses to disrupt MAVS-mediated innate immune responses has shaped evolution of both hepatoviruses and their hosts, and facilitates cross-species transmission of hepatitis A. Despite effective vaccines, hepatitis A virus (HAV) remains a common cause of acute viral hepatitis in many regions of the world.1The continued presence of this hepatotropic virus is dependent upon unbroken chains of fecal-oral transmission, since persistent infection is rare-to-nonexistent and chronic HAV shedders are unknown.Accordingly, HAV disappears at times from small, isolated, human populations, only to return with a vengeance when re-introduced.2,3Thus, unlike hepatitis B virus (HBV) and hepatitis C virus (HCV) that cause long-term persistant infections and co-evolved with humans and other primate species over millions of years,4–6 HAV likely became established among human populations only when groups living together became large enough to sustain chains of transmission 10–12 thousand years ago.Multiple, distantly related hepatoviruses have recently been discovered among bats, rodents and other small mammals.7,8These viruses are hepatotropic, and although distantly related phylogenetically, share antigenic determinants with human HAV.Their phylogeny provides evidence for multiple past host species shifts across different mammalian orders, and ancestral reconstructions suggest a rodent origin for human HAV.7
Recreational ketamine use has emerged as an important health and social issue worldwide. Although ketamine is associated with biliary tract damage, the clinical and radiological profiles of ketamine-related cholangiopathy have not been well described. Chinese individuals who had used ketamine recreationally at least twice per month for six months in the previous two years via a territory-wide community network of charitable organizations tackling substance abuse were recruited. Magnetic resonance cholangiography (MRC) was performed, and the findings were interpreted independently by two radiologists, with the findings analysed in association with clinical characteristics. Among the 343 ketamine users referred, 257 (74.9%) were recruited. The mean age and ketamine exposure duration were 28.7 (±5.8) and 10.5 (±3.7) years, respectively. A total of 159 (61.9%) had biliary tract anomalies on MRC, categorized as diffuse extrahepatic dilatation (n = 73), fusiform extrahepatic dilatation (n = 64), and intrahepatic ductal changes (n = 22) with no extrahepatic involvement. Serum alkaline phosphatase (ALP) level (odds ratio [OR] 1.007; 95% CI 1.002–1.102), lack of concomitant recreational drug use (OR 1.99; 95% CI 1.11–3.58), and prior emergency attendance for urinary symptoms (OR 1.95; 95% CI 1.03–3.70) had high predictive values for biliary anomalies on MRC. Among sole ketamine users, ALP level had an AUC of 0.800 in predicting biliary anomalies, with an optimal level of ≥113 U/L having a positive predictive value of 85.4%. Cholangiographic anomalies were reversible after ketamine abstinence, whereas decompensated cirrhosis and death were possible after prolonged exposure. We have identified distinctive MRC patterns in a large cohort of ketamine users. ALP level and lack of concomitant drug use predicted biliary anomalies, which were reversible after abstinence. The study findings may aid public health efforts in combating the growing epidemic of ketamine abuse. Recreational inhalation of ketamine is emerging as a major global social and health issue.1,2Although ketamine, an N-methyl-d-aspartate receptor antagonist, has medical uses in anaesthesia and chronic pain control, its highly addictive nature has led to a massive increase in recreational consumption worldwide.Because of the ease of production and low cost, the non-medical use of ketamine is increasing especially in East and South-East Asia, with its lifetime prevalence in the general population ranging from 0.3% to 2.0%,3 comprising up to 39.7% of total recreational drugs users in these regions.4The self-reported recreational use of ketamine in Western countries, including the UK, Australia, and Canada, is also increasing.5,6From 2008 to 2014, law enforcement seizures of ketamine worldwide increased by more than threefold.3
Recreational ketamine use has emerged as an important health and social issue worldwide. Although ketamine is associated with biliary tract damage, the clinical and radiological profiles of ketamine-related cholangiopathy have not been well described. Chinese individuals who had used ketamine recreationally at least twice per month for six months in the previous two years via a territory-wide community network of charitable organizations tackling substance abuse were recruited. Magnetic resonance cholangiography (MRC) was performed, and the findings were interpreted independently by two radiologists, with the findings analysed in association with clinical characteristics. Among the 343 ketamine users referred, 257 (74.9%) were recruited. The mean age and ketamine exposure duration were 28.7 (±5.8) and 10.5 (±3.7) years, respectively. A total of 159 (61.9%) had biliary tract anomalies on MRC, categorized as diffuse extrahepatic dilatation (n = 73), fusiform extrahepatic dilatation (n = 64), and intrahepatic ductal changes (n = 22) with no extrahepatic involvement. Serum alkaline phosphatase (ALP) level (odds ratio [OR] 1.007; 95% CI 1.002–1.102), lack of concomitant recreational drug use (OR 1.99; 95% CI 1.11–3.58), and prior emergency attendance for urinary symptoms (OR 1.95; 95% CI 1.03–3.70) had high predictive values for biliary anomalies on MRC. Among sole ketamine users, ALP level had an AUC of 0.800 in predicting biliary anomalies, with an optimal level of ≥113 U/L having a positive predictive value of 85.4%. Cholangiographic anomalies were reversible after ketamine abstinence, whereas decompensated cirrhosis and death were possible after prolonged exposure. We have identified distinctive MRC patterns in a large cohort of ketamine users. ALP level and lack of concomitant drug use predicted biliary anomalies, which were reversible after abstinence. The study findings may aid public health efforts in combating the growing epidemic of ketamine abuse. Long-term heavy use of ketamine is associated with different medical problems, including cognitive impairment and psychological issues.7Damage to the urological system is also well documented, with many ketamine users developing a large variety of urinary problems, ranging from lower urinary tract symptoms and bladder incontinence to hydronephrosis, renal impairment, and papillary necrosis.8,9Urinary tract damage seemed reversible in a proportion of patients who ceased ketamine use.10Long-term ketamine use is also associated with deranged liver biochemistry11,12 and biliary tract anomalies, ranging from common bile duct dilatation13 to intrahepatic beading and strictures.14The anatomical description of biliary anomalies had been limited to small case series;12,13,15,16 a detailed depiction of different cholangiopathic patterns and their correlation with clinical characteristics, as well as any potential reversibility of biliary anomalies after ketamine cessation, remains lacking.In addition, studies aimed at recruiting recreational drug users are hampered by high default rates,17 rendering the organisation of investigations and longitudinal follow-up difficult.
Ledipasvir/sofosbuvir (LDV/SOF) for 8 to 24 weeks is approved for the treatment of chronic hepatitis C virus infection (HCV). In the ION-3 study, 8 weeks of LDV/SOF was non-inferior to 12 weeks in previously untreated genotype 1 (GT1) patients without cirrhosis. According to the Summary of Product Characteristics (SmPC), 8-week treatment may be considered in naïve non-cirrhotic GT1-patients. However, there are only limited data on the effectiveness of an 8-week regimen of LDV/SOF under real-world conditions. The aim of the present study was to characterise patients receiving 8 weeks of LDV/SOF compared to those receiving 12 weeks of LDV/SOF, and to describe therapeutic outcomes in routine clinical practice. The German Hepatitis C-Registry is a large national real-world cohort that analyses effectiveness and safety of antiviral therapies in chronic HCV. This data set is based on 2,404 patients. Treatment with SOF/LDV (without RBV) for 8 or 12 weeks was initiated on or before September 30, 2015. Overall, 84.6% (2,034/2,404) of the safety population (intention-to-treat-1 [ITT1]) and 98.2% (2,029/2,066) of the per protocol (PP) population achieved sustained virological response at week 12 (SVR12). In the 8-week group, 85.1% (824/968) of ITT1 and 98.3% (821/835) of PP patients achieved SVR12, while in the 12-week group, 85.5% (1,210/1,415) of ITT1, and 98.1% (1,208/1,231) of PP patients achieved SVR12. When treated according to the SmPC, 98.7% (739/749) of the patients achieved SVR12 (PP). Relapse was observed in 9.5% (2/21) of cirrhotic patients treated for 8 weeks (PP). Under real-world conditions a high proportion of eligible patients receiving 8-week LDV/SOF treatment achieved SVR12. Relapse occurred more frequently in patients who did not meet the selection criteria according to the SmPC. The availability of all-oral direct-acting antivirals (DAAs) has led to rapid advances in the treatment of chronic hepatitis C virus (HCV) infection over recent years.Clinical trials have shown sustained virological response (SVR) rates above 90% with well-tolerated combinations of DAAs, including in patients with traditionally more difficult to treat disease, such as those with cirrhosis and HIV co-infection.Ledipasvir-sofosbuvir (LDV/SOF) is a fixed-dose combination of DAAs which inhibit HCV non-structural (NS) 5A and 5B proteins.The large-scale ION series of phase III clinical trials demonstrated SVR rates between 93–100% in treatment-naïve and treatment-experienced patients, with and without cirrhosis, with once-daily administration of LDV/SOF ± ribavirin (RBV) for 8 to 24 weeks.1–3Based on the findings of these and other studies, LDV/SOF with or without RBV is indicated in Europe for the treatment of adult patients with HCV genotype 1, 3, 4, 5 and 6 infections, with a treatment duration of 8, 12 or 24 weeks depending on HCV genotype and patient factors, including previous treatment history and the presence of cirrhosis.In the ION-3 study, treatment with LDV/SOF for 8 weeks was shown to be non-inferior to 12 weeks in previously untreated genotype 1 patients without cirrhosis, with no benefit of the addition of RBV.3Accordingly, the European Summary of Product Characteristics (SmPC) recommends that 8-week treatment with LDV/SOF can be considered in these patients.Relapse rates in ION-3 were low overall, although slightly higher in the 8-week treatment group compared with the 12-week group (5% vs. 1%, respectively).3Subsequently, a post hoc analysis of data from ION-3 determined a cut-off value for baseline HCV RNA levels of <6 million IU/ml, which may identify patients less likely to relapse with 8 weeks of therapy.While this cut-off is also listed as an indicator for 8 weeks of treatment in US prescribing information for LDV/SOF, the European SmPC does not include this as a recommendation, although European treatment guidelines suggest that this should be considered.4
Nivolumab, an immune checkpoint inhibitor, is approved in several countries to treat sorafenib-experienced patients with HCC, based on results from the CheckMate 040 study (NCT01658878). Marked differences exist in HCC clinical presentation, aetiology, treatment patterns and outcomes across regions. This analysis assessed the safety and efficacy of nivolumab in the Asian cohort of CheckMate 040. CheckMate 040 is an international, multicentre, open-label, phase I/II study of nivolumab in adults with advanced HCC, regardless of aetiology, not amenable to curative resection or local treatment and with/without previous sorafenib treatment. This analysis included all sorafenib-experienced patients in the intent-to-treat (ITT) overall population and Asian cohort. The analysis cut-off date was March 2018. There were 182 and 85 patients in the ITT population and Asian cohort, respectively. In both populations, most patients were older than 60 years, had BCLC (Barcelona Clinic Liver Cancer) Stage C disease, and had received previous systemic therapy. A higher percentage of Asian patients had HBV infections, extrahepatic metastases and prior therapies. Median follow-up was 31.6 and 31.3 months for the ITT and Asian patients, respectively. Objective response rates were 14% and 15% in the ITT population and Asian cohort, respectively. In the Asian cohort, patients with HBV, HCV or those who were uninfected had objective response rates of 13%, 14% and 21%, respectively. The median duration of response was longer in the ITT (19.4 months) vs. Asian patients (9.7 months). Median overall survival was similar between the ITT (15.1 months) and Asian patients (14.9 months), and unaffected by aetiology in Asian patients. The nivolumab safety profile was similar and manageable across both populations. Nivolumab safety and efficacy are comparable between sorafenib-experienced ITT and Asian patients. Worldwide, liver cancer is predicted to be the fourth most common cause of cancer-related mortality, accounting for an estimated 782,000 deaths in 2018, with most liver cancers HCC.1However, there are global differences in HCC incidence and trends, with Eastern and Southeast Asia having among the highest incidence of liver cancer.1This difference in HCC incidence between Asian and non-Asian regions is related to the high incidence of chronic viral hepatitis in Asia.2,3Most Asian countries have high rates of HBV infection,2 with the exception of Japan, which has the highest rate of HCV infection of all industrialised countries.4The incidence of HCC is trending downward in Asian regions, associated with improved control of HBV and HCV, whereas it is increasing in non-Asian regions, mainly related to non-viral etiologies.Genomic studies indicate differences in genetic mutations and signatures in HCC tumours with different aetiologies, suggesting that the genomic profile of HCC in Asian regions differs from that in non-Asian regions.3
Nivolumab, an immune checkpoint inhibitor, is approved in several countries to treat sorafenib-experienced patients with HCC, based on results from the CheckMate 040 study (NCT01658878). Marked differences exist in HCC clinical presentation, aetiology, treatment patterns and outcomes across regions. This analysis assessed the safety and efficacy of nivolumab in the Asian cohort of CheckMate 040. CheckMate 040 is an international, multicentre, open-label, phase I/II study of nivolumab in adults with advanced HCC, regardless of aetiology, not amenable to curative resection or local treatment and with/without previous sorafenib treatment. This analysis included all sorafenib-experienced patients in the intent-to-treat (ITT) overall population and Asian cohort. The analysis cut-off date was March 2018. There were 182 and 85 patients in the ITT population and Asian cohort, respectively. In both populations, most patients were older than 60 years, had BCLC (Barcelona Clinic Liver Cancer) Stage C disease, and had received previous systemic therapy. A higher percentage of Asian patients had HBV infections, extrahepatic metastases and prior therapies. Median follow-up was 31.6 and 31.3 months for the ITT and Asian patients, respectively. Objective response rates were 14% and 15% in the ITT population and Asian cohort, respectively. In the Asian cohort, patients with HBV, HCV or those who were uninfected had objective response rates of 13%, 14% and 21%, respectively. The median duration of response was longer in the ITT (19.4 months) vs. Asian patients (9.7 months). Median overall survival was similar between the ITT (15.1 months) and Asian patients (14.9 months), and unaffected by aetiology in Asian patients. The nivolumab safety profile was similar and manageable across both populations. Nivolumab safety and efficacy are comparable between sorafenib-experienced ITT and Asian patients. Not only is HCC a heterogeneous disease in terms of incidence, aetiology and genomic profiles, but there is also a range of therapeutic options that involve different treatment disciplines, including oncology, hepatology, surgery and interventional radiology.Consequently, substantial heterogeneity in management trends has been observed and a range of HCC management guidelines exist.5These differences can result in diverse HCC trial outcomes.3The pivotal first-line trials of sorafenib showed similar hazard ratios (HRs) for survival in the Western and Asian trials; however, the median overall survival (OS) differed, with values of 10.7 months in the West and 6.5 months in Asia Pacific.6,7Recently, the regorafenib phase III second-line trial also showed a trend toward better OS, progression-free survival (PFS), and time to progression outcomes in Asia vs. the rest of the world.8Thus, understanding the differences in drug treatment outcomes between patient populations with advanced HCC in different regions is important to optimise the treatment approach.
Nivolumab, an immune checkpoint inhibitor, is approved in several countries to treat sorafenib-experienced patients with HCC, based on results from the CheckMate 040 study (NCT01658878). Marked differences exist in HCC clinical presentation, aetiology, treatment patterns and outcomes across regions. This analysis assessed the safety and efficacy of nivolumab in the Asian cohort of CheckMate 040. CheckMate 040 is an international, multicentre, open-label, phase I/II study of nivolumab in adults with advanced HCC, regardless of aetiology, not amenable to curative resection or local treatment and with/without previous sorafenib treatment. This analysis included all sorafenib-experienced patients in the intent-to-treat (ITT) overall population and Asian cohort. The analysis cut-off date was March 2018. There were 182 and 85 patients in the ITT population and Asian cohort, respectively. In both populations, most patients were older than 60 years, had BCLC (Barcelona Clinic Liver Cancer) Stage C disease, and had received previous systemic therapy. A higher percentage of Asian patients had HBV infections, extrahepatic metastases and prior therapies. Median follow-up was 31.6 and 31.3 months for the ITT and Asian patients, respectively. Objective response rates were 14% and 15% in the ITT population and Asian cohort, respectively. In the Asian cohort, patients with HBV, HCV or those who were uninfected had objective response rates of 13%, 14% and 21%, respectively. The median duration of response was longer in the ITT (19.4 months) vs. Asian patients (9.7 months). Median overall survival was similar between the ITT (15.1 months) and Asian patients (14.9 months), and unaffected by aetiology in Asian patients. The nivolumab safety profile was similar and manageable across both populations. Nivolumab safety and efficacy are comparable between sorafenib-experienced ITT and Asian patients. In recent years, there has been interest in using immunotherapy to treat many tumour types, including HCC.Nivolumab, a fully human IgG4 antiprogrammed death-1 monoclonal antibody, inhibits immune checkpoint signalling.Nivolumab treatment improves survival compared with chemotherapy across several tumour types, including melanoma,9 non-small cell lung cancer10,11 and renal cell carcinoma.12The phase I/II CheckMate 040 study (NCT01658878; CA209-040) investigated nivolumab treatment in patients with advanced HCC.In the dose-expansion phase of this study, patients received nivolumab 3 mg/kg, and the objective response rate (ORR) was 20%, with a median duration of response (DOR) of 9.9 months and a 9-month OS rate of 74%.13The nivolumab safety profile was consistent with that observed in other tumour types and no new safety signals were observed.Nivolumab is approved in various countries, including the USA, Canada, Taiwan, Hong Kong and Australia, for patients with advanced HCC who were previously treated with sorafenib.Similarly, the phase II pembrolizumab trial also confirmed promising efficacy with good tolerability for checkpoint inhibitor therapy in patients with advanced HCC.Interestingly, the pembrolizumab trial demonstrated a trend in ORR in patients in the USA vs. those elsewhere (26% vs. 15%, respectively), again suggesting regional variation in treatment outcomes.14The pembrolizumab plus best supportive care phase III trial in patients previously treated with systemic therapy failed to meet its coprimary endpoints for PFS and OS vs. placebo plus best supportive care, but did show a clinically meaningful improvement in OS survival (HR 0.78) and an ORR of 17%.15
Non-alcoholic fatty liver disease (NAFLD) is associated with increased cardiovascular risk. Among categories of NAFLD, hepatic fibrosis is most likely to affect mortality. Myocardial function and its energy metabolism are tightly linked, which might be altered by an insulin resistant condition such as NAFLD. We investigated whether hepatic steatosis and fibrosis were associated with myocardial dysfunction relative to myocardial glucose uptake. A total of 308 patients (190 without NAFLD, 118 with NAFLD) were studied in a tertiary care hospital. Myocardial glucose uptake was evaluated at fasted state using [18F]-fluorodeoxyglucose-positron emission tomography (18FDG-PET). Hepatic steatosis and fibrosis were assessed by transient liver elastography (Fibroscan®) with controlled attenuation parameter, which quantifies hepatic fat and by surrogate indices (fatty liver index and NAFLD fibrosis score). Cardiac structure and function were examined by echocardiogram. Compared to those without NAFLD, patients with NAFLD had alterations in cardiac remodeling, manifested by increased left ventricular mass index, left ventricular end-diastolic diameter, and left atrial volume index (all p <0.05). Hepatic steatosis was significantly associated with left ventricular filling pressure (E/e’ ratio), which reflects diastolic dysfunction (p for trend <0.05). Those without NAFLD were more likely to have higher myocardial glucose uptake compared to those with NAFLD. Significant hepatic fibrosis was also correlated with diastolic dysfunction and impaired myocardial glucose uptake. Using multivariable linear regression, E/e’ ratio was independently associated with hepatic fibrosis (standardized β = 0.12 to 0.27; all p <0.05). Association between hepatic steatosis and E/e’ ratio was also significant (standardized β = 0.10 to 0.15; all p <0.05 excluding the model adjusted for adiposity). Hepatic steatosis and fibrosis are significantly associated with diastolic heart dysfunction. This association is linked with myocardial glucose uptake evaluated by 18FDG-PET. Non-alcoholic fatty liver disease (NAFLD) has become a common metabolic liver disease worldwide, with an estimated prevalence ranging from 25% to 45% in Asian as well as Western countries.1,2NAFLD is defined as accumulation of lipids, mainly triglycerides, in ≥5% of hepatocytes with no evidence of excessive alcohol consumption or other secondary causes.3The NAFLD spectrum ranges from simple steatosis, a benign disease with absence of hepatic inflammation and fibrosis, to non-alcoholic steatohepatitis (NASH), an aggressive condition that can develop into cirrhosis, hepatocellular carcinoma, and liver-related mortality.4,5A recent meta-analysis reported that in patients with NASH, 35% progressed to cirrhosis in an average of seven years, often followed by liver-related complications.6
Non-alcoholic fatty liver disease (NAFLD) is associated with increased cardiovascular risk. Among categories of NAFLD, hepatic fibrosis is most likely to affect mortality. Myocardial function and its energy metabolism are tightly linked, which might be altered by an insulin resistant condition such as NAFLD. We investigated whether hepatic steatosis and fibrosis were associated with myocardial dysfunction relative to myocardial glucose uptake. A total of 308 patients (190 without NAFLD, 118 with NAFLD) were studied in a tertiary care hospital. Myocardial glucose uptake was evaluated at fasted state using [18F]-fluorodeoxyglucose-positron emission tomography (18FDG-PET). Hepatic steatosis and fibrosis were assessed by transient liver elastography (Fibroscan®) with controlled attenuation parameter, which quantifies hepatic fat and by surrogate indices (fatty liver index and NAFLD fibrosis score). Cardiac structure and function were examined by echocardiogram. Compared to those without NAFLD, patients with NAFLD had alterations in cardiac remodeling, manifested by increased left ventricular mass index, left ventricular end-diastolic diameter, and left atrial volume index (all p <0.05). Hepatic steatosis was significantly associated with left ventricular filling pressure (E/e’ ratio), which reflects diastolic dysfunction (p for trend <0.05). Those without NAFLD were more likely to have higher myocardial glucose uptake compared to those with NAFLD. Significant hepatic fibrosis was also correlated with diastolic dysfunction and impaired myocardial glucose uptake. Using multivariable linear regression, E/e’ ratio was independently associated with hepatic fibrosis (standardized β = 0.12 to 0.27; all p <0.05). Association between hepatic steatosis and E/e’ ratio was also significant (standardized β = 0.10 to 0.15; all p <0.05 excluding the model adjusted for adiposity). Hepatic steatosis and fibrosis are significantly associated with diastolic heart dysfunction. This association is linked with myocardial glucose uptake evaluated by 18FDG-PET. Surprisingly, the most common cause of death in patients with NAFLD is cardiovascular disease, not liver-associated complications.7Individuals with NASH showed much higher risk of coronary artery disease-related mortality (12% to 16%)8,9 compared to those with NAFLD (1% to 3%),10,11 indicating a dose-dependent relationship between severity of NAFLD and risk of cardiovascular disease mortality.There are several possible mechanisms to explain cross-talk between the heart and liver.Small studies previously demonstrated that fatty liver was associated with insulin resistance in myocardium,12 altered left ventricular (LV) structure, and diastolic dysfunction.13,14Myocardium requires energy from fatty acids and glucose, which can be altered in patients with insulin resistant conditions,15 resulting in transformation of myocardial structures and function.These myocardial abnormalities may progress to diastolic heart failure, which is increasing in prevalence because of rising rates of obesity and diabetes in aging societies.16Diastolic heart failure, along with preserved ejection fraction (EF) and predominant abnormality in diastolic function,17 has a mortality risk similar to systolic heart failure.18However, there has been no study evaluating the association between NAFLD, especially NASH or hepatic fibrosis, and subclinical abnormalities in myocardial structure and function.
Hepatobiliary magnetic resonance imaging (MRI) provides additional information beyond the size and number of tumours, and may have prognostic implications. We examined whether pretransplant radiological features on MRI could be used to stratify the risk of tumour recurrence after liver transplantation (LT) for hepatocellular carcinoma (HCC). A total of 100 patients who had received a liver transplant and who had undergone preoperative gadoxetic acid-enhanced MRI, including the hepatobiliary phase (HBP), were reviewed for tumour size, number, and morphological type (e.g. nodular, nodular with perinodular extension, or confluent multinodular), satellite nodules, non-smooth tumour margins, peritumoural enhancement in arterial phase, peritumoural hypointensity on HBP, and apparent diffusion coefficients. The primary endpoint was time to recurrence. In a multivariable adjusted model, the presence of satellite nodules [hazard ratio (HR) 3.07; 95% confidence interval (CI) 1.14–8.24] and peritumoural hypointensity on HBP (HR 4.53; 95% CI 1.52–13.4) were identified as independent factors associated with tumour recurrence. Having either of these radiological findings was associated with a higher tumour recurrence rate (72.5% vs. 15.4% at three years, p <0.001). When patients were stratified according to the Milan criteria, the presence of these two high-risk radiological findings was associated with a higher tumour recurrence rate in both patients transplanted within the Milan criteria (66.7% vs. 11.6% at three years, p <0.001, n = 68) and those who were transplanted outside the Milan criteria (75.5% vs. 28.6% at three years, p <0.001, n = 32). Radiological features on preoperative hepatobiliary MRI can stratify the risk of tumour recurrence in patients who were transplanted either within or outside the Milan criteria. Therefore, hepatobiliary MRI can be a useful way to select potential candidates for LT. Liver transplantation (LT) is regarded as the best option for radical treatment in patients with very early (stage 0) or early (stage A) hepatocellular carcinoma (HCC), according to the Barcelona Clinic Liver Cancer guideline.1–3Careful patient selection is important in reducing the tumour recurrence rate and maximising the effectiveness of LT for HCC because the availability of deceased and living donor organs is limited.Thus, the Milan criteria are used as the gold standard to increase the post-transplant survival rate of patients with HCC, with a five-year survival rate of 70% after LT and a recurrence rate of <20%.1,2,4
Hepatobiliary magnetic resonance imaging (MRI) provides additional information beyond the size and number of tumours, and may have prognostic implications. We examined whether pretransplant radiological features on MRI could be used to stratify the risk of tumour recurrence after liver transplantation (LT) for hepatocellular carcinoma (HCC). A total of 100 patients who had received a liver transplant and who had undergone preoperative gadoxetic acid-enhanced MRI, including the hepatobiliary phase (HBP), were reviewed for tumour size, number, and morphological type (e.g. nodular, nodular with perinodular extension, or confluent multinodular), satellite nodules, non-smooth tumour margins, peritumoural enhancement in arterial phase, peritumoural hypointensity on HBP, and apparent diffusion coefficients. The primary endpoint was time to recurrence. In a multivariable adjusted model, the presence of satellite nodules [hazard ratio (HR) 3.07; 95% confidence interval (CI) 1.14–8.24] and peritumoural hypointensity on HBP (HR 4.53; 95% CI 1.52–13.4) were identified as independent factors associated with tumour recurrence. Having either of these radiological findings was associated with a higher tumour recurrence rate (72.5% vs. 15.4% at three years, p <0.001). When patients were stratified according to the Milan criteria, the presence of these two high-risk radiological findings was associated with a higher tumour recurrence rate in both patients transplanted within the Milan criteria (66.7% vs. 11.6% at three years, p <0.001, n = 68) and those who were transplanted outside the Milan criteria (75.5% vs. 28.6% at three years, p <0.001, n = 32). Radiological features on preoperative hepatobiliary MRI can stratify the risk of tumour recurrence in patients who were transplanted either within or outside the Milan criteria. Therefore, hepatobiliary MRI can be a useful way to select potential candidates for LT. However, as experience using LT to treat HCC has grown and recent developments in imaging techniques have enabled the identification of very small lesions (even <1 cm in diameter) that were undetectable a decade ago,5 physicians have raised concerns that the Milan criteria are too restrictive and fail to satisfy the increasing candidate list.6,7Several expansions of the Milan criteria, including larger and more numerous tumours, have been proposed and claim comparable post-transplant survival rates.6–9
Hepatobiliary magnetic resonance imaging (MRI) provides additional information beyond the size and number of tumours, and may have prognostic implications. We examined whether pretransplant radiological features on MRI could be used to stratify the risk of tumour recurrence after liver transplantation (LT) for hepatocellular carcinoma (HCC). A total of 100 patients who had received a liver transplant and who had undergone preoperative gadoxetic acid-enhanced MRI, including the hepatobiliary phase (HBP), were reviewed for tumour size, number, and morphological type (e.g. nodular, nodular with perinodular extension, or confluent multinodular), satellite nodules, non-smooth tumour margins, peritumoural enhancement in arterial phase, peritumoural hypointensity on HBP, and apparent diffusion coefficients. The primary endpoint was time to recurrence. In a multivariable adjusted model, the presence of satellite nodules [hazard ratio (HR) 3.07; 95% confidence interval (CI) 1.14–8.24] and peritumoural hypointensity on HBP (HR 4.53; 95% CI 1.52–13.4) were identified as independent factors associated with tumour recurrence. Having either of these radiological findings was associated with a higher tumour recurrence rate (72.5% vs. 15.4% at three years, p <0.001). When patients were stratified according to the Milan criteria, the presence of these two high-risk radiological findings was associated with a higher tumour recurrence rate in both patients transplanted within the Milan criteria (66.7% vs. 11.6% at three years, p <0.001, n = 68) and those who were transplanted outside the Milan criteria (75.5% vs. 28.6% at three years, p <0.001, n = 32). Radiological features on preoperative hepatobiliary MRI can stratify the risk of tumour recurrence in patients who were transplanted either within or outside the Milan criteria. Therefore, hepatobiliary MRI can be a useful way to select potential candidates for LT. Outside the size-and-number category, biological profiles reflecting tumour behaviour were recently incorporated into selection criteria for waiting lists.10Tumour markers, such as alpha-fetoprotein (AFP) and protein induced by vitamin K absence or antagonist-II (PIVKA-II), are widely used in biological profiles, and pathological and genomic markers that suggest the biological behaviour of HCC could also be included.Microvascular invasion (MVI) can predict patient outcomes after hepatectomy and LT7,11–13 and is associated with low survival rates in patients with HCC after LT.14–16 According to a recent international consensus meeting report, an MVI assessment should be used to determine prognosis after LT for HCC.4However, MVI assessment also has its limitations, because it cannot be readily evaluated preoperatively.
MicroRNAs (MiRNAs) derived from parasites, and even from plants, have been detected in body fluids and are known to modulate host genes. In this study, we aimed to investigate if the schistosome miRNAs are involved in the occurrence and progression of hepatic fibrosis during Schistosoma japonicum (S. japonicum) infection. The presence of miRNAs from S. japonicum (sja-miRNAs) in hepatic stellate cells (HSCs) was detected by RNA sequencing. sja-miRNAs were screened by transfecting HSCs with sja-miRNA mimics. The role of sja-miR-2162 in hepatic fibrosis was evaluated by either elevating its expression in naïve mice or by inhibiting its activity in infected mice, through administration of recombinant adeno-associated virus serotype 8 vectors expressing sja-miR-2162 or miRNA sponges, respectively. We identified a miRNA of S. japonicum, sja-miR-2162, that was consistently present in the HSCs of infected mice. Transfection of sja-miR-2162 mimics led to activation of HSC cells in vitro, characterized by elevation of collagens and α-SMA. The rAAV8-mediated delivery of sja-miR-2162 to naïve mice induced hepatic fibrosis, while sustained inhibition of sja-miR-2162 in infected mice attenuated hepatic fibrosis. The transforming growth factor beta receptor III (TGFBR3), a negative regulator of TGF-β signaling, was a direct target of sja-miR-2162 in HSCs. This study demonstrated that pathogen-derived miRNAs directly promote hepatic fibrogenesis in a cross-species manner, and their efficient and sustained inhibition might present a promising therapeutic intervention for infectious diseases. Schistosomiasis is one of the most prevalent, but unfortunately neglected, tropical infectious diseases, affecting more than 240 million people across 78 countries.1The 2 most important species that cause liver disease in humans are Schistosoma mansoni and Schistosoma japonicum (S. japonicum).Female schistosoma living in the mesenteric veins of hosts lay numerous eggs, many of which are trapped in the liver via the portal venous system.Highly immunogenic substances released by parasite eggs induce a granulomatous and fibrotic response, which is characterized by T helper-2 cytokines (interleukin [IL]-4 and IL-13), eosinophils, and alternatively activated macrophages.2Hepatic fibrosis is the primary cause of morbidity and mortality associated with schistosomiasis.It is a very complicated process that involves many host-derived mediators, such as cytokines, chemokines, growth factors, and microRNAs.3These host-derived mediators collectively contribute to promote the trans-differentiation of hepatic stellate cells (HSCs) into activated myofibroblasts, which are responsible for production of collagen and fibrogenesis within the granuloma site.4It would be interesting to explore parasite-derived factors that are associated with the pathogenesis and progression of this disease, in order to identify potential therapeutic targets.
MicroRNAs (MiRNAs) derived from parasites, and even from plants, have been detected in body fluids and are known to modulate host genes. In this study, we aimed to investigate if the schistosome miRNAs are involved in the occurrence and progression of hepatic fibrosis during Schistosoma japonicum (S. japonicum) infection. The presence of miRNAs from S. japonicum (sja-miRNAs) in hepatic stellate cells (HSCs) was detected by RNA sequencing. sja-miRNAs were screened by transfecting HSCs with sja-miRNA mimics. The role of sja-miR-2162 in hepatic fibrosis was evaluated by either elevating its expression in naïve mice or by inhibiting its activity in infected mice, through administration of recombinant adeno-associated virus serotype 8 vectors expressing sja-miR-2162 or miRNA sponges, respectively. We identified a miRNA of S. japonicum, sja-miR-2162, that was consistently present in the HSCs of infected mice. Transfection of sja-miR-2162 mimics led to activation of HSC cells in vitro, characterized by elevation of collagens and α-SMA. The rAAV8-mediated delivery of sja-miR-2162 to naïve mice induced hepatic fibrosis, while sustained inhibition of sja-miR-2162 in infected mice attenuated hepatic fibrosis. The transforming growth factor beta receptor III (TGFBR3), a negative regulator of TGF-β signaling, was a direct target of sja-miR-2162 in HSCs. This study demonstrated that pathogen-derived miRNAs directly promote hepatic fibrogenesis in a cross-species manner, and their efficient and sustained inhibition might present a promising therapeutic intervention for infectious diseases. MicroRNAs (MiRNAs) are a highly conserved group of small, non-coding RNA molecules found in animals, pathogens and plants that play an important role in post-transcriptional gene regulation.5It has been well documented that miRNAs are critically relevant in nearly all developmental, physiological and pathological processes.6Recent findings have indicated that miRNAs secreted by cells in one area can regulate the gene expression and phenotype of distant recipient cells.7Strikingly, miRNAs derived from parasites, and even from plants have been detected in human or animal body fluids and have been found to modulate host genes.8Recent studies show that miRNAs can be secreted from pathogens and plants and transported into distant host recipient cells, where they regulate host gene expression in a cross-species manner.The identified miRNAs include miR-168a from rice, which inhibits expression of low-density lipoprotein receptor adapter protein 1 in mice;9 miRNAs contained in parasite exosomes that suppress type 2 innate immunity in mice;10 and small RNAs from a fungal pathogen that silence plant host immunity genes.11Despite these findings, there is no experimental evidence to demonstrate that miRNAs from pathogens are involved in the occurrence and progression of host disease during infection.
Hepatitis C virus (HCV)-specific CD8+ T cells are functionally impaired in chronic hepatitis C. Even though HCV can now be rapidly and sustainably cleared from chronically infected patients, the repercussions of HCV clearance on virus-specific CD8+ T cells remain elusive. Here, we aimed to investigate if HCV clearance by direct-acting antivirals (DAAs) could restore the functionality of exhausted HCV-specific CD8+ T cell responses. HCV-specific CD8+ T cells in peripheral blood were obtained from 40 patients with chronic HCV infection, during and 6 months following IFN-free DAA therapy. These cells were analyzed for comprehensive phenotypes, proliferation, cytokine production, mitochondrial fitness and response to immune-checkpoint blockade. We show that, unlike activation markers that decreased, surface expression of multiple co-regulatory receptors on exhausted HCV-specific CD8+ T cells remained unaltered after clearance of HCV. Likewise, cytokine production by HCV-specific CD8+ T cells remained impaired following HCV clearance. The proliferative capacity of HCV multimer-specific CD8+ T cells was not restored in the majority of patients. Enhanced in vitro proliferative expansion of HCV-specific CD8+ T cells during HCV clearance was more likely in women, patients with low liver stiffness and low alanine aminotransferase levels in our cohort. Interestingly, HCV-specific CD8+ T cells that did not proliferate following HCV clearance could preferentially re-invigorate their proliferative capacity upon in vitro immune-checkpoint inhibition. Moreover, altered mitochondrial dysfunction exhibited by exhausted HCV-specific CD8+ T cells could not be normalized after HCV clearance. Taken together, our data implies that exhausted HCV-specific CD8+ T cells remain functionally and metabolically impaired at multiple levels following HCV clearance in most patients with chronic hepatitis C. Our results might have implications in cases of re-infection with HCV and for HCV vaccine development. Hepatitis C virus (HCV) persists and leads to chronicity in the majority of infected patients.Chronic hepatitis C is mainly characterized by functional impairment of virus-specific CD8+ T cells.1,2Several mechanisms have been suggested to lead to functional impairment of HCV-specific CD8+ T cells.However, the phenomena that have gained most attention for their contribution to failure of virus-specific CD8+ T cell responses are viral escape variants and CD8+ T cell exhaustion.3–5Persistent antigen stimulation during chronic HCV infection is suggested to lead to exhaustion of virus-specific CD8+ T cells.Exhausted virus-specific CD8+ T cells in-turn are characterized by the expression of multiple co-regulatory molecules,3 limited proliferative capacity,6 impaired cytokine production3,6 as well as impaired metabolism.7–9
Hepatitis C virus (HCV)-specific CD8+ T cells are functionally impaired in chronic hepatitis C. Even though HCV can now be rapidly and sustainably cleared from chronically infected patients, the repercussions of HCV clearance on virus-specific CD8+ T cells remain elusive. Here, we aimed to investigate if HCV clearance by direct-acting antivirals (DAAs) could restore the functionality of exhausted HCV-specific CD8+ T cell responses. HCV-specific CD8+ T cells in peripheral blood were obtained from 40 patients with chronic HCV infection, during and 6 months following IFN-free DAA therapy. These cells were analyzed for comprehensive phenotypes, proliferation, cytokine production, mitochondrial fitness and response to immune-checkpoint blockade. We show that, unlike activation markers that decreased, surface expression of multiple co-regulatory receptors on exhausted HCV-specific CD8+ T cells remained unaltered after clearance of HCV. Likewise, cytokine production by HCV-specific CD8+ T cells remained impaired following HCV clearance. The proliferative capacity of HCV multimer-specific CD8+ T cells was not restored in the majority of patients. Enhanced in vitro proliferative expansion of HCV-specific CD8+ T cells during HCV clearance was more likely in women, patients with low liver stiffness and low alanine aminotransferase levels in our cohort. Interestingly, HCV-specific CD8+ T cells that did not proliferate following HCV clearance could preferentially re-invigorate their proliferative capacity upon in vitro immune-checkpoint inhibition. Moreover, altered mitochondrial dysfunction exhibited by exhausted HCV-specific CD8+ T cells could not be normalized after HCV clearance. Taken together, our data implies that exhausted HCV-specific CD8+ T cells remain functionally and metabolically impaired at multiple levels following HCV clearance in most patients with chronic hepatitis C. Our results might have implications in cases of re-infection with HCV and for HCV vaccine development. HCV therapy has changed fundamentally since 2013.Interferon (IFN)-free direct-acting antiviral (DAA) therapies resulted in rapid clearance of HCV from infected patients.DAA-mediated rapid HCV clearance is also sustained after the end of treatment; a phenomenon termed sustained virologic response (SVR).Importantly, more than 95% SVR rates can now be achieved with IFN-free DAAs, making these therapies a treatment revolution relative to previous IFN-based treatment modalities.10,11This makes HCV a unique model to study the effects of clearance of a persistent viral infection on immune responses.
Hepatitis C virus (HCV)-specific CD8+ T cells are functionally impaired in chronic hepatitis C. Even though HCV can now be rapidly and sustainably cleared from chronically infected patients, the repercussions of HCV clearance on virus-specific CD8+ T cells remain elusive. Here, we aimed to investigate if HCV clearance by direct-acting antivirals (DAAs) could restore the functionality of exhausted HCV-specific CD8+ T cell responses. HCV-specific CD8+ T cells in peripheral blood were obtained from 40 patients with chronic HCV infection, during and 6 months following IFN-free DAA therapy. These cells were analyzed for comprehensive phenotypes, proliferation, cytokine production, mitochondrial fitness and response to immune-checkpoint blockade. We show that, unlike activation markers that decreased, surface expression of multiple co-regulatory receptors on exhausted HCV-specific CD8+ T cells remained unaltered after clearance of HCV. Likewise, cytokine production by HCV-specific CD8+ T cells remained impaired following HCV clearance. The proliferative capacity of HCV multimer-specific CD8+ T cells was not restored in the majority of patients. Enhanced in vitro proliferative expansion of HCV-specific CD8+ T cells during HCV clearance was more likely in women, patients with low liver stiffness and low alanine aminotransferase levels in our cohort. Interestingly, HCV-specific CD8+ T cells that did not proliferate following HCV clearance could preferentially re-invigorate their proliferative capacity upon in vitro immune-checkpoint inhibition. Moreover, altered mitochondrial dysfunction exhibited by exhausted HCV-specific CD8+ T cells could not be normalized after HCV clearance. Taken together, our data implies that exhausted HCV-specific CD8+ T cells remain functionally and metabolically impaired at multiple levels following HCV clearance in most patients with chronic hepatitis C. Our results might have implications in cases of re-infection with HCV and for HCV vaccine development. Several groups recently characterized the kinetics of soluble inflammatory mediators, mucosal-associated invariant T cells, gamma delta T cells and natural killer cell diversity during IFN-free DAA therapy.12–20These studies revealed partially conflicting data, with some studies showing declines in the activation of immune cells and functional improvements, while others suggested long-term imprints of HCV on immune cells that remain largely unchanged by HCV clearance.Following a first report of improvement of HCV-specific CD8+ T cell proliferative responses after HCV clearance,21 subsequent studies revealed maintenance of HCV-specific CD8+ T cells with memory potential during DAA therapy.22Nevertheless, a detailed understanding of HCV-specific CD8+ T cell functions, metabolism, regulations and associations with underlying clinical patterns during DAA-mediated HCV clearance is still lacking.
Plectin, a highly versatile cytolinker protein, controls intermediate filament cytoarchitecture and cellular stress response. In the present study, we investigate the role of plectin in the liver under basal conditions and in experimental cholestasis. We generated liver-specific plectin knockout (PleΔalb) mice and analyzed them using two cholestatic liver injury models: bile duct ligation (BDL) and 3,5-diethoxycarbonyl-1,4-dihydrocollidine (DDC) feeding. Primary hepatocytes and a cholangiocyte cell line were used to address the impact of plectin on keratin filament organization and stability in vitro. Plectin deficiency in hepatocytes and biliary epithelial cells led to aberrant keratin filament network organization, biliary tree malformations, and collapse of bile ducts and ductules. Further, plectin ablation significantly aggravated biliary damage upon cholestatic challenge. Coincidently, we observed a significant expansion of A6-positive progenitor cells in PleΔalb livers. After BDL, plectin-deficient bile ducts were prominently dilated with more frequent ruptures corresponding to an increased number of bile infarcts. In addition, more abundant keratin aggregates indicated less stable keratin filaments in PleΔalb hepatocytes. A transmission electron microscopy analysis revealed a compromised tight junction formation in plectin-deficient biliary epithelial cells. In addition, protein profiling showed increased expression of the adherens junction protein E-Cadherin, and inefficient upregulation of the desmosomal protein desmoplakin in response to BDL. In vitro analyses revealed a higher susceptibility of plectin-deficient keratin networks to stress-induced collapse, paralleled by elevated activation of p38 MAP kinase. Our study shows that by maintaining proper keratin network cytoarchitecture and biliary epithelial stability, plectin plays a critical role in protecting the liver from stress elicited by cholestasis. The biliary tree is a complex three-dimensional (3D) tubular network that drains the bile produced by hepatocytes to the small intestine.The bile is secreted into the bile canaliculi and flows through interconnected small and large bile ducts (BDs), which are lined with cuboidal biliary epithelial cells (BECs).Disorders affecting the function of BECs underlie diverse cholangiopathies (e.g. primary sclerosing cholangitis and primary biliary cirrhosis), often characterized by cholestatic condition.1Biliary obstructions elicit a toxic response, and increased biliary pressure leads to epithelial ruptures and leakage of bile into the parenchyma.In response to a subsequent hepatocellular injury, BECs and hepatic progenitor cells (termed the oval cells in rodents) start to proliferate in a “ductular reaction”.Their expansion gives rise to an increased number of biliary ductules.Thus, by forming a significantly denser duct meshwork around the portal vein, the biliary system adapts to effectively drain the accumulated biliary fluid.
Plectin, a highly versatile cytolinker protein, controls intermediate filament cytoarchitecture and cellular stress response. In the present study, we investigate the role of plectin in the liver under basal conditions and in experimental cholestasis. We generated liver-specific plectin knockout (PleΔalb) mice and analyzed them using two cholestatic liver injury models: bile duct ligation (BDL) and 3,5-diethoxycarbonyl-1,4-dihydrocollidine (DDC) feeding. Primary hepatocytes and a cholangiocyte cell line were used to address the impact of plectin on keratin filament organization and stability in vitro. Plectin deficiency in hepatocytes and biliary epithelial cells led to aberrant keratin filament network organization, biliary tree malformations, and collapse of bile ducts and ductules. Further, plectin ablation significantly aggravated biliary damage upon cholestatic challenge. Coincidently, we observed a significant expansion of A6-positive progenitor cells in PleΔalb livers. After BDL, plectin-deficient bile ducts were prominently dilated with more frequent ruptures corresponding to an increased number of bile infarcts. In addition, more abundant keratin aggregates indicated less stable keratin filaments in PleΔalb hepatocytes. A transmission electron microscopy analysis revealed a compromised tight junction formation in plectin-deficient biliary epithelial cells. In addition, protein profiling showed increased expression of the adherens junction protein E-Cadherin, and inefficient upregulation of the desmosomal protein desmoplakin in response to BDL. In vitro analyses revealed a higher susceptibility of plectin-deficient keratin networks to stress-induced collapse, paralleled by elevated activation of p38 MAP kinase. Our study shows that by maintaining proper keratin network cytoarchitecture and biliary epithelial stability, plectin plays a critical role in protecting the liver from stress elicited by cholestasis. A prominent group of genes that becomes upregulated in response to a cholestatic insult encodes keratins.2–4In the liver, both hepatocytes and BECs express keratins (K) 8 and 18, whereas only BECs additionally contain K7, K19 and K23.4,5Mutations in keratin genes resulting in unstable and abnormally organized keratin filaments (KFs) predispose their carriers to various liver diseases, including cholangiopathies.6,7Mouse models that either lack or mimic mutant human K8 or K18 show increased fragility of hepatocytes, increased necrosis and hemorrhage with subsequent lethality (as reviewed in8).Although little is known about biliary keratins, an attenuated ductular reaction and aggravated cholestatic injury upon targeted inactivation of K199 suggest their critical role in biliary epithelium.
Plectin, a highly versatile cytolinker protein, controls intermediate filament cytoarchitecture and cellular stress response. In the present study, we investigate the role of plectin in the liver under basal conditions and in experimental cholestasis. We generated liver-specific plectin knockout (PleΔalb) mice and analyzed them using two cholestatic liver injury models: bile duct ligation (BDL) and 3,5-diethoxycarbonyl-1,4-dihydrocollidine (DDC) feeding. Primary hepatocytes and a cholangiocyte cell line were used to address the impact of plectin on keratin filament organization and stability in vitro. Plectin deficiency in hepatocytes and biliary epithelial cells led to aberrant keratin filament network organization, biliary tree malformations, and collapse of bile ducts and ductules. Further, plectin ablation significantly aggravated biliary damage upon cholestatic challenge. Coincidently, we observed a significant expansion of A6-positive progenitor cells in PleΔalb livers. After BDL, plectin-deficient bile ducts were prominently dilated with more frequent ruptures corresponding to an increased number of bile infarcts. In addition, more abundant keratin aggregates indicated less stable keratin filaments in PleΔalb hepatocytes. A transmission electron microscopy analysis revealed a compromised tight junction formation in plectin-deficient biliary epithelial cells. In addition, protein profiling showed increased expression of the adherens junction protein E-Cadherin, and inefficient upregulation of the desmosomal protein desmoplakin in response to BDL. In vitro analyses revealed a higher susceptibility of plectin-deficient keratin networks to stress-induced collapse, paralleled by elevated activation of p38 MAP kinase. Our study shows that by maintaining proper keratin network cytoarchitecture and biliary epithelial stability, plectin plays a critical role in protecting the liver from stress elicited by cholestasis. Proper organization of intermediate filament (IF) networks is maintained by cytoskeletal linker proteins (cytolinkers) of the plakin protein family.Plectin, a prototypical ubiquitously expressed cytolinker, crosslinks IFs and anchors them at junctional complexes, including hemidesmosomes (HDs) and desmosomes (Ds) of epithelial cells.10In addition, plectin binds to actomyosin and microtubule network systems, thus affecting not only mechanical but also dynamic properties of the cytoskeleton.Multiple studies have shown that the deletion of plectin leads to aberrant KF organization11,12 manifesting as more bundled and less flexible filaments.11Without plectin, KF networks are less stable and more prone to collapse under both mechanical and non-mechanical stress conditions.11The absence of a plectin-mediated IF anchorage has been found to alter the structure and functionality of junctional complexes.12–15For instance, homeostasis and stability of HDs in keratinocytes is strictly dependent on plectin-mediated KF recruitment,15 and a reduction in the number of HDs and their dysfunction are directly linked to skin fragility.12,15Similarly, a tissue-specific plectin deletion in mouse endothelium has adverse effects on the formation of adherens and tight junctions (AJs and TJs) with consequences for endothelial barrier function.13
Sorafenib is the standard of care for advanced hepatocellular carcinoma (HCC). Combining sorafenib with another treatment, to improve overall survival (OS) within an acceptable safety profile, might be the next step forward in the management of patients with advanced HCC. We aimed to assess whether a combination of sorafenib and a statin improved survival in patients with HCC. The objective of the PRODIGE-11 trial was to compare the respective clinical outcomes of the sorafenib-pravastatin combination (arm A) versus sorafenib alone (arm B) in patients with advanced HCC. Child-Pugh A patients with advanced HCC who were naive to systemic treatment (n = 323) were randomly assigned to sorafenib-pravastatin combination (n = 162) or sorafenib alone (n = 161). The primary endpoint was OS; secondary endpoints were progression-free survival, time to tumor progression, time to treatment failure, safety, and quality of life. After randomization, 312 patients received at least 1 dose of study treatment. After a median follow-up of 35 months, 269 patients died (arm A: 135; arm B: 134) with no difference in median OS between treatments arms (10.7 months vs. 10.5 months; hazard ratio = 1.00; p = 0.975); no difference was observed in secondary survival endpoints either. In the univariate analysis, the significant prognostic factors for OS were CLIP score, performance status, and quality of life scores. The multivariate analysis showed that the only prognostic factor for OS was the CLIP score. The main toxicity was diarrhea (which was severe in 11% of patients in arm A, and 8.9% in arm B), while severe nausea-vomiting was rare, and no toxicity-related deaths were reported. Adding pravastatin to sorafenib did not improve survival in patients with advanced HCC. Hepatocellular carcinoma (HCC) incidence is still growing in Western countries.1,2Tumor development is frequently related to liver damage, typically caused by cirrhosis.While curative treatments (transplantation, resection, percutaneous destruction) can be proposed for small tumors, about two-thirds of patients are not eligible for such treatments.3
Sorafenib is the standard of care for advanced hepatocellular carcinoma (HCC). Combining sorafenib with another treatment, to improve overall survival (OS) within an acceptable safety profile, might be the next step forward in the management of patients with advanced HCC. We aimed to assess whether a combination of sorafenib and a statin improved survival in patients with HCC. The objective of the PRODIGE-11 trial was to compare the respective clinical outcomes of the sorafenib-pravastatin combination (arm A) versus sorafenib alone (arm B) in patients with advanced HCC. Child-Pugh A patients with advanced HCC who were naive to systemic treatment (n = 323) were randomly assigned to sorafenib-pravastatin combination (n = 162) or sorafenib alone (n = 161). The primary endpoint was OS; secondary endpoints were progression-free survival, time to tumor progression, time to treatment failure, safety, and quality of life. After randomization, 312 patients received at least 1 dose of study treatment. After a median follow-up of 35 months, 269 patients died (arm A: 135; arm B: 134) with no difference in median OS between treatments arms (10.7 months vs. 10.5 months; hazard ratio = 1.00; p = 0.975); no difference was observed in secondary survival endpoints either. In the univariate analysis, the significant prognostic factors for OS were CLIP score, performance status, and quality of life scores. The multivariate analysis showed that the only prognostic factor for OS was the CLIP score. The main toxicity was diarrhea (which was severe in 11% of patients in arm A, and 8.9% in arm B), while severe nausea-vomiting was rare, and no toxicity-related deaths were reported. Adding pravastatin to sorafenib did not improve survival in patients with advanced HCC. Sorafenib, a tyrosine kinase inhibitor that targets signaling pathways involved in tumor cell proliferation (Ras-Raf-MAPK) and intracellular angiogenesis (VEGF-R, PDGF-R, Ras), demonstrated its efficacy as a treatment for advanced HCC.Two major randomized phase III trials showed that sorafenib significantly improved overall survival (OS) and time to progression (TTP) compared to placebo, especially in patients with Child-Pugh A liver function.4,5Those results made sorafenib the standard of care for advanced HCC and led to its approval for this indication.However, median OS remains limited to about 11 months; therefore, the question of combining sorafenib to another treatment in order to improve OS with an acceptable safety profile is highly relevant.
Sorafenib is the standard of care for advanced hepatocellular carcinoma (HCC). Combining sorafenib with another treatment, to improve overall survival (OS) within an acceptable safety profile, might be the next step forward in the management of patients with advanced HCC. We aimed to assess whether a combination of sorafenib and a statin improved survival in patients with HCC. The objective of the PRODIGE-11 trial was to compare the respective clinical outcomes of the sorafenib-pravastatin combination (arm A) versus sorafenib alone (arm B) in patients with advanced HCC. Child-Pugh A patients with advanced HCC who were naive to systemic treatment (n = 323) were randomly assigned to sorafenib-pravastatin combination (n = 162) or sorafenib alone (n = 161). The primary endpoint was OS; secondary endpoints were progression-free survival, time to tumor progression, time to treatment failure, safety, and quality of life. After randomization, 312 patients received at least 1 dose of study treatment. After a median follow-up of 35 months, 269 patients died (arm A: 135; arm B: 134) with no difference in median OS between treatments arms (10.7 months vs. 10.5 months; hazard ratio = 1.00; p = 0.975); no difference was observed in secondary survival endpoints either. In the univariate analysis, the significant prognostic factors for OS were CLIP score, performance status, and quality of life scores. The multivariate analysis showed that the only prognostic factor for OS was the CLIP score. The main toxicity was diarrhea (which was severe in 11% of patients in arm A, and 8.9% in arm B), while severe nausea-vomiting was rare, and no toxicity-related deaths were reported. Adding pravastatin to sorafenib did not improve survival in patients with advanced HCC. Among potential substances that could be combined with sorafenib, statins are of particular interest because of their intrinsic action on HMG-CoA reductase, the concentration and the activity of which is increased in HCC cells.6Inhibition of HMG-CoA reductase leads to depletion of mevalonate and, thus, of its products, farnesyl pyrophosphate and geranylgeranyl pyrophosphate used in the cell for post-translational modifications of many regulators of proliferation.Moreover, the deregulation of cholesterol synthesis in HCC cells, particularly in mitochondria is implicated in their chemoresistance.7Indeed, statins demonstrated antitumoral properties both experimentally and clinically, especially in digestive cancers.8–10Pravastatin is one of the most widely studied statins.It has been shown that pravastatin inhibits in vitro and in vivo HCC tumor growth, and has a pro-apoptotic action on tumoral liver cell lines.11,12The antiproliferative activity is associated with a decrease in expression and activation of matrix metallopeptidase 2 and 9, which represents an original mechanism of action.13Thus, the combination of pravastatin and sorafenib appears to be promising for HCC: action on tumor cell proliferation by acting on 1 of the 2 main signaling pathways (Raf-Ras-MAPK) through 2 distinct mechanisms, and anti-invasive and anti-metastatic action of pravastatin as a complement to the anti-angiogenic action of sorafenib.
Sorafenib is the standard of care for advanced hepatocellular carcinoma (HCC). Combining sorafenib with another treatment, to improve overall survival (OS) within an acceptable safety profile, might be the next step forward in the management of patients with advanced HCC. We aimed to assess whether a combination of sorafenib and a statin improved survival in patients with HCC. The objective of the PRODIGE-11 trial was to compare the respective clinical outcomes of the sorafenib-pravastatin combination (arm A) versus sorafenib alone (arm B) in patients with advanced HCC. Child-Pugh A patients with advanced HCC who were naive to systemic treatment (n = 323) were randomly assigned to sorafenib-pravastatin combination (n = 162) or sorafenib alone (n = 161). The primary endpoint was OS; secondary endpoints were progression-free survival, time to tumor progression, time to treatment failure, safety, and quality of life. After randomization, 312 patients received at least 1 dose of study treatment. After a median follow-up of 35 months, 269 patients died (arm A: 135; arm B: 134) with no difference in median OS between treatments arms (10.7 months vs. 10.5 months; hazard ratio = 1.00; p = 0.975); no difference was observed in secondary survival endpoints either. In the univariate analysis, the significant prognostic factors for OS were CLIP score, performance status, and quality of life scores. The multivariate analysis showed that the only prognostic factor for OS was the CLIP score. The main toxicity was diarrhea (which was severe in 11% of patients in arm A, and 8.9% in arm B), while severe nausea-vomiting was rare, and no toxicity-related deaths were reported. Adding pravastatin to sorafenib did not improve survival in patients with advanced HCC. Beside biology, the potential HCC prevention effect of statin use was emphasized by a meta-analysis (adjusted odds ratio 0.63; 95% CI 0.52–0.76).14
Sorafenib is the standard of care for advanced hepatocellular carcinoma (HCC). Combining sorafenib with another treatment, to improve overall survival (OS) within an acceptable safety profile, might be the next step forward in the management of patients with advanced HCC. We aimed to assess whether a combination of sorafenib and a statin improved survival in patients with HCC. The objective of the PRODIGE-11 trial was to compare the respective clinical outcomes of the sorafenib-pravastatin combination (arm A) versus sorafenib alone (arm B) in patients with advanced HCC. Child-Pugh A patients with advanced HCC who were naive to systemic treatment (n = 323) were randomly assigned to sorafenib-pravastatin combination (n = 162) or sorafenib alone (n = 161). The primary endpoint was OS; secondary endpoints were progression-free survival, time to tumor progression, time to treatment failure, safety, and quality of life. After randomization, 312 patients received at least 1 dose of study treatment. After a median follow-up of 35 months, 269 patients died (arm A: 135; arm B: 134) with no difference in median OS between treatments arms (10.7 months vs. 10.5 months; hazard ratio = 1.00; p = 0.975); no difference was observed in secondary survival endpoints either. In the univariate analysis, the significant prognostic factors for OS were CLIP score, performance status, and quality of life scores. The multivariate analysis showed that the only prognostic factor for OS was the CLIP score. The main toxicity was diarrhea (which was severe in 11% of patients in arm A, and 8.9% in arm B), while severe nausea-vomiting was rare, and no toxicity-related deaths were reported. Adding pravastatin to sorafenib did not improve survival in patients with advanced HCC. In the field of HCC, there were only 2 clinical studies about the therapeutic interest of pravastatin.In a previous open-label trial published in 2001, 83 patients with advanced HCC, mostly Child-Pugh B, were treated by transarterial chemoembolization followed by fluoroacil for 2 months, then randomized to pravastatin or no added treatment.The median OS was significantly improved in patients with pravastatin (18 months vs. 9 months in the control arm; p = 0.006).15The second one, a randomized phase II trial conducted on 58 patients with advanced HCC, suggested that median OS was significantly longer for patients receiving pravastatin (7.2 months) than for patients receiving gemcitabine (3.5 months).16
Sorafenib is the standard of care for advanced hepatocellular carcinoma (HCC). Combining sorafenib with another treatment, to improve overall survival (OS) within an acceptable safety profile, might be the next step forward in the management of patients with advanced HCC. We aimed to assess whether a combination of sorafenib and a statin improved survival in patients with HCC. The objective of the PRODIGE-11 trial was to compare the respective clinical outcomes of the sorafenib-pravastatin combination (arm A) versus sorafenib alone (arm B) in patients with advanced HCC. Child-Pugh A patients with advanced HCC who were naive to systemic treatment (n = 323) were randomly assigned to sorafenib-pravastatin combination (n = 162) or sorafenib alone (n = 161). The primary endpoint was OS; secondary endpoints were progression-free survival, time to tumor progression, time to treatment failure, safety, and quality of life. After randomization, 312 patients received at least 1 dose of study treatment. After a median follow-up of 35 months, 269 patients died (arm A: 135; arm B: 134) with no difference in median OS between treatments arms (10.7 months vs. 10.5 months; hazard ratio = 1.00; p = 0.975); no difference was observed in secondary survival endpoints either. In the univariate analysis, the significant prognostic factors for OS were CLIP score, performance status, and quality of life scores. The multivariate analysis showed that the only prognostic factor for OS was the CLIP score. The main toxicity was diarrhea (which was severe in 11% of patients in arm A, and 8.9% in arm B), while severe nausea-vomiting was rare, and no toxicity-related deaths were reported. Adding pravastatin to sorafenib did not improve survival in patients with advanced HCC. In terms of safety, pravastatin has a good liver safety profile, including in cirrhotic patients, with 0.6% of cytolysis exceeding 2× the upper limit of normal (ULN).17,18Lastly, pravastatin metabolism is independent of CYP3A4, limiting drug interactions with sorafenib.Thus, the combination of sorafenib with pravastatin seemed promising in the treatment of HCC because of complementary mechanisms of action and limited risks of interactions.
Gasdermin D (GSDMD)-executed programmed necrosis is involved in inflammation and controls interleukin (IL)-1β release. However, the role of GSDMD in non-alcoholic steatohepatitis (NASH) remains unclear. We investigated the role of GSDMD in the pathogenesis of steatohepatitis. Human liver tissues from patients with non-alcoholic fatty liver disease (NAFLD) and control individuals were obtained to evaluate GSDMD expression. Gsdmd knockout (Gsdmd−/−) mice, obese db/db mice and their wild-type (WT) littermates were fed with methionine-choline deficient (MCD) or control diet to induce steatohepatitis. The Gsdmd−/− and WT mice were also used in a high-fat diet (HFD)-induced NAFLD model. In addition, Alb-Cre mice were administered an adeno-associated virus (AAV) vector that expressed the gasdermin-N domain (AAV9-FLEX-GSDMD-N) and were fed with either MCD or control diet for 10 days. GSDMD and its pyroptosis-inducing fragment GSDMD-N were upregulated in liver tissues of human NAFLD/NASH. Importantly, hepatic GSDMD-N protein levels were significantly higher in human NASH and correlated with the NAFLD activity score and fibrosis. GSDMD-N remained a potential biomarker for the diagnosis of NASH. MCD-fed Gsdmd−/− mice exhibit decreased severity of steatosis and inflammation compared with WT littermates. GSDMD was associated with the secretion of pro-inflammatory cytokines (IL-1β, TNF-α, and MCP-1 [CCL2]) and persistent activation of the NF-ĸB signaling pathway. Gsdmd−/− mice showed lower steatosis, mainly because of reduced expression of the lipogenic gene Srebp1c (Srebf1) and upregulated expression of lipolytic genes, including Pparα, Aco [Klk15], Lcad [Acadl], Cyp4a10 and Cyp4a14. Alb-Cre mice administered with AAV9-FLEX-GSDMD-N showed significantly aggravated steatohepatitis when fed with MCD diet. As an executor of pyroptosis, GSDMD plays a key role in the pathogenesis of steatohepatitis, by controlling cytokine secretion, NF-ĸB activation, and lipogenesis. Non-alcoholic fatty liver disease (NAFLD) represents a multi-step biological disorder in the liver, increasing the risk of cirrhosis and tumorigenesis.1,2The key aspects of steatohepatitis have been precisely mimicked by extensive basic and translation research.This has enabled the reductionist assessment of genes and dietary factors involved in the pathogenesis of NAFLD.3,4Toxic lipid accumulation in the liver acts as the primary insult which initiates and propagates damage, leading to hepatocyte injury and resultant inflammation.5,6It is important to note that inflammation in the liver is believed to be the compelling feature that transforms simple steatosis to steatohepatitis, perpetuating hepatocellular injury and subsequent cell death, and promoting liver fibrosis.7–9However, the molecular basis behind the inflammatory response leading to steatohepatitis is still largely unknown.
Sorafenib is the recommended treatment for patients with advanced hepatocellular carcinoma (HCC). We aimed to compare the efficacy and safety of a combination of sorafenib and selective internal radiation therapy (SIRT) – with yttrium-90 (90Y) resin microspheres – to sorafenib alone in patients with advanced HCC. SORAMIC is a randomised controlled trial comprising diagnostic, local ablation and palliative cohorts. Based on diagnostic study results, patients were assigned to local ablation or palliative cohorts. In the palliative cohort, patients not eligible for TACE were randomised 11:10 to SIRT plus sorafenib (SIRT + sorafenib) or sorafenib alone. The primary endpoint was overall survival (OS; Kaplan-Meier analysis) in the intention-to-treat (ITT) population. In the ITT cohort, 216 patients were randomised to SIRT + sorafenib and 208 to sorafenib alone. Median OS was 12.1 months in the SIRT + sorafenib arm, and 11.4 months in the sorafenib arm (hazard ratio [HR] 1.01; 95% CI 0.81–1.25; p = 0.9529). Median OS in the per protocol population was 14.0 months in the SIRT + sorafenib arm (n = 114), and 11.1 months in the sorafenib arm (n = 174; HR 0.86; p = 0.2515). Subgroup analyses of the per protocol population indicated a survival benefit of SIRT + sorafenib for patients without cirrhosis (HR 0.46; 0.25–0.86; p = 0.02); cirrhosis of non-alcoholic aetiology (HR 0.63; p = 0.012); or patients ≤65 years old (HR 0.65; p = 0.05). Adverse events (AEs) of Common Terminology Criteria for AE Grades 3–4 were reported in 103/159 (64.8%) patients who received SIRT + sorafenib, 106/197 (53.8%) patients who received sorafenib alone (p = 0.04), and 8/24 (33.3%) patients who only received SIRT. Addition of SIRT to sorafenib did not result in a significant improvement in OS compared with sorafenib alone. Subgroup analyses led to hypothesis-generating results that will support the design of future studies. Hepatocellular carcinoma (HCC) is the most common type of malignant primary liver tumour, accounting for 80–90% of all liver cancers.1In the USA, for example, 30,640 new liver and intrahepatic bile duct cancers are estimated to have occurred in 2013, with 21,670 associated deaths.2,3
Sorafenib is the recommended treatment for patients with advanced hepatocellular carcinoma (HCC). We aimed to compare the efficacy and safety of a combination of sorafenib and selective internal radiation therapy (SIRT) – with yttrium-90 (90Y) resin microspheres – to sorafenib alone in patients with advanced HCC. SORAMIC is a randomised controlled trial comprising diagnostic, local ablation and palliative cohorts. Based on diagnostic study results, patients were assigned to local ablation or palliative cohorts. In the palliative cohort, patients not eligible for TACE were randomised 11:10 to SIRT plus sorafenib (SIRT + sorafenib) or sorafenib alone. The primary endpoint was overall survival (OS; Kaplan-Meier analysis) in the intention-to-treat (ITT) population. In the ITT cohort, 216 patients were randomised to SIRT + sorafenib and 208 to sorafenib alone. Median OS was 12.1 months in the SIRT + sorafenib arm, and 11.4 months in the sorafenib arm (hazard ratio [HR] 1.01; 95% CI 0.81–1.25; p = 0.9529). Median OS in the per protocol population was 14.0 months in the SIRT + sorafenib arm (n = 114), and 11.1 months in the sorafenib arm (n = 174; HR 0.86; p = 0.2515). Subgroup analyses of the per protocol population indicated a survival benefit of SIRT + sorafenib for patients without cirrhosis (HR 0.46; 0.25–0.86; p = 0.02); cirrhosis of non-alcoholic aetiology (HR 0.63; p = 0.012); or patients ≤65 years old (HR 0.65; p = 0.05). Adverse events (AEs) of Common Terminology Criteria for AE Grades 3–4 were reported in 103/159 (64.8%) patients who received SIRT + sorafenib, 106/197 (53.8%) patients who received sorafenib alone (p = 0.04), and 8/24 (33.3%) patients who only received SIRT. Addition of SIRT to sorafenib did not result in a significant improvement in OS compared with sorafenib alone. Subgroup analyses led to hypothesis-generating results that will support the design of future studies. Only about 30% of patients are diagnosed early enough to benefit from potentially curative therapies, such as surgical resection, allogeneic liver transplantation or percutaneous ablation, which afford 5-year survival rates of 50–75%.4
Sorafenib is the recommended treatment for patients with advanced hepatocellular carcinoma (HCC). We aimed to compare the efficacy and safety of a combination of sorafenib and selective internal radiation therapy (SIRT) – with yttrium-90 (90Y) resin microspheres – to sorafenib alone in patients with advanced HCC. SORAMIC is a randomised controlled trial comprising diagnostic, local ablation and palliative cohorts. Based on diagnostic study results, patients were assigned to local ablation or palliative cohorts. In the palliative cohort, patients not eligible for TACE were randomised 11:10 to SIRT plus sorafenib (SIRT + sorafenib) or sorafenib alone. The primary endpoint was overall survival (OS; Kaplan-Meier analysis) in the intention-to-treat (ITT) population. In the ITT cohort, 216 patients were randomised to SIRT + sorafenib and 208 to sorafenib alone. Median OS was 12.1 months in the SIRT + sorafenib arm, and 11.4 months in the sorafenib arm (hazard ratio [HR] 1.01; 95% CI 0.81–1.25; p = 0.9529). Median OS in the per protocol population was 14.0 months in the SIRT + sorafenib arm (n = 114), and 11.1 months in the sorafenib arm (n = 174; HR 0.86; p = 0.2515). Subgroup analyses of the per protocol population indicated a survival benefit of SIRT + sorafenib for patients without cirrhosis (HR 0.46; 0.25–0.86; p = 0.02); cirrhosis of non-alcoholic aetiology (HR 0.63; p = 0.012); or patients ≤65 years old (HR 0.65; p = 0.05). Adverse events (AEs) of Common Terminology Criteria for AE Grades 3–4 were reported in 103/159 (64.8%) patients who received SIRT + sorafenib, 106/197 (53.8%) patients who received sorafenib alone (p = 0.04), and 8/24 (33.3%) patients who only received SIRT. Addition of SIRT to sorafenib did not result in a significant improvement in OS compared with sorafenib alone. Subgroup analyses led to hypothesis-generating results that will support the design of future studies. For patients with inoperable (liver-confined) intermediate stage HCC, locoregional treatment by transarterial chemoembolisation (TACE) is the recommended treatment of choice in treatment guidelines.5–10However, this recommendation is based on 2 randomised trials, with strict patient selection criteria, and the survival benefits were limited to patients with preserved liver function and limited tumour size and numbers.6,7Systemic therapy with sorafenib (Nexavar®; Bayer Healthcare, Leverkusen, Germany) has been shown to provide a survival benefit and is standard of care for patients with HCC with preserved liver function in advanced disease stages; including those with portal vein invasion, lymph node or distant metastases, or altered performance status.5,11,12
Sorafenib is the recommended treatment for patients with advanced hepatocellular carcinoma (HCC). We aimed to compare the efficacy and safety of a combination of sorafenib and selective internal radiation therapy (SIRT) – with yttrium-90 (90Y) resin microspheres – to sorafenib alone in patients with advanced HCC. SORAMIC is a randomised controlled trial comprising diagnostic, local ablation and palliative cohorts. Based on diagnostic study results, patients were assigned to local ablation or palliative cohorts. In the palliative cohort, patients not eligible for TACE were randomised 11:10 to SIRT plus sorafenib (SIRT + sorafenib) or sorafenib alone. The primary endpoint was overall survival (OS; Kaplan-Meier analysis) in the intention-to-treat (ITT) population. In the ITT cohort, 216 patients were randomised to SIRT + sorafenib and 208 to sorafenib alone. Median OS was 12.1 months in the SIRT + sorafenib arm, and 11.4 months in the sorafenib arm (hazard ratio [HR] 1.01; 95% CI 0.81–1.25; p = 0.9529). Median OS in the per protocol population was 14.0 months in the SIRT + sorafenib arm (n = 114), and 11.1 months in the sorafenib arm (n = 174; HR 0.86; p = 0.2515). Subgroup analyses of the per protocol population indicated a survival benefit of SIRT + sorafenib for patients without cirrhosis (HR 0.46; 0.25–0.86; p = 0.02); cirrhosis of non-alcoholic aetiology (HR 0.63; p = 0.012); or patients ≤65 years old (HR 0.65; p = 0.05). Adverse events (AEs) of Common Terminology Criteria for AE Grades 3–4 were reported in 103/159 (64.8%) patients who received SIRT + sorafenib, 106/197 (53.8%) patients who received sorafenib alone (p = 0.04), and 8/24 (33.3%) patients who only received SIRT. Addition of SIRT to sorafenib did not result in a significant improvement in OS compared with sorafenib alone. Subgroup analyses led to hypothesis-generating results that will support the design of future studies. Selective internal radiation therapy (SIRT; also known as radioembolisation) has been evaluated in a number of non-randomised trials with both yttrium-90 (90Y) resin microspheres (SIR-Spheres®, Sirtex Medical Ltd, Sydney, Australia) and 90Y glass microspheres (TheraSphere, BTG, London, UK) and has been shown to be effective and well tolerated in patients with unresectable HCC.13,14SIRT appears to have similar efficacy to TACE for the cohort of ideal candidates for locoregional therapy, and encouraging results have been reported for patients who are poor candidates for TACE or have failed TACE.13–17In 2 randomised trials, SARAH and SIRveNIB in patients with locally advanced HCC, SIRT with 90Y-resin microspheres failed to meet the primary endpoint of improving survival over sorafenib.Tolerability seemed to be favourable for radioembolisation.18,19
Sorafenib is the recommended treatment for patients with advanced hepatocellular carcinoma (HCC). We aimed to compare the efficacy and safety of a combination of sorafenib and selective internal radiation therapy (SIRT) – with yttrium-90 (90Y) resin microspheres – to sorafenib alone in patients with advanced HCC. SORAMIC is a randomised controlled trial comprising diagnostic, local ablation and palliative cohorts. Based on diagnostic study results, patients were assigned to local ablation or palliative cohorts. In the palliative cohort, patients not eligible for TACE were randomised 11:10 to SIRT plus sorafenib (SIRT + sorafenib) or sorafenib alone. The primary endpoint was overall survival (OS; Kaplan-Meier analysis) in the intention-to-treat (ITT) population. In the ITT cohort, 216 patients were randomised to SIRT + sorafenib and 208 to sorafenib alone. Median OS was 12.1 months in the SIRT + sorafenib arm, and 11.4 months in the sorafenib arm (hazard ratio [HR] 1.01; 95% CI 0.81–1.25; p = 0.9529). Median OS in the per protocol population was 14.0 months in the SIRT + sorafenib arm (n = 114), and 11.1 months in the sorafenib arm (n = 174; HR 0.86; p = 0.2515). Subgroup analyses of the per protocol population indicated a survival benefit of SIRT + sorafenib for patients without cirrhosis (HR 0.46; 0.25–0.86; p = 0.02); cirrhosis of non-alcoholic aetiology (HR 0.63; p = 0.012); or patients ≤65 years old (HR 0.65; p = 0.05). Adverse events (AEs) of Common Terminology Criteria for AE Grades 3–4 were reported in 103/159 (64.8%) patients who received SIRT + sorafenib, 106/197 (53.8%) patients who received sorafenib alone (p = 0.04), and 8/24 (33.3%) patients who only received SIRT. Addition of SIRT to sorafenib did not result in a significant improvement in OS compared with sorafenib alone. Subgroup analyses led to hypothesis-generating results that will support the design of future studies. SORAfenib in combination with local MICro-therapy guided by gadolinium-EOB-DTPA-enhanced MRI (SORAMIC) (EudraCT 2009-012576-27, NCT0112 6645) is a prospective study that comprised 3 sub-studies: (i) comparison of gadolinium-ethoxybenzyl-diethylenetriamine pentaacetic acid (gadoxetate disodium [Gd-EOB-DTPA] Primovist®)-enhanced magnetic resonance imaging (MRI) vs. contrast-enhanced multislice computed tomography (CT) for the stratification of patients to a local ablation (curative treatment) or palliative treatment group; (ii) comparison of radiofrequency ablation (RFA) plus sorafenib vs. control (RFA plus matching placebo) on time to recurrence; and (iii) comparison of SIRT with 90Y resin microspheres combined with sorafenib compared with control (sorafenib alone) on overall survival (OS).
Chronic liver diseases are characterized by expansion of the small immature cholangiocytes – a mechanism named ductular reaction (DR) – which have the capacity to differentiate into hepatocytes. We investigated the kinetics of this differentiation, as well as analyzing several important features of the newly formed hepatocytes, such as functional maturity, clonal expansion and resistance to stress in mice with long-term liver damage. We tracked cholangiocytes using osteopontin-iCreERT2 and hepatocytes with AAV8-TBG-Cre. Mice received carbon tetrachloride (CCl4) for >24 weeks to induce chronic liver injury. Livers were collected for the analysis of reporter proteins, cell proliferation and death, DNA damage, and nuclear ploidy; hepatocytes were also isolated for RNA sequencing. During liver injury we observed a transient DR and the differentiation of DR cells into hepatocytes as clones that expanded to occupy 12% of the liver parenchyma by week 8. By lineage tracing, we confirmed that these new hepatocytes derived from cholangiocytes but not from native hepatocytes. They had all the features of mature functional hepatocytes. In contrast to the exhausted native hepatocytes, these newly formed hepatocytes had higher proliferative capability, less apoptosis, a lower proportion of highly polyploid nuclei and were better at eliminating DNA damage. In chronic liver injury, DR cells differentiate into stress-resistant hepatocytes that repopulate the liver. The process might account for the observed parenchymal reconstitution in livers of patients with advanced-stage hepatitis and could be a target for regenerative purposes. Persistent injury of the hepatic tissue leads to fibrosis, which eventually evolves to cirrhosis, the end-stage of any chronic liver diseases.Cirrhosis is characterized by distortion of hepatic architecture, regenerative nodules and hepatocyte dysfunction and is associated with life-threatening complications such as hepatocellular insufficiency and hepatocellular carcinoma (HCC).1Liver cirrhosis is estimated to cause around 170,000 deaths annually.2So far, liver transplantation represents the only curative therapeutic solution for many chronic liver diseases.
Chronic liver diseases are characterized by expansion of the small immature cholangiocytes – a mechanism named ductular reaction (DR) – which have the capacity to differentiate into hepatocytes. We investigated the kinetics of this differentiation, as well as analyzing several important features of the newly formed hepatocytes, such as functional maturity, clonal expansion and resistance to stress in mice with long-term liver damage. We tracked cholangiocytes using osteopontin-iCreERT2 and hepatocytes with AAV8-TBG-Cre. Mice received carbon tetrachloride (CCl4) for >24 weeks to induce chronic liver injury. Livers were collected for the analysis of reporter proteins, cell proliferation and death, DNA damage, and nuclear ploidy; hepatocytes were also isolated for RNA sequencing. During liver injury we observed a transient DR and the differentiation of DR cells into hepatocytes as clones that expanded to occupy 12% of the liver parenchyma by week 8. By lineage tracing, we confirmed that these new hepatocytes derived from cholangiocytes but not from native hepatocytes. They had all the features of mature functional hepatocytes. In contrast to the exhausted native hepatocytes, these newly formed hepatocytes had higher proliferative capability, less apoptosis, a lower proportion of highly polyploid nuclei and were better at eliminating DNA damage. In chronic liver injury, DR cells differentiate into stress-resistant hepatocytes that repopulate the liver. The process might account for the observed parenchymal reconstitution in livers of patients with advanced-stage hepatitis and could be a target for regenerative purposes. In chronic liver diseases, extension of the fibrotic scars correlates with the presence of “ductular reaction” (DR).3,4This term refers to proliferation of small immature cholangiocytes, located at the most proximal branches of the biliary tree.5,6DR cells (also referred to as oval cells or liver progenitor cells) express hepatocyte (CK8, CK18) and cholangiocyte (OV-6, CK7, CK19)7–9 markers and have the potential to differentiate into either of these 2 liver epithelial lineages.10Studies on human chronic liver diseases, including chronic viral hepatitis, auto-immune hepatitis and cirrhotic alcoholic or non-alcoholic fatty liver diseases, have highlighted substantial DR and the emergence of cells intermediate in size and immunophenotype between DR cells and hepatocytes.3,11Several studies report that such intermediate cells represent more than half of the hepatocyte pool in the cirrhotic liver.8,9,12A morphological continuum between DR, intermediate cells and hepatocytes may be interpreted as a gradual differentiation of DR in hepatocytes or as a de-differentiation of hepatocytes with acquisition of biliary traits (metaplasia).This conundrum is hard to resolve by the observation of human material.For this reason, several (inducible) lineage tracing mouse strains tagging either cholangiocytes/DR cells or hepatocytes have been used in the last decade in attempt to unravel the origin, the dynamics and the fate of DR cells in various dietary, chemical or genetic rodent models of liver injury.The results of these studies remain conflicting.Studies by us and other authors, in which the fate of DR cells or hepatocytes was followed after hepatocellular injury caused by a choline-deficient and ethionine-supplemented (CDE) diet, support the in vivo capability of DR to differentiate into hepatocytes, although in discrete proportions (<2.5%).13–15Furthermore, DR cells isolated from CDE livers largely underwent hepatocyte differentiation when transplanted in vivo into a compromised liver, with an improvement of both liver architecture and function.16In zebrafish the biliary compartment is also capable of generating functional hepatocytes.17On the other hand, studies using the 3,5-diethoxycarbonyl-1,4-dihydrocollidine (DDC) diet as model of cholangiocytic injury failed to demonstrate a contribution of DR to the hepatocyte pool18,19 and other works even support a de-differentiation of traced hepatocytes into biliary-like cells.20–22Taken together such inconsistent data indicate that the involvement of DR cells in regeneration is conditioned by the epithelial compartment undergoing damage and is thus disease-specific; while considerable discrepancies between models and observations in human material may stem from fundamental differences in severity and chronicity of injury.Here, we aimed to analyze DR and its contribution to regeneration in a model replicating chronicity, severity and fibrotic progression seen in chronic hepatitis in humans.
Chronic liver diseases are characterized by expansion of the small immature cholangiocytes – a mechanism named ductular reaction (DR) – which have the capacity to differentiate into hepatocytes. We investigated the kinetics of this differentiation, as well as analyzing several important features of the newly formed hepatocytes, such as functional maturity, clonal expansion and resistance to stress in mice with long-term liver damage. We tracked cholangiocytes using osteopontin-iCreERT2 and hepatocytes with AAV8-TBG-Cre. Mice received carbon tetrachloride (CCl4) for >24 weeks to induce chronic liver injury. Livers were collected for the analysis of reporter proteins, cell proliferation and death, DNA damage, and nuclear ploidy; hepatocytes were also isolated for RNA sequencing. During liver injury we observed a transient DR and the differentiation of DR cells into hepatocytes as clones that expanded to occupy 12% of the liver parenchyma by week 8. By lineage tracing, we confirmed that these new hepatocytes derived from cholangiocytes but not from native hepatocytes. They had all the features of mature functional hepatocytes. In contrast to the exhausted native hepatocytes, these newly formed hepatocytes had higher proliferative capability, less apoptosis, a lower proportion of highly polyploid nuclei and were better at eliminating DNA damage. In chronic liver injury, DR cells differentiate into stress-resistant hepatocytes that repopulate the liver. The process might account for the observed parenchymal reconstitution in livers of patients with advanced-stage hepatitis and could be a target for regenerative purposes. Impaired hepatocyte proliferative capacity is the essential requirement for DR.23 Recently, the capacity of the DR cells to maintain the liver parenchyma was demonstrated using genetic approaches to induce hepatocyte replicative senescence or growth arrest: i) deletion of Mdm216 to provoke p53-mediated senescence in all the hepatocytes; ii) deletion of β1-integrin24 to inhibit hepatic growth factor signaling thereby precluding hepatocyte replication, or of β-catenin directly in the hepatocytes to impair their proliferation;25 iii) the overexpression of p21 in conjunction with induction of injury using a DDC diet, a methionine-choline deficient diet and thioacetamide (TAA).24Such genetic models artificially cause subacute hepatocyte failure, as opposed to progressive lesions occurring during chronic liver injury.Thus, Deng et al. demonstrated the capability of DR cells to differentiate into hepatocytes in a long-term injury model (up to 52 weeks) using the toxic agent TAA.26However, the kinetics of the response to the injury, as well as several features of the DR-derived hepatocytes, such as level of maturation, clonal expansion and resistance to stress remain unanswered.
Chronic liver diseases are characterized by expansion of the small immature cholangiocytes – a mechanism named ductular reaction (DR) – which have the capacity to differentiate into hepatocytes. We investigated the kinetics of this differentiation, as well as analyzing several important features of the newly formed hepatocytes, such as functional maturity, clonal expansion and resistance to stress in mice with long-term liver damage. We tracked cholangiocytes using osteopontin-iCreERT2 and hepatocytes with AAV8-TBG-Cre. Mice received carbon tetrachloride (CCl4) for >24 weeks to induce chronic liver injury. Livers were collected for the analysis of reporter proteins, cell proliferation and death, DNA damage, and nuclear ploidy; hepatocytes were also isolated for RNA sequencing. During liver injury we observed a transient DR and the differentiation of DR cells into hepatocytes as clones that expanded to occupy 12% of the liver parenchyma by week 8. By lineage tracing, we confirmed that these new hepatocytes derived from cholangiocytes but not from native hepatocytes. They had all the features of mature functional hepatocytes. In contrast to the exhausted native hepatocytes, these newly formed hepatocytes had higher proliferative capability, less apoptosis, a lower proportion of highly polyploid nuclei and were better at eliminating DNA damage. In chronic liver injury, DR cells differentiate into stress-resistant hepatocytes that repopulate the liver. The process might account for the observed parenchymal reconstitution in livers of patients with advanced-stage hepatitis and could be a target for regenerative purposes. Here, we used a model of chronic liver injury mimicking the evolution and severity of chronic human diseases to follow the fate of DR cells, evaluating their contribution to the pool of hepatocytes and characterizing the newly formed hepatocytes.We show that DR emerges from clonal expansion of cholangiocytes; DR cells then undergo hepatocyte differentiation and clonal proliferation.Hepatocyte lineage tracing studies confirmed that these emerging cells are not of hepatocyte origin.
Liver repair following hepatic ischemia/reperfusion (I/R) injury is crucial to survival. This study aims to examine the role of endogenous prostaglandin E2 (PGE2) produced by inducible microsomal PGE synthase-1 (mPGES-1), a terminal enzyme of PGE2 generation, in liver injury and repair following hepatic I/R. mPGES-1 deficient (Ptges−/−) mice or their wild-type (WT) counterparts were subjected to partial hepatic ischemia followed by reperfusion. The role of E prostanoid receptor 4 (EP4) was then studied using a genetic knockout model and a selective antagonist. Compared with WT mice, Ptges−/− mice exhibited reductions in alanine aminotransferase (ALT), necrotic area, neutrophil infiltration, chemokines, and proinflammatory cytokine levels. Ptges−/− mice also showed promoted liver repair and increased Ly6Clow macrophages (Ly6Clow/CD11bhigh/F4/80high-cells) with expression of anti-inflammatory and reparative genes, while WT mice exhibited delayed liver repair and increased Ly6Chigh macrophages (Ly6Chigh/CD11bhigh/F4/80low-cells) with expression of proinflammatory genes. Bone marrow (BM)-derived mPGES-1-deficient macrophages facilitated liver repair with increases in Ly6Clow macrophages. In vitro, mPGES-1 was expressed in macrophages polarized toward the proinflammatory profile. Mice treated with the mPGES-1 inhibitor Compound III displayed increased liver protection and repair. Hepatic I/R enhanced the hepatic expression of PGE receptor subtype, EP4, in WT mice, which was reduced in Ptges−/− mice. A selective EP4 antagonist and genetic deletion of Ptger4, which codes for EP4, accelerated liver repair. The proinflammatory gene expression was upregulated by stimulation of EP4 agonist in WT macrophages but not in EP4-deficient macrophages. These results indicate that mPGES-1 regulates macrophage polarization as well as liver protection and repair through EP4 signaling during hepatic I/R. Inhibition of mPGES-1 could have therapeutic potential by promoting liver repair after acute liver injury. Hepatic ischemia reperfusion (I/R) injury is a major complication of severe hypotension followed by fluid resuscitation, liver resection, and liver transplantation.1Hepatic I/R injury followed by hepatic tissue repair significantly affects post-operative liver function and survival, since insufficient liver restoration is associated with increased morbidity and mortality.2,3Thus, the balance between hepatocyte death and subsequent liver repair and regeneration determines the prognosis of patients with hepatic I/R injury.Nonetheless, therapeutic options for the prevention of hepatic I/R injury and that stimulate liver repair are limited.
Liver repair following hepatic ischemia/reperfusion (I/R) injury is crucial to survival. This study aims to examine the role of endogenous prostaglandin E2 (PGE2) produced by inducible microsomal PGE synthase-1 (mPGES-1), a terminal enzyme of PGE2 generation, in liver injury and repair following hepatic I/R. mPGES-1 deficient (Ptges−/−) mice or their wild-type (WT) counterparts were subjected to partial hepatic ischemia followed by reperfusion. The role of E prostanoid receptor 4 (EP4) was then studied using a genetic knockout model and a selective antagonist. Compared with WT mice, Ptges−/− mice exhibited reductions in alanine aminotransferase (ALT), necrotic area, neutrophil infiltration, chemokines, and proinflammatory cytokine levels. Ptges−/− mice also showed promoted liver repair and increased Ly6Clow macrophages (Ly6Clow/CD11bhigh/F4/80high-cells) with expression of anti-inflammatory and reparative genes, while WT mice exhibited delayed liver repair and increased Ly6Chigh macrophages (Ly6Chigh/CD11bhigh/F4/80low-cells) with expression of proinflammatory genes. Bone marrow (BM)-derived mPGES-1-deficient macrophages facilitated liver repair with increases in Ly6Clow macrophages. In vitro, mPGES-1 was expressed in macrophages polarized toward the proinflammatory profile. Mice treated with the mPGES-1 inhibitor Compound III displayed increased liver protection and repair. Hepatic I/R enhanced the hepatic expression of PGE receptor subtype, EP4, in WT mice, which was reduced in Ptges−/− mice. A selective EP4 antagonist and genetic deletion of Ptger4, which codes for EP4, accelerated liver repair. The proinflammatory gene expression was upregulated by stimulation of EP4 agonist in WT macrophages but not in EP4-deficient macrophages. These results indicate that mPGES-1 regulates macrophage polarization as well as liver protection and repair through EP4 signaling during hepatic I/R. Inhibition of mPGES-1 could have therapeutic potential by promoting liver repair after acute liver injury. Endogenous prostaglandin E2 (PGE2), a metabolite of arachidonic acid produced via cyclooxygenase (COX), is an important mediator of pain and inflammation.4The final step of PGE2 generation is catalyzed by specific PGE synthases (PGESs),5 of which there are at least three isoforms: cytosolic PGES (cPGES), and two types of microsomal PGES, mPGES-1 and mPGES-2.mPGES-1 is the dominant source of PGE2 biosynthesis under basal conditions or during inflammatory states.6The inducible mPGES-1 contributes to the exacerbation of stroke injury through PGE2 production following brain ischemia.7
Liver repair following hepatic ischemia/reperfusion (I/R) injury is crucial to survival. This study aims to examine the role of endogenous prostaglandin E2 (PGE2) produced by inducible microsomal PGE synthase-1 (mPGES-1), a terminal enzyme of PGE2 generation, in liver injury and repair following hepatic I/R. mPGES-1 deficient (Ptges−/−) mice or their wild-type (WT) counterparts were subjected to partial hepatic ischemia followed by reperfusion. The role of E prostanoid receptor 4 (EP4) was then studied using a genetic knockout model and a selective antagonist. Compared with WT mice, Ptges−/− mice exhibited reductions in alanine aminotransferase (ALT), necrotic area, neutrophil infiltration, chemokines, and proinflammatory cytokine levels. Ptges−/− mice also showed promoted liver repair and increased Ly6Clow macrophages (Ly6Clow/CD11bhigh/F4/80high-cells) with expression of anti-inflammatory and reparative genes, while WT mice exhibited delayed liver repair and increased Ly6Chigh macrophages (Ly6Chigh/CD11bhigh/F4/80low-cells) with expression of proinflammatory genes. Bone marrow (BM)-derived mPGES-1-deficient macrophages facilitated liver repair with increases in Ly6Clow macrophages. In vitro, mPGES-1 was expressed in macrophages polarized toward the proinflammatory profile. Mice treated with the mPGES-1 inhibitor Compound III displayed increased liver protection and repair. Hepatic I/R enhanced the hepatic expression of PGE receptor subtype, EP4, in WT mice, which was reduced in Ptges−/− mice. A selective EP4 antagonist and genetic deletion of Ptger4, which codes for EP4, accelerated liver repair. The proinflammatory gene expression was upregulated by stimulation of EP4 agonist in WT macrophages but not in EP4-deficient macrophages. These results indicate that mPGES-1 regulates macrophage polarization as well as liver protection and repair through EP4 signaling during hepatic I/R. Inhibition of mPGES-1 could have therapeutic potential by promoting liver repair after acute liver injury. In hepatic I/R injury, inhibition of COX-2, an inducible isoform of COX, minimizes the injury.8By contrast, PGE2 has been implicated in the prevention of hepatic I/R injury.9The administration of an agonist for EP4, one of the PGE receptor subtypes, is found to confer protection against ischemic injury in the liver.10To date, the implication of PGE2 in hepatic I/R injury remains controversial.
Glycogen synthase kinase 3β (Gsk3β [Gsk3b]) is a ubiquitously expressed kinase with distinctive functions in different types of cells. Although its roles in regulating innate immune activation and ischaemia and reperfusion injuries (IRIs) have been well documented, the underlying mechanisms remain ambiguous, in part because of the lack of cell-specific tools in vivo. We created a myeloid-specific Gsk3b knockout (KO) strain to study the function of Gsk3β in macrophages in a murine liver partial warm ischaemia model. Compared with controls, myeloid Gsk3b KO mice were protected from IRI, with diminished proinflammatory but enhanced anti-inflammatory immune responses in livers. In bone marrow-derived macrophages, Gsk3β deficiency resulted in an early reduction of Tnf gene transcription but sustained increase of Il10 gene transcription on Toll-like receptor 4 stimulation in vitro. These effects were associated with enhanced AMP-activated protein kinase (AMPK) activation, which led to an accelerated and higher level of induction of the novel innate immune negative regulator small heterodimer partner (SHP [Nr0b2]). The regulatory function of Gsk3β on AMPK activation and SHP induction was confirmed in wild-type bone marrow-derived macrophages with a Gsk3 inhibitor. Furthermore, we found that this immune regulatory mechanism was independent of Gsk3β Ser9 phosphorylation and the phosphoinositide 3-kinase–Akt signalling pathway. In vivo, myeloid Gsk3β deficiency facilitated SHP upregulation by ischaemia–reperfusion in liver macrophages. Treatment of Gsk3b KO mice with either AMPK inhibitor or SHP small interfering RNA before the onset of liver ischaemia restored liver proinflammatory immune activation and IRI in these otherwise protected hosts. Additionally, pharmacological activation of AMPK protected wild-type mice from liver IRI, with reduced proinflammatory immune activation. Inhibition of the AMPK–SHP pathway by liver ischaemia was demonstrated in tumour resection patients. Gsk3β promotes innate proinflammatory immune activation by restraining AMPK activation. Liver inflammation triggered by ischaemia–reperfusion (IR) is an innate immune-dominated response, mediated by the sentinel pattern recognition receptor (PRR) system.1,2Damage-associated molecular patterns (DAMPs),3 such as high-mobility group box 1, released from necrotic/stressed cells,4–6 activate PRRs, including Toll-like receptors (TLRs) and nucleotide-binding oligomerization domain-like receptors (NLRs),7–10 to initiate the proinflammatory immune response.At the cellular level, macrophages, the predominant innate immune cells in the liver, play a major role in reacting to DAMPs to trigger tissue inflammatory immune response.
Glycogen synthase kinase 3β (Gsk3β [Gsk3b]) is a ubiquitously expressed kinase with distinctive functions in different types of cells. Although its roles in regulating innate immune activation and ischaemia and reperfusion injuries (IRIs) have been well documented, the underlying mechanisms remain ambiguous, in part because of the lack of cell-specific tools in vivo. We created a myeloid-specific Gsk3b knockout (KO) strain to study the function of Gsk3β in macrophages in a murine liver partial warm ischaemia model. Compared with controls, myeloid Gsk3b KO mice were protected from IRI, with diminished proinflammatory but enhanced anti-inflammatory immune responses in livers. In bone marrow-derived macrophages, Gsk3β deficiency resulted in an early reduction of Tnf gene transcription but sustained increase of Il10 gene transcription on Toll-like receptor 4 stimulation in vitro. These effects were associated with enhanced AMP-activated protein kinase (AMPK) activation, which led to an accelerated and higher level of induction of the novel innate immune negative regulator small heterodimer partner (SHP [Nr0b2]). The regulatory function of Gsk3β on AMPK activation and SHP induction was confirmed in wild-type bone marrow-derived macrophages with a Gsk3 inhibitor. Furthermore, we found that this immune regulatory mechanism was independent of Gsk3β Ser9 phosphorylation and the phosphoinositide 3-kinase–Akt signalling pathway. In vivo, myeloid Gsk3β deficiency facilitated SHP upregulation by ischaemia–reperfusion in liver macrophages. Treatment of Gsk3b KO mice with either AMPK inhibitor or SHP small interfering RNA before the onset of liver ischaemia restored liver proinflammatory immune activation and IRI in these otherwise protected hosts. Additionally, pharmacological activation of AMPK protected wild-type mice from liver IRI, with reduced proinflammatory immune activation. Inhibition of the AMPK–SHP pathway by liver ischaemia was demonstrated in tumour resection patients. Gsk3β promotes innate proinflammatory immune activation by restraining AMPK activation. Glycogen synthase kinase 3 (Gsk3) is a ubiquitously expressed serine/threonine kinase with a large number of substrates.11–14There are two isoforms, α and β, which share extensive homology in the kinase domain, but are functionally distinctive possibly because of their unique N- and C-terminals.In mice, global Gsk3β (Gsk3b) knockout (KO) is embryonically lethal because of liver degeneration,15 whereas Gsk3α (Gsk3a) KO mice are viable and relatively normal.16Most research on Gsk3 has focused on the β isoform using chemical inhibitors, which do not discriminate between the α and β isoforms.17Gsk3β is a unique signalling kinase: constitutively active in resting cells and inhibited by Ser9 phosphorylation on stimulation.Gsk3β regulates diverse cellular activities in different cell types, including metabolism, proliferation, differentiation, apoptosis, and immune activation.11–14,18–20Gsk3β inhibition suppresses proinflammatory gene programmes but promotes anti-inflammatory gene programmes in macrophages.12TLR activation triggers Gsk3β inhibitory phosphorylation by the phosphoinositide 3-kinase (PI3K)–protein kinase B/Akt pathway, resulting in an increase of cyclic AMP response element-binding protein (CREB) activity but a decrease of NF-κB activity.12,21In patients with cirrhosis, defects in Akt-mediated Gsk3 inhibitory phosphorylation in monocytes resulted in an excessive proinflammatory response in vitro.22In vivo, Gsk3 inhibitors protected mice from endotoxin shock.21We have shown that the Gsk3 inhibitor SB216763 protects mice from liver IR injury (IRI) via an IL-10 (Il10)-mediated immune regulatory mechanism.23
Glycogen synthase kinase 3β (Gsk3β [Gsk3b]) is a ubiquitously expressed kinase with distinctive functions in different types of cells. Although its roles in regulating innate immune activation and ischaemia and reperfusion injuries (IRIs) have been well documented, the underlying mechanisms remain ambiguous, in part because of the lack of cell-specific tools in vivo. We created a myeloid-specific Gsk3b knockout (KO) strain to study the function of Gsk3β in macrophages in a murine liver partial warm ischaemia model. Compared with controls, myeloid Gsk3b KO mice were protected from IRI, with diminished proinflammatory but enhanced anti-inflammatory immune responses in livers. In bone marrow-derived macrophages, Gsk3β deficiency resulted in an early reduction of Tnf gene transcription but sustained increase of Il10 gene transcription on Toll-like receptor 4 stimulation in vitro. These effects were associated with enhanced AMP-activated protein kinase (AMPK) activation, which led to an accelerated and higher level of induction of the novel innate immune negative regulator small heterodimer partner (SHP [Nr0b2]). The regulatory function of Gsk3β on AMPK activation and SHP induction was confirmed in wild-type bone marrow-derived macrophages with a Gsk3 inhibitor. Furthermore, we found that this immune regulatory mechanism was independent of Gsk3β Ser9 phosphorylation and the phosphoinositide 3-kinase–Akt signalling pathway. In vivo, myeloid Gsk3β deficiency facilitated SHP upregulation by ischaemia–reperfusion in liver macrophages. Treatment of Gsk3b KO mice with either AMPK inhibitor or SHP small interfering RNA before the onset of liver ischaemia restored liver proinflammatory immune activation and IRI in these otherwise protected hosts. Additionally, pharmacological activation of AMPK protected wild-type mice from liver IRI, with reduced proinflammatory immune activation. Inhibition of the AMPK–SHP pathway by liver ischaemia was demonstrated in tumour resection patients. Gsk3β promotes innate proinflammatory immune activation by restraining AMPK activation. Gsk3β can play either pro- or anti-apoptotic roles, depending on cell death mechanisms.Active Gsk3β inhibits the extrinsic cell death pathway initiated by TNF-α (Tnf) or other death receptor ligands.11,15Meanwhile, Gsk3β inactivation prevents the opening of mitochondrial permeability transition pore, a key step in the intrinsic cell death pathway initiated by oxidative or endoplasmic reticulum stress.11,24,25Gsk3 inhibition has been shown to protect against tissue parenchymal cell death caused by IR,26–29 although the opposite effect has also been observed,30 possibly due to the difference in its targeted cell death processes.
p38 mitogen-activated protein kinases are important inflammatory factors. p38α alteration has been implicated in both human and mouse inflammatory disease models. Therefore, we aimed to characterize the cell type-specific role of p38α in non-alcoholic steatohepatitis (NASH). Human liver tissues were obtained from 27 patients with non-alcoholic fatty liver disease (NAFLD) and 20 control individuals. NASH was established and compared between hepatocyte-specific p38α knockout (p38αΔHep), macrophage-specific p38α knockout (p38αΔMΦ) and wild-type (p38αfl/fl) mice fed with high-fat diet (HFD), high-fat/high-cholesterol diet (HFHC), or methionine-and choline-deficient diet (MCD). p38 inhibitors were administered to HFHC-fed wild-type mice for disease treatment. p38α was significantly upregulated in the liver tissues of patients with NAFLD. Compared to p38αfl/fl littermates, p38αΔHep mice developed significant nutritional steatohepatitis induced by HFD, HFHC or MCD. Meanwhile, p38αΔMΦ mice exhibited less severe steatohepatitis and insulin resistance than p38αfl/fl mice in response to a HFHC or MCD. The effect of macrophage p38α in promoting steatohepatitis was mediated by the induction of pro-inflammatory factors (CXCL2, IL-1β, CXCL10 and IL-6) secreted by M1 macrophages and associated signaling pathways. p38αΔMΦ mice exhibited M2 anti-inflammatory polarization as demonstrated by increased CD45+F4/80+CD11b+CD206+ M2 macrophages and enhanced arginase activity in liver tissues. Primary hepatocytes from p38αΔMΦ mice showed decreased steatosis and inflammatory damage. In a co-culture system, p38α deleted macrophages attenuated steatohepatitic changes in hepatocytes through decreased secretion of pro-inflammatory cytokines (TNF-α, CXCL10 and IL-6), which mediate M1 macrophage polarization in p38αΔMΦ mice. Restoration of TNF-α, CXCL10 or IL‐6 induced lipid accumulation and inflammatory responses in p38αfl/fl hepatocytes co-cultured with p38αΔMΦ macrophages. Moreover, pharmacological p38 inhibitors suppressed HFHC-induced steatohepatitis. Macrophage p38α promotes the progression of steatohepatitis by inducing pro-inflammatory cytokine secretion and M1 polarization. p38 inhibition protects against steatohepatitis. Non-alcoholic fatty liver disease (NAFLD) is the hepatic manifestation of metabolic syndrome and a major healthcare burden worldwide.1The clinicopathological spectrum of NAFLD ranges from hepatic steatosis to non-alcoholic steatohepatitis (NASH), a more aggressive form which can progress to cirrhosis and/or hepatocellular carcinoma.2However, there is currently no effective pharmacological therapy approved for NASH, and efforts to control complications arising from the condition are far from satisfactory.3A better understanding of the molecular mechanisms of NASH is essential for developing promising treatment strategies for this highly prevalent disease.
p38 mitogen-activated protein kinases are important inflammatory factors. p38α alteration has been implicated in both human and mouse inflammatory disease models. Therefore, we aimed to characterize the cell type-specific role of p38α in non-alcoholic steatohepatitis (NASH). Human liver tissues were obtained from 27 patients with non-alcoholic fatty liver disease (NAFLD) and 20 control individuals. NASH was established and compared between hepatocyte-specific p38α knockout (p38αΔHep), macrophage-specific p38α knockout (p38αΔMΦ) and wild-type (p38αfl/fl) mice fed with high-fat diet (HFD), high-fat/high-cholesterol diet (HFHC), or methionine-and choline-deficient diet (MCD). p38 inhibitors were administered to HFHC-fed wild-type mice for disease treatment. p38α was significantly upregulated in the liver tissues of patients with NAFLD. Compared to p38αfl/fl littermates, p38αΔHep mice developed significant nutritional steatohepatitis induced by HFD, HFHC or MCD. Meanwhile, p38αΔMΦ mice exhibited less severe steatohepatitis and insulin resistance than p38αfl/fl mice in response to a HFHC or MCD. The effect of macrophage p38α in promoting steatohepatitis was mediated by the induction of pro-inflammatory factors (CXCL2, IL-1β, CXCL10 and IL-6) secreted by M1 macrophages and associated signaling pathways. p38αΔMΦ mice exhibited M2 anti-inflammatory polarization as demonstrated by increased CD45+F4/80+CD11b+CD206+ M2 macrophages and enhanced arginase activity in liver tissues. Primary hepatocytes from p38αΔMΦ mice showed decreased steatosis and inflammatory damage. In a co-culture system, p38α deleted macrophages attenuated steatohepatitic changes in hepatocytes through decreased secretion of pro-inflammatory cytokines (TNF-α, CXCL10 and IL-6), which mediate M1 macrophage polarization in p38αΔMΦ mice. Restoration of TNF-α, CXCL10 or IL‐6 induced lipid accumulation and inflammatory responses in p38αfl/fl hepatocytes co-cultured with p38αΔMΦ macrophages. Moreover, pharmacological p38 inhibitors suppressed HFHC-induced steatohepatitis. Macrophage p38α promotes the progression of steatohepatitis by inducing pro-inflammatory cytokine secretion and M1 polarization. p38 inhibition protects against steatohepatitis. Although the molecular pathogenesis of NASH remains elusive, compelling evidence shows that the initiation and perpetuation of liver inflammation is crucial to the pathogenesis of NASH.4Central to inflammatory signaling is the reversible phosphorylation of protein regulators and effectors, particularly mitogen-activated protein kinases (MAPK).5p38 MAPK (MAPK14) consists of 4 isoforms: p38α, p38β, p38γ and p38δ, with p38α being the most abundant family member.p38α protein kinase transduces a variety of extracellular signals that regulate cellular processes, such as inflammation, differentiation, proliferation and apoptosis.6The activation of p38 MAPK requires phosphorylation by MAPK kinases or autophosphorylation at pT180/pY182.7p38α has been implicated in many inflammatory diseases,5,8 displaying cell type-specific functions in skin injury5 and inflammatory bowel disease.8It appears to have dual functions in colorectal tumorigenesis: suppressing colitis-associated colon cancer on the one hand but contributing to the proliferation and survival of tumor cells on the other.9In the liver, p38α can act as a negative regulator of hepatocyte proliferation but also an oncogenic factor mediating inflammation associated with liver injury.10,11Cell type-specific functions of p38α are also evident in the liver as hepatic p38α is a pivotal regulator of hepatic gluconeogenesis,12 while macrophage p38α induces acute liver injury.13Pharmacological inhibitors of p38 have been tested for the treatment of inflammatory diseases.14While there are some hints that p38α may be involved in various liver processes, their possible role in NASH progression is still unknown.Therefore, clarifying the functional significance of p38α in NASH is of great clinical importance for the identification of specific therapeutic targets.
Programmed cell death 1 ligand 1 (PD-L1) expression on antigen-presenting cells is essential for T cell impairment, and PD-L1-expressing macrophages may mechanistically shape and therapeutically predict the clinical efficacy of PD-L1 or programmed cell death 1 blockade. We aimed to elucidate the mechanisms underlying PD-L1 upregulation in human tumor microenvironments, which remain poorly understood despite the clinical success of immune checkpoint inhibitors. Monocytes/macrophages were purified from peripheral blood, non-tumor, or paired tumor tissues of patients with hepatocellular carcinoma (HCC), and their possible glycolytic switch was evaluated. The underlying regulatory mechanisms and clinical significance of metabolic switching were studied with both ex vivo analyses and in vitro experiments. We found that monocytes significantly enhanced the levels of glycolysis at the peritumoral region of human HCC. The activation of glycolysis induced PD-L1 expression on these cells and subsequently attenuated cytotoxic T lymphocyte responses in tumor tissues. Mechanistically, tumor-derived soluble factors, including hyaluronan fragments, induced the upregulation of a key glycolytic enzyme, PFKFB3, in tumor-associated monocytes. This enzyme not only modulated the cellular metabolic switch but also mediated the increased expression of PD-L1 by activating the nuclear factor kappa B signaling pathway in these cells. Consistently, the levels of PFKFB3+CD68+ cell infiltration in peritumoral tissues were negatively correlated with overall survival and could serve as an independent prognostic factor for survival in patients with HCC. Our results reveal a mechanism by which the cellular metabolic switch regulates the pro-tumor functions of monocytes in a specific human tumor microenvironment. PFKFB3 in both cancer cells and tumor-associated monocytes is a potential therapeutic target in human HCC. Monocytes/macrophages constitute a major component of most solid tumors and exhibit great plasticity and diversity according to different environmental cues.1–3Instead of inducing anti-tumor immune responses, monocytes/macrophages can be educated by the tumor microenvironment and facilitate disease progression via diverse mechanisms.4,5For example, we and others have found that the peritumoral stroma of human hepatocellular carcinoma (HCC) is highly infiltrated by monocytes with activated phenotypes.6,7These activated monocytes attenuate the T cell response by expressing programmed cell death 1 ligand 1 (PD-L1) but retain their proinflammatory properties to induce angiogenesis and tissue remodeling by inducing interleukin (IL)-17A-producing cell expansion and neutrophil recruitment, thus rerouting the inflammatory response in a tumor-promoting direction.8,9The specific phenotype and functions of these tumor-infiltrating monocytes/macrophages are generally thought to be induced and maintained by local environmental factors, but the underlying mechanisms are still not well understood.
Programmed cell death 1 ligand 1 (PD-L1) expression on antigen-presenting cells is essential for T cell impairment, and PD-L1-expressing macrophages may mechanistically shape and therapeutically predict the clinical efficacy of PD-L1 or programmed cell death 1 blockade. We aimed to elucidate the mechanisms underlying PD-L1 upregulation in human tumor microenvironments, which remain poorly understood despite the clinical success of immune checkpoint inhibitors. Monocytes/macrophages were purified from peripheral blood, non-tumor, or paired tumor tissues of patients with hepatocellular carcinoma (HCC), and their possible glycolytic switch was evaluated. The underlying regulatory mechanisms and clinical significance of metabolic switching were studied with both ex vivo analyses and in vitro experiments. We found that monocytes significantly enhanced the levels of glycolysis at the peritumoral region of human HCC. The activation of glycolysis induced PD-L1 expression on these cells and subsequently attenuated cytotoxic T lymphocyte responses in tumor tissues. Mechanistically, tumor-derived soluble factors, including hyaluronan fragments, induced the upregulation of a key glycolytic enzyme, PFKFB3, in tumor-associated monocytes. This enzyme not only modulated the cellular metabolic switch but also mediated the increased expression of PD-L1 by activating the nuclear factor kappa B signaling pathway in these cells. Consistently, the levels of PFKFB3+CD68+ cell infiltration in peritumoral tissues were negatively correlated with overall survival and could serve as an independent prognostic factor for survival in patients with HCC. Our results reveal a mechanism by which the cellular metabolic switch regulates the pro-tumor functions of monocytes in a specific human tumor microenvironment. PFKFB3 in both cancer cells and tumor-associated monocytes is a potential therapeutic target in human HCC. Cellular metabolic changes occur not only as passive consequences of environmental cues but also as active regulators in many physiological and pathological conditions.10–12For example, lymphocytes employ distinct metabolic substrates and pathways to fuel their different effector functions in naive, activated, and memory states.13–15Cancer cells preferentially upregulate the glycolytic pathway to support and satisfy their distinct functional needs in tissue microenvironments, a phenomenon known as the Warburg effect.16To date, little is known about whether and how metabolic changes might occur and regulate the phenotypes and functions of monocytes/macrophages in specific tumor microenvironments.
Programmed cell death 1 ligand 1 (PD-L1) expression on antigen-presenting cells is essential for T cell impairment, and PD-L1-expressing macrophages may mechanistically shape and therapeutically predict the clinical efficacy of PD-L1 or programmed cell death 1 blockade. We aimed to elucidate the mechanisms underlying PD-L1 upregulation in human tumor microenvironments, which remain poorly understood despite the clinical success of immune checkpoint inhibitors. Monocytes/macrophages were purified from peripheral blood, non-tumor, or paired tumor tissues of patients with hepatocellular carcinoma (HCC), and their possible glycolytic switch was evaluated. The underlying regulatory mechanisms and clinical significance of metabolic switching were studied with both ex vivo analyses and in vitro experiments. We found that monocytes significantly enhanced the levels of glycolysis at the peritumoral region of human HCC. The activation of glycolysis induced PD-L1 expression on these cells and subsequently attenuated cytotoxic T lymphocyte responses in tumor tissues. Mechanistically, tumor-derived soluble factors, including hyaluronan fragments, induced the upregulation of a key glycolytic enzyme, PFKFB3, in tumor-associated monocytes. This enzyme not only modulated the cellular metabolic switch but also mediated the increased expression of PD-L1 by activating the nuclear factor kappa B signaling pathway in these cells. Consistently, the levels of PFKFB3+CD68+ cell infiltration in peritumoral tissues were negatively correlated with overall survival and could serve as an independent prognostic factor for survival in patients with HCC. Our results reveal a mechanism by which the cellular metabolic switch regulates the pro-tumor functions of monocytes in a specific human tumor microenvironment. PFKFB3 in both cancer cells and tumor-associated monocytes is a potential therapeutic target in human HCC. On the one hand, cellular metabolism provides the energy and material basis for the process of signaling transduction.On the other hand, many metabolic intermediates or enzymes can directly interact with signaling pathways to regulate downstream cellular functions.17As has been reported, it is not only the process of glycolysis itself that supports cellular growth and proliferation, but its key enzymes and byproducts can also actively regulate the phenotype and functions of immune cells by regulating downstream signaling pathways.18–20For example, glycolytic enzyme pyruvate kinase M2 (PKM2) can mediate both the glycolytic switch and IL-1β production via hypoxia inducible factor 1 α (HIF1α) stabilization in lipopolysaccharide (LPS)-activated murine macrophages,21 suggesting that in addition to canonical roles in anabolism or catabolism, key metabolic enzymes play direct roles in modulating immune responses.However, currently, the enzymes or intermediates that might be involved in regulating the phenotypes and functions of human monocytes/macrophages in specific tumor microenvironments, as well as the underlying signaling mechanisms, are not well understood.
Recently, Baveno VI guidelines suggested that esophagogastroduodenoscopy (EGD) can be avoided in patients with compensated advanced chronic liver disease (cACLD) who have a liver stiffness measurement (LSM) <20 kPa and platelet count >150,000/mm3. We aimed to: assess the performance of spleen stiffness measurement (SSM) in ruling out patients with high-risk varices (HRV); validate Baveno VI criteria in a large population and assess how the sequential use of Baveno VI criteria and SSM could safely avoid the need for endoscopy. We retrospectively analyzed 498 patients with cACLD who had undergone LSM/SSM by transient elastography (TE) (FibroScan®), platelet count and EGDs from 2012 to 2016 referred to our tertiary centre. The new combined model was validated internally by a split-validation method, and externally in a prospective multicentre cohort of 115 patients. SSM, LSM, platelet count and Child-Pugh-B were independent predictors of HRV. Applying the newly identified SSM cut-off (≤46 kPa) or Baveno VI criteria, 35.8% and 21.7% of patients in the internal validation cohort could have avoided EGD, with only 2% of HRVs being missed with either model. The combination of SSM with Baveno VI criteria would have avoided an additional 22.5% of EGDs, reaching a final value of 43.8% spared EGDs, with <5% missed HRVs. Results were confirmed in the prospective external validation cohort, as the combined Baveno VI/SSM ≤46 model would have safely spared (0 HRV missed) 37.4% of EGDs, compared to 16.5% when using the Baveno VI criteria alone. A non-invasive prediction model combining SSM with Baveno VI criteria may be useful to rule out HRV and could make it possible to avoid a significantly larger number of unnecessary EGDs compared to Baveno VI criteria only. The increase of portal pressure above the threshold of clinically significant portal hypertension (CSPH, hepatic venous pressure gradient [HVPG] ≥10 mmHg) is a landmark in the natural history of compensated advanced chronic liver disease (cACLD).Patients with CSPH are at risk of developing esophageal varices (EV) and clinical decompensation (variceal bleeding, ascites, jaundice, encephalopathy), which mark the transition from a compensated stage to a stage of the disease (decompensated) characterized by much higher mortality.1In particular, variceal bleeding is still associated with 10–15% mortality despite the advances in its treatment.2
Recently, Baveno VI guidelines suggested that esophagogastroduodenoscopy (EGD) can be avoided in patients with compensated advanced chronic liver disease (cACLD) who have a liver stiffness measurement (LSM) <20 kPa and platelet count >150,000/mm3. We aimed to: assess the performance of spleen stiffness measurement (SSM) in ruling out patients with high-risk varices (HRV); validate Baveno VI criteria in a large population and assess how the sequential use of Baveno VI criteria and SSM could safely avoid the need for endoscopy. We retrospectively analyzed 498 patients with cACLD who had undergone LSM/SSM by transient elastography (TE) (FibroScan®), platelet count and EGDs from 2012 to 2016 referred to our tertiary centre. The new combined model was validated internally by a split-validation method, and externally in a prospective multicentre cohort of 115 patients. SSM, LSM, platelet count and Child-Pugh-B were independent predictors of HRV. Applying the newly identified SSM cut-off (≤46 kPa) or Baveno VI criteria, 35.8% and 21.7% of patients in the internal validation cohort could have avoided EGD, with only 2% of HRVs being missed with either model. The combination of SSM with Baveno VI criteria would have avoided an additional 22.5% of EGDs, reaching a final value of 43.8% spared EGDs, with <5% missed HRVs. Results were confirmed in the prospective external validation cohort, as the combined Baveno VI/SSM ≤46 model would have safely spared (0 HRV missed) 37.4% of EGDs, compared to 16.5% when using the Baveno VI criteria alone. A non-invasive prediction model combining SSM with Baveno VI criteria may be useful to rule out HRV and could make it possible to avoid a significantly larger number of unnecessary EGDs compared to Baveno VI criteria only. Since the risk of variceal bleeding mostly depends upon the size of EV and can be reduced with appropriate medical or endoscopic treatment in patients with high-risk EV (HRV), endoscopic screening of EV is currently the standard of care in patients with cirrhosis.3
Recently, Baveno VI guidelines suggested that esophagogastroduodenoscopy (EGD) can be avoided in patients with compensated advanced chronic liver disease (cACLD) who have a liver stiffness measurement (LSM) <20 kPa and platelet count >150,000/mm3. We aimed to: assess the performance of spleen stiffness measurement (SSM) in ruling out patients with high-risk varices (HRV); validate Baveno VI criteria in a large population and assess how the sequential use of Baveno VI criteria and SSM could safely avoid the need for endoscopy. We retrospectively analyzed 498 patients with cACLD who had undergone LSM/SSM by transient elastography (TE) (FibroScan®), platelet count and EGDs from 2012 to 2016 referred to our tertiary centre. The new combined model was validated internally by a split-validation method, and externally in a prospective multicentre cohort of 115 patients. SSM, LSM, platelet count and Child-Pugh-B were independent predictors of HRV. Applying the newly identified SSM cut-off (≤46 kPa) or Baveno VI criteria, 35.8% and 21.7% of patients in the internal validation cohort could have avoided EGD, with only 2% of HRVs being missed with either model. The combination of SSM with Baveno VI criteria would have avoided an additional 22.5% of EGDs, reaching a final value of 43.8% spared EGDs, with <5% missed HRVs. Results were confirmed in the prospective external validation cohort, as the combined Baveno VI/SSM ≤46 model would have safely spared (0 HRV missed) 37.4% of EGDs, compared to 16.5% when using the Baveno VI criteria alone. A non-invasive prediction model combining SSM with Baveno VI criteria may be useful to rule out HRV and could make it possible to avoid a significantly larger number of unnecessary EGDs compared to Baveno VI criteria only. Recommendations on the use of screening esophagogastroduodenoscopy (EGD) for the detection of HRV in all cirrhotic patients have been issued since the first Baveno consensus workshop in 1992.4However, a large proportion of cirrhotic patients do not present with HRV, thus making endoscopy a non-ideal screening test that is associated with significant costs and patient discomfort.5Accordingly, in the last decade, increased attention has been dedicated to identifying sufficiently accurate non-invasive tests (NITs) that can rule in and rule out CSPH and HRV, reducing or avoiding the use of invasive methods such as HVPG measurement and EGD.6,7
Recently, Baveno VI guidelines suggested that esophagogastroduodenoscopy (EGD) can be avoided in patients with compensated advanced chronic liver disease (cACLD) who have a liver stiffness measurement (LSM) <20 kPa and platelet count >150,000/mm3. We aimed to: assess the performance of spleen stiffness measurement (SSM) in ruling out patients with high-risk varices (HRV); validate Baveno VI criteria in a large population and assess how the sequential use of Baveno VI criteria and SSM could safely avoid the need for endoscopy. We retrospectively analyzed 498 patients with cACLD who had undergone LSM/SSM by transient elastography (TE) (FibroScan®), platelet count and EGDs from 2012 to 2016 referred to our tertiary centre. The new combined model was validated internally by a split-validation method, and externally in a prospective multicentre cohort of 115 patients. SSM, LSM, platelet count and Child-Pugh-B were independent predictors of HRV. Applying the newly identified SSM cut-off (≤46 kPa) or Baveno VI criteria, 35.8% and 21.7% of patients in the internal validation cohort could have avoided EGD, with only 2% of HRVs being missed with either model. The combination of SSM with Baveno VI criteria would have avoided an additional 22.5% of EGDs, reaching a final value of 43.8% spared EGDs, with <5% missed HRVs. Results were confirmed in the prospective external validation cohort, as the combined Baveno VI/SSM ≤46 model would have safely spared (0 HRV missed) 37.4% of EGDs, compared to 16.5% when using the Baveno VI criteria alone. A non-invasive prediction model combining SSM with Baveno VI criteria may be useful to rule out HRV and could make it possible to avoid a significantly larger number of unnecessary EGDs compared to Baveno VI criteria only. The recent 2015 Baveno VI consensus workshop8 highlighted the diagnostic role of NITs such as liver stiffness measurement (LSM) in defining the presence of CSPH and HRV.In particular, the so-called Baveno VI criteria useful in excluding HRV were defined as follows: patients with LSM <20 kPa (assessed by transient elastography [TE]) and a platelet count (PLT) >150,000/mm3 were considered very unlikely to have high-risk varices (<5%), and endoscopy could consequently be safely avoided.The above criteria can also be applied for longitudinal follow-up, prompting screening endoscopy if LSM increases or PLT decreases.8To date, several papers have provided validation of these criteria7,9–14 confirming that Baveno VI criteria correctly identify 98–100% of patients who could safely avoid endoscopy.These criteria have recently also been adopted by the American Association for the Study of the Liver3 with a suggestion to use NIT stratification according to Baveno VI criteria in all patients with a new diagnosis of cirrhosis.
Recently, Baveno VI guidelines suggested that esophagogastroduodenoscopy (EGD) can be avoided in patients with compensated advanced chronic liver disease (cACLD) who have a liver stiffness measurement (LSM) <20 kPa and platelet count >150,000/mm3. We aimed to: assess the performance of spleen stiffness measurement (SSM) in ruling out patients with high-risk varices (HRV); validate Baveno VI criteria in a large population and assess how the sequential use of Baveno VI criteria and SSM could safely avoid the need for endoscopy. We retrospectively analyzed 498 patients with cACLD who had undergone LSM/SSM by transient elastography (TE) (FibroScan®), platelet count and EGDs from 2012 to 2016 referred to our tertiary centre. The new combined model was validated internally by a split-validation method, and externally in a prospective multicentre cohort of 115 patients. SSM, LSM, platelet count and Child-Pugh-B were independent predictors of HRV. Applying the newly identified SSM cut-off (≤46 kPa) or Baveno VI criteria, 35.8% and 21.7% of patients in the internal validation cohort could have avoided EGD, with only 2% of HRVs being missed with either model. The combination of SSM with Baveno VI criteria would have avoided an additional 22.5% of EGDs, reaching a final value of 43.8% spared EGDs, with <5% missed HRVs. Results were confirmed in the prospective external validation cohort, as the combined Baveno VI/SSM ≤46 model would have safely spared (0 HRV missed) 37.4% of EGDs, compared to 16.5% when using the Baveno VI criteria alone. A non-invasive prediction model combining SSM with Baveno VI criteria may be useful to rule out HRV and could make it possible to avoid a significantly larger number of unnecessary EGDs compared to Baveno VI criteria only. A possible limitation of the Baveno VI criteria8 is related to a substantially low number of spared endoscopies (15–25%).9,10Although different authors have tried to modify LSM and PLT cut-offs (25 kPa or 100,000–120,000/mm3), the rate of avoided EGDs did not exceed 30%.11,15Therefore, most patients not presenting with HRVs still require endoscopy according to the Baveno criteria and it is becoming evident that more accurate non-invasive methodologies are needed to improve risk stratification.16
Recently, Baveno VI guidelines suggested that esophagogastroduodenoscopy (EGD) can be avoided in patients with compensated advanced chronic liver disease (cACLD) who have a liver stiffness measurement (LSM) <20 kPa and platelet count >150,000/mm3. We aimed to: assess the performance of spleen stiffness measurement (SSM) in ruling out patients with high-risk varices (HRV); validate Baveno VI criteria in a large population and assess how the sequential use of Baveno VI criteria and SSM could safely avoid the need for endoscopy. We retrospectively analyzed 498 patients with cACLD who had undergone LSM/SSM by transient elastography (TE) (FibroScan®), platelet count and EGDs from 2012 to 2016 referred to our tertiary centre. The new combined model was validated internally by a split-validation method, and externally in a prospective multicentre cohort of 115 patients. SSM, LSM, platelet count and Child-Pugh-B were independent predictors of HRV. Applying the newly identified SSM cut-off (≤46 kPa) or Baveno VI criteria, 35.8% and 21.7% of patients in the internal validation cohort could have avoided EGD, with only 2% of HRVs being missed with either model. The combination of SSM with Baveno VI criteria would have avoided an additional 22.5% of EGDs, reaching a final value of 43.8% spared EGDs, with <5% missed HRVs. Results were confirmed in the prospective external validation cohort, as the combined Baveno VI/SSM ≤46 model would have safely spared (0 HRV missed) 37.4% of EGDs, compared to 16.5% when using the Baveno VI criteria alone. A non-invasive prediction model combining SSM with Baveno VI criteria may be useful to rule out HRV and could make it possible to avoid a significantly larger number of unnecessary EGDs compared to Baveno VI criteria only. In recent years, spleen stiffness measurement (SSM), carried out with various techniques (TE, acoustic radiation force impulse elastography [ARFI], 2D-shear wave elastography [2D-SWE] and magnetic resonance elastography [MRE]), has been proposed by several authors17,18 as an accurate diagnostic tool to assess CSPH and to predict EV presence or absence.In a study performed by our group, SSM was more closely related to the severity of portal hypertension than LSM,19 thus allowing a more accurate prediction of clinical decompensation, as documented in a subsequent study.20Furthermore, it has recently been observed that by using 2D-SWE the sequential use of LSM and SSM leads to an improvement of the non-invasive diagnosis of CSPH, compared to LSM alone21,22 and in combination with Lok-score to diagnose HRV.23
It has been proposed that serum hepatitis B core-related antigen (HBcrAg) reflects intrahepatic covalently closed circular (ccc)DNA levels. However, the correlation of HBcrAg with serum and intrahepatic viral markers and liver histology has not been comprehensively investigated in a large sample. We aimed to determine if HBcrAg could be a useful therapeutic marker in patients with chronic hepatitis B. HBcrAg was measured by chemiluminescent enzyme immunoassay in 130 (36 hepatitis B e antigen [HBeAg]+ and 94 HBeAg−) biopsy proven, untreated, patients with chronic hepatitis B. HBcrAg levels were correlated with: a) serum hepatitis B virus (HBV)-DNA, quantitative hepatitis B surface antigen and alanine aminotransferase levels; b) intrahepatic total (t)HBV-DNA, cccDNA, pregenomic (pg)RNA and cccDNA transcriptional activity (defined as pgRNA/cccDNA ratio); c) fibrosis and necroinflammatory activity scores. HBcrAg levels were significantly higher in HBeAg+ vs. HBeAg− patients and correlated with serum HBV-DNA, intrahepatic tHBV-DNA, pgRNA and cccDNA levels, and transcriptional activity. Patients who were negative for HBcrAg (<3 LogU/ml) had less liver cccDNA and lower cccDNA activity than the HBcrAg+ group. Principal component analysis coupled with unsupervised clustering identified that in a subgroup of HBeAg− patients, higher HBcrAg levels were associated with higher serum HBV-DNA, intrahepatic tHBV-DNA, pgRNA, cccDNA transcriptional activity and with higher fibrosis and necroinflammatory activity scores. Our results indicate that HBcrAg is a surrogate marker of both intrahepatic cccDNA and its transcriptional activity. HBcrAg could be useful in the evaluation of new antiviral therapies aiming at a functional cure of HBV infection either by directly or indirectly targeting the intrahepatic cccDNA pool. The development of novel antiviral agents and immunomodulatory approaches to cure hepatitis B virus (HBV) infection requires new biomarkers capable of reflecting the intrahepatic activity of the virus and chronic HBV (CHB) stages and defining new meaningful treatment endpoints.Indeed, there is an unmet need for standardized assays able to provide mechanistic insights into the effects of the novel antiviral and immunomodulatory agents and to assess treatment efficacy.1
It has been proposed that serum hepatitis B core-related antigen (HBcrAg) reflects intrahepatic covalently closed circular (ccc)DNA levels. However, the correlation of HBcrAg with serum and intrahepatic viral markers and liver histology has not been comprehensively investigated in a large sample. We aimed to determine if HBcrAg could be a useful therapeutic marker in patients with chronic hepatitis B. HBcrAg was measured by chemiluminescent enzyme immunoassay in 130 (36 hepatitis B e antigen [HBeAg]+ and 94 HBeAg−) biopsy proven, untreated, patients with chronic hepatitis B. HBcrAg levels were correlated with: a) serum hepatitis B virus (HBV)-DNA, quantitative hepatitis B surface antigen and alanine aminotransferase levels; b) intrahepatic total (t)HBV-DNA, cccDNA, pregenomic (pg)RNA and cccDNA transcriptional activity (defined as pgRNA/cccDNA ratio); c) fibrosis and necroinflammatory activity scores. HBcrAg levels were significantly higher in HBeAg+ vs. HBeAg− patients and correlated with serum HBV-DNA, intrahepatic tHBV-DNA, pgRNA and cccDNA levels, and transcriptional activity. Patients who were negative for HBcrAg (<3 LogU/ml) had less liver cccDNA and lower cccDNA activity than the HBcrAg+ group. Principal component analysis coupled with unsupervised clustering identified that in a subgroup of HBeAg− patients, higher HBcrAg levels were associated with higher serum HBV-DNA, intrahepatic tHBV-DNA, pgRNA, cccDNA transcriptional activity and with higher fibrosis and necroinflammatory activity scores. Our results indicate that HBcrAg is a surrogate marker of both intrahepatic cccDNA and its transcriptional activity. HBcrAg could be useful in the evaluation of new antiviral therapies aiming at a functional cure of HBV infection either by directly or indirectly targeting the intrahepatic cccDNA pool. HBV covalently closed circular (ccc)DNA constitutes the unique template for pregenomic (pg)RNA transcription and viral genome replication.Its persistence in the nucleus of infected cells is responsible for the chronicity of HBV infection.2So far, antiviral therapies have demonstrated a modest effect on the established cccDNA pool.3–6Measurement of intrahepatic cccDNA levels and transcriptional activity is therefore crucial for the management of patients with CHB and for treatment individualization.The need for liver biopsy strongly limits the evaluation of cccDNA in routine clinical practice.Currently, serum HBV-DNA, hepatitis B surface antigen (HBsAg) and the quantification of HBsAg (qHBsAg) are the most widely used viral markers to diagnose HBV infection and to monitor antiviral therapy.7Nucleos(t)ides analogues (NAs) block viral polymerase and are highly efficient in achieving viral suppression, despite the continuous presence of cccDNA in infected hepatocytes.Thus, the correlation between intrahepatic cccDNA and serum HBV-DNA is lost and serum HBV-DNA quantification cannot be considered a surrogate marker for cccDNA in NA-treated patients.4HBsAg is the hallmark of infection and its clearance is considered to be the most important clinical endpoint7 because both spontaneous and therapy-induced HBsAg loss are associated with histological improvement, a reduced risk of hepatocellular carcinoma and prolonged survival.8–11The degree of correlation between qHBsAg and intrahepatic viral markers, in particular cccDNA levels, varies greatly between studies and is still debated,12–18 particularly in HBeAg− carriers, where expression from HBV integrants may significantly contribute to HBsAg production, in addition to its expression from the cccDNA template,19 as has been shown in HBV infected chimpanzees.20
It has been proposed that serum hepatitis B core-related antigen (HBcrAg) reflects intrahepatic covalently closed circular (ccc)DNA levels. However, the correlation of HBcrAg with serum and intrahepatic viral markers and liver histology has not been comprehensively investigated in a large sample. We aimed to determine if HBcrAg could be a useful therapeutic marker in patients with chronic hepatitis B. HBcrAg was measured by chemiluminescent enzyme immunoassay in 130 (36 hepatitis B e antigen [HBeAg]+ and 94 HBeAg−) biopsy proven, untreated, patients with chronic hepatitis B. HBcrAg levels were correlated with: a) serum hepatitis B virus (HBV)-DNA, quantitative hepatitis B surface antigen and alanine aminotransferase levels; b) intrahepatic total (t)HBV-DNA, cccDNA, pregenomic (pg)RNA and cccDNA transcriptional activity (defined as pgRNA/cccDNA ratio); c) fibrosis and necroinflammatory activity scores. HBcrAg levels were significantly higher in HBeAg+ vs. HBeAg− patients and correlated with serum HBV-DNA, intrahepatic tHBV-DNA, pgRNA and cccDNA levels, and transcriptional activity. Patients who were negative for HBcrAg (<3 LogU/ml) had less liver cccDNA and lower cccDNA activity than the HBcrAg+ group. Principal component analysis coupled with unsupervised clustering identified that in a subgroup of HBeAg− patients, higher HBcrAg levels were associated with higher serum HBV-DNA, intrahepatic tHBV-DNA, pgRNA, cccDNA transcriptional activity and with higher fibrosis and necroinflammatory activity scores. Our results indicate that HBcrAg is a surrogate marker of both intrahepatic cccDNA and its transcriptional activity. HBcrAg could be useful in the evaluation of new antiviral therapies aiming at a functional cure of HBV infection either by directly or indirectly targeting the intrahepatic cccDNA pool. Indeed, during NA therapy, the kinetics of qHBsAg decline are much slower in HBeAg− patients than in HBeAg+ patients and slower than those of serum HBV-DNA, reflecting the pool of infected hepatocytes harboring either cccDNA or integrated viral sequences.21–23Quantification of serum HBV RNAs may represent a novel option to predict virological response to both NAs and interferon,24,25 but their correlation with intrahepatic viral parameters needs further investigation with well-defined assays.As yet, no surrogate serum marker satisfactorily reflects the pool of transcriptionally active cccDNA in the liver.The so-called hepatitis B core-related antigen (HBcrAg) assay utilizes a mixture of monoclonal antibodies isolated from HBV core antigen-immunized mice26 to detect and quantify HBV core antigen (HBcAg), free HBeAg, HBeAg−antibody complex, and the 22 kDa precore protein (p22cr).26,27Several reports suggest that HBcrAg levels correlate with serum HBV-DNA in untreated patients with CHB and might be useful to differentiate HBeAg− patients with active and inactive disease.28–32A correlation between HBcrAg levels and the size of the intrahepatic cccDNA pool has been suggested in cohorts of Asian patients with genotype B/C CHB, either untreated33–35 or undergoing NA therapy.35–37HBcrAg has been also shown to correlate with intrahepatic viral RNA levels in Asian patients treated with NAs.37No studies are available on HBcrAg and intrahepatic viral parameters in HBV genotypes other than B/C and it remains to be defined whether, and to what extent, HBcrAg serum levels reflect the transcriptional activity of cccDNA.
Treatment allocation in patients with hepatocellular carcinoma (HCC) on a background of Child-Pugh B (CP-B) cirrhosis is controversial. Liver resection has been proposed in small series with acceptable outcomes, but data are limited. The aim of this study was to evaluate the outcomes of patients undergoing liver resection for HCC in CP-B cirrhosis, focusing on the surgical risks and survival. Patients were retrospectively pooled from 14 international referral centers from 2002 to 2017. Postoperative and oncological outcomes were investigated. Prediction models for surgical risks, disease-free survival and overall survival were constructed. A total of 253 patients were included, of whom 57.3% of patients had a preoperative platelet count <100,000/mm3, 43.5% had preoperative ascites, and 56.9% had portal hypertension. A minor hepatectomy was most commonly performed (84.6%) and 122 (48.2%) were operated on by minimally invasive surgery (MIS). Ninety-day mortality was 4.3% with 6 patients (2.3%) dying from liver failure. One hundred and eight patients (42.7%) experienced complications, of which the most common was ascites (37.5%). Patients undergoing major hepatectomies had higher 90-day mortality (10.3% vs. 3.3%; p = 0.04) and morbidity rates (69.2% vs. 37.9%; p <0.001). Patients undergoing an open hepatectomy had higher morbidity (52.7% vs. 31.9%; p = 0.001) than those undergoing MIS. A prediction model for surgical risk was constructed (https://childb.shinyapps.io/morbidity/). The 5-year overall survival rate was 47%, and 56.9% of patients experienced recurrence. Prediction models for overall survival (https://childb.shinyapps.io/survival/) and disease-free survival (https://childb.shinyapps.io/DFsurvival/) were constructed. Liver resection should be considered for patients with HCC and CP-B cirrhosis after careful selection according to patient characteristics, tumor pattern and liver function, while aiming to minimize surgical stress. An estimation of the surgical risk and survival advantage may be helpful in treatment allocation, eventually improving postoperative morbidity and achieving safe oncological outcomes. Hepatocellular carcinoma (HCC) is the most common primary liver tumor and the third leading cause of cancer-related deaths worldwide.1,2When feasible, curative options such as liver transplantation (LT) and resection represent the treatment of choice as they offer long-term survival.3,4HCC occurs primarily in patients with underlying liver disease, negatively affecting prognosis and increasing the complexity of treatment;5,6 liver cirrhosis, in fact, is an independent prognostic factor for both short and long-term outcomes, and the assessment of liver function remains critical in the management of patients with HCC as selected treatments may induce collateral liver damage, eventually leading to decompensation.7Child-Pugh classification has been proposed as a scoring system to grade liver function and is currently adopted by most of the available guidelines on HCC treatment.7–13
Treatment allocation in patients with hepatocellular carcinoma (HCC) on a background of Child-Pugh B (CP-B) cirrhosis is controversial. Liver resection has been proposed in small series with acceptable outcomes, but data are limited. The aim of this study was to evaluate the outcomes of patients undergoing liver resection for HCC in CP-B cirrhosis, focusing on the surgical risks and survival. Patients were retrospectively pooled from 14 international referral centers from 2002 to 2017. Postoperative and oncological outcomes were investigated. Prediction models for surgical risks, disease-free survival and overall survival were constructed. A total of 253 patients were included, of whom 57.3% of patients had a preoperative platelet count <100,000/mm3, 43.5% had preoperative ascites, and 56.9% had portal hypertension. A minor hepatectomy was most commonly performed (84.6%) and 122 (48.2%) were operated on by minimally invasive surgery (MIS). Ninety-day mortality was 4.3% with 6 patients (2.3%) dying from liver failure. One hundred and eight patients (42.7%) experienced complications, of which the most common was ascites (37.5%). Patients undergoing major hepatectomies had higher 90-day mortality (10.3% vs. 3.3%; p = 0.04) and morbidity rates (69.2% vs. 37.9%; p <0.001). Patients undergoing an open hepatectomy had higher morbidity (52.7% vs. 31.9%; p = 0.001) than those undergoing MIS. A prediction model for surgical risk was constructed (https://childb.shinyapps.io/morbidity/). The 5-year overall survival rate was 47%, and 56.9% of patients experienced recurrence. Prediction models for overall survival (https://childb.shinyapps.io/survival/) and disease-free survival (https://childb.shinyapps.io/DFsurvival/) were constructed. Liver resection should be considered for patients with HCC and CP-B cirrhosis after careful selection according to patient characteristics, tumor pattern and liver function, while aiming to minimize surgical stress. An estimation of the surgical risk and survival advantage may be helpful in treatment allocation, eventually improving postoperative morbidity and achieving safe oncological outcomes. According to the Barcelona Clinic Liver Cancer (BCLC) algorithm which has been advocated by most as the optimal staging system to predict prognosis and guide treatment for HCC, only patients with early-stage tumors may be considered for liver resection as they are associated with long-term survival; furthermore, preserved liver function is required, namely “Child-Pugh A without any ascites, are considered conditions to obtain optimal outcomes after liver resection”.9Conversely, it is generally agreed that in the setting of Child-Pugh C cirrhosis, patients without significant risk factors should be listed for transplantation according to well-defined inclusion criteria.14Eventually, no clear recommendations are disclosed for patients with HCC and Child-Pugh B cirrhosis; treatment allocation remains difficult and controversial as these patients have an intermediate, partially compromised situation in between well preserved and terminal liver function.15,16Patients fulfilling the selection criteria are generally referred for LT, but are at risk of being removed from the waiting list because of tumor progression or liver decompensation: indeed, despite transplantation being a “definitive” treatment as it removes both the tumor and the liver disease, this is limited to a group of selected patients and worldwide organ availability is scarce.On the other hand, locoregional and systemic therapies are mainly adopted when patients are excluded from LT; in such conditions, survival outcomes have been reported to be worse compared to curative treatments and the hazards of inducing collateral liver damage are still unclear, with some reports disclosing a high chance of decompensation.9,15,16
Treatment allocation in patients with hepatocellular carcinoma (HCC) on a background of Child-Pugh B (CP-B) cirrhosis is controversial. Liver resection has been proposed in small series with acceptable outcomes, but data are limited. The aim of this study was to evaluate the outcomes of patients undergoing liver resection for HCC in CP-B cirrhosis, focusing on the surgical risks and survival. Patients were retrospectively pooled from 14 international referral centers from 2002 to 2017. Postoperative and oncological outcomes were investigated. Prediction models for surgical risks, disease-free survival and overall survival were constructed. A total of 253 patients were included, of whom 57.3% of patients had a preoperative platelet count <100,000/mm3, 43.5% had preoperative ascites, and 56.9% had portal hypertension. A minor hepatectomy was most commonly performed (84.6%) and 122 (48.2%) were operated on by minimally invasive surgery (MIS). Ninety-day mortality was 4.3% with 6 patients (2.3%) dying from liver failure. One hundred and eight patients (42.7%) experienced complications, of which the most common was ascites (37.5%). Patients undergoing major hepatectomies had higher 90-day mortality (10.3% vs. 3.3%; p = 0.04) and morbidity rates (69.2% vs. 37.9%; p <0.001). Patients undergoing an open hepatectomy had higher morbidity (52.7% vs. 31.9%; p = 0.001) than those undergoing MIS. A prediction model for surgical risk was constructed (https://childb.shinyapps.io/morbidity/). The 5-year overall survival rate was 47%, and 56.9% of patients experienced recurrence. Prediction models for overall survival (https://childb.shinyapps.io/survival/) and disease-free survival (https://childb.shinyapps.io/DFsurvival/) were constructed. Liver resection should be considered for patients with HCC and CP-B cirrhosis after careful selection according to patient characteristics, tumor pattern and liver function, while aiming to minimize surgical stress. An estimation of the surgical risk and survival advantage may be helpful in treatment allocation, eventually improving postoperative morbidity and achieving safe oncological outcomes. As a result of recent improvements in preoperative patients' assessment, surgical techniques, and postoperative care, more and more patients with intermediate and advanced HCC or with impaired liver function are referred for liver resection, achieving good short and long-term outcomes.3,17–19The BCLC recommendations have been questioned as they exclude many patients who may benefit from surgery;20–22 stratification of results within each BCLC class according to liver function is warranted as suggested by the guidelines themselves.9In this setting, liver resection in Child-Pugh B cirrhosis has been reported as an alternative to LT with acceptable short- and long-term outcomes.14,23–25Despite this, data is scant and originates primarily from single center case series with a small number of patients, leading to controversy and a lack of shared recommendations.26
Multifocal tumors, developed either from intrahepatic metastasis (IM) or multicentric occurrence (MO), is a distinct feature of hepatocellular carcinoma (HCC). Immunogenomic characterization of multifocal HCC is important for understanding immune escape in different lesions and developing immunotherapy. We combined whole exome/transcriptome sequencing, multiplex immunostaining, immunopeptidomes, T-cell receptor (TCR) sequencing and bioinformatic analyses of 47 tumors from 15 HCC patients with multifocal lesions. IM and MO demonstrated distinct clonal architecture, mutational spectrum and genetic susceptibility. The immune microenvironment also displayed spatiotemporal heterogeneity, such as less T cell and more M2 macrophage infiltration in IM and higher expression of inhibitory immune checkpoints in MO. Similar to mutational profiles, shared neoantigens and TCR repertoires among tumors from the same patients were abundant in IM but scarce in MO. Combining neoantigen prediction and immunopeptidomes identified T-cell specific neoepitopes and achieved a high verification rate in vitro. Immunoediting mainly occurred in MO but not IM, due to the relatively low immune infiltration. HLA LOH, identified in 17% of multifocal HCC, hampered the ability of MHC to present neoantigens, especially in IM. An integrated analysis of Immunoscore, Immunoediting, TCR clonality and HLA LOH of each tumor could stratify patients into two groups with high or low risk of recurrence (P=0.038). Our study comprehensively characterized the genetic structure, neoepitope landscape, T cell profile and immunoediting status that collectively shape tumor evolution, which may optimize personalized immunotherapies for multifocal HCC. Hepatocellular carcinoma (HCC) is the fourth leading cause of cancer-related death and ranks sixth in incidence globally, with 0.25–1 million cases annually [1].Most often, HCC occurs in the background of chronic hepatitis or liver cirrhosis.Surgery is potentially curative but only amenable for early stage patients.Recent breakthroughs in systemic and immune therapies have shown clinical benefits in HCC[2].However, improvements in patient outcomes are modest and long-term survival remains poor.One major challenge is that 41%-75% of HCC patients are initially diagnosed as multifocal tumors, which will increase the difficulty of clinical management and lead to poor prognosis [3].
Multifocal tumors, developed either from intrahepatic metastasis (IM) or multicentric occurrence (MO), is a distinct feature of hepatocellular carcinoma (HCC). Immunogenomic characterization of multifocal HCC is important for understanding immune escape in different lesions and developing immunotherapy. We combined whole exome/transcriptome sequencing, multiplex immunostaining, immunopeptidomes, T-cell receptor (TCR) sequencing and bioinformatic analyses of 47 tumors from 15 HCC patients with multifocal lesions. IM and MO demonstrated distinct clonal architecture, mutational spectrum and genetic susceptibility. The immune microenvironment also displayed spatiotemporal heterogeneity, such as less T cell and more M2 macrophage infiltration in IM and higher expression of inhibitory immune checkpoints in MO. Similar to mutational profiles, shared neoantigens and TCR repertoires among tumors from the same patients were abundant in IM but scarce in MO. Combining neoantigen prediction and immunopeptidomes identified T-cell specific neoepitopes and achieved a high verification rate in vitro. Immunoediting mainly occurred in MO but not IM, due to the relatively low immune infiltration. HLA LOH, identified in 17% of multifocal HCC, hampered the ability of MHC to present neoantigens, especially in IM. An integrated analysis of Immunoscore, Immunoediting, TCR clonality and HLA LOH of each tumor could stratify patients into two groups with high or low risk of recurrence (P=0.038). Our study comprehensively characterized the genetic structure, neoepitope landscape, T cell profile and immunoediting status that collectively shape tumor evolution, which may optimize personalized immunotherapies for multifocal HCC. Multifocal HCC can occur synchronously or metachronously either from intrahepatic metastasis (IM) or multicentric occurrence (MO)[4,5].The discrimination between the two types of multifocal HCC is of great clinical importance, because each type differs significantly in biological behaviors, treatment selection and prognosis [6-8].The IM tumors have undergone metastatic spreading while MO tumors are usually detected in early stages[9,10].In recent studies, next-generation sequencing based characterization of multifocal HCC revealed distinct evolutionary trajectories and inter-tumor genetic heterogeneity between IM and MO, showing a higher discriminating accuracy than traditional clinical and molecular features[4,5].Alternatively, distinct genetic profiles of IM/MO may confer them with different immunogenicity, which may invite diverse immune responses or tolerance and in turn shape tumor evolution.However, the interaction among mutational profiles, immunogenicity, and the host immune status in HCC stays less investigated.
Multifocal tumors, developed either from intrahepatic metastasis (IM) or multicentric occurrence (MO), is a distinct feature of hepatocellular carcinoma (HCC). Immunogenomic characterization of multifocal HCC is important for understanding immune escape in different lesions and developing immunotherapy. We combined whole exome/transcriptome sequencing, multiplex immunostaining, immunopeptidomes, T-cell receptor (TCR) sequencing and bioinformatic analyses of 47 tumors from 15 HCC patients with multifocal lesions. IM and MO demonstrated distinct clonal architecture, mutational spectrum and genetic susceptibility. The immune microenvironment also displayed spatiotemporal heterogeneity, such as less T cell and more M2 macrophage infiltration in IM and higher expression of inhibitory immune checkpoints in MO. Similar to mutational profiles, shared neoantigens and TCR repertoires among tumors from the same patients were abundant in IM but scarce in MO. Combining neoantigen prediction and immunopeptidomes identified T-cell specific neoepitopes and achieved a high verification rate in vitro. Immunoediting mainly occurred in MO but not IM, due to the relatively low immune infiltration. HLA LOH, identified in 17% of multifocal HCC, hampered the ability of MHC to present neoantigens, especially in IM. An integrated analysis of Immunoscore, Immunoediting, TCR clonality and HLA LOH of each tumor could stratify patients into two groups with high or low risk of recurrence (P=0.038). Our study comprehensively characterized the genetic structure, neoepitope landscape, T cell profile and immunoediting status that collectively shape tumor evolution, which may optimize personalized immunotherapies for multifocal HCC. Tumor-infiltrating T lymphocytes (TILs) are directed against tumor cells.Previous studies have shown that HCC patients infiltrated with more CD8+ T cells, less regulatory T cells (Treg) and decreased macrophages had favorable outcome[9,11,12].Data from other cancers have shown that the number of somatic mutations and neoantigens (peptides derived from somatic non-silent mutations that are presented to the immune system) significantly correlated with patient survival[13,14].Of note, tumor genetic heterogeneity has profound impact on immunotherapy, as patients with more clonal neoantigens are more prone to respond to immune checkpoint blockade [14].Meanwhile, TCR repertoires define their capacity to interact with neoepitopes presented on HLA, which may also determine the reaction to immune checkpoint blockade and patient prognosis[15,16].As such, decoding the TCR-neoantigen-HLA complex will help understand the dynamic tumor-immune interactions, bringing new insights into the pathogenesis of IM/MO.
Population-level evidence for the impact of direct-acting antiviral (DAA) therapy on hepatitis C virus (HCV)-related disease burden is lacking. We aimed to evaluate trends in HCV-related decompensated cirrhosis and hepatocellular carcinoma (HCC) hospitalisation, and liver-related and all-cause mortality in the pre-DAA (2001–2014) and DAA therapy (2015–2017) eras in New South Wales, Australia. HCV notifications (1993–2016) were linked to hospital admissions (2001–2017) and mortality (1995–2017). Segmented Poisson regressions and Poisson regression were used to assess the impact of DAA era and factors associated with liver-related mortality, respectively. Among 99,910 people with an HCV notification, 3.8% had a decompensated cirrhosis diagnosis and 1.8% had an HCC diagnosis, while 3.3% and 10.5% died of liver-related and all-cause mortality, respectively. In the pre-DAA era, the number of decompensated cirrhosis and HCC diagnoses, and liver-related and all-cause mortality consistently increased (incidence rate ratios 1.04 [95% CI 1.04–1.05], 1.08 [95% CI 1.07–1.08], 1.07 [95% CI 1.06–1.07], and 1.05 [95% CI 1.04–1.05], respectively) over each 6-monthly band. In the DAA era, decompensated cirrhosis diagnosis and liver-related mortality numbers declined (incidence rate ratios 0.97 [95% CI 0.95–0.99] and 0.96 [95% CI 0.94–0.98], respectively), and HCC diagnosis and all-cause mortality numbers plateaued (incidence rate ratio 1.00 [95% CI 0.97–1.03] and 1.01 [95% CI 1.00–1.02], respectively) over each 6-monthly band. In the DAA era, alcohol-use disorder (AUD) was common in patients diagnosed with decompensated cirrhosis and HCC (65% and 46% had a history of AUD, respectively). AUD was independently associated with liver-related mortality (incidence rate ratio 3.35; 95% CI 3.14–3.58). In the DAA era, there has been a sharp decline in liver disease morbidity and mortality in New South Wales, Australia. AUD remains a major contributor to HCV-related liver disease burden, highlighting the need to address comorbidities. The advent of highly curative, tolerable, short-duration direct-acting antiviral (DAA) therapy for hepatitis C virus (HCV) infection has transformed clinical management, and provided great optimism for the global HCV response.1The World Health Organization (WHO) have developed a global health strategy on viral hepatitis that incorporates key service and impact targets, including declines in HCV-related mortality of 10% by 2020 and 65% by 2030.2
Population-level evidence for the impact of direct-acting antiviral (DAA) therapy on hepatitis C virus (HCV)-related disease burden is lacking. We aimed to evaluate trends in HCV-related decompensated cirrhosis and hepatocellular carcinoma (HCC) hospitalisation, and liver-related and all-cause mortality in the pre-DAA (2001–2014) and DAA therapy (2015–2017) eras in New South Wales, Australia. HCV notifications (1993–2016) were linked to hospital admissions (2001–2017) and mortality (1995–2017). Segmented Poisson regressions and Poisson regression were used to assess the impact of DAA era and factors associated with liver-related mortality, respectively. Among 99,910 people with an HCV notification, 3.8% had a decompensated cirrhosis diagnosis and 1.8% had an HCC diagnosis, while 3.3% and 10.5% died of liver-related and all-cause mortality, respectively. In the pre-DAA era, the number of decompensated cirrhosis and HCC diagnoses, and liver-related and all-cause mortality consistently increased (incidence rate ratios 1.04 [95% CI 1.04–1.05], 1.08 [95% CI 1.07–1.08], 1.07 [95% CI 1.06–1.07], and 1.05 [95% CI 1.04–1.05], respectively) over each 6-monthly band. In the DAA era, decompensated cirrhosis diagnosis and liver-related mortality numbers declined (incidence rate ratios 0.97 [95% CI 0.95–0.99] and 0.96 [95% CI 0.94–0.98], respectively), and HCC diagnosis and all-cause mortality numbers plateaued (incidence rate ratio 1.00 [95% CI 0.97–1.03] and 1.01 [95% CI 1.00–1.02], respectively) over each 6-monthly band. In the DAA era, alcohol-use disorder (AUD) was common in patients diagnosed with decompensated cirrhosis and HCC (65% and 46% had a history of AUD, respectively). AUD was independently associated with liver-related mortality (incidence rate ratio 3.35; 95% CI 3.14–3.58). In the DAA era, there has been a sharp decline in liver disease morbidity and mortality in New South Wales, Australia. AUD remains a major contributor to HCV-related liver disease burden, highlighting the need to address comorbidities. Despite the clear benefits of DAA therapy through HCV infection cure, including improvements in quality of life,3 clinical trials were not designed to evaluate potential longer-term clinical benefits such as risk of decompensated cirrhosis, hepatocellular carcinoma (HCC), liver-related death, and all-cause mortality.The use of sustained virological response to define efficacy and clinical benefit was based on observational studies that demonstrated reductions in advanced liver disease complications and mortality in those achieving cure.4,5The recent Cochrane Collaboration review of DAA therapy6 and subsequent commentary by review authors7 including statements that “the clinical implications of achieving sustained virological response are unclear” and that “there is insufficient evidence to judge if DAAs reduce mortality or other liver related complications from chronic hepatitis C” have created considerable controversy,8–10 but also highlight the need to provide further clinical and population-level evidence of the impact of DAAs on morbidity and mortality.
Population-level evidence for the impact of direct-acting antiviral (DAA) therapy on hepatitis C virus (HCV)-related disease burden is lacking. We aimed to evaluate trends in HCV-related decompensated cirrhosis and hepatocellular carcinoma (HCC) hospitalisation, and liver-related and all-cause mortality in the pre-DAA (2001–2014) and DAA therapy (2015–2017) eras in New South Wales, Australia. HCV notifications (1993–2016) were linked to hospital admissions (2001–2017) and mortality (1995–2017). Segmented Poisson regressions and Poisson regression were used to assess the impact of DAA era and factors associated with liver-related mortality, respectively. Among 99,910 people with an HCV notification, 3.8% had a decompensated cirrhosis diagnosis and 1.8% had an HCC diagnosis, while 3.3% and 10.5% died of liver-related and all-cause mortality, respectively. In the pre-DAA era, the number of decompensated cirrhosis and HCC diagnoses, and liver-related and all-cause mortality consistently increased (incidence rate ratios 1.04 [95% CI 1.04–1.05], 1.08 [95% CI 1.07–1.08], 1.07 [95% CI 1.06–1.07], and 1.05 [95% CI 1.04–1.05], respectively) over each 6-monthly band. In the DAA era, decompensated cirrhosis diagnosis and liver-related mortality numbers declined (incidence rate ratios 0.97 [95% CI 0.95–0.99] and 0.96 [95% CI 0.94–0.98], respectively), and HCC diagnosis and all-cause mortality numbers plateaued (incidence rate ratio 1.00 [95% CI 0.97–1.03] and 1.01 [95% CI 1.00–1.02], respectively) over each 6-monthly band. In the DAA era, alcohol-use disorder (AUD) was common in patients diagnosed with decompensated cirrhosis and HCC (65% and 46% had a history of AUD, respectively). AUD was independently associated with liver-related mortality (incidence rate ratio 3.35; 95% CI 3.14–3.58). In the DAA era, there has been a sharp decline in liver disease morbidity and mortality in New South Wales, Australia. AUD remains a major contributor to HCV-related liver disease burden, highlighting the need to address comorbidities. The Australian Government has provided unrestricted access to subsidized DAA therapy for adults with chronic HCV infection since March 2016.The program incorporates a broad prescriber population, including general practitioners, and has no restrictions based on liver disease stage, or drug and alcohol use.In Australia, around 54,110 patients were treated over the period March 2016 to December 2017, equivalent to 24% of the estimated population with chronic HCV infection (B. Hajarizadeh, personal communication, February 2019).Importantly, in relation to potential prevention of advanced liver disease complications, an estimated 70% of Australian people with HCV-related cirrhosis had received DAA therapy.11DAA uptake in New South Wales, where around one-third of the Australian population reside, is consistent with the overall Australian uptake; approximately 19,200 people were treated in 2016–2017, equivalent to 24% of the estimated population with chronic HCV infection (B. Hajarizadeh, personal communication, February 2019).
Phenotypic and functional natural killer (NK)-cell alterations are well described in chronic hepatitis B virus (cHBV) infection. However, it is largely unknown whether these alterations result from general effects on the overall NK-cell population or the emergence of distinct NK-cell subsets. Human cytomegalovirus (HCMV) is common in cHBV and is associated with the emergence of memory-like NK cells. We aimed to assess the impact of these cells on cHBV infection. To assess the impact of memory-like NK cells on phenotypic and functional alterations in cHBV infection, we performed in-depth analyses of circulating NK cells in 52 patients with cHBV, 45 with chronic hepatitis C virus infection and 50 healthy donors, with respect to their HCMV serostatus. In patients with cHBV/HCMV+, FcεRIγ- memory-like NK cells were present in higher frequencies and with higher prevalence than in healthy donors with HCMV+. This pronounced HCMV-associated memory-like NK-cell expansion could be identified as key determinant of the NK-cell response in cHBV infection. Furthermore, we observed that memory-like NK cells consist of epigenetically distinct subsets and exhibit key metabolic characteristics of long-living cells. Despite ongoing chronic infection, the phenotype of memory-like NK cells was conserved in patients with cHBV/HCMV+. Functional characteristics of memory-like NK cells also remained largely unaffected by cHBV infection with the exception of an increased degranulation capacity in response to CD16 stimulation that was, however, detectable in both memory-like and conventional NK cells. The emergence of HCMV-associated memory-like NK cells shapes the overall NK-cell response in cHBV infection and contributes to a general shift towards CD16-mediated effector functions. Therefore, HCMV coinfection needs to be considered in the design of immunotherapeutic approaches that target NK cells in cHBV. Hepatitis B virus (HBV) is a non-cytopathic DNA virus that triggers immune-mediated liver pathology.It is estimated that 257 million people worldwide are suffering from chronic HBV (cHBV) infection and are therefore at high risk of developing progressive liver disease.The capacity of the immune system to control HBV infection provides a rationale for immunotherapeutic approaches.Direct and indirect roles of natural killer (NK) cells in mediating anti-HBV immunity have been described.1,2For example, in a hydrodynamic injection model of acute HBV infection, a direct antiviral effect of NK cells has been reported.3In that model, NK cells can also indirectly support HBV clearance by positively affecting HBV-specific T-cell responses via interferon γ (IFNγ) secretion.4However, in cHBV infection NK cells exhibit an impaired IFNγ production, consequently leading to reduced non-cytolytic antiviral potential and diminished support of T-cell responses.5,6A landmark study has also shown that despite their reduced cytokine production, NK cells obtained from patients with cHBV displayed a conserved cytotoxic function.This phenomenon has been termed functional dichotomy.5
Phenotypic and functional natural killer (NK)-cell alterations are well described in chronic hepatitis B virus (cHBV) infection. However, it is largely unknown whether these alterations result from general effects on the overall NK-cell population or the emergence of distinct NK-cell subsets. Human cytomegalovirus (HCMV) is common in cHBV and is associated with the emergence of memory-like NK cells. We aimed to assess the impact of these cells on cHBV infection. To assess the impact of memory-like NK cells on phenotypic and functional alterations in cHBV infection, we performed in-depth analyses of circulating NK cells in 52 patients with cHBV, 45 with chronic hepatitis C virus infection and 50 healthy donors, with respect to their HCMV serostatus. In patients with cHBV/HCMV+, FcεRIγ- memory-like NK cells were present in higher frequencies and with higher prevalence than in healthy donors with HCMV+. This pronounced HCMV-associated memory-like NK-cell expansion could be identified as key determinant of the NK-cell response in cHBV infection. Furthermore, we observed that memory-like NK cells consist of epigenetically distinct subsets and exhibit key metabolic characteristics of long-living cells. Despite ongoing chronic infection, the phenotype of memory-like NK cells was conserved in patients with cHBV/HCMV+. Functional characteristics of memory-like NK cells also remained largely unaffected by cHBV infection with the exception of an increased degranulation capacity in response to CD16 stimulation that was, however, detectable in both memory-like and conventional NK cells. The emergence of HCMV-associated memory-like NK cells shapes the overall NK-cell response in cHBV infection and contributes to a general shift towards CD16-mediated effector functions. Therefore, HCMV coinfection needs to be considered in the design of immunotherapeutic approaches that target NK cells in cHBV. The repertoire of human NK cells is altered in human cytomegalovirus seropositive (HCMV+) compared to HCMV seronegative (HCMV-) patients.7–11In particular, expansion of phenotypically and functionally distinct NK-cell subsets can be observed for long periods of time, giving these cells memory-like properties.8,12–16The expanded memory-like NK-cell subsets are characterized by the expression of NKG2C, or CD2, and in cases of homozygous NKG2C (KLRC2) deletion, by the lack of the adaptor protein FcεRIγ and by distinct epigenetic profiles that are similar to CD8 T cells.12,13,17,18Especially the lack of FcεRIγ marks memory-like NK cells that provide superior effector function in response to antibody triggering mediated by the FcγRIII (CD16).12,13
Inherited abnormalities in apolipoprotein E (ApoE) or low-density lipoprotein receptor (LDLR) function result in early onset cardiovascular disease and death. Currently, the only curative therapy available is liver transplantation. Hepatocyte transplantation is a potential alternative; however, physiological levels of hepatocyte engraftment and repopulation require transplanted cells to have a competitive proliferative advantage of over host hepatocytes. Herein, we aimed to test the efficacy and safety of a novel preparative regimen for hepatocyte transplantation. Herein, we used an ApoE-deficient mouse model to test the efficacy of a new regimen for hepatocyte transplantation. We used image-guided external-beam hepatic irradiation targeting the median and right lobes of the liver to enhance cell transplant engraftment. This was combined with administration of the hepatic mitogen GC-1, a thyroid hormone receptor-β agonist mimetic, which was used to promote repopulation. The non-invasive preparative regimen of hepatic irradiation and GC-1 was well-tolerated in ApoE−/− mice. This regimen led to robust liver repopulation by transplanted hepatocytes, which was associated with significant reductions in serum cholesterol levels after transplantation. Additionally, in mice receiving this regimen, ApoE was detected in the circulation 4 weeks after treatment and did not induce an immunological response. Importantly, the normalization of serum cholesterol prevented the formation of atherosclerotic plaques in this model. Significant hepatic repopulation and the cure of dyslipidemia in this model, using a novel and well-tolerated preparative regimen, demonstrate the clinical potential of applying this method to the treatment of inherited metabolic diseases of the liver. Lipid homeostasis requires the uptake of chylomicron remnants and other lipoprotein particles by hepatocytes via binding of apolipoprotein E (ApoE) and apolipoprotein B-100 (ApoB-100) lipoprotein ligands to the low-density lipoprotein (LDL) receptor (LDLR) family.1Inherited loss-of-function mutations in the LDLR, gain-of-function mutations in PCSK9 or ApoB-100 and/or ApoE deficiency all lead to familial hypercholesterolemia (FH) with elevated levels of plasma LDL cholesterol, the main cause of atherosclerotic disease.1–6
Inherited abnormalities in apolipoprotein E (ApoE) or low-density lipoprotein receptor (LDLR) function result in early onset cardiovascular disease and death. Currently, the only curative therapy available is liver transplantation. Hepatocyte transplantation is a potential alternative; however, physiological levels of hepatocyte engraftment and repopulation require transplanted cells to have a competitive proliferative advantage of over host hepatocytes. Herein, we aimed to test the efficacy and safety of a novel preparative regimen for hepatocyte transplantation. Herein, we used an ApoE-deficient mouse model to test the efficacy of a new regimen for hepatocyte transplantation. We used image-guided external-beam hepatic irradiation targeting the median and right lobes of the liver to enhance cell transplant engraftment. This was combined with administration of the hepatic mitogen GC-1, a thyroid hormone receptor-β agonist mimetic, which was used to promote repopulation. The non-invasive preparative regimen of hepatic irradiation and GC-1 was well-tolerated in ApoE−/− mice. This regimen led to robust liver repopulation by transplanted hepatocytes, which was associated with significant reductions in serum cholesterol levels after transplantation. Additionally, in mice receiving this regimen, ApoE was detected in the circulation 4 weeks after treatment and did not induce an immunological response. Importantly, the normalization of serum cholesterol prevented the formation of atherosclerotic plaques in this model. Significant hepatic repopulation and the cure of dyslipidemia in this model, using a novel and well-tolerated preparative regimen, demonstrate the clinical potential of applying this method to the treatment of inherited metabolic diseases of the liver. Until statin treatment became available, mortality rates of patients with heterozygous FH aged 20–29 were up to 125 times greater than in a comparable healthy demographic.7After statins, the mean age of death for homozygous FH increased by almost 14 years to 31.7, and LDL cholesterol (LDL-C) decreased by 10.0–26.4%.1,8PCSK9 inhibitors that increase cell surface LDLR levels are highly potent in LDL-C reduction but are ineffective in the absence of ApoE or LDLR.Other novel treatments such as lipoprotein apheresis and mipomersen (an anti-sense oligonucleotide for ApoB mRNA) are important advances but not ideal for long-term clinical use due to high cost (apheresis: ∼$100,000 per year, requires weekly procedures).9Liver transplantation restores normal lipid metabolism as it restores the entire metabolic pathway for cholesterol but this treatment is limited by the shortage in donor organs, cost, and the need for lifelong immunosuppression.10,11
Cholangiocarcinoma is an aggressive hepatobiliary malignancy originating from biliary tract epithelium. Whether cholangiocarcinoma is responsive to immune checkpoint antibody therapy is unknown, and knowledge of its tumor immune microenvironment is limited. We aimed to characterize tumor-infiltrating lymphocytes (TILs) in cholangiocarcinoma and assess functional effects of targeting checkpoint molecules on TILs. We isolated TILs from resected tumors of patients with cholangiocarcinoma and investigated their compositions compared with their counterparts in tumor-free liver (TFL) tissues and blood, by flow cytometry and immunohistochemistry. We measured expression of immune co-stimulatory and co-inhibitory molecules on TILs, and determined whether targeting these molecules improved ex vivo functions of TILs. Proportions of cytotoxic T cells and natural killer cells were decreased, whereas regulatory T cells were increased in tumors compared with TFL. While regulatory T cells accumulated in tumors, the majority of cytotoxic and helper T cells were sequestered at tumor margins, and natural killer cells were excluded from the tumors. The co-stimulatory receptor GITR and co-inhibitory receptors PD1 and CTLA4 were over-expressed on tumor-infiltrating T cells compared with T cells in TFL and blood. Antagonistic targeting of PD1 or CTLA4 or agonistic targeting of GITR enhanced effector molecule production and T cell proliferation in ex vivo stimulation of TILs derived from cholangiocarcinoma. The inter-individual variations in TIL responses to checkpoint treatments were correlated with differences in TIL immune phenotype. Decreased numbers of cytotoxic immune cells and increased numbers of suppressor T cells that over-express co-inhibitory receptors suggest that the tumor microenvironment in cholangiocarcinoma is immunosuppressive. Targeting GITR, PD1 or CTLA4 enhances effector functions of tumor-infiltrating T cells, indicating that these molecules are potential immunotherapeutic targets for patients with cholangiocarcinoma. Liver cancer is the second most common cause of cancer-related mortality worldwide.1Cholangiocarcinoma (CCA) accounts for 10% of primary liver cancers and the incidence is significantly increasing.CCA is an aggressive hepatobiliary malignancy originating from the biliary tract epithelium with features of cholangiocyte differentiation.2It is classified into the following types according to its anatomic location along the biliary tree: intrahepatic (iCCA), perihilar (pCCA) and distal (dCCA).2,3The median overall survival after diagnosis is 24 months and 5-year survival rate is around 10%.4The current treatment options for CCA are very limited.Surgical resection is potentially curative, but only 10% of patients are eligible for surgical resection and it is associated with a high recurrence rate (>50%).2Liver transplantation is a curative option for selected patients with pCCA but not with iCCA or dCCA.3The therapeutic effect of chemotherapy for advanced CCA is disappointing.2Therefore, more effective therapies for curing CCA and preventing recurrence are urgently needed.
Cholangiocarcinoma is an aggressive hepatobiliary malignancy originating from biliary tract epithelium. Whether cholangiocarcinoma is responsive to immune checkpoint antibody therapy is unknown, and knowledge of its tumor immune microenvironment is limited. We aimed to characterize tumor-infiltrating lymphocytes (TILs) in cholangiocarcinoma and assess functional effects of targeting checkpoint molecules on TILs. We isolated TILs from resected tumors of patients with cholangiocarcinoma and investigated their compositions compared with their counterparts in tumor-free liver (TFL) tissues and blood, by flow cytometry and immunohistochemistry. We measured expression of immune co-stimulatory and co-inhibitory molecules on TILs, and determined whether targeting these molecules improved ex vivo functions of TILs. Proportions of cytotoxic T cells and natural killer cells were decreased, whereas regulatory T cells were increased in tumors compared with TFL. While regulatory T cells accumulated in tumors, the majority of cytotoxic and helper T cells were sequestered at tumor margins, and natural killer cells were excluded from the tumors. The co-stimulatory receptor GITR and co-inhibitory receptors PD1 and CTLA4 were over-expressed on tumor-infiltrating T cells compared with T cells in TFL and blood. Antagonistic targeting of PD1 or CTLA4 or agonistic targeting of GITR enhanced effector molecule production and T cell proliferation in ex vivo stimulation of TILs derived from cholangiocarcinoma. The inter-individual variations in TIL responses to checkpoint treatments were correlated with differences in TIL immune phenotype. Decreased numbers of cytotoxic immune cells and increased numbers of suppressor T cells that over-express co-inhibitory receptors suggest that the tumor microenvironment in cholangiocarcinoma is immunosuppressive. Targeting GITR, PD1 or CTLA4 enhances effector functions of tumor-infiltrating T cells, indicating that these molecules are potential immunotherapeutic targets for patients with cholangiocarcinoma. Cancer immunotherapy aims at stimulating the immune system to combat cancer.T cells are critical immune cells in immune responses against cancer.CD8+ cytotoxic T cells that recognize tumor antigens, can kill tumor cells.CD4+ T helper (Th) lymphocytes that recognize tumor antigens, can provide help to CD8+ T cells, to macrophages to phagocytose tumor cells, and to B cells to produce antibodies against tumor cells.Upon antigen recognition, T cell activation is tightly regulated by 2 types of co-signaling receptor-ligand interactions which are also called ‘immune checkpoint pathways’, which either co-stimulate or co-inhibit T cells.T cells infiltrating into tumors generally express high levels of co-inhibitory receptors, while tumor cells and intratumoral antigen-presenting cells can express ligands for these co-inhibitory receptors.5,6Blocking co-inhibitory programmed cell death protein 1 (PD1)/programmed cell death 1 ligand 1 (PDL1) checkpoint interaction by specific antibodies has recently shown unprecedented and durable therapeutic effects, resulting in long-term patient survival in several types of advanced cancer, including hepatocellular carcinoma (HCC).7In early phase trials the safety and preliminary efficacy of agonistic targeting of co-stimulatory receptors, including GITR, for cancer immunotherapy are being addressed.8However, whether CCA is responsive to checkpoint antibody therapy is as yet unknown, and knowledge of the immune microenvironment in CCA tumors is still very limited.
Cholangiocarcinoma is an aggressive hepatobiliary malignancy originating from biliary tract epithelium. Whether cholangiocarcinoma is responsive to immune checkpoint antibody therapy is unknown, and knowledge of its tumor immune microenvironment is limited. We aimed to characterize tumor-infiltrating lymphocytes (TILs) in cholangiocarcinoma and assess functional effects of targeting checkpoint molecules on TILs. We isolated TILs from resected tumors of patients with cholangiocarcinoma and investigated their compositions compared with their counterparts in tumor-free liver (TFL) tissues and blood, by flow cytometry and immunohistochemistry. We measured expression of immune co-stimulatory and co-inhibitory molecules on TILs, and determined whether targeting these molecules improved ex vivo functions of TILs. Proportions of cytotoxic T cells and natural killer cells were decreased, whereas regulatory T cells were increased in tumors compared with TFL. While regulatory T cells accumulated in tumors, the majority of cytotoxic and helper T cells were sequestered at tumor margins, and natural killer cells were excluded from the tumors. The co-stimulatory receptor GITR and co-inhibitory receptors PD1 and CTLA4 were over-expressed on tumor-infiltrating T cells compared with T cells in TFL and blood. Antagonistic targeting of PD1 or CTLA4 or agonistic targeting of GITR enhanced effector molecule production and T cell proliferation in ex vivo stimulation of TILs derived from cholangiocarcinoma. The inter-individual variations in TIL responses to checkpoint treatments were correlated with differences in TIL immune phenotype. Decreased numbers of cytotoxic immune cells and increased numbers of suppressor T cells that over-express co-inhibitory receptors suggest that the tumor microenvironment in cholangiocarcinoma is immunosuppressive. Targeting GITR, PD1 or CTLA4 enhances effector functions of tumor-infiltrating T cells, indicating that these molecules are potential immunotherapeutic targets for patients with cholangiocarcinoma. Nevertheless, some recent data suggest that CCA tumors might be sensitive to immune checkpoint therapy.First, studies have revealed variable numbers of PD1+ lymphocytes and variable PDL1 expression in CCA tissues,9,10 and both are associated with the clinical course of the disease,11–13, suggesting that the PD1/PDL1 pathway may be an interesting immunotherapeutic target for CCA.Secondly, in order for checkpoint antibody therapy to be effective, T cells must recognize epitopes of tumor antigens on tumor cells and infiltration of T cells into the tumor is needed.14While the majority of CCA tumors contain a limited number of mutations and are therefore expected to express few neoantigens,15 a recent study has demonstrated that tumor-infiltrating lymphocytes (TILs) in CCA contain tumor antigen-reactive T cells and that adoptive transfer of enriched populations of tumor-reactive T cells induced tumor regression.16Thirdly, immune checkpoint therapy is generally only effective in tumors with pre-existing T cell infiltrates,17 and the limited number of studies available have shown CD8+ and CD4+ T cell as well as dendritic cell infiltrations in CCA tumors.12,18,19However, PDL1 expression and defective HLA class I antigen expression by iCCA tumor cells, as well as the presence of regulatory T cells (Tregs) and alternatively activated (M2) macrophages in CCA tumors suggest an immunosuppressive tumor microenvironment which contains immune infiltrates with T cells that can recognize the tumor cells but are immunosuppressive.12,20All these data are based on immunohistochemistry staining of CCA tissues.However, knowledge of phenotypic and functional characteristics of intratumoral lymphocytes in patients with CCA and of expression of immune checkpoint molecules other than PD1 and PDL1, and whether targeting immune checkpoint molecules can improve the functions of CCA TILs is lacking.
Hepatocellular carcinoma (HCC) is a frequent complication of liver disease. When feasible, hepatic resection is the first-choice therapy. However, tumor recurrence complicates at least 2/3 hepatic resections at 5 years. Early recurrences are mainly tumor or treatment-related, but predictors of late recurrences are undefined. We aimed to evaluate the factors related to HCC recurrence after curative resection, with liver and spleen stiffness measurement (LSM and SSM) as markers of severity and duration of the underlying liver disease. We enrolled patients with chronic liver disease and primary HCC suitable for hepatic resection. We followed up patients for at least 30 months or until HCC recurrence. We performed uni- and multivariate analyses to evaluate the predictive role of tumor characteristics, laboratory data, LSM and SSM for both early and late recurrence of HCC. We prospectively enrolled 175 patients. Early HCC recurrence at multivariate analysis was associated with viral etiology, HCC grading (3 or 4), resection margins <1 cm and being beyond the Milan criteria. HCC late recurrence at univariate analysis was associated with esophageal varices (hazard ratio [HR] 3.321, 95% CI 1.564–7.053), spleen length (HR 3.123, 95% CI 1.377–7.081), platelet/spleen length ratio if <909 (HR 2.170, 95% CI 1.026–4.587), LSM (HR 1.036, 95% CI 1.005–1.067), SSM (HR 1.046, 95% CI 1.020–1.073). HCC late recurrence at multivariate analysis was independently associated only with SSM (HR 1.046, CI 1.020–1.073). Late HCC recurrence-free survival was significantly different according to the SSM cut-off of 70 kPa (p = 0.0002). SSM seems to be the only predictor of late HCC recurrence, since it is directly correlated with the degree of liver disease and portal hypertension, both of which are involved in carcinogenesis. Hepatocellular carcinoma (HCC) is a frequent complication in patients with chronic liver diseases, and one of the most common malignancies worldwide.1,2Liver resection is the first option for the treatment of patients with small solitary tumors and preserved liver function.1,2Tumor recurrence complicates 70% of cases of hepatic resection at 5 years, and is the expression of both intrahepatic metastasis (mainly stated as early recurrence) and the development of de novo tumors (late recurrence).3–8
Hepatocellular carcinoma (HCC) is a frequent complication of liver disease. When feasible, hepatic resection is the first-choice therapy. However, tumor recurrence complicates at least 2/3 hepatic resections at 5 years. Early recurrences are mainly tumor or treatment-related, but predictors of late recurrences are undefined. We aimed to evaluate the factors related to HCC recurrence after curative resection, with liver and spleen stiffness measurement (LSM and SSM) as markers of severity and duration of the underlying liver disease. We enrolled patients with chronic liver disease and primary HCC suitable for hepatic resection. We followed up patients for at least 30 months or until HCC recurrence. We performed uni- and multivariate analyses to evaluate the predictive role of tumor characteristics, laboratory data, LSM and SSM for both early and late recurrence of HCC. We prospectively enrolled 175 patients. Early HCC recurrence at multivariate analysis was associated with viral etiology, HCC grading (3 or 4), resection margins <1 cm and being beyond the Milan criteria. HCC late recurrence at univariate analysis was associated with esophageal varices (hazard ratio [HR] 3.321, 95% CI 1.564–7.053), spleen length (HR 3.123, 95% CI 1.377–7.081), platelet/spleen length ratio if <909 (HR 2.170, 95% CI 1.026–4.587), LSM (HR 1.036, 95% CI 1.005–1.067), SSM (HR 1.046, 95% CI 1.020–1.073). HCC late recurrence at multivariate analysis was independently associated only with SSM (HR 1.046, CI 1.020–1.073). Late HCC recurrence-free survival was significantly different according to the SSM cut-off of 70 kPa (p = 0.0002). SSM seems to be the only predictor of late HCC recurrence, since it is directly correlated with the degree of liver disease and portal hypertension, both of which are involved in carcinogenesis. Some studies9–12 recently explored the differences between early and late recurrence and investigated the risk factors for each type of recurrence.Predictive factors for early recurrence, i.e. recurrence within 24 months of surgery, are well established and are mainly tumor- or treatment-related (i.e. tumor size, tumor number, presence of microsatellites and vascular invasion).13By contrast, only poor data are available for the prediction of late recurrence, i.e. recurrence 24 months post-surgery, which is probably related to the evolution of the underlying chronic liver disease.Among the possible predictive factors for late HCC recurrence, the presence and the degree of portal hypertension (PH) could play an important role.In fact clinically significant PH influences the natural history of advanced liver disease, as the degree of PH is directly correlated with the risk of developing complications,14 including HCC.15
Hepatocellular carcinoma (HCC) is a frequent complication of liver disease. When feasible, hepatic resection is the first-choice therapy. However, tumor recurrence complicates at least 2/3 hepatic resections at 5 years. Early recurrences are mainly tumor or treatment-related, but predictors of late recurrences are undefined. We aimed to evaluate the factors related to HCC recurrence after curative resection, with liver and spleen stiffness measurement (LSM and SSM) as markers of severity and duration of the underlying liver disease. We enrolled patients with chronic liver disease and primary HCC suitable for hepatic resection. We followed up patients for at least 30 months or until HCC recurrence. We performed uni- and multivariate analyses to evaluate the predictive role of tumor characteristics, laboratory data, LSM and SSM for both early and late recurrence of HCC. We prospectively enrolled 175 patients. Early HCC recurrence at multivariate analysis was associated with viral etiology, HCC grading (3 or 4), resection margins <1 cm and being beyond the Milan criteria. HCC late recurrence at univariate analysis was associated with esophageal varices (hazard ratio [HR] 3.321, 95% CI 1.564–7.053), spleen length (HR 3.123, 95% CI 1.377–7.081), platelet/spleen length ratio if <909 (HR 2.170, 95% CI 1.026–4.587), LSM (HR 1.036, 95% CI 1.005–1.067), SSM (HR 1.046, 95% CI 1.020–1.073). HCC late recurrence at multivariate analysis was independently associated only with SSM (HR 1.046, CI 1.020–1.073). Late HCC recurrence-free survival was significantly different according to the SSM cut-off of 70 kPa (p = 0.0002). SSM seems to be the only predictor of late HCC recurrence, since it is directly correlated with the degree of liver disease and portal hypertension, both of which are involved in carcinogenesis. The measurement of hepatic venous pressure gradient (HVPG) is the gold standard method used to assess PH, which stratifies the severity and prognosis of patients with chronic liver diseases.HVPG >10 mmHg has been identified as an independent predictive factor for HCC development.15However HVPG is invasive, thus in the last decade, several authors16,17 have tried to assess PH with non-invasive methods.In particular, the role of liver16 and spleen stiffness18–20 (LS and SS) have been investigated as non-invasive markers of PH and its complications.In addition, our research group also identified spleen stiffness measurement (SSM) as a predictor of clinical complications in patients with compensated cirrhosis, including HCC.19
Hepatocellular carcinoma (HCC) is a frequent complication of liver disease. When feasible, hepatic resection is the first-choice therapy. However, tumor recurrence complicates at least 2/3 hepatic resections at 5 years. Early recurrences are mainly tumor or treatment-related, but predictors of late recurrences are undefined. We aimed to evaluate the factors related to HCC recurrence after curative resection, with liver and spleen stiffness measurement (LSM and SSM) as markers of severity and duration of the underlying liver disease. We enrolled patients with chronic liver disease and primary HCC suitable for hepatic resection. We followed up patients for at least 30 months or until HCC recurrence. We performed uni- and multivariate analyses to evaluate the predictive role of tumor characteristics, laboratory data, LSM and SSM for both early and late recurrence of HCC. We prospectively enrolled 175 patients. Early HCC recurrence at multivariate analysis was associated with viral etiology, HCC grading (3 or 4), resection margins <1 cm and being beyond the Milan criteria. HCC late recurrence at univariate analysis was associated with esophageal varices (hazard ratio [HR] 3.321, 95% CI 1.564–7.053), spleen length (HR 3.123, 95% CI 1.377–7.081), platelet/spleen length ratio if <909 (HR 2.170, 95% CI 1.026–4.587), LSM (HR 1.036, 95% CI 1.005–1.067), SSM (HR 1.046, 95% CI 1.020–1.073). HCC late recurrence at multivariate analysis was independently associated only with SSM (HR 1.046, CI 1.020–1.073). Late HCC recurrence-free survival was significantly different according to the SSM cut-off of 70 kPa (p = 0.0002). SSM seems to be the only predictor of late HCC recurrence, since it is directly correlated with the degree of liver disease and portal hypertension, both of which are involved in carcinogenesis. The investigation of the degree of both liver fibrosis and PH with non-invasive tests could thus identify patients at risk of recurrence after resection.In fact, 2 recent studies correlated the degree of pre-resection LS, a marker of liver fibrosis and PH, with the late recurrence of HCC.21,22
An optimal allocation system for scarce resources should simultaneously ensure maximal utility, but also equity. The most frequent principles for allocation policies in liver transplantation are therefore criteria that rely on pre-transplant survival (sickest first policy), post-transplant survival (utility), or on their combination (benefit). However, large differences exist between centers and countries for ethical and legislative reasons. The aim of this study was to report the current worldwide practice of liver graft allocation and discuss respective advantages and disadvantages. Countries around the world that perform 95 or more deceased donor liver transplantations per year were analyzed for donation and allocation policies, as well as recipient characteristics. Most countries use the model for end-stage liver disease (MELD) score, or variations of it, for organ allocation, while some countries opt for center-based allocation systems based on their specific requirements, and some countries combine both a MELD and center-based approach. Both the MELD and center-specific allocation systems have inherent limitations. For example, most countries or allocation systems address the limitations of the MELD system by adding extra points to recipient’s laboratory scores based on clinical information. It is also clear from this study that cancer, as an indication for liver transplantation, requires special attention. The sickest first policy is the most reasonable basis for the allocation of liver grafts. While MELD is currently the standard for this model, many adjustments were implemented in most countries. A future globally applicable strategy should combine donor and recipient factors, predicting probability of death on the waiting list, post-transplant survival and morbidity, and perhaps costs. Liver transplantation (LT) has been undoubtedly one of the most successful procedures developed in the late 20th century, and as a consequence allocation of scarce liver grafts has caused many controversies (Figs. 1, 2).1In the early stages of the procedure, from the 1980s until the mid-1990s, liver grafts were prioritized in the USA based on the degree of sickness and localization of the patients in the hospital.2For example, candidates admitted to an intensive care unit (ICU) received the highest priority, ahead of patients hospitalized in a non-ICU setting and outpatients, somewhat independently of their accumulated waiting time.3This policy carried the obvious risk of spoiling the system by forcing competing centers to keep the candidates on the ICU in order to get priority, when an organ became available.Next to the location of the patients, listing time was an important variable; patients listed early in a compensated stage of liver disease could gain much priority.4As a consequence, a minimal listing criterion was introduced based on the Child-Turcotte-Pugh (CTP) score with a minimum of 7 out of 15 points to qualify for listing.5The introduction of this additional criterion, however, did not reduce the number of listed candidates because waiting time remained the most important recipient variable for organ allocation, until Freeman et al. reported a lack of correlation between waiting time and waiting list mortality.6This led to a change in the paradigm of organ allocation as waiting time ceased to be a key criterion.7
An optimal allocation system for scarce resources should simultaneously ensure maximal utility, but also equity. The most frequent principles for allocation policies in liver transplantation are therefore criteria that rely on pre-transplant survival (sickest first policy), post-transplant survival (utility), or on their combination (benefit). However, large differences exist between centers and countries for ethical and legislative reasons. The aim of this study was to report the current worldwide practice of liver graft allocation and discuss respective advantages and disadvantages. Countries around the world that perform 95 or more deceased donor liver transplantations per year were analyzed for donation and allocation policies, as well as recipient characteristics. Most countries use the model for end-stage liver disease (MELD) score, or variations of it, for organ allocation, while some countries opt for center-based allocation systems based on their specific requirements, and some countries combine both a MELD and center-based approach. Both the MELD and center-specific allocation systems have inherent limitations. For example, most countries or allocation systems address the limitations of the MELD system by adding extra points to recipient’s laboratory scores based on clinical information. It is also clear from this study that cancer, as an indication for liver transplantation, requires special attention. The sickest first policy is the most reasonable basis for the allocation of liver grafts. While MELD is currently the standard for this model, many adjustments were implemented in most countries. A future globally applicable strategy should combine donor and recipient factors, predicting probability of death on the waiting list, post-transplant survival and morbidity, and perhaps costs. Subsequently, the social and political requests for a better allocation system focusing on patient’s medical condition and some notion of justice led to the implementation of the currently widely used allocation policy based on the model for end-stage liver disease (MELD score).8The MELD score is composed of 3 objective and routine biochemical parameters (serum bilirubin, serum creatinine and the international normalized ratio [INR] of prothrombin time, which was originally designed as a predictive tool for survival of patients receiving a transjugular intrahepatic portosystemic shunt [Fig. S1]).9,10The model was subsequently validated in a large cohort of patients suffering from chronic liver disease for the prediction of the 3-month mortality irrespective of the etiology of liver disease or presence of portal hypertension.11
An optimal allocation system for scarce resources should simultaneously ensure maximal utility, but also equity. The most frequent principles for allocation policies in liver transplantation are therefore criteria that rely on pre-transplant survival (sickest first policy), post-transplant survival (utility), or on their combination (benefit). However, large differences exist between centers and countries for ethical and legislative reasons. The aim of this study was to report the current worldwide practice of liver graft allocation and discuss respective advantages and disadvantages. Countries around the world that perform 95 or more deceased donor liver transplantations per year were analyzed for donation and allocation policies, as well as recipient characteristics. Most countries use the model for end-stage liver disease (MELD) score, or variations of it, for organ allocation, while some countries opt for center-based allocation systems based on their specific requirements, and some countries combine both a MELD and center-based approach. Both the MELD and center-specific allocation systems have inherent limitations. For example, most countries or allocation systems address the limitations of the MELD system by adding extra points to recipient’s laboratory scores based on clinical information. It is also clear from this study that cancer, as an indication for liver transplantation, requires special attention. The sickest first policy is the most reasonable basis for the allocation of liver grafts. While MELD is currently the standard for this model, many adjustments were implemented in most countries. A future globally applicable strategy should combine donor and recipient factors, predicting probability of death on the waiting list, post-transplant survival and morbidity, and perhaps costs. Since 2002, the MELD score has been adopted by the United Network for Organ Sharing (UNOS) in the USA, followed by North Italian transplant (2006), Eurotransplant (2006), Canada (2004–2006), France (2007), Switzerland (2007) and other countries with a high number of transplantations such as China and Brazil (Table 1; Figs. 1, 2; Fig. S1).12,13The MELD-based allocation is used by most countries worldwide that perform more than 95 LT per year (Table S1).14In contrast, a center-specific allocation policy remains popular in other parts of the world, especially in areas with high donation rates, such as Portugal and Scandinavia.As a putative advantage, this policy offers transplant centers the degree of freedom to allocate and match the graft to the presumed optimal recipient.Moreover, some countries like Spain and Canada combine the MELD and the center-specific allocation policy with remarkable outcome results.15The UK introduced a new allocation scheme in 2018 based on survival benefit.Priority is given to urgent cases and to those patients on the list with the highest Transplant Benefit Score (TBS), based on the best match of 7 donor and 21 recipient parameters (Table 1; Tables S1, S2; Fig. S1).16
An optimal allocation system for scarce resources should simultaneously ensure maximal utility, but also equity. The most frequent principles for allocation policies in liver transplantation are therefore criteria that rely on pre-transplant survival (sickest first policy), post-transplant survival (utility), or on their combination (benefit). However, large differences exist between centers and countries for ethical and legislative reasons. The aim of this study was to report the current worldwide practice of liver graft allocation and discuss respective advantages and disadvantages. Countries around the world that perform 95 or more deceased donor liver transplantations per year were analyzed for donation and allocation policies, as well as recipient characteristics. Most countries use the model for end-stage liver disease (MELD) score, or variations of it, for organ allocation, while some countries opt for center-based allocation systems based on their specific requirements, and some countries combine both a MELD and center-based approach. Both the MELD and center-specific allocation systems have inherent limitations. For example, most countries or allocation systems address the limitations of the MELD system by adding extra points to recipient’s laboratory scores based on clinical information. It is also clear from this study that cancer, as an indication for liver transplantation, requires special attention. The sickest first policy is the most reasonable basis for the allocation of liver grafts. While MELD is currently the standard for this model, many adjustments were implemented in most countries. A future globally applicable strategy should combine donor and recipient factors, predicting probability of death on the waiting list, post-transplant survival and morbidity, and perhaps costs. An alternative to these allocations models are scores that define a threshold for declining livers to avoid unfavorable risk accumulation in patients with high MELD (balance of risk [BAR], survival outcome following LT [SOFT], product of donor age and MELD [D-MELD]) (Fig. S1).7,17–19The BAR score provides a new and simple scoring system to predict outcome after orthotopic LT with respect to recipient, donor and graft factors.It was calculated on 37,255 patients in the UNOS database and identifies the 6 strongest predictors of post transplantation patient survival.Analysis confirmed the superiority of BAR compared to other score systems like MELD, D-MELD, disease risk index (DRI) and SOFT.The score was validated using the European Liver Transplant Registry (ELTR) database.Compared to other scores, the BAR offers a well-defined cut-off for decision making.
An optimal allocation system for scarce resources should simultaneously ensure maximal utility, but also equity. The most frequent principles for allocation policies in liver transplantation are therefore criteria that rely on pre-transplant survival (sickest first policy), post-transplant survival (utility), or on their combination (benefit). However, large differences exist between centers and countries for ethical and legislative reasons. The aim of this study was to report the current worldwide practice of liver graft allocation and discuss respective advantages and disadvantages. Countries around the world that perform 95 or more deceased donor liver transplantations per year were analyzed for donation and allocation policies, as well as recipient characteristics. Most countries use the model for end-stage liver disease (MELD) score, or variations of it, for organ allocation, while some countries opt for center-based allocation systems based on their specific requirements, and some countries combine both a MELD and center-based approach. Both the MELD and center-specific allocation systems have inherent limitations. For example, most countries or allocation systems address the limitations of the MELD system by adding extra points to recipient’s laboratory scores based on clinical information. It is also clear from this study that cancer, as an indication for liver transplantation, requires special attention. The sickest first policy is the most reasonable basis for the allocation of liver grafts. While MELD is currently the standard for this model, many adjustments were implemented in most countries. A future globally applicable strategy should combine donor and recipient factors, predicting probability of death on the waiting list, post-transplant survival and morbidity, and perhaps costs. The recent extension of transplant indications, for example for malignancy including cholangiocarcinoma, hepatocellular carcinoma (HCC), and colorectal liver metastases, has further aggravated organ shortages, leading to competition in the allocation for liver grafts (Table 2; Table S2; Fig. 3; Fig. S1).4,17,20–23
Tenofovir alafenamide (TAF) is a new prodrug of tenofovir developed to treat patients with chronic hepatitis B virus (HBV) infection at a lower dose than tenofovir disoproxil fumarate (TDF) through more efficient delivery of tenofovir to hepatocytes. In 48-week results from two ongoing, double-blind, randomized phase III trials, TAF was non-inferior to TDF in efficacy with improved renal and bone safety. We report 96-week outcomes for both trials. In two international trials, patients with chronic HBV infection were randomized 2:1 to receive 25 mg TAF or 300 mg TDF in a double-blinded fashion. One study enrolled HBeAg-positive patients and the other HBeAg-negative patients. We assessed efficacy in each study, and safety in the pooled population. At week 96, the differences in the rates of viral suppression were similar in HBeAg-positive patients receiving TAF and TDF (73% vs. 75%, respectively, adjusted difference −2.2% (95% CI −8.3 to 3.9%; p = 0.47), and in HBeAg-negative patients receiving TAF and TDF (90% vs. 91%, respectively, adjusted difference −0.6% (95% CI −7.0 to 5.8%; p = 0.84). In both studies the proportions of patients with alanine aminotransferase above the upper limit of normal at baseline, who had normal alanine aminotransferase at week 96 of treatment, were significantly higher in patients receiving TAF than in those receiving TDF. In the pooled safety population, patients receiving TAF had significantly smaller decreases in bone mineral density than those receiving TDF in the hip (mean % change −0.33% vs. −2.51%; p <0.001) and lumbar spine (mean % change −0.75% vs. −2.57%; p <0.001), as well as a significantly smaller median change in estimated glomerular filtration rate by Cockcroft-Gault method (−1.2 vs. −4.8 mg/dl; p <0.001). In patients with HBV infection, TAF remained as effective as TDF, with continued improved renal and bone safety, two years after the initiation of treatment. Clinicaltrials.gov number: NCT01940471 and NCT01940341. . The World Health Organization estimates that approximately 240 million people worldwide are chronically infected with the hepatitis B virus (HBV).1Without treatment, chronic HBV infection can cause progressive liver fibrosis, which may lead to cirrhosis, decompensation, and hepatocellular carcinoma.2–4Suppressive antiviral treatment has been shown to reduce the risk of liver-related complications, and can halt or even reverse disease progression.5–7However, since few patients achieve seroclearance of the hepatitis B surface antigen (HBsAg), which is considered the hallmark of functional cure, treatment is generally life-long.6–9In an aging population with comorbidities, side effects of treatment such as renal and bone complications can be problematic with long-term treatment.9–13
Tenofovir alafenamide (TAF) is a new prodrug of tenofovir developed to treat patients with chronic hepatitis B virus (HBV) infection at a lower dose than tenofovir disoproxil fumarate (TDF) through more efficient delivery of tenofovir to hepatocytes. In 48-week results from two ongoing, double-blind, randomized phase III trials, TAF was non-inferior to TDF in efficacy with improved renal and bone safety. We report 96-week outcomes for both trials. In two international trials, patients with chronic HBV infection were randomized 2:1 to receive 25 mg TAF or 300 mg TDF in a double-blinded fashion. One study enrolled HBeAg-positive patients and the other HBeAg-negative patients. We assessed efficacy in each study, and safety in the pooled population. At week 96, the differences in the rates of viral suppression were similar in HBeAg-positive patients receiving TAF and TDF (73% vs. 75%, respectively, adjusted difference −2.2% (95% CI −8.3 to 3.9%; p = 0.47), and in HBeAg-negative patients receiving TAF and TDF (90% vs. 91%, respectively, adjusted difference −0.6% (95% CI −7.0 to 5.8%; p = 0.84). In both studies the proportions of patients with alanine aminotransferase above the upper limit of normal at baseline, who had normal alanine aminotransferase at week 96 of treatment, were significantly higher in patients receiving TAF than in those receiving TDF. In the pooled safety population, patients receiving TAF had significantly smaller decreases in bone mineral density than those receiving TDF in the hip (mean % change −0.33% vs. −2.51%; p <0.001) and lumbar spine (mean % change −0.75% vs. −2.57%; p <0.001), as well as a significantly smaller median change in estimated glomerular filtration rate by Cockcroft-Gault method (−1.2 vs. −4.8 mg/dl; p <0.001). In patients with HBV infection, TAF remained as effective as TDF, with continued improved renal and bone safety, two years after the initiation of treatment. Clinicaltrials.gov number: NCT01940471 and NCT01940341. . Tenofovir alafenamide (TAF) is an orally bioavailable prodrug of tenofovir (TFV), a nucleotide analog that inhibits reverse transcription of HIV and HBV.14–16TAF was designed to have greater plasma stability than tenofovir disoproxil fumarate (TDF) allowing delivery of the active metabolite, tenofovir diphosphate, to hepatocytes more efficiently than TDF, which must be dosed at relatively high levels to achieve a therapeutic concentration in hepatic cells.17,18Because of this high systemic exposure of tenofovir (TFV), the long-term use of TDF has been be associated with bone and renal toxicity in some patients.10,13,19–21When TAF is administered at a dose of 25 mg to patients with HBV or HIV infection, circulating levels of TFV were approximately 90% lower than levels with the standard 300 mg dose of TDF.22,23
Alcohol-associated liver disease is a leading indication for liver transplantation and a leading cause of mortality. Alterations to the gut microbiota contribute to the pathogenesis of alcohol-associated liver disease. Patients with alcohol-associated liver disease have increased proportions of Candida spp. in the fecal mycobiome, yet little is known about the effect of intestinal Candida on the disease. Herein, we evaluated the contributions of Candida albicans and its exotoxin candidalysin in alcohol-associated liver disease. C. albicans and the extent of cell elongation 1 (ECE1) were analyzed in fecal samples from controls, patients with alcohol use disorder and those with alcoholic hepatitis. Mice colonized with different and genetically manipulated C. albicans strains were subjected to the chronic-plus-binge ethanol diet model. Primary hepatocytes were isolated and incubated with candidalysin. The percentages of individuals carrying ECE1 were 0%, 4.76% and 30.77% in non-alcoholic controls, patients with alcohol use disorder and patients with alcoholic hepatitis, respectively. Candidalysin exacerbates ethanol-induced liver disease and is associated with increased mortality in mice. Candidalysin enhances ethanol-induced liver disease independently of the β-glucan receptor C-type lectin domain family 7 member A (CLEC7A) on bone marrow-derived cells, and candidalysin does not alter gut barrier function. Candidalysin can damage primary hepatocytes in a dose-dependent manner in vitro and is associated with liver disease severity and mortality in patients with alcoholic hepatitis. Candidalysin is associated with the progression of ethanol-induced liver disease in preclinical models and worse clinical outcomes in patients with alcoholic hepatitis. Alcohol-associated liver disease is one of the most prevalent liver diseases worldwide,1 and the leading cause of liver transplantation in the US.2Alcohol-related liver disease is associated with changes in the intestinal microbiota.Gut dysbiosis induces intestinal inflammation and gut barrier dysfunction, which allows viable bacteria, bacterial (such as lipopolysaccharide [LPS]) and fungal products (such as β-glucan) to translocate to the liver.Bacteria and microbial products bind to pathogen recognition receptors causing an inflammatory response driven by resident Kupffer cells and an infiltration of macrophages.Although many efforts have been made to evaluate the role of the bacterial microbiota in alcohol-associated liver disease, the interaction between fungi and their host, and especially their contribution to alcohol-associated liver disease remains poorly understood.We have shown that β-glucan, a cell wall component of many commensal fungi, binds to C-type lectin domain family 7 member A (CLEC7A; also known as DECTIN1) on hepatic macrophages to release interleukin-1β (IL-1β) and increase ethanol-induced liver disease in mice.3
Interventions aimed at lifestyle changes are pivotal for the treatment of non-alcoholic fatty liver disease (NAFLD), and web-based programs might help remove barriers in both patients and therapists. In the period 2010–15, 716 consecutive NAFLD cases (mean age, 52; type 2 diabetes, 33%) were treated in our Department with structured programs. The usual protocol included motivational interviewing and a group-based intervention (GBI), chaired by physicians, dietitians and psychologists (five weekly meetings, n = 438). Individuals who could not attend GBI entered a web-based intervention (WBI, n = 278) derived from GBI, with interactive games, learning tests, motivational tests, and mail contacts with the center. The primary outcome was weight loss ≥10%; secondary outcomes were alanine aminotransferase within normal limits, changes in lifestyle, weight, alanine aminotransferase, and surrogate markers of steatosis and fibrosis. GBI and WBI cohorts had similar body mass index (mean, 33 kg/m2), with more males (67% vs. 45%), younger age, higher education, and more physical activity in the WBI group. The two-year attrition rate was higher in the WBI group. Healthy lifestyle changes were observed in both groups and body mass index decreased by almost two points; the 10% weight target was reached in 20% of WBI cases vs. 15% in GBI (not significant). In logistic regression analysis, after adjustment for confounders and attrition rates, WBI was not associated with a reduction of patients reaching short- and long-term 10% weight targets. Liver enzymes decreased in both groups, and normalized more frequently in WBI. Fatty liver index was reduced, whereas fibrosis remained stable (NAFLD fibrosis score) or similarly decreased (Fib-4). WBI is not less effective than common lifestyle programs, as measured by significant clinical outcomes associated with improved histological outcomes in NAFLD. eHealth programs may effectively contribute to NAFLD control. The burden associated with non-alcoholic fatty liver disease (NAFLD) is becoming a major problem for health systems worldwide.1As part of the metabolic syndrome, NAFLD prevalence is increasing in parallel with the epidemics of obesity and diabetes;2 although in most cases NAFLD remains a non-progressive disease, in some cases non-alcoholic steatohepatitis (NASH) and progressive fibrosis may occur, finally progressing to cirrhosis and hepatocellular carcinoma.3Thus, the costs associated with liver disease of metabolic origin and its complications are likely to soon outweigh the costs of liver diseases of viral origin.4
Interventions aimed at lifestyle changes are pivotal for the treatment of non-alcoholic fatty liver disease (NAFLD), and web-based programs might help remove barriers in both patients and therapists. In the period 2010–15, 716 consecutive NAFLD cases (mean age, 52; type 2 diabetes, 33%) were treated in our Department with structured programs. The usual protocol included motivational interviewing and a group-based intervention (GBI), chaired by physicians, dietitians and psychologists (five weekly meetings, n = 438). Individuals who could not attend GBI entered a web-based intervention (WBI, n = 278) derived from GBI, with interactive games, learning tests, motivational tests, and mail contacts with the center. The primary outcome was weight loss ≥10%; secondary outcomes were alanine aminotransferase within normal limits, changes in lifestyle, weight, alanine aminotransferase, and surrogate markers of steatosis and fibrosis. GBI and WBI cohorts had similar body mass index (mean, 33 kg/m2), with more males (67% vs. 45%), younger age, higher education, and more physical activity in the WBI group. The two-year attrition rate was higher in the WBI group. Healthy lifestyle changes were observed in both groups and body mass index decreased by almost two points; the 10% weight target was reached in 20% of WBI cases vs. 15% in GBI (not significant). In logistic regression analysis, after adjustment for confounders and attrition rates, WBI was not associated with a reduction of patients reaching short- and long-term 10% weight targets. Liver enzymes decreased in both groups, and normalized more frequently in WBI. Fatty liver index was reduced, whereas fibrosis remained stable (NAFLD fibrosis score) or similarly decreased (Fib-4). WBI is not less effective than common lifestyle programs, as measured by significant clinical outcomes associated with improved histological outcomes in NAFLD. eHealth programs may effectively contribute to NAFLD control. Several drugs are under investigation to stop NAFLD progression,5 but none have been approved so far by regulatory agencies.Like any non-communicable disease, lifestyle changes remain the cornerstone of NAFLD prevention and treatment, and are also the background treatment suggested by all clinical practice guidelines, including the recent European guidelines shared by the Liver, Diabetes and Obesity Societies.6
Interventions aimed at lifestyle changes are pivotal for the treatment of non-alcoholic fatty liver disease (NAFLD), and web-based programs might help remove barriers in both patients and therapists. In the period 2010–15, 716 consecutive NAFLD cases (mean age, 52; type 2 diabetes, 33%) were treated in our Department with structured programs. The usual protocol included motivational interviewing and a group-based intervention (GBI), chaired by physicians, dietitians and psychologists (five weekly meetings, n = 438). Individuals who could not attend GBI entered a web-based intervention (WBI, n = 278) derived from GBI, with interactive games, learning tests, motivational tests, and mail contacts with the center. The primary outcome was weight loss ≥10%; secondary outcomes were alanine aminotransferase within normal limits, changes in lifestyle, weight, alanine aminotransferase, and surrogate markers of steatosis and fibrosis. GBI and WBI cohorts had similar body mass index (mean, 33 kg/m2), with more males (67% vs. 45%), younger age, higher education, and more physical activity in the WBI group. The two-year attrition rate was higher in the WBI group. Healthy lifestyle changes were observed in both groups and body mass index decreased by almost two points; the 10% weight target was reached in 20% of WBI cases vs. 15% in GBI (not significant). In logistic regression analysis, after adjustment for confounders and attrition rates, WBI was not associated with a reduction of patients reaching short- and long-term 10% weight targets. Liver enzymes decreased in both groups, and normalized more frequently in WBI. Fatty liver index was reduced, whereas fibrosis remained stable (NAFLD fibrosis score) or similarly decreased (Fib-4). WBI is not less effective than common lifestyle programs, as measured by significant clinical outcomes associated with improved histological outcomes in NAFLD. eHealth programs may effectively contribute to NAFLD control. Programs to promote lifestyle changes have been developed in the community, mainly in the area of obesity and diabetes, following the seminal Finnish Diabetes Prevention Study and U.S. Diabetes Prevention Program.Their effectiveness in promoting weight loss has been demonstrated on long-term follow-up.7Programs of cognitive-behavioral therapy have also been applied in NAFLD;8 weight loss through healthy and restrictive diet, coupled with habitual physical activity, has been reported to reduce NAFLD progression in small pilot trials,9–11 and a large prospective intervention study confirmed that weight loss is associated with histologic improvement on repeated liver biopsy.12Unfortunately, these programs require dedicated teams and support, which are rarely found in liver units.13In addition, it may be difficult to engage asymptomatic, scarcely motivated patients with NAFLD in intensive lifestyle protocols, because of space and time constraints.14eHealth technology is a possible resource to promote behavior changes,15 thus reducing NAFLD progression.The possibility to educate, to counsel and to induce permanent changes in motivated and engaged patients with NAFLD via an internet-based approach would reduce attendance to busy liver units, sparing patients’ and physicians’ time, and would expand lifestyle intervention to a much larger community.13
The immunogenomic characteristics of hepatocellular carcinomas (HCCs) with immune cell stroma (HCC-IS), defined histologically, have not been clarified. We investigated the clinical and molecular features of HCC-IS and the prognostic impact of Epstein-Barr virus (EBV) infection. We evaluated 219 patients with conventional HCC (C-HCC) and 47 with HCC-IS using in situ hybridization for EBV, immunohistochemistry, multiplex immunofluorescence staining, and whole exome and transcriptome sequencing. Human leukocyte antigen types were also extracted from the sequencing data. Genomic and prognostic parameters were compared between HCC-IS and C-HCC. Results: CD8 T cell infiltration was more frequent in HCC-IS than C-HCC (mean fraction/sample, 22.6% vs. 8.9%, false discovery rate q <0.001), as was EBV positivity in CD20-positive tumor-infiltrating lymphocytes (TILs) (74.5% vs. 4.6%, p <0.001). CTNNB1 mutations were not identified in any HCC-IS, while they were present in 24.1% of C-HCC (p = 0.016). Inhibitory and stimulatory immune modulators were expressed at similar levels in HCC-IS and EBV-positive C-HCC. Global hypermethylation, and expression of PD-1 and PD-L1 in TILs, and PD-L1 in tumors, were also associated with HCC-IS (p <0.001), whereas human leukocyte antigen type did not differ according to HCC type or EBV positivity. HCC-IS was an independent factor for favorable recurrence-free survival (adjusted hazard ratio [aHR] 0.23; p = 0.002). However, a subgroup of tumors with a high density of EBV-positive TILs had poorer recurrence-free (aHR 25.48; p <0.001) and overall (aHR 9.6; p = 0.003) survival, and significant enrichment of CD8 T cell exhaustion signatures (q = 0.0296). HCC-IS is a distinct HCC subtype associated with a good prognosis and frequent EBV-positive TILs. However, paradoxically, a high density of EBV-positive TILs in tumors is associated with inferior prognostic outcomes. Patients with HCC-IS could be candidates for immunotherapy. Hepatocellular carcinomas (HCCs) are biologically heterogeneous at the molecular level, and thus diverse molecular subgroups based on large-scale exome or transcriptome profiling have been identified.1–4This oncologic background may prevent novel molecular-targeted agents from significantly improving survival rates in patients with HCC.5Immune checkpoint inhibitors, including anti-programmed cell death 1 (PD-1) and anti-programmed cell death 1 ligand 1 (PD-L1), are emerging as potential anti-HCC agents,6,7 and a unique immune class specific for HCC was recently defined based on gene expression profiling of tumor tissues.8–10
The immunogenomic characteristics of hepatocellular carcinomas (HCCs) with immune cell stroma (HCC-IS), defined histologically, have not been clarified. We investigated the clinical and molecular features of HCC-IS and the prognostic impact of Epstein-Barr virus (EBV) infection. We evaluated 219 patients with conventional HCC (C-HCC) and 47 with HCC-IS using in situ hybridization for EBV, immunohistochemistry, multiplex immunofluorescence staining, and whole exome and transcriptome sequencing. Human leukocyte antigen types were also extracted from the sequencing data. Genomic and prognostic parameters were compared between HCC-IS and C-HCC. Results: CD8 T cell infiltration was more frequent in HCC-IS than C-HCC (mean fraction/sample, 22.6% vs. 8.9%, false discovery rate q <0.001), as was EBV positivity in CD20-positive tumor-infiltrating lymphocytes (TILs) (74.5% vs. 4.6%, p <0.001). CTNNB1 mutations were not identified in any HCC-IS, while they were present in 24.1% of C-HCC (p = 0.016). Inhibitory and stimulatory immune modulators were expressed at similar levels in HCC-IS and EBV-positive C-HCC. Global hypermethylation, and expression of PD-1 and PD-L1 in TILs, and PD-L1 in tumors, were also associated with HCC-IS (p <0.001), whereas human leukocyte antigen type did not differ according to HCC type or EBV positivity. HCC-IS was an independent factor for favorable recurrence-free survival (adjusted hazard ratio [aHR] 0.23; p = 0.002). However, a subgroup of tumors with a high density of EBV-positive TILs had poorer recurrence-free (aHR 25.48; p <0.001) and overall (aHR 9.6; p = 0.003) survival, and significant enrichment of CD8 T cell exhaustion signatures (q = 0.0296). HCC-IS is a distinct HCC subtype associated with a good prognosis and frequent EBV-positive TILs. However, paradoxically, a high density of EBV-positive TILs in tumors is associated with inferior prognostic outcomes. Patients with HCC-IS could be candidates for immunotherapy. However, the immunogenic subgroup of HCC defined by gene expression profiling may not always correspond to the immunogenic subgroup defined histologically.In fact, immune cells act in a variety of locations and under various conditions, such as on the periphery of tumors, and as secondary inflammatory reactions to bile duct obstruction and/or ischemic tumor cell necrosis, so that histologic evaluation of tumor tissue is useful for identifying true intra-tumor immune cell infiltration.11Therefore, a combined analysis of histology and gene expression in tumors and their microenvironments should be able to identify more reliable immunogenic subtypes of HCCs.Moreover, the subgroup of HCC with marked immune cell infiltration defined histologically has been reported to have favorable prognosis.12,13However, the microenvironmental and genomic features of this HCC subgroup with immune cell stroma (HCC-IS) have not been identified.
The immunogenomic characteristics of hepatocellular carcinomas (HCCs) with immune cell stroma (HCC-IS), defined histologically, have not been clarified. We investigated the clinical and molecular features of HCC-IS and the prognostic impact of Epstein-Barr virus (EBV) infection. We evaluated 219 patients with conventional HCC (C-HCC) and 47 with HCC-IS using in situ hybridization for EBV, immunohistochemistry, multiplex immunofluorescence staining, and whole exome and transcriptome sequencing. Human leukocyte antigen types were also extracted from the sequencing data. Genomic and prognostic parameters were compared between HCC-IS and C-HCC. Results: CD8 T cell infiltration was more frequent in HCC-IS than C-HCC (mean fraction/sample, 22.6% vs. 8.9%, false discovery rate q <0.001), as was EBV positivity in CD20-positive tumor-infiltrating lymphocytes (TILs) (74.5% vs. 4.6%, p <0.001). CTNNB1 mutations were not identified in any HCC-IS, while they were present in 24.1% of C-HCC (p = 0.016). Inhibitory and stimulatory immune modulators were expressed at similar levels in HCC-IS and EBV-positive C-HCC. Global hypermethylation, and expression of PD-1 and PD-L1 in TILs, and PD-L1 in tumors, were also associated with HCC-IS (p <0.001), whereas human leukocyte antigen type did not differ according to HCC type or EBV positivity. HCC-IS was an independent factor for favorable recurrence-free survival (adjusted hazard ratio [aHR] 0.23; p = 0.002). However, a subgroup of tumors with a high density of EBV-positive TILs had poorer recurrence-free (aHR 25.48; p <0.001) and overall (aHR 9.6; p = 0.003) survival, and significant enrichment of CD8 T cell exhaustion signatures (q = 0.0296). HCC-IS is a distinct HCC subtype associated with a good prognosis and frequent EBV-positive TILs. However, paradoxically, a high density of EBV-positive TILs in tumors is associated with inferior prognostic outcomes. Patients with HCC-IS could be candidates for immunotherapy. Among many tumors with marked lymphoid cell infiltration, a few epithelial carcinomas, such as lymphoepithelioma-like gastric cancers and head & neck undifferentiated carcinomas, develop in response to Epstein-Barr virus (EBV), and those tumors typically contain a lymphoid-rich stroma.14,15EBV positivity in these EBV-driven epithelial carcinomas was found only in the epithelial cancer cells, not the lymphocytic cells.14,15However, there have been no convincing reports of HCC infected by EBV, and moreover the status of EBV in tumor-infiltrating lymphocytes (TILs) is unclear.Considering that EBV-infected B lymphocytes can induce cellular immune responses,16 identifying EBV particles in TILs and determining their clinical role may be keys to understanding the pathogenesis of the HCC-IS subtype.
In 2015, the World Health Organization (WHO) issued guidelines for the management of chronic hepatitis B (CHB) in low- and middle-income countries, but little is known about the applicability of the WHO treatment criteria in sub-Saharan Africa. The aim of this study was to evaluate the diagnostic performance of the WHO guidelines in a large CHB cohort in Ethiopia. Treatment-naïve adults who attended a public CHB clinic in Addis Ababa were included in this analysis. All patients underwent a standardized evaluation at recruitment, including blood tests and transient elastography (Fibroscan®). A Fibroscan result >7.9 kPa was used to define significant fibrosis and >9.9 kPa to define cirrhosis. Treatment eligibility was assessed using the most recent guidelines from the European Association for the Study of the Liver (EASL) as the ‘gold standard’. Out of 1,190 patients with CHB, 300 (25.2%) were eligible for treatment based on the EASL 2017 guidelines and 182 (15.3%) based on the WHO 2015 guidelines. The sensitivity and specificity of the WHO criteria were 49.0 and 96.1%, respectively. Most patients (94 of 182; 51.6%) who fulfilled the WHO criteria had decompensated cirrhosis and might have a dismal prognosis even with therapy. Only 41 of 115 patients (35.7%) with compensated cirrhosis, who are likely to benefit the most from therapy, were eligible for treatment based on the WHO criteria. The WHO guidelines for CHB failed to detect half of the patients in need of treatment in Ethiopia, implying the need for a revision of the WHO treatment criteria. Chronic infection with hepatitis B virus (HBV) continues to be a significant health problem globally.Worldwide, around 2 billion people have evidence of past or present infection with HBV and an estimated 257 million are chronically infected.1,2Almost half of the world’s population resides in areas of high HBV endemicity, with the highest prevalence in Africa and East Asia.In sub-Saharan Africa, 5–10% of the adult population is living with chronic hepatitis B (CHB).3Annually, an estimated 887,000 deaths occur as a result of CHB, mainly due to its late complications viz cirrhosis and hepatocellular carcinoma (HCC).1Between 1990 and 2013 the number of HBV-related deaths due to liver cirrhosis and/or HCC increased by 33% globally.4
In 2015, the World Health Organization (WHO) issued guidelines for the management of chronic hepatitis B (CHB) in low- and middle-income countries, but little is known about the applicability of the WHO treatment criteria in sub-Saharan Africa. The aim of this study was to evaluate the diagnostic performance of the WHO guidelines in a large CHB cohort in Ethiopia. Treatment-naïve adults who attended a public CHB clinic in Addis Ababa were included in this analysis. All patients underwent a standardized evaluation at recruitment, including blood tests and transient elastography (Fibroscan®). A Fibroscan result >7.9 kPa was used to define significant fibrosis and >9.9 kPa to define cirrhosis. Treatment eligibility was assessed using the most recent guidelines from the European Association for the Study of the Liver (EASL) as the ‘gold standard’. Out of 1,190 patients with CHB, 300 (25.2%) were eligible for treatment based on the EASL 2017 guidelines and 182 (15.3%) based on the WHO 2015 guidelines. The sensitivity and specificity of the WHO criteria were 49.0 and 96.1%, respectively. Most patients (94 of 182; 51.6%) who fulfilled the WHO criteria had decompensated cirrhosis and might have a dismal prognosis even with therapy. Only 41 of 115 patients (35.7%) with compensated cirrhosis, who are likely to benefit the most from therapy, were eligible for treatment based on the WHO criteria. The WHO guidelines for CHB failed to detect half of the patients in need of treatment in Ethiopia, implying the need for a revision of the WHO treatment criteria. CHB has a variable spectrum of disease and its natural history ranges from an inactive carrier state with excellent prognosis to progressive liver fibrosis leading to cirrhosis with end-stage liver disease and a markedly increased risk of HCC.The natural course depends on both host and viral factors, and in the absence of treatment an estimated 15–40% of patients infected with HBV will die prematurely.5The challenge in clinical practice, therefore, is to avoid unnecessary treatment in patients who have a benign course even without therapy, and to reliably identify patients at risk of developing complications so that antiviral treatment can be timely initiated.By achieving a sustained suppression of HBV viral load levels in patients with progressive liver disease, it has been shown that liver fibrosis can be reversed and the risk of cirrhosis, liver failure, HCC and death markedly reduced.6–9
In 2015, the World Health Organization (WHO) issued guidelines for the management of chronic hepatitis B (CHB) in low- and middle-income countries, but little is known about the applicability of the WHO treatment criteria in sub-Saharan Africa. The aim of this study was to evaluate the diagnostic performance of the WHO guidelines in a large CHB cohort in Ethiopia. Treatment-naïve adults who attended a public CHB clinic in Addis Ababa were included in this analysis. All patients underwent a standardized evaluation at recruitment, including blood tests and transient elastography (Fibroscan®). A Fibroscan result >7.9 kPa was used to define significant fibrosis and >9.9 kPa to define cirrhosis. Treatment eligibility was assessed using the most recent guidelines from the European Association for the Study of the Liver (EASL) as the ‘gold standard’. Out of 1,190 patients with CHB, 300 (25.2%) were eligible for treatment based on the EASL 2017 guidelines and 182 (15.3%) based on the WHO 2015 guidelines. The sensitivity and specificity of the WHO criteria were 49.0 and 96.1%, respectively. Most patients (94 of 182; 51.6%) who fulfilled the WHO criteria had decompensated cirrhosis and might have a dismal prognosis even with therapy. Only 41 of 115 patients (35.7%) with compensated cirrhosis, who are likely to benefit the most from therapy, were eligible for treatment based on the WHO criteria. The WHO guidelines for CHB failed to detect half of the patients in need of treatment in Ethiopia, implying the need for a revision of the WHO treatment criteria. Various international liver societies have issued guidelines for the treatment of CHB.10–13Generally, treatment is recommended in patients with moderate or severe liver inflammation and/or fibrosis and ongoing viral replication.Although there are some differences, all guidelines base the decision to commence treatment on a combined assessment of liver fibrosis stage, serum level of alanine aminotransferase (ALT), and HBV viral load.In 2015, the World Health Organization (WHO) published guidelines for the prevention, care and treatment for persons with CHB, with an emphasis on resource-limited settings.14However, most of the evidence used to develop the WHO guidelines was based on studies from Asia, North America and Western Europe, and little is known about the accuracy and applicability of the WHO treatment criteria in sub-Saharan Africa.
To evaluate the hypothesis that increasing T cell frequency and activity may provide durable control of hepatitis B virus (HBV), we administered nivolumab, a programmed death receptor 1 (PD-1) inhibitor, with or without GS-4774, an HBV therapeutic vaccine, in virally suppressed patients with HBV e antigen (HBeAg)-negative chronic HBV. In a phase Ib study, patients received either a single dose of nivolumab at 0.1 mg/kg (n = 2) or 0.3 mg/kg (n = 12), or 40 yeast units of GS-4774 at baseline and week 4 and 0.3 mg/kg of nivolumab at week 4 (n = 10). The primary efficacy endpoint was mean change in HBV surface antigen (HBsAg) 12 weeks after nivolumab. Safety and immunologic changes were assessed through week 24. There were no grade 3 or 4 adverse events or serious adverse events. All assessed patients retained T cell PD-1 receptor occupancy 6–12 weeks post-infusion, with a mean total across 0.1 and 0.3 mg/kg cohorts of 76% (95% CI 75–77), and no significant differences were observed between cohorts (p = 0.839). Patients receiving 0.3 mg/kg nivolumab without and with GS-4774 had mean declines of −0.30 (95% CI −0.46 to −0.14) and −0.16 (95% CI −0.33 to 0.01) log10 IU/ml, respectively. Patients showed significant HBsAg declines from baseline (p = 0.035) with 3 patients experiencing declines of >0.5 log10 by the end of study. One patient, whose HBsAg went from baseline 1,173 IU/ml to undetectable at week 20, experienced an alanine aminotransferase flare (grade 3) at week 4 that resolved by week 8 and was accompanied by a significant increase in peripheral HBsAg-specific T cells at week 24. In virally suppressed HBeAg-negative patients, checkpoint blockade was well-tolerated and led to HBsAg decline in most patients and sustained HBsAg loss in 1 patient. More than 240 million people worldwide are chronically infected with the hepatitis B virus (HBV)1,2.In the Asia-Pacific region, where it is most prevalent, chronic HBV infection is the leading cause of cirrhosis, liver failure, and hepatocellular carcinoma.2–6While currently approved oral nucleos(t)ide analogs effectively suppress viral replication, providing important clinical benefits, there are several disadvantages of this therapeutic approach.First, since antiviral therapy is rarely curative, most patients must receive life-long treatment with the attendant cost, cumulative toxicity, and risk of breakthrough through either non-adherence or the emergence of antiviral resistance.Moreover, viral suppression with nucleos(t)ide analogs does not eliminate the risk of hepatocellular carcinoma.7A finite course of treatment that can provide sustained off-treatment HBV suppression and clinical response is a clear unmet medical need.Current guidelines recognize clearance of the hepatitis B surface antigen (HBsAg) from the patient’s serum as a so-called “functional cure” and can be distinguished from a “sterilizing cure” which requires the elimination of the covalently closed circular DNA (cccDNA) nuclear reservoir of the virus.5,8,9Functional cure has been associated with improved outcomes, however, and is a current goal of curative approaches for HBV.10
To evaluate the hypothesis that increasing T cell frequency and activity may provide durable control of hepatitis B virus (HBV), we administered nivolumab, a programmed death receptor 1 (PD-1) inhibitor, with or without GS-4774, an HBV therapeutic vaccine, in virally suppressed patients with HBV e antigen (HBeAg)-negative chronic HBV. In a phase Ib study, patients received either a single dose of nivolumab at 0.1 mg/kg (n = 2) or 0.3 mg/kg (n = 12), or 40 yeast units of GS-4774 at baseline and week 4 and 0.3 mg/kg of nivolumab at week 4 (n = 10). The primary efficacy endpoint was mean change in HBV surface antigen (HBsAg) 12 weeks after nivolumab. Safety and immunologic changes were assessed through week 24. There were no grade 3 or 4 adverse events or serious adverse events. All assessed patients retained T cell PD-1 receptor occupancy 6–12 weeks post-infusion, with a mean total across 0.1 and 0.3 mg/kg cohorts of 76% (95% CI 75–77), and no significant differences were observed between cohorts (p = 0.839). Patients receiving 0.3 mg/kg nivolumab without and with GS-4774 had mean declines of −0.30 (95% CI −0.46 to −0.14) and −0.16 (95% CI −0.33 to 0.01) log10 IU/ml, respectively. Patients showed significant HBsAg declines from baseline (p = 0.035) with 3 patients experiencing declines of >0.5 log10 by the end of study. One patient, whose HBsAg went from baseline 1,173 IU/ml to undetectable at week 20, experienced an alanine aminotransferase flare (grade 3) at week 4 that resolved by week 8 and was accompanied by a significant increase in peripheral HBsAg-specific T cells at week 24. In virally suppressed HBeAg-negative patients, checkpoint blockade was well-tolerated and led to HBsAg decline in most patients and sustained HBsAg loss in 1 patient. A major barrier to achieving a cure of chronic HBV infection is the presence of a dysfunctional immune response to the virus.11Whereas in acute self-limited infection, the CD8 T cell response is diverse and vigorous, the T cell response in patients with chronic infection is characterized by a depleted population of antigen-specific T cells and an inability to control or eliminate the virus.During the course of chronic HBV infection, inhibitory receptors on T cells are overexpressed, limiting T cell effector function.12Of the many inhibitory receptors present on exhausted T cells in patients with HBV, programmed death receptor 1 (PD-1) is the most highly expressed, especially on HBV-specific T cells within the liver.13,14One of the ligands for PD-1, programmed cell death ligand 1 (PD-L1), is normally found on cells within the liver, but its expression is increased in the setting of chronic infection.15Together, this increased expression of PD-L1 in HBV-infected hepatocytes and increased PD-1 on HBV-specific T cells likely contribute to T cell effector dysfunction.Reversal of “T cell exhaustion” through blockade of the PD-1:PD-L1 axis, has improved specific anti-HBV T cell responses in human intrahepatic T cells and in models of HBV, including mouse and woodchuck models.16–18In woodchucks with chronic woodchuck hepatitis virus infection, blockade of PD-L1 combined with DNA vaccination effectively controlled viremia, while antiviral treatment alone or antiviral treatment plus vaccination had no effect.19
To evaluate the hypothesis that increasing T cell frequency and activity may provide durable control of hepatitis B virus (HBV), we administered nivolumab, a programmed death receptor 1 (PD-1) inhibitor, with or without GS-4774, an HBV therapeutic vaccine, in virally suppressed patients with HBV e antigen (HBeAg)-negative chronic HBV. In a phase Ib study, patients received either a single dose of nivolumab at 0.1 mg/kg (n = 2) or 0.3 mg/kg (n = 12), or 40 yeast units of GS-4774 at baseline and week 4 and 0.3 mg/kg of nivolumab at week 4 (n = 10). The primary efficacy endpoint was mean change in HBV surface antigen (HBsAg) 12 weeks after nivolumab. Safety and immunologic changes were assessed through week 24. There were no grade 3 or 4 adverse events or serious adverse events. All assessed patients retained T cell PD-1 receptor occupancy 6–12 weeks post-infusion, with a mean total across 0.1 and 0.3 mg/kg cohorts of 76% (95% CI 75–77), and no significant differences were observed between cohorts (p = 0.839). Patients receiving 0.3 mg/kg nivolumab without and with GS-4774 had mean declines of −0.30 (95% CI −0.46 to −0.14) and −0.16 (95% CI −0.33 to 0.01) log10 IU/ml, respectively. Patients showed significant HBsAg declines from baseline (p = 0.035) with 3 patients experiencing declines of >0.5 log10 by the end of study. One patient, whose HBsAg went from baseline 1,173 IU/ml to undetectable at week 20, experienced an alanine aminotransferase flare (grade 3) at week 4 that resolved by week 8 and was accompanied by a significant increase in peripheral HBsAg-specific T cells at week 24. In virally suppressed HBeAg-negative patients, checkpoint blockade was well-tolerated and led to HBsAg decline in most patients and sustained HBsAg loss in 1 patient. To date, checkpoint blockade inhibitors have been tested mainly in the treatment of cancer and have shown efficacy in relieving in situ T cell dysfunction in patients with advanced malignancy.PD-1 and PD-L1 inhibitors disrupt PD-1 immune checkpoint signaling and restore antitumor activity of otherwise suppressed effector T cells.In clinical studies, nivolumab (Bristol-Myers Squibb, Princeton, NJ, United States), a fully human immunoglobulin G4, increased time to progression and improved survival in patients with advanced melanoma, non-small cell lung cancer, and non-Hodgkin’s lymphoma.Nivolumab is now approved treatment for these and other cancers.20In the first clinical study of nivolumab in patients with advanced hepatocellular carcinoma (CheckMate 040 Study), 3 of 51 patients (6%) with chronic HBV infection who received 3.0 mg/kg every 2 weeks demonstrated a 1 log decline in HBsAg during nivolumab therapy.21These studies suggest that the new PD-1 and PD-L1 inhibitors have potential in the treatment of patients with chronic HBV infection.
To evaluate the hypothesis that increasing T cell frequency and activity may provide durable control of hepatitis B virus (HBV), we administered nivolumab, a programmed death receptor 1 (PD-1) inhibitor, with or without GS-4774, an HBV therapeutic vaccine, in virally suppressed patients with HBV e antigen (HBeAg)-negative chronic HBV. In a phase Ib study, patients received either a single dose of nivolumab at 0.1 mg/kg (n = 2) or 0.3 mg/kg (n = 12), or 40 yeast units of GS-4774 at baseline and week 4 and 0.3 mg/kg of nivolumab at week 4 (n = 10). The primary efficacy endpoint was mean change in HBV surface antigen (HBsAg) 12 weeks after nivolumab. Safety and immunologic changes were assessed through week 24. There were no grade 3 or 4 adverse events or serious adverse events. All assessed patients retained T cell PD-1 receptor occupancy 6–12 weeks post-infusion, with a mean total across 0.1 and 0.3 mg/kg cohorts of 76% (95% CI 75–77), and no significant differences were observed between cohorts (p = 0.839). Patients receiving 0.3 mg/kg nivolumab without and with GS-4774 had mean declines of −0.30 (95% CI −0.46 to −0.14) and −0.16 (95% CI −0.33 to 0.01) log10 IU/ml, respectively. Patients showed significant HBsAg declines from baseline (p = 0.035) with 3 patients experiencing declines of >0.5 log10 by the end of study. One patient, whose HBsAg went from baseline 1,173 IU/ml to undetectable at week 20, experienced an alanine aminotransferase flare (grade 3) at week 4 that resolved by week 8 and was accompanied by a significant increase in peripheral HBsAg-specific T cells at week 24. In virally suppressed HBeAg-negative patients, checkpoint blockade was well-tolerated and led to HBsAg decline in most patients and sustained HBsAg loss in 1 patient. Doses of nivolumab selected for this pilot study (0.1 mg/kg and 0.3 mg/kg) were based on receptor occupancy (a surrogate marker of efficacy) and safety data reported from phase I dose-finding studies of nivolumab in patients with malignancies.22,23In these studies, peripheral receptor occupancy was similar across doses ranging from 0.1 mg/kg to 10 mg/kg.Importantly, doses below 1 mg/kg were safe with no serious adverse events (SAEs) and fewer treatment-related grade 3–4 adverse events (AEs) and no autoimmune disorders have been reported in patients who received single or multiple doses below 1 mg/kg.
Advancing liver disease results in deleterious changes in a number of critical organs. The ability to measure structure, blood flow and tissue perfusion within multiple organs in a single scan has implications for determining the balance of benefit vs. harm for therapies. Our aim was to establish the feasibility of magnetic resonance imaging (MRI) to assess changes in Compensated Cirrhosis (CC), and relate this to disease severity and future liver-related outcomes (LROs). A total of 60 patients with CC, 40 healthy volunteers and 7 patients with decompensated cirrhosis were recruited. In a single scan session, MRI measures comprised phase-contrast MRI vessel blood flow, arterial spin labelling tissue perfusion, T1 longitudinal relaxation time, heart rate, cardiac index, and volume assessment of the liver, spleen and kidneys. We explored the association between MRI parameters and disease severity, analysing differences in baseline MRI parameters in the 11 (18%) patients with CC who experienced future LROs. In the liver, compositional changes were reflected by increased T1 in progressive disease (p <0.001) and an increase in liver volume in CC (p = 0.006), with associated progressive reduction in liver (p <0.001) and splenic (p <0.001) perfusion. A significant reduction in renal cortex T1 and increase in cardiac index and superior mesenteric arterial blood flow was seen with increasing disease severity. Baseline liver T1 (p = 0.01), liver perfusion (p <0.01), and renal cortex T1 (p <0.01) were significantly different in patients with CC who subsequently developed negative LROs. MRI enables the contemporaneous assessment of organs in liver cirrhosis in a single scan without the requirement for a contrast agent. MRI parameters of liver T1, renal T1, hepatic and splenic perfusion, and superior mesenteric arterial blood flow were related to the risk of LROs. The evolution of liver cirrhosis to clinical liver-related outcomes resulting from portal hypertension is not simply dictated by architectural and haemodynamic changes within the liver.Rather, advancing liver disease results in deleterious changes in a number of critical organs and the understanding of this process is a central aspect in the clinical management of cirrhotic patients.
Advancing liver disease results in deleterious changes in a number of critical organs. The ability to measure structure, blood flow and tissue perfusion within multiple organs in a single scan has implications for determining the balance of benefit vs. harm for therapies. Our aim was to establish the feasibility of magnetic resonance imaging (MRI) to assess changes in Compensated Cirrhosis (CC), and relate this to disease severity and future liver-related outcomes (LROs). A total of 60 patients with CC, 40 healthy volunteers and 7 patients with decompensated cirrhosis were recruited. In a single scan session, MRI measures comprised phase-contrast MRI vessel blood flow, arterial spin labelling tissue perfusion, T1 longitudinal relaxation time, heart rate, cardiac index, and volume assessment of the liver, spleen and kidneys. We explored the association between MRI parameters and disease severity, analysing differences in baseline MRI parameters in the 11 (18%) patients with CC who experienced future LROs. In the liver, compositional changes were reflected by increased T1 in progressive disease (p <0.001) and an increase in liver volume in CC (p = 0.006), with associated progressive reduction in liver (p <0.001) and splenic (p <0.001) perfusion. A significant reduction in renal cortex T1 and increase in cardiac index and superior mesenteric arterial blood flow was seen with increasing disease severity. Baseline liver T1 (p = 0.01), liver perfusion (p <0.01), and renal cortex T1 (p <0.01) were significantly different in patients with CC who subsequently developed negative LROs. MRI enables the contemporaneous assessment of organs in liver cirrhosis in a single scan without the requirement for a contrast agent. MRI parameters of liver T1, renal T1, hepatic and splenic perfusion, and superior mesenteric arterial blood flow were related to the risk of LROs. The hyperdynamic circulation in cirrhosis is characterised by increased cardiac output and decreased systemic vascular resistance with low arterial blood pressure.1–3Splanchnic vasodilation, with a resulting decrease in the effective central volume, has been proposed as an important driver of the hyperdynamic circulation.1,4Associated with splanchnic vasodilation is an increase in portal vein blood flow which maintains and perpetuates portal hypertension.5Further, architectural and haemodynamic changes in the heart, spleen, and kidney have also been shown to occur and have important pathophysiological consequences.For example, cirrhotic cardiomyopathy is characterised by increased cardiac output with a sub-optimal ventricular response to stress, and structural and electrophysiological abnormalities.2Cardiac dysfunction associated with cirrhosis has been shown to be an important prognostic determinant of mortality at one year.6Renal vasoconstriction, related to splanchnic vasodilation, portal hypertension and activation of compensatory neurohormonal systems, is a precursor for the development of hepatorenal syndrome.3,6,7In cirrhosis, splenic enlargement may result from portal venous congestions and/or hyperplasia.In association, the splenic artery is suggested to dilate,8 and recent data suggests that the splenic artery to hepatic artery diameter ratio can predict the development of ascites and varices.9Splenic stiffness has also been found to have a strong association with portal hypertension.10,11However, there is an incomplete understanding of how changes in the different organs are inter-related, and what temporal relationships exist.
Advancing liver disease results in deleterious changes in a number of critical organs. The ability to measure structure, blood flow and tissue perfusion within multiple organs in a single scan has implications for determining the balance of benefit vs. harm for therapies. Our aim was to establish the feasibility of magnetic resonance imaging (MRI) to assess changes in Compensated Cirrhosis (CC), and relate this to disease severity and future liver-related outcomes (LROs). A total of 60 patients with CC, 40 healthy volunteers and 7 patients with decompensated cirrhosis were recruited. In a single scan session, MRI measures comprised phase-contrast MRI vessel blood flow, arterial spin labelling tissue perfusion, T1 longitudinal relaxation time, heart rate, cardiac index, and volume assessment of the liver, spleen and kidneys. We explored the association between MRI parameters and disease severity, analysing differences in baseline MRI parameters in the 11 (18%) patients with CC who experienced future LROs. In the liver, compositional changes were reflected by increased T1 in progressive disease (p <0.001) and an increase in liver volume in CC (p = 0.006), with associated progressive reduction in liver (p <0.001) and splenic (p <0.001) perfusion. A significant reduction in renal cortex T1 and increase in cardiac index and superior mesenteric arterial blood flow was seen with increasing disease severity. Baseline liver T1 (p = 0.01), liver perfusion (p <0.01), and renal cortex T1 (p <0.01) were significantly different in patients with CC who subsequently developed negative LROs. MRI enables the contemporaneous assessment of organs in liver cirrhosis in a single scan without the requirement for a contrast agent. MRI parameters of liver T1, renal T1, hepatic and splenic perfusion, and superior mesenteric arterial blood flow were related to the risk of LROs. The importance of assessing critical organs in liver cirrhosis in a holistic fashion is illustrated by the current controversy surrounding beta-blockers in liver cirrhosis.The debate regarding the safety of beta-blockers focusses on whether the beneficial effects of beta-blockers in liver cirrhosis, centred around a reduction in cardiac output, splanchnic vasodilation and portal inflow and improvement in intrahepatic resistance (alpha 1 blockade), is counterbalanced by deleterious effects in advanced cirrhosis centred on a reduction in renal perfusion and cardiac output as described previously.12A key limitation in being able to define the critical window6,13 of benefit of beta-blockers vs. harm is the lack of robust non-invasive tools to measure changes across organs in a contemporaneous manner.If this could be done, treatment could be individualised more effectively.This does not currently occur in clinical practice, in a consistent manner, as the tools for measurement are blunt (e.g. heart rate) or invasive (hepatic venous pressure gradient measurement [HVPG]).
Advancing liver disease results in deleterious changes in a number of critical organs. The ability to measure structure, blood flow and tissue perfusion within multiple organs in a single scan has implications for determining the balance of benefit vs. harm for therapies. Our aim was to establish the feasibility of magnetic resonance imaging (MRI) to assess changes in Compensated Cirrhosis (CC), and relate this to disease severity and future liver-related outcomes (LROs). A total of 60 patients with CC, 40 healthy volunteers and 7 patients with decompensated cirrhosis were recruited. In a single scan session, MRI measures comprised phase-contrast MRI vessel blood flow, arterial spin labelling tissue perfusion, T1 longitudinal relaxation time, heart rate, cardiac index, and volume assessment of the liver, spleen and kidneys. We explored the association between MRI parameters and disease severity, analysing differences in baseline MRI parameters in the 11 (18%) patients with CC who experienced future LROs. In the liver, compositional changes were reflected by increased T1 in progressive disease (p <0.001) and an increase in liver volume in CC (p = 0.006), with associated progressive reduction in liver (p <0.001) and splenic (p <0.001) perfusion. A significant reduction in renal cortex T1 and increase in cardiac index and superior mesenteric arterial blood flow was seen with increasing disease severity. Baseline liver T1 (p = 0.01), liver perfusion (p <0.01), and renal cortex T1 (p <0.01) were significantly different in patients with CC who subsequently developed negative LROs. MRI enables the contemporaneous assessment of organs in liver cirrhosis in a single scan without the requirement for a contrast agent. MRI parameters of liver T1, renal T1, hepatic and splenic perfusion, and superior mesenteric arterial blood flow were related to the risk of LROs. Recent advances in non-invasive magnetic resonance imaging (MRI) techniques allow the assessment of blood flow to organs,14 tissue perfusion,15,16 and compositional changes including fibrosis and inflammation,17–19 in the key organs associated with cirrhosis.Until now, such measures have only been examined in single organs rather than using a comprehensive multi-organ approach in a single scan session.
Radical resection is the best treatment for patients with advanced hepatic alveolar echinococcosis (AE). Liver transplantation is considered for selected advanced cases; however, a shortage of organ donors and the risk of postoperative recurrence are major challenges. The aim of this study was to assess the clinical outcomes of ex vivo liver resection and autotransplantation for end-stage AE. In this prospective study, 69 consecutive patients with end-stage hepatic AE were treated with ex vivo resection and liver autotransplantation between January 2010 and February 2017. The feasibility, safety and long-term clinical outcome of this technique were assessed. Ex vivo extended hepatectomy with autotransplantation was successful in all patients without intraoperative mortality. The median weight of the graft and AE lesion were 850 (370–1,600) g and 1,650 (375–5,000) g, respectively. The median duration of the operation and anhepatic phase were 15.9 (8–24) h and 360 (104–879) min, respectively. Six patients did not need any blood transfusion. Complications higher than IIIa according to Clavien classification were observed in 10 patients. The 30-day-mortality and overall mortality (>90 days) were 7.24% (5/69) and 11.5% (8/69), respectively. The mean hospital stay was 34.5 (12–128) days. Patients were followed-up systematically for a median of 22.5 months (14–89) without recurrence. This is the largest series assessing ex vivo liver resection and autotransplantation in end-stage hepatic AE. This technique could be an effective alternative to liver transplantation in patients with end-stage hepatic AE, with the advantage that it does not require an organ nor immunosuppressive agents. Hepatic alveolar echinococcosis (AE) is a lethal infectious disease caused by the larval stage of Echinococcus multilocularis (E. multilocularis).This severe disease remains a major public health issue in pastoral areas in China, Turkey, Central Asia, the Mediterranean and some European countries.1AE is usually chronic and asymptomatic while primary hepatic involvement presents with a mortality rate of 75% to 90% after 10 to 15 years if untreated.So far surgery associated with albendazole medication has been considered a major radical procedure for clinically diagnosed patients with AE.Of note, in late diagnosed cases, very few patients can benefit from surgery, due to extensive disease progression.2Retrospective studies have shown that palliative surgical procedures should be avoided in such cases, and the only feasible treatment is the long-term use of oral benzimidazoles.3Nevertheless, lifelong medications and their major potential side effects and the numerous complications of palliative resections with endless biliary drainage have motivated more radical approaches.4Given the devastating complications of AE and the limited surgical options available in advanced cases, transplantations emerged as an option for both curative and often palliative care.Although it was presented as a legitimate approach, the need for an organ donor is a limitation and lifelong immuno-suppressants increase the risk of recurrence.For these reasons, the decision to proceed to transplantation should be considered extremely cautiously.5
Congenital hepatic fibrosis (CHF) is a genetic liver disease resulting in abnormal proliferation of cholangiocytes and progressive hepatic fibrosis. CHF is caused by mutations in the PKHD1 gene and the subsequent dysfunction of the protein it encodes, fibrocystin. However, the underlying molecular mechanism of CHF, which is quite different from liver cirrhosis, remains unclear. This study investigated the molecular mechanism of CHF pathophysiology using a genetically engineered human induced pluripotent stem (iPS) cell model to aid the discovery of novel therapeutic agents for CHF. PKHD1-knockout (PKHD1-KO) and heterozygously mutated PKHD1 iPS clones were established by RNA-guided genome editing using the CRISPR/Cas9 system. The iPS clones were differentiated into cholangiocyte-like cells in cysts (cholangiocytic cysts [CCs]) in a 3D-culture system. The CCs were composed of a monolayer of cholangiocyte-like cells. The proliferation of PKHD1-KO CCs was significantly increased by interleukin-8 (IL-8) secreted in an autocrine manner. IL-8 production was significantly elevated in PKHD1-KO CCs due to mitogen-activated protein kinase pathway activation caused by fibrocystin deficiency. The production of connective tissue growth factor (CTGF) was also increased in PKHD1-KO CCs in an IL-8-dependent manner. Furthermore, validation analysis demonstrated that both the serum IL-8 level and the expression of IL-8 and CTGF in the liver samples were significantly increased in patients with CHF, consistent with our in vitro human iPS-disease model of CHF. Loss of fibrocystin function promotes IL-8-dependent proliferation of, and CTGF production by, human cholangiocytes, suggesting that IL-8 and CTGF are essential for the pathogenesis of CHF. IL-8 and CTGF are candidate molecular targets for the treatment of CHF. Congenital hepatic fibrosis (CHF) is a rare genetic liver disease (1/20,000 births) characterized by ductal plate malformation during bile duct development and progressive hepatic fibrosis.CHF is also frequently associated with autosomal recessive polycystic kidney disease.1Liver transplantation is necessary for the treatment of patients with progressive CHF and a severe phenotype.There is lots of pathological evidence to indicate that the mechanism of fibrosis in CHF is quite different from liver cirrhosis due to chronic hepatitis: in patients with CHF, neither necroinflammatory changes in hepatocytes nor hepatic stellate cell activation are observed.Fibrotic change in the CHF liver is limited to periportal areas of the hepatic lobes and is not observed around the central vein.2The pathophysiology of ductal plate malformation and progressive fibrosis in the CHF liver remains unclear.
Congenital hepatic fibrosis (CHF) is a genetic liver disease resulting in abnormal proliferation of cholangiocytes and progressive hepatic fibrosis. CHF is caused by mutations in the PKHD1 gene and the subsequent dysfunction of the protein it encodes, fibrocystin. However, the underlying molecular mechanism of CHF, which is quite different from liver cirrhosis, remains unclear. This study investigated the molecular mechanism of CHF pathophysiology using a genetically engineered human induced pluripotent stem (iPS) cell model to aid the discovery of novel therapeutic agents for CHF. PKHD1-knockout (PKHD1-KO) and heterozygously mutated PKHD1 iPS clones were established by RNA-guided genome editing using the CRISPR/Cas9 system. The iPS clones were differentiated into cholangiocyte-like cells in cysts (cholangiocytic cysts [CCs]) in a 3D-culture system. The CCs were composed of a monolayer of cholangiocyte-like cells. The proliferation of PKHD1-KO CCs was significantly increased by interleukin-8 (IL-8) secreted in an autocrine manner. IL-8 production was significantly elevated in PKHD1-KO CCs due to mitogen-activated protein kinase pathway activation caused by fibrocystin deficiency. The production of connective tissue growth factor (CTGF) was also increased in PKHD1-KO CCs in an IL-8-dependent manner. Furthermore, validation analysis demonstrated that both the serum IL-8 level and the expression of IL-8 and CTGF in the liver samples were significantly increased in patients with CHF, consistent with our in vitro human iPS-disease model of CHF. Loss of fibrocystin function promotes IL-8-dependent proliferation of, and CTGF production by, human cholangiocytes, suggesting that IL-8 and CTGF are essential for the pathogenesis of CHF. IL-8 and CTGF are candidate molecular targets for the treatment of CHF. Research models using human induced pluripotent stem (iPS) cell-derived hepatic cells have been used to investigate the pathophysiology of various diseases including genetic disorders.3Human iPS cell lines derived from patients with genetic diseases are often used to establish the disease models; however, there are some limitations in such studies due to different genetic backgrounds, lack of phenotypic markers, various mutation patterns in disease, and the varied multipotency of iPS cell lines.Therefore, it is reasonable that genetically engineered human iPS cells derived from healthy individuals are used for the study of diseases.4,5
Congenital hepatic fibrosis (CHF) is a genetic liver disease resulting in abnormal proliferation of cholangiocytes and progressive hepatic fibrosis. CHF is caused by mutations in the PKHD1 gene and the subsequent dysfunction of the protein it encodes, fibrocystin. However, the underlying molecular mechanism of CHF, which is quite different from liver cirrhosis, remains unclear. This study investigated the molecular mechanism of CHF pathophysiology using a genetically engineered human induced pluripotent stem (iPS) cell model to aid the discovery of novel therapeutic agents for CHF. PKHD1-knockout (PKHD1-KO) and heterozygously mutated PKHD1 iPS clones were established by RNA-guided genome editing using the CRISPR/Cas9 system. The iPS clones were differentiated into cholangiocyte-like cells in cysts (cholangiocytic cysts [CCs]) in a 3D-culture system. The CCs were composed of a monolayer of cholangiocyte-like cells. The proliferation of PKHD1-KO CCs was significantly increased by interleukin-8 (IL-8) secreted in an autocrine manner. IL-8 production was significantly elevated in PKHD1-KO CCs due to mitogen-activated protein kinase pathway activation caused by fibrocystin deficiency. The production of connective tissue growth factor (CTGF) was also increased in PKHD1-KO CCs in an IL-8-dependent manner. Furthermore, validation analysis demonstrated that both the serum IL-8 level and the expression of IL-8 and CTGF in the liver samples were significantly increased in patients with CHF, consistent with our in vitro human iPS-disease model of CHF. Loss of fibrocystin function promotes IL-8-dependent proliferation of, and CTGF production by, human cholangiocytes, suggesting that IL-8 and CTGF are essential for the pathogenesis of CHF. IL-8 and CTGF are candidate molecular targets for the treatment of CHF. PKHD1, the gene responsible for CHF, is a large gene composed of >400,000 bases and >400 mutation patterns have been reported in patients with CHF of different severity.6–8It encodes the fibrocystin protein, which is localized in the primary cilia of cholangiocytes.7–9The dysfunction of primary cilia lacking fibrocystin was suggested to be essential in the development of CHF.Animal models of CHF, spontaneous mutant polycystic kidney rat and gene-targeted Pkhd1 mutated mice, have been developed;10–12 however, there are several phenotypic differences between human CHF and these animal models.The marked infiltration of inflammatory cells around the portal vein, cysts of the pancreatic duct, and dilation of the extrahepatic bile duct are observed in Pkhd1-knockout (PKHD1-KO) mice, but are rare in patients with CHF, probably because of variations due to species specificity.10,12Thus, a disease model using human cells is necessary for the study of CHF pathophysiology.It is difficult to clarify such mechanisms using an iPS-cell model derived from patients with CHF because of the numerous mutation patterns without specific correlation between genetic and phenotype variations.Furthermore, the complete functional loss of fibrocystin is lethal in the fetal period.13
A causal link has recently been established between epigenetic alterations and hepatocarcinogenesis, indicating that epigenetic inhibition may have therapeutic potential. We aimed to identify and target epigenetic modifiers that show molecular alterations in hepatocellular carcinoma (HCC). We studied the molecular-clinical correlations of epigenetic modifiers including bromodomains, histone acetyltransferases, lysine methyltransferases and lysine demethylases in HCC using The Cancer Genome Atlas (TCGA) data of 365 patients with HCC. The therapeutic potential of epigenetic inhibitors was evaluated in vitro and in vivo. RNA sequencing analysis and its correlation with expression and clinical data in the TCGA dataset were used to identify expression programs normalized by Jumonji lysine demethylase (JmjC) inhibitors. Genetic alterations, aberrant expression, and correlation between tumor expression and poor patient prognosis of epigenetic enzymes are common events in HCC. Epigenetic inhibitors that target bromodomain (JQ-1), lysine methyltransferases (BIX-1294 and LLY-507) and JmjC lysine demethylases (JIB-04, GSK-J4 and SD-70) reduce HCC aggressiveness. The pan-JmjC inhibitor JIB-04 had a potent antitumor effect in tumor bearing mice. HCC cells treated with JmjC inhibitors showed overlapping changes in expression programs related with inhibition of cell proliferation and induction of cell death. JmjC inhibition reverses an aggressive HCC gene expression program that is also altered in patients with HCC. Several genes downregulated by JmjC inhibitors are highly expressed in tumor vs. non-tumor parenchyma, and their high expression correlates with a poor prognosis. We identified and validated a 4-gene expression prognostic signature consisting of CENPA, KIF20A, PLK1, and NCAPG. The epigenetic alterations identified in HCC can be used to predict prognosis and to define a subgroup of high-risk patients that would potentially benefit from JmjC inhibitor therapy. Hepatocellular carcinoma (HCC) is the most common primary liver cancer and usually occurs in patients with cirrhosis.1HCC is the sixth most frequent solid tumor and the second leading cause of cancer-related death worldwide and, unfortunately, its incidence and mortality are steadily increasing in Western countries.1Liver resection, transplantation, and tumor ablation are considered curative options although they are applied in only 30–40% of patients.The multikinase inhibitors sorafenib, as first line therapy, and regorafenib, as second line therapy, have been approved for advanced HCC, yet they have modest impact on patient survival.1,2Thus, there is an urgent need for new effective therapies.
A causal link has recently been established between epigenetic alterations and hepatocarcinogenesis, indicating that epigenetic inhibition may have therapeutic potential. We aimed to identify and target epigenetic modifiers that show molecular alterations in hepatocellular carcinoma (HCC). We studied the molecular-clinical correlations of epigenetic modifiers including bromodomains, histone acetyltransferases, lysine methyltransferases and lysine demethylases in HCC using The Cancer Genome Atlas (TCGA) data of 365 patients with HCC. The therapeutic potential of epigenetic inhibitors was evaluated in vitro and in vivo. RNA sequencing analysis and its correlation with expression and clinical data in the TCGA dataset were used to identify expression programs normalized by Jumonji lysine demethylase (JmjC) inhibitors. Genetic alterations, aberrant expression, and correlation between tumor expression and poor patient prognosis of epigenetic enzymes are common events in HCC. Epigenetic inhibitors that target bromodomain (JQ-1), lysine methyltransferases (BIX-1294 and LLY-507) and JmjC lysine demethylases (JIB-04, GSK-J4 and SD-70) reduce HCC aggressiveness. The pan-JmjC inhibitor JIB-04 had a potent antitumor effect in tumor bearing mice. HCC cells treated with JmjC inhibitors showed overlapping changes in expression programs related with inhibition of cell proliferation and induction of cell death. JmjC inhibition reverses an aggressive HCC gene expression program that is also altered in patients with HCC. Several genes downregulated by JmjC inhibitors are highly expressed in tumor vs. non-tumor parenchyma, and their high expression correlates with a poor prognosis. We identified and validated a 4-gene expression prognostic signature consisting of CENPA, KIF20A, PLK1, and NCAPG. The epigenetic alterations identified in HCC can be used to predict prognosis and to define a subgroup of high-risk patients that would potentially benefit from JmjC inhibitor therapy. Epigenetic mechanisms that affect DNA based processes, such as transcription, DNA repair and replication through changes in the chromatin conformation and ultimately in the cell state are common in human cancers.3Among epigenetic mechanisms, histone post-translational modifications (HPTMs) are a set generally reversible marks including phosphorylation, acetylation, methylation and Ubiquitination.3In particular, histone acetylation and methylation have emerged as key regulators of gene transcription that can dynamically modify gene expression.4The governance of chromatin structure through changes in HPTMs involves the action of writers, readers and erasers.Writers, including histone acetyltransferases (HATs) and lysine methyltransferases (KMTs), are enzymes that add the post-translational modifications.On the other hand, erasers such as lysine demethylases (KDMs) and histone deacetylases (HDACs) remove the HPTMs.Finally, readers are a set of proteins that recognize specific HPTMs allowing the binding of other proteins to specific chromatin locations.5Among readers, bromodomains (BRDs) recognize chromatin regions that have acetylated histones.5
A causal link has recently been established between epigenetic alterations and hepatocarcinogenesis, indicating that epigenetic inhibition may have therapeutic potential. We aimed to identify and target epigenetic modifiers that show molecular alterations in hepatocellular carcinoma (HCC). We studied the molecular-clinical correlations of epigenetic modifiers including bromodomains, histone acetyltransferases, lysine methyltransferases and lysine demethylases in HCC using The Cancer Genome Atlas (TCGA) data of 365 patients with HCC. The therapeutic potential of epigenetic inhibitors was evaluated in vitro and in vivo. RNA sequencing analysis and its correlation with expression and clinical data in the TCGA dataset were used to identify expression programs normalized by Jumonji lysine demethylase (JmjC) inhibitors. Genetic alterations, aberrant expression, and correlation between tumor expression and poor patient prognosis of epigenetic enzymes are common events in HCC. Epigenetic inhibitors that target bromodomain (JQ-1), lysine methyltransferases (BIX-1294 and LLY-507) and JmjC lysine demethylases (JIB-04, GSK-J4 and SD-70) reduce HCC aggressiveness. The pan-JmjC inhibitor JIB-04 had a potent antitumor effect in tumor bearing mice. HCC cells treated with JmjC inhibitors showed overlapping changes in expression programs related with inhibition of cell proliferation and induction of cell death. JmjC inhibition reverses an aggressive HCC gene expression program that is also altered in patients with HCC. Several genes downregulated by JmjC inhibitors are highly expressed in tumor vs. non-tumor parenchyma, and their high expression correlates with a poor prognosis. We identified and validated a 4-gene expression prognostic signature consisting of CENPA, KIF20A, PLK1, and NCAPG. The epigenetic alterations identified in HCC can be used to predict prognosis and to define a subgroup of high-risk patients that would potentially benefit from JmjC inhibitor therapy. Cancer hallmarks are a set of modifications acquired by cancer cells during tumor growth and development;6 it has been reported that epigenetic plasticity can affect many of these cancer cell traits.For instance, changes to DNA packaging allow tumor cells to resist cell death through tumor suppressor silencing or oncogene activation.Similarly, changes in gene expression patterns allow invasion and metastasis via epithelial-mesenchymal transition.6In human HCC, it has been reported that several lysine methyltransferases (EZH-2, SETDB1 and EHMT2/G9a) and lysine demethylases (KDM3A, KDM4B, KDM5B and KDM1A) are frequently upregulated.7–13In addition, high levels of these enzymes in HCC have been associated with poor prognosis in patients, and their knockdown decreased HCC cell proliferation and tumorigenicity in experimental models.7–13Thus, given the importance of HPTMs on HCC development, growth and metastasis, pharmacologic inhibition of these epigenetic pathways could emerge as a new therapeutic strategy.5Recently, 2 HDAC inhibitors, belinostat and reminostat, have been tested in phase I/II clinical trials in patients with HCC, showing that these targeted therapies have potential efficacy.14
People who inject drugs (PWID) and are on opioid agonist therapy (OAT) might have lower adherence to direct-acting antivirals (DAAs) against hepatitis C virus (HCV) and, therefore, lower rates of sustained virologic response (SVR). Because of this, we compared the SVR rates to interferon-free DAA combinations in individuals receiving OAT and those not receiving OAT in a real-world setting. The HEPAVIR-DAA cohort, recruiting HIV/HCV-coinfected patients (NCT02057003), and the GEHEP-MONO cohort (NCT02333292), including HCV-monoinfected individuals, are ongoing prospective multicenter cohorts of patients receiving DAAs in clinical practice. We compared SVR 12 weeks after treatment (SVR12) in non-drug users and PWID, including those receiving or not receiving OAT. Intention-to-treat and per protocol analyses were performed. Overall, 1,752 patients started interferon-free DAA treatment. By intention-to-treat analysis, 778 (95%, 95% CI 93%–96%) never injectors, 673 (92%, 95% CI 89%–93%) PWID not on OAT and 177 (89%, 95% CI 83%–92%) PWID on OAT achieved SVR12 (p = 0.002). SVR12 rates for ongoing drug users (with or without OAT) were 68 (79%) compared with 1,548 (95%) for non-drug users (p <0.001). Among ongoing drug users, 15 (17%) were lost-to-follow-up, and 3 (3.5%) became reinfected. In the per protocol analysis, 97% never injectors, 95% PWID not on OAT and 95% PWID on OAT achieved SVR12 (p = 0.246). After adjustment, ongoing drug use was associated with SVR12 (intention-to-treat) and OAT use was not. HCV-infected PWID achieve high SVR12 rates with DAAs whether they are on OAT or not, but their response rates are lower than those of patients who never used drugs. This is mainly attributable to more frequent loss to follow-up. Accounting for active drug use during DAA therapy nearly closed the gap in SVR rates between the study groups. The epidemic of hepatitis C virus (HCV) infection has been driven in many countries by parenteral transmission through shared needles among people who inject drugs (PWID).1,2Some reports have found that many PWID have a high willingness to receive HCV treatment.3,4However, treatment uptake in the interferon era among PWID was very low, with about 1–2% of all HCV-infected PWID treated yearly.5,6Most probably, this low incorporation of PWID to HCV treatment was due to barriers at different levels, including the poor tolerability of interferon.Simple, highly effective and safer all-oral direct-acting antivirals (DAAs) should have increased the uptake of HCV treatment among PWID.Indeed, current guidelines support the need to scale up treatment in PWID to effectively impact the HCV epidemic.7,8However, patients with ongoing drug use or on opioid agonist therapy (OAT) are ineligible in some settings9 or might not be considered suitable to receive DAAs by some practitioners.10To further complicate the scenario, PWID with HCV infection have been underrepresented in most clinical trials with DAAs.11–14Indeed, only a few trials have specifically assessed treatment with all-oral DAA regimens in individuals on OAT.15–18
Selection criteria for hepatectomy in patients with cirrhosis are controversial. In this study we aimed to build prognostic models of symptomatic post-hepatectomy liver failure (PHLF) in patients with cirrhosis. This was a cohort study of patients with histologically proven cirrhosis undergoing hepatectomy in 6 French tertiary care hepato-biliary-pancreatic centres. The primary endpoint was symptomatic (grade B or C) PHLF, according to the International Study Group of Liver Surgery’s definition. Twenty-six preoperative and 5 intraoperative variables were considered. An ordered ordinal logistic regression model with proportional odds ratio was used with 3 classes: O/A (No PHLF or grade A PHLF), B (grade B PHLF) and C (grade C PHLF). Of the 343 patients included, the main indication was hepatocellular carcinoma (88%). Laparoscopic liver resection was performed in 112 patients. Three-month mortality was 5.25%. The observed grades of PHLF were: 0/A: 61%, B: 28%, C: 11%. Based on the results of univariate analyses, 3 preoperative variables (platelet count, liver remnant volume ratio and intent-to-treat laparoscopy) were retained in a preoperative model and 2 intraoperative variables (per protocol laparoscopy and intraoperative blood loss) were added to the latter in a postoperative model. The preoperative model estimated the probabilities of PHLF grades with acceptable discrimination (area under the receiver-operating characteristic curve [AUC] 0.73, B/C vs. 0/A; AUC 0.75, C vs. 0/A/B) and the performance of the postoperative model was even better (AUC 0.77, B/C vs. 0/A; AUC 0.81, C vs. 0/A/B; p <0.001). By accurately predicting the risk of symptomatic PHLF in patients with cirrhosis, the preoperative model should be useful at the selection stage. Prediction can be adjusted at the end of surgery by also considering blood loss and conversion to laparotomy in a postoperative model, which might influence postoperative management. The safety of elective hepatectomies in cirrhotic patients has increased significantly during the last decades but mortality of such procedures is still estimated between 3 and 14%.1Post-hepatectomy liver failure (PHLF) is the most worrisome complication, with a reported mortality as high as 50%.2,3Moreover, it is the leading cause of prolonged hospitalization, increased costs, and poor long-term outcomes in patients undergoing this surgical procedure.
Selection criteria for hepatectomy in patients with cirrhosis are controversial. In this study we aimed to build prognostic models of symptomatic post-hepatectomy liver failure (PHLF) in patients with cirrhosis. This was a cohort study of patients with histologically proven cirrhosis undergoing hepatectomy in 6 French tertiary care hepato-biliary-pancreatic centres. The primary endpoint was symptomatic (grade B or C) PHLF, according to the International Study Group of Liver Surgery’s definition. Twenty-six preoperative and 5 intraoperative variables were considered. An ordered ordinal logistic regression model with proportional odds ratio was used with 3 classes: O/A (No PHLF or grade A PHLF), B (grade B PHLF) and C (grade C PHLF). Of the 343 patients included, the main indication was hepatocellular carcinoma (88%). Laparoscopic liver resection was performed in 112 patients. Three-month mortality was 5.25%. The observed grades of PHLF were: 0/A: 61%, B: 28%, C: 11%. Based on the results of univariate analyses, 3 preoperative variables (platelet count, liver remnant volume ratio and intent-to-treat laparoscopy) were retained in a preoperative model and 2 intraoperative variables (per protocol laparoscopy and intraoperative blood loss) were added to the latter in a postoperative model. The preoperative model estimated the probabilities of PHLF grades with acceptable discrimination (area under the receiver-operating characteristic curve [AUC] 0.73, B/C vs. 0/A; AUC 0.75, C vs. 0/A/B) and the performance of the postoperative model was even better (AUC 0.77, B/C vs. 0/A; AUC 0.81, C vs. 0/A/B; p <0.001). By accurately predicting the risk of symptomatic PHLF in patients with cirrhosis, the preoperative model should be useful at the selection stage. Prediction can be adjusted at the end of surgery by also considering blood loss and conversion to laparotomy in a postoperative model, which might influence postoperative management. The need to predict the risk of PHLF should be a major concern before performing a liver resection in a patient with cirrhosis, especially since the main indication is represented by hepatocellular carcinoma (HCC), which may be treated by other options, such as liver transplantation, thermo-ablation or transarterial chemo-embolization.However, given the shortage of liver grafts, primary surgical resection is increasingly considered in patients with HCC and could even be an acceptable treatment in selected patients beyond the Barcelona Clinic Liver Cancer (BCLC) criteria,4 provided that the estimated morbidity and mortality rates remain similar to those in the so-called “ideal” patients.5Moreover, associating liver resection and liver transplantation, with the use of liver resection as a down-staging or bridging approach,6 or considering liver transplantation as a salvage therapy in case of HCC recurrence7 may represent a curative strategy.
Selection criteria for hepatectomy in patients with cirrhosis are controversial. In this study we aimed to build prognostic models of symptomatic post-hepatectomy liver failure (PHLF) in patients with cirrhosis. This was a cohort study of patients with histologically proven cirrhosis undergoing hepatectomy in 6 French tertiary care hepato-biliary-pancreatic centres. The primary endpoint was symptomatic (grade B or C) PHLF, according to the International Study Group of Liver Surgery’s definition. Twenty-six preoperative and 5 intraoperative variables were considered. An ordered ordinal logistic regression model with proportional odds ratio was used with 3 classes: O/A (No PHLF or grade A PHLF), B (grade B PHLF) and C (grade C PHLF). Of the 343 patients included, the main indication was hepatocellular carcinoma (88%). Laparoscopic liver resection was performed in 112 patients. Three-month mortality was 5.25%. The observed grades of PHLF were: 0/A: 61%, B: 28%, C: 11%. Based on the results of univariate analyses, 3 preoperative variables (platelet count, liver remnant volume ratio and intent-to-treat laparoscopy) were retained in a preoperative model and 2 intraoperative variables (per protocol laparoscopy and intraoperative blood loss) were added to the latter in a postoperative model. The preoperative model estimated the probabilities of PHLF grades with acceptable discrimination (area under the receiver-operating characteristic curve [AUC] 0.73, B/C vs. 0/A; AUC 0.75, C vs. 0/A/B) and the performance of the postoperative model was even better (AUC 0.77, B/C vs. 0/A; AUC 0.81, C vs. 0/A/B; p <0.001). By accurately predicting the risk of symptomatic PHLF in patients with cirrhosis, the preoperative model should be useful at the selection stage. Prediction can be adjusted at the end of surgery by also considering blood loss and conversion to laparotomy in a postoperative model, which might influence postoperative management. Several predictors of PHLF have been reported in patients with cirrhosis, such as model for end-stage liver disease (MELD) score,8,9 hepatic venous pressure gradient (HVPG),10–12 indocyanine green (ICG) clearance13,14 and liver stiffness (LS).15In addition, multivariable models or decisional algorithms have been proposed to predict the risk of liver failure and mortality after HCC resection.2,14,16However, neither the European Association for the Study of the Liver (EASL) nor the American Association for the Study of the Liver Diseases (AASLD) have retained these models in the current international guidelines for the management of HCC17 and none of these models has been routinely adopted by other authors.The following may explain the lack of consideration of these tools in clinical practice: (i) some are based on expert opinions rather than statistical methods; (ii) some rely on biological or haemodynamic measurements, such as ICG clearance or HVPG, that are not routinely performed worldwide; (iii) some are derived from studies with heterogeneous inclusion criteria, e.g. regarding the proportion of cirrhotic patients2 or the proportion of HCC;18 (iv) few of them have been externally validated and a lack of reproducibility has been observed.In addition, several intraoperative variables, such as blood loss and duration of surgery,19 are not included in these models even though they have been shown to be correlated with PHLF.
There is a paucity of data regarding antiviral therapy in hepatitis B virus (HBV)-infected infants aged <1 year who have elevated alanine aminotransferase. This study aims to assess the efficacy and safety of antiviral therapy initiated in infancy. A real-world cohort study was conducted from January 2010 to December 2017. HBV-infected infants under 1 year of age, with persistent elevation of alanine aminotransferase and high viral load, were recruited and divided into 2 groups. Group I included 18 infants whose parents chose to initiate antiviral therapy with lamivudine before 1 year of age. Group II included 11 infants whose parents chose to initiate antiviral therapy with interferon-α after 1 year of age and not to receive any antiviral therapies before 1 year of age. The main outcome measure was rate of serum HBV surface antigen (HBsAg) loss at month 12 of treatment. There were no statistical differences between Groups I and II regarding baseline characteristics. No infants in Group II developed spontaneous HBsAg loss before 1 year of age. In Group I, the cumulative rates of HBsAg loss at month 3, 6, 9 and 12 of treatment were 39%, 67%, 78% and 83%, respectively. In Group II, the cumulative rates of HBsAg loss at month 3, 6, 9 and 12 of treatment were 18%, 27%, 27% and 36%, respectively. Statistical differences existed in the cumulative rates of HBsAg loss between the 2 groups (log-rank test, p = 0.0023). No serious adverse events occurred in the study. Early initiation of antiviral therapy for infantile-onset hepatitis B contributes to a rapid and significant loss of HBsAg. Further trials with larger cohorts are needed to verify our results. Hepatitis B virus (HBV) infection is a global public health problem that causes liver-related morbidity and mortality.1–3Despite passive-active immunoprophylaxis using hepatitis B vaccination with or without hepatitis B immunoglobulin (HBIg), up to 8%–10% of newborns of HBV surface antigen (HBsAg)-positive mothers still acquire HBV infection.4Most infected infants acquire HBV infection asymptomatically during perinatal period and have normal alanine aminotransferase (ALT) levels; however, some cases may present with onset hepatitis with elevated ALT.5As an unusual yet serious condition, infantile-onset hepatitis B after neonatal immunoprophylaxis has scarcely been studied.
A comprehensive analysis of changes in symptoms and functioning during and after direct-acting antiviral (DAA) therapy for chronic hepatitis C virus (HCV) infection has not been conducted for patients treated in real-world clinical settings. Therefore, we evaluated patient-reported outcomes (PROs) in a diverse cohort of patients with HCV treated with commonly prescribed DAAs. PROP UP is a US multicenter observational study of 1,601 patients with HCV treated with DAAs in 2016-2017. PRO data were collected at baseline (T1), early on-treatment (T2), late on-treatment (T3) and 3-months post-treatment (T4). PRO mean change scores were calculated from baseline and a minimally important change (MIC) threshold was set at 5%. Regression analyses investigated patient and treatment characteristics independently associated with PRO changes on-treatment and post-treatment. Of 1,564 patients, 55% were male, 39% non-white, 47% had cirrhosis. Sofosbuvir/ledipasvir was prescribed to 63%, sofosbuvir/velpatasvir to 21%, grazoprevir/elbasvir to 11%, and paritaprevir/ombitasvir/ritonavir + dasabuvir to 5%. During DAA therapy, mean PRO scores improved slightly in the overall cohort, but did not reach the 5% MIC threshold. Between 21–53% of patients experienced >5% improved PROs while 23–36% experienced >5% worse symptoms. Of 1,410 patients with evaluable sustained virologic response (SVR) data, 95% achieved SVR. Among those with SVR, all mean PRO scores improved, with the 5% MIC threshold met for fatigue, sleep disturbance, and functioning well-being. Regression analyses identified subgroups, defined by age 35–55, baseline mental health issues and a higher number of health comorbidities as predictors of PRO improvements. In real-world clinical practices, we observed heterogeneous patient experiences during and after DAA treatment. Symptom improvements were more pronounced in younger patients, those with baseline mental health issues and multiple comorbidities. Patients with chronic hepatitis C virus (HCV) infection often report neuropsychiatric, somatic, and gastrointestinal symptoms including fatigue, sleep disturbance, musculoskeletal pain, depression, and abdominal pain.1–3Patients may attribute these symptoms to HCV, a chronic viral infection associated with several extrahepatic disorders.Recent studies show that health-related quality of life and other patient-reported outcomes (PROs) improve during all-oral direct-acting antiviral (DAA) therapy and after patients achieve a sustained virologic response (SVR).4–6These studies were based exclusively on data derived from industry-sponsored registration trials.It remains critical to determine if these findings can be generalized to patients treated in real-world clinical practices given inherent biases of registration trial data.7,8
A comprehensive analysis of changes in symptoms and functioning during and after direct-acting antiviral (DAA) therapy for chronic hepatitis C virus (HCV) infection has not been conducted for patients treated in real-world clinical settings. Therefore, we evaluated patient-reported outcomes (PROs) in a diverse cohort of patients with HCV treated with commonly prescribed DAAs. PROP UP is a US multicenter observational study of 1,601 patients with HCV treated with DAAs in 2016-2017. PRO data were collected at baseline (T1), early on-treatment (T2), late on-treatment (T3) and 3-months post-treatment (T4). PRO mean change scores were calculated from baseline and a minimally important change (MIC) threshold was set at 5%. Regression analyses investigated patient and treatment characteristics independently associated with PRO changes on-treatment and post-treatment. Of 1,564 patients, 55% were male, 39% non-white, 47% had cirrhosis. Sofosbuvir/ledipasvir was prescribed to 63%, sofosbuvir/velpatasvir to 21%, grazoprevir/elbasvir to 11%, and paritaprevir/ombitasvir/ritonavir + dasabuvir to 5%. During DAA therapy, mean PRO scores improved slightly in the overall cohort, but did not reach the 5% MIC threshold. Between 21–53% of patients experienced >5% improved PROs while 23–36% experienced >5% worse symptoms. Of 1,410 patients with evaluable sustained virologic response (SVR) data, 95% achieved SVR. Among those with SVR, all mean PRO scores improved, with the 5% MIC threshold met for fatigue, sleep disturbance, and functioning well-being. Regression analyses identified subgroups, defined by age 35–55, baseline mental health issues and a higher number of health comorbidities as predictors of PRO improvements. In real-world clinical practices, we observed heterogeneous patient experiences during and after DAA treatment. Symptom improvements were more pronounced in younger patients, those with baseline mental health issues and multiple comorbidities. Clinical trials enroll highly selected patients and typically under-represent important subgroups of the HCV population.9–11Patients with psychosocial vulnerabilities (e.g., active psychiatric, drug use, alcohol use) are often excluded, yet these patients make up a sizeable majority of the population in need of treatment.In addition, a majority of these trials are comprised of predominantly white patients (66–97%) and those without advanced fibrosis (<20% cirrhosis).9–11Prior studies have also focused heavily on quality of life, work productivity, and fatigue outcomes but have not comprehensively evaluated specific somatic, gastrointestinal and neuropsychiatric symptoms often associated with chronic HCV.2A more comprehensive description of symptom and function changes would enhance our understanding of the full spectrum of patients’ experiences.Finally, PRO studies that allow for comparisons of patient experiences across different DAA regimens are lacking.
microRNAs (miRNAs) are deregulated in non-alcoholic fatty liver disease (NAFLD) and have been proposed as useful markers for the diagnosis and stratification of disease severity. We conducted a meta-analysis to identify the potential usefulness of miRNA biomarkers in the diagnosis and stratification of NAFLD severity. After a systematic review, circulating miRNA expression consistency and mean fold-changes were analysed using a vote-counting strategy. The sensitivity, specificity, positive and negative likelihood ratios, diagnostic odds ratio and area under the curve (AUC) for the diagnosis of NAFLD or non-alcoholic steatohepatitis (NASH) were pooled using a bivariate meta-analysis. Deeks’ funnel plot was used to assess the publication bias. Thirty-seven studies of miRNA expression profiles and six studies of diagnostic accuracy were ultimately included in the quantitative analysis. miRNA-122 and miRNA-192 showed consistent upregulation. miRNA-122 was upregulated in every scenario used to distinguish NAFLD severity. The miRNA expression correlation between the serum and liver tissue was inconsistent across studies. miRNA-122 distinguished NAFLD from healthy controls with an AUC of 0.82 (95% CI 0.75–0.89), and miRNA-34a distinguished non-alcoholic steatohepatitis (NASH) from non-alcoholic fatty liver (NAFL) with an AUC of 0.78 (95% CI 0.67–0.88). miRNA-34a, miRNA-122 and miRNA-192 were identified as potential diagnostic markers to segregate NAFL from NASH. Both miRNA-122, in distinguishing NAFLD from healthy controls, and miRNA-34a, in distinguishing NASH from NAFL, showed moderate diagnostic accuracy. miRNA-122 was upregulated in every scenario of NAFL, NASH and fibrosis. Non-alcoholic fatty liver disease (NAFLD) is characterised by excessive fat accumulation without a history of excessive alcohol intake, and the absence of other known liver diseases, such as hepatitis B and hepatitis C virus infection.1–3NAFLD has been described as the hepatic manifestation of metabolic syndrome, associated with insulin resistance and genetic susceptibility.NAFLD affects 30% to 40% of the United States population,4 2% to 44% of the European population,5 and 15% to 45% of the Asian population,6 while the Hispanic population is the most susceptible, as up to 45% of this population suffers from NAFLD.7Sedentary behaviour, low physical activity and poor diet have been defined as the “triple-hit behavioural phenotype”, which is associated with cardio-metabolic health, NAFLD and overall mortality.8Moreover, the prevalence of NAFLD in children is increasing and was estimated to be approximately 10%.9NAFLD encompasses a wide spectrum of liver damage, ranging from non-alcoholic fatty liver (NAFL) to non-alcoholic steatohepatitis (NASH).NAFL is defined as the presence of hepatocyte steatosis without evidence of inflammation.It is usually non-progressive, while NASH (steatosis with the concomitant presence of inflammation and ballooning) is often progressive, eventually advancing to cirrhosis and hepatocellular carcinoma (HCC).10Approximately 2–3% of the general population is affected by NASH; this incidence is increased to 20%–30% among obese or diabetic individuals.11,12NASH is the second most common indication for liver transplantation in the United States13 and is associated with an increased mortality with excess cardiovascular-, liver-, and cancer-related deaths.14Thus, improved detection of NASH is urgently needed.Moreover, distinguishing both NASH from NAFL and fibrosis from advanced fibrosis (>F3) are important goals.1,15,16
microRNAs (miRNAs) are deregulated in non-alcoholic fatty liver disease (NAFLD) and have been proposed as useful markers for the diagnosis and stratification of disease severity. We conducted a meta-analysis to identify the potential usefulness of miRNA biomarkers in the diagnosis and stratification of NAFLD severity. After a systematic review, circulating miRNA expression consistency and mean fold-changes were analysed using a vote-counting strategy. The sensitivity, specificity, positive and negative likelihood ratios, diagnostic odds ratio and area under the curve (AUC) for the diagnosis of NAFLD or non-alcoholic steatohepatitis (NASH) were pooled using a bivariate meta-analysis. Deeks’ funnel plot was used to assess the publication bias. Thirty-seven studies of miRNA expression profiles and six studies of diagnostic accuracy were ultimately included in the quantitative analysis. miRNA-122 and miRNA-192 showed consistent upregulation. miRNA-122 was upregulated in every scenario used to distinguish NAFLD severity. The miRNA expression correlation between the serum and liver tissue was inconsistent across studies. miRNA-122 distinguished NAFLD from healthy controls with an AUC of 0.82 (95% CI 0.75–0.89), and miRNA-34a distinguished non-alcoholic steatohepatitis (NASH) from non-alcoholic fatty liver (NAFL) with an AUC of 0.78 (95% CI 0.67–0.88). miRNA-34a, miRNA-122 and miRNA-192 were identified as potential diagnostic markers to segregate NAFL from NASH. Both miRNA-122, in distinguishing NAFLD from healthy controls, and miRNA-34a, in distinguishing NASH from NAFL, showed moderate diagnostic accuracy. miRNA-122 was upregulated in every scenario of NAFL, NASH and fibrosis. Liver biopsy remains the gold standard for the diagnosis of NASH.A common scoring system to distinguish NAFL and NASH is the NAFLD activity score, which is defined by the sum score of steatosis, ballooning, and lobular inflammation.17More recently, a new scoring system based on the degree of steatosis (S), the grade of necro-inflammatory activity (A) and fibrosis (F), also known as the SAF score, appears to help distinguish NAFL from NASH.18However, the potential risks of liver biopsy together with sampling and interpretation variability make it unsuitable for screening in the populations at risk.Biomarker discovery remains a big challenge because no reliable non-invasive disease biomarker can accurately distinguish mild from severe histological disease stages.
microRNAs (miRNAs) are deregulated in non-alcoholic fatty liver disease (NAFLD) and have been proposed as useful markers for the diagnosis and stratification of disease severity. We conducted a meta-analysis to identify the potential usefulness of miRNA biomarkers in the diagnosis and stratification of NAFLD severity. After a systematic review, circulating miRNA expression consistency and mean fold-changes were analysed using a vote-counting strategy. The sensitivity, specificity, positive and negative likelihood ratios, diagnostic odds ratio and area under the curve (AUC) for the diagnosis of NAFLD or non-alcoholic steatohepatitis (NASH) were pooled using a bivariate meta-analysis. Deeks’ funnel plot was used to assess the publication bias. Thirty-seven studies of miRNA expression profiles and six studies of diagnostic accuracy were ultimately included in the quantitative analysis. miRNA-122 and miRNA-192 showed consistent upregulation. miRNA-122 was upregulated in every scenario used to distinguish NAFLD severity. The miRNA expression correlation between the serum and liver tissue was inconsistent across studies. miRNA-122 distinguished NAFLD from healthy controls with an AUC of 0.82 (95% CI 0.75–0.89), and miRNA-34a distinguished non-alcoholic steatohepatitis (NASH) from non-alcoholic fatty liver (NAFL) with an AUC of 0.78 (95% CI 0.67–0.88). miRNA-34a, miRNA-122 and miRNA-192 were identified as potential diagnostic markers to segregate NAFL from NASH. Both miRNA-122, in distinguishing NAFLD from healthy controls, and miRNA-34a, in distinguishing NASH from NAFL, showed moderate diagnostic accuracy. miRNA-122 was upregulated in every scenario of NAFL, NASH and fibrosis. MicroRNAs (miRNAs) are non-coding small RNAs capable of controlling translation and modulating gene expression at the post-transcriptional level.Their impact on gene expression profiles can modify a variety of biological functions, such as lipid and glucose metabolism and thyroid, adipose tissue, stomach, muscle and liver function.19The miRNAs are very stable because they are resistant to degradation by ribonucleases.20Thus, circulating miRNAs, i.e., miRNAs detected in serum or plasma, have been proposed as attractive diagnostic tools.21The scope of applications associated with miRNAs is becoming broader because they are used in different clinical settings, such as early disease detection, disease prediction, monitoring of disease progression and response to treatment for a wide range of disorders.22miRNAs can also be found in extracellular vesicles (EVs), which are broadly divided into three types: microvesicles, apoptotic bodies and exosomes.23,24
A major limitation in the field of liver transplantation is the shortage of transplantable organs. Chimeric animals carrying human tissue have the potential to solve this problem. However, currently available chimeric organs retain a high level of xenogeneic cells, and the transplantation of impure organs needs to be tested. We created chimeric livers by injecting Lewis rat hepatocytes into C57Bl/6Fah−/−Rag2−/−Il2rg−/− mice, and further transplanted them into newly weaned Lewis rats (45 ± 3 g) with or without suboptimal immunosuppression (tacrolimus 0.6 mg/kg/day for 56 or 112 days). Control donors included wild-type C57Bl/6 mice (xenogeneic) and Lewis rats (syngeneic). Without immunosuppression, recipients of chimeric livers experienced acute rejection, and died within 8 to 11 days. With immunosuppression, they all survived for >112 days with normal weight gain compared to syngeneic controls, while all xenogeneic controls died within 98 days due to rejection with Banff scores >6 (p = 0.0014). The chimeric grafts underwent post-transplant remodelling, growing by 670% on average. Rat hepatocytes fully replaced mouse hepatocytes starting from day 56 (absence of detectable mouse serum albumin, histological clearance of mouse hepatocytes). In addition, rat albumin levels reached those of syngeneic recipients. Four months after transplantation of chimeric livers, we observed the development of diffuse mature rat bile ducts through transdifferentiation of hepatocytes (up to 72% of cholangiocytes), and patchy areas of portal endothelium originating from the host (seen in one out of five recipients). Taken together, these data demonstrate the efficacy of transplanting rat-to-mouse chimeric livers into rats, with a high potential for post-transplant recipient-oriented graft remodelling. Validation in a large animal model is still needed. The use of chimeric animals with organs compatible with specific patients in need of transplantation has the potential to solve the chronic lack of organ donors.The idea of using animals as incubators of human tissue is becoming more and more realistic, especially with the recent observation that human induced pluripotent stem cells (hiPSCs) can lead to chimeras after injection into pig blastocysts.1At this stage, the experiment was terminated before birth, and the contribution of human cells to the final chimeric pig embryos has remained low, but future studies are looking at replacing entire organs, as has been the case for various rodent combinations.1
A major limitation in the field of liver transplantation is the shortage of transplantable organs. Chimeric animals carrying human tissue have the potential to solve this problem. However, currently available chimeric organs retain a high level of xenogeneic cells, and the transplantation of impure organs needs to be tested. We created chimeric livers by injecting Lewis rat hepatocytes into C57Bl/6Fah−/−Rag2−/−Il2rg−/− mice, and further transplanted them into newly weaned Lewis rats (45 ± 3 g) with or without suboptimal immunosuppression (tacrolimus 0.6 mg/kg/day for 56 or 112 days). Control donors included wild-type C57Bl/6 mice (xenogeneic) and Lewis rats (syngeneic). Without immunosuppression, recipients of chimeric livers experienced acute rejection, and died within 8 to 11 days. With immunosuppression, they all survived for >112 days with normal weight gain compared to syngeneic controls, while all xenogeneic controls died within 98 days due to rejection with Banff scores >6 (p = 0.0014). The chimeric grafts underwent post-transplant remodelling, growing by 670% on average. Rat hepatocytes fully replaced mouse hepatocytes starting from day 56 (absence of detectable mouse serum albumin, histological clearance of mouse hepatocytes). In addition, rat albumin levels reached those of syngeneic recipients. Four months after transplantation of chimeric livers, we observed the development of diffuse mature rat bile ducts through transdifferentiation of hepatocytes (up to 72% of cholangiocytes), and patchy areas of portal endothelium originating from the host (seen in one out of five recipients). Taken together, these data demonstrate the efficacy of transplanting rat-to-mouse chimeric livers into rats, with a high potential for post-transplant recipient-oriented graft remodelling. Validation in a large animal model is still needed. Organ generation (thymus and pancreas) has been achieved by complementing genetically-deficient mouse and rat blastocysts with pluripotent stem cells (PSCs).3,4Going one step further, pancreatic islets isolated after complementing Pdx1−/− rat blastocysts with mouse PSCs could reverse diabetes after transplantation into mice.5However, the procedure is limited by the fact that Pdx1 codes for the pancreatic parenchyma only, while the vascular tissue remains of host origin.This aspect is of lesser impact when aiming at transplanting islets, but presents a significant challenge for solid organ transplantation.2,6In addition, other organs such as the kidney or the liver develop under the control of multiple genes,7,8 which makes the creation of organs with pure recipient features even more challenging, in part because multiple tissue knockouts are often lethal at a very early embryonic stage.9,10The first available solid organs generated by xenogeneic blastocyst complementation will therefore likely carry residual xenogeneic host tissues.2
Neutrophil extracellular traps (NETs) are an important strategy utilized by neutrophils to immobilize and kill invading microorganisms. Herein, we studied NET formation and the process of neutrophil cell death (NETosis), as well as the clearance of NETs by macrophages (MΦ) (efferocytosis) in acute sepsis following binge drinking. Healthy volunteers consumed 2 ml of vodka/kg body weight, before blood endotoxin and 16 s rDNA were measured. Peripheral neutrophils were isolated and exposed to alcohol followed by phorbol 12-myristate 13-acetate (PMA) stimulation. Mice were treated with three alcohol binges and intraperitoneal lipopolysaccharide (LPS) to assess the dynamics of NET formation and efferocytosis. In vivo, anti-Ly6G antibody (IA8) was used for neutrophil depletion. Inducers of NETs (endotoxin and bacterial DNA) significantly increased in the circulation after binge alcohol drinking in humans. Ex vivo, alcohol alone increased NET formation, but upon PMA stimulation alcohol attenuated NET formation. Binge alcohol in mice resulted in a biphasic response to LPS. Initially, binge alcohol reduced LPS-induced NET formation and resulted in a diffuse distribution of neutrophils in the liver compared to alcohol-naïve mice. Moreover, indicators of NET formation including citrullinated histone H3, neutrophil elastase, and neutrophil myeloperoxidase were decreased at an early time point after LPS challenge in mice receiving binge alcohol, suggesting decreased NET formation. However, in the efferocytosis phase (15 h after LPS) citrullinated histone-H3 was increased in the liver in alcohol binge mice, suggesting decreased clearance of NETs. In vitro alcohol treatment reduced efferocytosis and phagocytosis of NETotic neutrophils and promoted expression of CD206 on MΦ. Finally, depletion of neutrophils prior to binge alcohol ameliorated LPS-induced systemic inflammation and liver injury in mice. Dysfunctional NETosis and efferocytosis following binge drinking exacerbate liver injury associated with sepsis. Alcoholic liver disease (ALD) affects millions of people worldwide.The multifaceted disease spectrum is characterized by increased liver inflammation and steatosis, as a direct effect of alcohol, its metabolites and increased hepatic oxidative stress.1–3Sustained dysregulated hepatic inflammation during ALD is mediated by increased mobilization and recruitment of inflammatory cells to the liver.This occurrence normally precedes the breakdown of the gut barrier integrity and increased serum endotoxin levels, leading to prolonged hepatic inflammation and cell death.3,4During alcoholic hepatitis (AH), innate immune cells play a crucial role not only in recognizing and responding to pathogen-associated molecular patterns (PAMPs) but also in contributing to the activation of the inflammatory cascade that correlates with disease severity in AH.5–7Most studies on innate immune cells in AH and ALD have focused on the dysregulated migration, phagocytosis, and inflammatory cytokine release properties of macrophages (MΦ) and neutrophils.8–10During ALD associated with clinical sepsis, neutrophils are recruited to the liver within hours, adhering to activated blood vessels or migrating to the parenchyma.Neutrophil recruitment is increased under systemic inflammatory conditions.1,11Increased liver neutrophil infiltration correlates with mortality in acute AH; however, little is known about the functional capacity of neutrophils in AH.Mechanisms of neutrophil activation, recruitment, and innate immune functions, as well as their contribution to hepatic inflammation and injury during AH/ALD also remain incompletely understood.
Neutrophil extracellular traps (NETs) are an important strategy utilized by neutrophils to immobilize and kill invading microorganisms. Herein, we studied NET formation and the process of neutrophil cell death (NETosis), as well as the clearance of NETs by macrophages (MΦ) (efferocytosis) in acute sepsis following binge drinking. Healthy volunteers consumed 2 ml of vodka/kg body weight, before blood endotoxin and 16 s rDNA were measured. Peripheral neutrophils were isolated and exposed to alcohol followed by phorbol 12-myristate 13-acetate (PMA) stimulation. Mice were treated with three alcohol binges and intraperitoneal lipopolysaccharide (LPS) to assess the dynamics of NET formation and efferocytosis. In vivo, anti-Ly6G antibody (IA8) was used for neutrophil depletion. Inducers of NETs (endotoxin and bacterial DNA) significantly increased in the circulation after binge alcohol drinking in humans. Ex vivo, alcohol alone increased NET formation, but upon PMA stimulation alcohol attenuated NET formation. Binge alcohol in mice resulted in a biphasic response to LPS. Initially, binge alcohol reduced LPS-induced NET formation and resulted in a diffuse distribution of neutrophils in the liver compared to alcohol-naïve mice. Moreover, indicators of NET formation including citrullinated histone H3, neutrophil elastase, and neutrophil myeloperoxidase were decreased at an early time point after LPS challenge in mice receiving binge alcohol, suggesting decreased NET formation. However, in the efferocytosis phase (15 h after LPS) citrullinated histone-H3 was increased in the liver in alcohol binge mice, suggesting decreased clearance of NETs. In vitro alcohol treatment reduced efferocytosis and phagocytosis of NETotic neutrophils and promoted expression of CD206 on MΦ. Finally, depletion of neutrophils prior to binge alcohol ameliorated LPS-induced systemic inflammation and liver injury in mice. Dysfunctional NETosis and efferocytosis following binge drinking exacerbate liver injury associated with sepsis. Neutrophils represent the most abundant innate immune cell type accounting for about 40–75% of all circulating white blood cells in humans.12They have a short lifespan in the circulation of a few hours, which is dramatically increased to days when they leave the circulation and migrate into tissues.13,14Despite their short lifespans, neutrophils are constantly replenished from bone marrow stem cells.15When released from the bone marrow, neutrophils can phagocytose and kill pathogens.15During infection or sterile tissue injury, neutrophils are the first responder cell type to be recruited to the site, providing immune protection and contributing to healing and recovery.When exposed to pathogens or damage-associated molecular patterns (DAMPs), neutrophils engulf and degrade them in phagolysosomes by oxidative and non-oxidative mechanisms.16The migratory, phagocytic capacity of PAMPs/DAMPs is significantly reduced in ALD but oxidative burst is increased.In addition to this pathogen killing strategy, in 2004, Brinkmann et al. demonstrated the ability of bacteria-killing by neutrophils through neutrophil extracellular traps (NETs).17NETs are networks of extracellular ‘fibers in a spider-web like’ formation composed of DNA, citrullinated histones and antimicrobial peptides capable of directly killing some bacterial species.The functional role of NETs in ALD has not been described.
The mammalian circadian clock controls various aspects of liver metabolism and integrates nutritional signals. Recently, we described Hedgehog (Hh) signaling as a novel regulator of liver lipid metabolism. Herein, we investigated crosstalk between hepatic Hh signaling and circadian rhythm. Diurnal rhythms of Hh signaling were investigated in liver and hepatocytes from mice with ablation of Smoothened (SAC-KO) and crossbreeds with PER2::LUC reporter mice. By using genome-wide screening, qPCR, immunostaining, ELISA and RNAi experiments in vitro we identified relevant transcriptional regulatory steps. Shotgun lipidomics and metabolic cages were used for analysis of metabolic alterations and behavior. Hh signaling showed diurnal oscillations in liver and hepatocytes in vitro. Correspondingly, the level of Indian Hh, oscillated in serum. Depletion of the clock gene Bmal1 in hepatocytes resulted in significant alterations in the expression of Hh genes. Conversely, SAC-KO mice showed altered expression of clock genes, confirmed by RNAi against Gli1 and Gli3. Genome-wide screening revealed that SAC-KO hepatocytes showed time-dependent alterations in various genes, particularly those associated with lipid metabolism. The clock/hedgehog module further plays a role in rhythmicity of steatosis, and in the response of the liver to a high-fat diet or to differently timed starvation. For the first time, Hh signaling in hepatocytes was found to be time-of-day dependent and to feed back on the circadian clock. Our findings suggest an integrative role of Hh signaling, mediated mainly by GLI factors, in maintaining homeostasis of hepatic lipid metabolism by balancing the circadian clock. Circadian rhythm plays an important role in regulating physiology and behavior.In mammals, the natural light–dark cycle synchronizes the central circadian pacemaker in the suprachiasmatic nucleus (SCN), which, in turn, coordinates the rhythms of autonomous clocks in peripheral tissues such as the liver;1 these tissues adapt their functions to the rhythmic cycles of feeding and activity.Indeed, most metabolic and secretory functions of the liver show pronounced circadian rhythms.2,3In particular, liver carbohydrate and lipid metabolism, which are crucial for energy supply of the entire organism, oscillate throughout the day.The importance of this daily control is emphasized by epidemiological and experimental evidence that interruption or perturbation of circadian rhythms increases the risk for various types of liver disease and may even contribute to diabetes, obesity, metabolic syndrome and cancer.4–6
The mammalian circadian clock controls various aspects of liver metabolism and integrates nutritional signals. Recently, we described Hedgehog (Hh) signaling as a novel regulator of liver lipid metabolism. Herein, we investigated crosstalk between hepatic Hh signaling and circadian rhythm. Diurnal rhythms of Hh signaling were investigated in liver and hepatocytes from mice with ablation of Smoothened (SAC-KO) and crossbreeds with PER2::LUC reporter mice. By using genome-wide screening, qPCR, immunostaining, ELISA and RNAi experiments in vitro we identified relevant transcriptional regulatory steps. Shotgun lipidomics and metabolic cages were used for analysis of metabolic alterations and behavior. Hh signaling showed diurnal oscillations in liver and hepatocytes in vitro. Correspondingly, the level of Indian Hh, oscillated in serum. Depletion of the clock gene Bmal1 in hepatocytes resulted in significant alterations in the expression of Hh genes. Conversely, SAC-KO mice showed altered expression of clock genes, confirmed by RNAi against Gli1 and Gli3. Genome-wide screening revealed that SAC-KO hepatocytes showed time-dependent alterations in various genes, particularly those associated with lipid metabolism. The clock/hedgehog module further plays a role in rhythmicity of steatosis, and in the response of the liver to a high-fat diet or to differently timed starvation. For the first time, Hh signaling in hepatocytes was found to be time-of-day dependent and to feed back on the circadian clock. Our findings suggest an integrative role of Hh signaling, mediated mainly by GLI factors, in maintaining homeostasis of hepatic lipid metabolism by balancing the circadian clock. On the molecular level, circadian oscillations in the liver are regulated by several transcriptional and translational feedback loops based on the transcriptional activators Clock and Bmal1, which heterodimerize to stimulate the expression of the Cry1 and Cry2 as well as Per1, Per2 and Per3 repressors that then regulate Clock/Bmal1 activity.7The oscillating activity of these central clock proteins is also responsible for daily rhythms in metabolic function.8,9In the liver, the peripheral clock machinery is essential for the maintenance of metabolic homeostasis.Thereby almost all metabolic processes, such as carbohydrate, lipid as well as the amino acid metabolism and many more are dependent on the functional internal clock of the liver.10,11,2,12For example, transcription factors such as DBP, HLF, and TEF are involved in transmitting the oscillations to a wide variety of so-called clock-controlled genes responsible for different liver functions.13Recently, we revealed the hepatic Hh signaling as a master regulator of liver lipid metabolism which, if disturbed or disrupted, may cause liver steatosis.14Since this morphogen signaling pathway is also involved in modulating the insulin-like growth factor axis,15 which was reported to contribute to metabolic zonation of the liver16 and was recently suggested to modify the circadian clock in human cells,17 we were interested to determine whether Hh signaling itself exhibits a diurnal rhythm and whether, and how, it influences the rhythm of lipid metabolism and other hepatic functions.
Hepatitis delta virus (HDV) infection is the most severe form of viral hepatitis. Although HDV-associated liver disease is considered immune-mediated, adaptive immune responses against HDV are weak. Thus, the role of several other cell-mediated mechanisms such as those driven by mucosa-associated invariant T (MAIT) cells, a group of innate-like T cells highly enriched in the human liver, has not been extensively studied in clinical HDV infection. MAIT cells from a sizeable cohort of patients with chronic HDV were analyzed ex vivo and in vitro after stimulation. Results were compared with MAIT cells from hepatitis B virus (HBV) monoinfected patients and healthy controls. Circulating MAIT cells were dramatically decreased in the peripheral blood of HDV-infected patients. Signs of decline were also observed in the liver. In contrast, only a modest decrease of circulating MAIT cells was noted in HBV monoinfection. Unsupervised high-dimensional analysis of residual circulating MAIT cells in chronic HDV infection revealed the appearance of a compound phenotype of CD38hiPD-1hiCD28loCD127loPLZFloEomesloHelioslo cells indicative of activation. Corroborating these results, MAIT cells exhibited a functionally impaired responsiveness. In parallel to MAIT cell loss, HDV-infected patients exhibited signs of monocyte activation and increased levels of proinflammatory cytokines IL-12 and IL-18. In vitro, IL-12 and IL-18 induced an activated MAIT cell phenotype similar to the one observed ex vivo in HDV-infected patients. These cytokines also promoted MAIT cell death, suggesting that they may contribute to MAIT cell activation and subsequent loss during HDV infection. These results suggest that chronic HDV infection engages the MAIT cell compartment causing activation, functional impairment, and subsequent progressive loss of MAIT cells as the HDV-associated liver disease progresses. Hepatitis delta virus (HDV), a small, defective RNA virus, causes the most severe form of viral hepatitis.1,2For infection with HDV, coinfection with hepatitis B virus (HBV) is required.Up to 70 million individuals worldwide are chronically infected with HDV in conjunction with HBV infection.1–3Compared to other chronic viral hepatitis patients, HDV-infected patients experience an accelerated progression to liver fibrosis, increased risk of hepatocellular carcinoma, and earlier decompensation during liver cirrhosis.1,2Furthermore, treatment options against HDV infections are limited.At best, approximately 25% of infected patients respond to pegylated interferon-treatment with a measurable decline in HDV RNA viral load.4Alternative new treatment strategies are only in the very early stages of clinical development.5
Hepatitis delta virus (HDV) infection is the most severe form of viral hepatitis. Although HDV-associated liver disease is considered immune-mediated, adaptive immune responses against HDV are weak. Thus, the role of several other cell-mediated mechanisms such as those driven by mucosa-associated invariant T (MAIT) cells, a group of innate-like T cells highly enriched in the human liver, has not been extensively studied in clinical HDV infection. MAIT cells from a sizeable cohort of patients with chronic HDV were analyzed ex vivo and in vitro after stimulation. Results were compared with MAIT cells from hepatitis B virus (HBV) monoinfected patients and healthy controls. Circulating MAIT cells were dramatically decreased in the peripheral blood of HDV-infected patients. Signs of decline were also observed in the liver. In contrast, only a modest decrease of circulating MAIT cells was noted in HBV monoinfection. Unsupervised high-dimensional analysis of residual circulating MAIT cells in chronic HDV infection revealed the appearance of a compound phenotype of CD38hiPD-1hiCD28loCD127loPLZFloEomesloHelioslo cells indicative of activation. Corroborating these results, MAIT cells exhibited a functionally impaired responsiveness. In parallel to MAIT cell loss, HDV-infected patients exhibited signs of monocyte activation and increased levels of proinflammatory cytokines IL-12 and IL-18. In vitro, IL-12 and IL-18 induced an activated MAIT cell phenotype similar to the one observed ex vivo in HDV-infected patients. These cytokines also promoted MAIT cell death, suggesting that they may contribute to MAIT cell activation and subsequent loss during HDV infection. These results suggest that chronic HDV infection engages the MAIT cell compartment causing activation, functional impairment, and subsequent progressive loss of MAIT cells as the HDV-associated liver disease progresses. Similar to HBV and hepatitis C virus (HCV), HDV is non-cytopathic to infected hepatocytes.6,7Instead, it is thought that components of the immune system contribute to liver damage in HDV infection.6,7Despite this, adaptive immune responses against HDV are considered weak during chronic infection,8–11 possibly due to a defective initial role of innate immune cells in the immunopathogenesis of HDV infection.Indeed, we recently reported that natural killer cells, which are enriched in the human liver,12 are functionally compromised during chronic HDV infection.13,14
Hepatitis delta virus (HDV) infection is the most severe form of viral hepatitis. Although HDV-associated liver disease is considered immune-mediated, adaptive immune responses against HDV are weak. Thus, the role of several other cell-mediated mechanisms such as those driven by mucosa-associated invariant T (MAIT) cells, a group of innate-like T cells highly enriched in the human liver, has not been extensively studied in clinical HDV infection. MAIT cells from a sizeable cohort of patients with chronic HDV were analyzed ex vivo and in vitro after stimulation. Results were compared with MAIT cells from hepatitis B virus (HBV) monoinfected patients and healthy controls. Circulating MAIT cells were dramatically decreased in the peripheral blood of HDV-infected patients. Signs of decline were also observed in the liver. In contrast, only a modest decrease of circulating MAIT cells was noted in HBV monoinfection. Unsupervised high-dimensional analysis of residual circulating MAIT cells in chronic HDV infection revealed the appearance of a compound phenotype of CD38hiPD-1hiCD28loCD127loPLZFloEomesloHelioslo cells indicative of activation. Corroborating these results, MAIT cells exhibited a functionally impaired responsiveness. In parallel to MAIT cell loss, HDV-infected patients exhibited signs of monocyte activation and increased levels of proinflammatory cytokines IL-12 and IL-18. In vitro, IL-12 and IL-18 induced an activated MAIT cell phenotype similar to the one observed ex vivo in HDV-infected patients. These cytokines also promoted MAIT cell death, suggesting that they may contribute to MAIT cell activation and subsequent loss during HDV infection. These results suggest that chronic HDV infection engages the MAIT cell compartment causing activation, functional impairment, and subsequent progressive loss of MAIT cells as the HDV-associated liver disease progresses. Mucosa-associated invariant T (MAIT) cells represent an evolutionarily conserved subset of T cells with innate-like characteristics.15,16They express a semi-invariant T cell receptor, are abundant in mucosal tissues and peripheral blood,15,16 and are highly enriched in the human liver.17,18MAIT cells recognize vitamin B2 metabolites from many species of bacteria and fungi in complex with the major histocompatibility complex class I-related (MR1) protein.19,20Upon recognition of MR1-presented antigens, MAIT cells rapidly secrete proinflammatory cytokines such as interferon gamma (IFNγ), tumor necrosis factor (TNF), interleukin (IL)-17, and IL-22,15,21,22 and degranulate with concomitant release of cytotoxic effector molecules.23–25These effector functions contribute to their involvement in host responses towards bacterial infections as revealed by studies in both animal models and humans.26MAIT cells can also respond in an MR1-independent manner to innate cytokines, such as IL-12 and IL-18, produced by antigen-presenting cells and other cells in response to pathogens.27This mechanism may contribute to their involvement in several viral infections, such as those caused by human immunodeficiency virus (HIV), HCV, influenza virus, and dengue virus.28–31The role of MAIT cells in chronic HDV infection is currently unknown.However, sensing of RNA viruses via toll-like receptor (TLR) 8 in the liver can break tolerance and potentially trigger MAIT cell activation, suggesting a potential role for MAIT cells during chronic HDV infection.18
Neuronal function is exquisitely sensitive to alterations in the extracellular environment. In patients with hepatic encephalopathy (HE), accumulation of metabolic waste products and noxious substances in the interstitial fluid of the brain is thought to result from liver disease and may contribute to neuronal dysfunction and cognitive impairment. This study was designed to test the hypothesis that the accumulation of these substances, such as bile acids, may result from reduced clearance from the brain. In a rat model of chronic liver disease with minimal HE (the bile duct ligation [BDL] model), we used emerging dynamic contrast-enhanced MRI and mass-spectroscopy techniques to assess the efficacy of the glymphatic system, which facilitates clearance of solutes from the brain. Immunofluorescence of aquaporin-4 (AQP4) and behavioural experiments were also performed. We identified discrete brain regions (olfactory bulb, prefrontal cortex and hippocampus) of altered glymphatic clearance in BDL rats, which aligned with cognitive/behavioural deficits. Reduced AQP4 expression was observed in the olfactory bulb and prefrontal cortex in HE, which could contribute to the pathophysiological mechanisms underlying the impairment in glymphatic function in BDL rats. This study provides the first experimental evidence of impaired glymphatic flow in HE, potentially mediated by decreased AQP4 expression in the affected regions. The mechanisms underlying the pathogenesis of hepatic encephalopathy (HE) in patients with cirrhosis (chronic liver disease) are not completely understood.Data available in the literature suggest that noxious substances and metabolites such as lactate, glutamate, bile acids and drugs accumulate in the brain of patients with HE.1The prevailing hypothesis proposes that this occurs because of metabolic and transporter defects induced by hyperammonaemia, inflammation and alterations in blood brain barrier function.2,3
Neuronal function is exquisitely sensitive to alterations in the extracellular environment. In patients with hepatic encephalopathy (HE), accumulation of metabolic waste products and noxious substances in the interstitial fluid of the brain is thought to result from liver disease and may contribute to neuronal dysfunction and cognitive impairment. This study was designed to test the hypothesis that the accumulation of these substances, such as bile acids, may result from reduced clearance from the brain. In a rat model of chronic liver disease with minimal HE (the bile duct ligation [BDL] model), we used emerging dynamic contrast-enhanced MRI and mass-spectroscopy techniques to assess the efficacy of the glymphatic system, which facilitates clearance of solutes from the brain. Immunofluorescence of aquaporin-4 (AQP4) and behavioural experiments were also performed. We identified discrete brain regions (olfactory bulb, prefrontal cortex and hippocampus) of altered glymphatic clearance in BDL rats, which aligned with cognitive/behavioural deficits. Reduced AQP4 expression was observed in the olfactory bulb and prefrontal cortex in HE, which could contribute to the pathophysiological mechanisms underlying the impairment in glymphatic function in BDL rats. This study provides the first experimental evidence of impaired glymphatic flow in HE, potentially mediated by decreased AQP4 expression in the affected regions. In the periphery, the lymphatic system is responsible for interstitial fluid (ISF) clearance, a mechanism that is critical for maintaining tissue homeostasis.Although neuronal function is exquisitely sensitive to alterations in the extracellular environment, until recently, the brain was believed to be devoid of a lymphatic drainage/clearance system.Recent studies identified a brain-wide paravascular pathway that facilitates the efficient clearance of various molecules, including toxic interstitial proteins, lactate and other metabolites.4,5Subarachnoid cerebrospinal fluid (CSF) circulates through the brain parenchyma along the paravascular spaces surrounding penetrating arteries, exchanging with the surrounding ISF and facilitating the clearance of interstitial solutes.4ISF is then cleared along paravascular spaces surrounding large calibre cerebral veins, which reach the recently discovered meningeal lymphatic vessels6 and enter the systemic circulation.This pathway has been termed the “glymphatic system” because of its apparent dependence on glial water channels and its clearance function similar to that of the lymphatic system.That said, the precise definition of what constitutes “glymphatic” function in the brain is still being debated and discussed in the literature,7–9 and the precise role that glial water channels play in such a clearance pathway is unclear.A schematic describing this system in health and HE (according to the findings of the current study) is depicted in Fig. 1.
Neuronal function is exquisitely sensitive to alterations in the extracellular environment. In patients with hepatic encephalopathy (HE), accumulation of metabolic waste products and noxious substances in the interstitial fluid of the brain is thought to result from liver disease and may contribute to neuronal dysfunction and cognitive impairment. This study was designed to test the hypothesis that the accumulation of these substances, such as bile acids, may result from reduced clearance from the brain. In a rat model of chronic liver disease with minimal HE (the bile duct ligation [BDL] model), we used emerging dynamic contrast-enhanced MRI and mass-spectroscopy techniques to assess the efficacy of the glymphatic system, which facilitates clearance of solutes from the brain. Immunofluorescence of aquaporin-4 (AQP4) and behavioural experiments were also performed. We identified discrete brain regions (olfactory bulb, prefrontal cortex and hippocampus) of altered glymphatic clearance in BDL rats, which aligned with cognitive/behavioural deficits. Reduced AQP4 expression was observed in the olfactory bulb and prefrontal cortex in HE, which could contribute to the pathophysiological mechanisms underlying the impairment in glymphatic function in BDL rats. This study provides the first experimental evidence of impaired glymphatic flow in HE, potentially mediated by decreased AQP4 expression in the affected regions. In addition to clearing the brain, the glymphatic system is thought to contribute to the distribution of growth factors, neuromodulators, carrier proteins and other solutes within the brain.10Failure of the glymphatic system (achieved either experimentally11 or pathophysiologically12–15) may therefore have critical adverse consequences, and has been linked to the pathogenesis of neurodegenerative disease(s).5,16
Non-alcoholic fatty liver disease/non-alcoholic steatohepatitis (NAFLD/NASH) is an increasing clinical problem associated with progression to hepatocellular carcinoma (HCC). The effect of a high-fat diet on the early immune response in HCC is poorly understood, while the role of metformin in treating NAFLD and HCC remains controversial. Herein, we visualized the early immune responses in the liver and the effect of metformin on progression of HCC using optically transparent zebrafish. We used live imaging to visualize liver inflammation and disease progression in a NAFLD/NASH-HCC zebrafish model. We combined a high-fat diet with a transgenic zebrafish HCC model induced by hepatocyte-specific activated beta-catenin and assessed liver size, angiogenesis, micronuclei formation and inflammation in the liver. In addition, we probed the effects of metformin on immune cell composition and early HCC progression. We found that a high-fat diet induced an increase in liver size, enhanced angiogenesis, micronuclei formation and neutrophil infiltration in the liver. Although macrophage number was not affected by diet, a high-fat diet induced changes in macrophage morphology and polarization with an increase in liver associated TNFα-positive macrophages. Treatment with metformin altered macrophage polarization, reduced liver size and reduced micronuclei formation in NAFLD/NASH-associated HCC larvae. Moreover, a high-fat diet reduced T cell density in the liver, which was reversed by treatment with metformin. These findings suggest that diet alters macrophage polarization and exacerbates the liver inflammatory microenvironment and cancer progression in a zebrafish model of NAFLD/NASH-associated HCC. Metformin specifically affects the progression induced by diet and modulates the immune response by affecting macrophage polarization and T cell infiltration, suggesting possible effects of metformin on tumor surveillance. Hepatocellular carcinoma (HCC) is a common cause of cancer-related deaths with increasing mortality worldwide.1In Western societies, 30–40% of patients with HCC are obese and have non-alcoholic steatohepatitis (NASH), an aggressive form of non-alcoholic fatty liver disease (NAFLD).2–5Abnormal lipid accumulation in hepatocytes increases oxidative stress and leads to lipotoxicity, which triggers liver inflammation, a hallmark of NAFLD progression to HCC.6Pro-tumorigenic subsets of neutrophils, macrophages, and other immune cells provide the tumor microenvironment (TME) with growth factors, matrix-remodeling factors and inflammatory mediators that optimize tumor growth.7–10Hepatic macrophages in particular, including both monocyte-derived or tissue-resident macrophages known as Kupffer cells, have been identified as potential drug targets to treat liver disease.11Several studies have shown that NAFLD progression to HCC involves inflammatory macrophages12 and Kupffer cells.13Adaptive immune cells can also be modulators of hepatocarcinogenesis.NAFLD/NASH impairs tumor surveillance by inducing apoptosis of CD4+ T cells.14Taken together, this previous work suggests that the innate and adaptive immune systems are key players in the progression of NAFLD-associated HCC.However, the specific cellular and molecular immune mechanisms that regulate the pathogenesis of early NAFLD/NASH-associated HCC remain unclear.
Non-alcoholic fatty liver disease/non-alcoholic steatohepatitis (NAFLD/NASH) is an increasing clinical problem associated with progression to hepatocellular carcinoma (HCC). The effect of a high-fat diet on the early immune response in HCC is poorly understood, while the role of metformin in treating NAFLD and HCC remains controversial. Herein, we visualized the early immune responses in the liver and the effect of metformin on progression of HCC using optically transparent zebrafish. We used live imaging to visualize liver inflammation and disease progression in a NAFLD/NASH-HCC zebrafish model. We combined a high-fat diet with a transgenic zebrafish HCC model induced by hepatocyte-specific activated beta-catenin and assessed liver size, angiogenesis, micronuclei formation and inflammation in the liver. In addition, we probed the effects of metformin on immune cell composition and early HCC progression. We found that a high-fat diet induced an increase in liver size, enhanced angiogenesis, micronuclei formation and neutrophil infiltration in the liver. Although macrophage number was not affected by diet, a high-fat diet induced changes in macrophage morphology and polarization with an increase in liver associated TNFα-positive macrophages. Treatment with metformin altered macrophage polarization, reduced liver size and reduced micronuclei formation in NAFLD/NASH-associated HCC larvae. Moreover, a high-fat diet reduced T cell density in the liver, which was reversed by treatment with metformin. These findings suggest that diet alters macrophage polarization and exacerbates the liver inflammatory microenvironment and cancer progression in a zebrafish model of NAFLD/NASH-associated HCC. Metformin specifically affects the progression induced by diet and modulates the immune response by affecting macrophage polarization and T cell infiltration, suggesting possible effects of metformin on tumor surveillance. Metformin is a well-tolerated drug commonly used to treat diabetes with some evidence suggesting beneficial effects in HCC and other types of cancer.15In hepatocytes, metformin increases AMPK activity,16 and also decreases gluconeogenesis and increases fatty acid oxidation,17 supporting its use in NAFLD.18The role of metformin in treating HCC remains controversial,19 although its use is supported by some in vitro, ex vivo and xenotransplant mouse models.20,21However, the effect of metformin on the TME immune composition or HCC progression in vivo remains unclear.
Non-alcoholic steatohepatitis (NASH) is associated with dysregulation of lipid metabolism and hepatic inflammation. The causal mechanism underlying NASH is not fully elucidated. We aim to investigate the role of β-arrestin1 (ARRB1) in the progression of NASH. Human liver tissues from patients with NASH and control subjects were obtained to evaluate ARRB1 expression. NASH models were established in ARRB1 knockout and wild type mice fed high-fat diet (HFD) for 26 weeks or methionine/choline deficient (MCD) diet for 6 weeks. ARRB1 expression was diminished in NASH patient liver samples. Moreover, diminished ARRB1 levels were detected in mice NASH models. ARRB1 deficiency accelerated steatohepatitis development in HFD-/MCD diet-fed mice accompanied by upregulation of lipogenic genes and downregulation of β-oxidative genes. Intriguingly, ARRB1 was found to interact with GDF15 and facilitated the transportation of GDF15 precursor (pro-GDF15) to Golgi apparatus for cleavage and maturation. Treatment with recombinant GDF15 ablated the lipid accumulation in the presence of ARRB1 deletion in vitro and in vivo. Re-expression of ARRB1 in the NASH models ameliorated the liver disease, and the effect was greater in the presence of pro-GDF15 overexpression. In contrast, the effect of pro-GDF15 overexpression alone was impaired in ARRB1-deficient mice. In addition, the severity of liver disease in patients with NASH was negatively correlated with ARRB1 expression. ARRB1 acts as a vital regulator in the development of NASH via facilitating GDF15’s translocation to the Golgi apparatus and subsequent maturation. ARRB1 thus is a potential therapeutic target for the treatment of NASH. With the dramatic changes in people’s dietary choices and life styles, metabolic disorders including obesity, insulin resistance, and nonalcoholic fatty liver disease (NAFLD) have become a public health issue worldwide [1, 2].Excessive nutritional intake and decreased energy expenditure appear to be crucial in the pathogenesis of NAFLD.NAFLD comprises a spectrum of liver diseases ranging from simple fatty liver to non-alcoholic steatohepatitis (NASH), which can potentially progress to cirrhosis and liver cancer [3, 4].NASH is associated with reprogrammed hepatic metabolic profiles that lead to excessive lipid accumulation in the liver and imbalances in lipid metabolism and lipid catabolism [5, 6].More advanced NASH is associated with impaired lipid metabolism, thus leading to the accumulation of triglycerides and other lipids in hepatocytes [7].Lipotoxicity in the liver is the primary insult that initiates and propagates damage leading to hepatocyte injury and resultant inflammation [8].Hepatic lipid homeostasis is fine-tuned by a complex machinery comprising hormones, signaling/transcriptional pathways, and downstream genes associated with lipogenesis and lipolysis [9].Although many molecular regulatory networks have been described, the underlying mechanisms initiating the metabolic rearrangement and inflammatory response underlying NASH remain incompletely elucidated.
Non-alcoholic steatohepatitis (NASH) is associated with dysregulation of lipid metabolism and hepatic inflammation. The causal mechanism underlying NASH is not fully elucidated. We aim to investigate the role of β-arrestin1 (ARRB1) in the progression of NASH. Human liver tissues from patients with NASH and control subjects were obtained to evaluate ARRB1 expression. NASH models were established in ARRB1 knockout and wild type mice fed high-fat diet (HFD) for 26 weeks or methionine/choline deficient (MCD) diet for 6 weeks. ARRB1 expression was diminished in NASH patient liver samples. Moreover, diminished ARRB1 levels were detected in mice NASH models. ARRB1 deficiency accelerated steatohepatitis development in HFD-/MCD diet-fed mice accompanied by upregulation of lipogenic genes and downregulation of β-oxidative genes. Intriguingly, ARRB1 was found to interact with GDF15 and facilitated the transportation of GDF15 precursor (pro-GDF15) to Golgi apparatus for cleavage and maturation. Treatment with recombinant GDF15 ablated the lipid accumulation in the presence of ARRB1 deletion in vitro and in vivo. Re-expression of ARRB1 in the NASH models ameliorated the liver disease, and the effect was greater in the presence of pro-GDF15 overexpression. In contrast, the effect of pro-GDF15 overexpression alone was impaired in ARRB1-deficient mice. In addition, the severity of liver disease in patients with NASH was negatively correlated with ARRB1 expression. ARRB1 acts as a vital regulator in the development of NASH via facilitating GDF15’s translocation to the Golgi apparatus and subsequent maturation. ARRB1 thus is a potential therapeutic target for the treatment of NASH. β-arrestin1 (ARRB1), originally identified as a negative regulator of G protein-coupled receptor signaling, has been demonstrated to function as molecular scaffold that regulates cellular function by interacting with other partner proteins, and to be involve in multiple physiological process including immune response, tumorigenesis and inflammation [10-13].ARRB1 has been found to regulate the NF-κB pathway in multiple inflammatory disease models [14, 15].Our previous study has shown that ARRB1 participates in the regulating hepatocellular carcinoma aggressiveness through mediating the desensitization and internalization of CD97 [16].Moreover, ARRB1 partially represses diet-induced obesity and improves glucose tolerance through interaction with PPARγ in preadipocytes [17].However, the regulatory roles of ARRB1 in hepatic inflammation and lipid metabolism disorder during the progression of NASH remain unknown.
Non-alcoholic steatohepatitis (NASH) is associated with dysregulation of lipid metabolism and hepatic inflammation. The causal mechanism underlying NASH is not fully elucidated. We aim to investigate the role of β-arrestin1 (ARRB1) in the progression of NASH. Human liver tissues from patients with NASH and control subjects were obtained to evaluate ARRB1 expression. NASH models were established in ARRB1 knockout and wild type mice fed high-fat diet (HFD) for 26 weeks or methionine/choline deficient (MCD) diet for 6 weeks. ARRB1 expression was diminished in NASH patient liver samples. Moreover, diminished ARRB1 levels were detected in mice NASH models. ARRB1 deficiency accelerated steatohepatitis development in HFD-/MCD diet-fed mice accompanied by upregulation of lipogenic genes and downregulation of β-oxidative genes. Intriguingly, ARRB1 was found to interact with GDF15 and facilitated the transportation of GDF15 precursor (pro-GDF15) to Golgi apparatus for cleavage and maturation. Treatment with recombinant GDF15 ablated the lipid accumulation in the presence of ARRB1 deletion in vitro and in vivo. Re-expression of ARRB1 in the NASH models ameliorated the liver disease, and the effect was greater in the presence of pro-GDF15 overexpression. In contrast, the effect of pro-GDF15 overexpression alone was impaired in ARRB1-deficient mice. In addition, the severity of liver disease in patients with NASH was negatively correlated with ARRB1 expression. ARRB1 acts as a vital regulator in the development of NASH via facilitating GDF15’s translocation to the Golgi apparatus and subsequent maturation. ARRB1 thus is a potential therapeutic target for the treatment of NASH. The regulation of energy balance in the liver and other peripheral tissues is influenced by humoral factors that influence various metabolic activities such as lipolysis and lipogenesis.Dysregulation of hormones or cytokines including leptin, adiponectin, and insulin are well documented to contribute to metabolic disorders and hepatic lipid accumulation.Thus, more comprehensive elucidation of the causal mechanism underlying abnormal expression of these hormones or cytokines may enable the development of new therapeutic approaches for NASH.GDF15 (also known as macrophage inhibitor 1), is predominantly expressed in the liver and is a member of the TGF-beta superfamily [18].GDF15 is initially translated to pro-GDF15 in dimeric form and is subsequently cleaved and secreted as mature GFD15 dimers [19, 20].Recent studies have shown that GDF15 activates AKT, ERK1/2, and PLCγ through binding GFRAL and through a GFRAL–RET complex present in cells, thus, reducing food intake, driving weight loss and enhancing glucose homeostasis [18, 21-23].In addition, GDF15 alleviates fatty acid metabolic dysfunction in the liver, thus indicating that the liver is the direct target organ of GDF15 [24].However, the post-translational regulation of GDF15, such as the maturation of pro-GDF15, and the downstream molecular mechanisms of GDF15 in hepatocytes, remain to be investigated.
Although CD8+T cell exhaustion hampers viral control during chronic HBV infection, the pool of CD8+T cells is phenotypically and functionally heterogeneous. Therefore, a specific subpopulation of CD8+T cells should be further investigated. This study aims to dissect a subset of CD8+T cells expressing C-X-C motif chemokine receptor 5 (CXCR5) in chronic HBV infection. The frequency of CXCR5+CD8+T cells and the levels of C-X-C motif chemokine ligand 13 (CXCL13), a chemokine of CXCR5, were measured in patients with chronic HBV infection. C57BL/6, interleukin (IL)-21 receptor- or B cell-deficient mice were hydrodynamically injected with pAAV-HBV1.2 plasmids. Phenotype and functions of peripheral and intrahepatic CXCR5+ and CXCR5−CD8+T cells were assessed. CXCR5+CD8+T cells were partially exhausted but possessed a stronger antiviral ability than the CXCR5− subset in patients with chronic HBV infection; moreover, CXCR5+CD8+T cells were associated with a favorable treatment response in patients with chronic hepatitis B (CHB). High levels of CXCL13 from patients with CHB facilitated the recruitment of intrahepatic CXCR5+CD8+T cells, and this subpopulation produced high levels of HBV-specific interferon (IFN)-γ and IL-21. Notably, PD1 (programmed death 1) blockade and exogenous IL-21 enhanced the production of IFN-γ. More strikingly, mice injected with CXCR5+CD8+T cells showed remarkably decreased expression of HBsAg. Additionally, an impaired production of HBV-specific IFN-γ from intrahepatic CXCR5+CD8+T cells was observed in IL-21 receptor- or B cell-deficient mice. CXCL13 promotes the recruitment of CXCR5+CD8+T cells to the liver, and this subpopulation improves viral control in chronic HBV infection. The identification of this unique subpopulation may contribute to a better understanding of CD8+T cell functions and provide a potential immunotherapeutic target in chronic HBV infection. CD8+T cell responses are crucial to prevent and control HBV infection, but their activity is thought to be abolished during persistent HBV infection.Thus, functional defects in CD8+T cell responses, termed exhaustion, are considered the major factor driving the chronicity of HBV infection.1However, it is unclear how exhausted CD8+T cells are able to mediate the sustained inhibition of HBV replication in some patients with chronic HBV infection (CHB).Accumulated data have established that the pool of exhausted CD8+T cells consists of phenotypically and functionally distinct subpopulations.2,3Although CD8+T cells are poorly functional in chronic viral infections, they are not functionally inert.4–6Therefore, a distinct subpopulation of CD8+T cells in chronic HBV infection should be further identified.
Although CD8+T cell exhaustion hampers viral control during chronic HBV infection, the pool of CD8+T cells is phenotypically and functionally heterogeneous. Therefore, a specific subpopulation of CD8+T cells should be further investigated. This study aims to dissect a subset of CD8+T cells expressing C-X-C motif chemokine receptor 5 (CXCR5) in chronic HBV infection. The frequency of CXCR5+CD8+T cells and the levels of C-X-C motif chemokine ligand 13 (CXCL13), a chemokine of CXCR5, were measured in patients with chronic HBV infection. C57BL/6, interleukin (IL)-21 receptor- or B cell-deficient mice were hydrodynamically injected with pAAV-HBV1.2 plasmids. Phenotype and functions of peripheral and intrahepatic CXCR5+ and CXCR5−CD8+T cells were assessed. CXCR5+CD8+T cells were partially exhausted but possessed a stronger antiviral ability than the CXCR5− subset in patients with chronic HBV infection; moreover, CXCR5+CD8+T cells were associated with a favorable treatment response in patients with chronic hepatitis B (CHB). High levels of CXCL13 from patients with CHB facilitated the recruitment of intrahepatic CXCR5+CD8+T cells, and this subpopulation produced high levels of HBV-specific interferon (IFN)-γ and IL-21. Notably, PD1 (programmed death 1) blockade and exogenous IL-21 enhanced the production of IFN-γ. More strikingly, mice injected with CXCR5+CD8+T cells showed remarkably decreased expression of HBsAg. Additionally, an impaired production of HBV-specific IFN-γ from intrahepatic CXCR5+CD8+T cells was observed in IL-21 receptor- or B cell-deficient mice. CXCL13 promotes the recruitment of CXCR5+CD8+T cells to the liver, and this subpopulation improves viral control in chronic HBV infection. The identification of this unique subpopulation may contribute to a better understanding of CD8+T cell functions and provide a potential immunotherapeutic target in chronic HBV infection. Recently, a population of HIV-specific CD8+T cells expressing the C-X-C motif chemokine receptor type 5 (CXCR5) was shown to inversely correlate with serum viral levels in patients prior to antiretroviral therapy, and this subset displays more potent efficacy on viral replication than the CXCR5− cells in mice with chronic lymphocytic choriomeningitis virus (LCMV) infection.7These CXCR5+CD8+T cells, which were called “follicular cytotoxic T cells”, are able to migrate to B cell follicles and exert effects on CXCR5-expressing cells, such as B cell and follicular helper T cells.8,9These data suggest that CXCR5+CD8+T cells may exist in a reduced state of exhaustion and maintain suboptimal but critical functions, thus serving as an antiviral mechanism in chronic viral infection.We hypothesized that this situation may also exist in patients with chronic HBV infection, and that CXCR5+CD8+T cells may have an important role for even partial immune control of HBV replication in the setting of chronic infection.
Sofosbuvir, an NS5B inhibitor, combined with velpatasvir, an NS5A inhibitor (SOF/VEL), produces high sustained virologic response rates 12 weeks after treatment (SVR12) in patients with genotype 1–6 HCV infection, and has no anticipated clinically relevant drug-drug interactions with immunosuppressants. This study evaluated the safety and efficacy of SOF/VEL in adults with recurrent chronic genotype 1–4 HCV infection after liver transplant. Patients received SOF/VEL 400/100 mg daily for 12 weeks. Patients could be treatment experienced or treatment naïve with no cirrhosis or with compensated cirrhosis. The primary endpoints were SVR12 and discontinuations due to adverse events. A total of 79 patients were enrolled and treated in this study (37 [47%] had genotype 1, 3 [4%] genotype 2, 35 [44%] genotype 3, and 4 [5%] genotype 4 HCV). Of these, 81% were male, 82% were white, 18% had compensated cirrhosis, and 59% were treatment experienced. The most commonly used immunosuppressants were tacrolimus (71%), mycophenolic acid (24%), cyclosporine (14%), and azathioprine (11%). Median (range) time from liver transplantation was 7.5 (0.3, 23.9) years. The SVR12 rate was 96%. By genotype, SVR12 rates were 95% (genotype 1), 100% (genotype 2), 97% (genotype 3), and 100% (genotype 4). Two patients experienced virologic relapse: one with genotype 1a infection was non-cirrhotic and treatment naïve, and one with genotype 3 infection was non-cirrhotic and treatment experienced. One patient discontinued SOF/VEL due to hyperglycemia. No serious or severe adverse events were deemed SOF/VEL-related by the investigator, and no liver transplant rejection episodes or deaths occurred during the study period. Treatment with SOF/VEL for 12 weeks was highly effective and well tolerated in genotype 1–4 HCV-infected liver transplant recipients with and without cirrhosis. Sofosbuvir/velpatasvir is a combination of two drugs in one tablet that is approved for the treatment of patients with chronic hepatitis C virus (HCV) infection. When patients with chronic HCV infection receive a liver transplant, the HCV infection usually recurs, and damages the transplanted liver. This study tested the effects of 12 weeks of sofosbuvir/velpatasvir treatment in patients who had HCV recurrence after a liver transplant. Three months following the end of treatment, 96% of patients were cured of HCV infection. Among HCV-infected liver transplant recipients, HCV recurrence emerges in nearly all patients.1Within five years post-transplant, cirrhosis related to HCV ensues in approximately 30% of patients with recurrent, chronic HCV infection and is associated with increased graft loss rates and death.2–5In the setting of post-transplant immunosuppression, the rate of hepatic fibrosis is accelerated in HCV-infected patients compared to the pre-transplant period.6
Sofosbuvir, an NS5B inhibitor, combined with velpatasvir, an NS5A inhibitor (SOF/VEL), produces high sustained virologic response rates 12 weeks after treatment (SVR12) in patients with genotype 1–6 HCV infection, and has no anticipated clinically relevant drug-drug interactions with immunosuppressants. This study evaluated the safety and efficacy of SOF/VEL in adults with recurrent chronic genotype 1–4 HCV infection after liver transplant. Patients received SOF/VEL 400/100 mg daily for 12 weeks. Patients could be treatment experienced or treatment naïve with no cirrhosis or with compensated cirrhosis. The primary endpoints were SVR12 and discontinuations due to adverse events. A total of 79 patients were enrolled and treated in this study (37 [47%] had genotype 1, 3 [4%] genotype 2, 35 [44%] genotype 3, and 4 [5%] genotype 4 HCV). Of these, 81% were male, 82% were white, 18% had compensated cirrhosis, and 59% were treatment experienced. The most commonly used immunosuppressants were tacrolimus (71%), mycophenolic acid (24%), cyclosporine (14%), and azathioprine (11%). Median (range) time from liver transplantation was 7.5 (0.3, 23.9) years. The SVR12 rate was 96%. By genotype, SVR12 rates were 95% (genotype 1), 100% (genotype 2), 97% (genotype 3), and 100% (genotype 4). Two patients experienced virologic relapse: one with genotype 1a infection was non-cirrhotic and treatment naïve, and one with genotype 3 infection was non-cirrhotic and treatment experienced. One patient discontinued SOF/VEL due to hyperglycemia. No serious or severe adverse events were deemed SOF/VEL-related by the investigator, and no liver transplant rejection episodes or deaths occurred during the study period. Treatment with SOF/VEL for 12 weeks was highly effective and well tolerated in genotype 1–4 HCV-infected liver transplant recipients with and without cirrhosis. Sofosbuvir/velpatasvir is a combination of two drugs in one tablet that is approved for the treatment of patients with chronic hepatitis C virus (HCV) infection. When patients with chronic HCV infection receive a liver transplant, the HCV infection usually recurs, and damages the transplanted liver. This study tested the effects of 12 weeks of sofosbuvir/velpatasvir treatment in patients who had HCV recurrence after a liver transplant. Three months following the end of treatment, 96% of patients were cured of HCV infection. Historically, the therapeutic regimen to treat HCV recurrence in liver transplant recipients was the combination of interferon and ribavirin.This regimen was marked by sustained virologic response (SVR) rates of less than 30% and poor tolerability.7–9More recently, the regimen of sofosbuvir (a nucleotide analog HCV NS5B polymerase inhibitor) with ribavirin has led to improvements in tolerability and SVR rates coupled with shorter treatment duration.10,11Further improvement in SVR rates (88% to 97%) has been observed in liver transplant recipients treated with sofosbuvir in combination with a second direct-acting antiviral (DAA; simeprevir, ledipasvir, or daclatasvir) and ribavirin, or the regimen of ombitasvir, paritaprevir, ritonavir, dasabuvir, and ribavirin.12–16Despite these important advances in therapy, some regimens have suboptimal efficacy (<90% SVR rates), none of these regimens is pangenotypic, and all contain ribavirin, exposing patients to the known hematologic, constitutional, and neuropsychiatric effects.17,18Glecaprevir/pibrentasvir is a recently approved ribavirin-free regimen that led to high SVR rates in liver and kidney transplant recipients in clinical trials, but was not evaluated in cirrhotic or treatment-experienced genotype 3 HCV-infected transplant recipients.19Thus, among liver transplant recipients an unmet need remains for interferon- and ribavirin-free regimens that are effective for all genotypes regardless of prior treatment experience or cirrhosis status, are well tolerated, have high barriers to resistance, and low propensity for drug interactions.
Sofosbuvir, an NS5B inhibitor, combined with velpatasvir, an NS5A inhibitor (SOF/VEL), produces high sustained virologic response rates 12 weeks after treatment (SVR12) in patients with genotype 1–6 HCV infection, and has no anticipated clinically relevant drug-drug interactions with immunosuppressants. This study evaluated the safety and efficacy of SOF/VEL in adults with recurrent chronic genotype 1–4 HCV infection after liver transplant. Patients received SOF/VEL 400/100 mg daily for 12 weeks. Patients could be treatment experienced or treatment naïve with no cirrhosis or with compensated cirrhosis. The primary endpoints were SVR12 and discontinuations due to adverse events. A total of 79 patients were enrolled and treated in this study (37 [47%] had genotype 1, 3 [4%] genotype 2, 35 [44%] genotype 3, and 4 [5%] genotype 4 HCV). Of these, 81% were male, 82% were white, 18% had compensated cirrhosis, and 59% were treatment experienced. The most commonly used immunosuppressants were tacrolimus (71%), mycophenolic acid (24%), cyclosporine (14%), and azathioprine (11%). Median (range) time from liver transplantation was 7.5 (0.3, 23.9) years. The SVR12 rate was 96%. By genotype, SVR12 rates were 95% (genotype 1), 100% (genotype 2), 97% (genotype 3), and 100% (genotype 4). Two patients experienced virologic relapse: one with genotype 1a infection was non-cirrhotic and treatment naïve, and one with genotype 3 infection was non-cirrhotic and treatment experienced. One patient discontinued SOF/VEL due to hyperglycemia. No serious or severe adverse events were deemed SOF/VEL-related by the investigator, and no liver transplant rejection episodes or deaths occurred during the study period. Treatment with SOF/VEL for 12 weeks was highly effective and well tolerated in genotype 1–4 HCV-infected liver transplant recipients with and without cirrhosis. Sofosbuvir/velpatasvir is a combination of two drugs in one tablet that is approved for the treatment of patients with chronic hepatitis C virus (HCV) infection. When patients with chronic HCV infection receive a liver transplant, the HCV infection usually recurs, and damages the transplanted liver. This study tested the effects of 12 weeks of sofosbuvir/velpatasvir treatment in patients who had HCV recurrence after a liver transplant. Three months following the end of treatment, 96% of patients were cured of HCV infection. The single-tablet regimen of sofosbuvir and velpatasvir (SOF/VEL) combines sofosbuvir with the second-generation, pangenotypic HCV NS5A inhibitor velpatasvir; both drugs demonstrate potent antiviral activity against HCV replicons in genotypes 1 through 6.In phase III trials in non-transplant patients without cirrhosis or with compensated cirrhosis, a 12-week regimen of SOF/VEL led to SVR rates at 12 weeks after the end of treatment (SVR12) of 99% in patients with HCV genotypes 1, 2, 4, 5, and 6, and 95% in patients with HCV genotype 3, regardless of prior non-NS5A treatment experience.20,21These results have been confirmed in real-world trials.22–25Importantly, no drug interactions between SOF/VEL and immunosuppressant medications are anticipated.26,27These features make SOF/VEL well suited to address the unmet medical needs of liver transplant recipients.
Proof-of-concept studies frequently assess changes in intrahepatic triglyceride (IHTG) content by magnetic resonance-based techniques as a surrogate marker of histology. The aim of this study was to establish how reliable this strategy is to predict changes in liver histology in patients with non-alcoholic steatohepatitis (NASH). Patients with NASH who had participated in our prior randomized controlled trials of pioglitazone with complete paired data for IHTG content by magnetic resonance spectroscopy and liver histology were included in the study. A total of 121 patients were included. Changes in IHTG were assessed in several ways: as a continuous variable (correlations), as categorical groups (IHTG change ≥0%; or IHTG reduction of 1–30%; 31–50%; 51–70%; or >70%), and in a binomial way as steatosis resolution or not (defined as achieving IHTG <5.56%). Changes in IHTG correlated with steatosis on histology (r = 0.54; p <0.01). However, the magnitude of IHTG reduction was not associated with the rate of response of the primary histological outcome (2-point improvement in the NAFLD activity score from 2 different parameters, without worsening of fibrosis) or resolution of NASH without worsening of fibrosis, neither in patients receiving pioglitazone nor placebo. Changes in lobular inflammation, hepatocyte ballooning, or liver fibrosis were also independent of changes in IHTG, irrespective of treatment arm. Steatosis resolution was not associated with better histological outcomes either. Changes in IHTG predict changes in steatosis but not of other liver histological parameters. This implies that IHTG response to treatment should be interpreted with caution, as it may not be as reliable as previously believed to predict a treatment’s overall clinical efficacy in patients with NASH. In recent years, there has been a growing interest in the discovery of new pharmacological agents for the treatment of non-alcoholic fatty liver disease (NAFLD).This recent interest responds to an alarming rise in its prevalence and, more importantly, to the associated risk of progression to end-stage liver disease.1Several randomized, controlled trials have recently been completed,2–8 and many more are still ongoing.9
Proof-of-concept studies frequently assess changes in intrahepatic triglyceride (IHTG) content by magnetic resonance-based techniques as a surrogate marker of histology. The aim of this study was to establish how reliable this strategy is to predict changes in liver histology in patients with non-alcoholic steatohepatitis (NASH). Patients with NASH who had participated in our prior randomized controlled trials of pioglitazone with complete paired data for IHTG content by magnetic resonance spectroscopy and liver histology were included in the study. A total of 121 patients were included. Changes in IHTG were assessed in several ways: as a continuous variable (correlations), as categorical groups (IHTG change ≥0%; or IHTG reduction of 1–30%; 31–50%; 51–70%; or >70%), and in a binomial way as steatosis resolution or not (defined as achieving IHTG <5.56%). Changes in IHTG correlated with steatosis on histology (r = 0.54; p <0.01). However, the magnitude of IHTG reduction was not associated with the rate of response of the primary histological outcome (2-point improvement in the NAFLD activity score from 2 different parameters, without worsening of fibrosis) or resolution of NASH without worsening of fibrosis, neither in patients receiving pioglitazone nor placebo. Changes in lobular inflammation, hepatocyte ballooning, or liver fibrosis were also independent of changes in IHTG, irrespective of treatment arm. Steatosis resolution was not associated with better histological outcomes either. Changes in IHTG predict changes in steatosis but not of other liver histological parameters. This implies that IHTG response to treatment should be interpreted with caution, as it may not be as reliable as previously believed to predict a treatment’s overall clinical efficacy in patients with NASH. A percutaneous liver biopsy remains the gold-standard for the diagnosis of NASH and is required by the FDA for a new drug indication.10,11However, early proof-of-concept studies often use changes in intrahepatic triglyceride (IHTG) content by either magnetic resonance imaging-proton density fat fraction (MRI-PDFF) or proton magnetic resonance spectroscopy (1H-MRS) as a surrogate marker of resolution of NASH and/or improvement in fibrosis.12–14
Proof-of-concept studies frequently assess changes in intrahepatic triglyceride (IHTG) content by magnetic resonance-based techniques as a surrogate marker of histology. The aim of this study was to establish how reliable this strategy is to predict changes in liver histology in patients with non-alcoholic steatohepatitis (NASH). Patients with NASH who had participated in our prior randomized controlled trials of pioglitazone with complete paired data for IHTG content by magnetic resonance spectroscopy and liver histology were included in the study. A total of 121 patients were included. Changes in IHTG were assessed in several ways: as a continuous variable (correlations), as categorical groups (IHTG change ≥0%; or IHTG reduction of 1–30%; 31–50%; 51–70%; or >70%), and in a binomial way as steatosis resolution or not (defined as achieving IHTG <5.56%). Changes in IHTG correlated with steatosis on histology (r = 0.54; p <0.01). However, the magnitude of IHTG reduction was not associated with the rate of response of the primary histological outcome (2-point improvement in the NAFLD activity score from 2 different parameters, without worsening of fibrosis) or resolution of NASH without worsening of fibrosis, neither in patients receiving pioglitazone nor placebo. Changes in lobular inflammation, hepatocyte ballooning, or liver fibrosis were also independent of changes in IHTG, irrespective of treatment arm. Steatosis resolution was not associated with better histological outcomes either. Changes in IHTG predict changes in steatosis but not of other liver histological parameters. This implies that IHTG response to treatment should be interpreted with caution, as it may not be as reliable as previously believed to predict a treatment’s overall clinical efficacy in patients with NASH. IHTG content measured by either 1H-MRS or MRI-PDFF correlates well with steatosis assessed by histology.15–17Studies by Noureddin et al.16 and Middleton et al.17 demonstrated that longitudinal changes in IHTG content measured by magnetic resonance techniques strongly correlated with changes in steatosis on histology.However, while it is generally assumed that changes in steatosis are likely to be followed by similar changes in other histological parameters, as well as resolution of NASH, there is a paucity of information regarding this relationship.Moreover, the degree of IHTG content reduction required to improve lobular inflammation, hepatocyte ballooning, fibrosis, or to achieve resolution of NASH, remains unclear.In a small (n = 35) short-term (24 weeks) study, Patel et al. showed that histologic responders had a significantly higher reduction of liver fat compared to non-responders.18However, the authors defined histological response as a reduction in 2 points of the NAFLD activity score (NAS) score, which includes steatosis grade as a parameter.Therefore, it is possible that a reduction in the steatosis grade could have significantly confounded the association.More recently, the same group reported that changes in IHTG content during the phase II study of selonsertib did not predict changes in hepatocyte ballooning or fibrosis.19
Direct-acting antiviral (DAA) therapy for HCV has high efficacy and limited toxicity. We hypothesised that the efficacy of glecaprevir-pibrentasvir for chronic HCV with a simplified treatment monitoring schedule would be non-inferior to a standard treatment monitoring schedule. In this open-label multicentre phase IIIb trial, treatment-naïve adults with chronic HCV without cirrhosis were randomly assigned (2:1) to receive glecaprevir-pibrentasvir 300 mg–120 mg daily for 8 weeks administered with a simplified or standard monitoring strategy. Clinic visits occurred at baseline and post-treatment week 12 in the simplified arm, and at baseline, week 4, week 8, and post-treatment week 12 in the standard arm. Study nurse phone contact occurred at week 4 and week 8 in both arms. Participants requiring adherence support were not eligible, including those reporting recent injecting drug use. The primary endpoint was sustained virological response at post-treatment week 12 (SVR12), with a non-inferiority margin of 6%. Overall, 380 participants (60% male, 47% genotype 1, 32% genotype 3) with chronic HCV were randomised and treated with glecaprevir-pibrentasvir in the simplified (n = 253) and standard (n = 127) arms. In the intention-to-treat population, SVR12 was 92% (95% CI 89%–95%) in the simplified and 95% (95% CI 92%–99%) in the standard arm (difference between arms −3.2%; 95% CI −8.2% to 1.8%) and did not reach non-inferiority. In the per-protocol population, SVR12 was 97% (95% CI 96%–99%) in the simplified and 98% (95% CI 96%–100%) in the standard arm. No treatment-related serious adverse events were reported. In patients with chronic HCV infection without cirrhosis, treatment with glecaprevir-pibrentasvir was safe and effective. In comparison to standard monitoring, a simplified monitoring schedule did not achieve non-inferiority. Trial Registration: clinicaltrials.gov Identifier: NCT03117569. Globally, an estimated 71 million people have chronic HCV infection.1HCV treatment was interferon-based for more than 2 decades, with the addition of ribavirin,2 pegylated-interferon,3 and first-generation protease inhibitor direct-acting antiviral (DAA) therapies (telaprevir, boceprevir),4,5 providing stepwise improvements in efficacy as defined by sustained virological response (SVR).Despite these improvements, treatment uptake remained low in most countries, with <1% to 5% of people with chronic HCV initiating therapy each year.6
Direct-acting antiviral (DAA) therapy for HCV has high efficacy and limited toxicity. We hypothesised that the efficacy of glecaprevir-pibrentasvir for chronic HCV with a simplified treatment monitoring schedule would be non-inferior to a standard treatment monitoring schedule. In this open-label multicentre phase IIIb trial, treatment-naïve adults with chronic HCV without cirrhosis were randomly assigned (2:1) to receive glecaprevir-pibrentasvir 300 mg–120 mg daily for 8 weeks administered with a simplified or standard monitoring strategy. Clinic visits occurred at baseline and post-treatment week 12 in the simplified arm, and at baseline, week 4, week 8, and post-treatment week 12 in the standard arm. Study nurse phone contact occurred at week 4 and week 8 in both arms. Participants requiring adherence support were not eligible, including those reporting recent injecting drug use. The primary endpoint was sustained virological response at post-treatment week 12 (SVR12), with a non-inferiority margin of 6%. Overall, 380 participants (60% male, 47% genotype 1, 32% genotype 3) with chronic HCV were randomised and treated with glecaprevir-pibrentasvir in the simplified (n = 253) and standard (n = 127) arms. In the intention-to-treat population, SVR12 was 92% (95% CI 89%–95%) in the simplified and 95% (95% CI 92%–99%) in the standard arm (difference between arms −3.2%; 95% CI −8.2% to 1.8%) and did not reach non-inferiority. In the per-protocol population, SVR12 was 97% (95% CI 96%–99%) in the simplified and 98% (95% CI 96%–100%) in the standard arm. No treatment-related serious adverse events were reported. In patients with chronic HCV infection without cirrhosis, treatment with glecaprevir-pibrentasvir was safe and effective. In comparison to standard monitoring, a simplified monitoring schedule did not achieve non-inferiority. Trial Registration: clinicaltrials.gov Identifier: NCT03117569. Recent years have seen a revolution in HCV therapeutic development, with the availability of interferon-free DAA regimens.7Simple (once daily dosing oral regimens), well tolerated, short duration (8–12 weeks), pangenotypic DAA therapy with high efficacy (cure rates above 95%) is now standard of care for chronic HCV.The broad implementation of DAA therapy has considerable public health potential, with the World Health Organization setting ambitious HCV elimination impact targets for reductions in HCV incidence (80%) and liver disease mortality (65%) by 2030.8
Direct-acting antiviral (DAA) therapy for HCV has high efficacy and limited toxicity. We hypothesised that the efficacy of glecaprevir-pibrentasvir for chronic HCV with a simplified treatment monitoring schedule would be non-inferior to a standard treatment monitoring schedule. In this open-label multicentre phase IIIb trial, treatment-naïve adults with chronic HCV without cirrhosis were randomly assigned (2:1) to receive glecaprevir-pibrentasvir 300 mg–120 mg daily for 8 weeks administered with a simplified or standard monitoring strategy. Clinic visits occurred at baseline and post-treatment week 12 in the simplified arm, and at baseline, week 4, week 8, and post-treatment week 12 in the standard arm. Study nurse phone contact occurred at week 4 and week 8 in both arms. Participants requiring adherence support were not eligible, including those reporting recent injecting drug use. The primary endpoint was sustained virological response at post-treatment week 12 (SVR12), with a non-inferiority margin of 6%. Overall, 380 participants (60% male, 47% genotype 1, 32% genotype 3) with chronic HCV were randomised and treated with glecaprevir-pibrentasvir in the simplified (n = 253) and standard (n = 127) arms. In the intention-to-treat population, SVR12 was 92% (95% CI 89%–95%) in the simplified and 95% (95% CI 92%–99%) in the standard arm (difference between arms −3.2%; 95% CI −8.2% to 1.8%) and did not reach non-inferiority. In the per-protocol population, SVR12 was 97% (95% CI 96%–99%) in the simplified and 98% (95% CI 96%–100%) in the standard arm. No treatment-related serious adverse events were reported. In patients with chronic HCV infection without cirrhosis, treatment with glecaprevir-pibrentasvir was safe and effective. In comparison to standard monitoring, a simplified monitoring schedule did not achieve non-inferiority. Trial Registration: clinicaltrials.gov Identifier: NCT03117569. The capacity to scale-up DAA therapy should be enhanced by simplified treatment monitoring strategies.The coformulation of glecaprevir-pibrentasvir, an NS3/4a protease inhibitor and NS5A inhibitor combination, provides key features for HCV treatment simplification, including on-treatment monitoring: i) pan-genotypic activity with high efficacy; ii) minimal drug-related toxicity; iii) ease of dosing (3 pills once daily); and iv) short duration (8 weeks for patients without cirrhosis).In phase III clinical trials in patients without cirrhosis, an 8-week regimen of glecaprevir-pibrentasvir (300 mg–120 mg) led to SVR rates of 95–100% in HCV genotypes 1–6.9,10
Hepatic resection and liver transplantation with adjuvant chemo- and radiotherapy are the mainstay of hepatocellular carcinoma (HCC) treatment, but the 5-year survival rate remains poor because of frequent recurrence and intrahepatic metastasis. Only sorafenib and lenvatinib are currently approved for the first-line treatment of advanced, unresected HCC, but they yield modest survival benefits. Thus, there is a need to identify new therapeutic targets to improve current HCC treatment modalities. The HCC tumor model was generated by hydrodynamic transfection of AKT1 and β-catenin (CTNNB1) oncogenes. Cancer cells with stemness properties were characterized following isolation using side population (SP) and CD44 surface markers by flow cytometry. The effect of Jak/Stat inhibitors was analyzed in vitro by using tumorsphere culture and in vivo using an allograft mouse model. Co-activation of both Wnt/β-catenin and Akt/mTOR pathways was found in 14.4% of our HCC patient cohort. More importantly, these patients showed poorer survival than those with either Wnt/β-catenin or Akt/mTOR pathway activation alone, demonstrating the clinical relevance of our study. In addition, we observed that Akt/β-catenin tumors contained a subpopulation of cells with stem/progenitor-like characteristics identified through SP analysis and expression of the cancer stem cell-like marker CD44, which may contribute to tumor self-renewal and drug resistance. Consequently, we identified small molecule inhibitors of the Jak/Stat pathway that demonstrated efficacy in mitigating tumor proliferation and formation in Akt/β-catenin-driven HCC. In conclusion, we have shown that Akt/β-catenin tumors contain a subpopulation of tumor-initiating cells with stem/progenitor-like characteristics which can be effectively targeted with inhibitors of the Jak/Stat pathway, demonstrating that inhibition of the Jak/Stat pathway could be an alternative method to overcome drug resistance and effectively treat Akt/β-catenin-driven HCC tumors. Hepatocellular carcinoma (HCC) is the fourth leading cause of cancer-related death worldwide with a very high mortality rate.1Hepatitis B and C viral infections, aflatoxin B1, and alcohol abuse are main risks factors for HCC development.2Despite treatment with advanced surgical resection, liver transplantation, or ablation, the 5-year survival rate for HCC patients remains poor due to frequent recurrence and intrahepatic metastasis.3Currently, the first-line FDA-approved drugs for the treatment of advanced HCC are sorafenib and lenvatinib, both of which are multi-kinase inhibitors of Raf, VEGF, PDGF and c-kit signaling with anti-proliferative and anti-angiogenic activity.4–6However, patients treated with these multi-kinase inhibitors showed marginal overall survival improvements of 2 to 3 months compared to placebo group, with low partial treatment response rates due to drug resistance in most instances.7Therefore, there is a need to understand the biology of hepatocarcinogenesis and identify new therapeutic targets to improve current HCC treatment modalities.
Hepatic resection and liver transplantation with adjuvant chemo- and radiotherapy are the mainstay of hepatocellular carcinoma (HCC) treatment, but the 5-year survival rate remains poor because of frequent recurrence and intrahepatic metastasis. Only sorafenib and lenvatinib are currently approved for the first-line treatment of advanced, unresected HCC, but they yield modest survival benefits. Thus, there is a need to identify new therapeutic targets to improve current HCC treatment modalities. The HCC tumor model was generated by hydrodynamic transfection of AKT1 and β-catenin (CTNNB1) oncogenes. Cancer cells with stemness properties were characterized following isolation using side population (SP) and CD44 surface markers by flow cytometry. The effect of Jak/Stat inhibitors was analyzed in vitro by using tumorsphere culture and in vivo using an allograft mouse model. Co-activation of both Wnt/β-catenin and Akt/mTOR pathways was found in 14.4% of our HCC patient cohort. More importantly, these patients showed poorer survival than those with either Wnt/β-catenin or Akt/mTOR pathway activation alone, demonstrating the clinical relevance of our study. In addition, we observed that Akt/β-catenin tumors contained a subpopulation of cells with stem/progenitor-like characteristics identified through SP analysis and expression of the cancer stem cell-like marker CD44, which may contribute to tumor self-renewal and drug resistance. Consequently, we identified small molecule inhibitors of the Jak/Stat pathway that demonstrated efficacy in mitigating tumor proliferation and formation in Akt/β-catenin-driven HCC. In conclusion, we have shown that Akt/β-catenin tumors contain a subpopulation of tumor-initiating cells with stem/progenitor-like characteristics which can be effectively targeted with inhibitors of the Jak/Stat pathway, demonstrating that inhibition of the Jak/Stat pathway could be an alternative method to overcome drug resistance and effectively treat Akt/β-catenin-driven HCC tumors. Hepatocarcinogenesis is a multistep process involving multiple signaling cascades which contribute to a heterogeneous molecular profile.Recent deep molecular profiling efforts have identified several main mutations in HCC including the gene for β-catenin, CTNNB1 which occurs at a frequency of about 30% of all HCC tumors.8In addition, dysregulated signaling pathways of the Wnt/β-catenin, Ras/Raf/MEK/ERK, and PI3K/Akt/mTOR axis have been frequently identified in HCC.8In particular, components of the Wnt/β-catenin pathway (e.g. APC, β-catenin and GSK3β) are dysregulated in more than 25% of patients with HCC.Importantly, transcriptomic analysis of HCC clinical specimens identified specific pathway activation, in particular, the Wnt and Akt pathways that are associated with distinct clinical outcomes.9Moreover, Wnt and Akt pathway activation was observed in more than 50% of the tumors studied, suggesting that therapies targeting these activated signaling modules could have important clinical implications.
There is an emerging need to assess the metabolic state of liver allografts especially in the novel setting of machine perfusion preservation and donor in cardiac death (DCD) grafts. High-resolution magic-angle-spinning nuclear magnetic resonance (HR-MAS-NMR) could be a useful tool in this setting as it can extemporaneously provide untargeted metabolic profiling. The purpose of this study was to evaluate the potential value of HR-MAS-NMR metabolomic analysis of back-table biopsies for the prediction of early allograft dysfunction (EAD) and donor-recipient matching. The metabolic profiles of back-table biopsies obtained by HR-MAS-NMR, were compared according to the presence of EAD using partial least squares discriminant analysis. Network analysis was used to identify metabolites which changed significantly. The profiles were compared to native livers to identify metabolites for donor-recipient matching. The metabolic profiles were significantly different in grafts that caused EAD compared to those that did not. The constructed model can be used to predict the graft outcome with excellent accuracy. The metabolites showing the most significant differences were lactate level >8.3 mmol/g and phosphocholine content >0.646 mmol/g, which were significantly associated with graft dysfunction with an excellent accuracy (AUROClactates = 0.906; AUROCphosphocholine = 0.816). Native livers from patients with sarcopenia had low lactate and glycerophosphocholine content. In patients with sarcopenia, the risk of EAD was significantly higher when transplanting a graft with a high-risk graft metabolic score. This study underlines the cost of metabolic adaptation, identifying lactate and choline-derived metabolites as predictors of poor graft function in both native livers and liver grafts. HR-MAS-NMR seems a valid technique to evaluate graft quality and the consequences of cold ischemia on the graft. It could be used to assess the efficiency of graft resuscitation on machine perfusion in future studies. Liver transplantation is a life-saving procedure for patients with end-stage liver disease and a potentially curative treatment for hepatocellular carcinoma.The major limitation for liver transplantation is the current organ shortage caused by an increasing discrepancy between indications and a stable donor pool.In an attempt to answer to the issue of the increasing number of patients on waiting lists, many teams have extended the criteria for acceptance of liver grafts.Although there is hardly a wide consensus on its definition, extended criteria donors (ECD) represent a growing proportion of the donors.Many donor factors have been reported as influencing the outcome of liver transplantation mainly age,1 steatosis2,3 and cold ischemia time.4While ECD, particularly DCD, has increased the pool of donors, it may be associated with higher graft loss5 or increased risk of vascular and biliary complications.6,7Indeed the tolerance of allograft to cold ischemia-reperfusion injury is altered in ECD allografts.8,9
There is an emerging need to assess the metabolic state of liver allografts especially in the novel setting of machine perfusion preservation and donor in cardiac death (DCD) grafts. High-resolution magic-angle-spinning nuclear magnetic resonance (HR-MAS-NMR) could be a useful tool in this setting as it can extemporaneously provide untargeted metabolic profiling. The purpose of this study was to evaluate the potential value of HR-MAS-NMR metabolomic analysis of back-table biopsies for the prediction of early allograft dysfunction (EAD) and donor-recipient matching. The metabolic profiles of back-table biopsies obtained by HR-MAS-NMR, were compared according to the presence of EAD using partial least squares discriminant analysis. Network analysis was used to identify metabolites which changed significantly. The profiles were compared to native livers to identify metabolites for donor-recipient matching. The metabolic profiles were significantly different in grafts that caused EAD compared to those that did not. The constructed model can be used to predict the graft outcome with excellent accuracy. The metabolites showing the most significant differences were lactate level >8.3 mmol/g and phosphocholine content >0.646 mmol/g, which were significantly associated with graft dysfunction with an excellent accuracy (AUROClactates = 0.906; AUROCphosphocholine = 0.816). Native livers from patients with sarcopenia had low lactate and glycerophosphocholine content. In patients with sarcopenia, the risk of EAD was significantly higher when transplanting a graft with a high-risk graft metabolic score. This study underlines the cost of metabolic adaptation, identifying lactate and choline-derived metabolites as predictors of poor graft function in both native livers and liver grafts. HR-MAS-NMR seems a valid technique to evaluate graft quality and the consequences of cold ischemia on the graft. It could be used to assess the efficiency of graft resuscitation on machine perfusion in future studies. Donor-recipient matching is of utmost importance in this setting.Model for end-stage liver disease (MELD) score and life-support therapies have been identified as significant recipient factors that alter results, specifically when using extended criteria donor.The balance of risk (BAR) score is an example of application of such donor-recipient matching.Additionally, the addition of graft steatosis in the score further enhances the accuracy of the BAR score.10
There is an emerging need to assess the metabolic state of liver allografts especially in the novel setting of machine perfusion preservation and donor in cardiac death (DCD) grafts. High-resolution magic-angle-spinning nuclear magnetic resonance (HR-MAS-NMR) could be a useful tool in this setting as it can extemporaneously provide untargeted metabolic profiling. The purpose of this study was to evaluate the potential value of HR-MAS-NMR metabolomic analysis of back-table biopsies for the prediction of early allograft dysfunction (EAD) and donor-recipient matching. The metabolic profiles of back-table biopsies obtained by HR-MAS-NMR, were compared according to the presence of EAD using partial least squares discriminant analysis. Network analysis was used to identify metabolites which changed significantly. The profiles were compared to native livers to identify metabolites for donor-recipient matching. The metabolic profiles were significantly different in grafts that caused EAD compared to those that did not. The constructed model can be used to predict the graft outcome with excellent accuracy. The metabolites showing the most significant differences were lactate level >8.3 mmol/g and phosphocholine content >0.646 mmol/g, which were significantly associated with graft dysfunction with an excellent accuracy (AUROClactates = 0.906; AUROCphosphocholine = 0.816). Native livers from patients with sarcopenia had low lactate and glycerophosphocholine content. In patients with sarcopenia, the risk of EAD was significantly higher when transplanting a graft with a high-risk graft metabolic score. This study underlines the cost of metabolic adaptation, identifying lactate and choline-derived metabolites as predictors of poor graft function in both native livers and liver grafts. HR-MAS-NMR seems a valid technique to evaluate graft quality and the consequences of cold ischemia on the graft. It could be used to assess the efficiency of graft resuscitation on machine perfusion in future studies. Among significant factors impacting early outcomes after liver transplantation, sarcopenia and portal hypertension have been increasingly studied in the last years.Both are probably linked, as ascites is often associated with malnutrition, and portal hypertension may be a surrogate of long-term evolving cirrhosis.Sarcopenia may be a marker of significant metabolic shift and there are currently no clear data on differentially expressed metabolites in the liver of patients with cirrhosis and sarcopenia.
There is an emerging need to assess the metabolic state of liver allografts especially in the novel setting of machine perfusion preservation and donor in cardiac death (DCD) grafts. High-resolution magic-angle-spinning nuclear magnetic resonance (HR-MAS-NMR) could be a useful tool in this setting as it can extemporaneously provide untargeted metabolic profiling. The purpose of this study was to evaluate the potential value of HR-MAS-NMR metabolomic analysis of back-table biopsies for the prediction of early allograft dysfunction (EAD) and donor-recipient matching. The metabolic profiles of back-table biopsies obtained by HR-MAS-NMR, were compared according to the presence of EAD using partial least squares discriminant analysis. Network analysis was used to identify metabolites which changed significantly. The profiles were compared to native livers to identify metabolites for donor-recipient matching. The metabolic profiles were significantly different in grafts that caused EAD compared to those that did not. The constructed model can be used to predict the graft outcome with excellent accuracy. The metabolites showing the most significant differences were lactate level >8.3 mmol/g and phosphocholine content >0.646 mmol/g, which were significantly associated with graft dysfunction with an excellent accuracy (AUROClactates = 0.906; AUROCphosphocholine = 0.816). Native livers from patients with sarcopenia had low lactate and glycerophosphocholine content. In patients with sarcopenia, the risk of EAD was significantly higher when transplanting a graft with a high-risk graft metabolic score. This study underlines the cost of metabolic adaptation, identifying lactate and choline-derived metabolites as predictors of poor graft function in both native livers and liver grafts. HR-MAS-NMR seems a valid technique to evaluate graft quality and the consequences of cold ischemia on the graft. It could be used to assess the efficiency of graft resuscitation on machine perfusion in future studies. There is currently a lack of effective tools and biomarkers to evaluate the liver grafts before implantation.Liver biopsies with fibrosis and steatosis assessment may be informative,11 but do not take into consideration metabolic insults caused by static cold storage.Interpretation of frozen section biopsies can be confusing and can mislead clinicians who have to decide whether to use marginal grafts.12In order to further extend and enhance the quality of the grafts, machine perfusion is currently under intense evaluation for liver grafts.13–15The benefit of dynamic cold storage has been demonstrated in kidney transplantation16 and a growing set of data support its use in liver transplantation.The data available tend to show efficient “resuscitation” of ECD grafts especially steatotic grafts.17,18
There is an emerging need to assess the metabolic state of liver allografts especially in the novel setting of machine perfusion preservation and donor in cardiac death (DCD) grafts. High-resolution magic-angle-spinning nuclear magnetic resonance (HR-MAS-NMR) could be a useful tool in this setting as it can extemporaneously provide untargeted metabolic profiling. The purpose of this study was to evaluate the potential value of HR-MAS-NMR metabolomic analysis of back-table biopsies for the prediction of early allograft dysfunction (EAD) and donor-recipient matching. The metabolic profiles of back-table biopsies obtained by HR-MAS-NMR, were compared according to the presence of EAD using partial least squares discriminant analysis. Network analysis was used to identify metabolites which changed significantly. The profiles were compared to native livers to identify metabolites for donor-recipient matching. The metabolic profiles were significantly different in grafts that caused EAD compared to those that did not. The constructed model can be used to predict the graft outcome with excellent accuracy. The metabolites showing the most significant differences were lactate level >8.3 mmol/g and phosphocholine content >0.646 mmol/g, which were significantly associated with graft dysfunction with an excellent accuracy (AUROClactates = 0.906; AUROCphosphocholine = 0.816). Native livers from patients with sarcopenia had low lactate and glycerophosphocholine content. In patients with sarcopenia, the risk of EAD was significantly higher when transplanting a graft with a high-risk graft metabolic score. This study underlines the cost of metabolic adaptation, identifying lactate and choline-derived metabolites as predictors of poor graft function in both native livers and liver grafts. HR-MAS-NMR seems a valid technique to evaluate graft quality and the consequences of cold ischemia on the graft. It could be used to assess the efficiency of graft resuscitation on machine perfusion in future studies. Metabolomics is an emerging area in the omics field consisting of the simultaneous evaluation of cellular metabolic products on liquid or solid phase.Nuclear magnetic resonance (NMR) spectroscopy metabolomics has already been applied in the field of liver transplantation,19 but only mass spectroscopy has been shown to be predictive of allograft dysfunction.20Whereas many metabolomic methods are not relevant for clinical practice because they need complex sample handling and long treatment time, 1H high-resolution magic-angle-spinning NMR (1H HR-MAS NMR) spectroscopy is an attractive solution.Metabolomic profile of solid fresh frozen biopsy can be obtained in a short period of time without destruction of the sample, thus enabling further classical histopathological evaluation.
Although off-label use of sofosbuvir-containing regimens occurs regularly in patients with hepatitis C virus (HCV) infection undergoing dialysis for severe renal impairment or end-stage renal disease (ESRD), these regimens are not licensed for this indication, and there is an absence of dosing recommendations in this population. This study evaluated the safety and efficacy of sofosbuvir/velpatasvir in patients with HCV infection with ESRD undergoing dialysis. In this phase II, single-arm study, 59 patients with genotype 1–6 HCV infection with ESRD undergoing hemodialysis or peritoneal dialysis received open-label sofosbuvir/velpatasvir (400 mg/100 mg) once daily for 12 weeks. Patients were HCV treatment naive or treatment experienced without cirrhosis or with compensated cirrhosis. Patients previously treated with any HCV NS5A inhibitor were not eligible. The primary efficacy endpoint was the proportion of patients achieving sustained virologic response (SVR) 12 weeks after discontinuation of treatment (SVR12). The primary safety endpoint was the proportion of patients who discontinued study drug due to adverse events. Overall, 56 of 59 patients achieved SVR12 (95%; 95% CI 86–99%). Of the 3 patients who did not achieve SVR12, 2 patients had virologic relapse determined at post-treatment Week 4 (including 1 who prematurely discontinued study treatment), and 1 patient died from suicide after achieving SVR through post-treatment Week 4. The most common adverse events were headache (17%), fatigue (14%), nausea (14%), and vomiting (14%). Serious adverse events were reported for 11 patients (19%), and all were deemed to be unrelated to sofosbuvir/velpatasvir. Treatment with sofosbuvir/velpatasvir for 12 weeks was safe and effective in patients with ESRD undergoing dialysis. Sofosbuvir/velpatasvir is a combination direct-acting antiviral that is approved for treatment of patients with hepatitis C virus (HCV) infection. Despite the lack of dosing recommendations, sofosbuvir-containing regimens (including sofosbuvir/velpatasvir) are frequently used for HCV-infected patients undergoing dialysis. This study evaluated the safety and efficacy of sofosbuvir/velpatasvir for 12 weeks in patients with HCV infection who were undergoing dialysis. Treatment with sofosbuvir/velpatasvir was safe and well tolerated, resulting in a cure rate of 95% in patients with HCV infection and end-stage renal disease. Hepatitis C virus (HCV) infection is a global health challenge with an estimated 71 million individuals infected worldwide.1The disease burden of HCV infection is due to progression of chronic liver disease, which can lead to cirrhosis, liver failure, hepatocellular carcinoma, and death.Chronic HCV infection is also independently associated with the development of renal impairment referred to as chronic kidney disease (CKD) and has been shown to be more prevalent in patients with renal disease.2,3Chronic HCV infection has a significant negative impact on morbidity and mortality in patients undergoing dialysis.4HCV-infected patients with CKD have an accelerated rate of loss of kidney function, risk of progression to end-stage renal disease (ESRD), and increased risk of all-cause mortality when undergoing dialysis.2,5–7
Although off-label use of sofosbuvir-containing regimens occurs regularly in patients with hepatitis C virus (HCV) infection undergoing dialysis for severe renal impairment or end-stage renal disease (ESRD), these regimens are not licensed for this indication, and there is an absence of dosing recommendations in this population. This study evaluated the safety and efficacy of sofosbuvir/velpatasvir in patients with HCV infection with ESRD undergoing dialysis. In this phase II, single-arm study, 59 patients with genotype 1–6 HCV infection with ESRD undergoing hemodialysis or peritoneal dialysis received open-label sofosbuvir/velpatasvir (400 mg/100 mg) once daily for 12 weeks. Patients were HCV treatment naive or treatment experienced without cirrhosis or with compensated cirrhosis. Patients previously treated with any HCV NS5A inhibitor were not eligible. The primary efficacy endpoint was the proportion of patients achieving sustained virologic response (SVR) 12 weeks after discontinuation of treatment (SVR12). The primary safety endpoint was the proportion of patients who discontinued study drug due to adverse events. Overall, 56 of 59 patients achieved SVR12 (95%; 95% CI 86–99%). Of the 3 patients who did not achieve SVR12, 2 patients had virologic relapse determined at post-treatment Week 4 (including 1 who prematurely discontinued study treatment), and 1 patient died from suicide after achieving SVR through post-treatment Week 4. The most common adverse events were headache (17%), fatigue (14%), nausea (14%), and vomiting (14%). Serious adverse events were reported for 11 patients (19%), and all were deemed to be unrelated to sofosbuvir/velpatasvir. Treatment with sofosbuvir/velpatasvir for 12 weeks was safe and effective in patients with ESRD undergoing dialysis. Sofosbuvir/velpatasvir is a combination direct-acting antiviral that is approved for treatment of patients with hepatitis C virus (HCV) infection. Despite the lack of dosing recommendations, sofosbuvir-containing regimens (including sofosbuvir/velpatasvir) are frequently used for HCV-infected patients undergoing dialysis. This study evaluated the safety and efficacy of sofosbuvir/velpatasvir for 12 weeks in patients with HCV infection who were undergoing dialysis. Treatment with sofosbuvir/velpatasvir was safe and well tolerated, resulting in a cure rate of 95% in patients with HCV infection and end-stage renal disease. Over the last 2 years, direct-acting antiviral agents have been approved for use in patients with HCV infection and CKD.However, approved HCV treatments for patients with ESRD are associated with drug-drug interactions, baseline resistance testing, risk of hepatotoxicity, and contraindication for those with decompensated liver disease.8–10Additionally, some of these regimens are not pangenotypic.
Although off-label use of sofosbuvir-containing regimens occurs regularly in patients with hepatitis C virus (HCV) infection undergoing dialysis for severe renal impairment or end-stage renal disease (ESRD), these regimens are not licensed for this indication, and there is an absence of dosing recommendations in this population. This study evaluated the safety and efficacy of sofosbuvir/velpatasvir in patients with HCV infection with ESRD undergoing dialysis. In this phase II, single-arm study, 59 patients with genotype 1–6 HCV infection with ESRD undergoing hemodialysis or peritoneal dialysis received open-label sofosbuvir/velpatasvir (400 mg/100 mg) once daily for 12 weeks. Patients were HCV treatment naive or treatment experienced without cirrhosis or with compensated cirrhosis. Patients previously treated with any HCV NS5A inhibitor were not eligible. The primary efficacy endpoint was the proportion of patients achieving sustained virologic response (SVR) 12 weeks after discontinuation of treatment (SVR12). The primary safety endpoint was the proportion of patients who discontinued study drug due to adverse events. Overall, 56 of 59 patients achieved SVR12 (95%; 95% CI 86–99%). Of the 3 patients who did not achieve SVR12, 2 patients had virologic relapse determined at post-treatment Week 4 (including 1 who prematurely discontinued study treatment), and 1 patient died from suicide after achieving SVR through post-treatment Week 4. The most common adverse events were headache (17%), fatigue (14%), nausea (14%), and vomiting (14%). Serious adverse events were reported for 11 patients (19%), and all were deemed to be unrelated to sofosbuvir/velpatasvir. Treatment with sofosbuvir/velpatasvir for 12 weeks was safe and effective in patients with ESRD undergoing dialysis. Sofosbuvir/velpatasvir is a combination direct-acting antiviral that is approved for treatment of patients with hepatitis C virus (HCV) infection. Despite the lack of dosing recommendations, sofosbuvir-containing regimens (including sofosbuvir/velpatasvir) are frequently used for HCV-infected patients undergoing dialysis. This study evaluated the safety and efficacy of sofosbuvir/velpatasvir for 12 weeks in patients with HCV infection who were undergoing dialysis. Treatment with sofosbuvir/velpatasvir was safe and well tolerated, resulting in a cure rate of 95% in patients with HCV infection and end-stage renal disease. Treatment regimens containing the NS5B inhibitor, sofosbuvir, are the most widely prescribed treatments for HCV infection worldwide.The predominant circulating metabolite of sofosbuvir, GS-331007, is renally cleared and accumulates in patients with severe renal impairment or ESRD, which has resulted in the exclusion of this population in prior clinical trials, and consequently, a lack of dosing recommendations for patients with ESRD.However, real-world case series in patients with ESRD undergoing dialysis demonstrate substantial use of sofosbuvir-based regimens in this population, with no safety concerns identified.11–17
The macrophage scavenger receptor 1 (Msr1, also called SRA) is a pattern recognition receptor primarily expressed on myeloid cells, which plays an important role in the maintenance of immune homeostasis. Since MSR1 expression was upregulated in the livers of patients with fulminant hepatitis (FH), we investigated the functional mechanism of Msr1 in FH pathogenesis. Msr1-deficient (Msr1−/−) mice and their wild-type (WT) littermates were infected with mouse hepatitis virus strain-A59 (MHV-A59) to induce FH, and the levels of tissue damage, serum alanine aminotransferase, inflammatory cytokines and complement component 5a (C5a) were measured and compared. Liver injury was studied after MHV infection with or without neutrophil depletion. Our results showed that Msr1−/− mice were resistant to MHV-induced hepatitis. Treatment with the C5a receptor antagonist (C5aRa) diminished the differences in inflammatory responses and liver injury between MHV-infected wild-type and Msr1−/− mice, suggesting that C5a-induced pro-inflammatory response plays a critical role in the Msr1-mediated regulation of FH pathogenesis. We demonstrated that Msr1 efficiently enhanced transforming growth factor-activated kinase-1 phosphorylation in neutrophils upon MHV-A59 stimulation, thereby promoting the activation of the extracellular signal-regulated kinase pathway and subsequent NETosis formation. Moreover, we provided evidence that blockage of Msr1 attenuated the liver damage caused by MHV-A59 infection. Msr1 promotes the pathogenesis of virus-induced FH by enhancing induction of neutrophil NETosis and subsequent complement activation. Targeting Msr1 may be employed as a new immunotherapeutic strategy for FH. Fulminant hepatitis (FH) is a rare but potentially fatal disease caused by viral infection or inflammatory destruction of liver tissue.It is characterized by rapid deterioration of hepatic functions, massive hepatocyte necrosis, and hepatic encephalopathy.1Despite the recent therapeutic advances, FH is still associated with significant mortality.Mouse hepatitis virus (MHV) is a positive-strand RNA virus that causes a variety of diseases, including hepatitis, enteritis, and encephalitis in mice.2Noteworthy, the MHV-induced disease profiles are dependent on the viral strain and infection route, as well as genetic background and immune status of the mice.2,3Intraperitoneal infection of susceptible strains of mice (i.e. BALB/cJ, C57BL/6) with hepatotropic MHV, such as MHV-3 and MHV-A59, results in sinusoidal thrombosis and hepatocellular necrosis, and thus, has served as a useful model for FH in humans.4,5
The macrophage scavenger receptor 1 (Msr1, also called SRA) is a pattern recognition receptor primarily expressed on myeloid cells, which plays an important role in the maintenance of immune homeostasis. Since MSR1 expression was upregulated in the livers of patients with fulminant hepatitis (FH), we investigated the functional mechanism of Msr1 in FH pathogenesis. Msr1-deficient (Msr1−/−) mice and their wild-type (WT) littermates were infected with mouse hepatitis virus strain-A59 (MHV-A59) to induce FH, and the levels of tissue damage, serum alanine aminotransferase, inflammatory cytokines and complement component 5a (C5a) were measured and compared. Liver injury was studied after MHV infection with or without neutrophil depletion. Our results showed that Msr1−/− mice were resistant to MHV-induced hepatitis. Treatment with the C5a receptor antagonist (C5aRa) diminished the differences in inflammatory responses and liver injury between MHV-infected wild-type and Msr1−/− mice, suggesting that C5a-induced pro-inflammatory response plays a critical role in the Msr1-mediated regulation of FH pathogenesis. We demonstrated that Msr1 efficiently enhanced transforming growth factor-activated kinase-1 phosphorylation in neutrophils upon MHV-A59 stimulation, thereby promoting the activation of the extracellular signal-regulated kinase pathway and subsequent NETosis formation. Moreover, we provided evidence that blockage of Msr1 attenuated the liver damage caused by MHV-A59 infection. Msr1 promotes the pathogenesis of virus-induced FH by enhancing induction of neutrophil NETosis and subsequent complement activation. Targeting Msr1 may be employed as a new immunotherapeutic strategy for FH. Fibrin deposition and thrombosis within the microvasculature is now recognized to play a pivotal role in the hepatocellular injury in experimental and human FH.6,7Indeed, fibrinogen-like protein 2 (Fgl2) has been shown previously to have serine protease activity capable of directly cleaving prothrombin to thrombin, leading to fibrin deposition.8In the setting of mice with MHV infection, the induction of Fgl2 was seen in macrophages and endothelial cells in the liver, followed by fibrin deposition and liver necrosis.6Strikingly, pretreatment with neutralizing antibodies against Fgl2 prevented the coagulation disturbance and hepatic necrosis, thereby reducing mortality in infected mice.9In addition to Fgl2, accumulating evidence suggests that activation of immune cells and subsequent production of pro-inflammatory cytokines (i.e. tumour necrosis factor-α [TNF-α], interleukin [IL]-1β, IL-6, and interferon-γ [IFN-γ]) play a critical role in the pathogenesis of FH.7,10–12Moreover, MHV infection causes massive complement activation and elevation of pro-inflammatory complement component 5a (C5a) has been proposed to accelerate the development of FH.7,10Therefore, understanding the immunological mechanisms of FH will provide opportunities to potentially develop more pervasive and efficient treatment of FH for the benefits of the patients.
The macrophage scavenger receptor 1 (Msr1, also called SRA) is a pattern recognition receptor primarily expressed on myeloid cells, which plays an important role in the maintenance of immune homeostasis. Since MSR1 expression was upregulated in the livers of patients with fulminant hepatitis (FH), we investigated the functional mechanism of Msr1 in FH pathogenesis. Msr1-deficient (Msr1−/−) mice and their wild-type (WT) littermates were infected with mouse hepatitis virus strain-A59 (MHV-A59) to induce FH, and the levels of tissue damage, serum alanine aminotransferase, inflammatory cytokines and complement component 5a (C5a) were measured and compared. Liver injury was studied after MHV infection with or without neutrophil depletion. Our results showed that Msr1−/− mice were resistant to MHV-induced hepatitis. Treatment with the C5a receptor antagonist (C5aRa) diminished the differences in inflammatory responses and liver injury between MHV-infected wild-type and Msr1−/− mice, suggesting that C5a-induced pro-inflammatory response plays a critical role in the Msr1-mediated regulation of FH pathogenesis. We demonstrated that Msr1 efficiently enhanced transforming growth factor-activated kinase-1 phosphorylation in neutrophils upon MHV-A59 stimulation, thereby promoting the activation of the extracellular signal-regulated kinase pathway and subsequent NETosis formation. Moreover, we provided evidence that blockage of Msr1 attenuated the liver damage caused by MHV-A59 infection. Msr1 promotes the pathogenesis of virus-induced FH by enhancing induction of neutrophil NETosis and subsequent complement activation. Targeting Msr1 may be employed as a new immunotherapeutic strategy for FH. The macrophage scavenger receptor 1 (Msr1), also known as scavenger receptor type A (SRA), is preferentially expressed on myeloid cells, such as macrophages and dendritic cells (DCs).13Recently, Ozment et al. reported that Msr1 is present on circulating and bone marrow neutrophils and was upregulated in response to toll-like receptor 2 stimulation.14Msr1 acts as an innate pattern recognition receptor capable of recognizing a broad spectrum of “self” and “non-self” ligands, it thereby plays an important role in host defense against foreign microbial infections and the maintenance of immune homeostasis.13Importantly, emerging studies suggest that Msr1 participates in the pathogenesis of multiple inflammatory diseases and that its immunomodulatory function may be uncoupled from its endocytic/phagocytic activity.15We previously found that interference of Msr1-mediated regulatory pathways resulted in hypersensitivity of mice to T cell-mediated hepatitis.Interestingly, transfer of Msr1-positive hepatic mononuclear cells or administration of Msr1 protein could decrease the severity of liver damage.16However, little is known about the role of Msr1 in the pathogenesis of virus-induced FH.
Chronic hepatitis C virus (HCV) infection is a global health burden. Although HCV infection rarely contributes to morbidity during childhood, most HCV-infected children develop chronic HCV with a lifetime risk of liver disease. Little is known about the development of long-term liver disease and the effect of treatment in patients infected with HCV in childhood. This study was a retrospective review of patients infected with HCV in childhood enrolled in HCV Research UK. A total of 1,049 patients were identified and included. The main routes of infection were intravenous drug use (53%), blood product exposure (24%) and perinatal infection (11%). Liver disease developed in 32% of patients, a median of 33 years after infection, irrespective of the mode of infection. Therefore, patients with perinatal exposure developed cirrhosis at an earlier age than the rest of the risk groups. The incidence of hepatocellular carcinoma (HCC) was 5%, liver transplant 4% and death occurred in 3%. Overall, 663 patients were treated (55% with interferon/pegylated interferon and 40% with direct-acting antivirals). Sustained virological response (SVR) was achieved in 406 (75%). There was a higher mortality rate among patients without SVR vs. those with SVR (5% vs. 1%, p = 0.003). Treatment was more effective in patients without cirrhosis and disease progression was less frequent (13%) than in patients with cirrhosis at the time of therapy (28%) p < 0.001. Patients with cirrhosis were more likely to develop HCC, require liver transplantation, or die. HCV infection in young people causes significant liver disease, which can now be prevented with antiviral therapy. Early treatment, especially before development of cirrhosis is essential. Detection of HCV should be aimed at relevant risk groups and antiviral therapy should be made available in childhood to prevent long-term liver disease and spread of HCV. Chronic hepatitis C virus (HCV) infection is a global health burden with an estimated prevalence varying between 0.6%–10% dependent on geographical location and an estimated 71 million people worldwide with chronic infection.1–3In Western Europe, the estimated prevalence is 1.5%–3.5%, but in the UK it is 0.5%.Chronic HCV is associated with increased morbidity and mortality and is a leading cause of end-stage liver disease, cirrhosis and liver cancer worldwide.3–6Although HCV infection rarely contributes to morbidity during childhood, the majority of HCV-infected children develop chronic HCV with a lifetime risk of serious liver disease.7Furthermore, some studies indicate that HCV affects childhood quality of life and behaviour, as cognitive function has been shown to be affected and families report increased stress which affects family dynamics and wellbeing.7,8
Chronic hepatitis C virus (HCV) infection is a global health burden. Although HCV infection rarely contributes to morbidity during childhood, most HCV-infected children develop chronic HCV with a lifetime risk of liver disease. Little is known about the development of long-term liver disease and the effect of treatment in patients infected with HCV in childhood. This study was a retrospective review of patients infected with HCV in childhood enrolled in HCV Research UK. A total of 1,049 patients were identified and included. The main routes of infection were intravenous drug use (53%), blood product exposure (24%) and perinatal infection (11%). Liver disease developed in 32% of patients, a median of 33 years after infection, irrespective of the mode of infection. Therefore, patients with perinatal exposure developed cirrhosis at an earlier age than the rest of the risk groups. The incidence of hepatocellular carcinoma (HCC) was 5%, liver transplant 4% and death occurred in 3%. Overall, 663 patients were treated (55% with interferon/pegylated interferon and 40% with direct-acting antivirals). Sustained virological response (SVR) was achieved in 406 (75%). There was a higher mortality rate among patients without SVR vs. those with SVR (5% vs. 1%, p = 0.003). Treatment was more effective in patients without cirrhosis and disease progression was less frequent (13%) than in patients with cirrhosis at the time of therapy (28%) p < 0.001. Patients with cirrhosis were more likely to develop HCC, require liver transplantation, or die. HCV infection in young people causes significant liver disease, which can now be prevented with antiviral therapy. Early treatment, especially before development of cirrhosis is essential. Detection of HCV should be aimed at relevant risk groups and antiviral therapy should be made available in childhood to prevent long-term liver disease and spread of HCV. Several factors have been associated with accelerated disease progression, but it is not clear whether or not infection in early life carries a different risk of progressive disease than infection in adulthood.9,10Since the mode of infection and genotype differs between countries and regions, knowledge of epidemiology at a local level is important in order to effectively plan prevention, surveillance, and prioritise treatment.11Currently, there are few data on mode of infection, genotype and development of long-term liver disease in patients infected with HCV in childhood in the UK.
Chronic hepatitis C virus (HCV) infection is a global health burden. Although HCV infection rarely contributes to morbidity during childhood, most HCV-infected children develop chronic HCV with a lifetime risk of liver disease. Little is known about the development of long-term liver disease and the effect of treatment in patients infected with HCV in childhood. This study was a retrospective review of patients infected with HCV in childhood enrolled in HCV Research UK. A total of 1,049 patients were identified and included. The main routes of infection were intravenous drug use (53%), blood product exposure (24%) and perinatal infection (11%). Liver disease developed in 32% of patients, a median of 33 years after infection, irrespective of the mode of infection. Therefore, patients with perinatal exposure developed cirrhosis at an earlier age than the rest of the risk groups. The incidence of hepatocellular carcinoma (HCC) was 5%, liver transplant 4% and death occurred in 3%. Overall, 663 patients were treated (55% with interferon/pegylated interferon and 40% with direct-acting antivirals). Sustained virological response (SVR) was achieved in 406 (75%). There was a higher mortality rate among patients without SVR vs. those with SVR (5% vs. 1%, p = 0.003). Treatment was more effective in patients without cirrhosis and disease progression was less frequent (13%) than in patients with cirrhosis at the time of therapy (28%) p < 0.001. Patients with cirrhosis were more likely to develop HCC, require liver transplantation, or die. HCV infection in young people causes significant liver disease, which can now be prevented with antiviral therapy. Early treatment, especially before development of cirrhosis is essential. Detection of HCV should be aimed at relevant risk groups and antiviral therapy should be made available in childhood to prevent long-term liver disease and spread of HCV. Currently, access to clinical trials of direct-acting antivirals (DAAs) in childhood is limited which denies timely access to effective therapies for young infected children.HCV Research UK is a UK database and biobank which established a cohort of HCV-infected children and adults to address gaps in knowledge of epidemiology, treatment, disease progression, and prognosis.12,13
Chronic liver injury often results in the activation of hepatic myofibroblasts and the development of liver fibrosis. Hepatic myofibroblasts may originate from 3 major sources: hepatic stellate cells (HSCs), portal fibroblasts (PFs), and fibrocytes, with varying contributions depending on the etiology of liver injury. Here, we assessed the composition of hepatic myofibroblasts in multidrug resistance gene 2 knockout (Mdr2−/−) mice, a genetic model that resembles primary sclerosing cholangitis in patients. Mdr2−/− mice expressing a collagen-GFP reporter were analyzed at different ages. Hepatic non-parenchymal cells isolated from collagen-GFP Mdr2−/− mice were sorted based on collagen-GFP and vitamin A. An NADPH oxidase (NOX) 1/4 inhibitor was administrated to Mdr2−/− mice aged 12–16 weeks old to assess the therapeutic approach of targeting oxidative stress in cholestatic injury. Thy1+ activated PFs accounted for 26%, 51%, and 54% of collagen-GFP+ myofibroblasts in Mdr2−/− mice at 4, 8, and 16 weeks of age, respectively. The remaining collagen-GFP+ myofibroblasts were composed of activated HSCs, suggesting that PFs and HSCs are both activated in Mdr2−/− mice. Bone-marrow-derived fibrocytes minimally contributed to liver fibrosis in Mdr2−/− mice. The development of cholestatic liver fibrosis in Mdr2−/− mice was associated with early recruitment of Gr1+ myeloid cells and upregulation of pro-inflammatory cytokines (4 weeks). Administration of a NOX inhibitor to 12-week-old Mdr2−/− mice suppressed the activation of myofibroblasts and attenuated the development of cholestatic fibrosis. Activated PFs and activated HSCs contribute to cholestatic fibrosis in Mdr2−/− mice, and serve as targets for antifibrotic therapy. Chronic liver injury often results in liver fibrosis.The development of liver fibrosis is associated with migration and proliferation of collagen type I-producing myofibroblasts, which are not present in the normal liver.Activated myofibroblasts originate from 3 major sources: hepatic stellate cells (HSCs), portal fibroblasts (PFs), and fibrocytes.1−4Activated HSCs (aHSCs) were implicated in the pathogenesis of experimental toxic liver fibrosis, such as chronic CCl4 administration and alcoholic liver disease,1,5 while PFs are predominantly activated in response to cholestatic liver fibrosis, such as bile duct ligation (BDL).2
Chronic liver injury often results in the activation of hepatic myofibroblasts and the development of liver fibrosis. Hepatic myofibroblasts may originate from 3 major sources: hepatic stellate cells (HSCs), portal fibroblasts (PFs), and fibrocytes, with varying contributions depending on the etiology of liver injury. Here, we assessed the composition of hepatic myofibroblasts in multidrug resistance gene 2 knockout (Mdr2−/−) mice, a genetic model that resembles primary sclerosing cholangitis in patients. Mdr2−/− mice expressing a collagen-GFP reporter were analyzed at different ages. Hepatic non-parenchymal cells isolated from collagen-GFP Mdr2−/− mice were sorted based on collagen-GFP and vitamin A. An NADPH oxidase (NOX) 1/4 inhibitor was administrated to Mdr2−/− mice aged 12–16 weeks old to assess the therapeutic approach of targeting oxidative stress in cholestatic injury. Thy1+ activated PFs accounted for 26%, 51%, and 54% of collagen-GFP+ myofibroblasts in Mdr2−/− mice at 4, 8, and 16 weeks of age, respectively. The remaining collagen-GFP+ myofibroblasts were composed of activated HSCs, suggesting that PFs and HSCs are both activated in Mdr2−/− mice. Bone-marrow-derived fibrocytes minimally contributed to liver fibrosis in Mdr2−/− mice. The development of cholestatic liver fibrosis in Mdr2−/− mice was associated with early recruitment of Gr1+ myeloid cells and upregulation of pro-inflammatory cytokines (4 weeks). Administration of a NOX inhibitor to 12-week-old Mdr2−/− mice suppressed the activation of myofibroblasts and attenuated the development of cholestatic fibrosis. Activated PFs and activated HSCs contribute to cholestatic fibrosis in Mdr2−/− mice, and serve as targets for antifibrotic therapy. Several experimental models of cholestatic liver injury have been developed.6BDL causes rapid activation of PFs, especially at the onset of injury.Although the pathology resulting from BDL resembles that seen in human chronic cholestatic disease, the surgical stress and the severity of cholestatic injury limit the utility of the BDL model.The multidrug resistance gene 2 knockout (Mdr2−/−, also known as Abcb4−/−) mouse is another well-established model of chronic cholestatic liver injury.Deficiency of Mdr2, a canalicular phospholipid flippase, disrupts biliary phospholipid secretion, leading to the increase of potentially toxic bile acid, which induces hepatocyte damage and cholangiopathy,7−9 which is characterized by pericholangitis and onion-skin-type periductal fibrosis, resembling the pathological features of primary sclerosing cholangitis (PSC).10,11Despite extensive studies,12,13 the contribution of aHSCs and activated PFs (aPFs) to cholestatic fibrosis in Mdr2−/− mice has not been defined.
Chronic liver injury often results in the activation of hepatic myofibroblasts and the development of liver fibrosis. Hepatic myofibroblasts may originate from 3 major sources: hepatic stellate cells (HSCs), portal fibroblasts (PFs), and fibrocytes, with varying contributions depending on the etiology of liver injury. Here, we assessed the composition of hepatic myofibroblasts in multidrug resistance gene 2 knockout (Mdr2−/−) mice, a genetic model that resembles primary sclerosing cholangitis in patients. Mdr2−/− mice expressing a collagen-GFP reporter were analyzed at different ages. Hepatic non-parenchymal cells isolated from collagen-GFP Mdr2−/− mice were sorted based on collagen-GFP and vitamin A. An NADPH oxidase (NOX) 1/4 inhibitor was administrated to Mdr2−/− mice aged 12–16 weeks old to assess the therapeutic approach of targeting oxidative stress in cholestatic injury. Thy1+ activated PFs accounted for 26%, 51%, and 54% of collagen-GFP+ myofibroblasts in Mdr2−/− mice at 4, 8, and 16 weeks of age, respectively. The remaining collagen-GFP+ myofibroblasts were composed of activated HSCs, suggesting that PFs and HSCs are both activated in Mdr2−/− mice. Bone-marrow-derived fibrocytes minimally contributed to liver fibrosis in Mdr2−/− mice. The development of cholestatic liver fibrosis in Mdr2−/− mice was associated with early recruitment of Gr1+ myeloid cells and upregulation of pro-inflammatory cytokines (4 weeks). Administration of a NOX inhibitor to 12-week-old Mdr2−/− mice suppressed the activation of myofibroblasts and attenuated the development of cholestatic fibrosis. Activated PFs and activated HSCs contribute to cholestatic fibrosis in Mdr2−/− mice, and serve as targets for antifibrotic therapy. Under physiological conditions, quiescent HSCs (qHSCs) reside in the space of Disse (which is located between hepatocytes and sinusoidal endothelial cells),1 store vitamin A, and serve as liver pericytes.QHSCs express specific markers, such as glial fibrillary acid protein (GFAP), synaptophysin, nerve growth factor p75 (NGFR1), and lecithin retinol acyltransferase (Lrat).2,14,15In response to toxic liver injury, HSCs downregulate the expression of vitamin A in lipid droplets, migrate to the pericentral areas, and transdifferentiate into collagen type I and α-smooth muscle actin (αSMA) expressing myofibroblasts.
Chronic liver injury often results in the activation of hepatic myofibroblasts and the development of liver fibrosis. Hepatic myofibroblasts may originate from 3 major sources: hepatic stellate cells (HSCs), portal fibroblasts (PFs), and fibrocytes, with varying contributions depending on the etiology of liver injury. Here, we assessed the composition of hepatic myofibroblasts in multidrug resistance gene 2 knockout (Mdr2−/−) mice, a genetic model that resembles primary sclerosing cholangitis in patients. Mdr2−/− mice expressing a collagen-GFP reporter were analyzed at different ages. Hepatic non-parenchymal cells isolated from collagen-GFP Mdr2−/− mice were sorted based on collagen-GFP and vitamin A. An NADPH oxidase (NOX) 1/4 inhibitor was administrated to Mdr2−/− mice aged 12–16 weeks old to assess the therapeutic approach of targeting oxidative stress in cholestatic injury. Thy1+ activated PFs accounted for 26%, 51%, and 54% of collagen-GFP+ myofibroblasts in Mdr2−/− mice at 4, 8, and 16 weeks of age, respectively. The remaining collagen-GFP+ myofibroblasts were composed of activated HSCs, suggesting that PFs and HSCs are both activated in Mdr2−/− mice. Bone-marrow-derived fibrocytes minimally contributed to liver fibrosis in Mdr2−/− mice. The development of cholestatic liver fibrosis in Mdr2−/− mice was associated with early recruitment of Gr1+ myeloid cells and upregulation of pro-inflammatory cytokines (4 weeks). Administration of a NOX inhibitor to 12-week-old Mdr2−/− mice suppressed the activation of myofibroblasts and attenuated the development of cholestatic fibrosis. Activated PFs and activated HSCs contribute to cholestatic fibrosis in Mdr2−/− mice, and serve as targets for antifibrotic therapy. Portal fibroblasts, which reside around the portal area and maintain the integrity of the biliary tree and portal tract,16 comprise a small population of fibroblasts in the liver under physiological conditions.In response to cholestatic injury, PFs proliferate, get activated, and contribute to collagen type I deposition.APFs can be distinguished from aHSCs by expression of Thy1, Fibulin2, Elastin, Gremlin1, ecto-ATPase nucleoside triphosphate diphosphohydrolase 2, mesothelin (Msln), and mucin 16 (Muc16),2,14,16,17 and the lack of HSC markers.
Chronic liver injury often results in the activation of hepatic myofibroblasts and the development of liver fibrosis. Hepatic myofibroblasts may originate from 3 major sources: hepatic stellate cells (HSCs), portal fibroblasts (PFs), and fibrocytes, with varying contributions depending on the etiology of liver injury. Here, we assessed the composition of hepatic myofibroblasts in multidrug resistance gene 2 knockout (Mdr2−/−) mice, a genetic model that resembles primary sclerosing cholangitis in patients. Mdr2−/− mice expressing a collagen-GFP reporter were analyzed at different ages. Hepatic non-parenchymal cells isolated from collagen-GFP Mdr2−/− mice were sorted based on collagen-GFP and vitamin A. An NADPH oxidase (NOX) 1/4 inhibitor was administrated to Mdr2−/− mice aged 12–16 weeks old to assess the therapeutic approach of targeting oxidative stress in cholestatic injury. Thy1+ activated PFs accounted for 26%, 51%, and 54% of collagen-GFP+ myofibroblasts in Mdr2−/− mice at 4, 8, and 16 weeks of age, respectively. The remaining collagen-GFP+ myofibroblasts were composed of activated HSCs, suggesting that PFs and HSCs are both activated in Mdr2−/− mice. Bone-marrow-derived fibrocytes minimally contributed to liver fibrosis in Mdr2−/− mice. The development of cholestatic liver fibrosis in Mdr2−/− mice was associated with early recruitment of Gr1+ myeloid cells and upregulation of pro-inflammatory cytokines (4 weeks). Administration of a NOX inhibitor to 12-week-old Mdr2−/− mice suppressed the activation of myofibroblasts and attenuated the development of cholestatic fibrosis. Activated PFs and activated HSCs contribute to cholestatic fibrosis in Mdr2−/− mice, and serve as targets for antifibrotic therapy. Therapeutic approach to cholestatic fibrosis remains challenging; hence, liver transplantation is the only effective therapy for patients with late-stage PSC.18The activity of NADPH oxidase (NOX), an enzyme system that catalyzes the reduction of molecular oxygen to superoxide, plays an important role in the activation of HSCs and the development of hepatic fibrosis,19−22 and treatment with a NOX1/4 dual inhibitor decreases both CCl4-induced hepatotoxic fibrosis and BDL-induced cholestatic fibrosis.23
In a variety of animal models, omega-3 polyunsaturated fatty acids (Ω3-FAs) conferred strong protective effects, alleviating hepatic ischemia/reperfusion injury and steatosis, as well as enhancing regeneration after major tissue loss. Given these benefits along with its safety profile, we hypothesized that perioperative administration of Ω3-FAs in patients undergoing liver surgery may ameliorate the postoperative course. The aim of this study was to investigate the perioperative use of Ω3-FAs to reduce postoperative complications after liver surgery. Between July 2013 and July 2018, we carried out a multicentric, double-blind, randomized, placebo-controlled trial designed to test whether 2 single intravenous infusions of Omegaven® (Ω3-FAs) vs. placebo may decrease morbidity. The primary endpoints were postoperative complications by severity (Clavien-Dindo classification) integrated within the comprehensive complication index (CCI). A total of 261 patients (132 in the Omegaven and 129 in the placebo groups) from 3 centers were included in the trial. Most cases (87%, n = 227) underwent open liver surgery and 56% (n = 105) were major resections (≥3 segments). In an intention-to-treat analysis including the dropout cases, the mortality rate was 4% and 2% in the Omegaven and placebo groups (odds ratio 0.40; 95% CI 0.04–2.51; p = 0.447), respectively. Any complications and major complications (Clavien-Dindo ≥ 3b) occurred in 46% vs. 43% (p = 0.709) and 12% vs. 10% (p = 0.69) in the Omegaven and placebo groups, respectively. The mean CCI was 17 (±23) vs.14 (±20) (p = 0.417). An analysis excluding the dropouts provided similar results. The routine perioperative use of 2 single doses of intravenous Ω3-FAs (100 ml Omegaven) cannot be recommended in patients undergoing liver surgery (Grade A recommendation). Omega-3 polyunsaturated fatty acids (Ω3-FAs) are essential nutritional components, whose presence in the human body exclusively depends on its exogenous supply.In the late 1970s, Bang and Dyerberg observed an extraordinary low incidence of coronary heart disease among Greenland inuits, which was found to be related to a high intake of Ω3-FAs abundant in marine fish; the main food source of this population.1The liver was subsequently found to be a central metabolic target of Ω3-FAs with growing evidence of their beneficial effects on fatty liver and metabolic syndrome.2–4Ω3-FAs were found not only to lower intrahepatic lipid contents,5 but also to mitigate inflammatory changes in the liver.6,7Liver surgery in patients with fatty liver leads to an amplified ischemia-reperfusion injury (I/R) and impaired regeneration (LR).8A variety of animal models disclosed significant liver protection in hepatic surgery (reduction of I/R and improved LR) following treatment with Ω3-FAs.9–11Of note, these effects were not only observed in steatotic, but also in lean livers.11For example, in a mouse model, the intravenous (i.v.) administration of a single dose of Ω3-FAs (Omegaven®) prior to liver ischemia dramatically mitigated the production of reactive oxygen species (ROS) upon reperfusion.12This beneficial effect is mediated via the GPR120 receptor located on hepatic Kupffer cells.12
The RESORCE trial showed that regorafenib improves overall survival (OS) in patients with hepatocellular carcinoma progressing during sorafenib treatment (hazard ratio [HR] 0.62, 95% confidence interval [CI] 0.50–0.78; p <0.0001). This exploratory analysis describes outcomes of sequential treatment with sorafenib followed by regorafenib. In RESORCE, 573 patients were randomized 2:1 to regorafenib 160 mg/day or placebo for 3 weeks on/1 week off. Efficacy and safety were evaluated by last sorafenib dose. The time from the start of sorafenib to death was assessed. Time to progression (TTP) in RESORCE was analyzed by TTP during prior sorafenib treatment. HRs (regorafenib/placebo) for OS by last sorafenib dose were similar (0.67 for 800 mg/day; 0.68 for <800 mg/day). Rates of grade 3, 4, and 5 adverse events with regorafenib by last sorafenib dose (800 mg/day vs. <800 mg/day) were 52%, 11%, and 15% vs. 60%, 10%, and 12%, respectively. Median times (95% CI) from the start of sorafenib to death were 26.0 months (22.6–28.1) for regorafenib and 19.2 months (16.3–22.8) for placebo. Median time from the start of sorafenib to progression on sorafenib was 7.2 months for the regorafenib arm and 7.1 months for the placebo arm. An analysis of TTP in RESORCE in subgroups defined by TTP during prior sorafenib in quartiles (Q) showed HRs (regorafenib/placebo; 95% CI) of 0.66 (0.45–0.96; Q1); 0.26 (0.17–0.40; Q2); 0.40 (0.27–0.60; Q3); and 0.54 (0.36–0.81; Q4). These exploratory analyses show that regorafenib conferred a clinical benefit regardless of the last sorafenib dose or TTP on prior sorafenib. Rates of adverse events were generally similar regardless of the last sorafenib dose. For patients with unresectable hepatocellular carcinoma (HCC) who cannot benefit from resection, transplantation, or ablation, the oral multikinase inhibitor sorafenib at the approved dose of 800 mg/day is the standard first-line systemic treatment.1–3Recently, lenvatinib was shown to be non-inferior to sorafenib for overall survival (OS) for first-line systemic treatment of HCC.4Since the approval of sorafenib in 2008, one of the most important unmet needs in the treatment of HCC has been the development of agents that improve outcomes after disease progression during sorafenib treatment.To that end, several phase III trials evaluating novel drugs in this population were conducted, but until recently all failed to meet their primary endpoint of improving OS for these patients.5–8
The RESORCE trial showed that regorafenib improves overall survival (OS) in patients with hepatocellular carcinoma progressing during sorafenib treatment (hazard ratio [HR] 0.62, 95% confidence interval [CI] 0.50–0.78; p <0.0001). This exploratory analysis describes outcomes of sequential treatment with sorafenib followed by regorafenib. In RESORCE, 573 patients were randomized 2:1 to regorafenib 160 mg/day or placebo for 3 weeks on/1 week off. Efficacy and safety were evaluated by last sorafenib dose. The time from the start of sorafenib to death was assessed. Time to progression (TTP) in RESORCE was analyzed by TTP during prior sorafenib treatment. HRs (regorafenib/placebo) for OS by last sorafenib dose were similar (0.67 for 800 mg/day; 0.68 for <800 mg/day). Rates of grade 3, 4, and 5 adverse events with regorafenib by last sorafenib dose (800 mg/day vs. <800 mg/day) were 52%, 11%, and 15% vs. 60%, 10%, and 12%, respectively. Median times (95% CI) from the start of sorafenib to death were 26.0 months (22.6–28.1) for regorafenib and 19.2 months (16.3–22.8) for placebo. Median time from the start of sorafenib to progression on sorafenib was 7.2 months for the regorafenib arm and 7.1 months for the placebo arm. An analysis of TTP in RESORCE in subgroups defined by TTP during prior sorafenib in quartiles (Q) showed HRs (regorafenib/placebo; 95% CI) of 0.66 (0.45–0.96; Q1); 0.26 (0.17–0.40; Q2); 0.40 (0.27–0.60; Q3); and 0.54 (0.36–0.81; Q4). These exploratory analyses show that regorafenib conferred a clinical benefit regardless of the last sorafenib dose or TTP on prior sorafenib. Rates of adverse events were generally similar regardless of the last sorafenib dose. The oral multikinase inhibitor regorafenib blocks the activity of protein kinases involved in angiogenesis, oncogenesis, metastasis, and tumor immunity,9,10 and is approved for the treatment of metastatic colorectal cancer and advanced gastrointestinal stromal tumors.11,12After a single-arm, phase II trial of regorafenib in patients with HCC that had progressed on sorafenib showed evidence of acceptable tolerability and promising anti-tumor activity, the randomized, placebo-controlled phase III RESORCE trial was carried out.13,14The results of RESORCE demonstrated that regorafenib significantly improves OS compared with placebo in patients with HCC who had radiologic progression during sorafenib treatment (hazard ratio [HR] 0.62, 95% confidence interval [CI] 0.50–0.78; p <0.0001).14,15
Under the regulation of various oncogenic pathways, cancer cells undergo adaptive metabolic programming to maintain specific metabolic states that support their uncontrolled proliferation. As it has been difficult to directly and effectively inhibit oncogenic signaling cascades with pharmaceutical compounds, focusing on the downstream metabolic pathways that enable indefinite growth may provide therapeutic opportunities. Thus, we sought to characterize metabolic changes in hepatocellular carcinoma (HCC) development and identify metabolic targets required for tumorigenesis. We compared gene expression profiles of Morris Hepatoma (MH3924a) and DEN (diethylnitrosamine)-induced HCC models to those of liver tissues from normal and rapidly regenerating liver models, and performed gain- and loss-of-function studies of the identified gene targets for their roles in cancer cell proliferation in vitro and in vivo. The proline biosynthetic enzyme PYCR1 (pyrroline-5-carboxylate reductase 1) was identified as one of the most upregulated genes in the HCC models. Knockdown of PYCR1 potently reduced cell proliferation of multiple HCC cell lines in vitro and tumor growth in vivo. Conversely, overexpression of PYCR1 enhanced the proliferation of the HCC cell lines. Importantly, PYCR1 expression was not elevated in the regenerating liver, and KD or overexpression of PYCR1 had no effect on proliferation of non-cancerous cells. Besides PYCR1, we found that additional proline biosynthetic enzymes, such as ALDH18A1, were upregulated in HCC models and also regulated HCC cell proliferation. Clinical data demonstrated that PYCR1 expression was increased in HCC, correlated with tumor grade, and was an independent predictor of clinical outcome. Enhanced expression of proline biosynthetic enzymes promotes HCC cell proliferation. Inhibition of PYCR1 or ALDH18A1 may be a novel therapeutic strategy to target HCC. With an estimated 700,000 new cases per year, hepatocellular carcinoma (HCC) is the fifth commonest cancer and third leading cause of cancer-related death worldwide.Currently, HCC cases occur predominantly in East and Southeast Asia, and sub-Saharan Africa due to hepatitis B/C viral infection.However, the incidence of HCC is rising in Western nations, largely associated with metabolic complications, such as non-alcoholic steatohepatitis related to obesity and type II diabetes.The overall 5-year survival rate remains below 12% due to limited treatment options.1So far, 4 oral multikinase inhibitors (sorafenib, regorafenib, lenvatinib and cabozantinib) and 2 immunotherapeutic agents (nivolumab and pembrolizumab) have been FDA-approved for the treatment of HCC.2–7However, due to treatment eligibility criteria, side effects and modest clinical benefit, it remains a top priority to identify additional effective therapeutic targets.
Under the regulation of various oncogenic pathways, cancer cells undergo adaptive metabolic programming to maintain specific metabolic states that support their uncontrolled proliferation. As it has been difficult to directly and effectively inhibit oncogenic signaling cascades with pharmaceutical compounds, focusing on the downstream metabolic pathways that enable indefinite growth may provide therapeutic opportunities. Thus, we sought to characterize metabolic changes in hepatocellular carcinoma (HCC) development and identify metabolic targets required for tumorigenesis. We compared gene expression profiles of Morris Hepatoma (MH3924a) and DEN (diethylnitrosamine)-induced HCC models to those of liver tissues from normal and rapidly regenerating liver models, and performed gain- and loss-of-function studies of the identified gene targets for their roles in cancer cell proliferation in vitro and in vivo. The proline biosynthetic enzyme PYCR1 (pyrroline-5-carboxylate reductase 1) was identified as one of the most upregulated genes in the HCC models. Knockdown of PYCR1 potently reduced cell proliferation of multiple HCC cell lines in vitro and tumor growth in vivo. Conversely, overexpression of PYCR1 enhanced the proliferation of the HCC cell lines. Importantly, PYCR1 expression was not elevated in the regenerating liver, and KD or overexpression of PYCR1 had no effect on proliferation of non-cancerous cells. Besides PYCR1, we found that additional proline biosynthetic enzymes, such as ALDH18A1, were upregulated in HCC models and also regulated HCC cell proliferation. Clinical data demonstrated that PYCR1 expression was increased in HCC, correlated with tumor grade, and was an independent predictor of clinical outcome. Enhanced expression of proline biosynthetic enzymes promotes HCC cell proliferation. Inhibition of PYCR1 or ALDH18A1 may be a novel therapeutic strategy to target HCC. To meet the increased energetic and anabolic needs of sustained cell proliferation, cancer cells undergo dramatic metabolic reprogramming.8The expression changes observed in cancer cells can frequently be linked to the activity of multiple classic oncogenes and tumor suppressors.Overall, accumulating evidence demonstrates that changes in metabolism confer a potent influence on tumor development and growth, and dysregulated cellular energetics are now a recognized hallmark of cancer.9,10More importantly, these metabolic changes also hold the potential for therapeutic intervention.A notable advance has been the targeting of changes in amino acid metabolism, including metabolism of serine, glycine, and branched-chain amino acids for certain cancers.11–14
A total of 15% of patients with idiopathic non-cirrhotic portal hypertension (INCPH) are women of childbearing age. We aimed to determine maternal and fetal outcome of pregnancies occurring in women with INCPH. We retrospectively analyzed the charts of women with INCPH followed in the centers of the VALDIG network, having had ≥1 pregnancy during the follow-up of their liver disease. Data are represented as median (interquartile range). A total of 24 pregnancies occurred in 16 women within 24 (5–66) months after INCPH diagnosis. Four women had associated partial portal vein thrombosis before pregnancy. At conception, 2 out of the 16 women had detectable ascites and others were asymptomatic. Out of these 24 pregnancies, there were four miscarriages, one ectopic pregnancy, and one medical termination of pregnancy at 20 weeks of gestation. Out of the 18 other pregnancies reaching 20 weeks of gestation (in 14 patients), there were nine preterm and nine term deliveries. All infants were healthy at delivery, but one died at day 1 of unknown cause and one at day 22 of infectious meningitis; both were preterm. Concerning mothers, two had worsening of ascites, two had variceal bleeding despite non-selective betablockers during pregnancy and one developed a main portal vein thrombosis in early postpartum. Genital bleeding occurred in three patients, including two receiving anticoagulation. All 16 women were alive and asymptomatic after a median follow-up of 27 (9–93) months after last delivery. The overall outcome of women with INCPH who become pregnant is favorable despite a significant incidence of complications related to portal hypertension. Fetal outcome is favorable in most pregnancies reaching 20 weeks of gestation. Idiopathic portal hypertension, non-cirrhotic portal fibrosis and idiopathic non-cirrhotic portal hypertension (INCPH) indicate the same clinical entity.1These terms, thereafter referred to as INCPH, designate a heterogeneous group of liver diseases causing portal hypertension and characterized by the absence of cirrhotic modification of the liver parenchyma and the patency of the portal and hepatic veins.Liver histological lesions found in patients with INCPH include obliterative portal venopathy, hepatoportal sclerosis, nodular regenerative hyperplasia and incomplete septal cirrhosis.The main complications of INCPH include the development of portal vein thrombosis and of gastrointestinal bleeding related to portal hypertension.2Although called idiopathic, INCPH has been associated with various conditions including thrombophilia, hematologic malignancies, human immunodeficiency virus infection, genetic and immunological disorders.2
A total of 15% of patients with idiopathic non-cirrhotic portal hypertension (INCPH) are women of childbearing age. We aimed to determine maternal and fetal outcome of pregnancies occurring in women with INCPH. We retrospectively analyzed the charts of women with INCPH followed in the centers of the VALDIG network, having had ≥1 pregnancy during the follow-up of their liver disease. Data are represented as median (interquartile range). A total of 24 pregnancies occurred in 16 women within 24 (5–66) months after INCPH diagnosis. Four women had associated partial portal vein thrombosis before pregnancy. At conception, 2 out of the 16 women had detectable ascites and others were asymptomatic. Out of these 24 pregnancies, there were four miscarriages, one ectopic pregnancy, and one medical termination of pregnancy at 20 weeks of gestation. Out of the 18 other pregnancies reaching 20 weeks of gestation (in 14 patients), there were nine preterm and nine term deliveries. All infants were healthy at delivery, but one died at day 1 of unknown cause and one at day 22 of infectious meningitis; both were preterm. Concerning mothers, two had worsening of ascites, two had variceal bleeding despite non-selective betablockers during pregnancy and one developed a main portal vein thrombosis in early postpartum. Genital bleeding occurred in three patients, including two receiving anticoagulation. All 16 women were alive and asymptomatic after a median follow-up of 27 (9–93) months after last delivery. The overall outcome of women with INCPH who become pregnant is favorable despite a significant incidence of complications related to portal hypertension. Fetal outcome is favorable in most pregnancies reaching 20 weeks of gestation. About 15% of patients with INCPH are women of childbearing age, who can become pregnant.3,4However, pregnancy and postpartum are prothrombotic states and pregnancy might exacerbate portal hypertension.5–7As available reports on pregnancy in women with known INCPH are scarce and heterogeneous, it is unclear whether pregnancy should be contraindicated in this setting.5
The most prescribed non-nucleoside reverse transcriptase inhibitor, efavirenz, has been associated with elevated risk of dyslipidemia and hepatic steatosis in HIV-infected patients but the underlying mechanisms remain elusive. Herein, we investigated the role of pregnane X receptor (PXR) in mediating the adverse effects of efavirenz on lipid homeostasis. Cell-based reporter assays, primary cell culture, and multiple mouse models including conditional knockout and humanized mice were combined to study the impact of efavirenz on PXR activities and lipid homeostasis in vitro and in vivo. A novel liver-specific Pxr knockout mouse model was also generated to determine the contribution of hepatic PXR signaling to efavirenz-elicited dyslipidemia and hepatic steatosis. We found that efavirenz is a potent PXR-selective agonist that can efficiently activate PXR and induce its target gene expression in vitro and in vivo. Treatment with efavirenz-induced hypercholesterolemia and hepatic steatosis in mice but deficiency of hepatic PXR abolished these adverse effects. Interestingly, efavirenz-mediated PXR activation regulated the expression of several key hepatic lipogenic genes including fatty acid transporter CD36 and cholesterol biosynthesis enzyme squalene epoxidase (SQLE), leading to increased lipid uptake and cholesterol biosynthesis in hepatic cells. While CD36 is a known PXR target gene, we identified a DR-2-type of PXR-response element in the SQLE promoter and established SQLE as a direct transcriptional target of PXR. Since PXR exhibits considerable differences in its pharmacology across species, we also confirmed these findings in PXR-humanized mice and human primary hepatocytes. The widely prescribed antiretroviral drug efavirenz induces hypercholesterolemia and hepatic steatosis by activating PXR signaling. Activation of PXR should be taken into consideration for patients undergoing long-term treatment with PXR agonistic antiretroviral drugs. As the average lifespan of HIV-infected patients receiving antiretroviral therapy (ART) lengthens, morbidity and mortality from cardiovascular disease (CVD) pose considerable challenges.1–4Not only does HIV infection elevate the risk of CVD, but the use of antiretroviral (ARV) drugs has also been associated with dyslipidemia and increased risk of CVD, thereby exacerbating a serious health challenge for long-term survivors.1–2,4Current optimal ART options consist of a combination of several drug classes including protease inhibitors (PIs), nucleoside/nucleotide reverse transcriptase inhibitors (NRTIs), non-nucleoside reverse transcriptase inhibitor (NNRTIs), and integrase strand transfer inhibitors (INSTIs, also known as integrase inhibitors).2,3Although several first-generation PIs such as amprenavir and ritonavir with documented dyslipidemic properties have been used on a limited basis, many currently recommended first-line ARV drugs have also been associated with dyslipidemia and increased CVD risk.2–6For example, the NNRTI efavirenz, one of the most prescribed antiretroviral drugs worldwide, has been consistently associated with dyslipidemia including increased total, low-density lipoprotein (LDL), and high-density lipoprotein (HDL) cholesterol levels in multiple clinical studies.2,5–7Despite the strong evidence linking certain ARV drugs with dyslipidemia and CVD risk, the underlying mechanisms responsible for the adverse effects of ARV drugs remain elusive.
The most prescribed non-nucleoside reverse transcriptase inhibitor, efavirenz, has been associated with elevated risk of dyslipidemia and hepatic steatosis in HIV-infected patients but the underlying mechanisms remain elusive. Herein, we investigated the role of pregnane X receptor (PXR) in mediating the adverse effects of efavirenz on lipid homeostasis. Cell-based reporter assays, primary cell culture, and multiple mouse models including conditional knockout and humanized mice were combined to study the impact of efavirenz on PXR activities and lipid homeostasis in vitro and in vivo. A novel liver-specific Pxr knockout mouse model was also generated to determine the contribution of hepatic PXR signaling to efavirenz-elicited dyslipidemia and hepatic steatosis. We found that efavirenz is a potent PXR-selective agonist that can efficiently activate PXR and induce its target gene expression in vitro and in vivo. Treatment with efavirenz-induced hypercholesterolemia and hepatic steatosis in mice but deficiency of hepatic PXR abolished these adverse effects. Interestingly, efavirenz-mediated PXR activation regulated the expression of several key hepatic lipogenic genes including fatty acid transporter CD36 and cholesterol biosynthesis enzyme squalene epoxidase (SQLE), leading to increased lipid uptake and cholesterol biosynthesis in hepatic cells. While CD36 is a known PXR target gene, we identified a DR-2-type of PXR-response element in the SQLE promoter and established SQLE as a direct transcriptional target of PXR. Since PXR exhibits considerable differences in its pharmacology across species, we also confirmed these findings in PXR-humanized mice and human primary hepatocytes. The widely prescribed antiretroviral drug efavirenz induces hypercholesterolemia and hepatic steatosis by activating PXR signaling. Activation of PXR should be taken into consideration for patients undergoing long-term treatment with PXR agonistic antiretroviral drugs. Alongside others, we have previously identified several HIV PIs, including amprenavir and ritonavir, as potent agonists of pregnane X receptor (PXR; also known as steroid and xenobiotic receptor, or NR1I2).8,9PXR is a nuclear receptor activated by numerous endogenous hormones, dietary steroids, pharmaceutical agents, and xenobiotic chemicals.10–12PXR functions as a xenobiotic sensor that induces the expression of genes required for xenobiotic metabolism in the liver and intestine.11,12It has recently been demonstrated that PXR signaling also plays an important role in lipid homeostasis.9,12–15PXR signaling has also been implicated in contributing to ARV drugs’ dyslipidemic effects.For example, treatment with ritonavir, a potent PXR activator,8 caused hyperlipidemia and was associated with increased risk of CVD in patients with HIV.16We also demonstrated that another PI, amprenavir, can activate PXR to elevate plasma cholesterol levels in mice.9
Egypt has one of the highest burdens of HCV infection worldwide, and a large treatment programme, but reaching rural communities represents a major challenge. We report the feasibility and effectiveness of a comprehensive community-based HCV prevention, testing and treatment model in 73 villages across Egypt, with the goal to eliminate infection from all adult villagers. An HCV “educate, test and treat” programme was implemented in 73 villages across 7 governorates in Egypt between 06/2015 and 06/2018. The programme model comprised community mobilization facilitated by a network of village promoters to support the education, test and treat campaign as well as fund raising in the local community; a comprehensive testing, linkage to care and treatment of all eligible villagers aged 12 to 80 years using HCV antibody and HBsAg rapid diagnostic tests (RDTs), HCV RNA confirmation of positive cases, staging of liver disease using transient elastography (FibroScan) or FIB4 score, treatment with 12 or 24 weeks of a direct acting antiviral (DAA) regimen, and an assessment of cure at 12 weeks after completion of treatment (SVR12); and an education campaign to raise awareness and disseminate messages about safer practices to reduce transmission through public events, promotional materials and house-to-house visits. Key outcomes assessed in each village were: uptake of serological HCV and HBV testing, HCV viral load confirmation and treatment, and SVR12. 204749 (92.3%, 95% CI 91.6-93.5) of 221855 eligible villagers were screened for HCV antibody and HBsAg, and 33839 (16.5%, 95% CI 12.2-16.1) and 763 (0.4%, 95% CI 0.3-0.5) were positive, respectively. Nearly all 33839 HCV antibody positive individuals had immediate sample collected for HCV RNA and 15892 (47.0%, 95% CI 44.8-53.8) were positive. Overall, prevalence of HCV viremia was 7.8%, 95% CI 6.6-7.9. 14495 (91.2%, 95% CI 89.9-96.4) received treatment within a median of 2.1 weeks from serological diagnosis (IQR: 0.6 to 3.3 weeks). SVR12) was achieved among 14238 of the treated cases (98.3%, 95% CI 96.7-98.6). 3192 (20.1%) had cirrhosis, and of these 166 (5.2%) were diagnosed with HCC. Chronic hepatitis C infection is a major public health problem with an estimated 71 million people chronically infected with hepatitis C worldwide [1].The global response has been transformed by the availability of low-cost, curative, short course direct acting antiviral (DAA) therapy.The World Health Organization (WHO) has a stated goal to eliminate viral hepatitis C (and B) virus infection as a public health threat by 2030 – defined as a reduction in mortality by 65% and new infections by 90% [1,2].Achievement of these impact targets is feasible through a combination of substantial scale-up of access to affordable testing and treatment with the highly effective new direct-acting antiviral drug regimens alongside HCV preventative interventions such as harm reduction in key populations and reduction in unnecessary and unsafe injection practices [3].While there has been some encouraging progress in global treatment scale-up with more than 3 million treated with direct acting antivirals since 2015, access to and uptake of testing remains poor, and less than 10% of those with chronic HCV infection in low-resource settings (LRS) have been diagnosed [1].
Egypt has one of the highest burdens of HCV infection worldwide, and a large treatment programme, but reaching rural communities represents a major challenge. We report the feasibility and effectiveness of a comprehensive community-based HCV prevention, testing and treatment model in 73 villages across Egypt, with the goal to eliminate infection from all adult villagers. An HCV “educate, test and treat” programme was implemented in 73 villages across 7 governorates in Egypt between 06/2015 and 06/2018. The programme model comprised community mobilization facilitated by a network of village promoters to support the education, test and treat campaign as well as fund raising in the local community; a comprehensive testing, linkage to care and treatment of all eligible villagers aged 12 to 80 years using HCV antibody and HBsAg rapid diagnostic tests (RDTs), HCV RNA confirmation of positive cases, staging of liver disease using transient elastography (FibroScan) or FIB4 score, treatment with 12 or 24 weeks of a direct acting antiviral (DAA) regimen, and an assessment of cure at 12 weeks after completion of treatment (SVR12); and an education campaign to raise awareness and disseminate messages about safer practices to reduce transmission through public events, promotional materials and house-to-house visits. Key outcomes assessed in each village were: uptake of serological HCV and HBV testing, HCV viral load confirmation and treatment, and SVR12. 204749 (92.3%, 95% CI 91.6-93.5) of 221855 eligible villagers were screened for HCV antibody and HBsAg, and 33839 (16.5%, 95% CI 12.2-16.1) and 763 (0.4%, 95% CI 0.3-0.5) were positive, respectively. Nearly all 33839 HCV antibody positive individuals had immediate sample collected for HCV RNA and 15892 (47.0%, 95% CI 44.8-53.8) were positive. Overall, prevalence of HCV viremia was 7.8%, 95% CI 6.6-7.9. 14495 (91.2%, 95% CI 89.9-96.4) received treatment within a median of 2.1 weeks from serological diagnosis (IQR: 0.6 to 3.3 weeks). SVR12) was achieved among 14238 of the treated cases (98.3%, 95% CI 96.7-98.6). 3192 (20.1%) had cirrhosis, and of these 166 (5.2%) were diagnosed with HCC. Egypt has a very high prevalence and burden of chronic HCV infection (predominantly genotype 4) in the general population (7% HCV RNA positive in 2015) [2] largely as a result of poor injection safety practices during mass population anti-schistosomal eradication campaigns from the 1950s [4] together with other ongoing unsafe medical practices [2,5].The national government and Ministry of Health and Population (MOHP) established an early effective viral hepatitis response, with the goal of elimination of hepatitis C infection from Egypt.This response has included the development of a comprehensive action plan for the prevention, care and treatment of viral hepatitis [6], negotiation of low prices for originator and generic direct acting antivirals (DAAs), and establishment of the largest treatment programme worldwide (with treatment of more than 1.8 million Egyptians through a network of more than 73 treatment sites).As a result, there has been a decline in the estimated prevalence of chronic viraemic infection from 10% in 2008 [7] to 7% in 2015 [2].However, it is estimated that there are still between 3.5 to 4.2 million persons currently living with chronic hepatitis C infection in Egypt and in need of treatment [8,9].Rural communities account for 57% of Egypt’s 92 million population, and a substantial proportion of those with chronic HCV infection live in one of the 6635 villages across the 27 governorates.Therefore, to achieve the goal of elimination of hepatitis C infection in Egypt, the strategy of elimination needs to be increasingly directed towards case finding through mass screening in rural communities to identify and treat those not yet aware of their diagnosis [10].A further issue has been the continued high rate of new infections, estimated at 2-6 cases per 1000 (equivalent to at least 170,000 new cases) per year [9] due unsafe injection and health care practices [10], highlighting the need for concomitant effective awareness raising and education campaigns to prevent new infections [11].
Egypt has one of the highest burdens of HCV infection worldwide, and a large treatment programme, but reaching rural communities represents a major challenge. We report the feasibility and effectiveness of a comprehensive community-based HCV prevention, testing and treatment model in 73 villages across Egypt, with the goal to eliminate infection from all adult villagers. An HCV “educate, test and treat” programme was implemented in 73 villages across 7 governorates in Egypt between 06/2015 and 06/2018. The programme model comprised community mobilization facilitated by a network of village promoters to support the education, test and treat campaign as well as fund raising in the local community; a comprehensive testing, linkage to care and treatment of all eligible villagers aged 12 to 80 years using HCV antibody and HBsAg rapid diagnostic tests (RDTs), HCV RNA confirmation of positive cases, staging of liver disease using transient elastography (FibroScan) or FIB4 score, treatment with 12 or 24 weeks of a direct acting antiviral (DAA) regimen, and an assessment of cure at 12 weeks after completion of treatment (SVR12); and an education campaign to raise awareness and disseminate messages about safer practices to reduce transmission through public events, promotional materials and house-to-house visits. Key outcomes assessed in each village were: uptake of serological HCV and HBV testing, HCV viral load confirmation and treatment, and SVR12. 204749 (92.3%, 95% CI 91.6-93.5) of 221855 eligible villagers were screened for HCV antibody and HBsAg, and 33839 (16.5%, 95% CI 12.2-16.1) and 763 (0.4%, 95% CI 0.3-0.5) were positive, respectively. Nearly all 33839 HCV antibody positive individuals had immediate sample collected for HCV RNA and 15892 (47.0%, 95% CI 44.8-53.8) were positive. Overall, prevalence of HCV viremia was 7.8%, 95% CI 6.6-7.9. 14495 (91.2%, 95% CI 89.9-96.4) received treatment within a median of 2.1 weeks from serological diagnosis (IQR: 0.6 to 3.3 weeks). SVR12) was achieved among 14238 of the treated cases (98.3%, 95% CI 96.7-98.6). 3192 (20.1%) had cirrhosis, and of these 166 (5.2%) were diagnosed with HCC. We developed a community-based outreach model (educate, test and treat) for the prevention, diagnosis and treatment of hepatitis C infection as an important strategy for elimination of hepatitis C across villages in rural Egypt, but applicable to other high burden countries.We recently described this model and its successful implementation in one village in northern Egypt (Al-Othmanya, Gharbiah governorate) [12].The programme achieved high uptake of HCV testing, prompt and high levels of linkage to care and DAA treatment, and attainment of cure in 97% of those treated.The programme demonstrated the important role of community volunteers in achieving high level of testing uptake and linkage to care, improving community awareness and education leading to significant behavior changes, as well as the ability to fund-raise to cover the financial costs of HCV care.Re-testing of those HCV antibody negative during the original screening programme three years previously, has confirmed a very low incidence new infections (incidence rate of 0.18/1000py (95% CI=0.03-0.6) confirming near elimination of HCV infection from this village [13].
The ubiquitin ligase F-box and WD repeat domain-containing 7 (FBXW7) is recognized as a tumor suppressor in many cancer types due to its ability to promote the degradation of numerous oncogenic target proteins. Herein, we aimed to elucidate its role in intrahepatic cholangiocarcinoma (iCCA). Herein, we first confirmed that FBXW7 gene expression was reduced in human iCCA specimens. To identify the molecular mechanisms by which FBXW7 dysfunction promotes cholangiocarcinogenesis, we generated a mouse model by hydrodynamic tail vein injection of Fbxw7ΔF, a dominant negative form of Fbxw7, either alone or in association with an activated/myristylated form of AKT (myr-AKT). We then confirmed the role of c-MYC in human iCCA cell lines and its relationship to FBXW7 expression in human iCCA specimens. FBXW7 mRNA expression is almost ubiquitously downregulated in human iCCA specimens. While forced overexpression of Fbxw7ΔF alone did not induce any appreciable abnormality in the mouse liver, co-expression with AKT triggered cholangiocarcinogenesis and mice had to be euthanized by 15 weeks post-injection. At the molecular level, a strong induction of Fbxw7 canonical targets, including Yap, Notch2, and c-Myc oncoproteins, was detected. However, only c-MYC was consistently confirmed as a FBXW7 target in human CCA cell lines. Most importantly, selected ablation of c-Myc completely impaired iCCA formation in AKT/Fbxw7ΔF mice, whereas deletion of either Yap or Notch2 only delayed tumorigenesis in the same model. In human iCCA specimens, an inverse correlation between the expression levels of FBXW7 and c-MYC transcriptional activity was observed. Downregulation of FBXW7 is ubiquitous in human iCCA and cooperates with AKT to induce cholangiocarcinogenesis in mice via c-Myc-dependent mechanisms. Targeting c-MYC might represent an innovative therapy against iCCA exhibiting low FBXW7 expression. Cholangiocarcinoma (CCA) is the second most common primary liver cancer.1,2Depending on the anatomical site, CCA is classified into intrahepatic (iCCA) and extrahepatic cholangiocarcinoma (eCCA).3iCCA incidence has been rising over the last decade, while that of eCCA slightly decreased.4For iCCA detected at early stage, curative surgical resection is the optimal treatment strategy.5,6However, less than one-third of patients achieves negative tumor margins, and recurrence rates are high.5,7Furthermore, in patients not meeting the narrow criteria for surgical treatment, therapeutic options are limited.8Therefore, substantial efforts should be devoted to unravelling the molecular mechanisms of iCCA development and progression.This would lead to novel and more effective therapeutic strategies against this pernicious disease.
The ubiquitin ligase F-box and WD repeat domain-containing 7 (FBXW7) is recognized as a tumor suppressor in many cancer types due to its ability to promote the degradation of numerous oncogenic target proteins. Herein, we aimed to elucidate its role in intrahepatic cholangiocarcinoma (iCCA). Herein, we first confirmed that FBXW7 gene expression was reduced in human iCCA specimens. To identify the molecular mechanisms by which FBXW7 dysfunction promotes cholangiocarcinogenesis, we generated a mouse model by hydrodynamic tail vein injection of Fbxw7ΔF, a dominant negative form of Fbxw7, either alone or in association with an activated/myristylated form of AKT (myr-AKT). We then confirmed the role of c-MYC in human iCCA cell lines and its relationship to FBXW7 expression in human iCCA specimens. FBXW7 mRNA expression is almost ubiquitously downregulated in human iCCA specimens. While forced overexpression of Fbxw7ΔF alone did not induce any appreciable abnormality in the mouse liver, co-expression with AKT triggered cholangiocarcinogenesis and mice had to be euthanized by 15 weeks post-injection. At the molecular level, a strong induction of Fbxw7 canonical targets, including Yap, Notch2, and c-Myc oncoproteins, was detected. However, only c-MYC was consistently confirmed as a FBXW7 target in human CCA cell lines. Most importantly, selected ablation of c-Myc completely impaired iCCA formation in AKT/Fbxw7ΔF mice, whereas deletion of either Yap or Notch2 only delayed tumorigenesis in the same model. In human iCCA specimens, an inverse correlation between the expression levels of FBXW7 and c-MYC transcriptional activity was observed. Downregulation of FBXW7 is ubiquitous in human iCCA and cooperates with AKT to induce cholangiocarcinogenesis in mice via c-Myc-dependent mechanisms. Targeting c-MYC might represent an innovative therapy against iCCA exhibiting low FBXW7 expression. The E3 ubiquitin ligase F-box and WD repeat domain containing protein 7 (FBXW7) is a bona fide tumor suppressor gene.9,10Being a substrate recognition component of the Skp1-Cul1-F-box protein-type complex, FBXW7 is responsible for the binding to and the degradation of several oncogenes, including c-MYC,11,12 YAP,13 NOTCH1,14,15 mTOR,16 CCNE1,11 and CCND1.12Among them, YAP, NOTCH, and c-MYC proteins play a pivotal role in cholangiocarcinogenesis.
The ubiquitin ligase F-box and WD repeat domain-containing 7 (FBXW7) is recognized as a tumor suppressor in many cancer types due to its ability to promote the degradation of numerous oncogenic target proteins. Herein, we aimed to elucidate its role in intrahepatic cholangiocarcinoma (iCCA). Herein, we first confirmed that FBXW7 gene expression was reduced in human iCCA specimens. To identify the molecular mechanisms by which FBXW7 dysfunction promotes cholangiocarcinogenesis, we generated a mouse model by hydrodynamic tail vein injection of Fbxw7ΔF, a dominant negative form of Fbxw7, either alone or in association with an activated/myristylated form of AKT (myr-AKT). We then confirmed the role of c-MYC in human iCCA cell lines and its relationship to FBXW7 expression in human iCCA specimens. FBXW7 mRNA expression is almost ubiquitously downregulated in human iCCA specimens. While forced overexpression of Fbxw7ΔF alone did not induce any appreciable abnormality in the mouse liver, co-expression with AKT triggered cholangiocarcinogenesis and mice had to be euthanized by 15 weeks post-injection. At the molecular level, a strong induction of Fbxw7 canonical targets, including Yap, Notch2, and c-Myc oncoproteins, was detected. However, only c-MYC was consistently confirmed as a FBXW7 target in human CCA cell lines. Most importantly, selected ablation of c-Myc completely impaired iCCA formation in AKT/Fbxw7ΔF mice, whereas deletion of either Yap or Notch2 only delayed tumorigenesis in the same model. In human iCCA specimens, an inverse correlation between the expression levels of FBXW7 and c-MYC transcriptional activity was observed. Downregulation of FBXW7 is ubiquitous in human iCCA and cooperates with AKT to induce cholangiocarcinogenesis in mice via c-Myc-dependent mechanisms. Targeting c-MYC might represent an innovative therapy against iCCA exhibiting low FBXW7 expression. In human CCA, ∼35% specimens were found to harbor FBXW7 mutations, resulting in failure of substrate recognition.17In addition, the FBXW7 gene is downregulated in human CCA cells when compared with intrahepatic biliary epithelial cells.18Also, FBXW7 mRNA expression negatively correlates with tumor stage, metastasis, and differentiation in human iCCA specimens.18,19Furthermore, low FBXW7 levels are associated with poorer prognosis and shorter survival in patients with CCA and hepatocellular carcinoma (HCC).19,20
The lysyl oxidase-like protein 2 (LOXL2) promotes stabilization of the extracellular matrix, chemotaxis, cell growth and cell mobility. We aimed to (i) identify stimuli of LOXL2 in cholangiopathies, (ii) characterize the effects of LOXL2 on biliary epithelial cells’ (BECs) barrier function, (iii) compare LOXL2 expression in primary sclerosing cholangitis (PSC), primary biliary cholangitis, and disease controls, and (iv) to determine LOXL2 expression and its cellular sources in four mouse models of cholangiopathies. Cultured murine BECs were challenged with well-known triggers of cellular senescence, hypoxia, phospholipid-deficient Abcb4−/− mouse bile and chenodeoxycholic acid and investigated for LOXL2, SNAIL1 and E-cadherin expression and transepithelial electrical resistance with and without LOX-inhibition. In vivo, LOXL2 expression was studied in PSC livers, and controls and mouse models. We compared LOXL2 serum levels in patients with PSC, secondary SC, primary biliary cholangitis, and controls. Cellular senescence, hypoxia, Abcb4−/− bile and chenodeoxycholic acid induced LOXL2 and SNAIL1 expression, repressed E-cadherin expression, and significantly reduced transepithelial electrical resistance in BECs. Notably, all of the pathological changes could be recovered via pharmacological LOX-inhibition. Mouse models showed induced LOXL2 expression in the portal region and in association with ductular reaction. LOXL2 serum levels were significantly elevated in patients with cholangiopathies. In PSC, LOXL2 expression was located to characteristic periductal onion skin-type fibrosis, ductular reaction, Kupffer cells, and fibrotic septa. Importantly, in PSC, LOXL2 overexpression was paralleled by E-cadherin loss in BECs from medium-sized bile ducts. Reactive BECs produce LOXL2, resulting in increased tight junction permeability, which can be ameliorated by pharmacological LOX-inhibition in vitro. Reactive BECs, portal myofibroblasts, and Kupffer cells are the main sources of LOXL2 in cholangiopathies. Bile ducts play a pivotal and active role in bile formation and excretion.1–4Bile duct integrity is therefore a prerequisite for normal liver function.5Normal bile duct secretory function critically depends on regular epithelial barrier function including selective permeability for specific molecules.5This requires numerous specific proteins forming a tightly regulated tight junction protein complex between bile duct epithelial cells (BECs).6Cholangiopathies are frequently associated with alterations of BECs’ tight junctions.5,7,8Numerous exogenous and endogenous toxins, cytokines and chemokines, and pathogens may negatively impact on tight junction function studied primarily in vitro and in animal models (e.g. lithocholic acid [LCA]-fed mice, Mdr2−/− mice, alpha-naphthylisothiocyanate).9–11Increasing evidence from mouse models suggests that impaired tight junction integrity may play an important role in the pathogenesis of cholangiopathies and cholestasis: (i) E-cadherin (Cdh1−/−) knockout mice spontaneously develop sclerosing cholangitis,12 (ii) α-catenin (Ctnna1−/−) knockout mice develop cholestasis with reduced bile flow and increased susceptibility to cholic acid feeding injury,13 and (iii) claudin-2 deficiency in mice significantly reduced bile flow.14Consequently, altered bile composition could alter tight junction structure and function9,10,15,16 and conversely, tight junction alterations may affect bile formation and render bile ducts more susceptible to cholangitis.5,13,14,12,17,18
The lysyl oxidase-like protein 2 (LOXL2) promotes stabilization of the extracellular matrix, chemotaxis, cell growth and cell mobility. We aimed to (i) identify stimuli of LOXL2 in cholangiopathies, (ii) characterize the effects of LOXL2 on biliary epithelial cells’ (BECs) barrier function, (iii) compare LOXL2 expression in primary sclerosing cholangitis (PSC), primary biliary cholangitis, and disease controls, and (iv) to determine LOXL2 expression and its cellular sources in four mouse models of cholangiopathies. Cultured murine BECs were challenged with well-known triggers of cellular senescence, hypoxia, phospholipid-deficient Abcb4−/− mouse bile and chenodeoxycholic acid and investigated for LOXL2, SNAIL1 and E-cadherin expression and transepithelial electrical resistance with and without LOX-inhibition. In vivo, LOXL2 expression was studied in PSC livers, and controls and mouse models. We compared LOXL2 serum levels in patients with PSC, secondary SC, primary biliary cholangitis, and controls. Cellular senescence, hypoxia, Abcb4−/− bile and chenodeoxycholic acid induced LOXL2 and SNAIL1 expression, repressed E-cadherin expression, and significantly reduced transepithelial electrical resistance in BECs. Notably, all of the pathological changes could be recovered via pharmacological LOX-inhibition. Mouse models showed induced LOXL2 expression in the portal region and in association with ductular reaction. LOXL2 serum levels were significantly elevated in patients with cholangiopathies. In PSC, LOXL2 expression was located to characteristic periductal onion skin-type fibrosis, ductular reaction, Kupffer cells, and fibrotic septa. Importantly, in PSC, LOXL2 overexpression was paralleled by E-cadherin loss in BECs from medium-sized bile ducts. Reactive BECs produce LOXL2, resulting in increased tight junction permeability, which can be ameliorated by pharmacological LOX-inhibition in vitro. Reactive BECs, portal myofibroblasts, and Kupffer cells are the main sources of LOXL2 in cholangiopathies. Lysyl oxidases (LOX) comprise a family of secreted copper-dependent amine oxidases with five isoforms, including the prototype lysyl oxidase and lysyl oxidase-like (LOXL) proteins (LOXL1-4), which mediate the covalent molecular cross-linking of collagens and elastin through oxidative deamination of peptidyl lysine.19,20LOXL proteins may be of key relevance in tissue homeostasis as LOXL-mediated cross-linking has been shown to significantly affect extracellular matrix stability in the liver.21LOXL family mRNA expression is significantly induced in animal models such as bile duct ligated rats, Abcb4−/− mice as a preclinical primary sclerosing cholangitis (PSC) model22,23 and CCl4-intoxicated rats.24,25In addition, specific LOXL2-inhibition has been recently shown to attenuate biliary liver fibrosis and to promote fibrosis reversal in thioacetamide-treated, BALB/c.Mdr2−/− and diethoxycarbonyl-1,4-dihydrocollidine (DDC)-fed mice.25In addition, a specific LOXL2-blocking antibody has been shown to ameliorate liver fibrosis in CCl4-challanged mice.26Hepatocellular LOX and LOXL2 expression has been demonstrated in livers from patients with Wilson's disease and in some patients with primary biliary cholangitis (PBC),27 but evidence for LOXL2 involvement in human biliary-type liver fibrosis is limited.
The lysyl oxidase-like protein 2 (LOXL2) promotes stabilization of the extracellular matrix, chemotaxis, cell growth and cell mobility. We aimed to (i) identify stimuli of LOXL2 in cholangiopathies, (ii) characterize the effects of LOXL2 on biliary epithelial cells’ (BECs) barrier function, (iii) compare LOXL2 expression in primary sclerosing cholangitis (PSC), primary biliary cholangitis, and disease controls, and (iv) to determine LOXL2 expression and its cellular sources in four mouse models of cholangiopathies. Cultured murine BECs were challenged with well-known triggers of cellular senescence, hypoxia, phospholipid-deficient Abcb4−/− mouse bile and chenodeoxycholic acid and investigated for LOXL2, SNAIL1 and E-cadherin expression and transepithelial electrical resistance with and without LOX-inhibition. In vivo, LOXL2 expression was studied in PSC livers, and controls and mouse models. We compared LOXL2 serum levels in patients with PSC, secondary SC, primary biliary cholangitis, and controls. Cellular senescence, hypoxia, Abcb4−/− bile and chenodeoxycholic acid induced LOXL2 and SNAIL1 expression, repressed E-cadherin expression, and significantly reduced transepithelial electrical resistance in BECs. Notably, all of the pathological changes could be recovered via pharmacological LOX-inhibition. Mouse models showed induced LOXL2 expression in the portal region and in association with ductular reaction. LOXL2 serum levels were significantly elevated in patients with cholangiopathies. In PSC, LOXL2 expression was located to characteristic periductal onion skin-type fibrosis, ductular reaction, Kupffer cells, and fibrotic septa. Importantly, in PSC, LOXL2 overexpression was paralleled by E-cadherin loss in BECs from medium-sized bile ducts. Reactive BECs produce LOXL2, resulting in increased tight junction permeability, which can be ameliorated by pharmacological LOX-inhibition in vitro. Reactive BECs, portal myofibroblasts, and Kupffer cells are the main sources of LOXL2 in cholangiopathies. Specific LOX family isoforms, such as LOXL2, have been shown to participate in cancer progression via the induction of epithelial mesenchymal transition.LOXL2 stabilizes the SNAIL1 protein through oxidative deamination, causing blockage of the GSK3-phosphorylation sites and E-cadherin repression resulting in increased cell mobility.26,28–30This activity may also affect bile duct permeability in cholangiopathies.5,9,23,31–33In certain cholangiopathies, concentric periductal fibrosis results in vascular deprivation of bile ducts and consequently to relative hypoxia of BECs, potentially inducing BEC senescence as recently demonstrated in PSC livers and Abcb4−/− mice.34In addition, during cholestasis, bile acids accumulate in the liver and may also induce liver injury.35,36It is not yet known whether stimuli of cellular senescence, reactive hypoxia, or bile constituents affect LOXL2 expression in BECs and whether this could be linked to the pathobiology of cholangitis and liver fibrosis of the biliary type.Consequently, we hypothesized that increased portal LOXL2 expression promotes the development of bile duct leakiness and biliary type of liver fibrosis in cholangiopathies.5,9,32,33
In non-alcoholic steatohepatitis (NASH), the function of urea cycle enzymes (UCEs) may be affected, resulting in hyperammonemia and the risk of disease progression. We aimed to determine whether the expression and function of UCEs are altered in an animal model of NASH and in patients with non-alcoholic fatty liver disease (NAFLD), and whether this process is reversible. Rats were first fed a high-fat, high-cholesterol diet for 10 months to induce NASH, before being switched onto a normal chow diet to recover. In humans, we obtained liver biopsies from 20 patients with steatosis and 15 with NASH. Primary rat hepatocytes were isolated and cultured with free fatty acids. We measured the gene and protein expression of ornithine transcarbamylase (OTC) and carbamoylphosphate synthetase (CPS1), as well as OTC activity, and ammonia concentrations. Moreover, we assessed the promoter methylation status of OTC and CPS1 in rats, humans and steatotic hepatocytes. In NASH animals, gene and protein expression of OTC and CPS1, and the activity of OTC, were reversibly reduced. Hypermethylation of Otc promoter genes was also observed. Additionally, in patients with NAFLD, OTC enzyme concentration and activity were reduced and ammonia concentrations were increased, which was further exacerbated in those with NASH. Furthermore, OTC and CPS1 promoter regions were hypermethylated. In primary hepatocytes, induction of steatosis was associated with Otc promoter hypermethylation, a reduction in the gene expression of Otc and Cps1, and an increase in ammonia concentration in the supernatant. NASH is associated with a reduction in the gene and protein expression, and activity, of UCEs. This results in hyperammonemia, possibly through hypermethylation of UCE genes and impairment of urea synthesis. Our investigations are the first to describe a link between NASH, the function of UCEs, and hyperammonemia, providing a novel therapeutic target. Alarming increases in the rates of obesity and non-alcoholic fatty liver disease (NAFLD) in an ageing population, have made liver disease a major public health concern for the next decade.1,2NAFLD is a spectrum of liver diseases ranging from steatosis, through non-alcoholic steatohepatitis (NASH) to cirrhosis.3Recently, NASH has been defined as a “multiple parallel hits” disease4 that can progress to liver fibrosis, which is the principal factor contributing to NASH-associated morbidity and mortality.5,6The main cell type responsible for extracellular matrix deposition is hepatic stellate cells (HSCs), which undergo activation in conditions of frank hepatocellular injury, enabling them to participate in the wound healing process7 and making them key in the development of fibrosis.
In non-alcoholic steatohepatitis (NASH), the function of urea cycle enzymes (UCEs) may be affected, resulting in hyperammonemia and the risk of disease progression. We aimed to determine whether the expression and function of UCEs are altered in an animal model of NASH and in patients with non-alcoholic fatty liver disease (NAFLD), and whether this process is reversible. Rats were first fed a high-fat, high-cholesterol diet for 10 months to induce NASH, before being switched onto a normal chow diet to recover. In humans, we obtained liver biopsies from 20 patients with steatosis and 15 with NASH. Primary rat hepatocytes were isolated and cultured with free fatty acids. We measured the gene and protein expression of ornithine transcarbamylase (OTC) and carbamoylphosphate synthetase (CPS1), as well as OTC activity, and ammonia concentrations. Moreover, we assessed the promoter methylation status of OTC and CPS1 in rats, humans and steatotic hepatocytes. In NASH animals, gene and protein expression of OTC and CPS1, and the activity of OTC, were reversibly reduced. Hypermethylation of Otc promoter genes was also observed. Additionally, in patients with NAFLD, OTC enzyme concentration and activity were reduced and ammonia concentrations were increased, which was further exacerbated in those with NASH. Furthermore, OTC and CPS1 promoter regions were hypermethylated. In primary hepatocytes, induction of steatosis was associated with Otc promoter hypermethylation, a reduction in the gene expression of Otc and Cps1, and an increase in ammonia concentration in the supernatant. NASH is associated with a reduction in the gene and protein expression, and activity, of UCEs. This results in hyperammonemia, possibly through hypermethylation of UCE genes and impairment of urea synthesis. Our investigations are the first to describe a link between NASH, the function of UCEs, and hyperammonemia, providing a novel therapeutic target. The urea cycle, located exclusively in the liver, has evolved in humans to remove ammonia.The enzymes involved in this process are carbamoylphosphate synthetase (CPS1), ornithine transcarbamylase (OTC), argininosuccinate synthetase (ASS), argininosuccinate lyase (ASL) and arginase (ARG), but only the first two are present in the mitochondria.8In NASH, many lines of investigation indicate that mitochondria are dysfunctional.9Therefore, it is possible that the resultant mitochondrial injury leads to a modification of the OTC and CPS1 genes, reducing their expression and function, and resulting in hyperammonemia.Accordingly, in an experimental diet-induced NASH model, we demonstrated that the gene and protein expression of OTC and CPS1 were reduced significantly, resulting in a functional reduction in the in vivo capacity for ureagenesis.10This impairs nitrogen homeostasis and results in hyperammonemia.
In non-alcoholic steatohepatitis (NASH), the function of urea cycle enzymes (UCEs) may be affected, resulting in hyperammonemia and the risk of disease progression. We aimed to determine whether the expression and function of UCEs are altered in an animal model of NASH and in patients with non-alcoholic fatty liver disease (NAFLD), and whether this process is reversible. Rats were first fed a high-fat, high-cholesterol diet for 10 months to induce NASH, before being switched onto a normal chow diet to recover. In humans, we obtained liver biopsies from 20 patients with steatosis and 15 with NASH. Primary rat hepatocytes were isolated and cultured with free fatty acids. We measured the gene and protein expression of ornithine transcarbamylase (OTC) and carbamoylphosphate synthetase (CPS1), as well as OTC activity, and ammonia concentrations. Moreover, we assessed the promoter methylation status of OTC and CPS1 in rats, humans and steatotic hepatocytes. In NASH animals, gene and protein expression of OTC and CPS1, and the activity of OTC, were reversibly reduced. Hypermethylation of Otc promoter genes was also observed. Additionally, in patients with NAFLD, OTC enzyme concentration and activity were reduced and ammonia concentrations were increased, which was further exacerbated in those with NASH. Furthermore, OTC and CPS1 promoter regions were hypermethylated. In primary hepatocytes, induction of steatosis was associated with Otc promoter hypermethylation, a reduction in the gene expression of Otc and Cps1, and an increase in ammonia concentration in the supernatant. NASH is associated with a reduction in the gene and protein expression, and activity, of UCEs. This results in hyperammonemia, possibly through hypermethylation of UCE genes and impairment of urea synthesis. Our investigations are the first to describe a link between NASH, the function of UCEs, and hyperammonemia, providing a novel therapeutic target. We have recently demonstrated that pathological ammonia concentrations produce changes in human HSC behaviour, including significant alterations in cellular morphology, reactive oxygen species production and further HSC activation.11Removal of ammonia from the cell cultures restored HSC morphology and function towards normality indicating that the changes in the HSCs induced by ammonia are reversible.11These in vitro data were substantiated in bile duct-ligated rats with advanced fibrosis and hyperammonemia, where treatment with the ammonia lowering drug ornithine-phenylacetate significantly reduced plasma ammonia, markers of HSC activation and portal pressure, indicating that targeting ammonia in vivo reduces HSC activation.11
Emricasan, an oral pan-caspase inhibitor, decreased portal pressure in experimental cirrhosis and in patients with cirrhosis and portal pressure (assessed by the hepatic venous pressure gradient [HVPG]) ≥12 mmHg. We aimed to confirm these results in a randomized, placebo-controlled, double blind study. Multicenter study including 263 patients with cirrhosis due to non-alcoholic steatohepatitis (NASH) and baseline HVPG ≥12 mmHg randomized 1:1:1:1 to emricasan 5 (n=65), 25 (n=65), 50 (n=66) mg or placebo (n=67) orally twice daily for up to 48 weeks. Primary endpoint was change in HVPG (ΔHVPG) at week 24. Secondary endpoints were changes in biomarkers (aminotransferases, caspases, cytokeratins) and development of liver-related outcomes. There were no significant differences in ΔHVPG for any emricasan dose vs. placebo (-0.21, -0.45, -0.58 mmHg, respectively) adjusted by baseline HVPG, compensation status, and non-selective beta-blocker use. Compensated subjects (n=201 [76%]) tended to have a greater decrease in HVPG (emricasan all vs. placebo, p=0.06), the decrease being greater in those with higher baseline HVPG (p=0.018), with a significant interaction between baseline HVPG (continuous, p=0.024; dichotomous at 16 mmHg [median], p=0.013) and treatment. Biomarkers decreased significantly with emricasan at week 24 but returned to baseline levels by week 48. New or worsening decompensating events (∼10% over median exposure of 337 days), progression in MELD and Child Pugh scores, and treatment-emergent adverse events were similar among treatment groups. Despite reduction in biomarkers indicating target engagement, emricasan was not associated with improvement in HVPG or clinical outcomes in patients with NASH cirrhosis and severe portal hypertension. Compensated subjects with higher baseline HVPG had evidence of a small treatment effect. Emricasan treatment appeared safe and well-tolerated. Cirrhosis due to non-alcoholic fatty liver disease is fast becoming a major disease burden worldwide (1).In fact, non-alcoholic steatohepatitis (NASH) is a leading indication for liver transplantation in the U.S. (2).Portal hypertension is a key driver of the major complications of cirrhosis that define decompensation, the latter being the main predictor of death in cirrhosis.(3, 4) A portal pressure (determined by the hepatic venous pressure gradient [HVPG]) ≥10 or ≥12 mmHg is the strongest predictor of decompensation in patients with mostly viral-induced cirrhosis (4), but also in patients with NASH cirrhosis (5).Importantly, decreases in HVPG are associated with lower rates of decompensation and death in compensated and decompensated patients (6).There are currently no approved therapies that can ameliorate the abnormalities leading to significant portal hypertension in NASH cirrhosis patients.
Emricasan, an oral pan-caspase inhibitor, decreased portal pressure in experimental cirrhosis and in patients with cirrhosis and portal pressure (assessed by the hepatic venous pressure gradient [HVPG]) ≥12 mmHg. We aimed to confirm these results in a randomized, placebo-controlled, double blind study. Multicenter study including 263 patients with cirrhosis due to non-alcoholic steatohepatitis (NASH) and baseline HVPG ≥12 mmHg randomized 1:1:1:1 to emricasan 5 (n=65), 25 (n=65), 50 (n=66) mg or placebo (n=67) orally twice daily for up to 48 weeks. Primary endpoint was change in HVPG (ΔHVPG) at week 24. Secondary endpoints were changes in biomarkers (aminotransferases, caspases, cytokeratins) and development of liver-related outcomes. There were no significant differences in ΔHVPG for any emricasan dose vs. placebo (-0.21, -0.45, -0.58 mmHg, respectively) adjusted by baseline HVPG, compensation status, and non-selective beta-blocker use. Compensated subjects (n=201 [76%]) tended to have a greater decrease in HVPG (emricasan all vs. placebo, p=0.06), the decrease being greater in those with higher baseline HVPG (p=0.018), with a significant interaction between baseline HVPG (continuous, p=0.024; dichotomous at 16 mmHg [median], p=0.013) and treatment. Biomarkers decreased significantly with emricasan at week 24 but returned to baseline levels by week 48. New or worsening decompensating events (∼10% over median exposure of 337 days), progression in MELD and Child Pugh scores, and treatment-emergent adverse events were similar among treatment groups. Despite reduction in biomarkers indicating target engagement, emricasan was not associated with improvement in HVPG or clinical outcomes in patients with NASH cirrhosis and severe portal hypertension. Compensated subjects with higher baseline HVPG had evidence of a small treatment effect. Emricasan treatment appeared safe and well-tolerated. Emricasan is an oral pan-caspase inhibitor that decreased excessive apoptosis, inflammation, and fibrosis in animal models of NASH and decreased portal pressure and/or improved survival in rodent models of cirrhosis (7, 8).A 28-day open-label study of emricasan (25 mg orally twice daily) in 22 patients with compensated cirrhosis (primarily due to NASH or HCV) demonstrated clinically meaningful decrease in mean HVPG of -3.7 mmHg in a subgroup with baseline HVPG ≥12 mmHg, with concomitant decreases in aminotransferases that suggested an intrahepatic anti-inflammatory effect (9).
There are conflicting reports on the outcomes after live donor liver transplantation in patients with hepatocellular carcinoma (HCC). We aimed to compare the survival of patients with HCC, with a potential live donor (pLDLT) at listing vs. no potential donor (pDDLT), on an intention-to-treat basis. All patients with HCC listed for liver transplantation between 2000–2015 were included. The pLDLT group was comprised of recipients with a potential live donor identified at listing. Patients without a live donor were included in the pDDLT group. Survival was assessed by the Kaplan-Meier method. Multivariable Cox regression was applied to identify potential predictors of mortality. A total of 219 patients were included in the pLDLT group and 632 patients in the pDDLT group. In the pLDLT group, 57 patients (26%) were beyond the UCSF criteria whereas 119 patients (19%) in the pDDLT group were beyond (p = 0.02). Time on the waiting list was shorter for the pLDLT than the pDDLT group (4.8 [2.9–8.5] months vs. 6.2 [3.0–12.0] months, respectively, p = 0.02). The dropout rate was 32/219 (14.6%) in the pLDLT and 174/632 (27.5%) in the pDDLT group, p <0.001. The 1-, 3- and 5-year intention-to-treat survival rates were 86%, 72% and 68% in the pLDLT vs. 82%, 63% and 57% in the pDDLT group, p = 0.02. Having a potential live donor was a protective factor for death (hazard ratio [HR] 0.67; 95% CI 0.53–0.86). Waiting times of 9–12 months (HR 1.53; 95% CI 1.02–2.31) and ≥12 months (HR 1.69; 95% CI 1.23–2.32) were predictors of death. Having a potential live donor at listing was associated with a significant decrease in the risk of death in patients with HCC in this intention-to-treat analysis. This benefit is related to a lower dropout rate and a shorter waiting period. Live donor liver transplantation (LDLT) improves survival compared to deceased donor liver transplantation (DDLT) when analyzed from the time of listing (intention-to-treat [ITT] analysis) in patients with end-stage liver disease.1,2However, this benefit has never been shown in patients with hepatocellular carcinoma (HCC).Initial experiences with LDLT for HCC had suggested a higher HCC recurrence rate and a decreased survival after transplantation when compared to DDLT.3–6Concerns have been raised regarding liver regeneration after LDLT, the potential of suboptimal oncological resection since the inferior vena cava is not removed, and the fact that some patients with more aggressive tumor biology might be “fast tracked” to LT.3 More recent studies have reported similar outcomes for LDLT and DDLT for patients with HCC.7,8To the best of our knowledge, only 2 studies have compared the outcomes between LDLT and DDLT for HCC on an ITT analysis.7,8Both studies had a limited number of patients undergoing LDLT (36 and 79) and showed no survival differences between LDLT and DDLT.7,8
Liver macrophages can be involved in both pathogen clearance and/or pathogenesis. To get further insight on their role during chronic hepatitis B virus (HBV) infections, our aim was to phenotypically and functionally characterize in vivo and ex vivo the interplay between HBV, primary human liver macrophages (PLMs) and primary blood monocytes differentiated into pro-inflammatory or anti-inflammatory macrophages (M1-MDMs or M2-MDMs, respectively). PLMs or primary blood monocytes, either ex vivo differentiated into M1-MDMs or M2-MDMs, were exposed to HBV and their activation followed by ELISA or quantitative reverse transcription PCR (RT-qPCR). Liver biopsies from HBV-infected patients were analysed by RT-qPCR or immunohistochemistry. Viral parameters in HBV-infected primary human hepatocytes and differentiated HepaRG cells were followed by ELISA, qPCR and RT-qPCR analyses. HBc protein was present within the macrophages of liver biopsies taken from HBV-infected patients. Macrophages from HBV-infected patients also expressed higher levels of anti-inflammatory macrophage markers than those from non-infected patients. Ex vivo exposure of naive PLMs to HBV led to reduced secretion of pro-inflammatory cytokines. Upon exposure to HBV or HBV-producing cells during differentiation and activation, M1-MDMs secreted less IL-6 and IL-1β, whereas M2-MDMs secreted more IL-10 when exposed to HBV during activation. Finally, cytokines produced by M1-MDMs, but not those produced by HBV-exposed M1-MDMs, decreased HBV infection of hepatocytes. Altogether, our data strongly suggest that HBV modulates liver macrophage functions to favour the establishment of infection. Hepatitis B virus (HBV) chronically infects around 250 million people worldwide (World Health Organization data, 2016) and increases the risk of developing cirrhosis and hepatocellular carcinoma.1Current treatments, mainly based on nucleos(t)ide analogues, reduce blood viremia to undetectable levels in the majority of patients, but do not achieve virus elimination from the liver.2New treatments, including immune-therapeutic components, are therefore needed in order to progress toward a functional cure for HBV.
Liver macrophages can be involved in both pathogen clearance and/or pathogenesis. To get further insight on their role during chronic hepatitis B virus (HBV) infections, our aim was to phenotypically and functionally characterize in vivo and ex vivo the interplay between HBV, primary human liver macrophages (PLMs) and primary blood monocytes differentiated into pro-inflammatory or anti-inflammatory macrophages (M1-MDMs or M2-MDMs, respectively). PLMs or primary blood monocytes, either ex vivo differentiated into M1-MDMs or M2-MDMs, were exposed to HBV and their activation followed by ELISA or quantitative reverse transcription PCR (RT-qPCR). Liver biopsies from HBV-infected patients were analysed by RT-qPCR or immunohistochemistry. Viral parameters in HBV-infected primary human hepatocytes and differentiated HepaRG cells were followed by ELISA, qPCR and RT-qPCR analyses. HBc protein was present within the macrophages of liver biopsies taken from HBV-infected patients. Macrophages from HBV-infected patients also expressed higher levels of anti-inflammatory macrophage markers than those from non-infected patients. Ex vivo exposure of naive PLMs to HBV led to reduced secretion of pro-inflammatory cytokines. Upon exposure to HBV or HBV-producing cells during differentiation and activation, M1-MDMs secreted less IL-6 and IL-1β, whereas M2-MDMs secreted more IL-10 when exposed to HBV during activation. Finally, cytokines produced by M1-MDMs, but not those produced by HBV-exposed M1-MDMs, decreased HBV infection of hepatocytes. Altogether, our data strongly suggest that HBV modulates liver macrophage functions to favour the establishment of infection. HBV is a small DNA virus that persists as a covalently-closed-circular DNA (cccDNA) within the nucleus of liver parenchymal cells (hepatocytes).Viral RNAs, including mRNAs and the pre-genomic RNA (pgRNA) are transcribed from the cccDNA.The pgRNA is encapsulated within the nucleocapsid and converted into relaxed-circular DNA (rcDNA) by an HBV polymerase-mediated reverse-transcription step.Different viral products circulate in the blood of infected patients including HBe antigens (HBeAg), Dane particles (infectious particles), HBV RNA containing particles and empty (i.e., nucleocapsid free) enveloped subviral particles (SVPs).The latter 3 have envelope proteins at their surface and are indistinctly detected as HB surface antigens (HBsAg).2SVPs, which are produced in large excess compared to virions, are thought to play an important role in terms of immune subversion.3
Liver macrophages can be involved in both pathogen clearance and/or pathogenesis. To get further insight on their role during chronic hepatitis B virus (HBV) infections, our aim was to phenotypically and functionally characterize in vivo and ex vivo the interplay between HBV, primary human liver macrophages (PLMs) and primary blood monocytes differentiated into pro-inflammatory or anti-inflammatory macrophages (M1-MDMs or M2-MDMs, respectively). PLMs or primary blood monocytes, either ex vivo differentiated into M1-MDMs or M2-MDMs, were exposed to HBV and their activation followed by ELISA or quantitative reverse transcription PCR (RT-qPCR). Liver biopsies from HBV-infected patients were analysed by RT-qPCR or immunohistochemistry. Viral parameters in HBV-infected primary human hepatocytes and differentiated HepaRG cells were followed by ELISA, qPCR and RT-qPCR analyses. HBc protein was present within the macrophages of liver biopsies taken from HBV-infected patients. Macrophages from HBV-infected patients also expressed higher levels of anti-inflammatory macrophage markers than those from non-infected patients. Ex vivo exposure of naive PLMs to HBV led to reduced secretion of pro-inflammatory cytokines. Upon exposure to HBV or HBV-producing cells during differentiation and activation, M1-MDMs secreted less IL-6 and IL-1β, whereas M2-MDMs secreted more IL-10 when exposed to HBV during activation. Finally, cytokines produced by M1-MDMs, but not those produced by HBV-exposed M1-MDMs, decreased HBV infection of hepatocytes. Altogether, our data strongly suggest that HBV modulates liver macrophage functions to favour the establishment of infection. Several pro-inflammatory cytokines (IL-6, IL-1β and TNFα) and interferons (IFNα and IFNγ) were shown to induce a direct antiviral effect on HBV replication in hepatocytes,4–7 with IL-1β being one of the most efficient at inhibiting already established HBV infections in vitro.4IL-1β is a pro-inflammatory cytokine produced upon inflammasome activation.8In the liver, it is mostly produced by macrophages (MΦ) since hepatocytes do not possess functional inflammasomes.9,10Liver resident MΦ, named Kupffer cells (KCs), represent 80% of the MΦ count in the body at steady state.11They are specialised in the detection of pathogens coming from the enteric circulation, as well as in the elimination of aging blood cells, through their high phagocytic capacity.11Upon inflammation, monocytes from the blood circulation can be recruited in the liver and differentiate locally into MΦ that are called monocyte-derived-macrophages (MDMs).12KCs and MDMs have different embryonic origins and functions.13In vivo, a wide range of different phenotypes of MΦ exist, depending on their origin, activation status, localisation, and their micro-environment.14Inflammatory MΦ, commonly called M1, which are phenotypically described as CD40+ CD86+ HLA-DR+, are characterised by their capacity to secrete pro-inflammatory cytokines/chemokines, such as IL-1β, produce various antimicrobial factors, including nitric oxide, and are therefore implicated in inflammation and elimination of pathogens.14Anti-inflammatory MΦ, commonly called M2, express the arginase 1, mannose receptors, and the high affinity scavenger receptor CD163, and secrete anti-inflammatory cytokines, such as IL-10 and TGFβ, as well as angiogenic factors, such as VEGF.14M2 MΦ are involved in the resolution of inflammation and in tolerance mechanisms.14They are also found abundantly in the tumour microenvironment and are called tumour-associated MΦ (TAMs).15
Around 5% of patients with chronic hepatitis C virus (HCV) infection treated with direct-acting antiviral (DAA) agents do not achieve sustained virological response (SVR). The currently approved retreatment regimen for prior DAA failure is a combination of sofosbuvir, velpatasvir, and voxilaprevir (SOF/VEL/VOX), although there is little data on its use in clinical practice. The aim of this study was to analyse the effectiveness and safety of SOF/VEL/VOX in the real-world setting. This was a prospective multicentre study assessing the efficacy of retreatment with SOF/VEL/VOX in patients who had experienced a prior DAA treatment failure. The primary endpoint was SVR 12 weeks after the completion of treatment (SVR12). Data on safety and tolerability were also recorded. A total of 137 patients were included: 75% men, 35% with liver cirrhosis. Most were infected with HCV genotype (GT) 1 or 3. The most common prior DAA combinations were sofosbuvir plus an NS5A inhibitor or ombitasvir/paritaprevir/r+dasabuvir. A total of 136 (99%) patients achieved undetectable HCV RNA at the end of treatment. Overall SVR12 was 95% in the 135 patients reaching this point. SVR12 was lower in patients with cirrhosis (89%, p = 0.05) and those with GT3 infection (80%, p <0.001). Patients with GT3 infection and cirrhosis had the lowest SVR12 rate (69%). Of the patients who did not achieve SVR12, 1 was reinfected and 7 experienced treatment failure (6 GT3, 1 GT1a). The presence of resistance-associated substitutions did not impact SVR12. Adverse effects were mild and non-specific. Real-world data show that SOF/VEL/VOX is an effective, safe rescue therapy for patients with prior DAA treatment failure despite the presence of resistance-associated substitutions. However, patients with liver cirrhosis infected by GT3 remain the most-difficult-to-treat group. Current treatments with direct-acting antivirals (DAAs) for hepatitis C virus (HCV) infection lead to elimination of the virus in more than 95% of patients, regardless of the HCV genotype or presence of advanced liver fibrosis.1The American Association for the Study of Liver Diseases (AASLD) and the European Association for the Study of the Liver (EASL) guidelines both recommend combinations including an NS5A inhibitor with either a NS3/4 protease inhibitor, such as grazoprevir/elbasvir or glecaprevir/pibrentasvir, or a nucleotide analogue plus an NS5A inhibitor, such as sofosbuvir/velpatasvir, for durations ranging from 8 to 12 weeks.2–4Sofosbuvir/velpatasvir and glecaprevir/pibrentasvir combinations are pangenotypic and therefore, they are the preferred regimens to simplify HCV therapy.5–7Despite the high efficacy of these new combinations, the options for patients who do not achieve a sustained virological response (SVR) are limited.8The latest approved retreatment regimen is combined therapy with sofosbuvir plus the NS55 inhibitor, velpatasvir, and the NS3/4 protease inhibitor, voxilaprevir (SOF/VEL/VOX),9 which is recommended in the AASLD and EASL guidelines for retreating patients previously failing DAA regimens.2–4
Around 5% of patients with chronic hepatitis C virus (HCV) infection treated with direct-acting antiviral (DAA) agents do not achieve sustained virological response (SVR). The currently approved retreatment regimen for prior DAA failure is a combination of sofosbuvir, velpatasvir, and voxilaprevir (SOF/VEL/VOX), although there is little data on its use in clinical practice. The aim of this study was to analyse the effectiveness and safety of SOF/VEL/VOX in the real-world setting. This was a prospective multicentre study assessing the efficacy of retreatment with SOF/VEL/VOX in patients who had experienced a prior DAA treatment failure. The primary endpoint was SVR 12 weeks after the completion of treatment (SVR12). Data on safety and tolerability were also recorded. A total of 137 patients were included: 75% men, 35% with liver cirrhosis. Most were infected with HCV genotype (GT) 1 or 3. The most common prior DAA combinations were sofosbuvir plus an NS5A inhibitor or ombitasvir/paritaprevir/r+dasabuvir. A total of 136 (99%) patients achieved undetectable HCV RNA at the end of treatment. Overall SVR12 was 95% in the 135 patients reaching this point. SVR12 was lower in patients with cirrhosis (89%, p = 0.05) and those with GT3 infection (80%, p <0.001). Patients with GT3 infection and cirrhosis had the lowest SVR12 rate (69%). Of the patients who did not achieve SVR12, 1 was reinfected and 7 experienced treatment failure (6 GT3, 1 GT1a). The presence of resistance-associated substitutions did not impact SVR12. Adverse effects were mild and non-specific. Real-world data show that SOF/VEL/VOX is an effective, safe rescue therapy for patients with prior DAA treatment failure despite the presence of resistance-associated substitutions. However, patients with liver cirrhosis infected by GT3 remain the most-difficult-to-treat group. The SOF/VEL/VOX combination, evaluated in 2 phase II trials including DAA-experienced patients, yielded very high SVR rates.10,11Later, the single-tablet SOF/VEL/VOX combination in a 12-week regimen was evaluated in 2 phase III trials including patients previously treated with a DAA-containing regimen.12In POLARIS-1, 300 patients with HCV genotype (GT)1 infection who had previously received a regimen containing an NS5A inhibitor were randomly assigned to receive either SOF/VEL/VOX (150 patients) or placebo (150 patients) once daily for 12 weeks.In addition, 114 patients infected with HCV other than GT1 were enrolled in the SOF/VEL/VOX group.In another study, POLARIS-4, patients with HCV GT1 to GT4 who had previously received a DAA regimen without an NS5A inhibitor were randomly assigned to receive SOF/VEL/VOX (182 patients) or sofosbuvir/velpatasvir (151 patients) for 12 weeks.A substudy was also carried out including 147 patients randomized to receive placebo in POLARIS-1 who did not achieve an SVR 12 weeks after completion of treatment (SVR12).These patients were retreated with 12 weeks of SOF/VEL/VOX and 97% achieved SVR12.13Four patients had virological relapse; all were infected with GT1a but only one had liver cirrhosis.In total, 96% of patients in POLARIS-1, 98% in POLARIS-4, and 97% in the POLARIS-1 substudy achieved SVR, suggesting high efficacy of SOF/VEL/VOX as a retreatment regimen.
Around 5% of patients with chronic hepatitis C virus (HCV) infection treated with direct-acting antiviral (DAA) agents do not achieve sustained virological response (SVR). The currently approved retreatment regimen for prior DAA failure is a combination of sofosbuvir, velpatasvir, and voxilaprevir (SOF/VEL/VOX), although there is little data on its use in clinical practice. The aim of this study was to analyse the effectiveness and safety of SOF/VEL/VOX in the real-world setting. This was a prospective multicentre study assessing the efficacy of retreatment with SOF/VEL/VOX in patients who had experienced a prior DAA treatment failure. The primary endpoint was SVR 12 weeks after the completion of treatment (SVR12). Data on safety and tolerability were also recorded. A total of 137 patients were included: 75% men, 35% with liver cirrhosis. Most were infected with HCV genotype (GT) 1 or 3. The most common prior DAA combinations were sofosbuvir plus an NS5A inhibitor or ombitasvir/paritaprevir/r+dasabuvir. A total of 136 (99%) patients achieved undetectable HCV RNA at the end of treatment. Overall SVR12 was 95% in the 135 patients reaching this point. SVR12 was lower in patients with cirrhosis (89%, p = 0.05) and those with GT3 infection (80%, p <0.001). Patients with GT3 infection and cirrhosis had the lowest SVR12 rate (69%). Of the patients who did not achieve SVR12, 1 was reinfected and 7 experienced treatment failure (6 GT3, 1 GT1a). The presence of resistance-associated substitutions did not impact SVR12. Adverse effects were mild and non-specific. Real-world data show that SOF/VEL/VOX is an effective, safe rescue therapy for patients with prior DAA treatment failure despite the presence of resistance-associated substitutions. However, patients with liver cirrhosis infected by GT3 remain the most-difficult-to-treat group. In these trials, all patients underwent baseline deep sequencing of the NS3, NS5A, and NS5B coding regions of HCV to detect genetic changes associated with resistance to the treatment received.Resistance-associated substitutions (RASs) were reported when they were detected in more than 15% of the sequence reads.Nonetheless, the presence of RASs did not have a significant impact on the rates of SVR12.The POLARIS-1 and POLARIS-4 trials showed that treatment with SOF/VEL/VOX for 12 weeks is an effective and safe option for retreating patients with HCV, although the data on its use in clinical practice are limited.
Frail patients with low model for end-stage liver disease (MELD) scores may be under-prioritised. Low skeletal muscle mass, namely sarcopenia, has been identified as a risk factor for waiting list mortality. A recent study proposed incorporating sarcopenia in the MELD score (MELD-Sarcopenia score). We aimed to investigate the association between sarcopenia and waiting list mortality, and to validate the MELD-Sarcopenia score (i.e. MELD + 10.35 * Sarcopenia). We identified consecutive patients with cirrhosis listed for liver transplantation in the Eurotransplant registry between 2007–2014 and measured skeletal muscle mass on computed tomography. A competing risk analysis was used to compare survival of patients with and without sarcopenia, and concordance (c) indices were calculated to assess performance of the MELD and MELD-Sarcopenia score. We created a nomogram of the best predictive model. We included 585 patients with a median MELD score of 14 (interquartile range 9–19), of which 254 (43.4%) were identified as having sarcopenia. Median waiting list survival was shorter in patients with sarcopenia than those without (p <0.001). This effect was even more pronounced in patients with MELD ≤15. The discriminative performance of the MELD-Sarcopenia score (c-index 0.820) for three-month mortality was lower than MELD score alone (c-index 0.839). Apart from sarcopenia and MELD score, other predictive variables were occurrence of hepatic encephalopathy before listing and recipient age. A model including all these variables yielded a c-index of 0.851. Sarcopenia was associated with waiting list mortality in liver transplant candidates with cirrhosis, particularly in patients with lower MELD scores. The MELD-Sarcopenia score was successfully validated in this cohort. However, incorporating sarcopenia in the MELD score had limited added value in predicting waiting list mortality. Model for end-stage liver disease (MELD) score is the most frequently used method to prioritise patients with end-stage liver disease for liver transplantation and it is calculated using serum levels of bilirubin, creatinine and the international normalized ratio (INR).1Despite its strong predictive value, the MELD score underestimates disease severity in about 15–20% of patients with cirrhosis, resulting in an inaccurate prediction of survival.2Amongst others, conditions such as hyponatremia and hypoalbuminemia have been identified as additional risk factors for impaired waiting list survival.This knowledge resulted in modifications of the original MELD score; i.e. the MELDNa and five-variable MELD score, respectively.3–5Moreover, a frequently reported drawback of the MELD score is the lack of an objective parameter reflecting patients’ physical and nutritional status, as was albumin in the old Child-Turcotte-Pugh score.Consequently, patients with a biochemically low MELD score, but with malnutrition or low skeletal muscle mass (sarcopenia), may be under prioritised in the current system.6Indeed, sarcopenia, a hallmark of frailty and functional decline,7,8 has recently been found to predict waiting list mortality.9,10Montano-Loza et al. found a significantly shorter waiting list survival in patients with sarcopenia, and therefore included sarcopenia in the MELD score (MELD-Sarcopenia score).This score showed a higher predictive accuracy for waiting list mortality than the MELD score alone.10
It is important to know which patients with hepatitis C are likely to develop liver-related complications after achieving a sustained virological response (SVR) to direct-acting antiviral (DAA) therapy. We aimed to describe the incidence of liver-related events in a population of patients with HCV-associated compensated advanced chronic liver disease (cACLD) who achieved SVR and to identify non-invasive parameters that predict the occurrence of liver-related events. This 2-center prospective study included 572 patients with cACLD who had been treated with DAAs and had achieved SVR. Patients had liver stiffness measurement (LSM) ≥10 kPa at baseline and had never decompensated (Child-Pugh class A). Laboratory work-up and LSM were performed at baseline and at 1 year of follow-up. The median follow-up was 2.8 years during which 32 patients (5.6%) presented with a liver-related event. The incidence rate (IR) of portal hypertension-related decompensation was 0.34/100 patient-years. These patients all had baseline LSM >20 kPa, and LSM did not improve during follow-up in 4 out of 5 of them. Hepatocellular carcinoma (HCC) occurred in 25 patients (IR 1.5/100 patient-years). Albumin levels at follow-up (hazard ratio [HR] 0.08; 95% CI 0.02–0.25) and LSM <10 kPa at follow-up (HR 0.33; 95% CI 0.11–0.96) were independently associated with the risk of HCC. Combining both predictors identified 2 groups with differing risk of HCC occurrence: those with LSM ≥20 kPa at follow-up or those with LSM between 10–20 kPa and albumin levels <4.4 g/dl were at the highest risk (IR ≥1.9/100 patient-years). Visual nomograms predicting HCC risk based on LSM and albumin at 1 year of follow-up were constructed. In patients with HCV-related cACLD who have achieved SVR with DAAs, HCC is the most frequent liver-related event. Both albumin levels and LSM are useful for stratifying patients based on their risk of developing HCC during follow-up. Direct-acting antivirals (DAAs) have become the new standard of care for patients with chronic hepatitis C virus (HCV) infection, demonstrating high efficacy, with most patients achieving a sustained virological response (SVR) regardless of HCV genotype.Due to their good safety profile, any patient in any stage of chronic liver disease (from mild fibrosis to decompensated cirrhosis) can be treated with DAAs.1Therefore, it is important to know which patients will be prone to developing liver-related complications, such as hepatocellular carcinoma (HCC) or liver decompensation, requiring lifelong follow-up and which patients can be safely discharged from follow-up.
It is important to know which patients with hepatitis C are likely to develop liver-related complications after achieving a sustained virological response (SVR) to direct-acting antiviral (DAA) therapy. We aimed to describe the incidence of liver-related events in a population of patients with HCV-associated compensated advanced chronic liver disease (cACLD) who achieved SVR and to identify non-invasive parameters that predict the occurrence of liver-related events. This 2-center prospective study included 572 patients with cACLD who had been treated with DAAs and had achieved SVR. Patients had liver stiffness measurement (LSM) ≥10 kPa at baseline and had never decompensated (Child-Pugh class A). Laboratory work-up and LSM were performed at baseline and at 1 year of follow-up. The median follow-up was 2.8 years during which 32 patients (5.6%) presented with a liver-related event. The incidence rate (IR) of portal hypertension-related decompensation was 0.34/100 patient-years. These patients all had baseline LSM >20 kPa, and LSM did not improve during follow-up in 4 out of 5 of them. Hepatocellular carcinoma (HCC) occurred in 25 patients (IR 1.5/100 patient-years). Albumin levels at follow-up (hazard ratio [HR] 0.08; 95% CI 0.02–0.25) and LSM <10 kPa at follow-up (HR 0.33; 95% CI 0.11–0.96) were independently associated with the risk of HCC. Combining both predictors identified 2 groups with differing risk of HCC occurrence: those with LSM ≥20 kPa at follow-up or those with LSM between 10–20 kPa and albumin levels <4.4 g/dl were at the highest risk (IR ≥1.9/100 patient-years). Visual nomograms predicting HCC risk based on LSM and albumin at 1 year of follow-up were constructed. In patients with HCV-related cACLD who have achieved SVR with DAAs, HCC is the most frequent liver-related event. Both albumin levels and LSM are useful for stratifying patients based on their risk of developing HCC during follow-up. Several studies have demonstrated that in patients with cirrhosis who have achieved SVR after DAA therapy there is a decrease in liver-related events due to an improvement in liver function and portal hypertension, and a decrease in the incidence of de novo HCC, accompanied by an overall increase in survival rates.2–7These studies have also demonstrated that patients with cirrhosis are at higher risk of complications than those without cirrhosis.However, the variability in the methods used to select and define patients with cirrhosis in different published studies and the heterogeneity of patients included, mixing patients with compensated cirrhosis and decompensated cirrhosis, makes it hard to find predictors to identify high-risk populations and validate the results.
It is important to know which patients with hepatitis C are likely to develop liver-related complications after achieving a sustained virological response (SVR) to direct-acting antiviral (DAA) therapy. We aimed to describe the incidence of liver-related events in a population of patients with HCV-associated compensated advanced chronic liver disease (cACLD) who achieved SVR and to identify non-invasive parameters that predict the occurrence of liver-related events. This 2-center prospective study included 572 patients with cACLD who had been treated with DAAs and had achieved SVR. Patients had liver stiffness measurement (LSM) ≥10 kPa at baseline and had never decompensated (Child-Pugh class A). Laboratory work-up and LSM were performed at baseline and at 1 year of follow-up. The median follow-up was 2.8 years during which 32 patients (5.6%) presented with a liver-related event. The incidence rate (IR) of portal hypertension-related decompensation was 0.34/100 patient-years. These patients all had baseline LSM >20 kPa, and LSM did not improve during follow-up in 4 out of 5 of them. Hepatocellular carcinoma (HCC) occurred in 25 patients (IR 1.5/100 patient-years). Albumin levels at follow-up (hazard ratio [HR] 0.08; 95% CI 0.02–0.25) and LSM <10 kPa at follow-up (HR 0.33; 95% CI 0.11–0.96) were independently associated with the risk of HCC. Combining both predictors identified 2 groups with differing risk of HCC occurrence: those with LSM ≥20 kPa at follow-up or those with LSM between 10–20 kPa and albumin levels <4.4 g/dl were at the highest risk (IR ≥1.9/100 patient-years). Visual nomograms predicting HCC risk based on LSM and albumin at 1 year of follow-up were constructed. In patients with HCV-related cACLD who have achieved SVR with DAAs, HCC is the most frequent liver-related event. Both albumin levels and LSM are useful for stratifying patients based on their risk of developing HCC during follow-up. It is also important to note that currently most of the asymptomatic patients with advanced fibrosis or cirrhosis are diagnosed by elastographic methods such transient elastography and not by liver biopsy.So in many of these cases, the exact stage of the disease is suspected, but unconfirmed.For this reason, the Baveno VI consensus defined patients with liver stiffness measurement (LSM) ≥10 kPa and no prior decompensation as having “compensated advanced chronic liver disease” (cACLD).8This patient population is of particular interest because, although they have never decompensated, approximately 80–90% of them have portal hypertension and up to 50–60% have clinically significant portal hypertension (CSPH).9It would probably be easier to identify low-risk groups for liver-related events in this cACLD population, where some patients have an initial degree of advanced liver disease and higher odds of improving with therapy, than to identify them in mixed cirrhotic populations (Child-Pugh class A-B) affected by more advanced disease (with a higher proportion of CSPH and a greater risk of complications), in whom mechanisms of hepatocarcinogenesis might already have been initiated.
Most patients with hepatitis C virus (HCV) infection will undergo antiviral treatment with direct-acting antivirals (DAAs) and achieve sustained virologic response (SVR). We aimed to develop models estimating hepatocellular carcinoma (HCC) risk after antiviral treatment. We identified 45,810 patients who initiated antiviral treatment in the Veterans Affairs (VA) national healthcare system from 1/1/2009 to 12/31/2015, including 29,309 (64%) DAA-only regimens and 16,501 (36%) interferon ± DAA regimens. We retrospectively followed patients until 6/15/2017 to identify incident cases of HCC. We used Cox proportional hazards regression to develop and internally validate models predicting HCC risk using baseline characteristics at the time of antiviral treatment. We identified 1,412 incident cases of HCC diagnosed at least 180 days after initiation of antiviral treatment during a mean follow-up of 2.5 years (range 1.0–7.5 years). Models predicting HCC risk after antiviral treatment were developed and validated separately for four subgroups of patients: cirrhosis/SVR, cirrhosis/no SVR, no cirrhosis/SVR, no cirrhosis/no SVR. Four predictors (age, platelet count, serum aspartate aminotransferase/√alanine aminotransferase ratio and albumin) accounted for most of the models’ predictive value, with smaller contributions from sex, race-ethnicity, HCV genotype, body mass index, hemoglobin and serum alpha-fetoprotein. Fitted models were well-calibrated with very good measures of discrimination. Decision curves demonstrated higher net benefit of using model-based HCC risk estimates to determine whether to recommend screening or not compared to the screen-all or screen-none strategies. We developed and internally validated models that estimate HCC risk following antiviral treatment. These models are available as web-based tools that can be used to inform risk-based HCC surveillance strategies in individual patients. Most patients with chronic hepatitis C virus (HCV) infection have either already received antiviral treatment or are expected to receive treatment with direct-acting antivirals (DAAs) in the next 3–5 years in the United States.With sustained virologic response (SVR) rates well in excess of 90%, the vast majority of treated patients will achieve HCV eradication.SVR reduces hepatocellular carcinoma (HCC) risk substantially, irrespective of whether it is achieved by interferon (IFN) or DAA-based regimens.1It follows that HCC risk needs to be estimated specifically for the period following antiviral treatment, incorporating whether SVR was achieved or not, and that previous models predicting HCC risk in untreated HCV-infected patients do not apply to patients who have undergone antiviral treatment.
Most patients with hepatitis C virus (HCV) infection will undergo antiviral treatment with direct-acting antivirals (DAAs) and achieve sustained virologic response (SVR). We aimed to develop models estimating hepatocellular carcinoma (HCC) risk after antiviral treatment. We identified 45,810 patients who initiated antiviral treatment in the Veterans Affairs (VA) national healthcare system from 1/1/2009 to 12/31/2015, including 29,309 (64%) DAA-only regimens and 16,501 (36%) interferon ± DAA regimens. We retrospectively followed patients until 6/15/2017 to identify incident cases of HCC. We used Cox proportional hazards regression to develop and internally validate models predicting HCC risk using baseline characteristics at the time of antiviral treatment. We identified 1,412 incident cases of HCC diagnosed at least 180 days after initiation of antiviral treatment during a mean follow-up of 2.5 years (range 1.0–7.5 years). Models predicting HCC risk after antiviral treatment were developed and validated separately for four subgroups of patients: cirrhosis/SVR, cirrhosis/no SVR, no cirrhosis/SVR, no cirrhosis/no SVR. Four predictors (age, platelet count, serum aspartate aminotransferase/√alanine aminotransferase ratio and albumin) accounted for most of the models’ predictive value, with smaller contributions from sex, race-ethnicity, HCV genotype, body mass index, hemoglobin and serum alpha-fetoprotein. Fitted models were well-calibrated with very good measures of discrimination. Decision curves demonstrated higher net benefit of using model-based HCC risk estimates to determine whether to recommend screening or not compared to the screen-all or screen-none strategies. We developed and internally validated models that estimate HCC risk following antiviral treatment. These models are available as web-based tools that can be used to inform risk-based HCC surveillance strategies in individual patients. Current guidelines recommend the same screening strategy for all HCV-infected patients with cirrhosis (ultrasonography every six months ± serum alpha-fetoprotein [AFP] testing) while no screening is recommended for non-cirrhotic patients, regardless of their HCC risk.2This “one-size-fits-all” strategy raises many questions in the DAA era and leaves room for improvement.For example, a patient with cirrhosis may have favorable characteristics that, together with HCV eradication, substantially lower the patient’s HCC risk.Since surveillance is thought to increase survival or become cost-effective in cirrhotic patients only when HCC risk exceeds 1.5% per year,3,4 surveillance may not be warranted in such a patient.Conversely, in cirrhotic patients who fail antiviral treatments and/or have additional adverse characteristics, HCC risk may be so high that more aggressive surveillance strategies like annual magnetic resonance imaging (MRI), abbreviated MRI5 or computerized tomography (CT) become more efficacious or cost-effective than ultrasound scan (USS).6Furthermore, patients without established cirrhosis who fail antiviral treatment and have additional adverse characteristics, may have HCC risk sufficiently high to merit screening.However, no method is currently available to estimate HCC risk in these patients.
Most patients with hepatitis C virus (HCV) infection will undergo antiviral treatment with direct-acting antivirals (DAAs) and achieve sustained virologic response (SVR). We aimed to develop models estimating hepatocellular carcinoma (HCC) risk after antiviral treatment. We identified 45,810 patients who initiated antiviral treatment in the Veterans Affairs (VA) national healthcare system from 1/1/2009 to 12/31/2015, including 29,309 (64%) DAA-only regimens and 16,501 (36%) interferon ± DAA regimens. We retrospectively followed patients until 6/15/2017 to identify incident cases of HCC. We used Cox proportional hazards regression to develop and internally validate models predicting HCC risk using baseline characteristics at the time of antiviral treatment. We identified 1,412 incident cases of HCC diagnosed at least 180 days after initiation of antiviral treatment during a mean follow-up of 2.5 years (range 1.0–7.5 years). Models predicting HCC risk after antiviral treatment were developed and validated separately for four subgroups of patients: cirrhosis/SVR, cirrhosis/no SVR, no cirrhosis/SVR, no cirrhosis/no SVR. Four predictors (age, platelet count, serum aspartate aminotransferase/√alanine aminotransferase ratio and albumin) accounted for most of the models’ predictive value, with smaller contributions from sex, race-ethnicity, HCV genotype, body mass index, hemoglobin and serum alpha-fetoprotein. Fitted models were well-calibrated with very good measures of discrimination. Decision curves demonstrated higher net benefit of using model-based HCC risk estimates to determine whether to recommend screening or not compared to the screen-all or screen-none strategies. We developed and internally validated models that estimate HCC risk following antiviral treatment. These models are available as web-based tools that can be used to inform risk-based HCC surveillance strategies in individual patients. Central to these considerations is the concept that surveillance confers harm to patients who do not have HCC (or will not develop HCC in the timeframe of interest) as well as benefits to those who have (or will develop) HCC.Such harms include unnecessary anxiety, biopsies, imaging studies or even treatments.Therefore, HCC surveillance should not be recommended for every patient, but instead only for patients whose risk exceeds a predetermined risk threshold.It can be shown that an appropriate risk threshold depends on the ratio of the harms associated with a missed cancer to the harms associated with unnecessary screening.7,8For example, if surveillance is recommended for an annual HCC risk >2% it means that we consider the harms of missing a cancer to be approximately 50 (or 98/2) times greater than the harms of unnecessary screening.The appropriate risk threshold is likely different in different clinically relevant subgroups of patients such as those with/without cirrhosis and with/without SVR.
Non-invasive imaging is crucial for the early diagnosis and successful treatment of hepatocellular carcinoma (HCC). Terminology and criteria for interpreting and reporting imaging results must be standardized to optimize diagnosis. The aim of this study was to prospectively compare the diagnostic accuracy of the American Association for the Study of Liver Diseases (AASLD) and the 2014 version of Liver Imaging Reporting and Data System (LI-RADS®) criteria for the non-invasive diagnosis of small HCC, and to evaluate the diagnostic value of ancillary features used in the LI-RADS criteria. Between April 2009 and April 2012, patients with cirrhosis and one to three 10–30 mm nodules were enrolled and underwent computed tomography (CT) and magnetic resonance (MR) imaging. The diagnostic accuracy of both the AASLD and the LI-RADS criteria were determined based on their sensitivity, specificity, positive (PPV) and negative predictive values (NPV). A total of 595 nodules were included (559 [341 HCC, 61%] with MR imaging and 529 [332 HCC, 63%] with CT). Overall, no (0%) LR-1 and LR-2, 44 (33%) and 47 (41%) LR-3, 50 (53%) and 54 (55%) LR-4, 244 (94%) and 222 (91%) LR-5 and 4 (67%) and 9 (82%) LR-5V were HCC on MR imaging and CT, respectively. The sensitivity, specificity, PPV/NPV of the AASLD score was 72.5%, 87.6%, 90.2%, and 66.9% for MR imaging, and 71.4%, 77.7%, 84.3%, 61.7% for CT, respectively. For the combination of LR-5V and LR-5 nodules these measures were 72.5%, 89.9%, 91.9% and 67.5% on MRI and 66.9%, 88.3%, 90.9% and 63.3% on CT, respectively. For the combination of LR-5V, LR-5 and LR-4 nodules they were 87.1%, 69.1%, 81.6% and 77.3% on MRI and 85.8%, 66%, 81% on 73.5% on CT, respectively. The 2014 version of the LI-RADS is no more accurate than the AASLD score for the non-invasive diagnosis of small HCC in high-risk patients, but it provides important and complementary information on the probability of having HCC in high-risk patients, allowing for possible changes in the management of these patients. Hepatocellular carcinoma (HCC) is the fifth most frequent cancer and the second leading cause of death from cancer worldwide, occurring on cirrhosis in over 90% of cases.1,2Early detection is the only hope for effective and curative treatment of patients with HCC, emphasizing the crucial role of screening strategies in the monitoring of high-risk patients.Computed tomography (CT) and magnetic resonance (MR) imaging play a key role in the diagnostic strategy of HCC, and an imaged-based diagnosis of HCC is accepted by several guidelines in Europe, North America and Asia, based on the unique vascular profile of this tumor.2–6In fact, in nodules measuring >10 mm, the combination of hyperenhancement during the arterial phase followed by washout during the portal venous and/or delayed phases has been shown to have a specificity for HCC of nearly 100%, but the sensitivity and specificity is influenced by the size of the tumors.7,8
Non-invasive imaging is crucial for the early diagnosis and successful treatment of hepatocellular carcinoma (HCC). Terminology and criteria for interpreting and reporting imaging results must be standardized to optimize diagnosis. The aim of this study was to prospectively compare the diagnostic accuracy of the American Association for the Study of Liver Diseases (AASLD) and the 2014 version of Liver Imaging Reporting and Data System (LI-RADS®) criteria for the non-invasive diagnosis of small HCC, and to evaluate the diagnostic value of ancillary features used in the LI-RADS criteria. Between April 2009 and April 2012, patients with cirrhosis and one to three 10–30 mm nodules were enrolled and underwent computed tomography (CT) and magnetic resonance (MR) imaging. The diagnostic accuracy of both the AASLD and the LI-RADS criteria were determined based on their sensitivity, specificity, positive (PPV) and negative predictive values (NPV). A total of 595 nodules were included (559 [341 HCC, 61%] with MR imaging and 529 [332 HCC, 63%] with CT). Overall, no (0%) LR-1 and LR-2, 44 (33%) and 47 (41%) LR-3, 50 (53%) and 54 (55%) LR-4, 244 (94%) and 222 (91%) LR-5 and 4 (67%) and 9 (82%) LR-5V were HCC on MR imaging and CT, respectively. The sensitivity, specificity, PPV/NPV of the AASLD score was 72.5%, 87.6%, 90.2%, and 66.9% for MR imaging, and 71.4%, 77.7%, 84.3%, 61.7% for CT, respectively. For the combination of LR-5V and LR-5 nodules these measures were 72.5%, 89.9%, 91.9% and 67.5% on MRI and 66.9%, 88.3%, 90.9% and 63.3% on CT, respectively. For the combination of LR-5V, LR-5 and LR-4 nodules they were 87.1%, 69.1%, 81.6% and 77.3% on MRI and 85.8%, 66%, 81% on 73.5% on CT, respectively. The 2014 version of the LI-RADS is no more accurate than the AASLD score for the non-invasive diagnosis of small HCC in high-risk patients, but it provides important and complementary information on the probability of having HCC in high-risk patients, allowing for possible changes in the management of these patients. Nevertheless, the inter-observer variability of these imaging criteria can vary according to the radiologist’s experience, the local medical culture and differences in interpretation.9Recently, the Liver Imaging Reporting and Data System (LI-RADS®) was developed by the American College of Radiology to standardize terminology and criteria for interpreting and reporting CT and MR imaging results of the liver in patients at risk of HCC.The initial version of LI-RADS first appeared online in 2011 and has been updated several times.10The aim of LI-RADS is to help radiologists categorize findings in at-risk populations and to assist referring physicians in understanding the reports of liver imaging.Observations are categorized from LR-1 (definitely benign) to LR-5 (definitely HCC).LI-RADS includes major and ancillary criteria.The former includes lesion size (10 to 19 mm and ≥20 mm), enhancement on multiphasic contrast-enhanced imaging including arterial, portal venous, and/or delayed phases, the presence of a capsule, and growth over six months.The latter are ancillary features that can result in the upgrade or downgrade of the category.The relative weight of each feature remains unknown.
Non-invasive imaging is crucial for the early diagnosis and successful treatment of hepatocellular carcinoma (HCC). Terminology and criteria for interpreting and reporting imaging results must be standardized to optimize diagnosis. The aim of this study was to prospectively compare the diagnostic accuracy of the American Association for the Study of Liver Diseases (AASLD) and the 2014 version of Liver Imaging Reporting and Data System (LI-RADS®) criteria for the non-invasive diagnosis of small HCC, and to evaluate the diagnostic value of ancillary features used in the LI-RADS criteria. Between April 2009 and April 2012, patients with cirrhosis and one to three 10–30 mm nodules were enrolled and underwent computed tomography (CT) and magnetic resonance (MR) imaging. The diagnostic accuracy of both the AASLD and the LI-RADS criteria were determined based on their sensitivity, specificity, positive (PPV) and negative predictive values (NPV). A total of 595 nodules were included (559 [341 HCC, 61%] with MR imaging and 529 [332 HCC, 63%] with CT). Overall, no (0%) LR-1 and LR-2, 44 (33%) and 47 (41%) LR-3, 50 (53%) and 54 (55%) LR-4, 244 (94%) and 222 (91%) LR-5 and 4 (67%) and 9 (82%) LR-5V were HCC on MR imaging and CT, respectively. The sensitivity, specificity, PPV/NPV of the AASLD score was 72.5%, 87.6%, 90.2%, and 66.9% for MR imaging, and 71.4%, 77.7%, 84.3%, 61.7% for CT, respectively. For the combination of LR-5V and LR-5 nodules these measures were 72.5%, 89.9%, 91.9% and 67.5% on MRI and 66.9%, 88.3%, 90.9% and 63.3% on CT, respectively. For the combination of LR-5V, LR-5 and LR-4 nodules they were 87.1%, 69.1%, 81.6% and 77.3% on MRI and 85.8%, 66%, 81% on 73.5% on CT, respectively. The 2014 version of the LI-RADS is no more accurate than the AASLD score for the non-invasive diagnosis of small HCC in high-risk patients, but it provides important and complementary information on the probability of having HCC in high-risk patients, allowing for possible changes in the management of these patients. Studies have shown that LR-5 is highly specific for the diagnosis of small-sized HCC.11,12Results are more controversial with LR-4 because some studies11,12 have shown a similar specificity with LR-5, while others have found a much lower specificity.13Furthermore, up to 17%–29% of LR-4 diagnoses are downgraded into lower Li-RADS categories.14,15How radiologists should use LI-RADS instead of the American Association for the Study of Liver Diseases (AASLD) criteria for the diagnosis of HCC in high-risk populations has still not been determined.Although a few studies have assessed or compared the diagnostic accuracy and reproducibility of LI-RADS to other standardized systems for the diagnosis of HCC by imaging,9,12,13,16,17 data comparing LI-RADS to AASLD criteria are scarce.11These studies are mainly single center and retrospective, and CT and MR imaging have rarely been compared.
γδ T cells comprise a substantial proportion of tissue-associated lymphocytes. However, our current understanding of human γδ T cells is primarily based on peripheral blood subsets, while the immunobiology of tissue-associated subsets remains largely unclear. Therefore, we aimed to elucidate the T cell receptor (TCR) diversity, immunophenotype and function of γδ T cells in the human liver. We characterised the TCR repertoire, immunophenotype and function of human liver infiltrating γδ T cells, by TCR sequencing analysis, flow cytometry, in situ hybridisation and immunohistochemistry. We focussed on the predominant tissue-associated Vδ2− γδ subset, which is implicated in liver immunopathology. Intrahepatic Vδ2− γδ T cells were highly clonally focussed, with single expanded clonotypes featuring complex, private TCR rearrangements frequently dominating the compartment. Such T cells were predominantly CD27lo/− effector lymphocytes, whereas naïve CD27hi, TCR-diverse populations present in matched blood were generally absent in the liver. Furthermore, while a CD45RAhi Vδ2− γδ effector subset present in both liver and peripheral blood contained overlapping TCR clonotypes, the liver Vδ2− γδ T cell pool also included a phenotypically distinct CD45RAlo effector compartment that was enriched for expression of the tissue tropism marker CD69, the hepatic homing chemokine receptors CXCR3 and CXCR6, and liver-restricted TCR clonotypes, suggestive of intrahepatic tissue residency. Liver infiltrating Vδ2− γδ cells were capable of polyfunctional cytokine secretion, and unlike peripheral blood subsets, were responsive to both TCR and innate stimuli. These findings suggest that the ability of Vδ2− γδ T cells to undergo clonotypic expansion and differentiation is crucial in permitting access to solid tissues, such as the liver, which results in functionally distinct peripheral and liver-resident memory γδ T cell subsets. They also highlight the inherent functional plasticity within the Vδ2− γδ T cell compartment and provide information that could be used for the design of cellular therapies that suppress liver inflammation or combat liver cancer. γδ T cells are unconventional lymphocytes enriched in solid tissues, where they are thought to play critical roles in immunosurveillance.1Studies of mouse tissue-associated γδ subsets suggest γδ T cell function can be predominantly innate-like, involving semi-invariant T cell subsets that enable fast response kinetics without a requirement for clonal selection and differentiation.2–5This role may allow for rapid ‘lymphoid stress surveillance’, limiting damage to host tissues in the face of microbial or non-microbial challenges, prior to full activation of adaptive immunity.4,6As such, γδ T cells may critically complement the contributions of tissue-resident αβ subsets, which provide an augmented adaptive response to infections re-encountered at body surfaces,7 potentially explaining the retention of γδ T cells alongside the αβ T cell and B cell lineage over 450 million years of vertebrate evolution.8
γδ T cells comprise a substantial proportion of tissue-associated lymphocytes. However, our current understanding of human γδ T cells is primarily based on peripheral blood subsets, while the immunobiology of tissue-associated subsets remains largely unclear. Therefore, we aimed to elucidate the T cell receptor (TCR) diversity, immunophenotype and function of γδ T cells in the human liver. We characterised the TCR repertoire, immunophenotype and function of human liver infiltrating γδ T cells, by TCR sequencing analysis, flow cytometry, in situ hybridisation and immunohistochemistry. We focussed on the predominant tissue-associated Vδ2− γδ subset, which is implicated in liver immunopathology. Intrahepatic Vδ2− γδ T cells were highly clonally focussed, with single expanded clonotypes featuring complex, private TCR rearrangements frequently dominating the compartment. Such T cells were predominantly CD27lo/− effector lymphocytes, whereas naïve CD27hi, TCR-diverse populations present in matched blood were generally absent in the liver. Furthermore, while a CD45RAhi Vδ2− γδ effector subset present in both liver and peripheral blood contained overlapping TCR clonotypes, the liver Vδ2− γδ T cell pool also included a phenotypically distinct CD45RAlo effector compartment that was enriched for expression of the tissue tropism marker CD69, the hepatic homing chemokine receptors CXCR3 and CXCR6, and liver-restricted TCR clonotypes, suggestive of intrahepatic tissue residency. Liver infiltrating Vδ2− γδ cells were capable of polyfunctional cytokine secretion, and unlike peripheral blood subsets, were responsive to both TCR and innate stimuli. These findings suggest that the ability of Vδ2− γδ T cells to undergo clonotypic expansion and differentiation is crucial in permitting access to solid tissues, such as the liver, which results in functionally distinct peripheral and liver-resident memory γδ T cell subsets. They also highlight the inherent functional plasticity within the Vδ2− γδ T cell compartment and provide information that could be used for the design of cellular therapies that suppress liver inflammation or combat liver cancer. In contrast, the paradigms underlying human γδ T cell immunobiology are far from clear.In humans, the peripheral blood is dominated by the Vδ2+/Vγ9+ T cell subset, polyclonally activated by bacterial9 and endogenous phospho-antigens,10 arguably conforming to an innate-like paradigm.11In contrast, human solid tissues are enriched for Vδ2− γδ T cells, of which the Vδ1+ subset is the most prevalent.It is far less clear if this dominant human tissue-associated subset also adopts an innate-like biology.Indeed, Vδ2− T cells have been linked to recognition of a diverse range of ligands including to date Endothelial Protein C Receptor,12 CD1 molecules,13 Annexin-A2,14 and even phycoerythrin.15Moreover, recent data have provided strong evidence that Vδ1+ cells display an unconventional adaptive biology, undergoing clonal selection and differentiation from a naïve T cell receptor (TCR)-diverse precursor pool,16 with viral infection one trigger driving expansion.17However, such studies have focussed on the subset of Vδ2− γδ T cells that are retained in peripheral blood.To date, the immunobiology of human tissue-associated γδ T cells remains relatively unstudied, despite the Vδ2− T cell subset representing a considerable proportion of the total T cell infiltration in many human solid tissues, including gut,2 lung18 and liver.19
γδ T cells comprise a substantial proportion of tissue-associated lymphocytes. However, our current understanding of human γδ T cells is primarily based on peripheral blood subsets, while the immunobiology of tissue-associated subsets remains largely unclear. Therefore, we aimed to elucidate the T cell receptor (TCR) diversity, immunophenotype and function of γδ T cells in the human liver. We characterised the TCR repertoire, immunophenotype and function of human liver infiltrating γδ T cells, by TCR sequencing analysis, flow cytometry, in situ hybridisation and immunohistochemistry. We focussed on the predominant tissue-associated Vδ2− γδ subset, which is implicated in liver immunopathology. Intrahepatic Vδ2− γδ T cells were highly clonally focussed, with single expanded clonotypes featuring complex, private TCR rearrangements frequently dominating the compartment. Such T cells were predominantly CD27lo/− effector lymphocytes, whereas naïve CD27hi, TCR-diverse populations present in matched blood were generally absent in the liver. Furthermore, while a CD45RAhi Vδ2− γδ effector subset present in both liver and peripheral blood contained overlapping TCR clonotypes, the liver Vδ2− γδ T cell pool also included a phenotypically distinct CD45RAlo effector compartment that was enriched for expression of the tissue tropism marker CD69, the hepatic homing chemokine receptors CXCR3 and CXCR6, and liver-restricted TCR clonotypes, suggestive of intrahepatic tissue residency. Liver infiltrating Vδ2− γδ cells were capable of polyfunctional cytokine secretion, and unlike peripheral blood subsets, were responsive to both TCR and innate stimuli. These findings suggest that the ability of Vδ2− γδ T cells to undergo clonotypic expansion and differentiation is crucial in permitting access to solid tissues, such as the liver, which results in functionally distinct peripheral and liver-resident memory γδ T cell subsets. They also highlight the inherent functional plasticity within the Vδ2− γδ T cell compartment and provide information that could be used for the design of cellular therapies that suppress liver inflammation or combat liver cancer. To shed light on the function of tissue-associated γδ T cells and how this relates to peripheral subsets, we characterised human intrahepatic Vδ2− T cells.The liver is a site of considerable blood flow, receiving 75% of the total blood in the body every 2 h, with a third of this originating directly from the antigen-rich gut via the portal vein.In addition to providing a generally immunosuppressive microenvironment to facilitate tolerization of T cells toward non-pathogenic antigens present in the portal blood flow, the liver is also home to a large population of innate lymphoid cells, including natural killer (NK) cells, invariant natural killer T (iNKT) cells, mucosal associated invariant T (MAIT) cells20 and γδ T cells,19 in addition to CD8+ cytotoxic T cells.21This enrichment is believed to balance the need for tolerization with a requirement for rapid identification and elimination of potentially harmful pathogenic entities, for example via pathogen associated molecular pattern receptors and semi-invariant T cell populations.22To shed light on the immunobiology of γδ T cells in this context we exploited next generation sequencing (NGS) approaches, allowing us to probe the TCR repertoire, in parallel with immunophenotype, and function.
It is currently unclear which antiviral agent, entecavir (ETV) or tenofovir disoproxil fumarate (TDF), is superior for improving prognosis in patients with chronic hepatitis B (CHB). Here, we assessed the ability of these 2 antivirals to prevent liver-disease progression in treatment-naïve patients with CHB. From 2012 to 2014, treatment-naïve patients with CHB who received ETV or TDF as a first-line antiviral agent were recruited from 4 academic teaching hospitals. Patients with decompensated cirrhosis or hepatocellular carcinoma (HCC) at enrollment were excluded. Cumulative probabilities of HCC and death or orthotopic liver transplant (OLT) were assessed. In total, 2,897 patients (1,484 and 1,413 in the ETV and TDF groups, respectively) were recruited. The annual HCC incidence was not statistically different between the ETV and TDF groups (1.92 vs. 1.69 per 100 person-years [PY], respectively; adjusted hazard ratio [HR] 0.975 [p = 0.852] by multivariate analysis). Propensity score (PS)-matched and inverse probability of treatment weighting (ITPW) analyses yielded similar patterns of results (HR 1.021 [p = 0.884] and 0.998 [p = 0.988], respectively). The annual incidence of death or OLT was not statistically different between the ETV and TDF groups (0.52 vs. 0.53 per 100 PY, respectively; adjusted HR 1.202 [p = 0.451]). PS-matched and ITPW analyses yielded similar patterns of results (HR 1.248 [p = 0.385] and 1.239 [p = 0.360], respectively). These findings were consistently reproduced in patients with compensated cirrhosis (all p >0.05). The overall prognosis in terms of HCC and death or OLT was not statistically different between the ETV and TDF groups. Further studies are needed to validate our results. Chronic hepatitis B (CHB) is the most common chronic viral infection worldwide, affecting approximately 350 million people.1Because persistently high hepatitis B virus (HBV) replication is associated with an increased risk of compensated cirrhosis and hepatocellular carcinoma (HCC),2,3 replication-suppressing antiviral therapy is administered to patients with CHB to prevent liver-disease progression.4As a matter of fact, oral antiviral agents, particularly entecavir (ETV), reduce the risk of long-term complications such as cirrhosis and HCC, ultimately improving survival compared to controls.5,6Nevertheless, because HBV is rarely eradicated from hepatocytes, most patients with CHB require long-term antiviral therapy.7,8
It is currently unclear which antiviral agent, entecavir (ETV) or tenofovir disoproxil fumarate (TDF), is superior for improving prognosis in patients with chronic hepatitis B (CHB). Here, we assessed the ability of these 2 antivirals to prevent liver-disease progression in treatment-naïve patients with CHB. From 2012 to 2014, treatment-naïve patients with CHB who received ETV or TDF as a first-line antiviral agent were recruited from 4 academic teaching hospitals. Patients with decompensated cirrhosis or hepatocellular carcinoma (HCC) at enrollment were excluded. Cumulative probabilities of HCC and death or orthotopic liver transplant (OLT) were assessed. In total, 2,897 patients (1,484 and 1,413 in the ETV and TDF groups, respectively) were recruited. The annual HCC incidence was not statistically different between the ETV and TDF groups (1.92 vs. 1.69 per 100 person-years [PY], respectively; adjusted hazard ratio [HR] 0.975 [p = 0.852] by multivariate analysis). Propensity score (PS)-matched and inverse probability of treatment weighting (ITPW) analyses yielded similar patterns of results (HR 1.021 [p = 0.884] and 0.998 [p = 0.988], respectively). The annual incidence of death or OLT was not statistically different between the ETV and TDF groups (0.52 vs. 0.53 per 100 PY, respectively; adjusted HR 1.202 [p = 0.451]). PS-matched and ITPW analyses yielded similar patterns of results (HR 1.248 [p = 0.385] and 1.239 [p = 0.360], respectively). These findings were consistently reproduced in patients with compensated cirrhosis (all p >0.05). The overall prognosis in terms of HCC and death or OLT was not statistically different between the ETV and TDF groups. Further studies are needed to validate our results. Entecavir and tenofovir disoproxil fumarate (TDF) are potent nucleos(t)ide analogues (NUCs) with a high genetic barrier to resistance.4,9Because these 2 antivirals have similar short to intermediate-term clinical efficacy (including virologic, biochemical, serologic, and histologic responses), they are recommended by international practice guidelines as first-line antiviral agents for CHB, together with tenofovir alafenamide (TAF).4,9,10Furthermore, ETV and TDF have similar efficacy for preventing liver-disease progression,11–14 consistent with their similar antiviral effects in treatment-naïve patients with CHB.15–18Furthermore, effective rescue regimens may offset the potential hazard of virologic breakthrough or genotypic resistance in a very small proportion of patients treated with ETV.19,20
It is currently unclear which antiviral agent, entecavir (ETV) or tenofovir disoproxil fumarate (TDF), is superior for improving prognosis in patients with chronic hepatitis B (CHB). Here, we assessed the ability of these 2 antivirals to prevent liver-disease progression in treatment-naïve patients with CHB. From 2012 to 2014, treatment-naïve patients with CHB who received ETV or TDF as a first-line antiviral agent were recruited from 4 academic teaching hospitals. Patients with decompensated cirrhosis or hepatocellular carcinoma (HCC) at enrollment were excluded. Cumulative probabilities of HCC and death or orthotopic liver transplant (OLT) were assessed. In total, 2,897 patients (1,484 and 1,413 in the ETV and TDF groups, respectively) were recruited. The annual HCC incidence was not statistically different between the ETV and TDF groups (1.92 vs. 1.69 per 100 person-years [PY], respectively; adjusted hazard ratio [HR] 0.975 [p = 0.852] by multivariate analysis). Propensity score (PS)-matched and inverse probability of treatment weighting (ITPW) analyses yielded similar patterns of results (HR 1.021 [p = 0.884] and 0.998 [p = 0.988], respectively). The annual incidence of death or OLT was not statistically different between the ETV and TDF groups (0.52 vs. 0.53 per 100 PY, respectively; adjusted HR 1.202 [p = 0.451]). PS-matched and ITPW analyses yielded similar patterns of results (HR 1.248 [p = 0.385] and 1.239 [p = 0.360], respectively). These findings were consistently reproduced in patients with compensated cirrhosis (all p >0.05). The overall prognosis in terms of HCC and death or OLT was not statistically different between the ETV and TDF groups. Further studies are needed to validate our results. Choi et al.,21 using the database of the National Health Insurance Service (NHIS) of South Korea, reported that TDF is associated with a significantly lower risk of HCC (hazard ratio [HR] 0.61) and all-cause mortality or orthotopic liver transplant (OLT) (HR 0.77) compared to ETV in multivariate analysis.However, from a hospital-based validation cohort, TDF reduced the risk of HCC with a marginal significance (p = 0.04) and did not prevent all-cause mortality or OLT (p = 0.44).
Interleukin (IL)-1-type cytokines including IL-1α, IL-1β and interleukin-1 receptor antagonist (IL-1Ra) are among the most potent molecules of the innate immune system and exert biological activities through the ubiquitously expressed interleukin-1 receptor type 1 (IL-1R1). The role of IL-1R1 in hepatocytes during acute liver failure (ALF) remains undetermined. The role of IL-1R1 during ALF was investigated using a novel transgenic mouse model exhibiting deletion of all signaling-capable IL-1R isoforms in hepatocytes (Il1r1Hep−/−). ALF induced by D-galactosamine (D-GalN) and lipopolysaccharide (LPS) was significantly attenuated in Il1r1Hep−/− mice leading to reduced mortality. Conditional deletion of Il1r1 decreased activation of injurious c-Jun N-terminal kinases (JNK)/c-Jun signaling, activated nuclear factor-kappa B (NF-κB) p65, inhibited extracellular signal-regulated kinase (ERK) and prevented caspase 3-mediated apoptosis. Moreover, Il1r1Hep−/− mice exhibited reduced local and systemic inflammatory cytokine and chemokine levels, especially TNF-α, IL-1α/β, IL-6, CC-chemokine ligand 2 (CCL2), C-X-C motif ligand 1 (CXCL-1) and CXCL-2, and a reduced neutrophil recruitment into the hepatic tissue in response to injury. NLRP3 inflammasome expression and caspase 1 activation were suppressed in the absence of the hepatocellular IL-1R1. Inhibition of IL-1R1 using IL-1ra (anakinra) attenuated the severity of liver injury, while IL-1α administration exaggerated it. These effects were lost ex vivo and at later time points, supporting a role of IL-1R1 in inflammatory signal amplification during acute liver injury. IL-1R1 in hepatocytes plays a pivotal role in an IL-1-driven auto-amplification of cell death and inflammation in the onset of ALF. Acute liver failure (ALF) and acute-on-chronic liver failure (ACLF) both lead to a high mortality rate, with limited treatment options available.An altered inflammatory response has been implicated in their pathophysiology.1,2Despite obvious differences in their etiology, both conditions exhibit activation of immune mechanisms that augment inflammation after the initial insult and drive a lethal loss of hepatic function from increased cell death.In order to improve the poor prognosis of patients with ALF and ACLF, mechanistic studies that address the underlying pathomechanisms are urgently required with the aim of identifying potential, selective immune-modulatory therapies.
Interleukin (IL)-1-type cytokines including IL-1α, IL-1β and interleukin-1 receptor antagonist (IL-1Ra) are among the most potent molecules of the innate immune system and exert biological activities through the ubiquitously expressed interleukin-1 receptor type 1 (IL-1R1). The role of IL-1R1 in hepatocytes during acute liver failure (ALF) remains undetermined. The role of IL-1R1 during ALF was investigated using a novel transgenic mouse model exhibiting deletion of all signaling-capable IL-1R isoforms in hepatocytes (Il1r1Hep−/−). ALF induced by D-galactosamine (D-GalN) and lipopolysaccharide (LPS) was significantly attenuated in Il1r1Hep−/− mice leading to reduced mortality. Conditional deletion of Il1r1 decreased activation of injurious c-Jun N-terminal kinases (JNK)/c-Jun signaling, activated nuclear factor-kappa B (NF-κB) p65, inhibited extracellular signal-regulated kinase (ERK) and prevented caspase 3-mediated apoptosis. Moreover, Il1r1Hep−/− mice exhibited reduced local and systemic inflammatory cytokine and chemokine levels, especially TNF-α, IL-1α/β, IL-6, CC-chemokine ligand 2 (CCL2), C-X-C motif ligand 1 (CXCL-1) and CXCL-2, and a reduced neutrophil recruitment into the hepatic tissue in response to injury. NLRP3 inflammasome expression and caspase 1 activation were suppressed in the absence of the hepatocellular IL-1R1. Inhibition of IL-1R1 using IL-1ra (anakinra) attenuated the severity of liver injury, while IL-1α administration exaggerated it. These effects were lost ex vivo and at later time points, supporting a role of IL-1R1 in inflammatory signal amplification during acute liver injury. IL-1R1 in hepatocytes plays a pivotal role in an IL-1-driven auto-amplification of cell death and inflammation in the onset of ALF. The interleukin (IL)-1 family members IL-1α and IL-1β are key pro-inflammatory mediators and signal through the cell surface interleukin-1 receptor type 1 (IL-1R1).3Upon binding of IL-1α/β, IL-1R1 recruits, in complex with its receptor accessory protein (IL-1RacP), the cytosolic adapter molecule myeloid differentiation primary response gene 88 (MyD88) and through an intracellular signaling complex IL-1R-associated kinases (IRAK) and tumor necrosis factor receptor-associated factor 6 (TRAF6).4These in turn activate transforming growth factor beta-activated kinase 1 (TAK1) and mitogen-activated protein kinases (MAPK) that are involved in cellular survival, e.g. c-Jun terminal kinases (JNK) and extracellular signal-regulated kinase 1/2 (ERK), as well as downstream transcriptions factors that contribute to hepatic inflammation.5
Recently, the PAGE-B score and Toronto HCC risk index (THRI) have been developed to predict the risk of hepatocellular carcinoma (HCC) in Caucasian patients with chronic hepatitis B (CHB). We aimed to validate PAGE-B scores and THRI in Asian patients with CHB and suggested modified PAGE-B scores to improve the predictive performance. From 2007 to 2017, we examined 3,001 Asian patients with CHB receiving entecavir/tenofovir therapy. We assessed the performances of PAGE-B, THRI, CU-HCC, GAG-HCC, and REACH-B for HCC development. A modified PAGE-B score (mPAGE-B) was developed (derivation set, n = 2,001) based on multivariable Cox models. Bootstrap for internal validation and external validation (validation set, n = 1,000) were performed. The five-year cumulative HCC incidence rates were 6.6% and 7.2% in the derivation and validation datasets after entecavir/tenofovir onset. In the derivation dataset, age, gender, serum albumin levels, and platelet counts were independently associated with HCC. The mPAGE-B score was developed based on age, gender, platelet counts, and serum albumin levels (time-dependent area under receiver operating characteristic curves [AUROC] = 0.82). In the validation set, the PAGE-B and THRI showed similar AUROCs to CU-HCC, GAG-HCC, and REACH-B at five years (0.72 and 0.73 vs. 0.70, 0.71, and 0.61 respectively; all p >0.05 except REACH-B), whereas the AUROC of mPAGE-B at five years was 0.82, significantly higher than the five other models (all p <0.01). HCC incidence rates after initiation of entecavir/tenofovir therapy in patients with CHB were significantly decreased in all risk groups in long-term follow-up periods. Although PAGE-B and THRI are applicable in Asian patients with CHB receiving entecavir/tenofovir therapy, mPAGE-B scores including additional serum albumin levels showed better predictive performance than the PAGE-B score. Approximately 240 million people are diagnosed with chronic hepatitis B (CHB) worldwide, and the highest prevalence is found in Africa and Asia.1Early diagnosis of hepatocellular carcinoma (HCC) improves patients’ prognosis through timely intervention,2 and it is therefore important to stratify the risk of HCC in patients with CHB.
Recently, the PAGE-B score and Toronto HCC risk index (THRI) have been developed to predict the risk of hepatocellular carcinoma (HCC) in Caucasian patients with chronic hepatitis B (CHB). We aimed to validate PAGE-B scores and THRI in Asian patients with CHB and suggested modified PAGE-B scores to improve the predictive performance. From 2007 to 2017, we examined 3,001 Asian patients with CHB receiving entecavir/tenofovir therapy. We assessed the performances of PAGE-B, THRI, CU-HCC, GAG-HCC, and REACH-B for HCC development. A modified PAGE-B score (mPAGE-B) was developed (derivation set, n = 2,001) based on multivariable Cox models. Bootstrap for internal validation and external validation (validation set, n = 1,000) were performed. The five-year cumulative HCC incidence rates were 6.6% and 7.2% in the derivation and validation datasets after entecavir/tenofovir onset. In the derivation dataset, age, gender, serum albumin levels, and platelet counts were independently associated with HCC. The mPAGE-B score was developed based on age, gender, platelet counts, and serum albumin levels (time-dependent area under receiver operating characteristic curves [AUROC] = 0.82). In the validation set, the PAGE-B and THRI showed similar AUROCs to CU-HCC, GAG-HCC, and REACH-B at five years (0.72 and 0.73 vs. 0.70, 0.71, and 0.61 respectively; all p >0.05 except REACH-B), whereas the AUROC of mPAGE-B at five years was 0.82, significantly higher than the five other models (all p <0.01). HCC incidence rates after initiation of entecavir/tenofovir therapy in patients with CHB were significantly decreased in all risk groups in long-term follow-up periods. Although PAGE-B and THRI are applicable in Asian patients with CHB receiving entecavir/tenofovir therapy, mPAGE-B scores including additional serum albumin levels showed better predictive performance than the PAGE-B score. To date, three conventional models have been used for the prediction of HCC development in patients with CHB, including the modified guide with age, gender, HBV DNA, and cirrhosis-HCC (GAG-HCC), the Chinese University-HCC (CU-HCC), and the risk estimation for hepatocellular carcinoma in chronic hepatitis B (REACH-B).3The proportions of patients with CHB treated with antiviral therapy during the study periods for modified GAG-HCC, CU-HCC, and REACH-B models were 15.1%, 0%, and 0%, respectively, and all patients were treatment-naïve.3,4Furthermore, the patients enrolled in the study of the REACH-B model did not have cirrhosis.Given that HCC risk can be changed in patients with CHB treated with nucleos(t)ide analog (NA) antiviral therapy compared with untreated patients,5 the accuracy of the three scores for the prediction of HCC development may be limited in patients with CHB treated with antiviral therapy.
Recently, the PAGE-B score and Toronto HCC risk index (THRI) have been developed to predict the risk of hepatocellular carcinoma (HCC) in Caucasian patients with chronic hepatitis B (CHB). We aimed to validate PAGE-B scores and THRI in Asian patients with CHB and suggested modified PAGE-B scores to improve the predictive performance. From 2007 to 2017, we examined 3,001 Asian patients with CHB receiving entecavir/tenofovir therapy. We assessed the performances of PAGE-B, THRI, CU-HCC, GAG-HCC, and REACH-B for HCC development. A modified PAGE-B score (mPAGE-B) was developed (derivation set, n = 2,001) based on multivariable Cox models. Bootstrap for internal validation and external validation (validation set, n = 1,000) were performed. The five-year cumulative HCC incidence rates were 6.6% and 7.2% in the derivation and validation datasets after entecavir/tenofovir onset. In the derivation dataset, age, gender, serum albumin levels, and platelet counts were independently associated with HCC. The mPAGE-B score was developed based on age, gender, platelet counts, and serum albumin levels (time-dependent area under receiver operating characteristic curves [AUROC] = 0.82). In the validation set, the PAGE-B and THRI showed similar AUROCs to CU-HCC, GAG-HCC, and REACH-B at five years (0.72 and 0.73 vs. 0.70, 0.71, and 0.61 respectively; all p >0.05 except REACH-B), whereas the AUROC of mPAGE-B at five years was 0.82, significantly higher than the five other models (all p <0.01). HCC incidence rates after initiation of entecavir/tenofovir therapy in patients with CHB were significantly decreased in all risk groups in long-term follow-up periods. Although PAGE-B and THRI are applicable in Asian patients with CHB receiving entecavir/tenofovir therapy, mPAGE-B scores including additional serum albumin levels showed better predictive performance than the PAGE-B score. Recently, the PAGE-B score was developed for Caucasian patients with CHB treated with antiviral therapies such as entecavir (ETV) or tenofovir (TDF).The PAGE-B score showed an excellent concordance index value of 0.81 for the prediction of HCC development by five years in the original study.6In addition, Toronto HCC risk index (THRI) was also introduced to predict HCC development in patients with cirrhosis according to various etiologies including CHB.7Although the study suggested using THRI for patients with CHB regardless of potent antiviral therapy in Western countries, it also showed a good concordance index value of 0.77 for the prediction of HCC development.7It is necessary to validate the two scores in Asian patients with CHB receiving potent antiviral therapy.
Recently, the PAGE-B score and Toronto HCC risk index (THRI) have been developed to predict the risk of hepatocellular carcinoma (HCC) in Caucasian patients with chronic hepatitis B (CHB). We aimed to validate PAGE-B scores and THRI in Asian patients with CHB and suggested modified PAGE-B scores to improve the predictive performance. From 2007 to 2017, we examined 3,001 Asian patients with CHB receiving entecavir/tenofovir therapy. We assessed the performances of PAGE-B, THRI, CU-HCC, GAG-HCC, and REACH-B for HCC development. A modified PAGE-B score (mPAGE-B) was developed (derivation set, n = 2,001) based on multivariable Cox models. Bootstrap for internal validation and external validation (validation set, n = 1,000) were performed. The five-year cumulative HCC incidence rates were 6.6% and 7.2% in the derivation and validation datasets after entecavir/tenofovir onset. In the derivation dataset, age, gender, serum albumin levels, and platelet counts were independently associated with HCC. The mPAGE-B score was developed based on age, gender, platelet counts, and serum albumin levels (time-dependent area under receiver operating characteristic curves [AUROC] = 0.82). In the validation set, the PAGE-B and THRI showed similar AUROCs to CU-HCC, GAG-HCC, and REACH-B at five years (0.72 and 0.73 vs. 0.70, 0.71, and 0.61 respectively; all p >0.05 except REACH-B), whereas the AUROC of mPAGE-B at five years was 0.82, significantly higher than the five other models (all p <0.01). HCC incidence rates after initiation of entecavir/tenofovir therapy in patients with CHB were significantly decreased in all risk groups in long-term follow-up periods. Although PAGE-B and THRI are applicable in Asian patients with CHB receiving entecavir/tenofovir therapy, mPAGE-B scores including additional serum albumin levels showed better predictive performance than the PAGE-B score. Of note, in the initial process of developing the PAGE-B score and THRI which in common were composed of age, gender, and platelet counts in the original studies, the impact of some variables on the development of HCC and interaction with each variable, such as serum albumin, total bilirubin, creatinine levels, aspartate/alanine aminotransferase (AST/ALT), and international normalized ratio for prothrombin time (INR) was not well evaluated in univariable and multivariable analyses.6,7Among the various serum variables, previous studies have shown that serum bilirubin or albumin levels were independently associated with HCC development in patients with CHB.8–12
Biliary atresia (BA) results from a neonatal inflammatory and fibrosing obstruction of bile ducts of unknown etiology. Although the innate immune system has been linked to the virally induced mechanism of disease, the role of inflammasome-mediated epithelial injury remains largely undefined. Here, we hypothesized that disruption of the inflammasome suppresses the neonatal proinflammatory response and prevents experimental BA. We determined the expression of key inflammasome-related genes in livers from infants at diagnosis of BA and in extrahepatic bile ducts (EHBDs) of neonatal mice after infection with rotavirus (RRV) immediately after birth. Then, we determined the impact of the wholesale inactivation of the genes encoding IL-1R1 (Il1r1−/−), NLRP3 (Nlrp3−/−) or caspase-1 (Casp1−/−) on epithelial injury and bile duct obstruction. IL1R1, NLRP3 and CASP1 mRNA increased significantly in human livers at the time of diagnosis, and in EHBDs of RRV-infected mice. In Il1r1−/− mice, the epithelial injury of EHBDs induced by RRV was suppressed, with dendritic cells unable to activate natural killer cells. A similar protection was observed in Nlrp3−/− mice, with decreased injury and inflammation of livers and EHBDs. Long-term survival was also improved. In contrast, the inactivation of the Casp1 gene had no impact on tissue injury, and all mice died. Tissue analyses in Il1r1−/− and Nlrp3−/− mice showed decreased populations of dendritic cells and natural killer cells and suppressed expression of type-1 cytokines and chemokines. Genes of the inflammasome are overexpressed at diagnosis of BA in humans and in the BA mouse model. In the experimental model, the targeted loss of IL-1R1 or NLRP3, but not of caspase-1, protected neonatal mice against RRV-induced bile duct obstruction. Biliary atresia (BA) results from a rapidly progressing inflammation and obstruction of extrahepatic bile ducts (EHBDs) in early infancy and is the most common indication for pediatric liver transplantation.1–3The etiology of BA includes environmental triggers in the genetically susceptible host,4,5 followed by an over-activation of the neonatal immune response in the liver and EHBD.6,7We and others previously reported several cellular (dendritic cells [DCs], natural killer [NK] cells and CD8+ T cells) and molecular (IL-8, IL-15, IFN-γ, TNFα) effectors of bile duct epithelial injury.8–16Despite this progress, very little is known about how molecular sensors and related circuits regulate the hepatobiliary injury and duct obstruction in BA.
Cure rates in response to retreatment with sofosbuvir/velpatasvir/voxilaprevir (SOF/VEL/VOX) are high, but this regimen has not been studied in patients with a history of poor adherence or treatment interruption, nor in patients with HIV/HCV coinfection. Herein, we aimed to assess the safety and efficacy of this combination in patients with genotype 1 HCV infection who had relapsed following combination direct-acting antiviral (DAA) therapy, regardless of HIV infection or previous treatment course. The RESOLVE study was a multicenter, open-label, phase IIb study investigating the safety, tolerability and efficacy of SOF/VEL/VOX in 77 patients with virologic rebound following combination DAA therapy. Efficacy was defined as HCV RNA below the lower limit of detection 12 weeks after the end of treatment (SVR12), while safety endpoints included the incidence of grade 3 and 4 adverse events (AEs) following treatment, and the proportion of patients who stopped treatment prematurely due to AEs. In an intent-to-treat analysis, 70/77 (90.9%, 95% CI 82.1–95.8%) patients achieved SVR12, including 14/17 (82.4%) HIV coinfected participants and 18/22 (81.8%) of those with previous non-completion of DAA therapy. In an analysis of all patients who completed 12 weeks of study medication, 70/71 patients (99%) achieved SVR12. One patient experienced a grade 3 AE, and 4 experienced a grade 4 AE, all unrelated to study participation. Reported AEs were similar in HIV-coinfected patients, and patients receiving dolutegravir-based antiretroviral treatment experienced no clinically significant increases in aminotransferases. Retreatment with 12 weeks of SOF/VEL/VOX was safe and effective in patients with relapsed HCV following initial combination DAA-based treatment. Treatment response was not affected by HIV coinfection or previous treatment course. Treatment of chronic HCV infection has changed dramatically in the past 5 years.Combinations of direct-acting antivirals (DAAs) can achieve high rates of sustained virologic response (SVR), synonymous with eradication or cure of HCV, of above 95% in clinical trials, regardless of stage of hepatic fibrosis, treatment history, or HCV genotype.1Recently, real world efficacy studies have confirmed high cure rates, with SVR rates of 90–95% outside of clinical trials.2–4The proportion of patients in these reports who do not achieve SVR is low, at less than 10%, when combining those who have virologic relapse, are lost to follow-up, or discontinue treatment due to adverse events (AEs).Nevertheless, given that there are an estimated 71 million people with chronic HCV worldwide,5 the number of patients who will require HCV retreatment is substantial and will increase as HCV treatment becomes more accessible.
Cure rates in response to retreatment with sofosbuvir/velpatasvir/voxilaprevir (SOF/VEL/VOX) are high, but this regimen has not been studied in patients with a history of poor adherence or treatment interruption, nor in patients with HIV/HCV coinfection. Herein, we aimed to assess the safety and efficacy of this combination in patients with genotype 1 HCV infection who had relapsed following combination direct-acting antiviral (DAA) therapy, regardless of HIV infection or previous treatment course. The RESOLVE study was a multicenter, open-label, phase IIb study investigating the safety, tolerability and efficacy of SOF/VEL/VOX in 77 patients with virologic rebound following combination DAA therapy. Efficacy was defined as HCV RNA below the lower limit of detection 12 weeks after the end of treatment (SVR12), while safety endpoints included the incidence of grade 3 and 4 adverse events (AEs) following treatment, and the proportion of patients who stopped treatment prematurely due to AEs. In an intent-to-treat analysis, 70/77 (90.9%, 95% CI 82.1–95.8%) patients achieved SVR12, including 14/17 (82.4%) HIV coinfected participants and 18/22 (81.8%) of those with previous non-completion of DAA therapy. In an analysis of all patients who completed 12 weeks of study medication, 70/71 patients (99%) achieved SVR12. One patient experienced a grade 3 AE, and 4 experienced a grade 4 AE, all unrelated to study participation. Reported AEs were similar in HIV-coinfected patients, and patients receiving dolutegravir-based antiretroviral treatment experienced no clinically significant increases in aminotransferases. Retreatment with 12 weeks of SOF/VEL/VOX was safe and effective in patients with relapsed HCV following initial combination DAA-based treatment. Treatment response was not affected by HIV coinfection or previous treatment course. The fixed-dose combination DAA regimen of sofosbuvir (SOF, a pangenotypic NS5B nucleotide polymerase inhibitor), velpatasvir (VEL, a pangenotypic NS5A inhibitor), and voxilaprevir (VOX, a pangenotypic NS3/4a protease inhibitor) has been shown to be highly effective in the treatment of DAA-experienced patients.6The drug was the first Food and Drug Administration (FDA)-approved treatment for chronic HCV in patients who had previously been treated with sofosbuvir or NS5A inhibitors.7,8However, there is no data regarding treatment outcomes in patients who discontinued their initial combination DAA-based regimen due to non-completion of therapy, either due to early discontinuation or poor adherence,6 or in those with concurrent chronic viral infections with HIV and/or hepatitis B virus (HBV) coinfection.6,8,9
Acute-on-chronic liver failure (ACLF), which develops in patients with cirrhosis, is characterized by intense systemic inflammation and organ failure(s). Because systemic inflammation is energetically expensive, its metabolic costs may result in organ dysfunction/failure. Therefore, we aimed to analyze blood metabolome in patients with cirrhosis, with and without ACLF. We performed untargeted metabolomics using liquid chromatography coupled to high-resolution mass spectrometry in serum from 650 patients with AD (acute decompensation of cirrhosis, without ACLF), 181 with ACLF, 43 with compensated cirrhosis, and 29 healthy subjects. Of the 137 annotated metabolites identified, 100 were increased in patients with ACLF of any grade, relative to those with AD, and 38 composed a distinctive blood metabolite fingerprint for ACLF. Among patients with ACLF, the intensity of the fingerprint increased across ACLF grades, and was similar in patients with kidney failure and in those without, indicating that the fingerprint reflected not only decreased kidney excretion but also altered cell metabolism. The higher the ACLF-associated fingerprint intensity, the higher plasma levels of inflammatory markers, tumor necrosis factor α, soluble CD206, and soluble CD163. ACLF was characterized by intense proteolysis and lipolysis; amino acid catabolism; extra-mitochondrial glucose metabolism through glycolysis, pentose phosphate, and D-glucuronate pathways; depressed mitochondrial ATP-producing fatty acid β-oxidation; and extra-mitochondrial amino acid metabolism giving rise to metabolites which are metabotoxins. In ACLF, intense systemic inflammation is associated with blood metabolite accumulation witnessing profound alterations in major metabolic pathways, in particular inhibition in mitochondrial energy production, which may contribute to the existence of organ failures. The results of the CANONIC study were used to redefine acute-on-chronic liver failure (ACLF) as a syndrome which develops in patients with cirrhosis and acute decompensation (AD) and is characterized by an intense systemic inflammation1,2-4 associated with different combinations of organ failures among the six major organ systems (liver, kidney, brain, coagulation, circulation, and respiration), giving rise to different clinical phenotypes1.ACLF is very frequent, affecting 30%-40% of hospitalized patients, and is associated with high short-term mortality rate (30% at 28 days)1.In most cases of ACLF, the activation of innate immune cells by pathogen-associated molecular patterns (PAMPs) are thought to play a major role in the induction of systemic inflammation.5However, the intrinsic mechanisms of ACLF at the tissue and cellular levels that contribute to the development and maintenance of organ failures are unknown.Understanding these mechanisms should not only result in progresses in the knowledge on pathophysiology of ACLF, but also could provide clues to the development of new biomarkers of organ dysfunction/failure and identification of targets for new therapies of organ failures which are urgently needed.
Acute-on-chronic liver failure (ACLF), which develops in patients with cirrhosis, is characterized by intense systemic inflammation and organ failure(s). Because systemic inflammation is energetically expensive, its metabolic costs may result in organ dysfunction/failure. Therefore, we aimed to analyze blood metabolome in patients with cirrhosis, with and without ACLF. We performed untargeted metabolomics using liquid chromatography coupled to high-resolution mass spectrometry in serum from 650 patients with AD (acute decompensation of cirrhosis, without ACLF), 181 with ACLF, 43 with compensated cirrhosis, and 29 healthy subjects. Of the 137 annotated metabolites identified, 100 were increased in patients with ACLF of any grade, relative to those with AD, and 38 composed a distinctive blood metabolite fingerprint for ACLF. Among patients with ACLF, the intensity of the fingerprint increased across ACLF grades, and was similar in patients with kidney failure and in those without, indicating that the fingerprint reflected not only decreased kidney excretion but also altered cell metabolism. The higher the ACLF-associated fingerprint intensity, the higher plasma levels of inflammatory markers, tumor necrosis factor α, soluble CD206, and soluble CD163. ACLF was characterized by intense proteolysis and lipolysis; amino acid catabolism; extra-mitochondrial glucose metabolism through glycolysis, pentose phosphate, and D-glucuronate pathways; depressed mitochondrial ATP-producing fatty acid β-oxidation; and extra-mitochondrial amino acid metabolism giving rise to metabolites which are metabotoxins. In ACLF, intense systemic inflammation is associated with blood metabolite accumulation witnessing profound alterations in major metabolic pathways, in particular inhibition in mitochondrial energy production, which may contribute to the existence of organ failures. PAMP-elicited systemic inflammation stimulates the hypothalamic-pituitary adrenal axis to produce cortisol, glucagon, and adrenaline, which, in turn, induce rapid and widespread catabolism including glycogenolysis (liver), proteolysis (mainly muscles) and lipolysis (adipose tissue).6-8This exuberant catabolic process results in the release of nutrients (glucose, amino acids and fatty acids [FAs], respectively) to fuel the energetically expensive immune responses (which include the production of a battery of inflammatory mediators, immune cell proliferation, adhesion and migration, respiratory burst, and production of acute-phase proteins).6,9For example, during sepsis (which is a paradigm of acute systemic inflammation), glucose is reallocated to activated innate immune cells where it is used to rapidly generate ATP through glycolysis, while mitochondrial oxidative phosphorylation (OxPhos) is suppressed6,7,9 (See glucose metabolism in naïve and activated innate immune cells, in Fig. S1A and S1B, respectively).Sepsis also induces metabolic changes in peripheral organs where it inhibits mitochondrial β-oxidation of FAs, resulting in decreased energy production and blood accumulation of FAs attached to molecules of carnitine8,10 (See FA metabolism under healthy conditions and during sepsis, in Fig. S2A and S2B, respectively).Therefore, in ACLF, systemic inflammation should lead to profound changes in most metabolic pathways.
Acute-on-chronic liver failure (ACLF), which develops in patients with cirrhosis, is characterized by intense systemic inflammation and organ failure(s). Because systemic inflammation is energetically expensive, its metabolic costs may result in organ dysfunction/failure. Therefore, we aimed to analyze blood metabolome in patients with cirrhosis, with and without ACLF. We performed untargeted metabolomics using liquid chromatography coupled to high-resolution mass spectrometry in serum from 650 patients with AD (acute decompensation of cirrhosis, without ACLF), 181 with ACLF, 43 with compensated cirrhosis, and 29 healthy subjects. Of the 137 annotated metabolites identified, 100 were increased in patients with ACLF of any grade, relative to those with AD, and 38 composed a distinctive blood metabolite fingerprint for ACLF. Among patients with ACLF, the intensity of the fingerprint increased across ACLF grades, and was similar in patients with kidney failure and in those without, indicating that the fingerprint reflected not only decreased kidney excretion but also altered cell metabolism. The higher the ACLF-associated fingerprint intensity, the higher plasma levels of inflammatory markers, tumor necrosis factor α, soluble CD206, and soluble CD163. ACLF was characterized by intense proteolysis and lipolysis; amino acid catabolism; extra-mitochondrial glucose metabolism through glycolysis, pentose phosphate, and D-glucuronate pathways; depressed mitochondrial ATP-producing fatty acid β-oxidation; and extra-mitochondrial amino acid metabolism giving rise to metabolites which are metabotoxins. In ACLF, intense systemic inflammation is associated with blood metabolite accumulation witnessing profound alterations in major metabolic pathways, in particular inhibition in mitochondrial energy production, which may contribute to the existence of organ failures. Metabolomics, which identifies and quantifies small-molecule metabolites (the metabolome), is the omics closest to clinical phenotypes.11Interestingly, metabolites not only reflect the metabolic activity of tissues but, because many metabolites have powerful biological activity on critical pathophysiological processes, they can also influence the clinical phenotype.11The best approach to characterize the metabolic changes of such complex syndromes as ACLF, is to perform high throughput untargeted (agnostic) studies in large series of patients.This approach, which has been applied to sepsis,10 has so far not been applied in the investigation of ACLF.
Noncytolytic curing of hepatitis B virus (HBV) infected hepatocytes by cytokines including type I interferons (IFNs) is of importance for resolving acute and chronic infection. However, as IFNs stimulate hundreds of genes those most relevant for HBV suppression remain largely unknown. Amongst them are the large Mx GTPases. Human MX1 (or MxA) is active against many RNA viruses while MX2 (or MxB) was recently found to restrict human immunodeficiency virus 1, hepatitis C virus, and herpesviruses. Here we investigated the anti-HBV activity of MX2. Potential anti-HBV activity of MX2 and functional variants was assessed in transfected and HBV infected hepatoma cells and primary human hepatocytes, employing multiple assays to determine HBV nucleic acids as well as their synthesis and decay. The specific roles of MX2 in IFN-α inhibition of HBV transcription and replication were addressed by MX2-specific shRNA interference (RNAi). MX2 alone as well as IFN-α substantially inhibited HBV replication, due to significant deceleration of the synthesis and slight acceleration of the turnover of viral RNA. RNAi knock-down of MX2 significantly reduced the inhibitory effects of IFN-α. Strikingly, MX2 inhibited HBV infection by reducing covalently closed circular DNA (cccDNA), most likely by indirectly impairing relaxed circular DNA to cccDNA conversion rather than destabilizing existing cccDNA. Various mutations affecting the GTPase activity and oligomerization status reduced MX2’s anti-HBV activity. MX2 is an important IFN-α inducible effector that decreases HBV RNA levels but can also potently inhibit HBV infection by indirectly impairing cccDNA formation. MX2 likely has the potential of therapeutic application aimed at curing HBV infection by eliminating cccDNA. More than 250 million people are chronically infected with hepatitis B virus (HBV) and at a greatly increased risk to develop terminal liver disease.Currently approved treatments for chronic hepatitis B (CHB) are limited to type-I interferons (IFNs) and five nucleos(t)ide analogs (NAs) [1].NAs inhibit the viral reverse transcriptase but rarely induce HBV surface antigen (HBsAg) seroconversion (an indicator of functional cure [2]), probably necessitating life-long treatment.IFNs, though applicable in only a fraction of patients, can lead to sustained suppression of HBV replication after a finite (usually 48 week) therapy [3, 4], possibly by their immunomodulatory activity, including restoration of innate responses [5, 6], and/or via one or more products of the many antiviral IFN stimulated genes (ISGs) [7].
Noncytolytic curing of hepatitis B virus (HBV) infected hepatocytes by cytokines including type I interferons (IFNs) is of importance for resolving acute and chronic infection. However, as IFNs stimulate hundreds of genes those most relevant for HBV suppression remain largely unknown. Amongst them are the large Mx GTPases. Human MX1 (or MxA) is active against many RNA viruses while MX2 (or MxB) was recently found to restrict human immunodeficiency virus 1, hepatitis C virus, and herpesviruses. Here we investigated the anti-HBV activity of MX2. Potential anti-HBV activity of MX2 and functional variants was assessed in transfected and HBV infected hepatoma cells and primary human hepatocytes, employing multiple assays to determine HBV nucleic acids as well as their synthesis and decay. The specific roles of MX2 in IFN-α inhibition of HBV transcription and replication were addressed by MX2-specific shRNA interference (RNAi). MX2 alone as well as IFN-α substantially inhibited HBV replication, due to significant deceleration of the synthesis and slight acceleration of the turnover of viral RNA. RNAi knock-down of MX2 significantly reduced the inhibitory effects of IFN-α. Strikingly, MX2 inhibited HBV infection by reducing covalently closed circular DNA (cccDNA), most likely by indirectly impairing relaxed circular DNA to cccDNA conversion rather than destabilizing existing cccDNA. Various mutations affecting the GTPase activity and oligomerization status reduced MX2’s anti-HBV activity. MX2 is an important IFN-α inducible effector that decreases HBV RNA levels but can also potently inhibit HBV infection by indirectly impairing cccDNA formation. MX2 likely has the potential of therapeutic application aimed at curing HBV infection by eliminating cccDNA. Following sodium taurocholate co-transporting polypeptide (NTCP) dependent HBV entry into host cells [8, 9] the nucleocapsid is transported to nucleus for conversion of the encapsidated relaxed circular DNA (rcDNA) genome into covalently closed circular DNA (cccDNA), probably mediated by the cellular DNA repair machinery [10].Using the episomal cccDNA minichromosome as template, host RNA polymerase II (RNAPII) transcribes four groups of viral RNAs (3.5 kb, 2.4 kb, 2.1 kb, and 0.7 kb).The 3.5 kb RNAs consist of the precore RNA (pcRNA) encoding the precursor protein of the secretory HBV e antigen (HBeAg) plus the pregenomic (pg) RNA, which serves as mRNA for core and polymerase proteins yet also as a template for new viral DNA genomes.The subgenomic RNAs encode the large (L), middle (M), and small (S) HBV surface proteins, collectively termed hepatitis B surface antigen (HBsAg), plus the regulatory HBx protein.Production of new DNA genomes is initiated by binding of the viral polymerase to the ε stem-loop near the 5’ end of pgRNA, which triggers recruitment of core dimers to form immature nucleocapsids.Meanwhile, polymerase bound to ε initiates HBV reverse transcription to synthesize minus-strand DNA.Subsequent plus-strand DNA synthesis allows for envelopment of the mature nucleocapsid by the surface proteins and egress from the cell; alternatively, a portion of the mature nucleocapsids may recycle their rcDNA to the nucleus for cccDNA amplification [11].
Noncytolytic curing of hepatitis B virus (HBV) infected hepatocytes by cytokines including type I interferons (IFNs) is of importance for resolving acute and chronic infection. However, as IFNs stimulate hundreds of genes those most relevant for HBV suppression remain largely unknown. Amongst them are the large Mx GTPases. Human MX1 (or MxA) is active against many RNA viruses while MX2 (or MxB) was recently found to restrict human immunodeficiency virus 1, hepatitis C virus, and herpesviruses. Here we investigated the anti-HBV activity of MX2. Potential anti-HBV activity of MX2 and functional variants was assessed in transfected and HBV infected hepatoma cells and primary human hepatocytes, employing multiple assays to determine HBV nucleic acids as well as their synthesis and decay. The specific roles of MX2 in IFN-α inhibition of HBV transcription and replication were addressed by MX2-specific shRNA interference (RNAi). MX2 alone as well as IFN-α substantially inhibited HBV replication, due to significant deceleration of the synthesis and slight acceleration of the turnover of viral RNA. RNAi knock-down of MX2 significantly reduced the inhibitory effects of IFN-α. Strikingly, MX2 inhibited HBV infection by reducing covalently closed circular DNA (cccDNA), most likely by indirectly impairing relaxed circular DNA to cccDNA conversion rather than destabilizing existing cccDNA. Various mutations affecting the GTPase activity and oligomerization status reduced MX2’s anti-HBV activity. MX2 is an important IFN-α inducible effector that decreases HBV RNA levels but can also potently inhibit HBV infection by indirectly impairing cccDNA formation. MX2 likely has the potential of therapeutic application aimed at curing HBV infection by eliminating cccDNA. Directing HBV transcription and replication cccDNA represents the viral persistence reservoir [12].Silencing transcription from cccDNA [13] or, ideally, cccDNA elimination are critical for a functional or true cure, respectively, of CHB.ISGs provide a valid source for such activities, and several of them have indeed been shown to possess anti-HBV activity.As yet, however, only apolipoprotein B mRNA editing enzyme catalytic polypeptide (APOBEC) 3A and 3B (A3A and A3B) reportedly reduced nuclear cccDNA [14-17].Notably though, RNAi mediated silencing of A3 proteins including A3B did not weaken the anti-HBV activity of IFN-α [18], implying an important role for other ISGs.
Noncytolytic curing of hepatitis B virus (HBV) infected hepatocytes by cytokines including type I interferons (IFNs) is of importance for resolving acute and chronic infection. However, as IFNs stimulate hundreds of genes those most relevant for HBV suppression remain largely unknown. Amongst them are the large Mx GTPases. Human MX1 (or MxA) is active against many RNA viruses while MX2 (or MxB) was recently found to restrict human immunodeficiency virus 1, hepatitis C virus, and herpesviruses. Here we investigated the anti-HBV activity of MX2. Potential anti-HBV activity of MX2 and functional variants was assessed in transfected and HBV infected hepatoma cells and primary human hepatocytes, employing multiple assays to determine HBV nucleic acids as well as their synthesis and decay. The specific roles of MX2 in IFN-α inhibition of HBV transcription and replication were addressed by MX2-specific shRNA interference (RNAi). MX2 alone as well as IFN-α substantially inhibited HBV replication, due to significant deceleration of the synthesis and slight acceleration of the turnover of viral RNA. RNAi knock-down of MX2 significantly reduced the inhibitory effects of IFN-α. Strikingly, MX2 inhibited HBV infection by reducing covalently closed circular DNA (cccDNA), most likely by indirectly impairing relaxed circular DNA to cccDNA conversion rather than destabilizing existing cccDNA. Various mutations affecting the GTPase activity and oligomerization status reduced MX2’s anti-HBV activity. MX2 is an important IFN-α inducible effector that decreases HBV RNA levels but can also potently inhibit HBV infection by indirectly impairing cccDNA formation. MX2 likely has the potential of therapeutic application aimed at curing HBV infection by eliminating cccDNA. Some of the best-studied ISGs are the myxovirus resistance (Mx) proteins, evolutionarily conserved dynamin-like large GTPases.Humans have two MX genes, MX1 and MX2, which encode the MxA (or MX1) and MxB (or MX2) proteins, respectively [19]; they share ∼63% amino acid (aa) sequence identity, mainly in the C-terminal region (Fig. 1).The N-terminus of MX2 contains a nuclear localization signal (NLS) and a proline-rich domain (PRD, Fig. 1).Recent X-ray crystallographic data for human MX1 and MX2 (PDB ID: 3SZR and 4WHJ), both comprising the GTPase domain, bundle signaling element (BSE) and stalk domain [20, 21] confirmed this similarity on the structural level.Surprisingly, though, only MX1 was well known for its wide-spectrum antiviral activity, mainly against RNA viruses [22].MX2 seemed to lack antiviral activity until its recent identification as an important IFN-induced inhibitor of human immunodeficiency virus 1 (HIV-1), hepatitis C virus (HCV), and herpesvirus infection [23-28].The anti-HIV-1 activity was found to depend on MX2’s binding to the HIV-1 nucleocapsid through an N-proximal triple-arginine motif (aa 11-13; Fig. 1), possibly by preventing uncoating of the packaged genome [20, 29-31].Similarly, MX2 also inhibited the uncoating of herpesvirus DNA encapsidated in the invading nucleocapsids [28], although the exact mechanisms are poorly understood and may differ for different viruses [32].The anti-HCV activity was mediated by MX2 interaction with the NS5A protein, thereby interfering with NS5A association with CypA required for its localization to endoplasmic reticulum [26].
The aim of this study was to compare the performance of MRIs with extracellular contrast agents (ECA-MRI) to HB contrast agents (HBA-MRI) for the non-invasive diagnosis of small HCCs in a head-to-head comparison. Between August 2014 and October 2017; 171 cirrhotic patients, each with 1 to 3 nodules measuring 1 to 3 cm, were included across eight centers. All patients had both an ECA-MRI and an HBA-MRI within a month. The non-invasive diagnosis of HCC was made when the nodule was hyper-enhanced at the arterial phase (HA) with wash-out at the portal phase (PP) and/or delayed phase (DP) for the ECA-MRI, or PP and/or HB phase (HBP) for the HBA-MRI. The gold standard was defined by a composite algorithm previously published. 225 nodules, of which 153 were HCCs and 72 were not HCCs, were included. Both MRI sensitivities were similar (71.2% [63.4-78.3]). Specificity was 83.3% [72.7-91.1] for the ECA-MRI and 68.1% [56.0-78.6] for the HBA-MRI. With regard to HCCs, on ECA-MRI, 138 were HA, 84 had wash-out at PP and 104 at DP; on HBA-MRI, 128 were HA, 71 had wash-out at PP and 99 at HBP. For nodules from 2 to 3 cm, sensitivity and specificity were similar with 70.9% [57.1-82.4] and 75.0% [47.6-92.7] respectively. For nodules from 1 to 2 cm, specificity dropped to 66.1% [52.2-78.2] for the HBA-MRI vs. 85.7% [73.8-93.6] for the ECA-MRI. HBA-MRI specificity is lower than ECA-MRI for diagnosing small HCCs on cirrhotic patients. These results raise the question of the proper use of HBA-MRI in algorithms for the non-invasive diagnosis of small HCCs. Liver cancer is the second most frequent cause of cancer-related deaths worldwide and hepatocellular carcinoma (HCC) represents about 90% of primary liver cancers [1].The concept of imaging-based non-invasive diagnosis of HCC is part of both Western and Asian guidelines [1–5].In Western guidelines, the diagnosis can be made in cirrhotic patients for nodules ≥ 1cm showing typical hallmarks on contrast-enhanced multiphasic imaging (computer tomography (CT) or magnetic resonance imaging (MRI)) [1].
The aim of this study was to compare the performance of MRIs with extracellular contrast agents (ECA-MRI) to HB contrast agents (HBA-MRI) for the non-invasive diagnosis of small HCCs in a head-to-head comparison. Between August 2014 and October 2017; 171 cirrhotic patients, each with 1 to 3 nodules measuring 1 to 3 cm, were included across eight centers. All patients had both an ECA-MRI and an HBA-MRI within a month. The non-invasive diagnosis of HCC was made when the nodule was hyper-enhanced at the arterial phase (HA) with wash-out at the portal phase (PP) and/or delayed phase (DP) for the ECA-MRI, or PP and/or HB phase (HBP) for the HBA-MRI. The gold standard was defined by a composite algorithm previously published. 225 nodules, of which 153 were HCCs and 72 were not HCCs, were included. Both MRI sensitivities were similar (71.2% [63.4-78.3]). Specificity was 83.3% [72.7-91.1] for the ECA-MRI and 68.1% [56.0-78.6] for the HBA-MRI. With regard to HCCs, on ECA-MRI, 138 were HA, 84 had wash-out at PP and 104 at DP; on HBA-MRI, 128 were HA, 71 had wash-out at PP and 99 at HBP. For nodules from 2 to 3 cm, sensitivity and specificity were similar with 70.9% [57.1-82.4] and 75.0% [47.6-92.7] respectively. For nodules from 1 to 2 cm, specificity dropped to 66.1% [52.2-78.2] for the HBA-MRI vs. 85.7% [73.8-93.6] for the ECA-MRI. HBA-MRI specificity is lower than ECA-MRI for diagnosing small HCCs on cirrhotic patients. These results raise the question of the proper use of HBA-MRI in algorithms for the non-invasive diagnosis of small HCCs. In the past, CT or MR extracellular agents (ECAs) have been used.Recently, hepatobiliary MR contrast agents (HBAs) have been included in all guidelines for the non-invasive diagnosis of HCC [1–4,6].On hepatobiliary phase (HBP) images, the vast majority of HCCs display signal hypointensity compared to the surrounding liver parenchyma [7].The weight and value of this feature differ significantly depending on the guidelines.While part of the HCC hallmarks [2–4,8] is a major feature in Asian guidelines, Western ones consider it as an ancillary feature that favors malignancy when present, but is not sufficient to make a conclusive diagnosis of HCC [1,5].
The aim of this study was to compare the performance of MRIs with extracellular contrast agents (ECA-MRI) to HB contrast agents (HBA-MRI) for the non-invasive diagnosis of small HCCs in a head-to-head comparison. Between August 2014 and October 2017; 171 cirrhotic patients, each with 1 to 3 nodules measuring 1 to 3 cm, were included across eight centers. All patients had both an ECA-MRI and an HBA-MRI within a month. The non-invasive diagnosis of HCC was made when the nodule was hyper-enhanced at the arterial phase (HA) with wash-out at the portal phase (PP) and/or delayed phase (DP) for the ECA-MRI, or PP and/or HB phase (HBP) for the HBA-MRI. The gold standard was defined by a composite algorithm previously published. 225 nodules, of which 153 were HCCs and 72 were not HCCs, were included. Both MRI sensitivities were similar (71.2% [63.4-78.3]). Specificity was 83.3% [72.7-91.1] for the ECA-MRI and 68.1% [56.0-78.6] for the HBA-MRI. With regard to HCCs, on ECA-MRI, 138 were HA, 84 had wash-out at PP and 104 at DP; on HBA-MRI, 128 were HA, 71 had wash-out at PP and 99 at HBP. For nodules from 2 to 3 cm, sensitivity and specificity were similar with 70.9% [57.1-82.4] and 75.0% [47.6-92.7] respectively. For nodules from 1 to 2 cm, specificity dropped to 66.1% [52.2-78.2] for the HBA-MRI vs. 85.7% [73.8-93.6] for the ECA-MRI. HBA-MRI specificity is lower than ECA-MRI for diagnosing small HCCs on cirrhotic patients. These results raise the question of the proper use of HBA-MRI in algorithms for the non-invasive diagnosis of small HCCs. In recent years, HBA-MRI has been evaluated in many studies for the diagnosis of HCC showing higher sensitivity than ECA-MRI, especially when small, suggesting that for diagnosing HCC, HBA should be used upfront [2–4,7,9].However, others were discordant and reported either a lower sensitivity due to poorer arterial phase quality using HBA [10] or a lower specificity with HBA [11].However, these studies were not based on direct intra-individual comparisons of ECA-MRI and HBA-MRI (head-to-head nodule comparison).
The aim of this study was to compare the performance of MRIs with extracellular contrast agents (ECA-MRI) to HB contrast agents (HBA-MRI) for the non-invasive diagnosis of small HCCs in a head-to-head comparison. Between August 2014 and October 2017; 171 cirrhotic patients, each with 1 to 3 nodules measuring 1 to 3 cm, were included across eight centers. All patients had both an ECA-MRI and an HBA-MRI within a month. The non-invasive diagnosis of HCC was made when the nodule was hyper-enhanced at the arterial phase (HA) with wash-out at the portal phase (PP) and/or delayed phase (DP) for the ECA-MRI, or PP and/or HB phase (HBP) for the HBA-MRI. The gold standard was defined by a composite algorithm previously published. 225 nodules, of which 153 were HCCs and 72 were not HCCs, were included. Both MRI sensitivities were similar (71.2% [63.4-78.3]). Specificity was 83.3% [72.7-91.1] for the ECA-MRI and 68.1% [56.0-78.6] for the HBA-MRI. With regard to HCCs, on ECA-MRI, 138 were HA, 84 had wash-out at PP and 104 at DP; on HBA-MRI, 128 were HA, 71 had wash-out at PP and 99 at HBP. For nodules from 2 to 3 cm, sensitivity and specificity were similar with 70.9% [57.1-82.4] and 75.0% [47.6-92.7] respectively. For nodules from 1 to 2 cm, specificity dropped to 66.1% [52.2-78.2] for the HBA-MRI vs. 85.7% [73.8-93.6] for the ECA-MRI. HBA-MRI specificity is lower than ECA-MRI for diagnosing small HCCs on cirrhotic patients. These results raise the question of the proper use of HBA-MRI in algorithms for the non-invasive diagnosis of small HCCs. To our knowledge, only one study, has performed a prospective monocentric head-to-head comparison of ECA-MRI and HBA-MRI, using the Liver Reporting and Data System (LI-RADS) [12].This study suggested a superiority of ECA-MRI over HBA-MRI.However, other non-invasive sets of criteria, e.g., that of the European Association for the Study of the Liver (EASL) were not included.Therefore, the comparison of the diagnostic performance of MRI using ECA or HBA, and the criteria to be used for HBA-MRI, remain to be determined in Western populations for the non-invasive diagnosis of HCC.
The introduction of direct-acting antivirals for hepatitis C virus (HCV) in Egypt led to massive treatment uptake, with Egypt’s national HCV treatment program becoming the largest in the world. The aim of this paper is to present the Egyptian experience in planning and prioritizing mass treatment for patients with HCV, highlighting the difficulties and limitations of the program, as a guide for other countries of similarly limited resources. Baseline data of 337,042 patients, treated between October 2014 to March 2016 in specialized viral hepatitis treatment centers, were grouped into three equal time intervals of six months each. Patients were treated with different combinations of direct-acting antivirals, with or without ribavirin and pegylated interferon. Baseline data, percentage of patients with known outcome, and sustained virological response at week 12 (SVR12) were analyzed for the three cohorts. The outcomes of 94,258 patients treated in the subsequent two months are also included. For cohort-1, treatment was prioritized for patients with advanced fibrosis (F3-F4 fibrosis, liver stiffness ≥9.5 kPa, or Fibrosis-4 ≥3.25). Starting cohort-2, all stages of fibrosis were included (F0-F4). The prioritization strategy in the initial phase caused delays in enrollment and massive backlogs. Cohort-1 patients were significantly older, and more had advanced fibrosis compared to subsequent cohorts. The percentage of patients with known SVR12 results were low initially, and increased with each cohort, as several methods to capture patient results were adopted. Sofosbuvir-ribavirin therapy for 24 weeks had the lowest SVR12 rate (82.7%); while other therapies were associated with SVR12 rates between 94% and 98%. Prioritization based on fibrosis stage was not effective and enrollment increased greatly only after including all stages of fibrosis. The availability of generic drugs reduced costs, and helped massively increase uptake of the program. Post-treatment follow-up was initially very low, and although this has increased, further improvement is still needed. Egypt has the highest prevalence of hepatitis C virus (HCV) infection,1 having an antibody prevalence in the population aged 15–59 years of about 10% and viremic prevalence of 7%,2 with almost all infections due to genotype 4 (GT4).3This burden necessitated strong commitment on the part of policy-makers to adopt efficient curative interventions.This has translated into the largest national treatment program for HCV in the world.4Lessons from the successes and difficulties of the program can benefit all resource-limited countries with similar problems caused by hepatitis C.
The introduction of direct-acting antivirals for hepatitis C virus (HCV) in Egypt led to massive treatment uptake, with Egypt’s national HCV treatment program becoming the largest in the world. The aim of this paper is to present the Egyptian experience in planning and prioritizing mass treatment for patients with HCV, highlighting the difficulties and limitations of the program, as a guide for other countries of similarly limited resources. Baseline data of 337,042 patients, treated between October 2014 to March 2016 in specialized viral hepatitis treatment centers, were grouped into three equal time intervals of six months each. Patients were treated with different combinations of direct-acting antivirals, with or without ribavirin and pegylated interferon. Baseline data, percentage of patients with known outcome, and sustained virological response at week 12 (SVR12) were analyzed for the three cohorts. The outcomes of 94,258 patients treated in the subsequent two months are also included. For cohort-1, treatment was prioritized for patients with advanced fibrosis (F3-F4 fibrosis, liver stiffness ≥9.5 kPa, or Fibrosis-4 ≥3.25). Starting cohort-2, all stages of fibrosis were included (F0-F4). The prioritization strategy in the initial phase caused delays in enrollment and massive backlogs. Cohort-1 patients were significantly older, and more had advanced fibrosis compared to subsequent cohorts. The percentage of patients with known SVR12 results were low initially, and increased with each cohort, as several methods to capture patient results were adopted. Sofosbuvir-ribavirin therapy for 24 weeks had the lowest SVR12 rate (82.7%); while other therapies were associated with SVR12 rates between 94% and 98%. Prioritization based on fibrosis stage was not effective and enrollment increased greatly only after including all stages of fibrosis. The availability of generic drugs reduced costs, and helped massively increase uptake of the program. Post-treatment follow-up was initially very low, and although this has increased, further improvement is still needed. With the approval of sofosbuvir (SOF), SOF based therapies became standard of care,5,6 and the Egyptian National Committee for the Control of Viral Hepatitis (NCCVH) endeavored to make oral direct-acting antivirals (DAAs) available for the national treatment program at affordable prices.7–9A web-based registry was set up to arrange patient appointments and visits and to record patient data, through a central database.9
The introduction of direct-acting antivirals for hepatitis C virus (HCV) in Egypt led to massive treatment uptake, with Egypt’s national HCV treatment program becoming the largest in the world. The aim of this paper is to present the Egyptian experience in planning and prioritizing mass treatment for patients with HCV, highlighting the difficulties and limitations of the program, as a guide for other countries of similarly limited resources. Baseline data of 337,042 patients, treated between October 2014 to March 2016 in specialized viral hepatitis treatment centers, were grouped into three equal time intervals of six months each. Patients were treated with different combinations of direct-acting antivirals, with or without ribavirin and pegylated interferon. Baseline data, percentage of patients with known outcome, and sustained virological response at week 12 (SVR12) were analyzed for the three cohorts. The outcomes of 94,258 patients treated in the subsequent two months are also included. For cohort-1, treatment was prioritized for patients with advanced fibrosis (F3-F4 fibrosis, liver stiffness ≥9.5 kPa, or Fibrosis-4 ≥3.25). Starting cohort-2, all stages of fibrosis were included (F0-F4). The prioritization strategy in the initial phase caused delays in enrollment and massive backlogs. Cohort-1 patients were significantly older, and more had advanced fibrosis compared to subsequent cohorts. The percentage of patients with known SVR12 results were low initially, and increased with each cohort, as several methods to capture patient results were adopted. Sofosbuvir-ribavirin therapy for 24 weeks had the lowest SVR12 rate (82.7%); while other therapies were associated with SVR12 rates between 94% and 98%. Prioritization based on fibrosis stage was not effective and enrollment increased greatly only after including all stages of fibrosis. The availability of generic drugs reduced costs, and helped massively increase uptake of the program. Post-treatment follow-up was initially very low, and although this has increased, further improvement is still needed. With limited financial resources, limited initial supply of medication, and the limited capacities of treatment centers to handle the huge number of patients living with the diagnosis, initially there was a need to prioritize treatment for patients with advanced fibrosis and to postpone treatment for patients with less advanced disease.This was implemented through sequential modification of treatment protocols to accommodate availability of more medications, and to adapt to administrative difficulties as they appeared.Several changes were made during the first 18 months of the program, to include more patients, to change treatment regimens according to results, and to include newer drugs as they became available.Cost was always a factor in the decision making process, while selecting regimens suitable to the unique situation of HCV-GT4 in the Egyptian population was crucial.10–12
It remains unclear whether the classic imaging criteria for the non-invasive diagnosis of hepatocellular carcinoma (HCC) can be applied to chronic vascular liver diseases, such as Budd-Chiari syndrome (BCS). Herein, we aimed to evaluate the diagnostic value of washout for the discrimination between benign and malignant lesions in patients with BCS. This retrospective study included all patients admitted to our institution with a diagnosis of BCS and focal lesions on MRI from 2000 to 2016. MRI images were reviewed by 2 radiologists blinded to the nature of the lesions. Patient and lesion characteristics were recorded, with a focus on washout on portal venous and/or delayed phases. Lesions were compared using Chi-square, Fisher’s, Student’s t or Mann-Whitney U tests. A total of 49 patients (mean age 35 ± 12 years; 34 women [69%] and 15 men [31%]) with 241 benign lesions and 12 HCC lesions were analyzed. Patients with HCC were significantly older (mean age 44 ± 16 vs. 33 ± 9 years, p = 0.005), with higher alpha-fetoprotein (AFP) levels (median 16 vs. 3 ng/ml, p = 0.007). Washout was depicted in 9/12 (75%) HCC, and 69/241 (29%) benign lesions (p <0.001). A total of 52/143 (36%) lesions ≥1 cm with arterial hyperenhancement showed washout (9 HCC and 43 benign lesions). In this subgroup, the specificity of washout for the diagnosis of HCC was 67%. Adding T1-w hypointensity raised the specificity to 100%. A serum AFP >15 ng/ml was associated with 95% specificity. Washout was observed in close to one-third of benign lesions, leading to an unacceptably low specificity for the diagnosis of HCC. The non-invasive diagnostic criteria proposed for cirrhotic patients cannot be extrapolated to patients with BCS. Primary Budd-Chiari Syndrome (BCS) is a rare vascular disorder involving hepatic venous outflow impairment at any level between the small hepatic veins and the right atrium.1–4In Western countries, primary BCS is commonly associated with prothrombotic conditions such as myeloproliferative neoplasms, coagulation disorders of various causes, paroxysmal nocturnal hemoglobinuria, antiphospholipid syndrome and Behçet’s disease.The use of oral contraceptives is frequently associated with BCS, but it seems to act as a co-factor.3,5–7
It remains unclear whether the classic imaging criteria for the non-invasive diagnosis of hepatocellular carcinoma (HCC) can be applied to chronic vascular liver diseases, such as Budd-Chiari syndrome (BCS). Herein, we aimed to evaluate the diagnostic value of washout for the discrimination between benign and malignant lesions in patients with BCS. This retrospective study included all patients admitted to our institution with a diagnosis of BCS and focal lesions on MRI from 2000 to 2016. MRI images were reviewed by 2 radiologists blinded to the nature of the lesions. Patient and lesion characteristics were recorded, with a focus on washout on portal venous and/or delayed phases. Lesions were compared using Chi-square, Fisher’s, Student’s t or Mann-Whitney U tests. A total of 49 patients (mean age 35 ± 12 years; 34 women [69%] and 15 men [31%]) with 241 benign lesions and 12 HCC lesions were analyzed. Patients with HCC were significantly older (mean age 44 ± 16 vs. 33 ± 9 years, p = 0.005), with higher alpha-fetoprotein (AFP) levels (median 16 vs. 3 ng/ml, p = 0.007). Washout was depicted in 9/12 (75%) HCC, and 69/241 (29%) benign lesions (p <0.001). A total of 52/143 (36%) lesions ≥1 cm with arterial hyperenhancement showed washout (9 HCC and 43 benign lesions). In this subgroup, the specificity of washout for the diagnosis of HCC was 67%. Adding T1-w hypointensity raised the specificity to 100%. A serum AFP >15 ng/ml was associated with 95% specificity. Washout was observed in close to one-third of benign lesions, leading to an unacceptably low specificity for the diagnosis of HCC. The non-invasive diagnostic criteria proposed for cirrhotic patients cannot be extrapolated to patients with BCS. One of the consequences of chronic BCS is the development of focal liver lesions.The vast majority are benign and correspond to large regenerative lesions called focal nodular hyperplasia-like (FNH-like) lesions.8They develop as a consequence of chronic portal blood deprivation and the compensatory increase in the arterial blood supply to the liver parenchyma.8–10BCS is also a recognized risk factor for hepatocellular carcinoma (HCC) with an incidence varying from country to country: 6% to 41% of patients with BCS in Japan; 48% of patients in South Africa; and 25% in the US.3,11–13Thus, it is important to accurately differentiate benign lesions from HCC, as patient management will greatly differ.From this perspective, imaging, especially MRI, plays an important role as a first non-invasive step in the diagnostic process.
It remains unclear whether the classic imaging criteria for the non-invasive diagnosis of hepatocellular carcinoma (HCC) can be applied to chronic vascular liver diseases, such as Budd-Chiari syndrome (BCS). Herein, we aimed to evaluate the diagnostic value of washout for the discrimination between benign and malignant lesions in patients with BCS. This retrospective study included all patients admitted to our institution with a diagnosis of BCS and focal lesions on MRI from 2000 to 2016. MRI images were reviewed by 2 radiologists blinded to the nature of the lesions. Patient and lesion characteristics were recorded, with a focus on washout on portal venous and/or delayed phases. Lesions were compared using Chi-square, Fisher’s, Student’s t or Mann-Whitney U tests. A total of 49 patients (mean age 35 ± 12 years; 34 women [69%] and 15 men [31%]) with 241 benign lesions and 12 HCC lesions were analyzed. Patients with HCC were significantly older (mean age 44 ± 16 vs. 33 ± 9 years, p = 0.005), with higher alpha-fetoprotein (AFP) levels (median 16 vs. 3 ng/ml, p = 0.007). Washout was depicted in 9/12 (75%) HCC, and 69/241 (29%) benign lesions (p <0.001). A total of 52/143 (36%) lesions ≥1 cm with arterial hyperenhancement showed washout (9 HCC and 43 benign lesions). In this subgroup, the specificity of washout for the diagnosis of HCC was 67%. Adding T1-w hypointensity raised the specificity to 100%. A serum AFP >15 ng/ml was associated with 95% specificity. Washout was observed in close to one-third of benign lesions, leading to an unacceptably low specificity for the diagnosis of HCC. The non-invasive diagnostic criteria proposed for cirrhotic patients cannot be extrapolated to patients with BCS. Few studies have reported imaging findings of focal liver lesions in patients with BCS.9,10,14All have shown that both benign and malignant lesions appear markedly hyperenhanced on contrast-enhanced CT or MRI during the arterial phase.They have also shown that lesion size, number and serum alpha-fetoprotein (AFP) level are useful for further characterization.9,10,11,15
It remains unclear whether the classic imaging criteria for the non-invasive diagnosis of hepatocellular carcinoma (HCC) can be applied to chronic vascular liver diseases, such as Budd-Chiari syndrome (BCS). Herein, we aimed to evaluate the diagnostic value of washout for the discrimination between benign and malignant lesions in patients with BCS. This retrospective study included all patients admitted to our institution with a diagnosis of BCS and focal lesions on MRI from 2000 to 2016. MRI images were reviewed by 2 radiologists blinded to the nature of the lesions. Patient and lesion characteristics were recorded, with a focus on washout on portal venous and/or delayed phases. Lesions were compared using Chi-square, Fisher’s, Student’s t or Mann-Whitney U tests. A total of 49 patients (mean age 35 ± 12 years; 34 women [69%] and 15 men [31%]) with 241 benign lesions and 12 HCC lesions were analyzed. Patients with HCC were significantly older (mean age 44 ± 16 vs. 33 ± 9 years, p = 0.005), with higher alpha-fetoprotein (AFP) levels (median 16 vs. 3 ng/ml, p = 0.007). Washout was depicted in 9/12 (75%) HCC, and 69/241 (29%) benign lesions (p <0.001). A total of 52/143 (36%) lesions ≥1 cm with arterial hyperenhancement showed washout (9 HCC and 43 benign lesions). In this subgroup, the specificity of washout for the diagnosis of HCC was 67%. Adding T1-w hypointensity raised the specificity to 100%. A serum AFP >15 ng/ml was associated with 95% specificity. Washout was observed in close to one-third of benign lesions, leading to an unacceptably low specificity for the diagnosis of HCC. The non-invasive diagnostic criteria proposed for cirrhotic patients cannot be extrapolated to patients with BCS. In patients with cirrhosis, the classic imaging hallmarks of HCC (i.e. combination of hyperenhancement on arterial phase and washout for lesions greater than 1 cm) on either CT or MRI are sufficient to allow for a non-invasive diagnosis.16,17The question of whether these imaging criteria might be applied in chronic vascular liver diseases at risk of benign lesions and HCC, such as BCS, remains open.Indeed, cirrhosis and chronic BCS both share decreased liver perfusion and increased arterial inflow, but liver congestion is a peculiar finding observed in liver venous outflow obstruction and not in cirrhosis.As liver congestion is seen as persistent and heterogeneous enhancement on contrast-enhanced CT or MRI during portal venous and delayed phases, washout observed in liver lesions may be due to a different mechanism and could therefore be less specific for HCC in that setting.
The fixed-dose combination of sofosbuvir/velpatasvir was highly efficacious in patients infected with genotype (GT)1–6 hepatitis C virus (HCV) in the ASTRAL studies. This analysis evaluated the impact of baseline resistance-associated substitutions (RASs) on treatment outcome and emergence of RASs in patients infected with HCV GT1-6 who were treated with sofosbuvir/velpatasvir. Non-structural protein 5A and 5B (NS5A and NS5B) deep sequencing was performed at baseline and at the time of relapse for all patients treated with sofosbuvir/velpatasvir for 12 weeks (n = 1,778) in the ASTRAL-1–3, ASTRAL-5 and POLARIS-2–3 studies. Patients with 37 known and 19 novel HCV subtypes were included in these analyses. Overall, 28% (range 9% to 61% depending on genotype) had detectable NS5A class RASs at baseline, using a 15% sequencing assay cut-off. There was no significant effect of baseline NS5A class RASs on sustained virologic response at week 12 (SVR12) with sofosbuvir/velpatasvir; the SVR12 rate in the presence of NS5A class RASs was 100% and 97%, in patients with GT1a and GT1b infection, respectively, and 100% in patients with GT2 and GT4–6 infections. In GT3 infection, the SVR rate was 93% and 98% in patients with and without baseline NS5A class RASs, respectively. The overall virologic failure rate was low (20/1,778 = 1.1%) in patients treated with sofosbuvir/velpatasvir. Single NS5A class resistance was observed at virologic failure in 17 of the 20 patients. Sofosbuvir/velpatasvir taken for 12 weeks once daily resulted in high SVR rates in patients infected with GT1–6 HCV, irrespective of baseline NS5A RASs. NS5A inhibitor resistance, but not sofosbuvir resistance, was detected in the few patients with virologic failure. These data highlight the high barrier to resistance of this regimen for the treatment of chronic HCV across all genotypes in the vast majority of patients. Hepatitis C virus (HCV) infection is a global health burden with an estimated 71–150 million individuals infected worldwide.1–3Development of direct-acting antivirals (DAAs) in recent years has dramatically enhanced sustained virologic response (SVR) rates in HCV genotype (GT)1 chronically infected patients.4
The fixed-dose combination of sofosbuvir/velpatasvir was highly efficacious in patients infected with genotype (GT)1–6 hepatitis C virus (HCV) in the ASTRAL studies. This analysis evaluated the impact of baseline resistance-associated substitutions (RASs) on treatment outcome and emergence of RASs in patients infected with HCV GT1-6 who were treated with sofosbuvir/velpatasvir. Non-structural protein 5A and 5B (NS5A and NS5B) deep sequencing was performed at baseline and at the time of relapse for all patients treated with sofosbuvir/velpatasvir for 12 weeks (n = 1,778) in the ASTRAL-1–3, ASTRAL-5 and POLARIS-2–3 studies. Patients with 37 known and 19 novel HCV subtypes were included in these analyses. Overall, 28% (range 9% to 61% depending on genotype) had detectable NS5A class RASs at baseline, using a 15% sequencing assay cut-off. There was no significant effect of baseline NS5A class RASs on sustained virologic response at week 12 (SVR12) with sofosbuvir/velpatasvir; the SVR12 rate in the presence of NS5A class RASs was 100% and 97%, in patients with GT1a and GT1b infection, respectively, and 100% in patients with GT2 and GT4–6 infections. In GT3 infection, the SVR rate was 93% and 98% in patients with and without baseline NS5A class RASs, respectively. The overall virologic failure rate was low (20/1,778 = 1.1%) in patients treated with sofosbuvir/velpatasvir. Single NS5A class resistance was observed at virologic failure in 17 of the 20 patients. Sofosbuvir/velpatasvir taken for 12 weeks once daily resulted in high SVR rates in patients infected with GT1–6 HCV, irrespective of baseline NS5A RASs. NS5A inhibitor resistance, but not sofosbuvir resistance, was detected in the few patients with virologic failure. These data highlight the high barrier to resistance of this regimen for the treatment of chronic HCV across all genotypes in the vast majority of patients. HCV is classified into seven confirmed genotypes and 67 subtypes.5GT1 is the most prevalent HCV genotype worldwide and accounts for approximately 46% of chronic HCV infection globally.GT3 accounts for about 30% of chronic HCV infections, followed by GT2, 4, and 6 accounting for 23% of HCV cases; and GT5 accounting for <1%.6GT7 has so far only been identified in a few patient isolates.13While GT1 and 3 dominate in most countries irrespective of economic status, the largest proportions of GT4 and 5 are in lower-income countries.The development of pan-genotypic treatments with high efficacy and tolerability across genotypes, irrespective of degree of fibrosis, will be necessary to make an impact on global disease prevalence, especially in regions with high genotype diversity where HCV genotyping and fibrosis staging are not part of routine medical care for HCV infection.
PAGE-B and modified PAGE-B (mPAGE-B) scores are developed to predict risk of hepatocellular carcinoma (HCC) in patients on nucleos(t)ide analogue therapy. However, how and when to use these risk scores in clinical practice is uncertain. Consecutive adult patients with chronic hepatitis B who had received entecavir or tenofovir for at least 6 months between January 2005 and June 2018 were identified from a territory-wide database in Hong Kong. Performance of PAGE-B and mPAGE-B scores on HCC prediction at 5 years was assessed by area under the time-dependent receiver operating characteristic curve (AUROC), and different cut-off values of these two scores were evaluated by survival analysis. Of 32,150 identified chronic hepatitis B patients, 20,868 (64.9%) were male. Their mean age was 53.0±13.2 years. At a median (interquartile range) follow-up of 3.9 (1.8–5.0) years, 1,532 (4.8%) patients developed HCC. The AUROC (95% confidence interval [CI]) of PAGE-B and mPAGE-B scores to predict HCC at 5 years was 0.77 (0.76–0.78) and 0.80 (0.79–0.81), respectively (P<0.001). 9,417 (29.3%) patients were classified as low HCC risk by either PAGE-B or mPAGE-B scores; their 5-year cumulative incidence (95% CI) of HCC was 0.6% (0.4%–0.8%). This classification achieved a negative predictive value (95% CI) of 99.5% (99.4%–99.7%) to exclude patients without HCC development in five years. The AUROC of PAGE-B and mPAGE-B scores at baseline and 2-year on-treatment to predict HCC were similar. PAGE-B and mPAGE-B scores can be applied to identify patients who have low risk of HCC development on antiviral therapy. These patients may be considered exemption from HCC surveillance due to their very low HCC risk. Chronic hepatitis B (CHB) is a major healthcare problem in Asia.Hepatocellular carcinoma (HCC) is a key complication and cause of mortality of CHB.[1]To prevent mortality related to HCC, high-risk patients should be identified and treated.With the introduction of nucleos(t)ide analogues (NA), potent viral suppression leads to fibrosis and cirrhosis regression, reduction of cirrhotic complications, and reduced liver-related mortality.[2]Although the risk of HCC is reduced by NA, this dreadful complication still exists particularly in cirrhotic patients.[3, 4]In a long-term follow-up study in Europe, new cases of HCC still developed even after more than 5 years of NA treatment.[5]A Korean report showed that the age-standardised death rate related to HCC only reduced modestly by 27% as compared to a 75% reduction in liver-related death between 1999-2013 despite extensive use of antiviral agents.[6]
PAGE-B and modified PAGE-B (mPAGE-B) scores are developed to predict risk of hepatocellular carcinoma (HCC) in patients on nucleos(t)ide analogue therapy. However, how and when to use these risk scores in clinical practice is uncertain. Consecutive adult patients with chronic hepatitis B who had received entecavir or tenofovir for at least 6 months between January 2005 and June 2018 were identified from a territory-wide database in Hong Kong. Performance of PAGE-B and mPAGE-B scores on HCC prediction at 5 years was assessed by area under the time-dependent receiver operating characteristic curve (AUROC), and different cut-off values of these two scores were evaluated by survival analysis. Of 32,150 identified chronic hepatitis B patients, 20,868 (64.9%) were male. Their mean age was 53.0±13.2 years. At a median (interquartile range) follow-up of 3.9 (1.8–5.0) years, 1,532 (4.8%) patients developed HCC. The AUROC (95% confidence interval [CI]) of PAGE-B and mPAGE-B scores to predict HCC at 5 years was 0.77 (0.76–0.78) and 0.80 (0.79–0.81), respectively (P<0.001). 9,417 (29.3%) patients were classified as low HCC risk by either PAGE-B or mPAGE-B scores; their 5-year cumulative incidence (95% CI) of HCC was 0.6% (0.4%–0.8%). This classification achieved a negative predictive value (95% CI) of 99.5% (99.4%–99.7%) to exclude patients without HCC development in five years. The AUROC of PAGE-B and mPAGE-B scores at baseline and 2-year on-treatment to predict HCC were similar. PAGE-B and mPAGE-B scores can be applied to identify patients who have low risk of HCC development on antiviral therapy. These patients may be considered exemption from HCC surveillance due to their very low HCC risk. A second line of prevention of HCC-related mortality will be regular surveillance by ultrasonography and alpha-fetoprotein testing.There are good data to support half-yearly HCC surveillance can pick up early HCC for curative treatment, which may lead to an improved patient survival.[7]Owing to the residual HCC risk on NA, patients should still undergo HCC surveillance even under NA treatment.However, HCC surveillance is resource intensive and demands long-term patient adherence to the program.As NA treatment regresses liver fibrosis and reduces the risk of HCC, some of the treated patients may have HCC risk reduced to such a low level that does not warrant HCC surveillance.Based on the recommendations of American Association for the Study of Liver Diseases, an annual HCC risk of <0.2% is probably not cost-effective for HCC surveillance.[8]It will therefore be useful if one can predict the HCC risk among patients on NA, and exempt the low risk patients from HCC surveillance.[9]
PAGE-B and modified PAGE-B (mPAGE-B) scores are developed to predict risk of hepatocellular carcinoma (HCC) in patients on nucleos(t)ide analogue therapy. However, how and when to use these risk scores in clinical practice is uncertain. Consecutive adult patients with chronic hepatitis B who had received entecavir or tenofovir for at least 6 months between January 2005 and June 2018 were identified from a territory-wide database in Hong Kong. Performance of PAGE-B and mPAGE-B scores on HCC prediction at 5 years was assessed by area under the time-dependent receiver operating characteristic curve (AUROC), and different cut-off values of these two scores were evaluated by survival analysis. Of 32,150 identified chronic hepatitis B patients, 20,868 (64.9%) were male. Their mean age was 53.0±13.2 years. At a median (interquartile range) follow-up of 3.9 (1.8–5.0) years, 1,532 (4.8%) patients developed HCC. The AUROC (95% confidence interval [CI]) of PAGE-B and mPAGE-B scores to predict HCC at 5 years was 0.77 (0.76–0.78) and 0.80 (0.79–0.81), respectively (P<0.001). 9,417 (29.3%) patients were classified as low HCC risk by either PAGE-B or mPAGE-B scores; their 5-year cumulative incidence (95% CI) of HCC was 0.6% (0.4%–0.8%). This classification achieved a negative predictive value (95% CI) of 99.5% (99.4%–99.7%) to exclude patients without HCC development in five years. The AUROC of PAGE-B and mPAGE-B scores at baseline and 2-year on-treatment to predict HCC were similar. PAGE-B and mPAGE-B scores can be applied to identify patients who have low risk of HCC development on antiviral therapy. These patients may be considered exemption from HCC surveillance due to their very low HCC risk. Numerous risk scores have been developed to predict the risk of HCC.Early scores are largely developed in Asia from untreated patients, including the guide with age, gender, HBV DNA, core promoter mutations and cirrhosis-HCC (GAG-HCC),[10] the Chinese University of Hong Kong-HCC score (CU-HCC),[11] the risk estimation for HCC in CHB score (REACH-B),[12] and liver stiffness measurement-HCC score (LSM-HCC).[13]Among NA-treated patients, these scores have been found to have modest prediction of HCC in Asia, but their performances are not satisfactory in Caucasian patients.[14, 15]As HBV DNA is usually suppressed in NA-treated patients, its role in predicting HCC development is much less than in untreated patients.A simple risk score based on age, gender and platelet (PAGE-B) has been developed among Caucasian patients on entecavir (ETV) and tenofovir disoproxil fumarate (TDF) to predict HCC risk for up to 5 years.[16]PAGE-B score was subsequently modified slightly by Korean investigators by adding serum albumin and adjusting the weighting of age, gender and platelet count (mPAGE-B).[17]As these scores are based on objective demographic and laboratory parameters, they have the potential to be widely used in clinical practice.However, how and when to use these risk scores remains undetermined.
Although non-alcoholic fatty liver disease (NAFLD), non-alcoholic steatohepatitis (NASH) and NASH with advanced fibrosis are closely associated with type 2 diabetes mellitus (T2DM), their global prevalence rates have not been well described. Our aim was to estimate the prevalence of NAFLD, NASH, and advanced fibrosis among patients with T2DM, by regions of the world. We searched for terms including NAFLD, NASH and T2DM in studies published from January 1989 to September 2018, using PubMed, Ovid MEDLINE®, EMBASE and Web of Science. Strict exclusion criteria were applied. Regional and global mean prevalence weighted by population size in each country were estimated and pooled using random-effects meta-analysis. Potential sources of heterogeneity were investigated using stratified meta-analysis and meta-regression. Among 80 studies from 20 countries that met our inclusion criteria, there were 49,419 individuals with T2DM (mean age 58.5 years, mean body mass index 27.9 kg/m2, and males 52.9%). The global prevalence of NAFLD among patients with T2DM was 55.5% (95% CI 47.3–63.7). Studies from Europe reported the highest prevalence (68.0% [62.1–73.0%]). Among 10 studies that estimated the prevalence of NASH, the global prevalence of NASH among individuals with T2DM was 37.3% (95% CI 24.7–50.0%). Seven studies estimated the prevalence of advanced fibrosis in patients with NAFLD and T2DM to be 17.0% (95% CI 7.2–34.8). Meta-regression models showed that geographic region and mean age (p <0.5) were associated with the prevalence of NAFLD, jointly accounting for 63.9% of the heterogeneity. This study provides the global prevalence rates for NAFLD, NASH, and advanced fibrosis in patients with T2DM. These data can be used to estimate the clinical and economic burden of NASH in patients with T2DM around the world. Non-alcoholic fatty liver disease (NAFLD) is the most common cause of chronic liver disease worldwide, with a global prevalence of 25.2%.1NAFLD is defined by the presence of hepatic steatosis, detected either by imaging or histology, and a lack of secondary causes of hepatic fat accumulation (i.e. excessive alcohol consumption, steatogenic medication, or monogenic hereditary disorders).2
Early recurrence of hepatocellular carcinoma (HCC) after curative resection is common. However, the association between genetic mechanisms and early HCC recurrence, especially in Chinese patients, remains largely unknown. We performed whole-genome sequencing (49 cases), whole-exome sequencing (18 cases), and deep targeted sequencing (115 cases) on 182 primary HCC samples. Focusing on WNK2, we used Sanger sequencing and qPCR to evaluate all the coding exons and copy numbers of that gene in an additional 554 HCC samples. We also explored the functional effect and mechanism of WNK2 on tumor growth and metastasis. We identified 5 genes (WNK2, RUNX1T1, CTNNB1, TSC1, and TP53) harboring somatic mutations that correlated with early tumor recurrence after curative resection in 182 primary HCC samples. Focusing on WNK2, the overall somatic mutation and copy number loss occurred in 5.3% (39/736) and 27.2% (200/736), respectively, of the total 736 HCC samples. Both types of variation were associated with lower WNK2 protein levels, higher rates of early tumor recurrence, and shorter overall survival. Biofunctional investigations revealed a tumor-suppressor role of WNK2: its inactivation led to ERK1/2 signaling activation in HCC cells, tumor-associated macrophage infiltration, and tumor growth and metastasis. Our results delineate genomic events that characterize Chinese HCCs and identify WNK2 as a driver of early HCC recurrence after curative resection. Hepatocellular carcinoma (HCC) is a relatively common type of cancer with rising incidence and mortality rate.1Approximately half of the 782,500 new cases of liver cancer and the 745,500 deaths due to liver cancer that occur worldwide each year occur in China.1In China, most HCCs arise from hepatitis B,2 other risk factors such as exposures to aflatoxin3 and aristolochic acids4 can also be important in HCC development.Those etiological factors may result in different genetic alterations, as well as different therapeutic targets, in Chinese patients compared with patients from other countries and regions.
Early recurrence of hepatocellular carcinoma (HCC) after curative resection is common. However, the association between genetic mechanisms and early HCC recurrence, especially in Chinese patients, remains largely unknown. We performed whole-genome sequencing (49 cases), whole-exome sequencing (18 cases), and deep targeted sequencing (115 cases) on 182 primary HCC samples. Focusing on WNK2, we used Sanger sequencing and qPCR to evaluate all the coding exons and copy numbers of that gene in an additional 554 HCC samples. We also explored the functional effect and mechanism of WNK2 on tumor growth and metastasis. We identified 5 genes (WNK2, RUNX1T1, CTNNB1, TSC1, and TP53) harboring somatic mutations that correlated with early tumor recurrence after curative resection in 182 primary HCC samples. Focusing on WNK2, the overall somatic mutation and copy number loss occurred in 5.3% (39/736) and 27.2% (200/736), respectively, of the total 736 HCC samples. Both types of variation were associated with lower WNK2 protein levels, higher rates of early tumor recurrence, and shorter overall survival. Biofunctional investigations revealed a tumor-suppressor role of WNK2: its inactivation led to ERK1/2 signaling activation in HCC cells, tumor-associated macrophage infiltration, and tumor growth and metastasis. Our results delineate genomic events that characterize Chinese HCCs and identify WNK2 as a driver of early HCC recurrence after curative resection. Over the past decade, new discoveries have shed light on the molecular basis of HCC pathogenesis.Following technological advances, several pioneering studies have delineated the genetic landscape underlying liver carcinogenesis,5–11 including amplifications on chromosomes 6p21 (VEGFA) and 11q13 (FGF19/CNND1) and deletions on chromosome 9 (CDKN2A).Mutations in the coding regions of TP53 and CTNNB1 affect 25–30% of patients with HCC and, along with low-frequency mutations in some other genes (e.g., AXIN1, ARID2, ARID1A, TSC1/TSC2), define core pathways that are commonly de-regulated in HCC.Many genomic changes related to HCC have been characterized in Western and Japanese populations; however, the genomic landscape of HCC in the Chinese population has only been investigated in small patient cohorts, either with specified aflatoxin-associated HCC3 or with low sequencing depth.9To gain a comprehensive understanding of the molecular etiology of HCC, it is essential that we learn more about the genomic changes associated with HCC in the Chinese population.
Early recurrence of hepatocellular carcinoma (HCC) after curative resection is common. However, the association between genetic mechanisms and early HCC recurrence, especially in Chinese patients, remains largely unknown. We performed whole-genome sequencing (49 cases), whole-exome sequencing (18 cases), and deep targeted sequencing (115 cases) on 182 primary HCC samples. Focusing on WNK2, we used Sanger sequencing and qPCR to evaluate all the coding exons and copy numbers of that gene in an additional 554 HCC samples. We also explored the functional effect and mechanism of WNK2 on tumor growth and metastasis. We identified 5 genes (WNK2, RUNX1T1, CTNNB1, TSC1, and TP53) harboring somatic mutations that correlated with early tumor recurrence after curative resection in 182 primary HCC samples. Focusing on WNK2, the overall somatic mutation and copy number loss occurred in 5.3% (39/736) and 27.2% (200/736), respectively, of the total 736 HCC samples. Both types of variation were associated with lower WNK2 protein levels, higher rates of early tumor recurrence, and shorter overall survival. Biofunctional investigations revealed a tumor-suppressor role of WNK2: its inactivation led to ERK1/2 signaling activation in HCC cells, tumor-associated macrophage infiltration, and tumor growth and metastasis. Our results delineate genomic events that characterize Chinese HCCs and identify WNK2 as a driver of early HCC recurrence after curative resection. Advances in the treatment and management of patients with HCC have improved survival rates; however, HCC still has a high rate of early recurrence, which limits long-term survival even after surgical resection.12–14Although several studies have revealed biomarkers that shed light on the molecular mechanisms underlying HCC progression and prognosis,15 the association between genetic mechanisms and early HCC recurrence, especially in Chinese patients, remains largely unknown.
Endoplasmic reticulum aminopeptidase 1 (ERAP1) polymorphisms are linked with human leukocyte antigen (HLA) class I-associated autoinflammatory disorders, including ankylosing spondylitis and Behçet’s disease. Disease-associated ERAP1 allotypes exhibit distinct functional properties, but it remains unclear how differential peptide trimming in vivo affects the repertoire of epitopes presented to CD8+ T cells. The aim of this study was to determine the impact of ERAP1 allotypes on the virus-specific CD8+ T cell epitope repertoire in an HLA-B*27:05+ individual with acute hepatitis C virus (HCV) infection. We performed genetic and functional analyses of ERAP1 allotypes and characterized the HCV-specific CD8+ T cell repertoire at the level of fine epitope specificity and HLA class I restriction, in a patient who had acquired an HCV genotype 1a infection through a needle-stick injury. Two hypoactive allotypic variants of ERAP1 were identified in an individual with acute HCV infection. The associated repertoire of virus-derived epitopes recognized by CD8+ T cells was uncommon in a couple of respects. Firstly, reactivity was directed away from classically immunodominant epitopes, preferentially targeting either novel or subdominant epitopes. Secondly, reactivity was biased towards longer epitopes (10–11-mers). Despite the patient exhibiting favorable prognostic indicators, these atypical immune responses failed to clear the virus and the patient developed persistent low-level infection with HCV. ERAP1 allotypes modify the virus-specific CD8+ T cell epitope repertoire in vivo, leading to altered immunodominance patterns that may contribute to the failure of antiviral immunity after infection with HCV. Endoplasmic reticulum aminopeptidase 1 (ERAP1) trims peptides to an optimal length (usually 8 or 9 amino acids) for presentation in the context of human leukocyte antigen (HLA) class I molecules.1,2Genome-wide association studies have identified single nucleotide polymorphisms (SNPs) in ERAP1 as important risk factors in several HLA class I-associated autoinflammatory disorders,3 including ankylosing spondylitis,4 especially in conjunction with HLA-B*27,5 and Behçet’s disease, which is strongly linked with HLA-B*51.6In addition, SNPs in ERAP1 can combine to encode discrete allotypes with composite functional properties that further increase the risk of disease.7Biochemical analyses have revealed differential peptide trimming among ERAP1 allotypes, with hypoactive forms typically generating longer fragments (10–12-mers), and hyperactive forms typically generating shorter fragments (7–8-mers).8–10However, the impact of specific ERAP1 allotypes on the naturally presented repertoire of peptide epitopes is currently unclear.11
Endoplasmic reticulum aminopeptidase 1 (ERAP1) polymorphisms are linked with human leukocyte antigen (HLA) class I-associated autoinflammatory disorders, including ankylosing spondylitis and Behçet’s disease. Disease-associated ERAP1 allotypes exhibit distinct functional properties, but it remains unclear how differential peptide trimming in vivo affects the repertoire of epitopes presented to CD8+ T cells. The aim of this study was to determine the impact of ERAP1 allotypes on the virus-specific CD8+ T cell epitope repertoire in an HLA-B*27:05+ individual with acute hepatitis C virus (HCV) infection. We performed genetic and functional analyses of ERAP1 allotypes and characterized the HCV-specific CD8+ T cell repertoire at the level of fine epitope specificity and HLA class I restriction, in a patient who had acquired an HCV genotype 1a infection through a needle-stick injury. Two hypoactive allotypic variants of ERAP1 were identified in an individual with acute HCV infection. The associated repertoire of virus-derived epitopes recognized by CD8+ T cells was uncommon in a couple of respects. Firstly, reactivity was directed away from classically immunodominant epitopes, preferentially targeting either novel or subdominant epitopes. Secondly, reactivity was biased towards longer epitopes (10–11-mers). Despite the patient exhibiting favorable prognostic indicators, these atypical immune responses failed to clear the virus and the patient developed persistent low-level infection with HCV. ERAP1 allotypes modify the virus-specific CD8+ T cell epitope repertoire in vivo, leading to altered immunodominance patterns that may contribute to the failure of antiviral immunity after infection with HCV. Hepatitis C virus (HCV) has infected approximately 71 million people worldwide.Persistent infection ensues in 50–80% of cases, with a high risk of liver disease, which may progress to cirrhosis, liver failure, and hepatocellular carcinoma.12Virus-specific CD8+ T cells are thought to play a key role in immune protection against HCV.13This consensus is predicated on associative studies that have linked viral clearance with potent and broadly directed CD8+ T cell responses and the presence of certain HLA class I allotypes.14In particular, up to 80% of HLA-B*27+ individuals clear HCV spontaneously, contrasting with only 20–50% of individuals in the general population.15We previously attributed this protective effect to the immunodominant HLA-B*27-restricted CD8+ T cell epitope NS5B2841-2849 (ARMILMTHF), which is targeted in almost all HLA-B*27+ individuals during acute infection with HCV.16,17
Alcoholic liver disease (ALD) is characterized by gut dysbiosis and increased gut permeability. Hypoxia inducible factor 1α (HIF-1α) has been implicated in transcriptional regulation of intestinal barrier integrity and inflammation. We aimed to test the hypothesis that HIF-1α plays a critical role in gut microbiota homeostasis and the maintenance of intestinal barrier integrity in a mouse model of ALD. Wild-type (WT) and intestinal epithelial-specific Hif1a knockout mice (IEhif1α−/−) were pair-fed modified Lieber-DeCarli liquid diet containing 5% (w/v) alcohol or isocaloric maltose dextrin for 24 days. Serum levels of alanine aminotransferase and endotoxin were determined. Fecal microbiota were assessed. Liver steatosis and injury, and intestinal barrier integrity were evaluated. Alcohol feeding increased serum levels of alanine aminotransferase and lipopolysaccharide, hepatic triglyceride concentration, and liver injury in the WT mice. These deleterious effects were exaggerated in IEhif1α−/− mice. Alcohol exposure resulted in greater reduction of the expression of intestinal epithelial tight junction proteins, claudin-1 and occludin, in IEhif1α−/− mice. In addition, cathelicidin-related antimicrobial peptide and intestinal trefoil factor were further decreased by alcohol in IEhif1α−/− mice. Metagenomic analysis showed increased gut dysbiosis and significantly decreased Firmicutes/Bacteroidetes ratio in IEhif1α−/− mice compared to the WT mice exposed to alcohol. An increased abundance of Akkermansia and a decreased level of Lactobacillus in IEhif1α−/− mice were also observed. Non-absorbable antibiotic treatment reversed the liver steatosis in both WT and IEhif1α−/− mice. Intestinal HIF-1α is essential for the adaptative response to alcohol-induced changes in intestinal microbiota and barrier function associated with elevated endotoxemia and hepatic steatosis and injury. Alcoholic liver disease (ALD) ranges from hepatic steatosis to steatohepatitis, cirrhosis, and, potentially, hepatocellular carcinoma.1While alcohol induces deleterious effects in the liver, it also disrupts gut microbiota homeostasis and intestinal epithelial integrity resulting in increased permeability, bacterial translocation and release of bacteria-derived endotoxin into the circulation.2–4Clinical and experimental studies have demonstrated that serum levels of lipopolysaccharide (LPS) are indeed increased in alcoholic individuals.5,6LPS binding to Toll-like receptors on the surface of Kupffer cells leads to an elevated pro-inflammatory cytokine production, which, in turn, damages hepatocyte function.7,8
Alcoholic liver disease (ALD) is characterized by gut dysbiosis and increased gut permeability. Hypoxia inducible factor 1α (HIF-1α) has been implicated in transcriptional regulation of intestinal barrier integrity and inflammation. We aimed to test the hypothesis that HIF-1α plays a critical role in gut microbiota homeostasis and the maintenance of intestinal barrier integrity in a mouse model of ALD. Wild-type (WT) and intestinal epithelial-specific Hif1a knockout mice (IEhif1α−/−) were pair-fed modified Lieber-DeCarli liquid diet containing 5% (w/v) alcohol or isocaloric maltose dextrin for 24 days. Serum levels of alanine aminotransferase and endotoxin were determined. Fecal microbiota were assessed. Liver steatosis and injury, and intestinal barrier integrity were evaluated. Alcohol feeding increased serum levels of alanine aminotransferase and lipopolysaccharide, hepatic triglyceride concentration, and liver injury in the WT mice. These deleterious effects were exaggerated in IEhif1α−/− mice. Alcohol exposure resulted in greater reduction of the expression of intestinal epithelial tight junction proteins, claudin-1 and occludin, in IEhif1α−/− mice. In addition, cathelicidin-related antimicrobial peptide and intestinal trefoil factor were further decreased by alcohol in IEhif1α−/− mice. Metagenomic analysis showed increased gut dysbiosis and significantly decreased Firmicutes/Bacteroidetes ratio in IEhif1α−/− mice compared to the WT mice exposed to alcohol. An increased abundance of Akkermansia and a decreased level of Lactobacillus in IEhif1α−/− mice were also observed. Non-absorbable antibiotic treatment reversed the liver steatosis in both WT and IEhif1α−/− mice. Intestinal HIF-1α is essential for the adaptative response to alcohol-induced changes in intestinal microbiota and barrier function associated with elevated endotoxemia and hepatic steatosis and injury. Intestinal barrier function is key to preventing the increased bacterial translocation by alcohol.5,9,10Goblet cells in the intestinal epithelium produce protective trefoil factors and mucins, which are abundantly core glycosylated and either localized to the cell membrane or secreted into the lumen to form the mucus layer.11,12The mucus is the first barrier that intestinal bacteria meet, and pathogens must penetrate it to reach the epithelial cells.13–15Intestinal Paneth cells produce antimicrobial peptides that are retained in the mucus layer, thereby concentrating their bactericidal activity close to the epithelium.Together, Paneth and Goblet cells prevent bacterial invasion to some extent, and thereby play a prominent role in the innate immune surveillance of the gut.12The paracellular space in the intestinal epithelium is sealed by tight junctions (TJ), which regulate the flow of small molecules through the composition of claudins and other proteins in the junctional complex.16IgA production and other intestinal immune-regulations provide protection against bacterial transcytosis.17Recent studies demonstrate that targeting intestinal barrier function may be an effective strategy in the prevention/treatment of ALD.18
Alcoholic liver disease (ALD) is characterized by gut dysbiosis and increased gut permeability. Hypoxia inducible factor 1α (HIF-1α) has been implicated in transcriptional regulation of intestinal barrier integrity and inflammation. We aimed to test the hypothesis that HIF-1α plays a critical role in gut microbiota homeostasis and the maintenance of intestinal barrier integrity in a mouse model of ALD. Wild-type (WT) and intestinal epithelial-specific Hif1a knockout mice (IEhif1α−/−) were pair-fed modified Lieber-DeCarli liquid diet containing 5% (w/v) alcohol or isocaloric maltose dextrin for 24 days. Serum levels of alanine aminotransferase and endotoxin were determined. Fecal microbiota were assessed. Liver steatosis and injury, and intestinal barrier integrity were evaluated. Alcohol feeding increased serum levels of alanine aminotransferase and lipopolysaccharide, hepatic triglyceride concentration, and liver injury in the WT mice. These deleterious effects were exaggerated in IEhif1α−/− mice. Alcohol exposure resulted in greater reduction of the expression of intestinal epithelial tight junction proteins, claudin-1 and occludin, in IEhif1α−/− mice. In addition, cathelicidin-related antimicrobial peptide and intestinal trefoil factor were further decreased by alcohol in IEhif1α−/− mice. Metagenomic analysis showed increased gut dysbiosis and significantly decreased Firmicutes/Bacteroidetes ratio in IEhif1α−/− mice compared to the WT mice exposed to alcohol. An increased abundance of Akkermansia and a decreased level of Lactobacillus in IEhif1α−/− mice were also observed. Non-absorbable antibiotic treatment reversed the liver steatosis in both WT and IEhif1α−/− mice. Intestinal HIF-1α is essential for the adaptative response to alcohol-induced changes in intestinal microbiota and barrier function associated with elevated endotoxemia and hepatic steatosis and injury. The intestines are supported by an extensive underlying vasculature, and are therefore susceptible to conditions related to diminished blood flow and concomitant tissue hypoxia.19,20It is now clear that responses to hypoxia include transcriptionally regulated gene expression coordinated by the transcription factor hypoxia-inducible factor (HIF).21HIF-1α and HIF-2α have been implicated in transcriptional regulation of anti-inflammatory or tissue-protective-signaling pathways.22–24A number of barrier-protective genes are critically regulated by HIF-1α, including intestinal trefoil factor (ITF [TFF3]), CD73 (NT5E), P-glycoprotein (P-gp [ABCB1]), cathelicidins, claudin-1, mucin-3 and β-defensin-1 (DEFB1).19,25–30While hypoxia is an important factor for HIF-1α protein stabilization, there are also a number of factors other than hypoxia, such as LPS, that regulate HIF-1α expression and accumulation.31While acute LPS increased HIF-1α, prolonged exposure supressed HIF-α expression.32
Alcoholic liver disease (ALD) is characterized by gut dysbiosis and increased gut permeability. Hypoxia inducible factor 1α (HIF-1α) has been implicated in transcriptional regulation of intestinal barrier integrity and inflammation. We aimed to test the hypothesis that HIF-1α plays a critical role in gut microbiota homeostasis and the maintenance of intestinal barrier integrity in a mouse model of ALD. Wild-type (WT) and intestinal epithelial-specific Hif1a knockout mice (IEhif1α−/−) were pair-fed modified Lieber-DeCarli liquid diet containing 5% (w/v) alcohol or isocaloric maltose dextrin for 24 days. Serum levels of alanine aminotransferase and endotoxin were determined. Fecal microbiota were assessed. Liver steatosis and injury, and intestinal barrier integrity were evaluated. Alcohol feeding increased serum levels of alanine aminotransferase and lipopolysaccharide, hepatic triglyceride concentration, and liver injury in the WT mice. These deleterious effects were exaggerated in IEhif1α−/− mice. Alcohol exposure resulted in greater reduction of the expression of intestinal epithelial tight junction proteins, claudin-1 and occludin, in IEhif1α−/− mice. In addition, cathelicidin-related antimicrobial peptide and intestinal trefoil factor were further decreased by alcohol in IEhif1α−/− mice. Metagenomic analysis showed increased gut dysbiosis and significantly decreased Firmicutes/Bacteroidetes ratio in IEhif1α−/− mice compared to the WT mice exposed to alcohol. An increased abundance of Akkermansia and a decreased level of Lactobacillus in IEhif1α−/− mice were also observed. Non-absorbable antibiotic treatment reversed the liver steatosis in both WT and IEhif1α−/− mice. Intestinal HIF-1α is essential for the adaptative response to alcohol-induced changes in intestinal microbiota and barrier function associated with elevated endotoxemia and hepatic steatosis and injury. Disruption of epithelial HIF-1α resulted in an increased epithelial permeability, as shown in murine colitis models.33–36Our previous study showed that long-term exposure to alcohol decreased epithelial HIF-2α protein expression in mice.37We also demonstrated that knockdown of HIF1/2α resulted in increased permeability and decreased transepithelial electrical resistance in Caco-2 cells.37,38
Idarubicin shows high cytotoxicity against hepatocellular carcinoma (HCC) cells, a high hepatic extraction ratio, and high lipophilicity leading to stable emulsions with lipiodol. A dose-escalation phase I trial of idarubicin_lipiodol (without embolisation) was conducted in patients with cirrhotic HCC to estimate the maximum-tolerated dose (MTD) and to assess the safety, efficacy, and pharmacokinetics of the drug, and the health-related quality of life achieved by patients. Patients underwent two sessions of treatment with a transarterial idarubicin_lipiodol emulsion without embolisation. The idarubicin dose was escalated according to a modified continuous reassessment method. The MTD was defined as the dose closest to that causing dose-limiting toxicity (DLT) in 20% of patients. A group of 15 patients were enrolled, including one patient at 10 mg, four patients at 15 mg, seven patients at 20 mg, and three patients at 25 mg. Only two patients experienced DLT: oedematous ascitic decompensation and abdominal pain at 20 and 25 mg, respectively. The calculated MTD of idarubicin was 20 mg. The most frequent grade ≥3 adverse events were biological. One month after the second session, the objective response rate was 29% (complete response, 0%; partial response, 29%) based on modified Response Evaluation Criteria In Solid Tumours. The median time to progression was 5.4 months [95% confidence limit (CI) 3.0–14.6 months] and median overall survival was 20.6 months (95% CI 5.7–28.7 months). Pharmacokinetic analysis of idarubicin showed that the mean Cmax of idarubicin after intra-arterial injection of the idarubicin-lipiodol emulsion is approximately half the Cmax after intravenous administration. Health-related quality of life results confirmed the good safety results associated with use of the drug. The MTD of idarubicin was 20 mg after two chemolipiodolisation sessions. Encouraging safety results, and patient responses and survival were observed. A phase II trial has been scheduled. Transarterial chemoembolisation (TACE) is the standard of care for unresectable intermediate-stage hepatocellular carcinoma (HCC).1Although TACE has been widely used for several years, the procedure varies considerably across centres and interventional radiologists, especially regarding chemotherapeutic agents, doses, drug-releasing vectors, and embolisation agents.2This heterogeneity is explained by the absence of proof to clarify the mechanism by which TACE improves patient survival.Some clinicians believe that embolisation has a major role and that adding a chemotherapeutic agent does not improve efficacy.Indeed, six randomised trials failed to demonstrate the superiority of TACE over bland embolisation.3–8Others believe that the locally delivered chemotherapeutic agent has a major role and that embolisation is associated with greater harm than benefit.Three randomised trials failed to demonstrate the superiority of TACE over chemolipiodolisation (transarterial injection of a chemotherapeutic agent emulsified with lipiodol) in patients with unresectable HCC.9–11A large randomised study comparing three groups of patients with unresectable HCC receiving triple-drug chemolipiodolisation with (arm 1, n = 122) or without embolisation (arm 2, n = 121) or single-drug chemolipiodolisation with embolisation (arm 3, n = 122) showed significantly better survival in patients receiving triple-drug chemolipiodolisation,12 thereby highlighting the major role of the drugs in the efficacy of the procedure.
Idarubicin shows high cytotoxicity against hepatocellular carcinoma (HCC) cells, a high hepatic extraction ratio, and high lipophilicity leading to stable emulsions with lipiodol. A dose-escalation phase I trial of idarubicin_lipiodol (without embolisation) was conducted in patients with cirrhotic HCC to estimate the maximum-tolerated dose (MTD) and to assess the safety, efficacy, and pharmacokinetics of the drug, and the health-related quality of life achieved by patients. Patients underwent two sessions of treatment with a transarterial idarubicin_lipiodol emulsion without embolisation. The idarubicin dose was escalated according to a modified continuous reassessment method. The MTD was defined as the dose closest to that causing dose-limiting toxicity (DLT) in 20% of patients. A group of 15 patients were enrolled, including one patient at 10 mg, four patients at 15 mg, seven patients at 20 mg, and three patients at 25 mg. Only two patients experienced DLT: oedematous ascitic decompensation and abdominal pain at 20 and 25 mg, respectively. The calculated MTD of idarubicin was 20 mg. The most frequent grade ≥3 adverse events were biological. One month after the second session, the objective response rate was 29% (complete response, 0%; partial response, 29%) based on modified Response Evaluation Criteria In Solid Tumours. The median time to progression was 5.4 months [95% confidence limit (CI) 3.0–14.6 months] and median overall survival was 20.6 months (95% CI 5.7–28.7 months). Pharmacokinetic analysis of idarubicin showed that the mean Cmax of idarubicin after intra-arterial injection of the idarubicin-lipiodol emulsion is approximately half the Cmax after intravenous administration. Health-related quality of life results confirmed the good safety results associated with use of the drug. The MTD of idarubicin was 20 mg after two chemolipiodolisation sessions. Encouraging safety results, and patient responses and survival were observed. A phase II trial has been scheduled. In oncology, chemotherapeutic agents are usually administered every 2–3 weeks to limit tumour repopulation between each cycle while preserving an acceptable toxicity profile.13,14By contrast, TACE is usually administered either every 4–8 weeks to part of the liver or ‘on demand’.15Thus, liver tumours might receive the drug incorporated in TACE every 8–16 weeks or longer, in contrast to the previously mentioned rationale.
Idarubicin shows high cytotoxicity against hepatocellular carcinoma (HCC) cells, a high hepatic extraction ratio, and high lipophilicity leading to stable emulsions with lipiodol. A dose-escalation phase I trial of idarubicin_lipiodol (without embolisation) was conducted in patients with cirrhotic HCC to estimate the maximum-tolerated dose (MTD) and to assess the safety, efficacy, and pharmacokinetics of the drug, and the health-related quality of life achieved by patients. Patients underwent two sessions of treatment with a transarterial idarubicin_lipiodol emulsion without embolisation. The idarubicin dose was escalated according to a modified continuous reassessment method. The MTD was defined as the dose closest to that causing dose-limiting toxicity (DLT) in 20% of patients. A group of 15 patients were enrolled, including one patient at 10 mg, four patients at 15 mg, seven patients at 20 mg, and three patients at 25 mg. Only two patients experienced DLT: oedematous ascitic decompensation and abdominal pain at 20 and 25 mg, respectively. The calculated MTD of idarubicin was 20 mg. The most frequent grade ≥3 adverse events were biological. One month after the second session, the objective response rate was 29% (complete response, 0%; partial response, 29%) based on modified Response Evaluation Criteria In Solid Tumours. The median time to progression was 5.4 months [95% confidence limit (CI) 3.0–14.6 months] and median overall survival was 20.6 months (95% CI 5.7–28.7 months). Pharmacokinetic analysis of idarubicin showed that the mean Cmax of idarubicin after intra-arterial injection of the idarubicin-lipiodol emulsion is approximately half the Cmax after intravenous administration. Health-related quality of life results confirmed the good safety results associated with use of the drug. The MTD of idarubicin was 20 mg after two chemolipiodolisation sessions. Encouraging safety results, and patient responses and survival were observed. A phase II trial has been scheduled. Idarubicin, an anthracycline characterised by its high cytotoxicity against HCC cells,16 high hepatic extraction ratio (40% of the injected dose distributed in the liver),17 and high lipophilicity leading to stable emulsions with lipiodol,17 appears to be a suitable candidate for transarterial liver therapies.We hypothesised that the use of idarubicin combined with lipiodol would make embolisation unnecessary, provided that idarubicin_lipiodol is administered every three weeks to limit tumour repopulation.14
Yttrium-90 transarterial radioembolization (TARE) has shown promising efficacy in the treatment of patients with hepatocellular carcinoma (HCC), associated with portal vein tumor thrombus (PVTT). The aim of this study is to identify prognostic factors for survival in patients with HCC and PVTT undergoing TARE, and build a prognostic classification for these patients. This is a single center retrospective study conducted over six years (2010–2015), on consecutive patients undergoing TARE. Patients were included if they met the following criteria: presence of at least one measurable HCC, presence of PVTT not occluding the main portal trunk, absence of extrahepatic metastases, Child-Pugh score within B7, Eastern Cooperative Oncology Group performance status 0–1. Uni- and multivariable analysis was used to explore the variables that showed an independent relationship with survival. A prognostic score was then derived, and three prognostic categories were identified. A total of 120 patients were included in the study. Median overall survival (OS) was 14.1 months (95% CI 10.7–17.5) and median progression-free survival (PFS) was 6.5 months (95% CI 3.8–9.2). The only variables independently correlated with OS were bilirubin, extension of PVTT and tumor burden. Three prognostic categories were identified: favourable prognosis (0 points), intermediate prognosis (2–3 points) and dismal prognosis (>3 points). Median OS in the three categories was 32.2 months, 14.9 months and 7.8 months respectively (p <0.0001). PFS (p = 0.045) and the risk of liver decompensation (p <0.0001) also significantly differed along the same prognostic categories. Radioembolization with Yttrium-90 is an effective therapy for patients with HCC and PVTT. The proposed prognostic stratification may help to better identify good candidates for the treatment, and those for whom TARE may be futile. Hepatocellular carcinoma (HCC) is a global health problem and one of the leading causes of cancer-related death especially in patients with cirrhosis.1,2By reason of the improvements in surveillance protocols, diagnostic tools and therapeutic armamentaria, diagnosis of early HCC is feasible in 30–60% of cases.3However, a substantial proportion of patients still present with portal vein tumor thrombus (PVTT) either at onset of the disease or as a result of HCC recurrence or progression, leading to an advanced stage of the disease not amenable to curative treatments.4Patients with HCC and PVTT may have an asymptomatic presentation, although in most instances they have a significant degree of synthetic dysfunction and an impending liver decompensation that precludes any attempt at surgical cure.Moreover, when the portal circulation is compromised by thrombosis, transarterial embolo-therapies may increase the risk of liver failure; therefore, the presence of PVTT is generally considered a contraindication to transarterial chemoembolization (TACE).5,6
Yttrium-90 transarterial radioembolization (TARE) has shown promising efficacy in the treatment of patients with hepatocellular carcinoma (HCC), associated with portal vein tumor thrombus (PVTT). The aim of this study is to identify prognostic factors for survival in patients with HCC and PVTT undergoing TARE, and build a prognostic classification for these patients. This is a single center retrospective study conducted over six years (2010–2015), on consecutive patients undergoing TARE. Patients were included if they met the following criteria: presence of at least one measurable HCC, presence of PVTT not occluding the main portal trunk, absence of extrahepatic metastases, Child-Pugh score within B7, Eastern Cooperative Oncology Group performance status 0–1. Uni- and multivariable analysis was used to explore the variables that showed an independent relationship with survival. A prognostic score was then derived, and three prognostic categories were identified. A total of 120 patients were included in the study. Median overall survival (OS) was 14.1 months (95% CI 10.7–17.5) and median progression-free survival (PFS) was 6.5 months (95% CI 3.8–9.2). The only variables independently correlated with OS were bilirubin, extension of PVTT and tumor burden. Three prognostic categories were identified: favourable prognosis (0 points), intermediate prognosis (2–3 points) and dismal prognosis (>3 points). Median OS in the three categories was 32.2 months, 14.9 months and 7.8 months respectively (p <0.0001). PFS (p = 0.045) and the risk of liver decompensation (p <0.0001) also significantly differed along the same prognostic categories. Radioembolization with Yttrium-90 is an effective therapy for patients with HCC and PVTT. The proposed prognostic stratification may help to better identify good candidates for the treatment, and those for whom TARE may be futile. Two pivotal phase III trials have demonstrated a survival advantage for patients with advanced HCC treated with the oral multi-tyrosine kinase inhibitor sorafenib7,8 compared to placebo, and a subgroup analysis has confirmed this result in patients with PVTT.9,10Sorafenib has therefore been recognized as the standard of care for the treatment of advanced HCC by the American Association for the Study of Liver Diseases (AASLD),11 the European Association for the Study of the Liver (EASL)5 and the Asian Pacific Association for the Study of the Liver (APASL).12Nevertheless, sustained responses to sorafenib are rare, median survival in patients with advanced HCC remains limited to 6.5–10.7 months, and the treatment itself is associated with side effects that frequently lead to early treatment interruption.7,8,13,14
Yttrium-90 transarterial radioembolization (TARE) has shown promising efficacy in the treatment of patients with hepatocellular carcinoma (HCC), associated with portal vein tumor thrombus (PVTT). The aim of this study is to identify prognostic factors for survival in patients with HCC and PVTT undergoing TARE, and build a prognostic classification for these patients. This is a single center retrospective study conducted over six years (2010–2015), on consecutive patients undergoing TARE. Patients were included if they met the following criteria: presence of at least one measurable HCC, presence of PVTT not occluding the main portal trunk, absence of extrahepatic metastases, Child-Pugh score within B7, Eastern Cooperative Oncology Group performance status 0–1. Uni- and multivariable analysis was used to explore the variables that showed an independent relationship with survival. A prognostic score was then derived, and three prognostic categories were identified. A total of 120 patients were included in the study. Median overall survival (OS) was 14.1 months (95% CI 10.7–17.5) and median progression-free survival (PFS) was 6.5 months (95% CI 3.8–9.2). The only variables independently correlated with OS were bilirubin, extension of PVTT and tumor burden. Three prognostic categories were identified: favourable prognosis (0 points), intermediate prognosis (2–3 points) and dismal prognosis (>3 points). Median OS in the three categories was 32.2 months, 14.9 months and 7.8 months respectively (p <0.0001). PFS (p = 0.045) and the risk of liver decompensation (p <0.0001) also significantly differed along the same prognostic categories. Radioembolization with Yttrium-90 is an effective therapy for patients with HCC and PVTT. The proposed prognostic stratification may help to better identify good candidates for the treatment, and those for whom TARE may be futile. Transarterial radioembolization (TARE) is a form of brachytherapy performed by selective intra-arterial injection of microspheres loaded with Yttrium-90 (Y90): a pure beta-emitter characterized by limited penetration, that enables selective tumor treatment with limited damage to the surrounding tissue.15Unlike other transarterial therapies, TARE is a microembolic procedure that minimizes alterations of the hepatic arterial flow:16 several large series on TARE for patients with HCC and PVTT, mostly retrospective, showed an acceptable safety profile and good results in terms of local control of the disease.17–21Only recently, two phase III randomized clinical trials (RCT) comparing TARE to the standard of care sorafenib in the population of patients with locally advanced HCC, have been made accessible.22,23In both trials the primary endpoint was not met, since TARE did not provide a significant gain in survival compared to sorafenib in the overall population nor in the subgroup of patients with PVTT.Incidentally, both studies clearly showed a significantly better treatment tolerance and quality of life in patients undergoing TARE.
Prolactin (PRL) is a multifunctional polypeptide with effects on metabolism, however, little is known about its effect on hepatic steatosis and lipid metabolism. Herein, we aimed to assess the role of PRL in the development of non-alcoholic fatty liver disease (NAFLD). The serum PRL levels of 456 patients with NAFLD, 403 controls without NAFLD diagnosed by ultrasound, and 85 individuals with liver histology obtained during metabolic surgery (44 female and 30 male patients with NAFLD and 11 age-matched non-NAFLD female individuals) were evaluated. The expression of the gene encoding the prolactin receptor (PRLR) and signalling molecules involved in hepatic lipid metabolism were evaluated in human liver and HepG2 cells. The effects of overexpression of PRLR or fatty acid translocase (FAT)/CD36 or knockdown of PRLR on hepatic lipid metabolism were tested in free fatty acid (FFA)-treated HepG2 cells. Circulating PRL levels were lower in individuals with ultrasound-diagnosed NAFLD (men: 7.9 [range, 5.9–10.3] µg/L; women: 8.7 [range, 6.1–12.4] µg/L) than those with non-NAFLD (men: 9.1 [range, 6.8–13.0] µg/L, p = 0.002; women: 11.6 [range, 8.2–16.1] µg/L, p <0.001). PRL levels in patients with biopsy-proven severe hepatic steatosis were lower compared with those with mild-to-moderate hepatic steatosis in both men (8.3 [range, 5.4–9.5] µg/L vs. 9.7 [range, 7.1–12.3] µg/L, p = 0.031) and women (8.5 [range, 4.2–10.6] µg/L vs. 9.8 [range, 8.2–15.7] µg/L, p = 0.027). Furthermore, hepatic PRLR gene expression was significantly reduced in patients with NAFLD and negatively correlated with CD36 gene expression. In FFA-induced HepG2 cells, PRL treatment or PRLR overexpression significantly reduced the expression of CD36 and lipid content, effects that were abrogated after silencing of PRLR. Furthermore, overexpression of CD36 significantly reduced the PRL-mediated improvement in lipid content. Our results reveal a novel association between the central nervous system and the liver, whereby PRL/PRLR improved hepatic lipid accumulation via the CD36 pathway. . Non-alcoholic fatty liver disease (NAFLD) is a common public health problem affecting up to 25% of adults around the world1 and is associated with a series of metabolic co-morbidities.NAFLD is initially caused by an imbalance between lipid demand and supply; however, the pathogenesis of the disease also involves crosstalk between liver and extrahepatic organs, including adipose tissue and the central nervous system (CNS).2By sensing and integrating peripheral signals (such as leptin), hypothalamic arcuate nucleus (ARC) neurons in the CNS can induce hepatic fatty acid oxidation.3By contrast, pituitary-released hormones, such as growth hormone, have been shown to suppress lipid uptake in liver.4,5
Prolactin (PRL) is a multifunctional polypeptide with effects on metabolism, however, little is known about its effect on hepatic steatosis and lipid metabolism. Herein, we aimed to assess the role of PRL in the development of non-alcoholic fatty liver disease (NAFLD). The serum PRL levels of 456 patients with NAFLD, 403 controls without NAFLD diagnosed by ultrasound, and 85 individuals with liver histology obtained during metabolic surgery (44 female and 30 male patients with NAFLD and 11 age-matched non-NAFLD female individuals) were evaluated. The expression of the gene encoding the prolactin receptor (PRLR) and signalling molecules involved in hepatic lipid metabolism were evaluated in human liver and HepG2 cells. The effects of overexpression of PRLR or fatty acid translocase (FAT)/CD36 or knockdown of PRLR on hepatic lipid metabolism were tested in free fatty acid (FFA)-treated HepG2 cells. Circulating PRL levels were lower in individuals with ultrasound-diagnosed NAFLD (men: 7.9 [range, 5.9–10.3] µg/L; women: 8.7 [range, 6.1–12.4] µg/L) than those with non-NAFLD (men: 9.1 [range, 6.8–13.0] µg/L, p = 0.002; women: 11.6 [range, 8.2–16.1] µg/L, p <0.001). PRL levels in patients with biopsy-proven severe hepatic steatosis were lower compared with those with mild-to-moderate hepatic steatosis in both men (8.3 [range, 5.4–9.5] µg/L vs. 9.7 [range, 7.1–12.3] µg/L, p = 0.031) and women (8.5 [range, 4.2–10.6] µg/L vs. 9.8 [range, 8.2–15.7] µg/L, p = 0.027). Furthermore, hepatic PRLR gene expression was significantly reduced in patients with NAFLD and negatively correlated with CD36 gene expression. In FFA-induced HepG2 cells, PRL treatment or PRLR overexpression significantly reduced the expression of CD36 and lipid content, effects that were abrogated after silencing of PRLR. Furthermore, overexpression of CD36 significantly reduced the PRL-mediated improvement in lipid content. Our results reveal a novel association between the central nervous system and the liver, whereby PRL/PRLR improved hepatic lipid accumulation via the CD36 pathway. . Prolactin (PRL), a polypeptide produced predominantly by the anterior pituitary gland, has potent stimulation effects in lactation and reproduction on binding to its cell surface receptors (PRLRs) in the mammary gland.6Nonetheless, PRLR is also expressed in metabolic tissues, such as liver, pancreas, and adipose tissue.7Previous studies have documented favourable roles of PRL/PRLR in metabolic homeostasis.Serum PRL levels in both genders were markedly decreased in patients with type 2 diabetes mellitus (T2DM).8In addition, they were inversely correlated with triglycerides (TG) in females with polycystic ovary syndrome9 and positively associated with high-density lipoprotein (HDL) in obese children.10PRL can inhibit the mRNA and protein expression of fatty acid synthase in adipocytes, revealing its anti-lipogenic function.11PRL can also upregulate the expression of PRLR in human adipose explants, through which PRL exerts its anti-lipolytic action.12Moreover, Prlr−/− mice displayed greater fat mass when fed a high-fat diet (HFD).13However, the role of PRL/PRLR in hepatic lipid metabolism is yet to be elucidated.
Although there is increasing interest in its use, definitive evidence demonstrating a benefit for postmortem normothermic regional perfusion (NRP) in controlled donation after circulatory death (cDCD) liver transplantation is lacking. The aim of this study was to compare results of cDCD liver transplants performed with postmortem NRP vs. super-rapid recovery (SRR), the current standard for cDCD. This was an observational cohort study including all cDCD liver transplants performed in Spain between June 2012 and December 2016, with follow-up ending in December 2017. Each donor hospital determined whether organ recovery was performed using NRP or SRR. The propensity scores technique based on the inverse probability of treatment weighting (IPTW) was used to balance covariates across study groups; logistic and Cox regression models were used for binary and time-to-event outcomes. During the study period, there were 95 cDCD liver transplants performed with postmortem NRP and 117 with SRR. The median donor age was 56 years (interquartile range 45–65 years). After IPTW analysis, baseline covariates were balanced, with all absolute standardised differences <0.15. IPTW-adjusted risks were significantly improved among NRP livers for overall biliary complications (odds ratio 0.14; 95% CI 0.06–0.35, p <0.001), ischaemic type biliary lesions (odds ratio 0.11; 95% CI 0.02–0.57; p = 0.008), and graft loss (hazard ratio 0.39; 95% CI 0.20–0.78; p = 0.008). The use of postmortem NRP in cDCD liver transplantation appears to reduce postoperative biliary complications, ischaemic type biliary lesions and graft loss, and allows for the transplantation of livers even from cDCD donors of advanced age. Donation after circulatory death (DCD) donors, who are declared dead following cardiorespiratory arrest, are an increasingly common source of organs.The period of donor warm ischaemia surrounding arrest can damage the quality of organs in general and the liver in particular, because biliary cells are exquisitely susceptible to warm ischaemia.1Thus, initial experiences with DCD liver transplantation described high rates of graft dysfunction and non-function and ischaemic type biliary lesions (ITBL).Although complication rates have improved with experience, the rate of post-transplant ITBL remains higher among recipients of DCD grafts vs. those receiving donation after brain death (DBD) grafts (16% vs. 3%, according to 2 meta-analyses2,3).Development of ITBL leads to repeat biliary procedures and hospitalisations; up to 70% of patients with ITBL either require retransplantation or die.4
Although there is increasing interest in its use, definitive evidence demonstrating a benefit for postmortem normothermic regional perfusion (NRP) in controlled donation after circulatory death (cDCD) liver transplantation is lacking. The aim of this study was to compare results of cDCD liver transplants performed with postmortem NRP vs. super-rapid recovery (SRR), the current standard for cDCD. This was an observational cohort study including all cDCD liver transplants performed in Spain between June 2012 and December 2016, with follow-up ending in December 2017. Each donor hospital determined whether organ recovery was performed using NRP or SRR. The propensity scores technique based on the inverse probability of treatment weighting (IPTW) was used to balance covariates across study groups; logistic and Cox regression models were used for binary and time-to-event outcomes. During the study period, there were 95 cDCD liver transplants performed with postmortem NRP and 117 with SRR. The median donor age was 56 years (interquartile range 45–65 years). After IPTW analysis, baseline covariates were balanced, with all absolute standardised differences <0.15. IPTW-adjusted risks were significantly improved among NRP livers for overall biliary complications (odds ratio 0.14; 95% CI 0.06–0.35, p <0.001), ischaemic type biliary lesions (odds ratio 0.11; 95% CI 0.02–0.57; p = 0.008), and graft loss (hazard ratio 0.39; 95% CI 0.20–0.78; p = 0.008). The use of postmortem NRP in cDCD liver transplantation appears to reduce postoperative biliary complications, ischaemic type biliary lesions and graft loss, and allows for the transplantation of livers even from cDCD donors of advanced age. Although DCD donors are typically classified among 4 categories depending on conditions surrounding cardiac arrest,5 category III controlled DCD (cDCD) donors are the most frequent source of DCD organs for transplantation globally.These are ventilated patients with a devastating brain injury that does not meet the criteria for brain death; the decision is made to withdraw life-sustaining therapy because it is no longer beneficial.Experience gained over the years has allowed for better donor and graft selection to the point that outcomes are comparable to those achieved with livers arising through donation after brain death.6,7However, achieving these results has come at the cost of high liver discard rates.8
Although there is increasing interest in its use, definitive evidence demonstrating a benefit for postmortem normothermic regional perfusion (NRP) in controlled donation after circulatory death (cDCD) liver transplantation is lacking. The aim of this study was to compare results of cDCD liver transplants performed with postmortem NRP vs. super-rapid recovery (SRR), the current standard for cDCD. This was an observational cohort study including all cDCD liver transplants performed in Spain between June 2012 and December 2016, with follow-up ending in December 2017. Each donor hospital determined whether organ recovery was performed using NRP or SRR. The propensity scores technique based on the inverse probability of treatment weighting (IPTW) was used to balance covariates across study groups; logistic and Cox regression models were used for binary and time-to-event outcomes. During the study period, there were 95 cDCD liver transplants performed with postmortem NRP and 117 with SRR. The median donor age was 56 years (interquartile range 45–65 years). After IPTW analysis, baseline covariates were balanced, with all absolute standardised differences <0.15. IPTW-adjusted risks were significantly improved among NRP livers for overall biliary complications (odds ratio 0.14; 95% CI 0.06–0.35, p <0.001), ischaemic type biliary lesions (odds ratio 0.11; 95% CI 0.02–0.57; p = 0.008), and graft loss (hazard ratio 0.39; 95% CI 0.20–0.78; p = 0.008). The use of postmortem NRP in cDCD liver transplantation appears to reduce postoperative biliary complications, ischaemic type biliary lesions and graft loss, and allows for the transplantation of livers even from cDCD donors of advanced age. In contrast to most of the Western world, the initial Spanish experience with DCD was with donors suffering sudden out-of-hospital cardiac arrest who were unable to be resuscitated after repeated attempts.Category II uncontrolled DCD (uDCD) donors are declared dead in the hospital, and femoral vasculature is cannulated to establish normothermic regional perfusion (NRP) to reperfuse and reoxygenate abdominal organs while donor evaluation and preparations for organ recovery are undertaken.9,10Using NRP, even livers with extensive prerecovery warm ischaemic periods of up to 2.5 h have been successfully transplanted, with biliary complication and graft survival rates comparable to those seen using cDCD livers exposed to considerably shorter periods of warm ischaemia.9–13
People with cirrhosis have unmet needs, which could benefit from a palliative care approach. Developing effective services needs to be based on evidence from those with personal experience. This review aims to explore; patient and family perspectives of perceived needs including communication; health professionals’ perspectives on delivery of care and improving palliative care between specialities. A literature search was conducted in Medline, Embase and CINAHL using key words reporting on the perspectives of patients with liver cirrhosis (18 years and over), family members or health professionals on the provision of care in liver cirrhosis. Study quality was assessed using the Mixed Methods Appraisal Tool. Qualitative and quantitative findings were grouped together according to the main relevant themes identified. Nineteen research studies predominantly from high-income Western countries were identified, with a total sample consisting of 1,413 patients, 31 family carers and 733 health professionals. Patients and family members had limited understanding of cirrhosis or its impact. They wanted better information about their disease, its treatment and help with psychological and practical needs. Health professionals had difficulty communicating about these issues to patients and their families. General practitioners left care predominantly to the liver clinicians, who lacked confidence to have discussions about prognosis or future care preferences. The role of palliative care was recognised as important in caring for this group through earlier integration with liver and community services. Health professionals need support to improve their communication with patients, to address patients’ broader needs beyond medical treatment and to develop new models to improve palliative care coordination between different medical specialities. Future research should focus on developing communication aides, testing existing tools to identify suitable patients for supportive care and exploring robust ways of evaluating supportive care interventions, with more studies needed from middle- and low-income countries. Registration number: PROSPERO CRD42017064770. Advanced liver cirrhosis is characterised by the development of clinical complications of portal hypertension or liver insufficiency.1It is a growing international public health problem due to increases in alcohol consumption, rates of obesity and viral hepatitis.2–5It often affects people of working age2,6 and is the third most common cause of premature death in the UK.7
People with cirrhosis have unmet needs, which could benefit from a palliative care approach. Developing effective services needs to be based on evidence from those with personal experience. This review aims to explore; patient and family perspectives of perceived needs including communication; health professionals’ perspectives on delivery of care and improving palliative care between specialities. A literature search was conducted in Medline, Embase and CINAHL using key words reporting on the perspectives of patients with liver cirrhosis (18 years and over), family members or health professionals on the provision of care in liver cirrhosis. Study quality was assessed using the Mixed Methods Appraisal Tool. Qualitative and quantitative findings were grouped together according to the main relevant themes identified. Nineteen research studies predominantly from high-income Western countries were identified, with a total sample consisting of 1,413 patients, 31 family carers and 733 health professionals. Patients and family members had limited understanding of cirrhosis or its impact. They wanted better information about their disease, its treatment and help with psychological and practical needs. Health professionals had difficulty communicating about these issues to patients and their families. General practitioners left care predominantly to the liver clinicians, who lacked confidence to have discussions about prognosis or future care preferences. The role of palliative care was recognised as important in caring for this group through earlier integration with liver and community services. Health professionals need support to improve their communication with patients, to address patients’ broader needs beyond medical treatment and to develop new models to improve palliative care coordination between different medical specialities. Future research should focus on developing communication aides, testing existing tools to identify suitable patients for supportive care and exploring robust ways of evaluating supportive care interventions, with more studies needed from middle- and low-income countries. Registration number: PROSPERO CRD42017064770. Most people dying from liver cirrhosis are not suitable for liver transplantation and of those in the UK who are suitable, 17% will die before a donor becomes available.10Living with cirrhosis may involve considerable symptom burden, and when liver failure ensues the prognosis is poor, often requiring repeated hospital admissions and multiple clinical interventions to deal with complex physical symptoms.11People experience both physical and psychosocial challenges12,13 and often have unmet needs in five key areas: informational/educational, practical, physical, patient care and support and psychological.13Moreover, there are difficulties with regards to accessing general health care in low- and middle-incoming countries (defined using the World Bank definition),8 where high treatment costs are an additional stressor in people with cirrhosis.9
People with cirrhosis have unmet needs, which could benefit from a palliative care approach. Developing effective services needs to be based on evidence from those with personal experience. This review aims to explore; patient and family perspectives of perceived needs including communication; health professionals’ perspectives on delivery of care and improving palliative care between specialities. A literature search was conducted in Medline, Embase and CINAHL using key words reporting on the perspectives of patients with liver cirrhosis (18 years and over), family members or health professionals on the provision of care in liver cirrhosis. Study quality was assessed using the Mixed Methods Appraisal Tool. Qualitative and quantitative findings were grouped together according to the main relevant themes identified. Nineteen research studies predominantly from high-income Western countries were identified, with a total sample consisting of 1,413 patients, 31 family carers and 733 health professionals. Patients and family members had limited understanding of cirrhosis or its impact. They wanted better information about their disease, its treatment and help with psychological and practical needs. Health professionals had difficulty communicating about these issues to patients and their families. General practitioners left care predominantly to the liver clinicians, who lacked confidence to have discussions about prognosis or future care preferences. The role of palliative care was recognised as important in caring for this group through earlier integration with liver and community services. Health professionals need support to improve their communication with patients, to address patients’ broader needs beyond medical treatment and to develop new models to improve palliative care coordination between different medical specialities. Future research should focus on developing communication aides, testing existing tools to identify suitable patients for supportive care and exploring robust ways of evaluating supportive care interventions, with more studies needed from middle- and low-income countries. Registration number: PROSPERO CRD42017064770. A supportive and palliative care approach could benefit people living with cirrhosis.14‘Supportive and palliative care’ are two broad encompassing terms which overlap with each other.Supportive care may be defined as care which meets the physical, informational, social, spiritual and practical needs of a person with chronic disease at all stages of the patient’s illness from pre-diagnosis, diagnosis, treatment and follow-up.13,15The WHO definition of palliative care is, interdisciplinary care to improve the quality of life of patients facing life threatening illness by addressing their physical, emotional and spiritual needs and by supporting their families.16For the purposes of this review and to ensure consistency, the terms ‘supportive and palliative care’ will be used to incorporate these two related concepts together.
People with cirrhosis have unmet needs, which could benefit from a palliative care approach. Developing effective services needs to be based on evidence from those with personal experience. This review aims to explore; patient and family perspectives of perceived needs including communication; health professionals’ perspectives on delivery of care and improving palliative care between specialities. A literature search was conducted in Medline, Embase and CINAHL using key words reporting on the perspectives of patients with liver cirrhosis (18 years and over), family members or health professionals on the provision of care in liver cirrhosis. Study quality was assessed using the Mixed Methods Appraisal Tool. Qualitative and quantitative findings were grouped together according to the main relevant themes identified. Nineteen research studies predominantly from high-income Western countries were identified, with a total sample consisting of 1,413 patients, 31 family carers and 733 health professionals. Patients and family members had limited understanding of cirrhosis or its impact. They wanted better information about their disease, its treatment and help with psychological and practical needs. Health professionals had difficulty communicating about these issues to patients and their families. General practitioners left care predominantly to the liver clinicians, who lacked confidence to have discussions about prognosis or future care preferences. The role of palliative care was recognised as important in caring for this group through earlier integration with liver and community services. Health professionals need support to improve their communication with patients, to address patients’ broader needs beyond medical treatment and to develop new models to improve palliative care coordination between different medical specialities. Future research should focus on developing communication aides, testing existing tools to identify suitable patients for supportive care and exploring robust ways of evaluating supportive care interventions, with more studies needed from middle- and low-income countries. Registration number: PROSPERO CRD42017064770. Initiating early supportive and palliative care can improve symptom control in people with cirrhosis.17,18Supportive and palliative interventions should seek to improve communication and care coordination whilst encouraging discussions on unmet information needs and end of life preferences.19To achieve this, it is important to know what people with cirrhosis understand about the nature and prognosis of their disease, and to understand what unmet needs they may have as a result.As delivery of supportive and palliative care requires a multidisciplinary approach, it is key to explore how health professionals from different specialities feel they can work together to provide this care.Notably, whilst liver professionals acknowledge they have a role to play in this aspect of care,20,21 it is important to understand how liver health professionals communicate with their patients and family members about the disease and its treatment.
Over the last decade, liver transplantation of sicker, older non-hepatitis C cirrhotics with multiple co-morbidities has increased in the United States. We sought to identify an easily applicable set of recipient factors among HCV negative adult transplant recipients associated with significant morbidity and mortality within five years after liver transplantation. We collected national (n = 31,829, 2002–2015) and center-specific data. Coefficients of relevant recipient factors were converted to weighted points and scaled from 0–5. Recipient factors associated with graft failure included: ventilator support (five patients; hazard ratio [HR] 1.59; 95% CI 1.48–1.72); recipient age >60 years (three patients; HR 1.29; 95% CI 1.23–1.36); hemodialysis (three patients; HR 1.26; 95% CI 1.16–1.37); diabetes (two patients; HR 1.20; 95% CI 1.14–1.27); or serum creatinine ≥1.5 mg/dl without hemodialysis (two patients; HR 1.15; 95% CI 1.09–1.22). Graft survival within five years based on points (any combination) was 77.2% (0–4), 69.1% (5–8) and 57.9% (>8). In recipients with >8 points, graft survival was 42% (model for end-stage liver disease [MELD] score <25) and 50% (MELD score 25–35) in recipients receiving grafts from donors with a donor risk index >1.7. In center-specific data within the first year, subjects with ≥5 points (vs. 0–4) had longer hospitalization (11 vs. 8 days, p <0.01), higher admissions for rehabilitation (12.3% vs. 2.7%, p <0.01), and higher incidence of cardiac disease (14.2% vs. 5.3%, p <0.01) and stage 3 chronic kidney disease (78.6% vs. 39.5%, p = 0.03) within five years. The impact of co-morbidities in an MELD-based organ allocation system need to be reassessed. The proposed clinical tool may be helpful for center-specific assessment of risk of graft failure in non-HCV patients and for discussion regarding relevant morbidity in selected subsets. Liver transplant (LT) recipients in the United States are globally sicker in the current era.1,2The number of candidates delisted for being too sick has nearly tripled over the last few years.3Indications for LT have also evolved with a noticeable increase in transplantation of candidates with non-alcoholic fatty liver disease (NAFLD) and cryptogenic cirrhosis, as well as patients with significant co-morbidities.2,4,5In the current organ allocation policy based on urgency, people at the highest risk of death are offered liver transplantation (LT) first.However, several recipient characteristics may diminish the guiding principle of an MELD-based system.6–8
Over the last decade, liver transplantation of sicker, older non-hepatitis C cirrhotics with multiple co-morbidities has increased in the United States. We sought to identify an easily applicable set of recipient factors among HCV negative adult transplant recipients associated with significant morbidity and mortality within five years after liver transplantation. We collected national (n = 31,829, 2002–2015) and center-specific data. Coefficients of relevant recipient factors were converted to weighted points and scaled from 0–5. Recipient factors associated with graft failure included: ventilator support (five patients; hazard ratio [HR] 1.59; 95% CI 1.48–1.72); recipient age >60 years (three patients; HR 1.29; 95% CI 1.23–1.36); hemodialysis (three patients; HR 1.26; 95% CI 1.16–1.37); diabetes (two patients; HR 1.20; 95% CI 1.14–1.27); or serum creatinine ≥1.5 mg/dl without hemodialysis (two patients; HR 1.15; 95% CI 1.09–1.22). Graft survival within five years based on points (any combination) was 77.2% (0–4), 69.1% (5–8) and 57.9% (>8). In recipients with >8 points, graft survival was 42% (model for end-stage liver disease [MELD] score <25) and 50% (MELD score 25–35) in recipients receiving grafts from donors with a donor risk index >1.7. In center-specific data within the first year, subjects with ≥5 points (vs. 0–4) had longer hospitalization (11 vs. 8 days, p <0.01), higher admissions for rehabilitation (12.3% vs. 2.7%, p <0.01), and higher incidence of cardiac disease (14.2% vs. 5.3%, p <0.01) and stage 3 chronic kidney disease (78.6% vs. 39.5%, p = 0.03) within five years. The impact of co-morbidities in an MELD-based organ allocation system need to be reassessed. The proposed clinical tool may be helpful for center-specific assessment of risk of graft failure in non-HCV patients and for discussion regarding relevant morbidity in selected subsets. First, cirrhotics with equivalent MELD scores may have divergent post-transplant outcomes based on presence of relevant co-morbidities.6–9For example, a cirrhotic with diabetes, advanced age, intubated in the intensive care unit (ICU), with a high MELD score (>25) may do worse than a cirrhotic with an equivalent MELD score but without these co-morbidities.Further, with the changing transplant population, subsets of patients may achieve acceptable survival after LT, but at the expense of significant morbidity, manifested as frequent hospitalizations, worsening of chronic conditions and significant resource utilization.6,10
Over the last decade, liver transplantation of sicker, older non-hepatitis C cirrhotics with multiple co-morbidities has increased in the United States. We sought to identify an easily applicable set of recipient factors among HCV negative adult transplant recipients associated with significant morbidity and mortality within five years after liver transplantation. We collected national (n = 31,829, 2002–2015) and center-specific data. Coefficients of relevant recipient factors were converted to weighted points and scaled from 0–5. Recipient factors associated with graft failure included: ventilator support (five patients; hazard ratio [HR] 1.59; 95% CI 1.48–1.72); recipient age >60 years (three patients; HR 1.29; 95% CI 1.23–1.36); hemodialysis (three patients; HR 1.26; 95% CI 1.16–1.37); diabetes (two patients; HR 1.20; 95% CI 1.14–1.27); or serum creatinine ≥1.5 mg/dl without hemodialysis (two patients; HR 1.15; 95% CI 1.09–1.22). Graft survival within five years based on points (any combination) was 77.2% (0–4), 69.1% (5–8) and 57.9% (>8). In recipients with >8 points, graft survival was 42% (model for end-stage liver disease [MELD] score <25) and 50% (MELD score 25–35) in recipients receiving grafts from donors with a donor risk index >1.7. In center-specific data within the first year, subjects with ≥5 points (vs. 0–4) had longer hospitalization (11 vs. 8 days, p <0.01), higher admissions for rehabilitation (12.3% vs. 2.7%, p <0.01), and higher incidence of cardiac disease (14.2% vs. 5.3%, p <0.01) and stage 3 chronic kidney disease (78.6% vs. 39.5%, p = 0.03) within five years. The impact of co-morbidities in an MELD-based organ allocation system need to be reassessed. The proposed clinical tool may be helpful for center-specific assessment of risk of graft failure in non-HCV patients and for discussion regarding relevant morbidity in selected subsets. Prior studies have examined predictors of mortality after LT, however studies are limited by utilization of extensive lists of variables8,11 and lack of granular data linking morbidity and mortality.Further, previous analyses of national data have included patients transplanted with hepatitis C. However, analysis of historical outcomes after LT for HCV recipients may not accurately reflect future trends in survival, given the impact of current antiviral therapy both before and after LT.12 Further, patients transplanted with lower MELD scores may be systematically different than those transplanted with high MELD scores.13
More than 90% of cases of hepatocellular carcinoma (HCC) occur in patients with cirrhosis, of which alcohol is a major cause. The CIRRAL cohort aimed to assess the burden of complications in patients with alcoholic cirrhosis, particularly the occurrence of HCC. Patients with biopsy-proven compensated alcoholic cirrhosis were included then prospectively followed. The main endpoint was the incidence of HCC. Secondary outcomes were incidence of hepatic focal lesions, overall survival (OS), liver-related mortality and event-free survival (EFS). From October 2010 to April 2016, 652 patients were included in 22 French and Belgian centers. During follow-up (median 29 months), HCC was diagnosed in 43 patients. With the limitation derived from the uncertainty of consecutive patients’ inclusion and from a sizable proportion of dropouts (153/652), the incidence of HCC was 2.9 per 100 patient-years, and one- and two-year cumulative incidences of 1.8% and 5.2%, respectively. Although HCC fulfilled the Milan criteria in 33 cases (77%), only 24 patients (56%) underwent curative treatment. An explorative prognostic analysis showed that age, male gender, baseline alpha-fetoprotein, bilirubin and prothrombin were significantly associated with the risk of HCC occurrence. Among 73 deaths, 61 had a recorded cause and 27 were directly attributable to liver disease. At two years, OS, EFS and cumulative incidences of liver-related deaths were 93% (95% CI 90.5–95.4), 80.3% (95% CI 76.9–83.9), and 3.2% (95% CI 1.6–4.8) respectively. This large prospective cohort incompletely representative of the whole population with alcoholic cirrhosis showed: a) an annual incidence of HCC of up to 2.9 per 100 patient-years, suggesting that surveillance might be cost effective in these patients; b) a high proportion of HCC detected within the Milan criteria, but only one-half of detected HCC cases were referred for curative treatments; c) a two-year mortality rate of up to 7%. Primary liver cancer has a high incidence in Europe, especially in France, which has almost 9,000 cases per year.1In more than 90% of cases, hepatocellular carcinoma (HCC) occurs in patients with cirrhosis.Alcohol is the leading cause of the underlying cirrhosis that is associated with HCC in France; it is responsible for more than 60% of the cases,2 which is much higher than the proportion of cases from hepatitis B and C and non-alcoholic steatohepatitis.Although precise data are lacking, as France has lost its longstanding European leadership in alcohol consumption, one can speculate that alcoholic cirrhosis is currently the main cause of HCC occurrence in Europe, and it will gain an even more important predominance as alcoholic consumption is on the rise in many countries, while active viral infection is on the decline.
More than 90% of cases of hepatocellular carcinoma (HCC) occur in patients with cirrhosis, of which alcohol is a major cause. The CIRRAL cohort aimed to assess the burden of complications in patients with alcoholic cirrhosis, particularly the occurrence of HCC. Patients with biopsy-proven compensated alcoholic cirrhosis were included then prospectively followed. The main endpoint was the incidence of HCC. Secondary outcomes were incidence of hepatic focal lesions, overall survival (OS), liver-related mortality and event-free survival (EFS). From October 2010 to April 2016, 652 patients were included in 22 French and Belgian centers. During follow-up (median 29 months), HCC was diagnosed in 43 patients. With the limitation derived from the uncertainty of consecutive patients’ inclusion and from a sizable proportion of dropouts (153/652), the incidence of HCC was 2.9 per 100 patient-years, and one- and two-year cumulative incidences of 1.8% and 5.2%, respectively. Although HCC fulfilled the Milan criteria in 33 cases (77%), only 24 patients (56%) underwent curative treatment. An explorative prognostic analysis showed that age, male gender, baseline alpha-fetoprotein, bilirubin and prothrombin were significantly associated with the risk of HCC occurrence. Among 73 deaths, 61 had a recorded cause and 27 were directly attributable to liver disease. At two years, OS, EFS and cumulative incidences of liver-related deaths were 93% (95% CI 90.5–95.4), 80.3% (95% CI 76.9–83.9), and 3.2% (95% CI 1.6–4.8) respectively. This large prospective cohort incompletely representative of the whole population with alcoholic cirrhosis showed: a) an annual incidence of HCC of up to 2.9 per 100 patient-years, suggesting that surveillance might be cost effective in these patients; b) a high proportion of HCC detected within the Milan criteria, but only one-half of detected HCC cases were referred for curative treatments; c) a two-year mortality rate of up to 7%. Despite recommendations,3–5 including those for alcoholic liver disease,6 screening for HCC in patients with cirrhosis is poorly performed in practice and is even contested, particularly in alcoholics.7One matter of disagreement is the incidence of HCC in these patients.Based on medico-economic studies, the threshold of HCC incidence that is associated with cost-effectiveness is 1.5% per year, and surveillance should only be undertaken in patients whose risk of HCC is over this threshold.There are few and conflicting data regarding the incidence of HCC in patients with alcoholic cirrhosis.A retrospective Danish study, using its National Patient Registry,8 and the United Kingdom’s General Practice Research Database population-based study9 have demonstrated a low risk of HCC (between 0.25 and 0.50% per year in the Danish study and less than 2% at 10 years in the British study), raising some doubts regarding the opportunity of screening for HCC in these patients.In contrast, a small but very homogeneous, Spanish cohort of patients with alcoholic cirrhosis prospectively followed for HCC occurrence showed a high incidence of HCC (up to 2.6% per year),10 which suggests that the implementation of a surveillance program for the early diagnosis of HCC is warranted in these patients.
More than 90% of cases of hepatocellular carcinoma (HCC) occur in patients with cirrhosis, of which alcohol is a major cause. The CIRRAL cohort aimed to assess the burden of complications in patients with alcoholic cirrhosis, particularly the occurrence of HCC. Patients with biopsy-proven compensated alcoholic cirrhosis were included then prospectively followed. The main endpoint was the incidence of HCC. Secondary outcomes were incidence of hepatic focal lesions, overall survival (OS), liver-related mortality and event-free survival (EFS). From October 2010 to April 2016, 652 patients were included in 22 French and Belgian centers. During follow-up (median 29 months), HCC was diagnosed in 43 patients. With the limitation derived from the uncertainty of consecutive patients’ inclusion and from a sizable proportion of dropouts (153/652), the incidence of HCC was 2.9 per 100 patient-years, and one- and two-year cumulative incidences of 1.8% and 5.2%, respectively. Although HCC fulfilled the Milan criteria in 33 cases (77%), only 24 patients (56%) underwent curative treatment. An explorative prognostic analysis showed that age, male gender, baseline alpha-fetoprotein, bilirubin and prothrombin were significantly associated with the risk of HCC occurrence. Among 73 deaths, 61 had a recorded cause and 27 were directly attributable to liver disease. At two years, OS, EFS and cumulative incidences of liver-related deaths were 93% (95% CI 90.5–95.4), 80.3% (95% CI 76.9–83.9), and 3.2% (95% CI 1.6–4.8) respectively. This large prospective cohort incompletely representative of the whole population with alcoholic cirrhosis showed: a) an annual incidence of HCC of up to 2.9 per 100 patient-years, suggesting that surveillance might be cost effective in these patients; b) a high proportion of HCC detected within the Milan criteria, but only one-half of detected HCC cases were referred for curative treatments; c) a two-year mortality rate of up to 7%. The French and Belgian CIRRAL prospective cohort was established to assess the natural history of alcohol-related compensated cirrhosis based on rigorous, multicenter, protocol-driven, systematic data collection and the analysis of predefined outcomes.
The Wnt/β-catenin pathway is the most frequently deregulated pathway in hepatocellular carcinoma (HCC). Inactivating mutations of the gene encoding AXIN1, a known negative regulator of the Wnt/β-catenin signaling pathway, are observed in about 10% of HCCs. Whole-genome studies usually place HCC with AXIN1 mutations and CTNNB1 mutations in the group of tumors with Wnt/β-catenin activated program. However, it has been shown that HCCs with activating CTNNB1 mutations form a group of HCCs, with a different histology, prognosis and genomic signature to those with inactivating biallelic AXIN1 mutations. We aimed to elucidate the relationship between CTNNB1 mutations, AXIN1 mutations and the activation level of the Wnt/β-catenin program. We evaluated two independent human HCC datasets for the expression of a 23-β-catenin target genes program. We modeled Axin1 loss of function tumorigenesis in two engineered mouse models and performed gene expression profiling. Based on gene expression, we defined three levels of β-catenin program activation: strong, weak or no activation. While more than 80% CTNNB1-mutated tumors were found in the strong or in the weak activation program, most of the AXIN1-mutated tumors (>70%) were found in the subgroup with no activation. We validated this result by demonstrating that mice with a hepatocyte specific AXIN1 deletion developed HCC in the absence of β-catenin induction. We defined a 329-gene signature common in human and mouse AXIN1 mutated HCC that is highly enriched in Notch and YAP oncogenic signatures. AXIN1-mutated HCCs occur independently of the Wnt/β-catenin pathway and involve Notch and YAP pathways. These pathways constitute potentially interesting targets for the treatment of HCC caused by AXIN1 mutations. Hepatocellular carcinoma (HCC) is the third most frequent cause of cancer death worldwide.HCC is a highly heterogeneous disease, occurring in a context of chronic liver injury and inflammation leading to cirrhosis.1Recent genomic studies have provided an accurate description of the landscape of genetic changes underlying HCC and identified the molecular pathways most frequently altered, among which the Wnt/β-catenin pathway is prominent.2–6This conserved signaling pathway governs embryonic development, homeostasis and liver zonation in adults.In the absence of Wnt stimulation, the cytosolic concentration of β-catenin remains low because of a multiprotein destruction complex including CK1α, GSK3β, YAP/TAZ, APC and AXIN1, which promotes the phosphorylation of β-catenin.Once phosphorylated, β-catenin is degraded by the proteasome.In response to Wnt stimulation, the destruction complex is recruited to the membrane.This stabilizes β-catenin, which then enters the nucleus and activates the expression of Wnt target genes, mostly through the lymphoid enhancer-binding factor 1 (LEF-1) and T-cell transcription factor (TCF).
The effectiveness of direct-acting antivirals (DAAs) against hepatitis C virus (HCV), following successful treatment of early hepatocellular carcinoma (HCC), has been studied extensively. However, the benefit in terms of overall survival (OS) remains to be conclusively demonstrated. The aim of this study was to assess the impact of DAAs on OS, HCC recurrence, and hepatic decompensation. We prospectively enrolled 163 consecutive patients with HCV-related cirrhosis and a first diagnosis of early Barcelona Clinic Liver Cancer stage 0/A HCC, who had achieved a complete radiologic response after curative resection or ablation and were subsequently treated with DAAs. DAA-untreated patients from the ITA.LI.CA. cohort (n = 328) served as controls. After propensity score matching, outcomes of 102 DAA-treated (DAA group) and 102 DAA-untreated patients (No DAA group) were compared. In the DAA group, 7/102 patients (6.9%) died, HCC recurred in 28/102 patients (27.5%) and hepatic decompensation occurred in 6/102 patients (5.9%), after a mean follow-up of 21.4 months. OS was significantly higher in the DAA group compared to the No DAA group (hazard ratio [HR] 0.39; 95% CI 0.17–0.91; p = 0.03). HCC recurrence was not significantly different between the DAA and No DAA groups (HR 0.70; 95% CI 0.44–1.13; p = 0.15). A significant reduction in the rate of hepatic decompensation was observed in the DAA group compared with the No DAA group (HR 0.32; 95% CI 0.13–0.84; p = 0.02). In the DAA group, sustained virologic response was a significant predictor of OS (HR 0.02; 95% CI 0.00–0.19; p <0.001), HCC recurrence (HR 0.25; 95% CI 0.11–0.57; p <0.001) and hepatic decompensation (HR 0.12; 95% CI 0.02–0.38; p = 0.02). In patients with HCV-related cirrhosis who had been successfully treated for early HCC, DAAs significantly improved OS compared with No DAA treatment. Hepatocellular carcinoma (HCC) is the third leading cause of cancer-related death globally, and the leading cause of mortality in cirrhotic patients, with hepatitis C virus (HCV) being the major risk factor in the Western world and Japan.1Orthotopic liver transplantation (OLT) is the definitive treatment for HCC and cirrhotic liver, but this approach cannot be offered to all patients due to limited graft availability and rigorous selection criteria.2Alternative curative treatment options for patients with compensated cirrhosis are surgical resection and loco-regional ablation of early HCC (i.e. Barcelona Clinic Liver Cancer [BCLC] stage 0/A).2
The effectiveness of direct-acting antivirals (DAAs) against hepatitis C virus (HCV), following successful treatment of early hepatocellular carcinoma (HCC), has been studied extensively. However, the benefit in terms of overall survival (OS) remains to be conclusively demonstrated. The aim of this study was to assess the impact of DAAs on OS, HCC recurrence, and hepatic decompensation. We prospectively enrolled 163 consecutive patients with HCV-related cirrhosis and a first diagnosis of early Barcelona Clinic Liver Cancer stage 0/A HCC, who had achieved a complete radiologic response after curative resection or ablation and were subsequently treated with DAAs. DAA-untreated patients from the ITA.LI.CA. cohort (n = 328) served as controls. After propensity score matching, outcomes of 102 DAA-treated (DAA group) and 102 DAA-untreated patients (No DAA group) were compared. In the DAA group, 7/102 patients (6.9%) died, HCC recurred in 28/102 patients (27.5%) and hepatic decompensation occurred in 6/102 patients (5.9%), after a mean follow-up of 21.4 months. OS was significantly higher in the DAA group compared to the No DAA group (hazard ratio [HR] 0.39; 95% CI 0.17–0.91; p = 0.03). HCC recurrence was not significantly different between the DAA and No DAA groups (HR 0.70; 95% CI 0.44–1.13; p = 0.15). A significant reduction in the rate of hepatic decompensation was observed in the DAA group compared with the No DAA group (HR 0.32; 95% CI 0.13–0.84; p = 0.02). In the DAA group, sustained virologic response was a significant predictor of OS (HR 0.02; 95% CI 0.00–0.19; p <0.001), HCC recurrence (HR 0.25; 95% CI 0.11–0.57; p <0.001) and hepatic decompensation (HR 0.12; 95% CI 0.02–0.38; p = 0.02). In patients with HCV-related cirrhosis who had been successfully treated for early HCC, DAAs significantly improved OS compared with No DAA treatment. Unfortunately, hepatic decompensation of underlying cirrhosis, the major driver of death, and tumour recurrence contribute to long-term mortality after successful treatment of early HCC.3A recent meta-analysis showed that, among HCV-infected compensated cirrhotic patients in whom early HCC was successfully treated, who remained unexposed to direct-acting antivirals (DAAs), the 2-year actuarial pooled HCC recurrence rate was 47.0% and the 3-year actuarial pooled survival rate was 79.8%.4These data indicate that there is an urgent need for an effective adjuvant strategy given the prior failures of other adjuvant treatments, including sorafenib.5
The effectiveness of direct-acting antivirals (DAAs) against hepatitis C virus (HCV), following successful treatment of early hepatocellular carcinoma (HCC), has been studied extensively. However, the benefit in terms of overall survival (OS) remains to be conclusively demonstrated. The aim of this study was to assess the impact of DAAs on OS, HCC recurrence, and hepatic decompensation. We prospectively enrolled 163 consecutive patients with HCV-related cirrhosis and a first diagnosis of early Barcelona Clinic Liver Cancer stage 0/A HCC, who had achieved a complete radiologic response after curative resection or ablation and were subsequently treated with DAAs. DAA-untreated patients from the ITA.LI.CA. cohort (n = 328) served as controls. After propensity score matching, outcomes of 102 DAA-treated (DAA group) and 102 DAA-untreated patients (No DAA group) were compared. In the DAA group, 7/102 patients (6.9%) died, HCC recurred in 28/102 patients (27.5%) and hepatic decompensation occurred in 6/102 patients (5.9%), after a mean follow-up of 21.4 months. OS was significantly higher in the DAA group compared to the No DAA group (hazard ratio [HR] 0.39; 95% CI 0.17–0.91; p = 0.03). HCC recurrence was not significantly different between the DAA and No DAA groups (HR 0.70; 95% CI 0.44–1.13; p = 0.15). A significant reduction in the rate of hepatic decompensation was observed in the DAA group compared with the No DAA group (HR 0.32; 95% CI 0.13–0.84; p = 0.02). In the DAA group, sustained virologic response was a significant predictor of OS (HR 0.02; 95% CI 0.00–0.19; p <0.001), HCC recurrence (HR 0.25; 95% CI 0.11–0.57; p <0.001) and hepatic decompensation (HR 0.12; 95% CI 0.02–0.38; p = 0.02). In patients with HCV-related cirrhosis who had been successfully treated for early HCC, DAAs significantly improved OS compared with No DAA treatment. DAAs improve HCV infection outcomes, even in patients with advanced liver disease,6,7 with a good safety profile and a sustained virologic response (SVR) rate exceeding 90% in clinical practice.In 2016, an alarm signal was released about a potentially increased risk of early HCC recurrence after DAA therapy, raising concerns about the safety of DAA use in patients with previously treated early HCC.8,9Since then, several prospective studies10–13 and 2 meta-analyses14,15 have provided evidence that the risk of HCC recurrence after treatment with DAAs in patients with history of successful treatment of early HCC is similar, if not lower, than that observed in interferon-treated or DAA-unexposed controls.However, field-practice prospective studies that prove the benefit of DAAs on overall survival and hepatic decompensation are lacking, and the longer-term effect of DAAs on mortality remains to be established.Since DAAs are the accepted standard of care even in patients with previously treated early HCC, randomized controlled trials (RCTs) comparing DAAs to No DAAs are not feasible, ethical or timely.Therefore, we analysed our observational data, as an attempt to emulate a randomized trial, outlining a framework for comparative effectiveness using observational data.16For this reason, an appropriately matched control group of DAA-unexposed patients is needed to assess the benefit of DAA treatment on hepatic decompensation, HCC recurrence and finally overall survival.
Virus-induced fulminant hepatitis is a major cause of acute liver failure. During acute viral hepatitis the impact of type I interferon (IFN-I) on myeloid cells, including liver-resident Kupffer cells (KC), is only partially understood. Herein, we dissected the impact of locally induced IFN-I responses on myeloid cell function and hepatocytes during acute liver inflammation. Two different DNA-encoded viruses, vaccinia virus (VACV) and murine cytomegalovirus (MCMV), were studied. In vivo imaging was applied to visualize local IFN-β induction and IFN-I receptor (IFNAR) triggering in VACV-infected reporter mice. Furthermore, mice with a cell type-selective IFNAR ablation were analyzed to dissect the role of IFNAR signaling in myeloid cells and hepatocytes. Experiments with Cx3cr1+/gfp mice revealed the origin of reconstituted KC. Finally, mixed bone marrow chimeric mice were studied to specifically analyze the effect of IFNAR triggering on liver infiltrating monocytes. VACV infection induced local IFN-β responses, which lead to IFNAR signaling primarily within the liver. IFNAR triggering was needed to control the infection and prevent fulminant hepatitis. The severity of liver inflammation was independent of IFNAR triggering of hepatocytes, whereas IFNAR triggering of myeloid cells protected from excessive inflammation. Upon VACV or MCMV infection KC disappeared, whereas infiltrating monocytes differentiated to KC afterwards. During IFNAR triggering such replenished monocyte-derived KC comprised more IFNAR-deficient than -competent cells in mixed bone marrow chimeric mice, whereas after the decline of IFNAR triggering both subsets showed an even distribution. Upon VACV infection IFNAR triggering of myeloid cells, but not of hepatocytes, critically modulates acute viral hepatitis. During infection with DNA-encoded viruses IFNAR triggering of liver-infiltrating blood monocytes delays the development of monocyte-derived KC, pointing towards new therapeutic strategies for acute viral hepatitis. Many different viral infections can cause acute hepatitis that may result in acute liver failure.Besides infections with hepatotropic virus (e.g., hepatitis B virus and hepatitis C virus) other viruses such as cytomegalovirus (CMV) can cause acute hepatitis.1–5Yet, it is unclear to what extent viral pathogenicity or immunopathology cause liver damage.The liver is the central metabolic organ, which is characterized by a tolerogenic environment.Hepatic sinusoids are populated with antigen-presenting cells such as Kupffer cells (KC).KC are self-maintaining liver-resident macrophages, which shape the local immune milieu.6–8During homeostasis, KC display an overall anti-inflammatory phenotype (M2-like phenotype) and primarily work as scavenger cells that eliminate insoluble macromolecules and antigens from blood.9–11Upon infection, KC express enhanced levels of scavenger receptors, take up and kill invading pathogens, and express major histocompatibility complex class II molecules to present antigens (M1-like phenotype).8,12Macrophage-depleted mice show enhanced susceptibility to infection with viruses and bacteria.13,14Furthermore, liver-infiltrating monocytes can contribute to KC responses and differentiate to DC or monocyte-derived macrophages (MoMF).15–17
Virus-induced fulminant hepatitis is a major cause of acute liver failure. During acute viral hepatitis the impact of type I interferon (IFN-I) on myeloid cells, including liver-resident Kupffer cells (KC), is only partially understood. Herein, we dissected the impact of locally induced IFN-I responses on myeloid cell function and hepatocytes during acute liver inflammation. Two different DNA-encoded viruses, vaccinia virus (VACV) and murine cytomegalovirus (MCMV), were studied. In vivo imaging was applied to visualize local IFN-β induction and IFN-I receptor (IFNAR) triggering in VACV-infected reporter mice. Furthermore, mice with a cell type-selective IFNAR ablation were analyzed to dissect the role of IFNAR signaling in myeloid cells and hepatocytes. Experiments with Cx3cr1+/gfp mice revealed the origin of reconstituted KC. Finally, mixed bone marrow chimeric mice were studied to specifically analyze the effect of IFNAR triggering on liver infiltrating monocytes. VACV infection induced local IFN-β responses, which lead to IFNAR signaling primarily within the liver. IFNAR triggering was needed to control the infection and prevent fulminant hepatitis. The severity of liver inflammation was independent of IFNAR triggering of hepatocytes, whereas IFNAR triggering of myeloid cells protected from excessive inflammation. Upon VACV or MCMV infection KC disappeared, whereas infiltrating monocytes differentiated to KC afterwards. During IFNAR triggering such replenished monocyte-derived KC comprised more IFNAR-deficient than -competent cells in mixed bone marrow chimeric mice, whereas after the decline of IFNAR triggering both subsets showed an even distribution. Upon VACV infection IFNAR triggering of myeloid cells, but not of hepatocytes, critically modulates acute viral hepatitis. During infection with DNA-encoded viruses IFNAR triggering of liver-infiltrating blood monocytes delays the development of monocyte-derived KC, pointing towards new therapeutic strategies for acute viral hepatitis. Type I interferon (IFN-I) is an anti-viral cytokine that is induced early after infection, confers direct anti-viral effects, and modulates immune cell functions.18,19Previous studies showed that IFN-I, which is produced in the liver by myeloid cells during many infections, significantly contributes to anti-viral defense.13,15,20,21Furthermore, in poly(I:C)-treated mice, IFN-I receptor (IFNAR) triggering of myeloid cells results in IL-1RA expression, which protects mice from severe liver damage.22In contrast, upon lymphocytic choriomeningitis virus (LCMV) infection, IFNAR triggering enhances immunopathology within the liver by inducing oxidative damage in hepatocytes.23,24
Surgery in cirrhosis is associated with a high morbidity and mortality. Retrospectively reported prognostic factors include emergency procedures, liver function (MELD/Child-Pugh scores) and portal hypertension (assessed by indirect markers). This study assessed the prognostic role of hepatic venous pressure gradient (HVPG) and other variables in elective extrahepatic surgery in patients with cirrhosis. A total of 140 patients with cirrhosis (Child-Pugh A/B/C: 59/37/4%), who were due to have elective extrahepatic surgery (121 abdominal; 9 cardiovascular/thoracic; 10 orthopedic and others), were prospectively included in 4 centers (2002–2011). Hepatic and systemic hemodynamics (HVPG, indocyanine green clearance, pulmonary artery catheterization) were assessed prior to surgery, and clinical and laboratory data were collected. Patients were followed-up for 1 year and mortality, transplantation, morbidity and post-surgical decompensation were studied. Ninety-day and 1-year mortality rates were 8% and 17%, respectively. Variables independently associated with 1-year mortality were ASA class (American Society of Anesthesiologists), high-risk surgery (defined as open abdominal and cardiovascular/thoracic) and HVPG. These variables closely predicted 90-, 180- and 365-day mortality (C-statistic >0.8). HVPG values >16 mmHg were independently associated with mortality and values ≥20 mmHg identified a subgroup at very high risk of death (44%). Twenty-four patients presented persistent or de novo decompensation at 3 months. Low body mass index, Child-Pugh class and high-risk surgery were associated with death or decompensation. No patient with HVPG <10 mmHg or indocyanine green clearance >0.63 developed decompensation. ASA class, HVPG and high-risk surgery were prognostic factors of 1-year mortality in cirrhotic patients undergoing elective extrahepatic surgery. HVPG values >16 mmHg, especially ≥20 mmHg, were associated with a high risk of post-surgical mortality. Cirrhosis is a life-threatening condition and a major cause of morbidity and mortality worldwide.Improvements in the management of its related complications, of its etiologies (i.e. viral eradication), and the option of liver transplantation have increased life expectancy of patients with cirrhosis.In this setting, it is not unusual that major surgical procedures are proposed for patients with cirrhosis to address orthopedic, malignancy or cirrhosis related complications.In fact, patients with cirrhosis have a high incidence of gallstones and abdominal wall hernias that require surgical repair.1–4Surgery in cirrhosis has always been associated with high perioperative morbidity (about 30%, including infections, renal failure, decompensation, blood transfusion, re-intervention, etc.) and mortality, ranging from 10 to 30% in the most recent series.5–11The main factors associated with these poor outcomes have been related to liver function (Child-Pugh or model for end-stage liver disease [MELD] scores), to the type of surgery (higher risk in open abdominal, cardiovascular and thoracic surgeries), and to the presence of signs or symptoms of portal hypertension (PHT).5,8,10–14However, there are no universally accepted prospective scores to assess surgical risk for patients with cirrhosis.The most widely accepted score is probably that from the Mayo Clinic, based on MELD, ASA class and age.11,15Although it was developed in a very large cohort, this model combines emergency and elective surgery, combining different profiles of patients that may act as confounding factors (for example, MELD score is usually higher in emergency surgery patients).The major weakness of prognostic studies of surgery in cirrhosis is their retrospective nature and the lack of prospective validation studies.
Surgery in cirrhosis is associated with a high morbidity and mortality. Retrospectively reported prognostic factors include emergency procedures, liver function (MELD/Child-Pugh scores) and portal hypertension (assessed by indirect markers). This study assessed the prognostic role of hepatic venous pressure gradient (HVPG) and other variables in elective extrahepatic surgery in patients with cirrhosis. A total of 140 patients with cirrhosis (Child-Pugh A/B/C: 59/37/4%), who were due to have elective extrahepatic surgery (121 abdominal; 9 cardiovascular/thoracic; 10 orthopedic and others), were prospectively included in 4 centers (2002–2011). Hepatic and systemic hemodynamics (HVPG, indocyanine green clearance, pulmonary artery catheterization) were assessed prior to surgery, and clinical and laboratory data were collected. Patients were followed-up for 1 year and mortality, transplantation, morbidity and post-surgical decompensation were studied. Ninety-day and 1-year mortality rates were 8% and 17%, respectively. Variables independently associated with 1-year mortality were ASA class (American Society of Anesthesiologists), high-risk surgery (defined as open abdominal and cardiovascular/thoracic) and HVPG. These variables closely predicted 90-, 180- and 365-day mortality (C-statistic >0.8). HVPG values >16 mmHg were independently associated with mortality and values ≥20 mmHg identified a subgroup at very high risk of death (44%). Twenty-four patients presented persistent or de novo decompensation at 3 months. Low body mass index, Child-Pugh class and high-risk surgery were associated with death or decompensation. No patient with HVPG <10 mmHg or indocyanine green clearance >0.63 developed decompensation. ASA class, HVPG and high-risk surgery were prognostic factors of 1-year mortality in cirrhotic patients undergoing elective extrahepatic surgery. HVPG values >16 mmHg, especially ≥20 mmHg, were associated with a high risk of post-surgical mortality. Development of PHT in cirrhosis is associated with marked systemic and splanchnic hemodynamic disturbances that progress in parallel to cirrhosis and are of prognostic significance.16These disturbances impact on cardiopulmonary and renal circulation and may contribute to post-surgical complications.Although PHT has been evaluated in several studies, it has always been done by means of indirect signs (clinical, laboratory or imaging) such as the presence of splenomegaly, ascites, encephalopathy, esophageal varices or a low platelet count.9,10Although the presence of these signs in cirrhosis is unequivocally associated with clinically significant PHT, it may also be present in their absence.17,18In addition, studies assessing the prognostic value of PHT in the natural history of cirrhosis have identified different risk thresholds.Indeed, most clinical events occurring in cirrhosis are associated with the degree of PHT: ascites and collateral formation for HVPG values ≥10 mmHg, variceal bleeding when ≥12 mmHg, and worse prognosis if ≥20 mmHg for variceal bleeding.18HVPG has an important value for prognostic stratification in surgery for hepatocellular carcinoma (HCC), but studies assessing its prognostic value in extrahepatic surgery are lacking.18,19In this regard, accurate assessments of the severity of PHT and of liver function by HVPG measurement and indocyanine green clearance, respectively, might reveal more sensitive prognostic factors for post-surgical morbidity and mortality in cirrhosis.19,20
Although patients with cryptogenic cirrhosis have historically been considered as having “burnt-out” non-alcoholic steatohepatitis (NASH), some controversy remains. The aim of this study was to compare outcomes of patients with cryptogenic cirrhosis and NASH-related cirrhosis from a cohort with longitudinal follow-up data. Patients with cryptogenic cirrhosis or NASH cirrhosis were screened for a clinical trial. Patients with <5% hepatic steatosis regardless of other histologic features were considered to have cryptogenic cirrhosis. Clinico-laboratory data and adjudicated liver-related events (e.g. decompensation, qualification for transplantation, death) were available. A total of 247 patients with cirrhosis (55.3 ± 7.4 years, 37% male) were included; 144 had NASH cirrhosis and 103 had cryptogenic cirrhosis. During a median follow-up of 29 (IQR 21–33) months (max 45 months), 20.6% of patients had liver-related clinical events. Patients with NASH cirrhosis and cryptogenic cirrhosis were of a similar age and gender, as well as having a similar body mass index, PNPLA3 rs738409 genotype, and prevalence of diabetes (p >0.05). However, patients with cryptogenic cirrhosis had higher serum fibrosis markers and greater collagen content and α-smooth muscle actin expression on liver biopsy. Compared to cirrhotic patients with NASH, patients with cryptogenic cirrhosis experienced significantly shorter mean time to liver-related clinical events (12.0 vs. 19.4 months; p = 0.001) with a hazard ratio of 1.76 (95% CI 1.02–3.06). Populations with NASH and cryptogenic cirrhosis have similar demographics, but patients with cryptogenic cirrhosis have evidence of more active fibrosis and a higher risk of liver-related clinical events. Thus, we believe these patients belong to the same spectrum of disease, with cryptogenic cirrhosis representing a more advanced stage of fibrosis. Non-alcoholic steatohepatitis (NASH) is not only a very common cause of chronic liver disease worldwide, but also is a subtype of non-alcoholic fatty liver disease (NAFLD) that can potentially progress to cirrhosis and its complications.1A number of studies have shown that approximately one-third of patients with NAFLD can develop progressive liver disease, while 20% develop cirrhosis and are at increased risk of mortality.1,2
Acute-on-chronic liver failure (ACLF) is characterised by the presence of organ failure in patients with decompensated cirrhosis and is associated with high short-term mortality. However, there are limited data on the prevalence and short-term outcomes of ACLF in patients with cirrhosis seen in the US. We aimed to study the prevalence and risk factors associated with the development and short term mortality in a large cohort of patients in the US. Using the US Department of Veterans Affairs (VA) Corporate Data Warehouse, we identified patients with ACLF during hospitalisation for decompensated cirrhosis at any of the 127 VA hospitals between January 1, 2004, and December 31, 2014. We examined the prevalence of ACLF and variables associated with 28- and 90-day mortality in ACLF, and trends in prevalence and survival over time. Of 72,316 patients hospitalised for decompensated cirrhosis, 19,082 (26.4%) patients met the criteria of ACLF on admission. Of these, 12.8% had 1, 10.1% had 2, and 3.5% had 3 or more organ failures. Overall, 25.5% and 40.0% of ACLF patients died within 28 days and 90 days of admission, respectively. Older age, White race, liver cancer, higher model for end-stage liver disease sodium corrected score, and non-liver transplant centre were associated with increased risk of death in ACLF. Over the study period, the prevalence of ACLF decreased, and all grades but ACLF-3 had improvement in survival. In a US cohort of hospitalised patients with decompensated cirrhosis, ACLF was common and associated with high short-term mortality. Over a decade, ACLF prevalence decreased but survival improvement of ACLF-3 was not seen. Early recognition and aggressive management including timely referral to transplant centres may lead to improved outcomes in ACLF. Acute-on-chronic liver failure (ACLF) is a recently recognised condition characterised by multiorgan failure in patients with decompensated cirrhosis and associated with high short-term mortality.1–3
Acute-on-chronic liver failure (ACLF) is characterised by the presence of organ failure in patients with decompensated cirrhosis and is associated with high short-term mortality. However, there are limited data on the prevalence and short-term outcomes of ACLF in patients with cirrhosis seen in the US. We aimed to study the prevalence and risk factors associated with the development and short term mortality in a large cohort of patients in the US. Using the US Department of Veterans Affairs (VA) Corporate Data Warehouse, we identified patients with ACLF during hospitalisation for decompensated cirrhosis at any of the 127 VA hospitals between January 1, 2004, and December 31, 2014. We examined the prevalence of ACLF and variables associated with 28- and 90-day mortality in ACLF, and trends in prevalence and survival over time. Of 72,316 patients hospitalised for decompensated cirrhosis, 19,082 (26.4%) patients met the criteria of ACLF on admission. Of these, 12.8% had 1, 10.1% had 2, and 3.5% had 3 or more organ failures. Overall, 25.5% and 40.0% of ACLF patients died within 28 days and 90 days of admission, respectively. Older age, White race, liver cancer, higher model for end-stage liver disease sodium corrected score, and non-liver transplant centre were associated with increased risk of death in ACLF. Over the study period, the prevalence of ACLF decreased, and all grades but ACLF-3 had improvement in survival. In a US cohort of hospitalised patients with decompensated cirrhosis, ACLF was common and associated with high short-term mortality. Over a decade, ACLF prevalence decreased but survival improvement of ACLF-3 was not seen. Early recognition and aggressive management including timely referral to transplant centres may lead to improved outcomes in ACLF. Previous research has provided insight into the prevalence and outcomes of patients with ACLF, but most studies were conducted in European cohorts of patients with cirrhosis;2 data from the United States are mainly limited to a single cohort of fewer than 3,000 patients with cirrhosis.4In addition, these studies have been conducted in highly specialised centres, with possibly limited generalisability to patients seen outside the tertiary care settings.Hence, only limited data are available on the prevalence and short-term outcomes of ACLF in patients with cirrhosis seen in the United States.
Acute-on-chronic liver failure (ACLF) is characterised by the presence of organ failure in patients with decompensated cirrhosis and is associated with high short-term mortality. However, there are limited data on the prevalence and short-term outcomes of ACLF in patients with cirrhosis seen in the US. We aimed to study the prevalence and risk factors associated with the development and short term mortality in a large cohort of patients in the US. Using the US Department of Veterans Affairs (VA) Corporate Data Warehouse, we identified patients with ACLF during hospitalisation for decompensated cirrhosis at any of the 127 VA hospitals between January 1, 2004, and December 31, 2014. We examined the prevalence of ACLF and variables associated with 28- and 90-day mortality in ACLF, and trends in prevalence and survival over time. Of 72,316 patients hospitalised for decompensated cirrhosis, 19,082 (26.4%) patients met the criteria of ACLF on admission. Of these, 12.8% had 1, 10.1% had 2, and 3.5% had 3 or more organ failures. Overall, 25.5% and 40.0% of ACLF patients died within 28 days and 90 days of admission, respectively. Older age, White race, liver cancer, higher model for end-stage liver disease sodium corrected score, and non-liver transplant centre were associated with increased risk of death in ACLF. Over the study period, the prevalence of ACLF decreased, and all grades but ACLF-3 had improvement in survival. In a US cohort of hospitalised patients with decompensated cirrhosis, ACLF was common and associated with high short-term mortality. Over a decade, ACLF prevalence decreased but survival improvement of ACLF-3 was not seen. Early recognition and aggressive management including timely referral to transplant centres may lead to improved outcomes in ACLF. Studies show that the number of organ failures – used to define the stage of disease – is the strongest predictor of short-term mortality in ACLF.Other factors, including patient demographics, aetiology of cirrhosis, and type of precipitating factors have been variably implicated in impacting outcomes in ACLF.Healthcare system factors also predict the quality and outcomes of patients with cirrhosis in general.Yet, there is little information about which of these factors may influence ACLF outcomes and even less information about how the effect of these factors varies by ACLF stage overall and over time.
Despite direct-acting antivirals being highly effective at eradicating hepatitis C virus infection, their impact on the development of hepatocellular carcinoma (HCC) remains controversial. We analyzed the clinical and radiological outcome of cirrhotic patients treated with interferon-free regimens to estimate the risk of developing HCC. This was a retrospective multicenter study focusing on cirrhotic patients treated with direct-acting antivirals until December 2016. Clinical and radiologic characteristics were collected before the start of antiviral therapy, at follow-up and at HCC development. Diagnosis of HCC was centrally validated and its incidence was expressed as HCC/100 person-years. A total of 1,123 patients were included (60.6% males, 83.8% Child-Pugh A) and 95.2% achieved a sustained virologic response. Median time of follow-up was 19.6 months. Seventy-two patients developed HCC within a median of 10.3 months after starting antiviral treatment. HCC incidence was 3.73 HCC/100 person-years (95% CI 2.96–4.70). Baseline liver function, alcohol intake and hepatic decompensation were associated with a higher risk of HCC. The relative risk was significantly increased in patients with non-characterized nodules at baseline 2.83 (95% CI 1.55–5.16) vs. absence of non-characterized nodules. When excluding these patients, the risk remained increased. These data expose a clear-cut time association between interferon-free treatment and HCC. The mechanisms involved in the increased risk of HCC emergence in the short term require further investigation. Elimination of hepatitis C virus (HCV) infection is now a feasible goal through the availability of oral direct-acting antivirals (DAAs).1Treatment is safe and it achieves a high rate of sustained virologic response (SVR) both in cirrhotic and non-cirrhotic patients.1Progressive liver function impairment is halted in patients that achieve SVR and long-term follow-up studies should define the impact of successful treatment on the incidence of hepatocellular carcinoma (HCC).Malignant transformation occurs years before clinical recognition and thus, even if the viral infection is solved, the progression of transformed clones may still take place and ultimately, result in clinically detectable HCC.2–5Hence, the elimination of viral infection should prevent new oncogenic events and HCC incidence is likely to decrease after several years.However, the current controversial issue is not whether HCC incidence will vanish in the long-term, but whether there is a time-associated peak in HCC incidence after DAA therapy.6Such a peak would mimic the increased HCC recurrence associated with DAAs that was reported some time ago and is still a matter of debate.7–12The suggested mechanism for an increased recurrence risk was an imbalance in immune surveillance related to a sudden change in hepatic inflammation, which may also occur in patients without a prior diagnosis of HCC.11,13–16The absence of randomized controlled trials prevents a direct comparison between treated and untreated patients.Thus, any insight into this issue should come from careful description of HCC development in large cohorts of patients treated with DAA.11Comparison should be attempted with existing data of almost contemporary cohorts of untreated patients.Any comparisons must consider that impaired liver function does not impede DAA therapy, as was the case for interferon-based regimens.In that sense, the prospective HALT-C17 and EPIC trials18 reported the HCC risk in 2 well-followed homogeneous cohorts of untreated HCV-viremic patients with compensated cirrhosis, in which HCC development was the main endpoint.The prospective CIRVIR study in France has also provided the follow-up data in a population of HCV cirrhotic patients undergoing HCC screening by ultrasound (US).19,20Hence, they offer valuable comparative information.Unfortunately, none of the registration studies assessing the efficacy and safety of DAAs included prospective US examination to detect HCC and follow-up was too short in some studies.11Therefore, they are not useful to attempt any comparison.
Baveno VI and expanded Baveno VI criteria can avoid the need for esophagogastroduodenoscopy (EGD) to screen for varices needing treatment (VNT) in a substantial proportion of compensated patients with viral and/or alcoholic cirrhosis. This multicenter, cross-sectional study aims to validate these criteria in patients with compensated cirrhosis due to non-alcoholic fatty liver disease (NAFLD), accounting for possible differences in liver stiffness measurement (LSM) values between M and XL probes. We assessed 790 patients with NAFLD-related compensated cirrhosis who had EGD within six months of a reliable LSM, measured by FibroScan® using M and/or XL probe. Baveno VI and expanded Baveno VI criteria were tested. The main variable used to optimize criteria was the percentage of endoscopies spared, keeping the risk of missing large VNT below a 5% threshold. LSM was measured by both M and XL probes (training set) in 314 patients, while only M or XL probe (validation sets) were used to measure LSM in 338 and 138 patients, respectively. In the training set, use of Baveno VI and expanded Baveno VI criteria reduced the number of EGD by 33.3% and by 58%, with 0.9% and 3.8% of large esophageal varices missed, respectively. The best thresholds to rule-out VNT were identified as platelet count >110,000/mm3 and LSM <30 kPa for M probe, and platelet count >110,000/mm3 and LSM <25 kPa for XL probe (NAFLD cirrhosis criteria). Thus, usage of NAFLD cirrhosis criteria would have led to an absolute reduction in the number of EGD screened patients of 34.7% and 10.5% with respect to Baveno VI and expanded Baveno VI criteria, respectively. The new NAFLD cirrhosis criteria, established for the FibroScan probe, can reduce the use of EGD for screening of VNT in NAFLD cirrhosis by more than half, with a chance of missing VNT below 5%. The pandemic spreading of obesity and diabetes makes non-alcoholic fatty liver disease (NAFLD) the most rapidly growing cause of chronic liver disease and cirrhosis,1 an increasing risk factor for hepatocellular carcinoma,2 and an emerging indication for liver transplantation.3Consistent with these data, the management of patients with NAFLD-related cirrhosis represents a challenge in terms of epidemiological, clinical and economic burden.In this complex picture, the diagnosis of esophageal varices (EV) and especially large (grade 2/3) EV requiring primary prophylaxis (varices needing treatment [VNT]), is of paramount prognostic importance in all patients with cirrhosis, including those with NAFLD.4,5However, VNT are not frequent in patients with compensated cirrhosis, and strategies to reduce the number of unnecessary esophagogastroduodenoscopy (EGD) procedures have been proposed.Recently, the Baveno VI guidelines proposed that compensated cirrhotic patients with a liver stiffness measurement (LSM) <20 kPa and a platelet count >150,000/μl can avoid screening endoscopy,6 the specificity of this strategy for excluding VNT being validated in different studies.7Furthermore, expanded Baveno VI criteria, obtained by optimizing LSM and platelet count (PLT) values (<25 kPa and >110,000/μl, respectively), have also been proposed and demonstrated to spare a higher proportion of unnecessary EGD when compared to Baveno VI criteria.8
Baveno VI and expanded Baveno VI criteria can avoid the need for esophagogastroduodenoscopy (EGD) to screen for varices needing treatment (VNT) in a substantial proportion of compensated patients with viral and/or alcoholic cirrhosis. This multicenter, cross-sectional study aims to validate these criteria in patients with compensated cirrhosis due to non-alcoholic fatty liver disease (NAFLD), accounting for possible differences in liver stiffness measurement (LSM) values between M and XL probes. We assessed 790 patients with NAFLD-related compensated cirrhosis who had EGD within six months of a reliable LSM, measured by FibroScan® using M and/or XL probe. Baveno VI and expanded Baveno VI criteria were tested. The main variable used to optimize criteria was the percentage of endoscopies spared, keeping the risk of missing large VNT below a 5% threshold. LSM was measured by both M and XL probes (training set) in 314 patients, while only M or XL probe (validation sets) were used to measure LSM in 338 and 138 patients, respectively. In the training set, use of Baveno VI and expanded Baveno VI criteria reduced the number of EGD by 33.3% and by 58%, with 0.9% and 3.8% of large esophageal varices missed, respectively. The best thresholds to rule-out VNT were identified as platelet count >110,000/mm3 and LSM <30 kPa for M probe, and platelet count >110,000/mm3 and LSM <25 kPa for XL probe (NAFLD cirrhosis criteria). Thus, usage of NAFLD cirrhosis criteria would have led to an absolute reduction in the number of EGD screened patients of 34.7% and 10.5% with respect to Baveno VI and expanded Baveno VI criteria, respectively. The new NAFLD cirrhosis criteria, established for the FibroScan probe, can reduce the use of EGD for screening of VNT in NAFLD cirrhosis by more than half, with a chance of missing VNT below 5%. Both Baveno VI and expanded Baveno VI criteria were proven to be reliable in large cohorts of patients with compensated cirrhosis, mostly of viral and/or alcoholic etiologies, while patients with NAFLD-related cirrhosis are absent or under-represented.This is a significant limitation considering that in patients with NAFLD-related cirrhosis, LSM could also account for the severity of steatosis and obesity.9,10Furthermore, studies assessing the diagnostic accuracy of Baveno VI and expanded Baveno VI criteria considered LSM values obtained only by M probe.However, there is a high prevalence of obesity in patients with NAFLD, resulting in the frequent need to measure LSM by XL probe in cases of M probe failure.
Current antiviral therapies lack the potential to eliminate persistent hepatitis B virus (HBV) infection. HBV-specific T cells are crucial for HBV control and have recently been shown to be protective in patients following discontinuation of antiviral therapy. Thus, T cell-based approaches may greatly improve the therapeutic landscape of HBV infection. We aimed to augment HBV-specific CD4 T cells from chronically infected patients by targeting different immunological pathways. Expression of various co-stimulatory and inhibitory receptors on HBV- and influenza-specific CD4 T cells was analyzed directly ex vivo by MHC class II-tetramers. Patients infected with HBV genotype D were screened for CD4 T cell responses by IFN-γ ELISpot and intracellular cytokine staining following stimulation with overlapping peptides (OLPs) spanning the HBV-polyprotein. Stimulation with recombinant IL-7, an agonistic OX40-antibody or blockade of PD-L1 was performed in antigen-specific in vitro cultures. Cytokine secretion and expression of transcription factors were analyzed by flow cytometry. Responses targeting influenza, Epstein-Barr virus and tetanus toxoid served as controls. Tetramer-staining revealed that the IL-7 receptor-alpha (CD127), OX40 and PD-1 constitute possible therapeutic targets as they were all strongly expressed on HBV-specific CD4 T cells ex vivo. The HBV-specific CD4 T cell responses identified by OLP screening targeted predominantly the HBV-polymerase and core proteins. Combined OX40 stimulation and PD-L1 blockade significantly augmented IFN-γ and IL-21 producing HBV-specific CD4 T cells in vitro, suggesting active T helper type 1 cell and follicular T helper cell programs. Indeed, transcription factors T-bet and Bcl6 were strongly expressed in cytokine-producing cells. Combined OX40 stimulation and PD-L1 blockade augmented secretion of the helper T cell signature cytokines IFN-γ and IL-21, suggesting that immunotherapeutic approaches can improve HBV-specific CD4 T cell responses. Persistent infection with the hepatitis B virus (HBV) is a major risk factor for the development of chronic liver injury, cirrhosis and hepatocellular carcinoma and affects an estimated 350 million people worldwide.1While there is a prophylactic vaccination to prevent chronic infection, therapeutic approaches for persistent infection are limited and mostly fail to eliminate the virus.2Promising therapeutic approaches include T cell-based immunotherapy as both HBV-specific CD4 and CD8 T cells have been shown to be required for viral clearance but are functionally impaired in the context of chronic infection.3–5
Current antiviral therapies lack the potential to eliminate persistent hepatitis B virus (HBV) infection. HBV-specific T cells are crucial for HBV control and have recently been shown to be protective in patients following discontinuation of antiviral therapy. Thus, T cell-based approaches may greatly improve the therapeutic landscape of HBV infection. We aimed to augment HBV-specific CD4 T cells from chronically infected patients by targeting different immunological pathways. Expression of various co-stimulatory and inhibitory receptors on HBV- and influenza-specific CD4 T cells was analyzed directly ex vivo by MHC class II-tetramers. Patients infected with HBV genotype D were screened for CD4 T cell responses by IFN-γ ELISpot and intracellular cytokine staining following stimulation with overlapping peptides (OLPs) spanning the HBV-polyprotein. Stimulation with recombinant IL-7, an agonistic OX40-antibody or blockade of PD-L1 was performed in antigen-specific in vitro cultures. Cytokine secretion and expression of transcription factors were analyzed by flow cytometry. Responses targeting influenza, Epstein-Barr virus and tetanus toxoid served as controls. Tetramer-staining revealed that the IL-7 receptor-alpha (CD127), OX40 and PD-1 constitute possible therapeutic targets as they were all strongly expressed on HBV-specific CD4 T cells ex vivo. The HBV-specific CD4 T cell responses identified by OLP screening targeted predominantly the HBV-polymerase and core proteins. Combined OX40 stimulation and PD-L1 blockade significantly augmented IFN-γ and IL-21 producing HBV-specific CD4 T cells in vitro, suggesting active T helper type 1 cell and follicular T helper cell programs. Indeed, transcription factors T-bet and Bcl6 were strongly expressed in cytokine-producing cells. Combined OX40 stimulation and PD-L1 blockade augmented secretion of the helper T cell signature cytokines IFN-γ and IL-21, suggesting that immunotherapeutic approaches can improve HBV-specific CD4 T cell responses. Blockade of the inhibitory programmed cell death 1 (PD-1)/programmed cell death ligand 1 (PD-L1) pathway has been shown to significantly enhance T cell responses in various settings and has already been implemented in clinical practice in the treatment of various solid cancers.6However, data from clinical trials in patients with both hepatocellular carcinoma and chronic HBV infection suggest that PD-1 blockade alone is insufficient to improve viral control as none of the included patients achieved HBV surface antigen (HBsAg) seroconversion during the course of therapy.7Therefore, additional approaches are required to improve T cell responses in chronic HBV infection.The co-stimulatory molecule OX40 (CD134) could be one such approach as it has also been shown to serve as a target for immunotherapeutic approaches.8In a recent phase I trial in late stage cancer patients, it has safely been administered with remarkable effects on tumor burden in some patients.9Moreover, OX40 signaling has been shown to facilitate immunological control against a variety of viruses, including HBV.10–12Similarly, it has been shown that IL-7 can substantially improve the antiviral CD4 T cell response in a mouse model of persistent viral infection.13Thus, targeting PD-1, OX40 or the IL-7 axis might improve the functionality of HBV-specific CD4 T cells.
