MITRE: DESCRIPTION OF THE ALEMBIC SYSTEM USED FOR MUC-6 John Aberdeen, John Burger, David Day, Lynette Hirschman, Patricia Robinson, and Marc Vilain The MITRE Corporation 202 Burlington Rd .  In addition to these finite state pattern approaches, a variant of Brill rules has been applied to the problem, as outlined in (Aberdeen et al, 1995). The typical machine learning approaches for English NE are transformation-based learning [Aberdeen et al 1995]. Each document in the collection to be summarized is processed by a sentence tokenizer, the Alembic part-of-speech tagger (Aberdeen et al 1995). Many research groups are making progress toward efficient customization, such as BBN (Weischedel, 1995), NYU (Grishman, 1995), SRI (Appelt et al, 1995), SRA (Krupka, 1995), MITRE (Aberdeen et al, 1995), UMass (Fisher et al, 1995) ... etc.
Cascaded Grammatical Relation Assignment In this paper we discuss cascaded Memory- Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder. This experiment is repeated on two different existing systems, which are reported in Buchholz et al (1999) and Carroll et al (1999), respectively. For example, the Buchholz et al (1999) system ignores verb complements of verbs and is designed to look for relationships to verbs and not GRs that exist between nouns, etc. For example, in our experiments, the Buchholz et al (1999) system uses the annotation np-sbj to indicate a subject, while the Ferro et al (1999) system uses the annotation subj.  As an example, take the results of using the Buchholz et al (1999) system on the 1391GR instance training set. When using the Buchholz et al (1999) system, the improvement in the other modifier is now no longer statistically significant. We rerun rule learning on the smaller (1391 GR instance) training set with a Union of the Buchholz et al (1999) and Carroll et al (1999) systems's translated GR annotations. TheIaU F-score is statistically signicantly better than the F-scores with either using Buchholz et al (1999) (IaB) or not using any initial annotations (NI). Preiss (2003) compares the parsers of Collins (2003) and Charniak (2000), the GR finder of Buchholz et al (1999), and the RASP parser, using the Carroll et al (1998) gold-standard. Buchholz et al (1999) achieve 71.2 F-score for grammatical relation assignment on automatically tagged and chunked text after training on about 40,000 Wall Street Journal sentences. The computation of grammatical relations from shallow parsers or chunkers is still at an early stage (Buchholz et al, 1999, Carroll et al, 1998) and there are few other robust semantic processors, and none in the medical domain. 
Decoding Complexity In Word-Replacement Translation Models Statistical machine translation is a relatively new approach to the long-standing problem of translating human languages by computer. Current statistical techniques uncover translation rules from bilingual training texts and use those rules to translate new texts. The general architecture is the source-channel model: an English string is statistically generated (source), then statistically transformed into French (channel). In order to translate (or &quot;decode&quot;) a French string, we look for the most likely English source. We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence. We trace this complexity to factors not present in other decoding problems. The bias introduced by TMEMs is a practical alternative to finding optimal translations, which is NP-complete (Knight, 1999). However, as phrase-based decoding usually casts translation as a string concatenation problem and permits arbitrary permutation, it proves to be NP-complete (Knight, 1999). In (Knight,1999) it was proved that the Exact Decoding problem is NP-Hard when the language model is a bigram model. Note that our results for decoding are sharper than that of (Knight, 1999). However allowing reordering in translation is computationally expensive and in some cases even provably NP-complete (Knight, 1999). In theory, this process can be reduced to the Traveling Salesman Problem and thus requires an exponential time algorithm (Knight, 1999). Part of the complexity arises from the expressive power of the translation model: for example, a phrase or word-based model with full reordering has exponential complexity (Knight, 1999). This approach offers four features absent from IBM-style models: (1) a recursive phrase-based translation, (2) a syntax-based language model, (3) the ability to condition a word's translation on the translation of syntactically related words, and (4) polynomial-time optimal alignment and decoding (Knight, 1999). Both of the two steps are very time-consuming due to the exponential number of translation rules and the complex nature of machine translation as an NP-hard search problem (Knight, 1999). If arbitrary re-orderings are allowed, the search problem is NP-complete (Knight, 1999). Under certain restrictions, both algorithms handle MT-related problems efficiently that are generally NP complete (Knight, 1999). In the general case, no efficient search algorithm exists to search all word or phrase reorderings (Knight, 1999). (Knight, 1999) shows that the decoding problem for SMT as well as some bilingual tiling problems are NP-complete, so no efficient algorithm exists in the general case. Investigation of the computational complexity of translation models has started in (Knight, 1999) for word-to-word models. Knight (1999) has shown that even for a simple form of statistical MT models, the decoding problem is NP-complete. in particular (Knight, 1999) has shown that any TSP instance can be mapped to a sub-case of a word-based SMT model, demonstrating NP-hardness of the decoding task. As already mentioned, the similarity between SMT decoding and TSP was recognized in (Knight, 1999), who focussed on showing that any TSP can be reformulated as a sub-class of the SMT decoding problem, proving that SMT decoding is NP-hard. Knight (1999) has shown the problem to be NP-complete.  It has been known that phrase-based decoding should be constrained to some extent not only for transferring the NP-hard problem (Knight,1999) into a tractable one in practice but also for improving translation quality.
A Plan-Based Analysis Of Indirect Speech Act We propose an account of indirect forms of speech acts to request and inform based on the hypothesis that language users can recognize actions being performed by others, infer goals being sought, and cooperate in their achievement. This cooperative behaviour is independently motivated and may or may not be intended by speakers. If the hearer believes it is intended, he or she can recognize the speech act as indirect; otherwise it is interpreted directly. Heuristics are suggested to decide among the interpretations.  In addition to conveying information about one's own mental state, pragmatic principles and rules, such as those we have presented, may be deployed to reason about the intentions and beliefs of others (Perrault and Allen, 1980). Indirect speech acts are studied by Clark (1979), Perrault and Allen (1980), Allen and Perrault (1980) and Asher and Lascarides (2003), who identify a wide range of factors that govern how speakers convey their intended messages and how hearers seek to uncover those messages from uncertain and conflicting signals. This is computationally much less complex than classical speech act planning (Perrault and Allen, 1980), in which the intended physical effect comes at the end of a long chain of inferences.
Chinese Whispers - An Efficient Graph Clustering Algorithm And Its Application To Natural Language Processing Problems We introduce Chinese Whispers, a randomized graph-clustering algorithm, which is time-linear in the number of edges. After a detailed definition of the algorithm and a discussion of its strengths and weaknesses, the performance of Chinese Whispers is measured on Natural Language Processing (NLP) problems as diverse as language separation, acquisition of syntactic word classes and word sense disambiguation. At this, the fact is employed that the small-world property holds for many graphs in NLP. For assigning classes, we use the Chinese Whispers (CW) graph-clustering algorithm, which has been proven useful in NLP applications as described in (Biemann 2006). We use the co-occurrence based graph clustering framework introduced in (Biemann, 2006). We use Chinese Whispers (Biemann, 2006), a special case of MCL that performs the iteration in a more aggressive way, with an optimized linear complexity with the number of graph edges. The weight applied to each edge connecting vertices vi and vj (collocations cab ,cde) is the maximum of their conditional probabilities (max (p (cab|cde), p (cde|cab))). Finally, the graph is clustered using Chinese whispers (Biemann, 2006). To cluster the graph we used the Chinese Whispers clustering tool (Biemann, 2006), whose algorithm does not require to pre-set the desired number of clusters and is reported to outperform other algorithms for several NLP tasks. To generate the projection, sentences were represented as vectors of terms weighted by their frequency in each sentence. As described in (Biemann, 2006), the neighborhood graph is clustered with Chinese Whispers. The first is Chinese Whispers (CW; Biemann (2006)), a randomized graph-clustering algorithm which like the HRG also takes as input a graph with weighted edges. Chinese Whispers (CW) (Biemann, 2006) was used to cluster the graph. The graph is partitioned into vertex clusters representing semantic roles using a variant of Chinese Whispers, a graph-clustering algorithm proposed by Biemann (2006). A wide range of methods exist for finding partition sin graphs (Schaeffer, 2007), besides Chinese Whispers (Biemann, 2006), which could be easily applied to the semantic role induction problem. The Chinese Whispers algorithm itself (Biemann, 2006) has been previously applied to several tasks including word sense induction (Klapaftis and M., 2010) and unsupervised part-of-speech tagging (Christodoulopoulos et al, 2010). Graph partitioning is realized with a variant of Chinese Whispers (Biemann, 2006) whose details are given below. In Biemann and Teresniak (2005) and more detailed in Biemann (2006a), the Chinese Whispers (CW) Graph Clustering algorithm is described, which is a randomized algorithm with edge-linear run-time. In Biemann (2006b), the tagger output was directly compared to supervised taggers for English, German and Finnish via information-theoretic measures. Chinese Whispers (CW) (Biemann, 2006) is a parameter-free graph clustering method that has been applied in sense induction to cluster the cooccurrence graph of a target word (Biemann, 2006), as well as a graph of collocations related to the tar get word (Klapaftis and Manandhar, 2008). For clustering the graphs of CWU and CWW we employ, Chinese Whispers (Biemann, 2006). To obtain a clustering of nouns, we used Chinese Whispers (Biemann, 2006), a randomized graph-clustering algorithm.
Supervised And Unsupervised Learning For Sentence Compression Summarization - Step Sentence Knight and Marcu (Knight and Marcu, 2000) (K&M) present a noisy-channel model for sentence compression. The main difficulty in using this method is the lack of data; Knight and Marcu use a corpus of 1035 training sentences. More data is not easily available, so in addition to improving the original K&M noisy-channel model, we create unsupervised and semi-supervised models of the task. Finally, we point out problems with modeling the task in this way. They suggest areas for future research. Recently Turner and Charniak (2005) presented supervised and semi-supervised versions of the Knight and Marcu noisy-channel model. We also exploited more general tree productions known as synchronous tree substitution grammar (STSG) rules, in an approach quite similar to (Turner and Charniak, 2005). Alternatively, the rules of compression are approximated from a non-parallel corpus (e.g., the Penn Treebank) by considering context-free grammar derivations with matching expansions (Turner and Charniak 2005). Although both models yield comparable performance, Turner and Charniak (2005) show that the latter is not an appropriate compression model since it favours uncompressed sentences over compressed ones.     Turner and Charniak (2005) argue that the noisy-channel model is not an appropriate compression model since it uses a source model trained on uncompressed sentences and as a result tends to consider compressed sentences less likely than uncompressed ones. Turner and Charniak (2005) have shown that applying handcrafted rules for trimming sentences can improve both content and linguistic quality. While data sparsity is a common problem of many NLP tasks, it is much more severe for sentence compression, leading Turner and Charniak (2005) to question the applicability of the channel model for this task altogether. Turner and Charniak (2005) question the viability of a noisy channel model for the sentence compression task. See (Turner and Charniak, 2005) for a discussion of problems that can occur for text compression when using a language model trained on data from the uncompressed side.  Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. Turner and Charniak (Turner and Charniak, 2005) revised and improved Knight and Marcu's algorithm; however, their algorithm also uses only mother and daughter relations and has the same problem. Turner and Charniak (Turner and Charniak, 2005) solve this problem by appending special rules that are applied when a mother node and its daughter node have the same label. In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005).
PRINCIPAR - An Efficient Broad-Coverage Principle-Based Parser We present an efI\]cient, broad-coverage, principle-based parser for English. The parser has been implemented in C++ and runs on SUN Sparcstations with X-windows. It con- rains a lexicon with over 90,000 entries, con- structed automatically b applying a set of ex- traction and conversion rules to entries from machine readable dictionaries. Principle-based parsers are also designed with universal grammar in mind (Lin 1994), but have yet to demonstrate large scale coverage in several languages. In this paper, we adopt Minipar (Lin, 1994) to parse sentences and to construct syntactic trees. English dependency trees are provided by Minipar (Lin, 1994). It is based on Principar, which is described in Lin (1994). These features are obtained by parsing a large corpus using Minipar (Lin 1994), a broad-coverage English parser. We used Minipar (Lin 1994), a broad coverage parser, to parse 3GB of newspaper text from the Aquaint (TREC-9) collection. We parse questions and candidate sentences with MiniPar (Lin, 1994), a fast and robust parser for grammatical dependency relations. We used Minipar (Lin 1994), a broad coverage English parser, to parse about 1GB (144M words) of newspaper text from the TREC collection (1988 AP Newswire, 1989-90 LA Times, and 1991 San Jose Mercury) at a speed of about 500words/second on a PIII-750 with 512MB memory. We parse a 6 GB newspaper (TREC9 and TREC 2002 collection) corpus using the dependency parser Minipar (Lin, 1994). For the co-occurrence model, we used Minipar (Lin 1994), a broad coverage parser, to parse each data set. For the algorithm discussed in Section 4.1, we derived our descriptive properties using the output of the dependency analysis generated by the Minipar (Lin, 1994) dependency parser.  We parsed a 125-million word newspaper corpus with Minipar3, a descendent of Principar (Lin, 1994). We used Minipar (Lin 1994), a broad coverage parser, to analyze text. We used Minipar (Lin 1994), a broad coverage parser, to parse two 3GB corpora (TREC-9 and TREC-2002). English dependency trees are provided by Minipar (Lin, 1994). The Simplified Clause: In order to extract clauses from the text, we use Lin's parser MINI PAR (Lin, 1994).  To get the context values and implement the syntactic filters, we parsed our corpora with Minipar (Lin,1994). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a declarative grammar, and the Stanford parser (Levy and Manning, 2004), which performs a conversion from a standard phrase-structure parse.
Named Entity Recognition Without Gazetteers It is often claimed that Named Entity recognition systems need extensive gazetteers—lists of names of people, organisations, locations, and other named entities. Indeed, the compilation of such gazetteers is sometimes mentioned as a bottleneck in the design of Named Entity recognition systems. We report on a Named Entity recognition system which combines rule-based grammars with statistical (maximum entropy) models. We report on the system's performance with gazetteers of different types and different sizes, using test material from the muc-7 competition. We show that, for the text type and task of this competition, it is sufficient to use relatively small gazetteers of well-known names, rather than large gazetteers of low-frequency names. We conclude with observations about the domain independence of the competition and of our experiments. The importance of dictionaries in NERs has been investigated in the literature (Mikheev et al, 1999).  It has been shown in (Mikheev et al, 1999) that a NE Recognition system performs reasonably well for most classes even without gazetteers. It is common practice in NER to utilize the discourse level to disambiguate items in non predictive contexts (see e.g. Mikheev et al, 1999). Mikheev et al (1999) exploit label consistency information within a document using relatively ad hoc multi-stage labeling procedures. To date, Named Entity Recognition (NER) has only used gazetteers as evidence that a text span could be some kind of place name (LOCATION), even though their finite nature makes lists of names of limited use for classification (Mikheev et al, 1999). Other work has supported the use of gazetteers in general but has found that lists of only moderate size are sufficient to provide most of the benefit (Mikheev et al, 1999). To investigate the role of gazetteers in NER, Mikheev et al (1999) combine grammar rules with maximum entropy models and vary the gazetteer size. The internal and external evidence is being used by all NERC systems such as LTG (Mikheev et al 1999), FASTUS (Hobbs et al 1997), Proteus (Yangarber, Grishman, 1998). In such cases all tokens forming an NE and all their combinations are stored in the dynamic lexicon (Mikheev et al 1999:5). Since many tagging systems utilise gazetteers of known entities, some research has focused on their automatic extraction from the web (Etzioni et al, 2005) or Wikipedia (Toral et al, 2008), although Mikheev et al (1999) and others have shown that larger NE lists do not necessarily correspond to increased NER performance.  Mikheev et al (1999) and Finkel et al (2004) incorporate label consistency information by using ad hoc multi-stage labeling procedures that are effective but special-purpose.
A Shortest Path Dependency Kernel For Relation Extraction We present a novel approach to relation extraction, based on the observation thatthe information required to assert a rela tionship between two named entities in the same sentence is typically capturedby the shortest path between the two entities in the dependency graph. Exper iments on extracting top-level relationsfrom the ACE (Automated Content Ex traction) newspaper corpus show that thenew shortest path dependency kernel outperforms a recent approach based on de pendency tree kernels. In our model, we adopted the subtree kernel method for the shortest path dependency kernel (Bunescu and Mooney, 2005). It has been shown in previous work on relation extraction that the shortest dependency path between any two entities captures the in formation required to assert a relationship between them (Bunescu and Mooney, 2005). The respective dependency parse tree is included through following the shortest dependency path hypothesis (Bunescu and Mooney, 2005), by using the syntactical and dependency information of edges (e) and vertices (v). It has been shown in previous work on relation extraction that the shortest path between any two entities captures the information required to assert a relationship between them (Bunescu and Mooney, 2005). It has been shown in previous work on relation extraction that the shortest dependency path between any two entities captures the information required to assert a relationship between them (Bunescu and Mooney, 2005). The information in the shortest path between two entities in a dependency tree can be used to assert whether a relationship exists between them (Bunescu and Mooney, 2005). 
Catching The Drift: Probabilistic Content Models With Applications To Generation And Summarization We consider the problem of modeling the content structure of texts within a specific do main, in terms of the topics the texts address and the order in which these topics appear. We first present an effective knowledge-leanmethod for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and ex tractive summarization. Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods. This methodology is very similar to the way [Barzilay and Lee, 2004] evaluate their probabilistic TS model in comparison to the approach of [Lapata, 2003]. Thus, there might exist more than one equally good solution for TS, a view shared by almost all TS researchers, but which has not been accounted for in the evaluation methodologies of [Karamanis et al, 2004] and [Barzilay and Lee, 2004]. The simplest formulation we consider is an HMM where each state contains a unigram language model (LM), proposed by Chotimongkol (2008) for task-oriented dialogue and originally developed for discourse analysis by Barzilay and Lee (2004). In this work, we also assume a content model, which we fix to be the document-level HMM as used in Barzilay and Lee (2004). Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. Barzilay and Lee (2004) showed that it is possible to obtain schema-like knowledge automatically from a corpus for the purposes of extracting sentences and ordering them. However, their work represents patterns at the sentence level, and is thus not directly comparable to our work, given our focus on sentence generation. Like Barzilay and Lee (2004), this model was used to order extracted sentences in summaries. Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. Barzilay and Lee (2004) have proposed content models to deal with topic transition in domain specific text. These methods identify regularities in words (Barzilay and Lee, 2004). To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance as signing each sentence a single topic and then topic specific word models can be used: the hidden topic Markov model (Gruber et al2007) that models the transitional topic structure; a global model based on the generalised Mallows model (Chen et al2009), and a HMM based content model (Barzilay and Lee, 2004). Barzilay and Lee (2004)'s knowledge-lean approach attempts to automate the inference of knowledge-rich information using a distributional view of content. This is specific to their approach as both Lapata (2003)'s and Barzilay and Lee (2004)'s approaches are not tailored to summarization and therefore do not experience the topic bias problem. The results for Coreference+Syntax+Salience+ and HMM-Based Content Models are reproduced from Barzilay and Lapata (2008). When compared to the results obtained by Barzilay and Lapata (2008) and Barzilay and Lee (2004), it would appear that direct sentence to-sentence similarity (as suggested by the Barzilay and Lapata baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset. This assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004). However, as Barzilay and Lee (2004) observe, the content of document collections is highly structured, consisting of several topical themes, each with its own vocabulary and ordering preferences. An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004). Following Ruch et al (2003) and Barzilay and Lee (2004), we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts. An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee,2004).
Automatically Labeling Semantic Classes Systems that automatically discover semantic classes have emerged in part to address the limitations of broad-coverage lexical resources such as WordNet and Cyc. The current state of the art discovers many semantic classes but fails to label their concepts. We propose an algorithm labeling semantic and for leveraging them to extract relationships using a top-down approach. We present an algorithm for extracting is-a relations, designed for the terascale, and compare it to a state of the art method that employs deep analysis of text (Pantel and Ravichandran 2004). Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. Our co-occurrence model (Pantel and Ravichandran 2004) makes use of semantic classes like those generated by CBC. These relationships, automatically learned in (Pantel and Ravichandran 2004), include appositions, nominal subjects, such as relationships, and like relationships. The syntactical co-occurrence approach has worst-case time complexity O (n2k), where n is the number of words in the corpus and k is the feature space (Pantel and Ravichandran 2004). Like Chambers and Jurafsky, we also used the discounting method suggested by Pantel and Ravichandran (2004) for low frequency observations. Pantel and Ravichandran (2004) addressed the more specific task of labelling a semantic class by applying Hearst-style lexico-semantic patterns to each member of that class. Pantel and Ravichandran (2004) have used a method that is not related to query expansion, but yet very related to our work.  Lin et al (2003) and Pantel and Ravichandran (2004) have proposed to classify the output of systems based on feature vectors using lexico-syntactic patterns, respectively in order to remove antonyms from a related words list and to name clusters of related terms. We also adopt the 'discount score' to penalize low occuring words (Pantel and Ravichandran, 2004). Few recent attempts on related (though different) tasks were made to classify (Lin et al, 2003) and label (Pantel and Ravichandran, 2004) distributional similarity output using lexical-syntactic patterns, in a pipeline architecture. First, instead of separately addressing the tasks of collecting unlabeled sets of instances (Lin, 1998), assigning appropriate class labels to a given set of instances (Pantel and Ravichandran, 2004), and identifying relevant attributes for a given set of classes (Pasca, 2007), our integrated method from Section 2 enables the simultaneous extraction of class instances, associated labels and attributes. Given pre-existing sets of instances, (Pantel and Ravichandran, 2004) investigates the task of acquiring appropriate class labels to the sets from unstructured text. In (Pantel and Ravichandran, 2004), given a collection of news articles that is both cleaner and smaller than Web document collections, a syntactic parser is applied to document sentences in order to identify and exploit syntactic dependencies for the purpose of selecting candidate class labels. Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. We thus multiply pmi (i, p) with the discounting factor suggested in (Pantel and Ravichandran 2004). (Pantel and Ravichandran, 2004) have proposed an algorithm for labeling semantic classes, which can be viewed as a form of cluster. Pantel and Ravichandran (2004) note that the nouns computer and company both have a WordNet sense that is a hyponym of person, falsely indicating these nouns would be compatible with pronouns like he or she. We use PMI (point-wise mutual information) of hyponymy relation candidate (hyper, hypo) as a collocation feature (Pantel and Ravichandran, 2004), where we assume that hyper and hypo in candidates would frequently co-occur in the same sentence if they hold a hyponymy relation.
Measures Of Distributional Similarity distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences. Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification Long before Weed and Weir, Lee (1999) proposed an asymmetric similarity measure as well. As an asymmetric measure, we examine skew divergence defined by the following equation (Lee, 1999), where Px denotes a probability distribution estimated from a feature set Fx. We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others. We use the cosine similarity measure for window based contexts and the following commonly used similarity measures for the syntactic vector space: Hindle's (1990) measure, the weighted Linmeasure (Wu and Zhou, 2003), the Skew divergence measure (Lee, 1999), the Jensen-Shannon (JS) divergence measure (Lin, 1991), Jaccard's coefficient (van Rijsbergen, 1979) and the Confusion probability (Essen and Steinbiss, 1992). While Lee (1999) argues that -Skew's asymmetry can be advantageous for nouns, this probably does not hold for verbs: verb hierarchies have much shallower structure than noun hierarchies with most verbs concentrated on one level (Miller et al, 1990). In general we may be able to identify related phrases (for example with distributional similarity (Lee, 1999)), but would like to be able to automatically classify the related phrases by the type of the relationship. Similarity measures such as Cosine, Jaccard, Dice, etc (Lee, 1999), can be employed to compute the similarities between the seeds and other feature expressions. The Jensen-Shannon (JS) divergence measure (Rao, 1983) and the -skew divergence measure (Lee, 1999) are based on the Kullback-Leibler (KL) divergence measure. Internally, the ranking of attributes uses Jensen-Shannon (Lee, 1999) to compute similarity scores between internal representations of seed attributes, on one hand, and each of the newly acquired attributes, on the other hand. For words that do not appear in WordNet, we use distributional similarity (Lee, 1999) as a proxy for word relatedness.  We are interested in some distributional similarities (Lee, 1999) given certain context. Dagan et al (1997) find that the symmetric information radius measure performs best on a pseudo word sense disambiguation task, while Lee (1999) find that the asymmetric skew divergence, a generalisation of Kullback-Leibler divergence performs best for improving probability estimates for unseen word co-occurrences. The second approach is from Lee (1999). This choice of similarity measures was motivated by results of studies by (Levy et al 1998) and (Lee 1999) which compared several well known measures on similar tasks and found these three to be superior to many others. Another reason for this choice is that there are different ideas underlying these measures: while the Jaccard's coefficient is a binary measure, L1 and the skew divergence are probabilistic, the former being geometrically motivated and the latter being a version of the information theoretic Kullback Leibler divergence (cf., Lee 1999). Some enhanced KL models were developed to prevent these problems such as Jensen-Shannon (Jianhua, 1991), which introducing a probabilistic variable m, or -Skew Divergence (Lee, 1999), by adopting adjustable variable. Similarly, McCarthy (2000) uses skew divergence (a variant of KL divergence proposed by Lee, 1999) to compare the sense profile of one argument of a verb (e.g., the subject position of the intransitive) to another argument of the same verb (e.g., the object position of the transitive), to determine if the verb participates in an argument alternation involving the two positions.  We compare SPD to other measures applied directly to the (unpropagated) probability profiles given by the Clark-Weir method: the probability distribution distance given by skew divergence (skew) (Lee, 1999), as well as the general vector distance given by cosine (cos).
Annealing Structural Bias In Multilingual Weighted Grammar Induction first show how a structural bias improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004). Next, by annealing the free parameter that controls this bias, we achieve further improvements. We then describe an alternative kind of structural bias, toward “broken” hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement. We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1–17% (absolute) over CE (and 8–30% over EM), achieving to our knowledge the best results on this to date. Our method, is a general technique with broad applicability to hidden-structure discovery problems.  (Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data. For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words. We follow the idea of annealing proposed in Rose et al (1990) and Smith and Eisner (2006) for the ? by gradually loosening hard constraints on ? as the variational EM algorithm proceeds. This is a strict model reminiscent of the successful application of structural bias to grammar induction (Smith and Eisner, 2006). These include the constituent-context model (CCM) (Klein and Manning, 2002), its extension using a dependency model (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar-based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of (Seginer, 2007). Smith and Eisner (2006) propose structural annealing (SA), in which a strong bias for local dependency attachments is enforced early in learning, and then gradually relaxed. Finally, note that structural annealing (Smith and Eisner, 2006) provides 66.7% accuracy on WSJ10 when choosing the best performing annealing schedule (Smith, 2006). These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995). These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. Analogously, Baby Steps induces an early structural locality bias (Smith and Eisner, 2006), then relaxes it, as if annealing (Smith and Eisner, 2004). Smith and Eisner (2006) used a structural locality bias, experimenting on five languages. Notable examples are (Clark, 2003) for unsupervised POS tagging and (Smith and Eisner, 2006) for unsupervised dependency parsing. Following the example of Smith and Eisner (2006), we strip punctuation from the sentences and keep only sentences of length > 10. Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a). These alignment classes are called configurations (Smith and Eisner, 2006a, and following). Thus, our generative model is a quasi-synchronous grammar, exactly as in (Smith and Eisner, 2006a).  Smith and Eisner (2005) use contrastive estimation instead of EM, while Smith and Eisner (2006) use structural annealing which penalizes long-distance dependencies initially, gradually weakening the penalty during training.
An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing We present a novel deterministic dependency parsing algorithm that attempts to create the easiest arcs in the dependency structure first in a non-directional manner. Traditional deterministic parsing algorithms are based on a shift-reduce framework: they traverse the sentence from left-to-right and, at each step, perform one of a possible set of actions, until a complete tree is built. A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right. In contrast, our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step. This allows incorporation of features from already built structures both to the left and to the right of the attachment point. The parser learns both the attachment preferences and the order in which they should be performed. The result is a determinbest-first, which is significantly more accurate than best-first transition based parsers, and nears the performance of globally optimized parsing models. Similar methods to Shen et al (2007) have also been used in Shen and Joshi (2008) and Goldberg and Elhadad (2010). To identify explicit predicate-argument relationships we utilized dependency parsing by the Easy-First parser (Goldberg and Elhadad, 2010). Goldberg and Elhadad (2010) observed that parsing time is dominated by feature extraction and score calculation. BIUTEE provides state-of-the-art pre-processing utilities: Easy-First parser (Goldberg and Elhadad, 2010), Stanford named-entity-recognizer (Finkel et al, 2005) and ArkRef coreference resolver (Haghighi and Klein, 2009), as well as utilities for sentence splitting and numerical-normalizations.  This has some similarity to Goldberg and Elhadad (2010). We are using a multi-sieve approach (Raghunathan et al., 2010), which splits pairwise "co-reference" vs. "non-coreference" decisions to different types and attempts to make the easy decisions first (Goldberg and Elhadad, 2010).  The Hebrew tagger and parsing models are described in Goldberg and Elhadad (2010). We plan to examine to model such a complex structure (granduncle) (Goldberg and Elhadad, 2010) or higher-order structure than third-order for reranking which is computationally expensive for a baseline parser. The easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) is attractive due to its good accuracy, fast speed and simplicity. As an alternative, greedy search which only explores a tiny fraction of the search space is adopted (Goldberg and Elhadad, 2010). The easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) builds a dependency tree by performing two types of actions.  Besides the features in (Goldberg and Elhadad, 2010), we also include some trigram features and valency features which are useful for transition-based dependency parsing (Zhang and Nivre, 2011). Our system not only outperforms the best single system (Bjorkelund et al., 2013) by 1.4%, but it also tops the ensemble system that combines three powerful parsers: the Mate parser (Bohnet, 2010), the Easy-First parser (Goldberg and Elhadad, 2010) and the Turbo parser (Martins et al., 2013). In the context of dependency parsing, the strategy of delaying arc construction when the current configuration is not informative is called the easy-first strategy, and has been first explored by Goldberg and Elhadad (2010). The parsing approach is based upon the non directional easy-first algorithm recently presented by Goldberg and Elhadad (2010). While theoretically slower, this has a limited impact upon actual parsing times. See Goldberg and Elhadad (2010) for more explanation. The feature set is based off the features used by Goldberg and Elhadad (2010) but has a significant number of extensions.
A Latent Dirichlet Allocation Method for Selectional Preferences computation of preferthe admissible argument values for a relation, is a well-known NLP task with applicability. We present which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences. By simultaneously inferring latent topics and topic distributions over relations, the benefits of previous approaches: like traditional classbased approaches, it produces humaninterpretable classes describing each relation’s preferences, but it is competitive with non-class-based methods in predictive power. compare several state-ofthe-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007). We also evaleffectiveness at filtering improper applications of inference rules, where we show substantial improvement Pantel system (Pantel et al., To support inference, (Ritter et al, 2010) learn the selectional restrictions of semantic relations, and (Pennacchiotti and Pantel, 2006) ontologize the learned arguments using WordNet. Alternatively, unsupervised approaches (Ritter et al., 2010) can be used to learn clusters of similar words, but the resulting types (=cluster numbers) are not human-interpretable, which makes analysis difficult. Dinu and Lapata (2010) used Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to model templates' latent senses, determining rule applicability based on the similarity between the two sides of the rule when instantiated by the context, while Ritter et al (2010) used LDA to model argument classes, considering a rule valid for a given argument instantiation if its instantiated templates are drawn from the same hidden topic. In other research on selectional preferences, Pantel et al (2007), Kozareva and Hovy (2010) and Ritter et al (2010) focus on generating admissible arguments for relations, and Erk (2007) and Bergsma et al (2008) investigate classifying a relation-instance pair as plausible or not. Important to this paper is the Wikipedia category network (Remy, 2002) and work on refining it. Se?aghdha (2010) and Ritter et al (2010) and the lexical substitution models of Dinu and Lapata (2010). We believe that a more data-driven type generalization that uses distributional similarity (e.g., (Ritter et al 2010)) may help much more. Recently, topic modeling methods have found widespread applications in NLP for various tasks such as summarization (Daume III and Marcu, 2006), inferring concept-attribute attachments (Reisinger and Pasca, 2009), selectional preferences (Ritter et al, 2010) and cross-documeint co-reference resolution (Haghighi and Klein, 2010). Szpektor et al. (2008) introduce a model of contextual preferences, generalizing the notion of selectional preference (cf. Ritter et al, 2010) to arbitrary terms, allowing for context-sensitive inference. Ritter et al (2010) and O Se?aghdha (2010), e.g., model selectional restrictions of verb arguments by inducing topic distributions that characterize mixtures of topics observed in verb argument positions. O Se?aghdha (2010) applies topic models for the SP induction with three variations: LDA, RoothLDA, and Dual-LDA; Ritter et al (2010) focus on inferring latent topics and their distributions over multiple arguments and relations (e.g., the subject and direct object of a verb). This metric is firstly employed for the SP evaluation by Ritteret al (2010). It shows LDA SP performs good correlation with human ratings, where LDASP+Bayes refers to the Bayes prediction method of Ritter et al (2010). Specifically, Ritter et al (2010) utilized the dot product form for their similarity measure: (4) simDC (d, d?, w)=? t [p (t|d, w)? p (t|d?, w)] (the subscript DC stands for double-conditioning, as both distributions are conditioned on the argument word, unlike the measure below). Ritter et al (2010) investigated joint selectional preferences.
Fast Cheap and Creative: Evaluating Translation Quality Using Amazon&rsquo;s Mechanical Turk Manual evaluation of translation quality is generally thought to be excessively time consuming and expensive. We explore a fast and inexpensive way of doing it using Amazon’s Mechanical Turk to pay small sums to a large number of non-expert annotators. For $10 we redundantly recreate judgments from a WMT08 translation task. We find that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality, and correlate more strongly with expert judgments than Bleu does. We go on to show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), to conduct reading comprehension experiments with machine translation, and to create high quality reference translations. It is possible that the length of stay of an annotator in the pool is not independent of her diligence; for example, Callison-Burch (2009) found in his AMT experiments with tasks related to machine translation that lazy annotators tended to stay longer and do more annotations. We will perform a semi-automatic validation of BabelNet, e.g. by exploiting Amazon's Mechanical Turk (Callison-Burch, 2009) or designing a collaborative game (von Ahn, 2006) to validate low-ranking mappings and translations. For example, Callison-Burch (2009) used MTurk to evaluate machine translations. (Callison-Burch, 2009) uses MTurk workers for manual evaluation of automatic translation quality and experiments with weighed voting to combine multiple annotations. The use of crowd sourcing to evaluate machine translation and to build development sets was pioneered by Callison-Burch (2009) and Zaidan and Callison-Burch (2009). Following Callison-Burch (2009), we treat evaluation as a weighted voting problem where each annotator's contribution is weighted by agreement with either a gold standard or with other annotators. It has also been used in MT evaluation (Callison-Burch, 2009), though that evaluation used reference translations. As an example, among the collected material several translations in languages other than English revealed a massive and defective use of on-line translation tools by untrusted workers, as also observed by (Callison-Burch, 2009). We do not select German, French and other language pairs as they have already been explored by Callison-Burch (2009). Furthermore, previous research shows the effectiveness of crowd sourcing as a method of accomplishing labor intensive natural language processing tasks (Callison-Burch, 2009) and the effectiveness of using MTurk for a variety of natural language automation tasks (Snow, Jurafsy, & O'Connor, 2008). The value of this upper bound is quite consistent with the bound computed similarly by Callison-Burch (2009).   Callison-Burch (2009) showed similar results for machine translation evaluation, and further showed that Turkers could accomplish complex tasks like translating Urdu or creating reading comprehension tests.  Recently, AMT has been shown to be an effective tool for annotation and evalatuation in NLP tasks ranging from word similarity detection and emotion detection (Snow et al, 2008) to Machine Translation quality evaluation (Callison-Burch, 2009). Callison-Burch (2009) proposed several ways to evaluate MT output on MTurk. Over the last several of years, Mechanical Turk, introduced by Amazon as "artificial artificial intelligence", has been used successfully for a number of NLP tasks, including robust evaluation of machine translation systems by reading comprehension (Callison-Burch, 2009), and other tasks explored in the recent NAACL workshop (Callison-Burch and Dredze, 2010b). There have been several research papers on using MTurk to help natural language processing tasks, Callison-Burch (2009) used MTurk to evaluate machine translation results. On another way, an application can combine active learning (Arora et al, 2009) and crowd sourcing, asking non-expertise such as workers of Amazon Mechanical Turk to label crucial alignment links that can improve the system with low cost, which is now a promising methodology in NLP areas (Callison-Burch, 2009).
Improved Inference for Unlexicalized Parsing We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we present a novel coarse-to-fine method in which a grammar’s own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank. In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy. Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning. Similar methods were applied by Matsuzaki et al (2005) and Petrov and Klein (2007) for parsing under a PCFG with nonterminals with latent annotations. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al, 2005). For preprocessing, all the sentences in the Bioscope corpus are tokenized and then parsed using the Berkeley parser (Petrov and Klein, 2007) trained on the GENIA TreeBank (GTB) 1.0 (Tateisi et al, 2005), which is a bracketed corpus in (almost) PTB style. On standard evaluations using both the Penn Tree bank and the Penn Chinese Treebank, our parser gave higher accuracies than the Berkeley parser (Petrov and Klein, 2007), a state-of-the-art chart parser. We use Berkeley PCFG parser (Petrov and Klein, 2007) for all experiments. Note that Algorithm 1 & 2 rely on the use of Berkeley parser (Petrov and Klein, 2007). The Berkeley and BUBS parsers both parse with the Berkeley latent-variable grammar (Petrov and Klein, 2007b), while the Charniak parser uses a lexicalized grammar, and the exhaustive CKY algorithm is run with a simple Markov order-2 grammar. These are precisely the kinds of distinctions between determiners that state-splitting in the Berkeley parser has shown to be useful (Petrov and Klein, 2007), and existing work (Mikolov et al., 2013b) has observed that such regular embedding structure extends to many other parts of speech. For the experiments in this paper, we will use the Berkeley parser (Petrov and Klein, 2007) and the related Maryland parser (Huang and Harper,2011). In this paper, we used the Berkeley Parser (Petrov and Klein, 2007) for learning these structures. We used the Berkeley parser (Petrov and Klein, 2007) to train the parsing model for HFE and to parse HFE. The split-merge smooth implementation of (Petrov et al, 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al, 2009) and for many other languages (Petrov and Klein, 2007). We used the Berkeley Parser (Petrov and Klein, 2007) with the standard model they provide for building syntactic parse trees and defined the patterns for extracting various syntactic features from the trees using the Tregex pattern matcher (Levy and Andrew, 2006). For this work, we have re-implemented and enhanced the Berkeley parser (Petrov and Klein 2007) in several ways: (1) developed a new method to handle rare words in English and Chinese; (2) developed a new model of unknown Chinese words based on characters in the word; (3) increased robustness by adding adaptive modification of pruning thresholds and smoothing of word emission probabilities. To obtain parse trees over both sides of each parallel corpus, we used the English and Chinese grammars of the Berkeley parser (Petrov and Klein, 2007).  Chinese and Arabic parses were generated using the Berkeley parser (Petrov and Klein, 2007). An approach found to be effective by Petrov and Klein (2007) for coarse-to-fine parsing is to use likelihood-based hierarchical EM training. This process not only guarantees that the clusters are hierarchical, it also avoids the state drift discussed by Petrov and Klein (2007). A similar observation was reported in the parsing literature, where coarse-to-fine inference with multiple passes of roughly equal complexity produces tremendous speed-ups (Petrov and Klein, 2007).
A Syntax-Based Statistical Translation Model We present a syntax-based statistical translation model. Our model transforms a source-language parse tree into a target-language string by applying stochastic operations at each node. These operations capture linguistic differences such as word order and case marking. Model parameters are estimated in polynomial time using an EM algorithm. The model produces word alignments that are better than those Very recently, Yamada and Knight (2001) described a model in which the noisy-channel takes as input a parsed sentence rather than simple words. Zhu et al. (2010) constructed a parallel corpus (PWKP) of 108,016/114,924 complex/simple sentences by aligning sentences from EWKP and SWKP and used the resulting bitext to train a simplification model inspired by syntax-based machine translation (Yamada and Knight, 2001). We refer the reader to (Yamada and Knight, 2001) for more details. Both Yamada and Knight (2001) and Chiang (2005) use SCFGs as the underlying model, so their translation schemata are syntax-directed as in Fig. This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common construction like this which is five levels deep (from VP to by). In applications such as the syntax based machine translation model of (Yamada and Knight, 2001), a low quality tree might lead to errorenous translation of the sentence. Additionally, we present novel on-the-fly variants of these algorithms, and compare their performance on a syntax machine translation cascade based on (Yamada and Knight, 2001). We adapt the Japanese-to-English translation model of Yamada and Knight (2001) by transforming it from an English-tree-to-Japanese-string model to an English-tree-to-Japanese-tree model. Yamada and Knight (2001) use a parser in the target language to train probabilities on a set of 609 operations that transform a target parse tree into a source string. Therefore, we have introduced a variation of the Inside-Outside algorithm as seen in (Yamada and Knight, 2001) for E step computation. Yamada and Knight (2001) further extended the model to a syntax-to-string translation modeling.  Yamada and Knight (2001) present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures. We begin by summarizing the model of Yamada and Knight (2001), which can be thought of as representing translation as an Alexander Calder mobile. In part to deal with this problem, Yamada and Knight (2001) flatten the trees in a pre-processing step by collapsing nodes with the same lexical head-word. Based on an example from (Yamada and Knight, 2001), we provide a sample SCFG fragment translating from English to Japanese, specified by means of the following synchronous productions. Variant of this definition can be found where the input is a single parse tree for w (Yamada and Knight, 2001), or where the output is a single parse tree, chosen according to some specific criteria (Wu and Wong, 1998). Syntax-based Statistical Translation (Yamada and Knight, 2001): This model extends the above by allowing all possible permutations of the RHS of the English rules. Finally, there are three resulting parameter tables analogous to the r-table; as stated in (Yamada and Knight, 2001), consisting of POS and constituent based patterns allowing for reordering and monotone distortion (examples can be found in Table 5). As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or pre ordering (Xia and McCord, 2004).
Two-Level Many-Paths Generation Large-scale natural language generation requires the integration of vast amounts of knowledge: lexical, grammatical, and conceptual. A robust generator must be able to well pieces of knowledge are missing. It must also be robust against incomplete or inaccurate inputs. To attack these problems, we have built a hybrid generator, in which gaps in symbolic knowledge are filled by statistical methods. We describe algorithms and show experimental results. We also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability, even when perfect knowledge is in principle obtainable. Some NLG techniques use sampling strategies (Knight and Hatzivassiloglou, 1995) where a set of sentences is sampled from a data structure created from an underlying grammar and ranked according to how well they meet the communicative goal. Over the years, several proposals of generic NLG system shave been made: Penman (Matthiessen and Bate man, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al, 2002), etc. The extraction algorithm is presented in (Knight and Hatzivassiloglou, 1995). Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimotoetal., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. (Belz, 2005) To incorporate the statistical component, which allows for robust generalization, per (Knight and Hatzivassiloglou, 1995), the NLG on the target side is filtered through a language model (described above). And Knight and Hatzivassiloglou (1995) use a language model for selecting a fluent sentence among the vast number of surface realizations corresponding to a single semantic representation. A language model is then used to select the most probable realization (Knight and Hatzivassiloglou, 1995). Some current methods have a generate and filter approach (Knight and Hatzivassiloglou, 1995): all constructions are generated and then filtered based on a statistical model.
SemEval-2007 Task-17: English Lexical Sample SRL and All Words This paper describes our experience in preparing the data and evaluating the results for three subtasks of SemEval-2007 Task-17 ? Lexical Sample, Semantic Role Labeling(SRL) and All-Words respectively. We tab ulate and analyze the results of participating systems. We call this setting 'SemEval' because the SemEval-2007 competition (Pradhan et al, 2007) was performed using this configuration. Various aspects of the model discussed in Section 3 are evaluated in the English lexical sample tasks from Senseval2 (Edmonds and Cotton, 2001) and SemEval2007 (Pradhan et al, 2007). Various aspects of the model discussed in Section 3 are evaluated in the English lexical sample tasks from Senseval2 (Edmonds and Cotton, 2001) and SemEval2007 (Pradhan et al, 2007). Given that the WSD literature shows that all features are necessary for optimal performance (Pradhan et al, 2007), we propose the following alternative to construct the matrix. We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al, 2007) English All Words data sets. English Lexical Sample The Semeval workshop holds WSD tasks such as the English Lexical Sample (ELS) (Pradhan et al, 2007). It outperforms most of the systems participating in the task (Pradhan et al., 2007). Experimental results are provided for two datasets: the Semeval-2007 lexical sample task (Pradhanetal., 2007) and the Turk bootstrap Word Sense Inventory (TWSI1, (Biemann and Nygaard, 2010)).  In this paper the relevance feedback approach described by Stevenson et al (2008a) is evaluated using three data sets: the NLM-WSD corpus (Weeber et al, 2001) which Stevenson et al (2008a) used for their experiments, the Senseval-3 lexical sample task (Mihalcea et al, 2004) and the coarse grained version of the SemEval English lexical sample task (Pradhan et al, 2007). We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al, 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. Lexical Sample The Semeval workshop holds WSD tasks such as the English Lexical Sample (ELS) (Pradhan et al, 2007). The improvements seen using our system are substantial, beating most of the systems originally proposed for the task (Pradhan et al, 2007). The use of coarse-grained sense groups (Palmer et al, 2007) has led to considerable advances in WSD performance, with accuracies of around 90% (Pradhan et al, 2007). Given that the WSD literature has shown that all features, including local and syntactic features, are necessary for optimal performance (Pradhan et al, 2007), we propose the following alternative to construct the matrix. The algorithm proved effective at Senseval-3 (Mihalcea and Edmonds, 2004) and, nowadays, it still represents the state-of-the-art in WSD (Pradhan et al, 2007). Specifically, they addressed these issues: (i) independently modeling domain and syntagmatic aspects of sense distinction to improve feature representativeness; and (ii) exploiting external knowledge acquired from unlabeled data, with the purpose of drastically reducing the amount of labeled 3http: //download.wikimedia.org/enwiki/ 20090306 4 A context corresponds to a line of text in the Wikipedia dump and it is represented as a paragraph in a Wikipedia article. For unsupervised WSD (applied to text only), we use WordNet: :SenseRelate: :TargetWord, here after PBP (Patwardhan et al, 2007), the highest scoring unsupervised lexical sample word sense disambiguation algorithm at SemEval07 (Pradhan et al., 2007).
Reading Level Assessment Using Support Vector Machines And Statistical Language Models Reading proficiency is a fundamental component of language competency. However, finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers. This task can be addressed with natural language processing technology to assess reading level. Existing measures of reading level are not well suited to this task, but previous work and our own pilot experiments have shown the benefit of using statistical language models. In this paper, we also use support vector machines to combine features from traditional reading level measures, statistical language models, and other language processing tools to produce a better method of assessing reading level. For first language (L1) learners (i.e., children learning their native tongue), reading level has been predicted using a variety of techniques, based on models of a student's lexicon, grammatical surface features such as sentence length (Flesch, 1948), or combinations of such features (Schwarm and Ostendorf, 2005). Prior work on first language readability by Schwarm and Ostendorf (2005) incorporated grammatical surface features such as parse tree depth and average number of verb phrases.   Schwarm and Ostendorf (2005) developed a SVM categoriser combining a classifier based on trigram language models (one for each level of difficulty), some parsing features such as average tree height, and variables traditionally used in readability. Support vector machines have already been shown to be useful for readability purposes (Schwarm and Ostendorf, 2005). A corpus of Weekly Reader articles was previously used in work by Schwarm and Ostendorf (2005). Schwarm and Ostendorf (2005) studied four parse tree features (average parse tree height, average number of SBARs, noun phrases, and verb phrases per sentences). For comparison, we replicated 6 out-of-vocabulary features described in Schwarm and Ostendorf (2005). We also replicated the 12 perplexity features implemented by Schwarm and Ostendorf (2005) (see Section 3.2). Table 8 compares a classifier trained on the four parse features of Schwarm and Ostendorf (2005) to a classifier trained on our expanded set of parse features. The most closely related previous study is the work of Schwarm and Ostendorf (2005). Also, relatedly, (Schwarm and Ostendorf, 2005) use a statistical language model to train SVM classifiers to classify text for grade levels 2-5. A measure by Schwarmand Ostendorf (2005) incorporates syntactic analyses, among a variety of other types of features.   Schwarm and Ostendorf (2005) implemented four parse tree features (average parse tree height, aver age number of SBARs, NPs per sentence and VPs per sentence) in their work. In order to verify the impact of our choice of features, we also did a replication of the parsed syntactic feature measures reported by (Schwarm and Ostendorf, 2005) on the WeeklyReader corpus and obtained essentially the same accuracy as the one published (50.7% vs. 50.91%), supporting the comparability of the WeeklyReader data used.  Syntactic complexity is an obvious factor: indeed (Heilman et al, 2007) and (Schwarm and Ostendorf, 2005) also used syntactic features, such as parse tree height or the number of passive sentences, to predict reading grade levels.
Planning Coherent Multisentential Text The relations are used as plans; their intended effects are interpreted as the goals they achieve. In other words, in order to bring about the state which both speaker and hearer know that the purpose of know that they both know it, etc. ), the structurer uses Purpose as a plan and tries to satisfy its constraints. In this system, constraints and goals are interfor example, in the event that believed not by the satellite constraint of the resimply becomes the goal to achieve (BMB (RESULT Similarly, the propo- Ow s SCAN-1 ?ACT-2)) (DMB (08.1 are interpreted as the goal to find some element that could legitimately take place of In order to enable the relations to nest recursome relations' nucleuses and satellites contain requirements that specify additional relations, such as examples, contrasts, etc. Of course, these additional requirements may only be included if such material can coherently follow the content of the nucleus or satellite. The question of ordering such additional constituents is still under investigation. The question of whether such additional material should be included at all is not addressed; the structurer tries to say everything it is given. The structurer produces all coherent paragraphs (that is, coherent as defined by the relations) that satisfy the given goal(s) for any set of input elements. For example, paragraph (b) is produced to the initial goal S H (SEQUENCE goal is produced by PEA, together with the appropriate representation ele- (ASK-I. in response to the does the system a program?. Different initial goals will result in different paragraphs. Each paragraph is represented as a tree in which branch points are RST relations and leaves are input elements. Figure 1 is the tree for para- (b). It contains the relations (signalled by 'then' and 'finally&quot;), Elaboration (&quot;in particular&quot;), and Purpose (&quot;in order to&quot;). In the corresponding paragraph produced by Penman, the relations' characteristic words or phrases (boldfaced below) appear between the blocks of text they relate: [The system asks the user to tell it the characteristic of the program to be system applies to the progran2. ](0 system scans the proo order to opportunities to apply transformations to the system resolves [It confirms the enhancewith the [it performs the enhancement.ho 166 input sentence generator --ot update agenda choose final plan get next bud RST relations expand bud grow tree Figure 2: Hierarchical Planning Structurer 6-The Structurer As stated above, the structurer is a simplified top-down hierarchical expansion planner (see Figure 2). It operates as follows: given one or more communicative goals, it finds RST relations whose intended effects match (some of) these goals; it then inspects which of the input elements match the nucleus and subgoal constraints for each relation. Unmatched constraints become subgoals which are posted on an agenda for the next level of planning. The tree can be expanded in either depth-first or breadth-first fashion. Eventually, process bottoms out when either: (a) all input elements have been used and unsatisfied subgoals remain (in which case the structurer could request more input with desired properties from the encapsulating system); or (b) all goals are satisfied. If more than one plan (i.e., paragraph tree structure) is produced, the results are ordered by preferring trees with the minimum unused number of input elements and the minimum number of remaining unsatisfied subgoals. The best tree is then traversed in left-to-right order; leaves provide input to Penman to be generated in English and relations at branch points provide typical interclausal relation words or phrases. In this way the structurer performs top-down goal refinement down to the level of the input elements. and Further Work This work is also being tested in a completely separate domain: the generation of text in a multimedia system that answers database queries. Penman produces the following description of the ship Knox (where CTG 070.10 designates a group of ships): Knox is en route in rendezvous with CTG 070.10, arriving in Pearl Harbor on 4/24, for port visit until 4/30. In this text, each clause (en route, rendezvous, arrive, visit) is a separate input element; the structurer linked them using the relations Sequence and Purpose (the same Purpose as shown above; it is signalled by &quot;in order to&quot;). However, Penman can also be made to produce (d). Knox is en route in order to rendezvous with CTG 070.10. It will arrive in Pearl Harbor on 4/24. It will be on port visit until 4/30. The problem is clear: how should sentences in the paragraph be scoped? At present, avoiding any claims about a theory, the structurer can feed 167 Penman either extreme: make everything one sentence, or make each input element a separate sentence. However, neither extreme is satisfactory; as is clear from paragraph (b), &quot;short&quot; spans of text can be linked and &quot;long&quot; ones left separate. A simple way to implement this is to count the number of leaves under each branch (nucleus or satellite) in the paragraph structure tree. Another shortcoming is the treatment of input elements as indivisible entities. This shortcoming is a result of factoring out the problem of aggregation as a separate text planning task. Chunking together input elements (to eliminate detail) or taking them apart (to be more detailed) has received scant mention — see Ellovy 871, and for the related problem of paraphrase see [Schank 75] — but this task should interact with text structuring in order to provide text that is both optimally detailed and coherent. At the present time, only about 20% of the RST relations have been formalized to the extent that they can be used by the structurer. This formalization process is difficult, because it goes handin-hand with the development of terms with which to characterize the relations' goals/constraints. Though the formalization can never be completely finalized — who can hope to represent something like motivation or justification complete with all ramifications? — the hope is that, by having the requirements stated in rather basic terms, the relations will be easily adaptable to any new representation scheme and domain. (It should be noted, of course, that, to be useful, these formalizations need only be as specific and as detailed as the domain model and representation requires.) In addition, the availability of a set of communicative goals more detailed than just say or ask (for example), should make it easier for programs that require output text to interface with the generator. This is one focus of current text planning work at ISI.   Some text planners take as their goal the delivery of a single fact, or a set of facts, but take into account that other information may need to be given first for each message to be understood, other information may need to be given after to counter misconceptions, and examples may be given to aide assimilation (for in stance, the RST text planners, such as Hovy 1988). This approach is adopted from text generation where plan-operators are responsible for choosing linguistic means in order to create coherent stretches of text (see, for instance, (Moore and Paris, 1989) and (Hovy, 1988)). As for any generated text, a good summary also requires a text plan (Hovy, 1988) (McKeown, 1985).
Unsupervised Coreference Resolution in a Nonparametric Bayesian Model We present an unsupervised, nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document. While most existing coreference work is driven by pairwise decisions, our model is fully generative, producing each mention from a combination of global entity properties and local attentional state. Despite being unsupervised, our system achieves a 70.3 on the MUC-6 test set, broadly in the range of some recent supervised results. More recently, Haghighi and Klein (2007) use the distinction between pronouns, nominals and proper nouns in their unsupervised, generative model for coreference resolution; for their model, this is absolutely critical for achieving better accuracy.  Other recent unsupervised graphical model approaches using Gibbs sampling (Haghighi and Klein, 2007) may be able to incorporate partially annotated documents in semi-supervised training. Recent work has applied Bayesian non-parametric models to anaphora resolution (Haghighi and Klein, 2007), lexical acquisition (Goldwater, 2007) and language modeling (Teh, 2006) with good results. In terms of applying non-parametric Bayesian approaches to NLP, Haghighi and Klein (2007) evaluated the clustering properties of DPMMs by performing anaphora resolution with good results. In addition, their system does not classify non-anaphoric pronouns, A third paper that has significantly influenced our work is that of (Haghighi and Klein, 2007). The probabilities are ordered according to, at least my, intuition with pronoun being the most likely (0.094), followed by proper nouns (0.057), followed by common nouns (0.032), a fact also noted by (Haghighi and Klein,2007). Since Soon (Soon et al, 2001) started the trend of using the machine learning approach by using a binary classifier in a pairwise manner for solving co-reference resolution problem, many machine learning-based systems have been built, using both supervised and, unsupervised learning methods (Haghighi and Klein, 2007). Since we can not afford to manually annotate our entire data set with coreference information, we follow Haghighi and Klein (2007)'s work on unsupervised co reference resolution, and develop a series of generative Bayesian models for our task.  In this work, we take a primarily unsupervised approach to co reference resolution, broadly similar to Haghighi and Klein (2007), which addresses this issue. As Blei and Frazier (2009) notes, when marginalizing out the Ai in this trivial case, the DD-CRP reduces to the traditional CRP (Pitman, 2002), so our discourse model roughly matches Haghighi and Klein (2007) for proper mentions. Haghighi and Klein (2007), proposed an unsupervised nonparametric Bayesian model for coreference resolution. Poon and Domingos (2008) outperformed Haghighi and Klein (2007). The model of Haghighi and Klein (2007) incorporated a latent variable for named entity class. Like Haghighi and Klein (2007), we give our model information about the basic types of pronouns in English. As stated above, we aim to build an unsupervised generative model for named entity clustering, since such a model could be integrated with unsupervised coreference models like Haghighi and Klein (2007) for joint inference. Our system improves over the latent named-entity tagging in Haghighiand Klein (2007), from 61% to 87%. Turning to unsupervised CoRe, Haghighi and Klein (2007) proposed a generative Bayesian model with good performance. P&D (Poon and Domingos, 2008) on ACE-2; and to Ng (Ng, 2008) and H&K5 (Haghighi and Klein, 2007) on ACE2003.
Parsing Algorithms And Metrics Many different metrics exist for evaluating parsing results, including Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several others. However, most parsing algorithms, including the Viterbi algorithm, attempt to optimize the same metric, namely the probability of getting the correct labelled tree. By choosing a parsing algorithm appropriate for the evaluation metric, better performance can be achieved. We present two new algorithms: the &quot;Labelled Recall Algorithm,&quot; which maximizes the expected Labelled Recall Rate, and the &quot;Bracketed Recall Algorithm,&quot; which maximizes the Bracketed Recall Rate. Experimental results are given, showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria, especially the ones that they optimize Two measures are used to evaluate the parses: lexical accuracy, which is the percentage of correctly tagged words compared to the extracted gold standard corpus (Watkinson and Manandhar, 2001) and average crossing bracket rate (CBR) (Goodman, 1996). The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). The orthogonal technique of minimum Bayes risk decoding has achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004). A probability model permits alternative decoding procedures (Goodman, 1996). These expectations can be easily computed from the inside/outside scores, similarly as in the maximum bracket recall algorithm of Goodman (1996), or in the variational approximation of Matsuzaki et al (2005). Their algorithm is therefore the labelled recall algorithm of Goodman (1996) but applied to rules. Since this method is not a contribution of this paper, we refer the reader to the fuller presentations in Goodman (1996) and Matsuzaki et al (2005). The coarse PCFG has an extremely beneficial interaction with the fine all-fragments SDP grammar, wherein the accuracy of the combined grammars is significantly higher than either individually (This is similar to the maximum recall objective for approximate inference (Goodman, 1996b)). Goodman (1996) observed that the Viterbi parse is in general not the optimal parse for evaluation metrics such as f-score that are based on the number of correct constituents in a parse. The performance of web-page structuring algorithms can be evaluated via the nested-list form of tree by bracketed recall and bracketed precision (Goodman, 1996). This algorithm is a natural synchronous generalization of the monolingual Maximum Constituents Parse algorithm of Goodman (1996). We then compute outside scores for bi spans under a max-sum (Goodman, 1996).   Goodman (1996b), Petrov and Klein (2007), and Matsuzaki et al (2005) describe the details of constituent, rule-sum and variational objectives respectively. In the field of natural language processing this approach has been applied for example in parsing (Goodman, 1996) and word alignment (Kumar and Byrne, 2002). And finally, we show that the parsing algorithm described in Clark and Curran (2003) is extremely slow in some cases, and suggest an efficient alternative based on Goodman (1996). This is equivalent to minimum Bayes risk decoding (Goodman, 1996), which is used by Cohen and Smith (2007) and Smith and Eisner (2008). A closely related method, applied by Goodman (1996) is called minimum-risk decoding. While the most probable parse problem is NP-complete (Simaan, 1992), several approximate methods exist, including n-best re-ranking by parse likelihood, the labeled bracket alorithm of Goodman (1996), and a variational approximation introduced in Matsuzakiet al (2005).
Local and Global Algorithms for Disambiguation to Wikipedia Disambiguating concepts and entities in a context sensitive way is a fundamental problem in natural language processing. The comprehensiveness of Wikipedia has made the online encyclopedia an increasingly popular target for disambiguation. Disambiguation to Wikipedia is similar to a traditional Word Sense Disambiguation task, but distinct in that the Wikipedia link structure provides additional information about which disambiguations are compatible. In this work we analyze approaches that utilize this information to arrive at coherent sets of disambiguations for a given document (which we call “global” approaches), and compare them to more traditional (local) approaches. We show that previous approaches for global disambiguation can be improved, but even then the local disambiguation provides a baseline which is very hard to beat. Such mention type information as shown on the left of Figure 1 can be obtained from various sources such as dictionaries, gazetteers, rule-based systems (Stro?tgen and Gertz, 2010), statistically trained classifiers (Ratinov and Roth, 2009), or some web resources such as Wikipedia (Ratinov et al, 2011). We tagged each sentence with Wikipedia terms using the Illinois Wikifier (Ratinov et al, 2011) and with UMLS (Bodenreider, 2004) terms using Health Term Finder (LipskyGorman and Elhadad, 2011). The need to identify named entities such as persons, locations, organizations and places, arises both in applications where the entities are first class objects of interest, such as in Wikification of documents (Ratinov et al, 2011), and in applications where knowledge of named entities is helpful in boosting performance, e.g., machine translation (Babych and Hartley, 2003) and question answering (Leidner et al, 2003). Semantic word vector spaces are at the core of many useful natural language applications such as search query expansions (Jones et al 2006), fact extraction for information retrieval (Pas?ca et al 2006) and automatic annotation of text with disambiguated Wikipedia links (Ratinov et al 2011), among many others (Turney and Pantel, 2010). In fact, (Ratinov et al, 2011) show that even though global approaches can be improved, local methods based on only similarity sim (d, e) of context d and entity e are hard to beat. Additionally, while (Rahman and Ng, 2011) uses the union of all possible meanings a mention may have in Wikipedia, we deploy GLOW (Ratinov et al., 2011), a context-sensitive system for disambiguation to Wikipedia. We uses a mixture of local and global features to train the coefficients of a linear ranking SVM to rank different NE candidates.
Bootstrapping disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the for Computational pages 189–196. see that view independence does not imply preindepence, consider an example in which always. This is compatible with rule independence, but implies that = 1 and = 0, violating precision independence. A A bootstrapping approach using machine learning is a possible alternative that will be explored in the future (Abney 2002). Abney (2002) suggests that the disagreement rate of two independent hypotheses upper-bounds the error rate of either hypothesis. In recent work, (Abney, 2002) shows that the independence assumption can be relaxed, and co-training is still effective under a weaker independence assumption. However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations. Dasgupta et al (2001) and Abney (2002) conducted theoretical analyses on the performance (generalization error) of co-training. Abney (2002) refined Dasgupta et als result by relaxing the view independence assumption with a new constraint. Rather than comparing the two learners on whether they categorically select the same preferred parse on a number of examples, we can view active learning as the inverse of agreement-based co-training (Abney, 2002). Further avenues to explore include the development of selection methods to efficiently approximate maximizing the objective function of parser agreement on unlabeled data, following the work of Dasgupta et al (2002) and Abney (2002). We have implemented the Greedy Agreement Algorithm (Abney, 2002) which, based on two independent views of the data, is able to learn two binary classifiers from a set of hand-typed seed rules. See (Abney, 2002) for a formal proof that this algorithm tends to gradually reduce the classification error given the adequate seed rules. In fact, results are reported to be competitive against more sophisticated methods (Co-DL, Co Boost, etc.) for this specific task in (Abney, 2002). Third, how the algorithm, presented in (Abney, 2002) for binary classification, can be extended to a multi class problem. These results are comparable to the ones presented in (Abney, 2002), taking into account, apart from the language change, that we have introduced a fourth class to be treated the same as the other three. Theorem 5 in (Abney, 2002) provides a theoretical explanation for these results: if certain independence conditions between the classifier rules are satisfied and the precision of each rule is larger than a threshold T, then the precision of the final classifier is larger than T. Abney (2002) presents an analysis to relax the (fairly strong) conditional independence assumption to weak rule dependence. Although this result was previously observed in a different context by Abney in (Abney, 2002), he does not use it to derive a semi-supervised learning algorithm. Semantic Classifiers Bootstrapping refers to a problem of inducing a classifier given a small set of labeled data and a large set of unlabeled data (Abney, 2002). We used bootstrapping (Abney, 2002) which refers to a problem setting in which one is given a small set of labeled data and a large set of unlabeled data, and the task is to induce a classifier. Abney (2002) argues that the conditional independence assumption is remarkably strong and is rarely satisfied in real data sets, showing that a weaker independence assumption suffices. Co-training algorithms such as CoBoost (Collins and Singer, 1999) and Greedy Agreement (Abney, 2002) that explicitly trade classifier agreement on unlabeled data against error on labeled data may be more robust to the underlying assumptions of co-training and can conceivably perform better than the Blum and Mitchell algorithm for problems without a natural feature split.
Exploring Various Knowledge In Relation Extraction Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.  Zhou et al (2005) reported the best result as 63.1% / 49.5% / 55.5% in Precision / Recall / F-measure on the extraction of ACE relation subtypes using feature based method, which outperforms tree kernel based method by Culotta and Soresen (2004). However, detailed research (Zhou et al, 2005) shows that it is difficult to extract new effective features to further improve the extraction accuracy. For those interested in feature-based methods, please refer to Zhou et al (2005) for more details.  Based on his work, Zhou et al (2005) further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.   Zhou et al (2005) argued that most information useful for IE derived from full parsing was shallow. This approach is also applicable to IE in other domains, where related entities are in a short distance like the work of Zhou et al (2005). This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).  However, it is difficult for them to effectively capture structured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction. As the state-of-the-art, Zhang et al (2006) applied the convolution tree kernel (Collins and Duffy 2001) and achieved comparable performance with a state-of-the art linear kernel (Zhou et al　2005) on the 5 relation types in the ACE RDC 2003 corpus. Zhou et al (2005) further systematically explored diverse features through a linear kernel and Support Vector Machines, and achieved the F measures of 68.0 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively. Another problem is that, although they can explore some structured information in the parse tree (e.g. Kambhatla (2004) used the non-terminal path connecting the given two entities in a parse tree while Zhou et al (2005) introduced additional chunking features to enhance the performance), it is found difficult to well preserve structured information in the parse trees using the feature-based methods. Here, we use the same set of flat features (i.e. word, entity type, mention level, overlap, base phrase chunking, dependency tree, parse tree and semantic information) as Zhou et al (2005).   For the choice of features, we use the full set of features from Zhou et al (2005) since it is reported to have a state-of-the-art performance (Sun et al 2011).
Inside-Outside Reestimation From Partially Bracketed Corpora K. Lan i and S. J. Young. 1990. The estimation of stochastic context-free grammars using the Insidealgorithm. Speech and Lan- K. Lan i and S. J. Young. 1991. Applications of stochastic context-free grammars using the Insidealgorithm. Speech and Lan- David Magerman and Mitchell Marcus. 1990. Parsing a natural language using mutual informastatistics. In MA. Yves Schabes. 1992. Stochastic lexicalized treegrammars. In 92. Forthcoming. The latter property is important as we would like to be able to use bracketing information in the input corpus as in (Pereira and Schabes, 1992). We use the data given by Pereira and Schabes (1992) on raw text and compare with an inferred SLTAG. As shown in Figure 2, the grannnar converges very rapidly to a lower value of the log probability than the stochastic ontext-free grammar eported by Pereira and Schabes (1992). Similarly to its context-free counterpart, the re-estimation algorithm can be extended to handle partially parsed corpora (Pereira and Schabes, 1992). This can be viewed as a variation of Pereira and Schabes (1992). The algorithm is a special variant of the inside-outside algorithm of Pereira and Schabes (1992). The derivation of the estimation algorithm is largely omitted; see Pereira and Schabes (1992) for details. These improvements could make DBMs quick-and easy to bootstrap directly from any available partial bracketings (Pereira and Schabes, 1992), for example capitalized phrases (Spitkovsky et al2012b). However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB. These some what negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models. Following Pereira and Schabes (1992) given t=(s, U), a node's span in the parse forest is valid if it does not overlap with any span outlined in U, and hence, a derivation is correct if the span of every node in the derivation is valid in U. Experi mental results con rming this wisdom have been presented, e.g., by Elworthy (1994) and Pereira and Schabes (1992) for EM training of Hidden Markov Models and PCFGs. Hwa (1999) used a variant of the inside-outside algorithm presented in Pereira and Schabes (1992) to exploit a partially labeled out-of-domain tree bank, and found an advantage to adaptation over direct grammar induction. For this we use an approach similar to Pereira and Schabes' grammar induction from partially bracketed text (Pereira and Schabes, 1992). The role of supervision is to permit some constituents to be built but not others (Pereira and Schabes, 1992). Specifically, we use the Inside-Outside algorithm defined in (Pereira and Schabes, 1992) to learn transformed dependency grammars annotated with hidden symbols. CFGs extracted from such structures were then annotated with hidden variables encoding the constraints described in the previous section and trained until convergence by means of the Inside-Outside algorithm defined in (Pereira and Schabes, 1992) and applied in (Matsuzaki et al, 2005). The summation over target word sequences and alignments given fixed? t bears a resemblance to the inside algorithm, except that the tree structure is fixed (Pereira and Schabes, 1992). Bracketed Corpora Training introduced by Pereira and Schabes (1992) employs a context free grammar and a training corpus, which is partially tagged with brackets. The language processing community actively works on the problem of automatically inducing grammatical structure from a corpus of text (Pereira and Schabes, 1992).
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 34?35, Vancouver, October 2005.  We use OpinionFinder (Wilson et al, 2005) which employs negative and positive polarity cues.  Sentiment lexicons such as SentiWordNet (Baccianella et al (2010)) and OpinionFinder (Wilson et al (2005a)) show low agreement rate with human, which is somewhat as expected: human judges in this study are labeling for subtle connotation, not for more explicit sentiment. Opinionfinder (Wilson et al, 2005) is a system for mining opinions from text. The list of polarity words that we use in this component has been taken from the OpinionFinder system (Wilson et al, 2005).  For CRF-style features, we consider the string representation of the current word, its part-of speech, and a dictionary-derived feature, which is based on a subjectivity lexicon provided by Wilson et al (2005).  We use Opinion Finder (Wilson et al 2005a) to identify words with positive or negative semantic orientation.  Opinionfinder (Wilson et al, 2005a) is a system for mining opinions from text. This component uses the publicly available tool, opinion finder (Wilson et al., 2005a), as a framework for polarity identification.   For this reason, we employ OpinionFinder (Wilson et al 2005a), which has a pre-trained classifier for annotating the phrases in a sentence with their contextual polarity values. We use OpinionFinder (Wilson et al, 2005a) to identify polarized words and their polarities.  We begin with a seed set combining PARADIGM+ (Jo and Oh, 2011) with 'strongly subjective' adjectives from the OpinionFinder lexicon (Wilson et al, 2005), yielding 1342 seeds.
ORANGE: A Method For Evaluating Automatic Evaluation Metrics For Machine Translation Comparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as Pearson?s product moment correlation coefficient or Spearman?s rank order correlation coefficient between human scores and automatic scores. However, such comparisons rely on human judgments of translation qualities such as adequacy and fluency. Unfortunately, these judgments are often inconsistent and very expensive to acquire. In this paper, we introduce a new evaluation method, ORANGE, for evaluating automatic machine translation evaluation metrics automatically without extra human involvement other than using a set of reference translations. We also show the results of comparing several existing automatic metrics and three new automatic metrics using ORANGE. In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement. This locality assumption eases efficient implementation of our algorithm, and can be realized using a sentence-level evaluation measure such as BLEU+1 (Lin and Och, 2004). 156 at the sentence levels moothed-bleu (Lin and Och, 2004) was used in this case. ROUGE utilizes, skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). BLEU is smoothed (Lin and Och, 2004b), and it considers only matching up to bi grams because this has higher correlations with human judgments than when higher-ordered n-grams are included. Smoothed per-sentence BLEU (Lin and Och, 2004) was used as a similarity metric. Stemming is enabled (Lin and Och, 2004a). Metrics in the Rouge family allow for skip n-grams (Lin and Och,2004a); Kauchak and Barzilay (2006) take para phrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamedetal., 2003) calculate both recall and precision; ME TEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. BLEU is smoothed to be more appropriate for sentence level evaluation (Lin and Och, 2004b), and the bi gram versions of BLEU and HWCM are reported because they have higher correlations than when longer n-grams are included. We then used these coefficients to estimate the confidence interval, after excluding the top 25 and bottom 25 coefficients, following (Lin and Och, 2004). Unless otherwise stated, we will assume the use of sentence BLEU with add1 smoothing (Lin and Och, 2004). As F1 score is not decomposable, we optimize sentence-level F1 score which serves as an approximation of the corpus-level F1 score. Similarly, Hopkins and May optimize a sentence level BLEU approximation (Lin and Och, 2004) in stead of the corpus-level BLEU score (Papinenietal., 2002). The optimization objective is sentence level BLEU (Lin and Och, 2004). As we would like to avoid this problem, we use the smoothed sentence-level Bleuscoreas suggested in (Lin and Och, 2004). They obtained similar results in both cases (Lin and Och, 2004a). While the F measure over Precision and Recall satisfies these constraints, precision and recall in isolation do not satisfy all of them: maximum recall can be achieved without resembling the gold standard text decomposition; and maximum precision can be achieved with only a few overlapped elements.BLEU (Papineni et al, 2001a) computes the n gram precision while the metric ROUGE (LinandOch, 2004a) computes the n-gram recall. The introduction of smoothing (Lin and Och, 2004) solves this problem only partially. As the BLEU score is unsuitable for sentence level evaluation in its original definition, BLEU-S smoothing as described by (Lin and Och, 2004) is performed. Skip-bigrams (Lin and Och, 2004) are pairs of words in sentence order allowing for gaps in between. Using the notation by (Lin and Och, 2004), we denote the skip bigram overlap between two sentences X and Y as Skip2 (X, Y).
Reranking And Self-Training For Parser Adaptation Statistical parsers trained and tested on the Wall Street Journal treebank have shown vast improvements over the last 10 years. Much of this improvement, however, is based upon an ever-increasing number of features to be trained on (typithe data. This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres. Such worries have merit. The standard “Charniak parser” checks in at a labeled precisionof 89.7% on the Penn set, but only 82.9% on the test set from the Brown treebank corpus. This paper should allay these fears. In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%. Furthermore, use of the self-training techniques described in (Mc- Closky et al., 2006) raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data. This is remarkable since training the parser and reranker on labeled Brown data achieves only 88.4%.       In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). (McClosky et al, 2006) presents a successful instance of parsing with self-training by using a re-ranker.  Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and self training and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al, 2006a; McClosky et al, 2006b). The syntactic parser is the version that is self trained using 2,500,000 sentences from NANC, and where the starting version is trained only on WSJ data (McClosky et al, 2006b).    McClosky et al (2006b) used self-training and corpus weighting to adapt their parser trained on WSJ corpus to Browncorpus. Recently, McClosky et al (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al, 2006b). Note that our approach is different from the self-training technique proposed in (McClosky et al, 2006a), although both methods belong to semi-supervised training category. Note that the results of our model are not directly comparable with previous parsing results shown in (McClosky et al, 2006a), since the parsing accuracy is measured in terms of dependency relations while their results are f-score of the bracketings implied in the phrase structure. 
Unsupervised Semantic Parsing We present the first unsupervised approach to the problem of learning a semantic parser, using Markov logic. Our USP system transforms dependency trees into quasi-logical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning. The MAP semantic parse of a sentence is obtained by recursively assigning its parts to lambda-form clusters and composing them. We evaluate our approach by using it to extract a knowledge base from biomedical abstracts and answer questions. USP substantially outperforms TextRunner, DIRT and an informed baseline on both precision and recall on this task. Take the following example from Poon and Domingos (2009), in which the same semantic relation can be expressed by a transitive verb or an attributive prepositional phrase: (1) Utah borders Idaho. To do so, we semi-automatically restrict the question-answer pairs by using the output of an unsupervised clustering semantic parser (Poon and Domingos, 2009). We used the question-answer pairs extracted by the Poon and Domingos (2009) semantic parser from the GENIA biomedical corpus that have been manually checked to be correct (295 pairs). More recent examples of similar techniques include the Resolver system (Yates and Etzioni, 2009) and Poon and Domingos's USP system (Poon and Domingos, 2009). On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. Our non-parametric model automatically discovers granularity of clustering appropriate for the dataset, unlike the parametric method of (Poon and Domingos, 2009) which have to perform model selection and use heuristics to penalize more complex models of semantics. In our case, the state space size equals the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus (Poon and Domingos, 2009). In both cases, we follow (Poon and Domingos, 2009) in using the corpus of biomedical abstracts. Second, lambda calculus is a considerably more powerful formalism than the predicate-argument structure used in frame semantics, normally supporting quantification and logical connectors (for example, negation and disjunction), neither of which is modeled by our model or in (Poon and Domingos, 2009). Semantic classes correspond to lambda-form clusters in (Poon and Domingos, 2009) terminology. The work of (Poon and Domingos, 2009) models joint probability of the dependency tree and its latent semantic representation using Markov Logic Networks (MLNs) (Richardson and Domingos, 2006), selecting parameters (weights of first-order clauses) to maximize the probability of the observed dependency structures. In order to overcome this problem, (Poon and Domingos, 2009) group parameters and impose local normalization constraints within each group. We now turn to the QA task and compare our model (USP-BAYES) with the results of baselines considered in (Poon and Domingos, 2009). Other approaches are completely unsupervised, but do not tie the language to an existing meaning representation (Poon and Domingos, 2009). USP (Poon and Domingos, 2009) is based on Markov Logic Networks and attempts to create a full semantic parse in an unsupervised fashion. Markov Logic has been used previously in other NLP application (e.g. Poon and Domingos (2009)). Despite the existence of a large amount of related work in the literature, distinguishing synonyms and antonyms is still considered as a difficult open problem in general (Poon and Domingos, 2009). Dependency information is useful for a wealth of natural language processing tasks, including question answering (Wang et al, 2007), semantic parsing (Poon and Domingos, 2009), and machine translation (Galley and Manning, 2009). Such annotated resources are scarce and expensive to create, motivating the need for unsupervised or semi-supervised techniques (Poon and Domingos, 2009).
Making Tree Kernels Practical For Natural Language Learning In recent years tree kernels have been proposed for the automatic learning of natural language applications. Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods. In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods. Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis. The time needed for tree kernel function was not so problematic as we could use the fast evaluation proposed in (Moschitti, 2006).     SVMLight (Joachims,1999), in the SVMLight/TK (Moschitti, 2006) variant, allows to use tree-valued features.  The algorithm for the efficient evaluation of ? for the syntactic tree kernel (STK) has been widely discussed in (Collins and Duffy, 2002) whereas its fast evaluation is proposed in (Moschitti, 2006b), so we only describe the equations of the partial tree kernel (PTK). PTFs have been defined in (Moschitti, 2006a). Miwa et al (2009a) proposed a hybrid kernel, which combines the all-paths graph (APG) kernel (Airola et al 2008), the bag-of-words kernel, and the subset tree kernel (Moschitti, 2006) (applied on the shortest dependency paths between target protein pairs). The function can be computed recursively in closed form, and quite efficient implementations are available (Moschitti, 2006). It should be stressed that we are comparing against a fast TK implementation that is almost linear in time with respect to the number of tree nodes (Moschitti, 2006). For classification we applied the updated tree-kernel package (Moschitti 2006), distributed with the svm-light tool (Joachims 1999) for learning Support Vector Machines (SVMs). The worse case is not really informative since as shown in (Moschitti, 2006), we can design fast algorithm with a linear average running time (we use such algorithm for SSTK).  Collins and Duffy (Collins and Duffy, 2002) suggested to employ convolution kernels to measure similarity between two trees in terms of their sub structures, and more recently, Moschitti (Moschitti, 2006) described in details a fast implementation of tree kernels. More recently, Moschitti (Moschitti, 2006) introduced in details a fast implementation of tree kernels, where a node pair set is first constructed for those associated with same production rules.   Although this kernel achieves state-of-the-art performance in NLP tasks, such as Question Classification (Bloehdorn and Moschitti, 2007b) and Textual Entailment (Mehdad et al, 2010), it offers clearly possibility of improvement: (i) better possibility to exploit semantic smoothing since, e.g., trivially STK only matches the syntactic structure apple/orange when comparing the big beautiful apple to a nice large orange; and (ii) STK cannot be effectively applied to dependency structures, e.g. see experiments and motivation in (Moschitti, 2006a).
Coping With Ambiguity And Unknown Words Through Probabilistic Models Heights, NY. Katz, S. M. (1987). &quot;Estimation of probabilities from sparse data for the language model component of a speech In Transactions on Acoustics, Speech, and Signal Processing, Vol. ASSP-35 No. 3. Kuhn, R., and De Mori, R. (1990). &quot;A cache-based natural language model for recognition.&quot; In Transactions on Pattern Analysis and Machine Intelligence, 12,570-583. Kupiec, J. (1989). &quot;Augmenting a hidden Markov model for phrase-dependent tagging.&quot; In Speech and Language Workshop. We were already using a generative statistical model for part-of-speech tagging (Weischedel et al 1993). Word features are introduced primarily to help with unknown words, as in (Weischedel et al 1993). Weischedel's group (Weischedel et al, 1993) examines unknown words in the context of part-of-speech tagging. in (Weischedel et al, 1993) where an unknown word was guessed given the probabilities for an unknown word to be of a particular pos, its capitalisation feature and its ending. For words that were unknown in our subtree set, we guessed their categories by means of the method described in Weischedel et al (1993) which uses statistics on word-endings, hyphenation and capitalization. More recently, the natural language processing community has effectively employed these models for part-of speech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al, 1993). For words that were unknown in the training set, we guessed their categories by means of the method described in Weischedel et al (1993) which uses statistics on word-endings, hyphenation and capitalization. More advanced methods like those described by Weischedel et al (1993) incorporate the treatment of unknown words within the probability model. The output produced is in the tradition of partial parsing (Hindle 1983, McDonald 1992, Weischedel et al 1993) and concentrates on the simple noun phrase. Weischedel et al (1993) combine several heuristics in order to estimate the token generation prob ability according to various types of information. In our framework, we employ a simple HMM-based tagger, where the most probable tag sequence, given the words, is out put (Weischedel et al, 1993). In addition to the ending, Weischedel et al (1993) exploit capitalisation.   The practice of allowing only open-class tags for unknown words goes back a long way (Weischedel et al, 1993), and proved highly beneficial also in our case.
Integrated Annotation For Biomedical Information Extraction We describe an approach to two areas of biomedical information extraction, drug development and cancer genomics. We have developed a framework which includes corpus annotation integrated at multiple levels: a Treebank containing syntactic structure, a Propbank containing predicate-argument structure, and annotation of entities and relations among the entities. Crucial to this approach is the proper characterization of entities as relation components, which allows the integration of the entity annotation with the syntactic structure while retaining the capacity to annotate and extract more complex events. We are training statistical taggers using this annotation for such extraction as well as using them for improving the annotation process. We omit discussion here of the corpus currently in production by the University of Pennsylvania and the Children's Hospital of Philadelphia (Kulick et al 2004), since it is not yet available in finished form. A current project at the University of Pennsylvania and the Children's Hospital of Philadelphia (Kulick et al 2004) is producing a corpus that follows many of these basic principles. For example, the BioText (Rosarioand Hearst, 2004) corpus has no specific annotation guideline and contains several inconsistencies, while PennBioIE (Kulick et al, 2004) is very specific to a particular sub-domain of diseases. The first dataset (PBIO) is based on the annotations of the PENNBIOIE corpus for biomedical entity extraction (Kulick et al, 2004). In the biomedical domain, for example, several annotated corpora such as GENIA (Kim et al, 2003), PennBioIE (Kulick et al, 2004), and GENETAG (Tanabe et al, 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suit able for text mining tasks that other researchers need to address. Our annotation guidelines are based on those developed for annotating full sub-NP structure in the biomedical domain (Kulick et al, 2004). For the second adaptation task we were given a large collection of unlabeled data in the chemistry domain (Kulick et al 2004) as well as a test set of 5000 tokens (200 sentences) to parse (eng-lish_pchemtbtb_test.conll). Their corpus contains MEDLINE abstracts on the inhibition of the enzyme CYP450 (Kulick et al, 2004), specifically those abstracts that contain at least one overlapping and one discontinuous annotation. Both the GENIA corpus (Kim et al, 2003) and the BioIE cytochrome P450 corpus (Kulick et al, 2004) come with named entity annotations that include a proportion of chemicals, and at least a few abstracts that are recognis able as chemistry abstracts. The BioIE P450 corpus (Kulick et al, 2004), by contrast, includes chemicals, proteins and other sub stances such as foodstuffs in a single category called substance.  Notable examples of corpus construction projects for the biomedical domain are PennBioIE (Kulick et al, 2004) and GENIA (Kim et al, 2003). Similar to the approach in (Miller et al, 2000) and (Kulick et al, 2004), our parser integrates both syntactic and semantic annotations into a single annotation as shown in Figure 2.  First, there is some evidence suggesting that standoff annotation and embedded XML are the two most highly preferred corpus annotation formats, and second, these for mats are employed by the two largest extant curated biomedical corpora, GENIA (Kim et al, 2001) and BioIE (Kulick et al, 2004). The development data was 200 sentences of labeled biomedical oncology text (BIO, the ONCO portion of the Penn Biomedical Treebank), as well as 200K unlabeled sentences (Kulick et al, 2004). Based on an analysis of the PennBioIE corpus (Kulick et al, 2004), detailed distributional results are provided on alternation patterns for several nominalizations with high frequency of occurrence in biomedical text, such as activation and treatment. From the sub language biology domain, we used the oncology part of the PENNBIOIE corpus (Kulick et al,2004) and removed all but three gene entity subtypes (generic, protein, and rna). (Co hen et al, 2008) investigated syntactic alternations of verbs and their nominalized forms which occurred in the PennBioIE corpus (Kulick et al, 2004), whilst keeping PASs of the PASBio in their minds. The BioFrameNet (Dolbey et al, 2006) is an at tempt to extend the FrameNet with specific frames to the bio-medical domain, and to apply the frames to corpus annotation. Similar attempt of constructing integrated corpora is being done in University of Pennsylvania, where a corpus of MEDLINE abstracts in CYP450 and oncology domains where annotated for named entities, POS, and tree structure of sentences (Kulick et al, 2004).
Development And Use Of A Gold-Standard Data Set For Subjectivity Classifications This paper presents a case study of analyzing and improving intercoder reliability in discourse using statistical techniques. corrected tags are formulated and successfully used to guide a revision of the coding manual and develop an automatic classifier. Specifically, each sentence was assigned a subjective or objective classitication, according to concensus lags derived by a stalistical analysis of the chisses assigned by three human judges (see (Wiebe et al, 1999) for further information). Other approaches to annotator quality control include using EM-based algorithms for estimating annotator bias (Wiebe et al 1999, Ipeirotis et al 2010). Wiebe et al (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. We followed (Wiebe et al, 1999) in rationalizing the subjective vs. the objective categories. Wiebe et al (1999) use statistical methods to automatically correct the biases in an notations of speaker subjectivity. Our experimental results show that the subjectivity classifier performs well (77% recall with 81% precision) and that the learned nouns improve upon previous state-of-the-art subjectivity results (Wiebe et al, 1999). Row (2) is a Naive Bayes classifier that uses the WBO features, which performed well in prior research on sentence-level subjectivity classification (Wiebe et al, 1999). In contrast, our work classifies individual sentences, as does the research in (Wiebe et al, 1999). Bruc eand Wiebe (1999) annotated 1,001 sentences as subjective or objective, and Wiebe et al (1999) described a sentence-level Naive Bayes classifier using as features the presence or absence of particular syntactic classes (pronouns, adjectives, cardinal numbers, modal verbs, adverbs), punctuation, and sentence position. While words and n-grams had little performance effect for the opinion class, they increased the recall for the fact class around five fold compared to the approach by Wiebe et al (1999). Wiebe et al (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. Following (Wiebe et al, 1999), if the primary goal of a sentence is judged as the objective reporting of information, it was labeled as OBJ. Wiebe et al (1999) train a sentence-level probabilistic classifier on data from the WSJ to identify subjectivity in these sentences. Again, our feature set is richer than Wiebe et al (1999). Previous work on sentence-level subjectivity classification (Wiebe et al, 1999) used training corpora that had been manually annotated for subjectivity.  According to the coding manual (Wiebe et al, 1999), subjective sentences are those expressing evaluations, opinions, emotions, and speculations. One judge annotated all articles in four datasets of the Wall Street Journal Treebank corpus (Marcus et al, 1993) (W9-4, W9-10, W9-22, and W9 33, each approximately 160K words) as well as the corpus of Wall Street Journal articles used in (Wiebe et al, 1999) (called WSJ-SE below). Additionally, it may be possible to refine the classications automatically using methods such as those described in (Wiebe et al, 1999). During the past few years, the problem of polarity recognition has been usually faced as a step beyond the identification of the subjectivity or objectivity of texts (Wiebe et al, 1999).
Chinese Segmentation And New Word Detection Using Conditional Random Fields Chinese word segmentation is a difficult, im portant and widely-studied sequence modelingproblem. This paper demonstrates the abil ity of linear-chain conditional random fields(CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework that easily supports the in tegration of domain knowledge in the form of multiple lexicons of characters and words. We also present a probabilistic new word detection method, which further improves performance. Our system is evaluated on four datasets usedin a recent comprehensive Chinese word segmentation competition. State-of-the-art perfor mance is obtained. A simple approach would be to let a character be a token (i.e., character-based Begin/Inside tagging) so that boundary ambiguity never occur (Peng et al, 2004). Peng et al (2004) uses the CRFs to address this issue. In last part of the experiments, the generality of the datasets and the toughness of our system are tested (Peng et al, 2004). The superiority of CRFs on Chinese information processing was also demonstrated in word segmentation (Peng et al 2004). Different from (Peng et al, 2004), we represent the positions of a hanzi (Chinese character) with four different tags: B for a hanzi that starts a word, I for a hanzi that continues the word, F for a hanzi that ends the word, S for a hanzi that occurs as a single-character word. CRFs using this technique have been shown to be very successful at the task of Chinese word segmentation (CWS), starting with the model of Peng et al (2004). A popular discriminative model that have been used for this task is the conditional random fields (CRFs) (Lafferty et al, 2001), starting with the model of Peng et al (2004). Since Chinese Word Segmentation was firstly treated as a character-based tagging task in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al, 2004), (Tseng et al, 2005), (Low et al., 2005), (Zhao et al, 2006). CRF is a statistical sequence modeling framework introduced by Lafferty et al (2001), and was first used for the Chinese word segmentation task by Peng et al (2004), who treated word segmentation as a binary decision task. We follow the format from Peng et al (2004). represents the CRF model from Peng et al (2004), and the last row represents our model. RMNs are the special case of Conditional Markov Networks (or Conditional Random Fields) in which graph structure and parameter tying are determined by SQL-like form. As for the marginal probability to use as a confidence measure shown in Figure 4, Peng et al (2004) has applied linear-chain CRFs to Chinese word segmentation. Aiming to improve both tasks, work by Peng et al (2004) and Sun et al (2012) conduct segmentation and detection sequentially, but in an iterative manner rather than joint. Work by Peng et al (2004) first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one. 3.2.1 Results on Sighanbakeoff 2003 Experiments done while developing this system showed that its performance was significantly better than that of Peng et al (2004).  Peng et al (2004) defined the word segmentation problem as labeling each character as whether or not the previous character boundary of the current character is a word boundary. We also used lexical features consulting a dictionary: one is to check if any of the above defined character n-grams appear in a dictionary (Peng et al, 2004), and the other is to check if there are any words in the dictionary that start or end at the current character boundary. text chunking model (Ramshaw and Marcus, 1995), which has been previously applied to Chinese segmentation (Peng et al, 2004). Different from (Peng et al, 2004), we represent the positions of a hanzi (Chinese character) with four different tags: B for a hanzi 196 that starts a word, I for a hanzi that continues the word, F for a hanzi that ends the word, S for a hanzi that occurs as a single-character word.
A Machine Learning Approach To Coreference Resolution Of Noun Phrases In this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data sets. Decision tree algorithms were used for reference resolution by Aone and Bennett (1995, C4.5), McCarthy and Lehnert (1995, C4.5) and Soon et al (2001, C5.0). It was criticized (Soon et al, 2001) that the features used by McCarthy and Lehnert (1995) are highly idiosyncratic and applicable only to one particular domain. Soon et al (2001) use twelve features (see Table 1). Soon et al (2001) include all noun phrases returned by their NP identifier and report an F-measure of 62.6% for MUC-6 data and 60.4% for MUC-7 data. Features like the string ident and substring match features were used by other researchers (Soon et al, 2001), while the features ante med and ana med were used by Strube et al (2002) in order to improve the performance for definite NPs. However, reference resolution algorithms based on these classifiers achieve reasonable performance of about 60 to 63% F-measure (Soon et al, 2001). While the second best system (Bjorkelund and Farkas, 2012) followed the widely used baseline of Soon et al (2001), the winning system (Fernandes et al, 2012) proposed the use of a tree representation. For instance, the popular pairwise instance creation method suggested by Soon et al (2001) assumes non-branching trees, where the antecedent of every mention is its linear predecessor (i.e., he b 2 is the antecedent of Gary Wilber b 3). By using a simple co-reference resolution tool adapted from (Soon et al, 2001), we add all the mentions referring to the target into the extended target set. Instances are created following Soon et al (2001). Following Ng & Cardie (2002), our baseline system reimplements the Soon et al (2001) system. We start with a baseline system using all the features from Soon et al (2001) that were not removed in the feature selection process (i.e. DISTANCE). It supports both local (Soon et al (2001)-style) and global (ILP, Denis and Baldridge (2007)-style) models of coreference. Our local model of coreference is a reimplementation of the algorithm, proposed by Soon et al (2001) with an extended feature set. PAIRWISE: as in the work by Soon et al (2001), antecedent identification and anaphoricity determination are simultaneously executed by a single classifier. Our event-anaphora resolution system adopts the common learning-based model for object anaphora resolution, as employed by (Soon et al, 2001) and (Ng and Cardie, 2002a). We construct this entity-mention graph by learning to decide for each mention which preceding mention, if any, belongs in the same equivalence class; this approach is commonly called the pairwise coreference model (Soon et al, 2001). Soon et al (2001) use the Closest-Link method: They select as an antecedent the closest preceding mention that is predicted coreferential by a pairwise coreference module; this is equivalent to choosing the closest mention whose pc value is above a threshold. Distance features are important for a system that makes links based on the best pairwise coreference value rather than implicitly incorporating distance by linking only the closest pair whose score is above a threshold, as done by e.g. Soon et al (2001). The remaining predicates in Table 1 are a subset of features used by other coreference resolution systems (cf. Soon et al., 2001).
Comparative Experiments On Disambiguating Word Senses: An Illustration Of The Role Of Bias In Machine Learning This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context. The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques. The specific problem tested involves disambiguating six senses of the word &quot;line&quot; using the words in the current and proceeding sentence as context. The statistical and neural-network methods perform the best on this particular problem and we discuss a potential reason for this obdifference. We also discuss the role of in machine learning and its importance in explaining performance differences observed on specific problems. This includes basic linguistic problems such as morphological analysis (van den Bosch et al., 1996), parsing (Zelle and Mooney, 1996), word sense disambiguation (Mooney, 1996), and anaphora resolution (Aone and Bennett, 1996). Mooney (1996) argues that Naive Bayes classification and perceptron classifiers are particularly fit for lexical sample word sense disambiguation problems, because they combine weighted evidence from all features rather than select a subset of features for early discrimination. While it is unquestionable that certain algorithms are better suited to the WSD problem than others (for a comparison, see Mooney (1996)), it seems that, given similar input features, various algorithms exhibit roughly similar accuracies. Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). Bag of words feature sets made up of unigrams have had a long history of success in text classification and word sense disambiguation (Mooney, 1996), and we believe that despite creating quite a bit of noise can provide useful information for discrimination. Mooney (Mooney, 1996) has discussed the effect of bias on inductive learning methods. Naive Bayes models (e.g., Mooney (1996), Chodorow et al (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)) in particular have shown a large degree of success for WSD, and have established challenging state-of-the-art benchmarks. Naive Bayes is particularly useful when relatively small amounts of training CSF instances are available (Zhang, 2004), and achieves good results when compared to other classifiers for the WSD task (Mooney, 1996), which might explain our results. They have a long history of use in word sense disambiguation, dating back to early work by (Black, 1988), and have fared well in comparative studies such as (Mooney,1996) and (Pedersen and Bruce, 1997). Achieving higher precision in supervised word sense disambiguation (WSD) tasks without resorting to ad hoc voting or similar ensemble techniques has become somewhat daunting in recent years, given the challenging benchmarks set by naive Bayes models (e.g., Mooney (1996), Chodorowetal. Some clusters of studies have used common test suites, most notably the 2094-word Hne data of Leacock et al (1993), shared by Lehman (1994) and Mooney (1996) and evaluated on the system of Gale, Church and Yarowsky (1992). Some researchers use neural networks in their word sense disambiguation systems Because of its strong capability in classification (Waltz et al, 1985, Gallant, 1991, Leacock et al, 1993, and Mooney, 1996).
SemEval-2007 Task 19: Frame Semantic Structure Extraction This task consists of recognizing words and phrases that evoke semantic frames as defined in the FrameNet project (http: //framenet.icsi.berkeley.edu), and their semantic dependents, which are usually, but not always, their syntacticdependents (including subjects). The train ing data was FN annotated sentences. In testing, participants automatically annotated three previously unseen texts to match goldstandard (human) annotation, including pre dicting previously unseen frames and roles. Precision and recall were measured both for matching of labels of frames and FEs and for matching of semantic dependency trees based on the annotation. Therefore, there is nowadays a pressing need to adopt learning approaches to extend the coverage of the FrameNet lexicon by automatically acquiring new LUs, a task we call LU induction, as recently proposed at SemEval-2007 (Baker et al, 2007). We also tested our models on a realistic gold-standard set of 24 unknown LUs extracted from the SemEval-2007 corpus (Baker et al., 2007). Johansson and Nugues (2007) presented the best performing system at SemEval 2007 (Baker et al, 2007), and Das et al (2010) improved performance, and later set the current state of the art on this task (Das et al,2014). Previous shared tasks have shown that frame-semantic SRL of running text is a hard problem (Baker et al, 2007), partly due to the fact that running text is bound to contain many frames for which no or little annotated training data are available. It is one of the main reason for the performance drop of supervised SRL systems inout-of-domain scenarios (Baker et al, 2007) (Johansson and Nugues, 2008). LU induction has been integrated at SemEval-2007 as part of the Frame Semantic Structure Extraction shared task (Baker et al, 2007), where systems are requested to assign the correct frame to a given LU, even when the LU is not yet present in FrameNet. In our experiments, we use FrameNet 1.5, which contains a lexicon of 877 frames and 1,068 role labels, and 78 documents with multiple predicate argument annotations (a superset of the SemEval shared task dataset; Baker et al, 2007). We will also use our measures in applications, to check their effectiveness in supporting various tasks, e.g. in mapping frames across Text and Hypothesis in RTE, in linking related frames in discourse, or in inducing frames for LU which are not in FrameNet (Baker et al, 2007). They used the mapping in the Semeval-2007 task on frame-semantic structure extraction (Baker et al, 2007) in order to find target words in open text and assign frames. Crespo and Buitelaar (2008) carried out an automatic mapping of medical-oriented frames to WordNet synsets applying a Statistical Hypothesis Testing to select synsets attached to a lexical unit that were statistically significant using a given reference corpus. Our parser achieves the best published results to date on the SemEval 07 FrameNet task (Baker et al, 2007). More details can be found in Baker et al (2007). Recent work on frame-semantic parsing in which sentences may contain multiple frames to be recognized along with their arguments has used the SemEval 07 data (Baker et al, 2007). Domain-oriented semantic structures are valuable assets because their representation suits information needs in the domain; however, the extraction of such structures is difficult due to the large gap between the text and these structures. On the other hand, the extraction of linguistically oriented semantics from text has long been studied in computational linguistics, and has recently been formalized as Semantic Role Labeling (Gildea and Jurafsky, 2002), and semantic structure extraction (Baker et al, 2007) (Surdeanu et al, 2008). A variety of methods have been developed for semantic role labeling with reasonably good performance (F 1 measures in the low 80s on standard test collections for English; we refer the interested reader to the proceedings of the SemEval-2007 shared task (Baker et al, 2007) for an overview of the state-of-the-art). Considering how the performance of supervised systems degrades on out-of-domain data (Bakeret al, 2007), not to mention unseen events, semi supervised or unsupervised methods seem to offer the primary near-term hope for broad coverage semantic role labeling. We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see Baker et al (2007) for an overview). A variety of other systems have focused on FrameNet-based (1998) SRL instead, including those that participated in the SemEval-2007 Task 19 (Baker et al, 2007) and work by Das et al (2010). The approaches are too numerous to list; we refer the interested reader to the proceedings of the SemEval-2007 shared task (Baker et al, 2007) for an overview of the state of-the-art. We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see Baker et al (2007) for an overview). SVM classifiers were trained to identify the arguments and label them with appropriate roles. Recently, since the release of full-text annotations in SemEval 07 (Baker et al, 2007), there has been work on identifying multiple frames and their corresponding sets of arguments in a sentence.
Finite-State Transducers In Language And Speech Processing Finite-state machines have been used in various domains of natural language processing. We consider here the use of a type of transducer that supports very efficient programs: sequential transducers. We recall classical theorems and give new ones characterizing sequential string-tostring transducers. Transducers that output weights also play an important role in language and speech processing. We give a specific study of string-to-weight transducers, including algorithms for determinizing and minimizing these transducers very efficiently, and characterizations of the transducers admitting determinization and the corresponding algorithms. Some applications of these algorithms in speech recognition are described and illustrated. for a recent elaboration of these concepts see Mohri, 1997 [13]). Weighted finite-state transducers have found recent favor as models of natural language (Mohri, 1997). Application of cascades of weighted string transducers (WSTs) has been well-studied (Mohri,1997). We recall previous formal presentations of WSTs (Mohri, 1997) and note informally that they may be represented as directed graphs with designated start and end states and edges labeled with input symbols, output symbols, and weights. There exist a general weighted determinization and weight pushing algorithms that can be used to create a deterministic and pushed automaton equivalent to an input word or phone lattice (Mohri, 1997). To apply the rules defining the classes to the input corpus, we just need to compose the automaton with a and project the result on the output: X can be made stochastic using a pushing algorithm (Mohri, 1997).  The most attractive feature of the FST (Finite State Transducer) formalism lies in its superior time and space efficiency [Mohri 1997]. Sample XFST code Finite-state transducers are robust and time and space efficient (Mohri, 1997). This two-way power of the finite-state transducer (Mohri, 1997) has significantly reduced the amount of efforts to build the HUMT system. Another very important and powerful strength of finite-state transducers, they can be composed together to build a single transducer that can perform the same task that could be done with help of two or more transducers when applied sequentially (Mohri, 1997), not only allows us to build a direct Hindi Urdu transducer, but also helps to divide difficult and complex problems into simple ones, and has indeed simplified the process of building the HUMT system. Equivalence is efficiently tested by pushing the (deterministic) automata to canonicalize their arc labels and then testing unweighted equivalence (Mohri, 1997). However, the computational consequences of this can be lessened by lazy evaluation techniques (Mohri, 1997) and we believe that this finite state approach to constructing semantic representations is viable for a broad range of sophisticated language interface tasks. The combined WFST will be more efficient by optimizing with determinization, minimization and pushing algorithms of WFSTs (Mohri, 1997). Regular languages, especially generated by deterministic finite state automata are widely used in Natural Language processing, for various different tasks (Mohri, 1997). Not all PFAs can be determinized, as discussed by (Mohri, 1997). Numerous examples of the utility of word lattices come from the field of finite state automata, language modeling, speech recognition, parsing and machine translation (Mohri, 1997, inter alia). O-TRIE is a deterministic right-branching trie encoding (Leermakers, 1992) with weights pushed left (Mohri, 1997). Additionally, with I-tries, only the top-level intermediate rules have probability less than 1, while for O-tries, one can back-weight probability as in (Mohri, 1997). For example transducer determinization (Mohri, 1997) uses a set of pairs of states and weights.
Better K-Best Parsing We discuss the relevance of k-best parsing torecent applications in natural language pro cessing, and develop efficient algorithms for k-best trees in the framework of hypergraphparsing. To demonstrate the efficiency, scal ability and accuracy of these algorithms, we present experiments on Bikel?s implementation of Collins? lexicalized PCFG model, and on Chiang?s CFG-based decoder for hierarchicalphrase-based translation. We show in particu lar how the improved output of our algorithms has the potential to improve results from parse reranking systems and other applications. We will present an algorithm for determinizing weighted finite tree recognizers, and use a variant of the procedure found in (Huang and Chiang, 2005) to obtain -best lists of trees that are weighted correctly and contain no repetition.  We follow the third algorithm in Huang and Chiang (2005), where first a traditional Viterbi-chart is created, which enumerates in an efficient way all possible sub derivations. In terms of decoding time, even though we used Algorithm 3 described in (Huang and Chiang, 2005), which lazily generated the N-best translation candidates, the decoding time tended to be increased because more rules were available during cube pruning. The latter function uses bi nary lazy enumeration in a manner similar to (Huang and Chiang, 2005), and relies on two global variables: I and L. The k= 200-best parses at the top cell of the chart are calculated using the efficient algorithm of (Huang and Chiang, 2005). This technique utilizes a bunch of linguistic features to re-rank the k-best (Huang and Chiang 2005) output on the forest level or tree level. We could also have used the more efficient k-best hyper graph parsing technique by Huang and Chiang (2005), but we have not yet incorporated this into our implementation. N-best list not the lazy algorithm of (Huang and Chiang, 2005). It is found to be well handled by the K-Best parsing method in Huang and Chiang (2005). Those weighted tree languages are recognizable and there exist algorithms (Huang and Chiang, 2005) that efficiently extract the k-best parse trees (i.e., those with the highest probability) for further processing. k-best lists are extracted from the CRF trellis using the lazy enumeration algorithm of Huang and Chiang (2005). Translation hyper graphs are generated by each baseline system during the MAPde coding phase, and 1000-best lists used for MERT algorithm are extracted from hyper graphs by the k-best parsing algorithm (Huang and Chiang, 2005). Although Viterbi and k-best extraction algorithms are often expressed as INSIDE algorithms with the tropical semiring ,cdec provides a separate derivation extraction framework that makes use of a &lt; operator (Huang and Chiang, 2005). The amount of work done in the k-best phase is no more than the amount of work done by the algorithm of Huang and Chiang (2005). In a certain sense, described in greater detail below, this precomputation of exact heuristics is equivalent to the k-best extraction algorithm of Huang and Chiang (2005). By exploiting a local ordering amongst derivations, we can be more conservative about combination and gain the advantages of a lazy successor function (Huang and Chiang, 2005). This triggering is similar to the lazy frontier used by Huang and Chiang (2005). As a baseline, we compared KA∗ to the approach of Huang and Chiang (2005), which we will call EXH (see below for more explanation) since it requires exhaustive parsing in the bottom-up pass. While formulated very differently, one limiting case of our algorithm relates closely to the EXH algorithm of Huang and Chiang (2005).
The Second Release Of The RASP System We describe the new release of the RASP (robust accurate statistical parsing) system, designed for syntactic annotation of free text. The new version includes a revised and more semantically-motivated output representation, an enhanced grammar and part-of-speech tagger lexicon, and a more flexible and semi-supervised training method for the structural parse ranking model. We evaluate the released version on the WSJ using a relational evaluation scheme, and describe how the new release allows users to enhance performance using (in-domain) lexical information. Its main components are the Conditional Random Fields toolkit MALLET (McCallum, 2002) and the RASP syntactic parsing toolkit (Briscoe et al, 2006), which are both publicly available. In brief, the abstracts of 16,609 articles curated by FlyBase were retrieved and tokenized by RASP (Briscoe et al, 2006). The parameters of the model were estimated from the British National Corpus that was parsed using the RASP parser of Briscoe et al (2006). We parsed the corpus with Rasp (Briscoe et al, 2006) and with the Stanford PCFG parser (Klein & Manning, 2003). The RASP toolkit (Briscoe et al, 2006) is used for sentence boundary detection, tokenisation, PoStagging and finding grammatical relations (GR) between words in the text. The search space for metaphor identification was the British National Corpus (BNC) that was parsed using the RASP parser of Briscoe et al (2006). The Preiss system extracts a verb instance's GRs using the Rasp general-language unlexicalized parser (Briscoe et al, 2006) as input, and based on hand crafted rules, maps verb instances to a predefined inventory of 168 SCFs. Our input has been parsed into Rasp-style tGRs (Briscoe et al, 2006), which facilitates comparison with previous work based on the same data set. Since the parser that produced them is known to perform well on general language (Briscoe et al, 2006), the tGRs are of high quality: it makes sense that reverting to the pg Rs is unnecessary in this case.  All models were trained on the 90-million word written component of the British National Corpus, lemmatised, POS-tagged and parsed with the RASP toolkit (Briscoe et al., 2006). All features are automatically extracted from the Robust Minimal Recursion Semantics (RMRS, Copestake, 2004) representation of the sentence in which the noun phrase appears (obtained via a RASP parse, Briscoe et al, 2006). The corpus used for our distributional similarity baseline consists of a subset of Wikipedia to talling 500 MB in size, parsed first with RASP (Briscoe et al, 2006) and then into a Robust Minimal Recursion Semantics form (RMRS, Copes take, 2004) using a RASP-to-RMRS converter. We expect our reimplementation of the method to extract data more accurately, since we use a more robust parser (RASP (Briscoe et al, 2006)), take into account more syntactic structures (coordination, passive), and extract our data from a newer version of the BNC. The parameters of the model were estimated from the British National Corpus (BNC) (Burnard, 2007) that was parsed using the RASP parser of Briscoe et al (2006). In addition we use a version annotated with the Rasp system (Briscoe et al, 2006), that tokenizes, tags, lemmatizes and parses the input sentences, outputting syntactic trees and then adding grammatical relations (GR) as described by (Buttery and Korhonen, 2005). Conjunctions are identified in the BNC by first parsing the corpus with Rasp (Briscoe et al, 2006) and extracting in stances of the conj grammatical relation. Several methods exist to do this - e.g. producing RMRS output from RASP (Briscoe et al, 2006) is described in Frank (2004). The only major differences with (Furstenau and Lapata, 2009) are the dependency parser which was used (the MALT parser (Nivre et al, 2006) instead of the RASP parser (Briscoe et al, 2006)) and the corpus employed to learn semantic similarities (the Reuters corpus instead of the British National Corpus). We parsed the BNC corpus with the RASP parser (Briscoe et al, 2006) and used it for feature extraction.
Bitext Maps And Alignment Via Pattern Recognition that are available in two languages becoming more and more plentiful, both in private data warehouses and on publicly accessible sites on the World Wide Web. As with other kinds of data, the value of bitexts largely depends on the efficacy of the available data mining tools. The first step in extracting useful information from bitexts is to find corresponding words and/or segment boundaries in their two halves maps). This article advances the state of the art of bitext mapping by formulating the problem in terms of pattern recognition. From this point of view, the success of a bitext mapping algorithm hinges on how well it performs three tasks: signal generation, noise filtering, and search. The Smooth Injective Map Recognizer (SIMR) algorithm presented here integrates innovative approaches to each of these tasks. Objective evaluation has shown that SIMR's accuracy is consistently high for language pairs as diverse as French/English and Korean/English. If necessary, SIMR's bitext maps can be efficiently converted into segment alignments using the Geometric Segment Alignment (GSA) algorithm, which is also presented here. SIMR has produced bitext maps for over 200 megabytes of French-English bitexts. GSA has converted these maps into alignments. Both the maps and the alignments are available from the Melamed (1999) aligns texts using correspondence points taken either from orthographic cognates (Michel Simard et al., 1992) or from a seed translation lexicon. The latter approach uses other filtering parameters: maximum point ambiguity level, point dispersion and angle deviation (Melamed, 1999, pp. 115-116). Melamed (1999) also filtered candidate correspondence points obtained from orthographic cognates. This approach is similar to Melamed (1999) but, in contrast, it is statistically supported and uses no heuristics. We use a similarity function proposed in (Contractor et al, 2010) which is based on Longest Common Subsequence Ratio (LCSR) (Melamed, 1999). The former includes string edit distance (Wagner and Fischer, 1974), longest common subsequence ratio (Melamed, 1999), and measures based on shared character n-grams (Brew and McKelvie, 1996). Eight baseline systems were prepared for comparison: Levenshtein distance (LD), normalized Levenshtein distance (NLD), Dicecoefficient on letter bigrams (DICE) (Adamson and Boreham, 1974), Longest Common Substring Ratio (LCSR) (Melamed, 1999), Longest Common Prefix Ratio (PREFIX) (Kondrak, 2005), Porter's stemmer (Porter, 1980), Morpha (Minnen et al,2001), and CST's lemmatiser (Dalianis and Jonge 3LRSPL table includes trivial spelling variants that can be handled using simple character/string operations. Other popular measures include Dice's Coefficient (DICE) (Adamson and Boreham, 1974), and the length-normalized measures Longest Common Subsequence Ratio (LCSR) (Melamed, 1999), and Longest Common Prefix Ratio (PREFIX) (Kondrak,2005). We adopt an improved definition (suggested by Melamed (1999) for the French-English Canadian Hansards) that does not over-propose shorter word pairs. Our labelled set is then generated from pairs with LCSR 0.58 (using the cutoff from Melamed (1999)). A few of the errors were genuine, and could be explained by failures of the sentence alignment program that was used to create the corpus (Melamed, 1999). The Korean-English parallel data was collected from news websites and sentence aligned using two different tools described by Moore (2002) and Melamed (1999).  Melamed (1999) normalized LCS by dividing the length of the longest common subsequence by the length of the longer string and called it longest common subsequence ratio (LCSR). The Longest Common Subsequence Ratio (LCSR) (Melamed, 1999) of two strings is the ratio of the length of their LCS and the length of the longer string. Our work is based on a modification of the SIMR bitext mapping algorithm (Melamed, 1999). For sentence alignment, the length-based Gale & Church aligner (1993) can be used, or - alternatively - Dan Melamed's GSA-algorithm (Geometric Sentence Alignment; Melamed, 1999).
Dependency Tree Kernels For Relation Extraction We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences. Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles. We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a “bag-of-words” kernel. More recently, (Zelenko et al, 2003) have proposed extracting relations by computing kernel functions between parse trees and (Culotta and Sorensen, 2004) have extended this work to estimate kernel functions between augmented dependency trees. Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees and achieved the F-measure of 45.8 on the 5 relation types in the ACE RDC 2003 corpus. This makes it suffer from the similar behavior with that of Culotta and Sorensen (2004): high precision but very low recall.  Culotta and Sorensen (2004) describe a slightly generalized version of this kernel based on dependency trees, in which a bag-of words kernel is used to compensate for errors in syntactic analysis.   Thus structure-based kernels can well model syntactic parse tree in a variety of applications, such as relation extraction (Zelenko et al, 2003), named entity recognition (Culotta and Sorensen, 2004), semantic role labeling (Moschitti et al, 2008) and so on. For example, (Culotta and Sorensen, 2004) add hypernyms of entities to features derived from WordNet. Table 5 summarizes the results of a comparison between the latent topic feature and the features used by (Culotta and Sorensen, 2004).  The Dependency Tree Kernel (DTK) of Culottaand Sorensen (2004) is based on the work of Zelenko et al (2003). To compare relations in two instance sentences X, Y Culotta and Sorensen (2004) proposes to compare the subtrees induced by the relation arguments x 1, x 2 and y 1, y 2, i.e. computing the node kernel between the two lowest common ancestors (lca) in the dependecy tree of the relation argument nodes K DTK (X, Y)=? (lca (x 1, x 2) ,lca (y 1, y 2)).  Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees. In (Culotta and Sorensen, 2004) a dependency tree kernel is used to detect the Named Entity classes in natural language texts.  This suggests that dependency information play a critical role in PPI extraction as well as in relation extraction from newswire stories (Culotta and Sorensen, 2004). Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005).
Typed Unification Grammars Martin C. Emele, Dhni Zajac Project Polygloss* University of Stuttgart IMS~CL/Ifl~AIS, Keplerstrage 17, D - 7000 Stuttgart 1, Federal Republic of Germany {emele,zajac} @is.informatik.uni-st ut gart.dbp.de Abstract We introduce TFS, a computer formal- ism in the class of logic ibrmaiisms which integrates a powerful type system.  As mentioned in (Emele and Zajac, 1990), the proposed approach inevitably leads to the consequence that the data structure becomes slightly complicated.  Implementations of sorted feature formalisms such as TDL (Krieger and Schifer, 1994), ALE (Carpenter, 1993), CUF (DSrre and Dorna, 1993), TFS (Emele and Zajac, 1990) and others have been used successfully for the development and testing of large grammars and lexicons, but they may be too slow for actual use in applications 180 because they are generally built on top of Prolog or LISP, and can therefore not be as efficient as the built-in unification of Prolog. A more radical approach however rooted in the traditional model is to fully map the typed unification grammars [Emele and Zajac, 1990 on the SNAP.  This gives the possibility to define more general relations, and in particular functions can be defined in a way similar to, for example, (Johnson and Rosner, 1989) and (Emele and Zajac, 1990). In this way the user can create an inheritance hierarchy which is similar but not identical to how inheritance is used in other formalisms uch as TFS (Emele and Zajac, 1990) or ALE (Carpenter, 1992).
Paraphrasing For Automatic Evaluation This paper studies the impact of paraphrases on the accuracy of automatic evaluation. Given a reference sentence and a machine-generated sentence, we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference. We apply our paraphrasing method in the context of machine translation evaluation. Our experiments show that the use of a paraphrased synthetic reference refines the accuracy of automatic evaluation. We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation. Kauchak and Barzilay (2006) have shown that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT system's output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy. Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al, 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al, 2001), and machine translation (Blatz et al, 2004) to name a few. Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al (2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases. Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). Training a classifier for word paraphrasing, Kauchak and Barzilay (2006) used occurrences of the rule's RHS as positive context examples, and randomly picked negative examples. Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. In text summarization (Zhou et al, 2006) and machine translation (Kauchak and Barzilay, 2006), summaries comparison based on sentence similarity has been applied for automatic evaluation. In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. Kauchak and Barzilay (2006) and Owczarzak et al (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al (2006). We introduced synonyms and paraphrases into the process of evaluation, creating new best-matching references for the translations using either paraphrases derived from the test set itself (following Owczarzak et al (2006)) or WordNet synonyms (as in Kauchak and Barzilay (2006)). To maximize the number of matches between a translation and a reference, Kauchak and Barzilay (2006) use WordNet synonyms during evaluation. For example, (Kauchak and Barzilay, 2006) paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like BLEU (Papineni et al, 2002). Therefore, synonym lexicons found with statistical methods might provide a viable alternative for manually constructed lexicons (Kauchak and Barzilay, 2006). In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score.  PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboueand Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al, 1991), text simplification in computer-aided reading (Carroll et al, 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al, 2006). This application is important for the automatic evaluation of machine translation and summarization, since we can paraphrase the human translations/summaries to make them more similar to the system outputs, which can refine the accuracy of the evaluation (Kauchak and Barzilay, 2006). Sentence Similarity computation: Kauchak and Barzilay (2006) have tried paraphrasing-based sentence similarity computation.
An All-Subtrees Approach To Unsupervised Parsing We investigate generalizations of the allsubtrees &quot;DOP&quot; approach to unsupervised parsing. Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees. We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent. We report state-ofthe-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data. To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal, leading the surprising result that unsupervised parsing model beats a widely used supervised model (a treebank PCFG). While Klein and Manning's approach may be described as an "all-substrings" approach to unsupervised parsing, an even richer model consists of an "all-subtrees" approach to unsupervised parsing, called U-DOP (Bod 2006). Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Klein and Manning (2002, 2004). While we do not achieve as high an f-score as the UML-DOP model in Bod (2006), we will show that U-DOP* can operate without subtree sampling, and that the model can be trained on corpora that are two orders of magnitude larger than in Bod (2006). We will use the same all subtrees methodology as in Bod (2006), but now by applying the efficient and consistent DOP* based estimator. This is a huge reduction compared to Bod (2006) where the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work. Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP (Bod 2006). To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Klein and Manning (2002, 2004) and Bod (2006): Penn's WSJ10 which contains 7422 sentences ? 10 words after removing empty elements and punctuation, the German NEGRA10 corpus and the Chinese Treebank CTB10 both containing 2200+ sentences ? 10 words after removing punctuation. All trees in the test set were binarized beforehand, in the same way as in Bod (2006). Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in Bod (2006), the CCM model in Klein and Manning (2002), the DMV dependency model in Klein and Manning (2004) and their combined model DMV+CCM. Bod (2006) reports that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences ? 40 words, while the binarized version achieves only 64.6% f-score. While a similar result was obtained in Bod (2006), the absolute difference between unsupervised parsing and the treebank grammar was extremely small in Bod (2006): 1.8%, while the difference in table 5 is 7.2%, corresponding to 19.7% error reduction. DOP maximizes what has been called the 'structural analogy' between a sentence and a corpus of previous sentence-structures (Bod 2006b). Although several alternative versions of U DOP have been proposed (e.g. Bod 2006a, 2007), we will stick to the computation of the MPSD for the current paper. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models. Bod (2006) describes an unsupervised system within the Data-Oriented-Parsing frame work. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ10). For some time, multipoint performance degradations caused by switching to automatically induced word categories have been interpreted as indications that "good enough" parts-of-speech induction methods exist, justifying the focus on grammar induction with supervised part-of-speech tags (Bod, 2006), pace (Cramer, 2007). Finally, Seginer (2007) and Bod (2006) approach unsupervised parsing by constructing novel syntactic models.
A Tree Sequence Alignment-based Tree-to-Tree Translation Model mzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sg aaiti@i2r.a-star.edu.sg lisheng@hit.edu.cn hli@i2r.a-star.edu.sg Abstract This paper presents a translation model that is based on tree sequence alignment, where a tree sequence refers to a single sequence of subtrees that covers a phrase. The model leverages on the strengths of both phrase-based and linguistically syntax-based method. It automatically learns aligned tree sequence pairs with mapping probabilities from word-aligned biparsed parallel texts. Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span. This gives our model stronger expressive power than other reported models. Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems. This method generates all possible tree fragments rooted by each node in the source parse tree or forest, and then matches all the generated tree fragments against the source parts (left hand side) of translation rules to extract the useful rules (Zhang et al, 2008a). Works that apply the TTT model include Gildea (2003) and Zhang et al (2008). These works mainly try to incorporate non-syntatic phrases into a syntax-based model: while Liu et al (2006) integrates bilingual phrase tables as separate TTS templates, Zhang et al (2008) uses an algorithm to convert leaves in a parse tree to phrases be fore rule extraction. In particular, I will investigate settings that incorporate non syntactic phrases, using methods similar to Liu et al (2006) and Zhang et al (2008). For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al (2008). Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al, 2008). Synchronous tree-sequence substitution grammar (STSSG) al lows either side of a rule to comprise a sequence of trees instead of a single tree (Zhang et al, 2008). Finally, STSSG, which have been derived from rational tree relations (Raoult, 1997), have been discussed by Zhang et al (2008a), Zhang et al (2008b), and Sun et al (2009). However, Zhang et al (2008b) and Sunet al (2009) demonstrate that the additional expressivity gained from non-contiguous rules greatly improves the translation quality. A model that is even more powerful than LMBOT is the non-contiguous version of STSSG (synchronous tree-sequence substitution grammar) of Zhang et al (2008a), Zhanget al (2008b), and Sun et al (2009), which allows sequences of trees on both sides of rules [see also (Raoult, 1997)]. Chiang (2005) and Graehl et al (2008) argue that STSG have sufficient expressive power for syntax based machine translation, but Zhang et al (2008a) show that the additional expressive power of tree sequences helps the translation process. Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al, 2008).  Similar to the definition of tree sequence used in a single parse tree defined in Liu et al (2007) and Zhang et al (2008a), a tree sequence in a forest also refers to an ordered sub-tree sequence that covers a continuous phrase without overlapping.  Among them, Zhang et al (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence pairs, and find them useless via real syntax-based translation systems. In our opinion, the non-contiguous phrasal rules themselves may not play a trivial role, as reported in Zhang et al (2008a). tree-to-tree translation model based on tree sequence alignment (Zhang et al 2008a) without losing of generality to most syntactic tree based models. By means of the Initial rules, we derive the Abstract rules similarly as in Zhang et al (2008a). ncPR refers to non-contiguous phrasal rules derived from contiguous tree sequence pairs with at least one non-terminal leaf node between two lexicalized leaf nodes (i.e., all non-contiguous rules in STSSG defined as in Zhang et al (2008a).
Assessing Agreement On Classification Tasks: The Kappa Statistic Currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other. Meanwhile, researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic. We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as afield adopting techniques from content analysis. As advocated by Carletta (1996), we have used the Kappa coefficient (Siegel and Castellan, 1988) as a measure of coder agreement. Following the works of Carletta (1996) and Artstein and Poesio (2008), there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort, chance-corrected measures of inter-annotator agreement should be used. (Cohen, 1960, introduced to computational linguistics by Carletta, 1996) and pi (Scott, 1955). The reliability of the annotation was evaluated using the kappa statistic (Carletta, 1996). (Carletta 1996) is another method of comparing inter-annotator agreement. We measured inter-annotator agreement with the Kappa statistic (Carletta, 1996) using the 1,391 items that two annotators scored in common. To measure inter-annotator agreement, we compute Cohen's Kappa (Carletta, 1996) from the two sets of annotations. Obtained percent agreement of 0.988 and coefficient (Carletta, 1996) of 0.975 suggest high convergence of both annotations. P (A) is the observed agreement between annotators and P (E) is the probability of agreement due to chance (Carletta, 1996). Annotation was highly reliable with a kappa (Carletta, 1996) of 3. With the help of the kappa coefficient (Carletta, 1996) proposes to represent the dialog success independently from the task intrinsic complexity, thus opening the way to task generic comparative evaluation. To test the reliability of the annotation, we first considered the kappa statistic (Siegel and Castellan, 1988) which is used extensively in empirical studies of dis course (Carletta, 1996). Inter-annotator agreement was sufficient (κ = 0.77 on average (Carletta, 1996)). In addition to raw inter-tagger agreement, the kappa statistic, which removes from the agreement rate the amount of agreement that is expected by chance (Carletta, 1996), was also determined. Following the suggestions in (Carletta, 1996), Core et al. consider kappa scores above 0.67 to indicate significant agreement and scores above 0.8 reliable agreement. In order to test the reliability of our annotation, we double coded about 18% of the data, namely 21 sub-dialogues comprising 213 pronouns, on which we computed the Kappa coefficient (Carletta, 1996). As reported elsewhere the resulting Kappa statistics (Carletta, 1996) over the annotated data yields 0.7. Since co-reference is a clustering task, any general-purpose method for evaluating a response partition against a key partition (e.g., Kappa (Carletta, 1996)) can be used for coreference scoring (see Popescu-Belis et al (2004)). The reliability for the two annotation tasks (statistics (Carletta, 1996)) was of 0.94 and 0.90 respectively. From the first effort an inter-annotator agreement (Carletta, 1996) of 0.89 for Cohen's kappa was obtained.
For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia We report on work in progress on extractlexical simplifications (e.g., focusing on utilizing edit histories in Simple English Wikipedia for this task. We consider two main approaches: (1) deriving simplification probabilities via an edit model that accounts for a mixture of different operations, and (2) using metadata to focus on edits that are more likely to be simplification operations. We find our methods to outperform a reasonable baseline and yield many high-quality lexical simplifications not included in an independently-created manually prepared list. Yatskar et al (2010) learn lexical simplification rules from the edit histories of Wikipedia Simple articles. (Yatskar et al., 2010) use an unsupervised learning method and meta data from the Simple English Wikipedia. Simple English Wikipedia has been used before in simplicity analysis, as described in (Yatskar et al, 2010). The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al, 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al, 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al, 2006). More recently, Yatskar et al (2010) explore data-driven methods to learn lexical simplifications from Wikipedia revision histories. In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. Indeed, Yatskar et al (2010) learn lexical simplifications without taking syntactic context into account. Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edit changes identified in both Wikipedias revision histories, though they only provide a list of the top phrasal rules and do not utilize them in an end-to-end simplification system. In particular Yatskar et al (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs. Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzottoand Pennacchiotti, 2010). (Yatskar et al, 2010) focus on using edit histories in Simple English Wikipedia to extract lexical simplifications. The Wikipedia revision history has been used for spelling correction, text summarization (Nelken and Yamangil, 2008), lexical simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al, 2010). Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edits identified in the revision histories of both Simple English Wikipedia and English Wikipedia. Woodsend and Lapata (2011) and Yatskar et al (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification.
Improved Statistical Alignment Models In this paper, we present and compare various single-word based alignment models for statistical machine translation. We discuss the five IBM alignment models, the Hidden- Markov alignment model, smoothing techniques and various modifications. We present different methods to combine alignments. As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies. IBM Model 5 was sequentially bootstrapped with Model 1, an HMM Model, and Model 3 (Och and Ney, 2000). The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al, 2003). We use GIZA++ (Och and Ney,2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006) to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. The development of techniques in all these areas would be facilitated by automatic performance metrics, and alignment and translation quality metrics have been proposed (Och and Ney, 2000b; Papineni et al, 2002). The Alignment Error Rate (AER) introduced by Och and Ney (2000b) measures the fraction of links by which the automatic alignment differs from the reference alignment. The IBM-3 models were trained on a subset of the Canadian Hansards French-English data which consisted of 50,000 parallel sentences (Och and Ney, 2000b). The GIZA++ toolkit (Och and Ney, 2000a) was used for training the IBM-3 models (as in (Och and Ney, 2000b)). Our unseen test data consisted of 207 French English sentence pairs from the Hansards corpus (Och and Ney, 2000b). The performance of the four decoders was measured with respect to the alignments provided by human experts (Och and Ney, 2000b). Once the training data was preprocessed, a word-to-word alignment was performed in both directions, source-to-target and target-to-source, by using GIZA++ (Och and Ney, 2000). The approach was first presented by Brown et al (1993) and has since been used in many translation systems (Wang and Waibel, 1998), (Och and Ney, 2000), (Yamadaand Knight, 2000), (Vogel et al, 2003). In order to assess the quality of the word alignment, we randomly selected from the training corpus 350 sentences, and a manual gold standard alignment has been done with the criterion of Sure and Possible links, in order to compute Alignment Error Rate (AER) as described in (Och and Ney, 2000) and widely used in literature, together with appropriately redefined Recall and Precision measures. We use GIZA++ (Och and Ney, 2000) to generate the baseline alignment for each direction and then apply grow-diagonal-final (gdf). We adopted the same evaluation methodology as in (Och and Ney, 2000), which compared alignment outputs with manually aligned sentences. We trained our alignment program with the same 50K pairs of sentences as (Och and Ney, 2000) and tested it on the same 500 manually aligned sentences.  Table 2 compares the results of our algorithm with the results in (Och and Ney, 2000), where an HMM model is used to bootstrap IBM Model 4. This demonstrates that we are competitive with the methods described in (Och and Ney, 2000).  The first feature is the absolute difference between ai and ai-1 + 1 and is similar to information used in other HMM word alignment models (Och and Ney, 2000) as well as phrase translation models (Koehn, 2004).
An Empirically Based System For Processing Definite Descriptions We present an implemented system for processing definite descriptions in arbitrary domains. The design of the system is based on the results of a corpus analysis previously reported, which highlighted the prevalence of discourse-new descriptions in newspaper corpora. The annotated corpus was used to extensively evaluate the proposed techniques for matching definite descriptions with their antecedents, discourse segmentation, recognizing discourse-new descriptions, and suggesting anchors for bridging descriptions. Vieira and Poesio (2000), extending this class to include only and a few others, make use of them in identifying discourse-new definite descriptions. Vieira and Poesio (2000) describe heuristics for processing definite descriptions in news text. We make use of the classification of bridging references proposed by Vieira and Poesio (2000). Markert et al (2003) used the Web and the construction method to extract information about hyponymy used to resolve other-anaphora (achieving an f value of around 67%) as well as the BDs in the Vieira-Poesio dataset (their results for these cases were not better than those obtained by (Vieira and Poesio, 2000)). DDs and PNs in associative relations account for 27% of all NPs in the test data, which is almost double the number of bridging cases (associative plus coreferent cases where head nouns are not the same) reported for newspaper texts in Vieira and Poesio (2000). One common approach involves the design of heuristic rules to identify specific types of (non) anaphoric NPs such as pleonastic pronouns (e.g., Paice and Husk (1987), Lappin and Leass (1994), Kennedy and Boguraev (1996), Denber (1998)) and definite descriptions (e.g., Vieira and Poesio (2000)). While the resolution of pronominal anaphora and tracking of named entities is possible with good accuracy, the resolution of definite NPs (having a common noun as their head) is usually limited to the cases that Vieira and Poesio (2000) call direct coreference, where both coreferent mentions have the same head. Vieira and Poesio (2000) proposed an algorithm for definite description (DD) resolution that incorporates a number of heuristics for detecting discourse-new descriptions. Vieira and Poesio (2000) proposed an algorithm for definite description resolution that incorporates a number of heuristics for detecting discourse-new (henceforth: DN) descriptions. (Vieira and Poesio, 2000) is run, which attempts to find an head-matching antecedent within a given window and taking premodification into account. The baseline algorithm without DN detection incorporated in GUITAR described above (i.e., only the direct anaphora resolution part of (Vieira and Poesio, 2000)). A major obstacle in the resolution of definite noun phrases with full lexical heads is that only a small proportion of them is actually anaphoric (ca. 30% (Vieira and Poesio, 2000)). However, R. Vieira and M. Poesio have recently shown in (Vieira and Poesio, 2000) that such an exhaustive search is not needed, because many noun phrases are not anaphoric at all - about 24365 of definite NPs in their corpus have no prior referents. In the system by Vieira and Poesio (2000), for example, WordNet is consulted to obtain the synonymy, hypernymy and meronymy relations for resolving the definite anaphora. We note that around 80% of the definite NPs are anaphoric in our corpus, instead of the 50% presented in (Vieira and Poesio, 2000) for news paper texts. The Vieira/Poesio algorithm (Vieira and Poesio, 2000) attempts to classify each definite description as either direct anaphora, discourse-new, or bridging description. Uryupina (2003) and Vieira and Poesio (2000) also take capital and low case letters into account. A more fine-grained distinction is made by Bean and Riloff (1999) and Vieira and Poesio (2000) to distinguish restrictive from non-restrictive post modification by ommitting those modifiers that occur between commas, which should not be classified as chain starting. The precision and recall results obtained by these classifiers - tested on MUC corpora - are around the eighties, and around the seventies in the case of Vieira and Poesio (2000), who use the Penn Treebank. For the discourse-new classification task, the model's most important feature is whether the head word of the NP to be classified has occurred previously (as in Ng and Cardie (2002) and Vieira and Poesio (2000)).
  and (2) the relation between predicates, question stem and the words that determine the answer type (Narayanan and Harabagiu, 2004). Ever since Gildea and Jurafsky (2002), SRL has become an important technology used in applications requiring semantic interpretation, ranging from information extraction (Frank et al, 2007) and question answering (Narayanan and Harabagiu, 2004), to practical problems including textual entailment (Burchardt et al, 2007) and pictorial communication systems (Goldberg et al, 2008). In particular, the well-defined semantic role labeling (SRL) task has been drawing increasing attention in recent years due to its importance in natural language processing (NLP) applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al, 2003), and co-reference resolution (Kong et al, 2009). Systems for addressing complex information needs are interesting because they provide an opportunity to explore the role of semantic structures in question answering ,e.g., (Narayanan and Harabagiu, 2004). For instance, information extraction (Surdeanu et al, 2003), question answering (Narayanan and Harabagiu, 2004) and machine translation (Boas, 2002) could stand to benefit from broad coverage semantic processing. In particular, the well-defined semantic role labeling (SRL) task has been drawing more and more attention in recent years due to its importance in deep NLP applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al, 2003), and co-reference resolution (Ponzetto and Strube, 2006). Lexical semantic features are known to be helpful in both deep (Tetreault, 2005) and shallow interpretation tasks (Narayanan and Harabagiu,2004). Recently, predicate argument structure analysis has attracted the attention of researchers because this information can increase the precision of text processing tasks, such as machine translation ,information extraction (Hirschman et al, 1999), question answering (Narayanan and Harabagiu, 2004) (Shen and Lapata, 2007), and summarization (Melli et al., 2005). Semantic types and role labelling are helpful in both deep (Tetreault, 2005) and shallow interpretation tasks (Narayanan and Harabagiu, 2004). One such question answering system (Narayanan and Harabagiu, 2004) takes PropBank/FrameNet annotations as input, uses the PropBank targets to indicate which actions are being described with which arguments and produces an answer using probabilistic models of actions as the tools of inference. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al 2003), and Machine Translation (Boas 2002). Harabagiu, 2004) computed automatically from collections of documents relevant to a scenario in order to approximate the semantic content of a scenario, (Narayanan and Harabagiu, 2004) employed formal models of the interrelated events, actions, states, and relations implicit to a scenario in order to produce fine-grained, context sensitive inferences that could be used to answer questions. Examples include information extraction (Surdeanu et al, 2003), question answering (Narayanan and Harabagiu, 2004), machine translation (Boas, 2005), and summarization (Melli et al, 2005). Much progress in the area of semantic role labeling is due to the creation of resources like FrameNet (Fillmore et al, 2003), which document the surface realization of semantic roles in real world corpora. It was shown that the identification of such event frames has a significant contribution for many Natural Language Processing (NLP) applications such as Information Extraction (Surdeanu et al, 2003) and Question Answering (Narayanan and Harabagiu, 2004). Thus, Narayanan and Harabagiu (2004) apply the argument-predicate relationship from PropBank (Palmer et al, 2005) together with the semantic frames from FrameNet (Baker et al, 1998) to create an inference mechanism to improve QA. Thus, Narayanan and Harabagiu (2004) apply the argument-predicate relationship from PropBank (Palmer et al, 2005) together with the semantic frames from FrameNet (Baker et al, 1998) to create an inference mechanism to improve QA. The benefit of semantic roles has already been demonstrated for a number of tasks, among others for machine translation (Boas, 2002), information extraction (Surdeanu et al, 2003), and question answering (Narayanan and Harabagiu, 2004). Robust and accurate automatic semantic role assignment, a prerequisite for the wide-range use of semantic roles in NLP, has been investigated in a number of studies and shared tasks. Narayanan and Harabagiu (2004) were the first to stress the importance of semantic roles in answering complex questions.
Exploiting Syntactic Structure for Language Modeling The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies. The model assigns probability to every joint sequence of words-binary-parse-structure with headword annotation and operates in a left-to-right manner — therefore usable for automatic speech recognition. The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved. But (Chelba and Jelinek, 1998) chooses the lexical heads of the two previous constituents as determined by a shift-reduce parser, and works better than a trigram model. This marraige of models has been tested in other fields such as speech recognition (Chelba and Jelinek, 1998) with success. (Chelba and Jelinek, 1998) combine unlabeled and labeled data for parsing with a view towards language modeling applications. Aware that the n-gram obscures many linguistically-signi cant distinctions (Chomsky, 1956, section 2.3), many speech researchers (Jelinek and Laerty, 1991) sought to incorporate hierarchical phrase structure into language modeling (see (Stolcke, 1997)) although it was not until the late 1990s that such models were able to signi cantly improve on 3-grams (Chelba and Jelinek, 1998). The language model described by Chelba and Jelinek (1998) similarly conditions on linguistically relevant words by assigning partial phrase structure to the history and percolating headwords. In this situation, a structured language model (SLM) was proposed by Chelba and Jelinek (1998).  Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. As an alternative option to our verb-modifier experiments, structured language models (Chelba and Jelinek, 1998) might be considered to improve clause coherence, until full-featured syntax-based MT models (Yamada and Knight (2002), Eisner (2003), Chiang (2005) among many others) aretested when translating to morphologically rich languages. In the first type, probability of a word is decided based on a parse-tree information like grammatical headwords in a sentence (Charniak, 2001) (Chelba and Jelinek, 1998), or based on part of-speech (POS) tag information (Galescu and Ringger, 1999).
Unsupervised Learning of Narrative Event Chains used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring deep semantic knowledge. We propose unsupervised of similar schemata called chains raw newswire text. A narrative event chain is a partially ordered set of events related by a common protagonist. We describe a three step process to learning narrative event chains. The first uses unsupervised distributional methods to learn narrative relations between events sharing coreferring arguments. The second applies a temporal classifier to partially order the connected events. Finally, the third prunes and clusters self-contained chains from the space of events. introduce two evaluations: the evaluate event relatedness, and an orcoherence to evaluate narrative order. show a over baseline narrative prediction and temporal coherence. tate learning, and thus this paper addresses the three of chain induction: event ordering of events selection (pruning the event space into discrete sets). Learning these prototypical schematic sequences of events is important for rich understanding of text. Scripts were central to natural language understanding research in the 1970s and 1980s for proposed tasks such as summarization, coreference resolution and question answering. For example, Schank and Abelson (1977) proposed that understanding text about restaurants required knowledge about the Restaurant Script, including the participants (Customer, Waiter, Cook, Tables, etc. ), the events constituting the script (entering, sitting down, asking for menus, etc. ), and the various preconditions, ordering, and results of each of the constituent actions. Consider these two distinct narrative chains. Chambers and Jurafsky (2008) extracted narrative event chains based on common protagonists. These knowledge structures, comparable to scripts (Schank and Abelson, 1977) or narrative chains (Chambers and Jurafsky, 2008), describe typical sequences of events in a particular context. Given the number of potential scripts, their development by hand becomes a resource intensive process. In the past, some work has been devoted to automatically construct script-like structures from compiled corpora (Fujiki et al, 2003) (Chambers and Jurafsky, 2008). (Chambers and Jurafsky, 2008) attempt to identify narrative chains in newspaper corpora. This virtue of discourse structure of coherent stories has been described in (Trabasso et al, 1984) and applied by (Fujiki et al, 2003) as subject and object overlap and by (Chambers and Jurafsky, 2008) as following a common protagonist in a story. We utilize the definition of PMI described in (Chambers and Jurafsky, 2008). The second metric M2 utilizes point wise mutual information as defined in (Chambers and Jurafsky, 2008). Features 2 and 5 are inspired by the work of Chambers and Jurafsky (2008), who investigated unsupervised learning of narrative event sequences using point wise mutual information (PMI) between syntactic positions. We could have obtained a more accurate ordering using a temporal classifier (see Chambers and Jurafsky 2008), however we leave this to future work. Chambers and Jurafsky (2008) define their event ranking function based on point wise mutual information. We follow the approach of Chambers and Jurafsky (2008), evaluating our models for predicting script events in a narrative cloze task. In particular, it outperforms the state-of-the-art point wise mutual information method introduced by Chambers and Jurafsky (2008), and it does soby a large margin, more than doubling the Recall@ 50 on the Reuters corpus. Some exceptions include recent work on learning common event sequences in news stories (Chambers and Jurafsky, 2008), an approach based on statistical methods, and the development of an event calculus for characterizing stories written by children (Halpin et al, 2004), a knowledge-based strategy. Here narrative event chains were defined by Chambers and Jurafsky (2008) as partially ordered sets of events involving the same protagonist. Fabulas can be viewed as distributions over characters, events and other entities; this conceptualization of what constitutes a narrative is broader than Chambers and Jurafsky (2008). Chambers and Jurafsky (2008) suggested inducing a similar structure called a narrative chain: focus on the situational descriptions explicitly pertaining to a single protagonist, a series of references within a document that are automatically labeled as co referent. Setup Following Chambers and Jurafsky (2008), we extracted and lemmatized the verbs from the New York Times section of the Gigaword Corpus using the Stanford POS tagger (Toutanova et al, 2004) and the Morphalemmatizer (Minnen et al, 2000). Triples of verb tokens were sampled at random from the narrative cloze test set of Chambers and Jurafsky (2008). The primary research effort in event temporality has gone into ordering events with respect to one another (e.g., Chambers and Jurafsky (2008)), and detecting their typical durations (e.g., Pan et al (2006)). Utilizing verb co-occurrence at the document level, Chambers and Jurafsky (2008) estimate whether a pair of verbs is narratively related by counting the number of times the verbs share an argument in the same document. Narrative score Chambers and Jurafsky (2008) suggested a method for learning sequences of actions or events (expressed by verbs) in which a single entity is involved.
Phrase Clustering for Discriminative Learning We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers. To demonstrate the power and generality of this approach, we apply the method in two very different applications: named entity recognition and query classification. Our results show that phrase clusters offer significant improvements over word clusters. Our NER system achieves the best current result on the widely used CoNLL benchmark. Our query classifier is on par with the best system in KDDCUP 2005 without resorting to labor intensive knowledge engineering efforts. We tried two kinds of indicator features: Bucket features: For both parent and child vectors in a potential dependency, we fire one indicator feature per dimension of each embedding. A similar effect, when changing distributional context window sizes, was found by Lin and Wu (2009). Lin and Wu (2009) further explored a two-stage cluster-based approach: first clustering phrases and then relying on a supervised learner to identify useful clusters and assign proper weights to cluster features. Our phrase pair clustering approach is similar inspirit to the work of Lin and Wu (2009), who use K means to cluster (monolingual) phrases and use the resulting clusters as features in discriminative classifiers for a named-entity-recognition and a query classification task. Another distinction is that Lin and Wu (2009) work with phrase types instead of phrase instances, obtaining a phrase type's contexts by averaging the contexts of all its phrase instances.  Our clustering approach is related to Lin and Wu's work (Lin and Wu, 2009). We check the match of not just the top 1 cluster ids, but also farther down in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word. Lin and Wu (2009) report an F1 score of 90.90 on the original split of the CoNLL data. Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa.  We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and for NER Lin and Wu (2009). Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. K-Means clustering algorithm described in Lin and Wu (2009). Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional context of that word. We follow Lin and Wu (2009) in applying various thresholds during K-Means, such as a frequency threshold for the initial vocabulary, a total count threshold for the feature vectors, and a threshold for PMI scores. In addition to the features described in Lin and Wu (2009), we introduce features from a bilingual parallel corpus that encode reverse-translation information from the source-language (Spanish or Japanese in our experiments). Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER. 
A Property-Sharing Constraint In Centering A constraint is proposed in the Centering approach to pronoun resolution in discourse. This &quot;property-sharing&quot; constraint requires that two pronominal expressions that retain the same Cb across adjacent utterances share a certain common grammatical property. This property is expressed along the dimension of the grammatical function SUBJECT for both Japanese and English discourses, where different pronominal forms are primarily used to realize the Cb. It is the zero pronominal in Japanese, and the (unstressed) overt pronoun in English. The resulting constraint complements the original Centering, accounting for its apparent violations and providing a solution to the interpretation of multi-pronominal utterances. It also provides an alternative account of anaphora interpretation that appears to be due to structural parallelism. This reconciliation of centering/focusing and parallelism is a major advantage. I will then add another dimension called the &quot;speaker identification&quot; to the constraint to handle a group of special cases in Japanese discourse. It indicates a close association between centering and the speaker's viewpoint, and sheds light on what underlies the effect of perception reports on pronoun resolution in general. These results, by drawing on facts in two very different languages, demonstrate the cross-linguistic applicability of the centering framework. this Centers are semantic objects--(sets of) individuals, objects, states, actions, or events--represented in complex ways so that a strict coreference need not hold between related A center mentioned in the current utterance may be mentioned again in the next utterance (by the same or a different speaker). In this sense, a center is &quot;forward-looking&quot; (Cf). Crucially, one of the centers may be identified as &quot;backward-looking&quot; (Cb). Cb the entity an utterance centrally Its main role is to connect the current utterance to the preceding The term Center is used for the Cb. Thus an utterance may be associated with any number of Cfs, one of which may be the Cb. These Cfs are given a default Cb order, that &quot;how much each center is expected to be the next Cb&quot;. I regard Cb to be optional for It comes into exsistence by way of a that is, the process in which a previous non-Cb becomes the new Cb in discourse. (1981, 1983) focus foci local focusing correspond to Cb and Cfs, respectively. The difference is that Sidner uses two immediate foci (Discourse Focus and Actor Focus) while centering uses only one (Cb) (see Grosz et. al. 1983 for discussion). Various factors --syntactic, semantic, and pragmatic-are combined for the identification of the Cb. One of them the of pronominal expressions, expressed in the This is the same sort of preference that is addressed by property-sharing constraint (Kameyama, 1986). Kameyama (1986) proposed a fourth transition type, Center Establishment (EST), for utterances E.g., in Bruno was the bully of the neighborhood. In this paper, we use centering theory (Kameyama, 1986) to determine how easily a noun phrase can be referred to in the following context. Kameyama (1986) emphasized the importance of a property-sharing constraint.  There are two versions of the centering theory that have been applied to Japanese zero pronoun resolution: Kameyama's (Kameyama, 1986) and Walker's (Walker et al, 1994).
Multi-Paragraph Segmentation Of Expository Text This paper describes TextTiling, an algorithm for partitioning expository texts into coherent multi-paragraph discourse units which reflect the subtopic structure of the texts. The algorithm uses domain-independent lexical frequency and distribution information to recognize the interactions of multiple simultaneous themes. Two fully-implemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts. Our salience factors mirror those used by (Lappin and Leass, 1994), with the exception of Poss-s, discussed below, and CNTX-S, which is sensitive to the context in which a discourse referent appears, where a context is a topically coherent segment of text, as determined by a text-segmentation algorithm which follows (Hearst, 1994). TextTiling (TT) (Hearst, 1994) relies on the simplest coherence relation word repetition and computes similarities between textual units based on the similarities of word space vectors. Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). Messages are partitioned into multi-paragraph segments using TextTiling, which reportedly has an overall precision of 83% and recall of 78% (Hearst, 1994). This part of the Discourse Analysis field has received a constant interest since the initial work in this domain such as (Hearst, 1994). As TextTiling, the topic segmentation method of Hearst (Hearst, 1994), the topic segmenter we propose, called F06, first evaluates the lexical cohesion of texts and then finds their topic shifts by identifying breaks in this cohesion. Texts without adequate paragraph marking could be segmented using tools such as TextTiling (Hearst, 1994). The first task can be addressed by using the hierarchical structure readily available in the text (e.g., chapters, sections and subsections) or by employing existing topic segmentation algorithms (Hearst, 1994). This division can either be automatically computed using one of the many available text segmentation algorithms (Hearst, 1994), or it can be based on demarcations already present in the input (e.g., paragraph markers). For example, the TextTiling algorithm, introduced by (Hearst, 1994), assumes that the local minima of the word similarity curve are the points of low lexical cohesion and thus the natural boundary candidates. Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). Discourse segmentation of the documents composed of parallel parts is a novel and challenging problem, as previous research has mostly focused on the linear segmentation of isolated texts (e.g., (Hearst, 1994)). Fordocuments where hierarchical information is not explicitly provided, such as automatic speech transcripts, we can use automatic segmentation methods to induce such a structure (Hearst, 1994). In this study we apply the methods of Foltz et al (1998), Hearst (1994, 1997), and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue. Both Hearst (1994, 1997) and Foltz et al (1998) use vector space methods discussed below to represent and compare units of text. However, Hearst (1994, 1997) and Foltz et al (1998) differ on how text units are defined and on how to interpret the results of a comparison. The text unit's definition in Hearst (1994, 1997) and Foltz et al (1998) is generally task dependent, depending on what size gives the best results. Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens (Hearst, 1994), but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size. Hearst (1994, 1997) in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores. Hearst (1994, 1997) was replicated using the JTextTile (Choi, 1999) Java soft ware.
Discovering Relations Among Named Entities From Large Corpora Discovering the significant relations embedded in documents would be very useful not only for information retrieval but also for question answering and summarization. Prior methods for relation discovery, however, needed large annotated corpora which cost a great deal of time and effort. We propose an unsupervised method for relation discovery from large corpora. The key idea is clustering pairs of named entities according to the similarity of context words intervening between the named entities. Our experiments using one year of newspapers reveals not only that the relations among named entities could be detected with high recall and precision, but also that appropriate labels could be automatically provided for the relations. Hasegawa, et al put forward an unsupervised approach for relation extraction from large text corpora (Hasegawa et al, 2004). For Hasegawa's method (Hasegawa et al, 2004), we set the cluster number to be identical with the number of ground truth classes.   One approach for Open IE is based on clustering of entity pairs to produce relations, as introduced by Hasegawa et al (Hasegawa et al, 2004). Fully unsupervised Open IE systems are mainly based on clustering of entity pair contexts to produce clusters of entity pairs that share the same relations, as introduced by Hasegawa et al (Hasegawa et al, 2004).   Hasegawa et al (2004) performs unsupervised hierarchical clustering over a simple set of features. Unfortunately, this number is often unavailable in in formation extraction tasks in general (Hasegawa et al, 2004), and attribute extraction in particular. (Hasegawa et al 2004) used large corpora and an Extended Named Entity tagger to find novel relations and their participants. Some existing studies use corpus-based statistics for relation extraction (Hasegawa et al, 2004). Hasegawa et al (2004) described a paraphrase discovery approach based on clustering concurrent name pairs. Compared with supervised and semi-supervised methods, Hasegawa et al (2004)'s unsupervised approach for relation extraction can overcome the difficulties on requirement of a large amount of labeled data and enumeration of all class labels. Hasegawa et al (2004)'s method is to use a hierarchical clustering method to cluster pairs of named entities according to the similarity of context words intervening between the named entities. It also does not need to pre-define the number of the context clusters or pre-specify the similarity threshold for the clusters as Hasegawa et al (2004)'s method.  In (Hasegawa et al, 2004), they preformed unsupervised relation extraction based on hierarchical clustering and they only used word features between entity mention pairs to construct context vectors. We reported the clustering results using the same clustering strategy as Hasegawa et al (2004) proposed. In Table 5, Hasegawa's Method1 means the test used the word feature as Hasegawa et al (2004) while Hasegawa's Method2 means the test used the same feature set as our method.
The First International Chinese Word Segmentation Bakeoff This paper presents the results from the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan. We give the motivation for having an international segmentation contest (given that there have been two within-China contests to date) and we report on the results of this first international contest, analyze these results, and make some recommendations for the future. For details on the word segmentation bakeoff, see (Sproat and Emerson, 2003). We refer readers to (Sproat and Emerson, 2003) for details on the evaluation measures. We conduct experiments on the SIGHAN 2003 (Sproat and Emerson, 2003) and 2005 (Emerson, 2005) bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm. After analyzing the results presented in the first and second Bakeoffs, (Sproat and Emerson,2003) and (Emerson, 2005), we created a new Chinese word segmentation system named as? Achilles? that consists of four modules mainly: Regular expression extractor, dictionary-based Ngramsegmentation, CRF-based sub word tagging (Zhang et al, 2006), and confidence-based segmentation. In the last SIGHAN bakeoff, there is no single system consistently outperforms the others on different test standards of Chinese WS and NER standards (Sproat and Emerson, 2003). The official scorer program is publicly available and described in (Sproat and Emerson, 2003). Measuring homogeneity by counting word/ lexeme frequencies introduces additional difficulties as it assumes that the word is an obvious, well-defined unit, which is not the case in the Chinese (Sproat and Emerson 2003) or Japanese language (Matsumoto et al, 2002), for instance, where word segmentation is not trivial. The out-of-vocabulary (OOV) is defined as tokens in the test set that are not in the training set (Sproat and Emerson, 2003). Following (Sproat and Emerson, 2003), we also measured the recall onOOV (ROOV) tokens and in-vocabulary (RIV) tokens. In 2003 SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics (ACL) conducted the first International ChineseWord Segmentation Bakeoff (Sproat and Emerson, 2003). A recent Chinese word segmentation competition (Sproat and Emerson, 2003) has made comparisons easier.  This is due to significant inconsistent segmentation in training and testing (Sproat and Emerson, 2003). No other material was allowed (Sproat and Emerson, 2003). In order to show the impact to the evaluation result caused by EIs existing in test data of Bakeoff, we conduct the baseline close test with PK and AS corpora, i.e. we compile lexicons only containing words in their training data and then use the lexicons with a forward maximum matching algorithm to segment their test data respectively (Sproat and Emerson, 2003). For instance, 'vice president' is considered to be one word in the Penn Chinese Treebank (Xue et al, 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). We use three Chinese word-segmented corpora, the Academia Sinica corpus (AS), the Hong Kong City University corpus (HK) and the Beijing University corpus (PK), all of which were used in the First International Chinese Word Segmentation Bake off (Sproat and Emerson, 2003) at ACL-SIGHAN 2003. The top three systems participated in the SIGHAN Bakeoff (Sproat and Emerson, 2003). In Chinese text processing context, lexicons are particularly important for dictionary-based word segmentation techniques in which out-of-vocabulary words are an important cause of errors (Sproat and Emerson, 2003). 
Joint Extraction Of Entities And Relations For Opinion Recognition We present an approach for the joint extraction of entities and relations in the con text of opinion recognition and analysis. We identify two types of opinion-related entities ? expressions of opinions andsources of opinions ? along with the linking relation that exists between them. In spired by Roth and Yih (2004), we employ an integer linear programming approach to solve the joint opinion recognition task,and show that global, constraint-based inference can significantly boost the perfor mance of both relation extraction and theextraction of opinion-related entities. Performance further improves when a seman tic role labeling system is incorporated. The resulting system achieves F-measuresof 79 and 69 for entity and relation extrac tion, respectively, improving substantially over prior results in the area. One exception is Choi et al (2006), which proposed an ILP approach to jointly identify opinion holders, opinion expressions and their IS-FROM linking relations, and demonstrated the effectiveness of joint inference. Most similar to our method is Choi et al (2006), which jointly extracts opinion expressions, holders and their IS-FROM relations using an ILP approach. Similar to the preprocessing approach in (Choi et al,2006), we filter pairs of opinion and argument candidates that do not overlap with any gold standard relation in our training data. This makes our ILP formulation advantageous over the ILP formulation proposed in Choi et al (2006), which needs m binary decisions for a candidate span, where m is the number of types of opinion entities, and the score for each possible label assignment is obtained by the sum of raw scores from m independent extraction models. We adopted the evaluation metrics for entity and relation extraction from Choi et al (2006), which include precision, recall, and F1-measure according to overlap and exact matching metrics. It can be viewed as an extension to the ILP approach in Choi et al (2006) that includes opinion targets and uses simpler ILP formulation with only one parameter and fewer binary variables and constraints to represent entity label assignments. For example, our model failed to identify the IS-ABOUT relation (offers, general aid) from the following sentence Powellhad contacted ... and received offers of [gen formulation in Choi et al (2006) on extracting opinion holders, opinion expressions and IS-FROM relations, and showed that the proposed ILP formulation performs better on all three extraction tasks. Opinion Finder (Wilson et al, 2005a) (Version1.4): We used the +/labels assigned by its contextual polarity classifier (Wilson et al, 2005b) to create +/states and the MPQASD tags produced by its Direct Subjective and Speech Event Identifier (Choi et al, 2006) to produce mental (M) states. However since the contextual information in a domain is specific, the model got by their approach cannot easily converted to other domains. Choi et al (2006) used an integer linear programming approach to jointly extract entities and relations in the context of opinion oriented information extraction. Choi et al (2005) and Choi et al (2006) explore conditional random fields, Wieg and and Klakow (2010) examine different combinations of convolution kernels, while Johansson and Moschitti (2010) present a re-ranking approach modeling complex relations between multiple opinions in a sentence. Similarly, Choi et al (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for the same task. Choi et al (2006) is an extension of Choi et al (2005) in that opinion holder extraction is learnt jointly with opinion detection. The first part of each pipeline extracts opinion expressions, and this is followed by a multiclass classifier assigning a polarity to a given opinion expression, similar to that described by Wilson et al (2009). The first of the two baselines extracts opinion expressions using a sequence labeler similar to that by Breck et al (2007) and Choi et al (2006). Choi et al (2006) address the task of extracting opinion entities and their relations, and incorporate syntactic features to their relation extraction model. For the task of subjective expression detection, Choi et al (2006) and Breck et al (2007) used syntactic features in a sequence model. Similarly, Choi et al (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction. Others extend the token-level approach to jointly identify opinion holders (Choi et al 2006), and to determine the polarity and intensity of the opinion expressions (Choi and Cardie, 2010).
Learning For Semantic Parsing With Statistical Machine Translation We present a novel statistical approach to parsing, for constructing a complete, formal meaning representation of a sentence. A semantic parser is learned given a set of sentences annotated with their correct meaning represen- The main innovation of is its use of state-of-the-art statistical machine translation techniques. A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model. show that favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order. We compared our system's performance with the following existing systems: the string and tree versions of SILT (Kate et al, 2005), a system that learns transformation rules relating NL phrases to MRL expressions; WASP (Wong and Mooney, 2006), a system that learns transformation rules using statistical machine translation techniques; SCISSOR (Ge and Mooney, 2005), a system that learns an integrated syntactic-semantic parser; and CHILL (Tang and Mooney, 2001) an ILP-based semantic parser. We use a maximum-entropy model similar to that of Zettlemoyer and Collins (2005) and Wong and Mooney (2006). Following Wong and Mooney (2006), only candidate predicates and composition rules that are used in the best semantic derivations for the training set are retained for testing. WASP (Wong and Mooney, 2006) is a system motivated by statistical machine translation techniques. To make our system directly comparable to previous systems, all our experiments were based on identical training and test data splits of both corpora as reported in the experiments of Wong and Mooney (2006). Executable system actions include access to databases such as the GEOQUERY database on U.S. geography (Wong and Mooney (2006), inter alia), the ATIS travel planning database (Zettlemoyer and Collins (2009), inter alia), robotic control in simulated navigation tasks (Chen and Mooney (2011), interalia), databases of simulated card games (Goldwasser and Roth (2013), interalia), or the user-generated contents of FREEBASE (Cai and Yates (2013), inter alia).  In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL). For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the GEOQUERY functional query language in Wong and Mooney (2006)). Our work is based on the WASP semantic parsing algorithm (Wong and Mooney, 2006), which translates NL sentences into MRs using an SCFG. While WASP works well for target MRLs that are free of logical variables such as CLANG (Wong and Mooney, 2006), it cannot easily handle various kinds of logical forms used in computational semantics, such as predicate logic. We use the maximum-entropy model proposed in Wong and Mooney (2006), which defines a conditional probability distribution over derivations given an observed NL sentence. For details regarding non-isomorphic NL/MR parse trees, removal of bad links from alignments, and extraction of word gaps (e.g. the token (1) in the last rule of Figure 3), see Wong and Mooney (2006). The larger GEOQUERY corpus consists of 880 English questions gathered from various sources (Wong and Mooney, 2006). We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL. Like the hybrid tree semantic parser (Lu et al, 2008) and the synchronous grammar based WASP (Wong and Mooney, 2006), our model simultaneously generates the input MR tree and the output NL string. WASP (Wong and Mooney, 2006) is an example of the former perspective, coupling the generation of the MR and NL with a synchronous grammar, a formalism closely related to tree transducers. We evaluate the system on GeoQuery (Wong and Mooney, 2006), a parallel corpus of 880 English questions and database queries about United States geography, 250 of which were translated into Spanish, Japanese, and Turkish. WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system. A recent SMT-based semantic parser, WASP (Wong and Mooney, 2006), in order to produce a more effective generation system.
Learning Subjective Nouns Using Extraction Pattern Bootstrapping We explore the idea of creating a subjectivity classifier that uses lists of subjective nouns learned by bootstrapping algorithms. The goal of our research is to develop a system that can distinguish subjective sentences from objective sentences. First, we use two bootstrapping algorithms that exploit extraction patterns to learn sets of subjective nouns. Then we train a Naive Bayes classifier using the subjective nouns, discourse features, and subjectivity clues identified in prior research. The bootstrapping algorithms learned over 1000 subjective nouns, and the subjectivity classifier performed well, achieving 77% recall with 81% precision. Others were derived from corpora, including subjective nouns learned from unannotated data using bootstrapping (Riloff et al, 2003). Subjectivity classification of small units of text, such as individual micro blog posts (Jiang et al, 2011) and sentences (Riloff et al, 2003), has been shown to benefit from additional context. Riloff et al (2003) learned lists of subjective nouns in English, seeding their method with 20 high-frequency, strongly subjective words. There are some Natural Language Processing (NLP) researches that demonstrate the benefit of hedge detection experimentally in several subjects, such as the ICD-9-CM coding of radiology reports and gene named Entity Extraction (Szarvas, 2008), question answering systems (Riloff et al, 2003), information extraction from biomedical texts (Medlock and Briscoe, 2007). However, as demonstrated by Pang et al (2002), Pang and Lee (2004), Hu and Liu (2004), and Riloff et al (2003), there are some nouns and verbs that are useful sentiment indicators as well. Riloff et al. proposed a bootstrapping approach for learning subjective nouns (Riloff et al, 2003). In a different work, Riloff et al (2003) use manually derived pattern templates to extract subjective nouns by bootstrapping. Riloff et al (2003) develop a method to determine whether a term has a Subjective or an Objective connotation, based on bootstrapping algorithms. To build our subjective spoken corpus (more than 2,000 texts), we used a parallel corpus of English Portuguese speeches and a tool to automatically classify sentences in English as objective or subjective (OpinionFinder (Riloff et al, 2003)). Extracting syntactic patterns contribute towards the affective orientation of a sentence (Riloff et al, 2003). Riloff et al, (2003) have conducted experiments that use Bag Of-Words (BoW) as features to generate a Naive Bayes subjectivity classifier for the MPQA corpus in English. Riloff et al (2003) extracted nouns and Riloff and Wiebe (2003) extracted patterns for subjective expressions using a bootstrapping process. More details are provided in (Riloff et al, 2003). Basilisk was originally designed for semantic class induction using lexico-syntactic patterns, but has also been used to learn subjective and objective nouns (Riloff et al, 2003). Riloff et al (2003) explore bootstrapping techniques to identify subjective nouns and subsequently classify subjective vs. objective sentences in newswire text. Riloff et al (2003) mined subjective nouns from unannotated texts with two bootstrapping algorithms that exploit lexico-syntactic extraction patterns and manually-selected subjective seeds. Riloff et al (Riloff et al, 2003) applied bootstrapping to recognise subjective noun keywords and classify sentences as subjective or objective in newswire texts. Riloff et al (2003) focused on the collection of subjective nouns. Most previous works used seeds selected based on a user's or domain expert's intuition (Curran et al, 2007), which may then have to meet a frequency criterion (Riloff et al, 2003). Patterns are extracted using AutoSlog (Riloff et al, 2003).
Syntactic Features For Evaluation Of Machine Translation Automatic evaluation of machine translabased on computing similarity between system output and human reference translations, has revolutionized the development of MT systems. We explore the use of syntactic information, including constituent labels and head-modifier dependencies, in computing similarity between output and reference. Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments. Inspired in the work by Liu and Gildea (2005), who introduced a series of metrics based on con stituent/dependency syntactic matching, we have designed three subgroups of syntactic similarity metrics. Owczarzak et al (2007a, b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syn tactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. Liu and Gildea (2005) measure the syntactic similarity between MT output and reference translation. Similarities are captured from different viewpoints: DP-HWC (i) -l This metric corresponds to the HWC metric presented by Liu and Gildea (2005). This metric corresponds to the STM metric presented by Liu and Gildea (2005). Our method follows and substantially extends the earlier work of Liu and Gildea (2005), who use syntactic features and unlabelled dependencies to evaluate MT quality, outperforming BLEU on segment-level correlation with human judgement. While Liu and Gildea (2005) calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency structures produced by a Lexical Functional Grammar (LFG) parser. These dependencies differ from those used by Liu and Gildea (2005), in that they are extracted according to the rules of the LFG grammar and they are labelled with a type of grammatical relation that connects the head and the modifier, such as subject, determiner, etc. Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). This finding has been previously reported, among others, in Liu and Gildea (2005). The use of dependencies in MT evaluation has not been extensively researched before (one exception here would be Liu and Gildea (2005)), and requires more research to improve it, but the method shows potential to become an accurate evaluation metric. These metrics are similar to the Syntac tic Tree Matching metric defined by Liu and Gildea (2005), in this case applied to DRSsinstead of constituent trees. We use three different kinds of metrics: DR-STM Semantic Tree Matching, a la Liu and Gildea (2005), but over DRS instead of over constituency trees. The usual practice to model the wellformedness of a sentence is to employ the n-gram language model or compute the syntactic structure similarity (Liu and Gildea 2005). coefficients reported in Albrecht and Hwa (2007) including smoothed BLEU (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), HWCM (Liu and Gildea 2005), and the metric proposed in Albrecht and Hwa (2007) using the full feature set. Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but can not form coherent sentences (Liu and Gildea, 2005). For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees. This phenomenon has been previously observed by Liu and Gildea (2005). This direction was first explored by (Liu and Gildea, 2005), who used syntactic structure and dependency information to go beyond the surface level matching. With the addition of partial matching and n-best parses, Owczarzak et al (2007)'s method considerably outperforms Liu and Gildea's (2005 )w.r.t. correlation with human judgement.
Word Sense Disambiguation using Conceptual Density Eneko Agirre* Lengoaia eta Sistema Informatikoak saila.      In the second column, recall and precision are relative to polysemous verbs only, and in spite of an obvious decrease compare well with related work carried out with different methods (see, for instance, [Agirre and Rigau, 1996]), and are in fact very promising if one considers the comparatively small size of EB, and that only part of its attested words are semantically disambiguated. On the contrary, the Conceptual Density (CD) (Agirre and Rigau, 1996) is a flexible semantic similarity which depends on the generalizations of word senses not referring to any fixed level of the hierarchy. For example, paradigmatic relations in WordNet have been used by many to determine similarity, including Li et al (1995) and Agirre and Rigau (1996). Conceptual Density (CD) was originally introduced by (Agirre and Rigau, 1996). Agirre and Rigau (Agirre and Rigau 1996) use a conceptual distance formula that was created to by sensitive to the length of the shortest path that connects the concepts involved, the depth of the hierarchy and the density of concepts in the hierarchy. Conceptual Density (Agirre and Rigau 1996) is the paradigmatic component chosen to discriminate semantically among potential noun corrections. The semantic classes of x and y are then inferred using conceptual density (Agirre and Rigau 1996), a Word Net-based measure applied to all instantiation of x and y in the corpus.  To overcome the problem of varying link distances, Agirre and Rigau (1996) propose a semantic similarity measure (referred to as& quot; conceptual density& quot;) which is sensitive to i) the length of the path, ii) the depth of the nodes in the hierarchy (deeper nodes are ranked closer) and iii) the density of nodes in the sub hierarchies (concepts involved in a denser sub hierarchy are ranked closer than those in a more sparse region). To compute similarity for nouns we adopt conceptual density (cd) (Agirre and Rigau, 1996), a semantic similarity model previously applied to word sense disambiguation tasks. In (Agirre and Rigau, 1996) a method called Conceptual Distance is proposed to reduce this problem, but the reported performance in disambiguation still do not reach 50%. A method not evaluated by (Patwardhan et al., 2003) and using another semantic relatedness measure (conceptual density) is (Agirre and Rigau, 1996).  This proposal has a partial similarity with the Conceptual Density (Agirre and Rigau, 1996) and DRelevant (Va ?zquez et al, 2004) to get the concepts from a hierarchy that they associate with the sentence. Another method which could be used for class labelling is given by the conceptual density algorithm ofAgirre and Rigau (1996), which those authors applied to word sense disambiguation. Note that the notion of density here is not to be confused with the conceptual density used by Agirre and Rigau (1996), which is essentially a semantic similarity measure by itself.
Advances In Domain Independent Linear Text Segmentation This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering. Choi (2000) used the rank of the cosine, rather than the cosine itself, to measure the similarity of sentences. The statistical model for the algorithm is described in Section 2, and the algorithm for obtaining the maximum-probability segmentation is described in Section 3. (Choi, 2000) Table 1 gives the corpus statistics. (Choi, 2000) Segmentation accuracy was measured by the probabilistic error metric proposed by Beeferman, et al (1999) Low indicates high accuedges in the minimum-cost path, then the resulting segmentation often contains very small segments consisting of only one or two sentences. in Table 3 are slightly different from those listed in Table 6 of Choi's paper (Choi, 2000). which are implemented in Java (Choi, 2000), due to the difference in programming languages. As dataset the Choi dataset (Choi, 2000) is used. In this paper, we selected for comparison three systems based merely on the lexical reiteration feature: TextTiling (Hearst, 1997), C99 (Choi, 2000) and TextSeg (Utiyama and Isahara, 2001). The C99 algorithm (Choi, 2000) makes a linear segmentation based on a divisive clustering strategy and the cosine similarity measure between any two minimal units. Choi (2000) designed an artificial dataset, built by concatenating short pieces of texts that have been extracted from the Brown corpus. C99 KA to denote the C99 algorithm (Choi, 2000) when the aver age number of boundaries in the reference data was provided to the algorithm. For example, most algorithms achieve high performance on synthetic collections, generated by concatenation of random text blocks (Choi, 2000). Therefore, we only use manual transcriptions of these lectures. Synthetic Corpus Also as part of our analysis, we used the synthetic corpus created byChoi (2000) which is commonly used in the evaluation of segmentation algorithms. We follow Choi (2000) and compute the mean segment length used in determining the parameter k on each reference text separately. We also plot the Receiver Operating Character is tic (ROC) curve to gauge performance at a finer level of discrimination (Swets, 1988). We test the system on a range of data sets, including the Physics and AI lectures and the synthetic corpus created by Choi (2000). Comparison with local dependency models We compare our system with the state-of-the-art similarity-based segmentation system developed by Choi (2000). We use the publicly available implementation of the system and optimize the system on a range of mask-sizes and different parameter settings described in (Choi, 2000) on a held out development set of three lectures. Existing methods for topic segmentation typically assume that fragments of text (e.g. sentences or sequences of words of a fixed length) with similar lexical distribution are about the same topic; the goal of these methods is to find the boundaries where the lexical distribution changes (e.g. Choi (2000), Malioutov and Barzilay (2006)). Nevertheless, we will compare our topic identification approach to a state-of-the-art topic segmentation algorithm (Choi, 2000) in the evaluation. Choi 2000 Choi's (2000) state-of-the-art approach to finding segment boundaries. We use the freely available C99 software described in Choi (2000), varying a parameter that allows us to control the average number of sentences per segment and reporting the best result on the test data.
Automatic Processing of Large Corpora fbr the Resolution of Anaphor  References Ido Dagan * Alon Itai Computer Science Department Technion, tIaifa, Israel dagan~techunix .b i tnet ,  i ta i~ cs.technion, ac.il Abstract Manual acquisition of semantic onstraints in broad domains is very expensive.  Dagan and Itai (Dagan and Itai, 1990) experimented with co-occurrence statistics that are similar to our lexical case frame expectations. Researchers since Dagan and Itai (1990) have variously argued for and against the utility of collocation statistics between nouns and parents for improving the performance of pronoun resolution. Dagan an dItai (1990) use the distribution of a pronoun's context to determine which candidate antecedents can fit the context. Dagan and Itai (1990), for example, developed a statistical approach for pronominal anaphora, but the information they used was simply the patterns obtained from the previous analysis of the text. Consider the example given in the work of Dagan and Itai (1990): (1) They know full well that companies held tax money aside for collection later on the basis that the government said it was going to collect it2. Dagan and Itai (1990 )proposed a heuristics-based approach to pronoun resolution. One of the earliest methods for using predicate-argument frequencies in pronoun resolution is that of Dagan and Itai (1990).
Automatic Identification Of Non-Compositional Multi-Word Expressions Using Latent Semantic Analysis Making use of latent semantic analysis, we explore the hypothesis that local linguistic context can serve to identify multi-word expressions that have noncompositional meanings. We propose that vector-similarity between distribution vectors associated with an MWE as a whole and those associated with its constitutent parts can serve as a good measure of the degree to which the MWE is compositional. We present experiments that show that low (cosine) similarity does, in fact, correlate with non-compositionality. They measure compositionality as a combination of two similarity values: firstly, similar to (Katz and Giesbrecht, 2006), the similarity (cosine similarity) between the context of a VNC and the contexts of its constituent words. Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and non literal usages of a given expression in the trainning data. The few token-based approaches include a study by Katz and Giesbrecht (2006), who devise a supervised method in which they compute the meaning vectors for the literal and non-literal usages of a given expression in the training data. The few token-based approaches include a study by Katz and Giesbrecht (2006), who devise a supervised method in which they compute the meaning vectors for the literal and non-literal us ages of a given expression in the training data. Supervised classifiers have been used be fore for this task, notably by Katz and Giesbrecht (2006). Note that our results are noticeably higher than those reported by Cook et al (2007), Fazly et al (To appear) and Katz and Giesbrecht (2006) for similar supervised classifiers. Katz and Giesbrecht (2006) used a supervised learning method to distinguish between compositional and non-compositional uses of an expression (in German text) by using contextual information in the form of Latent Semantic Analy sis (LSA) vectors. There are several studies relevant to detecting compositionality of noun-noun, verb-particle and light verb constructions and verb noun pairs (e.g. Katz and Giesbrecht (2006)). Katz and Giesbrecht (2006) compared the word vector of an idiom in context and that of the constituent words of the idiom using LSA in order to determine if the expression is idiomatic. The performance of the supervised one is obtained by the method of Katz and Giesbrecht (2006). Katz and Giesbrecht (2006) and Baldwin et al (2003) use Latent Semantic Analysis for this purpose. For example, the method used in (Katz and Giesbrecht, 2006) relies primarily on local co-occurrence lexicon to construct feature vectors for each target token. Among the earliest studies on token-based classification were the ones by Hashimoto et al (2006) on Japanese and Katz and Giesbrecht (2006) on German. Katz and Giesbrecht (2006) compute meaning vectors for literal and non-literal examples in the training set and then classify test instances based on the closeness of their meaning vectors to those of the training examples. Such techniques either do not use any information regarding the linguistic properties of MWEs (Birkeand Sarkar, 2006), or mainly focus on their non compositionality (Katz and Giesbrecht, 2006). In supervised approaches, such as that of Katz and Giesbrecht (2006), co-occurrence vectors for literal and idiomatic meanings are formed from manually annotated training data. However, this approach follows that of Katz and Giesbrecht (2006) in assuming that literal meanings are compositional. We also compare our unsupervised methods against the supervised method proposed by Katz and Giesbrecht (2006). Our results using 1NN, 72:4%, are comparable to those of Katz and Giesbrecht (2006) using this method on their German data (72%). L-NCF depends highly on the accuracy of the automatically acquired canonical forms, it is not surprising that these two methods perform 5This was also noted by Katz and Giesbrecht (2006) in their second experiment.
Discriminative Reordering Models For Statistical Machine Translation We present discriminative reordering models for phrase-based statistical machine translation. The models are trained using the maximum entropy principle. We use several types of features: based on words, based on word classes, based on the local context. We evaluate the overall performance of the reordering models as well as the contribution of the individual feature types on a word-aligned corpus. Additionally, we show improved translation performance using these reordering models compared to a state-of-the-art baseline system. Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006). Despite their high perplexities, reordered LMs yield some improvements when integrated to a PSMT baseline that already includes a discriminative phrase orientation model (Zens and Ney, 2006). Unlike previous discriminative local orientation models (Zens and Ney, 2006), our framework permits the definition of global features. Adopting the idea of predicting the orientation, (Zens and Ney, 2006) started exploiting the context and grammar which may relate to phrase reorderings. Figure 2: Classification results with respect to d. We used GIZA++ to produce alignments, enabling us to compare using a DPR model against a baseline lexicalized reordering model (Koehn et al., 2005) that uses MLE orientation prediction and a discriminative model (Zens and Ney, 2006) that utilizes an ME framework. Contrasting the direct use of the reordering probabilities used in (Zens and Ney, 2006), we utilize the probabilities to adjust the word distance-based reordering cost, where the reordering cost of a sentence is computed as P o (f, e). In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding. Zens and Ney (2006) and Xiong et al (2006) utilized contextual information to improve phrase reordering. In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006).  The first contrastive submission is a phrase-based system, enhanced with a triplet lexicon model and a discriminative word lexicon model (Mauser et al, 2009) both trained on in-domain news commentary data only as well as a sentence-level single-word lex icon model and a discriminative reordering model (Zens and Ney, 2006a). Following models were applied: n-gram posteriors (Zens and Ney, 2006b), sentence length model, a 6-gram LM and single word lexicon models in both normal and inverse direction. Moreover, we tested the impact of the discriminative reordering model (Zens and Ney, 2006a). The classifier can be trained with maximum likelihood like Moses lexicalized reordering (Koehn et al, 2007) and hierarchical lexical ized reordering model (Galley and Manning, 2008) or be trained under maximum entropy framework (Zens and Ney, 2006). Figure 3 is an illustration of (Zens and Ney, 2006). j is the source word position which is aligned to the last target word of the current phrase. j is the source word position which is aligned to the first target word position of the next phrase. (Zens and Ney, 2006) proposed a maximum entropy classifier to predict the orientation of the next phrase given the current phrase. Clustered word classes have also been used in a discriminate reordering model (Zens and Ney, 2006), and were shown to reduce the classification error rate. Word clusters have also been used for unsupervised and semi-supervised parsing. One way to approach reordering is by extending the translation model, either by adding extra models, such as lexicalized (Koehn et al, 2005) or discriminative (Zens and Ney, 2006) reordering models or by directly modelling reordering in hierarchical (Chiang, 2007) or syntactical translation models (Yamada and Knight, 2002). Using these Chinese grammatical relations, we improve a phrase orientation classifier (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies. To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier.
Predicting The Semantic Orientation Of Adjectives We identify and validate from a large corpus constraints from conjunctions on the positive or negative semantic orientation of the conjoined adjectives. A log-linear regression model uses these constraints to predict whether conjoined adjectives are of same or different orientations, achieving 82% accuracy in this task when each conjunction is considered independently. Combining the constraints across many adjectives, a clustering algorithm separates the adjectives into groups of different orientations, and finally, adjectives are labeled positive or negative. Evaluations on real data and simulation experiments indicate high levels of performance: classification precision is more than 90% for adjectives that occur in a modest number of conjunctions in the corpus. Hatzivassiloglou and McKeown (1997) presented a method for automatically assigning a + or - orientation label to adjectives known to have some semantic orientation. In this paper, we use the model labels assigned by hand by Hatzivassiloglou and McKeown, and tile labels automatically obtained by their method and reported in (Hatzivassiloglou and McKeown, 1997) with the following extension: An adjective that appears in k conjunctions will receive (possibly different) labels when analyzed together with all adjectives appearing in at least 2, 3 ..., k conjunctions. The features developed in lhis paper are not only good clues of subjectivity, they can be Mentilied automatically from corpora (see (Hatzivassiloglou and McKeown, 1997), and Section 3 in the present paper). We are using the observations about conjunctive constructions from (Hatzivassiloglou and McKeown, 1997) in our approach. As explained by Hatzivassiloglou and McKeown (1997), two opinion terms appearing in a conjunctive constructions tend to have semantic orientations with the same or opposite directions, depending on the conjunction employed. (Hirschberg and Litman, 1994), and adjectives with positive or negative polarity (Hatzivassiloglou and McKeown, 1997). Hatzivassiloglou and McKeown (1997) determine the polarity of adjectives by mining pairs of conjoined adjectives from text, and observing that conjunctions such as and tend to conjoin adjectives of the same polarity while conjunctions such as but tend to conjoin adjectives of opposite polarity. Hatzivassiloglou and McKeown (1997) clustered adjectives into (+) and (-) sets based on conjunction constructions, weighted similarity graphs, minimum-cuts, supervised learning, and clustering. One of the earliest work on learning polarity of terms was by Hatzivassiloglou and McKeown (1997) who deduce polarity by exploiting constraints on conjoined adjectives in the Wall Street Journal corpus.  The work of Hatzivassiloglou and McKeown (1997) is among the earliest efforts that addressed this problem. We used the list of labeled seeds from (Hatzivassiloglouand McKeown, 1997) and (Stone et al, 1966).  In previous work, Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them in a large corpus. For example, Hatzivassiloglou (Hatzivassiloglou and McKeown, 1997) and Kanayama (Kanayama and Nasukawa, 2006) used conjunction rules to solve this problem from large domain corpora. For sentiment, the authors use a seed set of positive and negative adjectives, and iteratively propagate sentiment polarity through conjunction relations (like those used by Hatzivassiloglou and McKeown 1997, above). Our method for determining sentiment polarity is based on an adaptation of Hatzivassiloglou and McKeown (1997). In each case, OPINE makes use of local constraints on label assignments (e.g., conjunctions and disjunctions constraining the assignment of SO labels to words (Hatzivassiloglou and McKeown, 1997)). Hatzivassiloglou and McKeown (1997) proposed a method for identifying word polarity of adjectives. Following the method presented in (Hatzivassiloglou and McKeown, 1997), we can connect words if they appear in a conjunctive form in the corpus.
Chinese Word Segmentation As LMR Tagging PK F-score ❳ c the set by the MEMM tagger that scans the input from left to right and the last column is the results after the Transformation- Based Learner is applied. The results show that using Transformation-Based learning only give rise to slight improvements. It seems that the bidirectional approach does not help much for the LMR tagging. Therefore, we only submitted the results of our leftto-right MEMM tagger, retrained on the entire training sets, as our official results. F-score MEMM MEMM+TBL AS 0.9595 0.9603 HK 0.9143 N/A PK 0.9391 0.9398 Table 2: F-score on development data The results on the official test data is similar to we have got on our except that the F-score on the Beijing Univ. corpus is over 2 lower in absolute accuracy than what we expected. The reason is that in the training data of Beijing University corpus, all the numbers are encoded in GBK, while in the test data many numbers are encoded in ASCII, which are unknown to our tagger. With this problem fixed, the results of the official test data are compatible with the results on However, we have withdrawn our segmentation results on the Beijing University corpus. corpus R P F AS 0.961 0.958 0.959 0.729 0.966 HK 0.917 0.915 0.916 0.670 0.936 Table 3: Official Bakeoff Outcome 4 Conclusions and Future Work Our closed track experiments on the first Sighan Bakeoff data show that the LMR algorithm produces promising results. Our system ranks the second when tested on the Academia Sinica corpus and third on the Hong Kong City University corpus. In the future, we will try to incorporate a large word list into our tagger to test its performance on open track experiments. Its high accuracy on makes it a good candidate as a general purpose segmenter. References E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study part-of-speech tagging. 21(4):543–565. C. K. Fan and W. H. Tsai. 1988. Automatic word identification in chinese sentences by the relaxation tech- Processing of Chinese and Oriental 4(1):33–56. Kok-Wee Gan, Martha Palmer, and Kim-Teng Lua. 1996. A statistically emergent approach for language processing: Application to modeling context effects in chinese word boundary perception. Com- 22(4):531–53. J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for stgmenand labeling sequence data. In of G. Ngai and R. Florian. 2001. Transformation-based in the fast lane. In of NAACLpages 40–47. Adwait Ratnaparkhi. 1996. A maximum entropy part-oftagger. In of the Empirical Methin Natural Language Processing University of Pennsylvania. L. Shen and A. K. Joshi. 2003. A SNoW based supertagwith application to NP chunking. In R. Sproat, Chilin Shih, William Gale, and Nancy Chang. 1996. A stochastic finite-state word-segmentation for chinese. 22(3):377–404. H. van Halteren, J. Zavrel, and W. Daelmans. 1998. Improving data driven wordclass tagging by system com- In of COLING-ACL Andi Wu. 2003. Customizable segmentation of morderived words in chinese. and Chinese Language Nianwen Xue. 2003. Chinese word segmentation as tagging. Linguistics and The word segmenter we built is similar to the maximum entropy word segmenter of (Xue and Shen, 2003). The default feature, boundary tag feature of the previous character, and boundary tag feature of the character two before the current character used in (Xue and Shen, 2003) were dropped from our word segmenter, as they did not improve word segmentation accuracy in our experiments. We observed that character features were successfully used to build our word segmenter and that of (Xue and Shen, 2003). Our maximum entropy word segmenter is similar to that of (Xue and Shen, 2003), but the additional features we used and the post processing step gave improved word segmentation accuracy. Word segmentation can be formalized as a character classification problem (Xue and Shen, 2003), where each character in the sentence is given a boundary tag representing its position in a word. This kind of strategy has been widely used in the applications of machine learning to named entity recognition and has also been used in Chinese word segmentation (Xue and Shen, 2003). Table 3 compares the three types of kernel for Perceptron, where for the semi quadratic kernel we used the co-occurrences of characters in context window as those used in (Xue and Shen, 2003), namely{ c? 2c? 1, c? 1c0 ,c0c1 ,c1c2, c? 1c1}. Character n-gram features have proven their effectiveness in ML-based CWS (Xue and Shen,2003). It was first used in Chinese word segmentation by (Xue and Shen, 2003), where maximum entropy methods were used. Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al, 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al, 2004). The conditional maximum entropy model in our implementation is based on the one described in Section 2.5 in (Ratnaparkhi, 1998), and features are the same as those described in (Xue and Shen, 2003). Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ngand Low, 2004). By casting the problem as a character labeling task, sequence labeling models such as Conditional Random Fields can be applied on the problem (Xue and Shen, 2003). Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al, 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (Max Ent) (Xue and Shen, 2003), support vector machine Now the second author is affiliated with NTT. In the third step, we used the maximum entropy (MaxEnt) approach (the results of CRF are given in Section 3.4) to train the IOB tagger (Xue and Shen, 2003). It was first implemented in Chinese word segmentation by (Xue and Shen, 2003) using the maximum entropy methods. Xue and Shen (2003) describe for the first time the character classification approach for Chinese word segmentation, where each character is given a boundary tag denoting its relative position in a word. In this paper, we propose a statistical approach based on the works of (Xue and Shen, 2003), in which the Chinese word segmentation problem is first transformed into a tagging problem, then the Maximum Entropy classifier is applied to solve the problem. we briefly discuss the scheme proposed by (Xue and Shen, 2003), followed by our additional works to improve the performance. One of the difficulties in Chinese word segmentation is that, Chinese characters can appear in different positions within a word (Xue and Shen, 2003), and LMR Tagging was proposed to solve the problem.
K-Vec: A New Approach For Aligning Parallel Texts Various methods have been proposed for aligning texts in two or more languages such as the Canadian Parliamentary Debates (Hansards). Some of these methods generate a bilingual lexicon as a by-product. We present an alternative alignment strategy which we call K-vec, that starts by estimating the lexicon. For example, it discovers that the English word fisheries is similar to the French p~ches by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French. K-vec does not depend on sentence boundaries. However, Fung and Church (1994) point out that such a constraint does not exist between languages across language groups such as Chinese and English. Constraints Image IP techniques Alignment Pattern Resolution Structure Edge Convolution Phrase preserving One-to-one Texture Feature Sentence extraction Non-crossing Line Hough Discourse transform information (Ker and Chang 1996), cognates (Simard 1992), K-vec (Fung and Church 1994), DTW (Fung and McKeown 1994), etc. As Fung and Church (1994) we wish to estimate the bilingual lexicon directly. 
Statistical Parsing With An Automatically-Extracted Tree Adjoining Grammar We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance. We find that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG. One may object that this example is somewhat far-fetched, but Chiang (2000) notes that head-lexicalized stochastic grammars fall short in encoding even simple dependency relations such as between left and John in the sentence John should have left. The only other model that uses frontier lexicalization and that was tested on the standard WSJ split is Chiang (2000) who extracts a stochastic tree-insertion grammar or STIG (Schabes; Waters 1996) from the WSJ, obtaining 86.6% LP and 86.9% LR for sentences 40 words. They induce a probabilistic Tree Adjoining Grammar from a training set algning frames and sentences using the grammar induction technique of (Chiang, 2000) and use a beam search that uses weighted features learned from the training data to rank alternative expansions at each step. In particular, our grammars differs from the traditional probabilistic Tree Adjoining Grammar extracted as described in e.g., (Chiang, 2000) in that they encode both syntax and semantics rather than just syntax. We use sister adjunction which is commonly used in LTAG statistical parsers to deal with the relatively flat Penn Tree bank trees (Chiang, 2000). In LTAG-based statistical parsers, high accuracy is obtained by using the Magerman Collins head-percolation rules in order to provide the etrees (Chiang, 2000). Our implementation uses an extension of our monolingual parser (Chiang, 2000) based on tree-substitution grammar with sister adjunction (TSG+SA) . Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sister adjunction (TSG+SA). Another kind of grammar is a TAG automatically extracted from a treebank using the techniques of (Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)). Indeed, since TAGLET thus induces bigram dependency structures from trees, this invites the estimation of probability distributions on TAGLET derivations based on observed bigram dependencies; see (Chiang, 2000). While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. While different representations make direct comparison inappropriate, the OSTAG results lie in the same range as previous work with statistical TIG on this task, such as Chiang (2000) (86.00) and Shindo et al (2011) (85.03). In addition to adjunction, we also use sister adjunction as defined in the LTAG statistical parser described in (Chiang, 2000). Improvements along this line may be attained by use of a full TAG parser, such as Chiang (2000) for example. The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar. A striking use of sister adjunction in (Chiang, 2000) is exactly the elegant way it solves this problem: the non-argument tree can be adjoined onto a node (say, VP), positioning itself in between the VP's children, which is not possible with TAGs. Our method is similar to (Chiang, 2000), but is even simpler in ignoring the distinction between arguments and adjuncts (and thus the sister adjunction operation). Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. 
A Fast Algorithm for the Generation of Referring Expressions Abst rac t We simplify previous work in the development of algorithms for the generation of referring expre~ sions while at the same time taking account of psy- cholinguistic findings and transcript data.  First, as in (Reiter and Dale, 1992), we stipulate that some attributes of entities are more important than others, and that some words more naturally describe those attributes. Unlike in traditional generation, the starting point in1The predicate dog1 is selected because it has a dis tin guished status, referred to as type in Reiter and Dale (1992). The incremental algorithm (Reiter and Dale, 1992) is the most widely discussed attribute selection algorithm. In terms of the referring expression generation algorithm described by (Reiter and Dale, 1992), in which the description which eliminates the most dis tractors is selected, our 1http: //www.cs.waikato.ac.nz/ml/weka/ 2not all positive examples were visible 159results suggest that the human subjects chose to reduce the size of the dis tractor set before producing a description, presumably in order to reduce the computational load required to calculate the optimal description. Reiter and Dale (1992) describe a fast algorithm for generating referring expressions in the context of a natural language generation system. As shown in [Reiter and Dale, 1992], a referring expression must communicate enough information to be able to uniquely identify the intended referent in the current discourse context, but avoiding the presence of redundant or otherwise unnecessary modifiers. It would be also interesting to compare our solution with different approaches found in the literature, as for example [Reiter and Dale, 1992] or [Krahmer and Theune, 2000] for the referring expression generation, and the one of Dalianis and Hovy [Dalianis and Hovy, 1996] for the aggregation. In natural language generation, the process of generating referring expressions occurs in stages (Reiter and Dale, 1992). We plan to incorporate the results of this study in an extension of (Reiter and Dale, 1992) algorithm that would take into account other types of properties of the objects like visual salience, temporal attributes (for example time elapsed between mentions), if it participated in an action (like the case of a door opening, or a button being pushed) or its importance to the overall task completion.  For example, (Reiter and Dale, 1992) apply generalizations about the salience of properties of objects and conventions about what words make base level attributions to incrementally select words for inclusion in a description. The construction and maintenance of context according to models of text planning [Reiter and Dale, 1992], allow the author to break a complex CG into a manageable collection of small utterances. For micro planning, we have implemented the algorithm for reference planning described in [Reiter and Dale, 1992] and the aggregation algorithm described in [Shaw, 1995]. The authors present in the paper a computational framework for the generation of spatial locative expressions in such contexts, relying on the Reiter and Dale (Reiter and Dale, 1992) algorithm. Another interesting work related to referring expression generation in spatial environments can be found in (Varges, 2005). This choice should be considered in terms of particular implementation details. Reference agents rely on the Reiter and Dale algorithm (Reiter and Dale, 1992).
CONSTRAINT GRAMMAR AS A FRAMEWORK FOR PARSING RUNNING TEXT Fred Karlsson University of Helsinki Department of General Linguistics Hallituskatu 11 SF-00100 Helsinki Finland e-mail: KARLSS?N@FINUH.bitnet 1.  Previous work on this approach, largely based on Karlsson's original proposal [Karlsson, 1990], is documented in [Karlsson et ai., forthcoming]. For a description of the Constraint Grammar approach we refer thereader to (Karlsson, 1990). Our work is partly based on the work done with the Constraint Grammar framework that was originally proposed by Fred Karlsson (1990).
Finding Predominant Word Senses In Untagged Text word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed. The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of handtagged data. Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration. We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically. The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task. This is a very promising result given that our method does not require any hand-tagged text, such as SemCor. Furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora. The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account (McCarthy et al, 2004). Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to a given genre or domain (McCarthy et al., 2004) and also because there will be words that occur with insufficient frequency in the hand-tagged resources available. The method is described in (McCarthy et al, 2004), which we summarise here.  McCarthy et al (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD. Previous research in inducing sense rankings from an untagged corpus (McCarthy et al, 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction. McCarthy et al (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus. Research by (McCarthy et al, 2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn. In addition, we implemented the unsupervised method of (McCarthy et al, 2004), which calculates a prevalence score for each sense of a word to predict the predominant sense.  McCarthy et al (2004) reported that the best results were obtained using k= 50 neighbors and the Wordnet Similarity jcn measure (Jiang and Conrath, 1997). In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al (2004). We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each senses' Sx, following the approach of McCarthy et al (2004). As explained in Section 2.2, ASR is performed using the method of McCarthy et al (2004).  This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al, 2004). More radical solutions than sense grouping that have been proposed are to restrict the task to determining predominant sense in a given domain (McCarthy et al, 2004), or to work directly with para phrases (McCarthy and Navigli, 2009). In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by Word net resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al (2004)). It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al, 2004). The first, most frequent sense (MFS) (McCarthy et al, 2004), is widely used baseline for supervised WSD systems.
Forest Rescoring: Faster Decoding with Integrated Language Models Efficient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality. We develop faster approaches for problem based on parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy. Since we approach decoding as xR transduction, the process is identical to that of constituency based algorithms (e.g. Huang and Chiang, 2007). Traditional decoders (Huang and Chiang, 2007) try thousands of combinations of hypotheses and phrases, hoping to find ones that the language model likes. Thus they cannot integrate LM scoring into their decoding, requiring them to rescore the decoder output with a variant of cube growing (Huang and Chiang, 2007). A hyper graph is analogous to a parse forest (Huang and Chiang, 2007). Kriya supports the entire translation pipeline of SCFG rule extraction and decoding with cube pruning (Huang and Chiang, 2007) and LM integration (Chiang, 2007). Huang and Chiang (2007) searches with the full model, but makes assumptions about the the amount of reordering the language model can trigger in order to limit exploration. Vilar and Ney (2011) study several modifications to cube pruning and cube growing (Huang and Chiang, 2007). Beam search and cube pruning (Huang and Chiang, 2007) are used to prune the search space in all the three baseline systems. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). In our experiments, we use the cube pruning algorithm (Huang and Chiang, 2007) to carry out the search. Tree-to-string decoding with STSG is usually treated as forest rescoring (Huang and Chiang, 2007) that involves two steps. cdec therefore supports three pruning strategies that can be used during intersection: full unpruned intersection (useful for tagging models to incorporate, e.g., Markov features, but not generally practical for translation), cube pruning, and cube growing (Huang and Chiang, 2007). In hierarchical phrase-based translation (Chiang, 2005) a weighted synchronous context-free grammar is induced from parallel text, the search is based on CYK+ parsing (Chappelier and Rajman, 1998) and typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007). Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a sub translation (Huang and Chiang, 2007), whereas a phrase-based decoder only needs to do this at one end since the translation is always growing left-to-right. The complexity of this dynamic programming algorithm for g-gram decoding is O (2nn2|V|g-1) where n is the sentence length and |V| is the English vocabulary size (Huang and Chiang, 2007). An alternative approach to computing a synchronous parse forest is based on cube pruning (Huang and Chiang, 2007). We utilize the cube pruning algorithm (Huang and Chiang, 2007) for decoding and optimize the model weights with MERT. Per Non-Terminal Pruning The decoder uses a combination of beam and cube-pruning (Huang and Chiang, 2007). Huang and Chiang (2007) describe a variation of cube pruning called cube growing, and they apply it to a source-tree to target string translator.
Coreference Resolution Using Competition Learning Approach In this paper we propose a competition learning approach to coreference resolution. Traditionally, supervised machine learning approaches adopt the singlecandidate model. Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model. By contrast, our approach adopts a twin-candidate learning model. Such a model can present the competition criterion for antecedent candidates reliably, and ensure that the most preferred candidate is selected. Furthermore, our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution. The experimental results on MUC-6 and MUC-7 data set show that our approach can outperform those based on the singlecandidate model. (2) Givenananaphor and two antecedents, decide which antecedent is more likely to be the correct one (Yang et al, 2003). Each instance is represented by 33 lexical, grammatical, semantic, and 540 positional features that have been employed by high performing resolvers such as Ng and Cardie (2002) and Yang et al (2003), as described below.  Yang et al (2005) made use of nonanaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and thus equipped it with the nonanaphoricity determination capability. learning method proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates.  The model in (Yang et al., 2003) expands the conditioning scope by including a competing candidate. A first approach towards group-wise classifiers is the twin-candidate model (Yang et al, 2003).    However, as the purpose of the predicate-argument statistics is to evaluate the preference of the candidates in semantics, it is possible that the statistics-based semantic feature could be more effectively applied in the twin candidate (Yang et al, 2003) that focusses on the preference relationships among candidates. Yang et al (2003) proposed an alternative twin candidate model for anaphora resolution task.  These results not only affirm the claim by Yang et al (2003) that the TC model is superior to the SC model for pronoun resolution, but also indicate that TC is more reliable than SC in applying the statistics-based semantic feature, for N-Pron resolution.   On the MUC6 data set, for example, the best published MUC score using extracted CEs is approximately 71 (Yang et al, 2003), while multiple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g. Luo et al (2004), McCallum and Wellner (2004)). In some sense, this is a natural extension of the twin-candidate learning approach proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. Yang et al (2005) made use of non-anaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and improved the performance by 2.9 and 1.6 to 67.3 and 67.2 in F1-measure on the MUC-6 and MUC-7corpora, respectively.
Part-of-Speech Tagging for Twitter: Annotation Features and Experiments We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets. Finally, we use a POS tagger trained on tweets (Gimpel et al, 2011) to perform POS tagging on the tweets data and apart from the previously recognised named entities, only words tagged with nouns, verbs or adjectives are kept. However, POS taggers for Twitter are only available for a limited number of languages such as English (Gimpel et al, 2011). Table 3: Explicit requests for sharing (where only occurrences POS-tagged as verbs count, according to the Gimpel et al (2011) tagger). Our POS results, gathered using a Twitter-specific tagger (Gimpel et al, 2011), echo those of Ashok et al (2013) who looked at predict 14 Of course, simply inserting garbage isn't going to lead to more re-tweets, but adding more information generally involves longer text. The training and testing data are run through tweet-specific tokenization, similar to that used in the CMU Twitter NLP tool (Gimpel et al, 2011). This poses considerable problems for traditional NLP tools, which we redeveloped with other domains in mind, which of ten make strong assumptions about orthographic uniformity (i.e., there is just one way to spell you). One approach to cope with this problem is to annotate in-domain data (Gimpel et al, 2011). More specifically, we had lay annotators on the crowd sourcing platform Crowdflower re-annotate the training section of Gimpel et al (2011). We crowd source the training section of the data from Gimpel et al (2011) 2 with POS tags. Recognizing the limitations of existing systems, Gimpel et al (2011) develop a POS tagger specifically for Twitter, by creating a training corpus as well as devising a tag set that includes parts of speech that are uniquely found in on line language, such as emoticons (smilies). In this work, we focus on Twitter because the labeled corpus by Gimpel et al (2011) allows us to quantitatively evaluate our approach. We then present POS tagging results on the Twitter POS dataset (Gimpel et al, 2011). Our data is the recent Twitter POS dataset released at ACL 2011 by Gimpel et al (2011) consisting of approximately 26,000 words across 1,827 tweets. The Twitter dataset uses a domain-dependent tag set of 25 tags that are described in (Gimpel et al, 2011). For this experiment, we again make use of the Twitter POS dataset (Gimpel et al, 2011). We tokenize each tweet with Twitter NLP (Gimpel et al, 2011), remove the @ user and URLs of each tweet, and filter the tweets that are too short (< 7 words). A tweet-specific tokenizer (Gimpel et al, 2011) is employed, and the dependency parsing results are computed by Stanford Parser (Klein and Manning, 2003).
Self-Training for Biomedical Parsing Parser self-training is the technique of taking an existing parser, parsing extra data and then creating a second parser by treating the extra data as further training data. Here we apply this technique to parser adaptation. In particular, we self-train the standard Charniak/Johnson Penn-Treebank parser using unlabeled biomedical abstracts. This an of 84.3% on a standard test set of biomedical abstracts from the Genia corpus. This is a 20% error reduction over the best previous result on biomedical data (80.2% on the same test set). We applied two mainstream Penn Treebank (PTB) phrase structure parsers: the Bikel parser, implementing Collins' parsing model (Bikel, 2004) and trained on PTB, and the reranking parser of (Charniak and Johnson, 2005) with the self-trained biomedical parsing model of (McClosky and Charniak, 2008). Concerning techniques for improving out-of domain parsing, a related approach has been to use self-training with auto-parsed out-of-domain data, as McClosky and Charniak (2008) do for English constituency parsing, though in that approach lexical generalization is not explicitly performed. Parse trees come from the Charniak-Johnson parser (Charniak and Johnson, 2005) with a self-trained biomedical parsing model (McClosky and Charniak, 2008), and are converted to dependency structures again using Stanford CoreNLP. The organizers provide parses from a version of the McClosky-Charniak parser, MCCC (McClosky and Charniak, 2008), which is a two-stage parser/reranker trained on the GENIA corpus. The re-ranking parser of Charniak & Johnson adapted to the biomedical domain (McClosky and Charniak, 2008). For syntactic parsing, we use the output of the BLLIP re-ranking parser adapted to the biomedical domain by McClosky and Charniak (2008), as provided by the shared task organizers in the Stanford collapsed dependency format with conjunct dependency propagation. Parsing is performed using the Charniak-Johnson parser (Charniak and Johnson, 2005) with the self-trained biomedical parsing model of McClosky and Charniak (2008).
Using Contextual Speller Techniques and Language Modeling for ESL Error Correction We present a modular system for detection and correction of errors made by non native (English as a Second Language = ESL) writers. We focus on two error types: the incorrect use of determiners and the choice of prepositions. We use a decision tree approach inspired by contextual spelling systems for detection and correction suggestions, and a large language model trained on the Gigaword corpus to provide additional information to filter out spurious suggestions. We show how this system performs on a corpus of non-native English text and discuss strategies for future enhancements. Projects involving learner corpora in analyzing and categorizing learner errors include NICT Japanese Learners of English (JLE), the Chinese Learners of English Corpus (Gamon et al, 2008) and English Taiwan Learner Corpus (or TLC) (Wible et al, 2003). Most of these methods use large corpora of well-formed native English text to train statistical models, e.g. (Han et al., 2004), (Gamon et al., 2008) and (De Felice and Pulman, 2008). In our implementation we approach these tasks in a two-step approach as proposed in (Gamon et al, 2008). Our error correction system implements a correction validation mechanism as proposed in (Gamon et al., 2008). We build from Dickinson et al. (2010) in two main ways: first, we implement a presence-selection pipeline that has proven effective for English preposition error detection (cf. Gamon et al., 2008). We may also include heuristic-based filters, such as the ones implemented in Criterion (see Leacock et al, 2010), as well as a language model approach (Gamon et al, 2008). Gamon et al (2008) introduce a system for the detection of a variety of learner errors in non native English text, including preposition errors. Knight and Chander (1994) and Gamon et al (2008) used decision tree classifiers but, in general, maximum entropy classifiers have become the classification algorithm of choice. Training data are normally drawn from sizeable corpora of native English text (British National Corpus for DeFelice and Pulman (2007, 2008), Wall Street Journal in Knight and Chander (1994), a mix of Reuters and Encarta in Gamon et al (2008, 2009).  Gamon et al (2008) used word-based language models to detect and correct common ESL errors, while Leacock and Chodorow (2003) used part-of-speech bigram language models to identify potentially ungrammatical two-word sequences in ESL essays. For example, Tetreault and Chodorow (2008), Gamon et al (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. T&C08, De Felice and Pulman (2008) and Gamon et al (2008) describe very similar preposition error detection systems in which a model of correct prepositional usage is trained from well formed text and a writer's preposition is compared with the predictions of this model. Gamon et al (2008) train a decision tree model and a language model to correct errors in article and preposition usage. While there has been considerable emphasis placed on the system development aspect of the field, with researchers tackling some of the toughest ESL errors such as those involving articles (Han et al, 2006) and prepositions (Gamon et al, 2008), (Felice and Pullman, 2009), there has been a woeful lack of attention paid to developing best practices for annotation and evaluation. Gamon et al (2008) and Gamon (2010) used a language model in addition to a classifier and combined the classifier output and language model scores in a meta classifier. Note that this use of a host of language model features is substantially different from using a single language model score on hypothesized error and potential correction to filter out unlikely correction candidates as in Gamon et al (2008) and Gamon (2010). Gamon et al (2008) worked on a similar approach using only tagged trigram left and right contexts: a model of prepositions uses serves to identify preposition errors and the Web provides examples of correct form.  Gamon et al (2008) and Gamon (2010) use a combination of classification and language modeling.
Enriching The Knowledge Sources Used In A Maximum Entropy Part-Of-Speech Tagger This paper presents results for a maximumentropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging. In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs. The best resulting accuracy for the tagger on the Penn Treebank is 96.86% overall, and 86.91% on previously unseen words. The primary reason for the large disparity between the Brill tagger output and original Penn Treebank annotation is that it is notoriously difficult to differentiate between particles, prepositions and adverbs (Toutanova and Manning, 2000).  In order to make a fair comparison between the human texts and our own, we used a part-of-speech (POS) tagger (Toutanova and Manning, 2000) to extract those grammatical categories that we aim to control within our framework, i.e. nouns, verbs, prepositions, adjectives and adverbs. Toutanova and Manning (2000), Toutanova et al (2003), Lafferty et al (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. The features used in this work are typical for modern MEMM POS tagging and are mostly based on work by Toutanova and Manning (2000). We used the Stanford log-linear POS tagger (Toutanova and Manning, 2000) in this study. We use the Google Web 1T data (Brants and Franz (2006)), and POS-tagged ngrams using Stanford POS Tagger (Toutanova and Manning (2000)). Under the hypothesis that action item utterances will exhibit particular syntactic patterns, we use a conditional Markov model part-of-speech (POS) tagger (Toutanova and Manning, 2000) trained on the Switchboard corpus (Godfrey et al, 1992) to tag utterance words for part of speech. We apply the Stanford POS tagger (Toutanova and Manning, 2000) on Twitter messages, and only select nouns and adjectives as valid candidates for user tags.  So we have also used the Stanford POS tagger (Toutanova and Manning, 2000) to tag these transcripts before calculating the Discriminative TFIDF score. Next, we replace all nouns with their POS tag; we use the Stanford POS Tagger (Toutanova and Manning, 2000). For POS-tagging, we used the Stanford POS Tagger (Toutanova and Manning, 2000). We consider a word as a predicate if it is tagged as a verb by a Part-of-Speech tagger (Toutanova and Manning, 2000). The Stanford POS tagger (Toutanova and Manning, 2000) and the Stanford parser (Klein and Manning, 2003) were used to produce the part of speech and dependency annotations. It uses a maximum-entropy approach to handle information diversity without assuming predictor independence (Toutanova and Manning, 2000). For part-of-speech (POS) tagging of the sentences, we used Stanford POS Tagger (Toutanova and Manning, 2000). Following Toutanova and Manning (2000) approximately, more information is defined for words that are considered rare (which we define here as words that occur fewer than fifteen times). Toutanova and Manning (2000) achieves 96.9% (on seen) and 86.9% (on unseen) with an MEMM. This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in Collins (2002), using the same data splits, and a larger error reduction of 12.1% from the more similar best previous log linear model in Toutanova and Manning (2000).
Disambiguating Noun Groupings With Respect To Wordnet Senses Word groupings useful for language processing tasks are increasingly available, as thesauri appear on-line, and as distributional word clustering techniques improve. However, for many tasks, one is in relationships among word words. This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns — the kind of data one finds in on-line thesauri, or as the output of distributional clustering algorithms. Disambiguation is performed with respect to WordNet senses, which are fairly fine-grained; however, the method also permits the assignment of higher-level WordNet categories rather than sense labels. The method is illustrated primarily by example, though results of a more rigorous evaluation are also presented. For instance, work on word sense disambiguation i corpora (e.g. Resnik 1995), could lead to an estimate of frequencies for word senses in general, with rule-derived senses simply being a special case. An adaptation of Lesk dictionary-based WSD algorithm has been used to disambiguate adjectives and ad verbs (Banerjee and Pedersen, 2002), an adaptation of the Resnik algorithm has been used to disambiguate nouns (Resnik, 1995), while the algorithm we developed for disambiguating verbs exploits the nouns in the context of the verb as well as the nouns both in the glosses and in the phrases that WordNet utilizes to describe the usage of a verb. The procedure is obtained by making some variations to the algorithm designed by Resnik (1995) for disambiguating noun groups. JIGSAW nouns differs from the original algorithm by Resnik (1995) in the similarity measure used to compute relatedness of two senses. Although the assessment of semantic similarity using a dictionary database as knowledge source has been recognized as providing significant cues for word clustering (Resnik 1995b) and the determination of lexical cohesion (Morris& amp; Hirst, 1991), its relevance for word disambiguation in running text remains relatively unexplored. Resnik (1995a) defines the semantic similarity between two words as the entropy value of the most informative concept subsuming the two words in a hierarchically structured thesaurus. At present, we are trying to integrate the word sense disambiguation method proposed in (Resnik, 1995) into our system. Semantic tags are assigned from on-line thesaura like WordNet (Basili et al 1996) (Resnik, 1995), Roget's categories (Yarowsky 1992) (Chen and Chen, 1996), the Japanese BGH (Utsuro et al 1993), or assigned manually (Basili et al 1992). The verbs are tagged with respect to senses in WordNet (Miller 1990), which has become widely used, for example in corpus-annotation projects (Miller et al 1994, Ng& amp; Hian 1996, and Grishman et al 1994) and for performing disambiguation (Resnik 1995 and Leacock et ai. Finally, disambiguating the direct object according to WordNet categories, e.g., Resnik (1995), would improve the accuracy of using these categories to disambiguate verbs. There have been a number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., Hearst and Grefenstette 1992, Resnik 1995). Eventually, the sense supported by those patterns which are semantically closer to the context in question is selected as the most likely one (see, among others, [Dolan, 1994], [Resnik, 1995a, 1995b], [Agirre and Rigau, 1996], [Sanfilippo, 1997]). Some of them have been fully tested in real size texts (e.g. statistical methods (Yarowsky, 1992), (Yarowsky, 1994), (Miller and Teibel, 1991), knowledge based methods (Sussna, 1993), (Agirre and Rigau, 1996), or mixed methods (Richardson et al, 1994), (Resnik, 1995)).
The LinGO Redwoods Treebank Motivation and Preliminary Applications Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christopher Manning, Dan Flickinger, and Thorsten Brants {oe |kristina |manning |dan}@csli.stanford.edu, shieber@deas.harvard.edu, brants@parc.xerox.com Abstract The LinGO Redwoods initiative is a seed activity in the de- sign and development of a new type of treebank.  These tools could use the type hierarchy to predict where conflicts are likely to arise and bring these to the engineer's attention, possibly inspired by the approach under development at CSLI for the dynamic maintenance of the LinGO Redwoods tree bank (Oepen et al, 2002). Fuchss et al (2004) supported the claim by investigating MRS structures in the Redwoods corpus (Oepen et al, 2002). Furthermore, to test the grammar precision and accuracy, we use two tree banks: Redwoods (Oepen et al, 2002) for English and Hinoki (Bond et al, 2004) for Japanese. We now briefly describe the Redwoods treebanking environment (Oepen et al, 2002), our parse selection models and their performance. Penn Treebank (Marcus et al, 1993) the HPSG LinGo Redwoods Treebank (Oepen et al, 2002), and a smaller dependency tree bank (Buchholz and Marsi, 2006). We do so in the context of Redwoods (Oepen et al, 2002), a tree bank that contains HPSG analyses for sentences from the Verbmobil appointment scheduling and travel planning domains. We show that sample selection metrics based on tree entropy (Hwa, 2000) and disagreement between two different parse selection models significantly reduce the number of annotated sentences necessary to match a given level of performance according to random selection. To address this limitation, the Redwoods treebank has been created to provide annotated training material to permit statistical models for ambiguity resolution to be combined with the precise interpretations produced by the ERG (Oepen et al, 2002). The structure of our treebank is inspired by the Redwoods tree bank of English (Oepen et al,2002) in which utterances are parsed and the annotator selects the best parse from the full analyses derived by the grammar. We apply these ideas in the context of parse disambiguation for sentence analyses produced by a Head-driven Phrase Structure Grammar (HPSG), the grammar formalism underlying the Redwoods corpus (Oepen et al, 2002). Although annotated corpora exist for HPSG, such corpora do not exist in significant volumes and are limited to a few small domains (Oepen et al, 2002). To overcome this limitation for the HPSG English ResourceGrammar (ERG, Flickinger (2000)), the Redwoods treebank has been created to provide annotated training material (Oepen et al, 2002). With the evolution of the grammar, the treebank as the output from the grammar changes over time (Oepen et al, 2002). This kind of dynamic, discriminant-based treebanking was pioneered in the Redwoods treebank of English (Oepen et al, 2002), so we refer to it as Redwoods-style treebanking.  However, despite research on HPSG processing efficiency (Oepen et al, 2002a), the application of HPSG parsing is still limited to specific domains and short sentences (Oepen et al, 2002b; Toutanova and Manning, 2002). However, despite the development of methods to improve HPSG parsing efficiency (Oepen et al, 2002a), the exhaustive parsing of all sentences in a tree bank is still expensive. The lexical acquisition model was trained with the Redwoods treebank (Oepen et al, 2002), following Zhang et al (2006). In this paper, we evaluate the truth of these assumptions on the MRS expressions which the ERGcomputes for the sentences in the Redwoods Treebank (Oepen et al, 2002). As a test corpus, we use the Redwoods Treebank (Oepen et al, 2002) which contains 6612 sentences. The structure of our treebank is inspired by the Redwoods treebank of English in which utterances are parsed and the annotator selects the best parse from the full analyses derived by the grammar (Oepen et al., 2002).
V-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure We present V-measure, an external entropybased cluster evaluation measure. V measure provides an elegant solution tomany problems that affect previously defined cluster evaluation measures includ ing 1) dependence on clustering algorithm or data set, 2) the ?problem of matching?, where the clustering of only a portion of datapoints are evaluated and 3) accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness. We compare V-measure to a num ber of popular cluster evaluation measuresand demonstrate that it satisfies several desirable properties of clustering solutions, us ing simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering. There are many possible methods for evaluating clustering quality (Rosenberg and Hirschberg, 2007). We evaluate both the inferred phonetic categories and words using the clustering evaluation measure V-Measure (VM; Rosenberg and Hirschberg, 2007). F is not suitable for comparing results with different cluster numbers (Rosenberg and Hirschberg, 2007). In the first one, unsupervised evaluation, systems' answers were evaluated according to: (1) V Measure (Rosenberg and Hirschberg, 2007), and (2) paired F-Score (Artiles et al, 2009). This section presents the measures of unsupervised evaluation, i.e V-Measure (Rosenberg and Hirschberg, 2007) and (2) paired F-Score (Artiles et al, 2009). V-Measure (Rosenberg and Hirschberg, 2007) assesses the quality of a clustering solution by explicitly measuring its homogeneity and its completeness. Homogeneity refers to the degree that each cluster consists of data points primarily belonging to a single GS class, while completeness refers to the degree that each GS class consists of data points primarily assigned to a single cluster (Rosenberg and Hirschberg, 2007). The V-measure (VM) (Rosenberg and Hirschberg, 2007) is an information theoretic metric that reports the harmonic mean of homogeneity (each cluster should contain only instances of a single class) and completeness (all instances of a class should be members of the same cluster). A significant limitation of F-Score is that it does not evaluate the make up of clusters beyond the majority class (Rosenberg and Hirschberg, 2007). These two limitations define the matching problem of F-Score (Rosenberg and Hirschberg, 2007) which can lead to: (1) identical scores between different clustering solutions, and (2) inaccurate assessment of the clustering quality. ubsequently, we present the use of V measure (Rosenberg and Hirschberg, 2007) as an evaluation measure that can overcome the current limitations of F-Score. As it can be observed, F-Score assesses the quality of a clustering solution by considering two different angles, i.e. homogeneity and completeness (Rosenberg and Hirschberg, 2007). The former situation is present, due to the fact that F-Score does not consider the make-up of the clusters beyond the majority class (Rosenberg and Hirschberg, 2007). V-measure assesses the quality of a clustering solution by explicitly measuring its homogeneity and its completeness (Rosenberg and Hirschberg, 2007). This happens when each Gs class is included in all clusters with a distribution equal to the distribution of sizes (Rosenberg and Hirschberg, 2007). This is due to the fact that V-measure considers as the worst solution in terms of completeness the one, in which each class is represented by every cluster, and specifically with a distribution equal to the distribution of cluster sizes (Rosenberg and Hirschberg, 2007). Two evaluation metrics are used during the unsupervised evaluation in order to estimate the quality of the clustering solutions, the V-Measure (Rosenberg and Hirschberg, 2007) and the paired F Score (Artiles et al, 2009). Recently, two measures have been proposed that avoid many of the weaknesses of previous measures and exhibit several attractive properties (see Sections 2 and 3): the VI measure (Meila, 2007) and the V measure (Rosenberg and Hirschberg, 2007). As noted in (Rosenberg and Hirschberg, 2007), these measures evaluate not only the quality of the proposed clustering but also of the mapping scheme. As noted by (Rosenberg and Hirschberg, 2007), the Q measure does not explicitly address the completeness of the suggested clustering.
11001 New Features for Statistical Machine Translation We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrasebased translation system and our syntax-based translation system. On a large-scale Chinese- English translation task, we obtain statistically improvements of respectively. We analyze the impact of the new features and the performance of the learning algorithm. Chiang et al (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above. Our baseline decoder contains a large and powerful set of features, which include: 7 sparse feature types, totaling 50k features (Chiang et al., 2009). Potentially, improvements could be gained from using separate weights for individual local models, but this would require an optimization procedure such as MIRA (Chiang et al, 2009), which can handle a larger number of features. Given the current infrastructure, other training methods (e.g., maximum conditional likelihood or MIRA as used by Chiang et al (2009)) can also be easily supported with minimum coding. See (Chan et al, 2007) for the relevance of word sense disambiguation and (Chiang et al, 2009) for the role of prepositions in MT. Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al (2009). Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al, 2009). Alternatively, by using the large margin optimizer in (Chiang et al, 2009) and moving it into the for-each loop (lines 4-9), one can get an online algorithm such PMO-MIRA. With the training sentences yi and their simulated confusion sets N (yi) — represented as hypergraphs D(yi)) — we can perform the discriminative training using any of a number of procedures such as MERT (Och, 2003) or MIRA as used by Chiang et al. (2009). Additionally, we use 50,000 sparse, binary-valued features such as "Is the bi-gram 'united states' present in the output?", based on (Chiang et al., 2009). The second one is the MIRA algorithm, first applied for machine translation in (Chiang et al, 2009). In addition, it uses a large number of discriminatively tuned features, which were inspired by Chiang et al (2009) and implemented in a way described in (Devlin 2009). Following (Chiang et al, 2009), we only use 100 most frequent words for word context feature. Such features could include syntactical features (Chiang et al, 2009). For example, the features introduced by Chiang et al (2008) and Chiang et al (2009) for an SCFG model for Chinese/English translation are of two types: The first type explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals. The base feature set for all systems was similar to the expanded set recently used for Hiero (Chiang et al, 2009), but with bigram features (source and target word) instead of trigram features (source and target word and neighboring source word). The situation might be improved by using a model that does a better job of both capturing the structure of the source and target sentences and their allowable reorderings, such as a syntactic tree-to-string system that uses contextually rich rewrite rules (Galley et al, 2006), or by making use of larger more fine grained feature sets (Chiang et al., 2009) that allow for better discrimination between hypotheses. This was well understood in previous work, so heuristic filtering was usually applied (Chiang et al, 2009, inter alia). Recent work by (Chiang et al, 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing. Discount features for rule frequency bins (cf. Chiang et al. (2009), Section 4.1).
A Maximum Entropy Approach to Chinese Word Segmentation Jin Kiat Low 1 and Hwee Tou Ng 1,2 and Wenyuan Guo 2 1.  In ME model, the following features (Jin Kiat Low et al, 2005) are selected: a) cn (n=-2, -1, 0, 1, 2) b) cncn+1 (n=-2, -1, 0, 1) c) c-1c+1 where cn indicates the character in the left or right position n relative to the current character c0. The current state-of-the-art segmentation software developed by (Low et al, 2005), which ranks as the best in the SIGHAN bakeoff (Emerson, 2005), attains word precision and recall of 96.9% and 96.8%, respectively, on the PKU track. 3) New feature templates were added, such as the templates that were used in representing numbers, dates, letters etc. (Low et al., 2005). Those for the 4-tag set, adopted from (Xue, 2003) and (Low et al., 2005), include C−2, C−1, C0, C1, C2, C−2C−1, C−1C0, C−1C1, C0C1 and C1C2. Third, the post processing method (Low et al, 2005) is employed to enhance the unknown word segmentation.  We add a new feature, which also used in maximum entropy model for word segmentation task by (Low et al, 2005), to the feature templates for CRF model while keep the other features same as (Zhao et al, 2006). 3) New feature templates were added, such as templates used in representing numbers, dates, letters etc. (Low et al., 2005). Briefly, after ensuring the corpora were sentence-aligned, we tokenized the English texts and performed word segmentation on the Chinese texts (Low et al, 2005). Briefly, after ensuring the corpora were sentence-aligned, we tokenized the English texts and performed word segmentation on the Chinese texts (Low et al, 2005). We use the maximum entropy segmenter of (Low et al, 2005) to segment the Chinese part of the FBIS corpus.  Our Chinese word segmenter is a modification of the system described by Low et al (2005), which they entered in the 2005 Second International Chinese Word Segmentation Bakeoff. Much of this can be attributed to the value of using an external dictionary and additional training data, as illustrated by the experiments run by Low et al (2005) with their model. It should be noted that in our testing during development, even when we strove to create a system which matched as closely as possible the one described by Low et al (2005), we were unable to achieve scores for the 2005 bakeoff data as high as their system did. Especially, character-based tagging method which was proposed by Nianwen Xue (2003) achieves great success in the second International Chinese word segmentation Bakeoff in 2005 (Low et al, 2005). We utilized four of the five basic feature templates suggested in (Low et al, 2005), described as follows: Cn (n=-2, -1, 0, 1, 2), CnCn+ 1 (n=-2, -1, 0, 1), Pu (C0)? T (C-2) T (C-1) T (C0) T (C1) T (C2) where C refers to a Chinese character. See detail description and the example in (Low et al, 2005). Especially, the character-based tagging method which was proposed by Nianwen Xue (2003) achieves great success in the second International Chinese word segmentation Bakeoff in 2005 (Low et al, 2005). Low et al (2005) introduce an external dictionary as features of a discriminative model.
Word Sense Disambiguation Using A Second Language Monolingual Corpus This paper presents a new approach for resolving lexical ambiguities in one language using statistical data from a monolingual corpus of another language. This approach exploits the differences between mappings of words to senses in different languages. The paper concentrates on the problem of target word selection in machine translation, for which the approach is directly applicable. The presented algorithm identifies syntactic relations between words, using a source language parser, and maps the alternative interpretations of these relations to the target language, using a bilingual lexicon. The preferred senses are then selected according to statistics on lexical relations in the target language. The selection is based on a statistical model and on a constraint propagation algorithm, which simultaneously handles all ambiguities in the sentence. The method was evaluated using three sets of Hebrew and German examples and was found to be very useful for disambiguation. The paper includes a detailed comparative analysis of statistical sense disambiguation methods. Due to the recent availability of large text corpora, various statistical approaches have been tried including using 1) parallel corpora (Brown et al., 1990), (Brown et al., 1991), (Brown, 1997), 2) non-parallel bilingual corpora tagged with topic area (Yamabana et al., 1998) and 3) un-tagged mono-language corpora in the target language (Dagan and Itai, 1994), (Tanaka and Iwasaki, 1996), (Kikui, 1998). Dagan and Itai (1994) have also addressed the lexical selection problem from the TL point of view. We used two measurements, applicability and precision (Dagan and Itai 1994), to evaluate the performance of our method. In addition, (Dagan and Itai, 1994) and (Li, 2002) propose using two monolingual corpora for word sense disambiguation. The idea of obtaining linguistic information about a text in one language by exploiting parallel or comparable texts in another language has been explored in the field of Word Sense Disambiguation (WSD) since the early 90's, the most representative works being (Brown et al, 1991), (Gale et al, 1992), and (Dagan and Itai, 1994). Note that, unlike Dagan and Itai (1994), we give no consideration to statistical confidence as we are after 100% recall, whatever the cost to precision. It may, therefore, be desirable to apply a dynamic threshold on the discriminative ratio (cf. (Dagan and Itai, 1994)) to accept only those translation pairs with sufficiently high statistical confidence, for example. However, most of previous research has focused on using multilingual resources typically used in SMT systems to improve WSD accuracy Dagan and Itai (1994). For example, Dagan and Itai (1994) carried out WSD experiments using monolingual corpora, a bilingual lexicon and a parser for the source language. Dagan and Itai (1994) indicate that two languages are more informative than one; an English corpus is very helpful in disambiguating polysemous words in Hebrew text. The work of (Dagan and Itai, 1994) has also successfully used WSD to improve the accuracy of machine translation. Dagan and Itai (1994) proposed an approach to WSD using monolingual corpora, a bilingual lexicon and a parser for the source language. The first method is based on a decision threshold (Dagan and Itai, 1994): the algorithm rejects decisions taken when the difference of the maximum likelihood among the competing senses is not big enough. As in (Dagan and Itai, 1994), we adjusted the measure to the amount of evidence. (Dagan and Itai, 1994) explicitly suggests performing word sense disambiguation in the target language (English in the article) with the goal of resolving ambiguity in the source language (Hebrew).
A Probability Model To Improve Word Alignment Word alignment plays a crucial role in statistical machine translation. Word-aligned corpora have been found to be an excellent source of translation-related knowledge. We present a statistical model for computing the probability of an alignment given a sentence pair. This model allows easy integration of context-specific features. Our experiments show that this model can be an effective tool for improving an existing word alignment. The details of this algorithm are described in (Cherry and Lin, 2003). (Cherry and Lin, 2003) exploited such cohesion between the dependency structures to improve the quality of word alignment of parallel sentences. (Cherry and Lin, 2003) recently proposed a direct alignment formulation and state that it would be straightforward to estimate the parameters given a supervised alignment corpus. As in (Cherryand Lin, 2003), the above functions simplify the conditioning portion, h by utilizing only the words and context involved in the link li. The basic intuition behind this feature is that words inside prepositional phrases tend to align, which is similar to the dependency structure feature of (Cherry and Lin, 2003). This parser has been used in a much different alignment model (Cherry and Lin, 2003). Recently, researchers like Cherry and Lin (2003) have begun to use syntactic analyses to guide and restrict the word alignment process. Finally, our work is similar to that of Cherry and Lin (2003) in our use of the conditional probability of a link given the co-occurrence of the linked words.  In (Cherry and Lin, 2003) a probability model Pr (aJ1 | fJ1, eI1) is used, which is symmetric per definition. These approaches include an enhanced HMM alignment model that uses part-of speech tags (Toutanova et al, 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al, 2005). Our model extends to phrase alignment the concept of a sentence pair generating a word alignment developed by Cherry and Lin (2003). Finally, inspired by these intuitive notions of translational correspondence, Cherry and Lin (2003) include dependency features in a word alignment model to improve non-syntactic baseline systems.  Cherry and Lin (2003) developed a statistical model to find word alignments, which allow easy integration of context-specific features. Cherry and Lin (2003) use the phrasal cohesion of a dependency tree as a constraint on a beam search aligner. More details on the probability model used by ProAlign are available in (Cherry and Lin, 2003). Most other researchers take either the HMM alignments (Liang et al, 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4.
The CoNLL 2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies The Conference on Computational Natural Language Learning is accompanied every year by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting. In 2008 the shared task was dedicated to the joint parsing of syntactic and semantic dependencies. This shared task not only unifies the shared tasks of the previous four years under a unique dependency-based formalism, but also extends them significantly: this year’s syntactic dependencies include more information such as named-entity boundaries; the semantic dependencies model roles of both verbal and nominal predicates. In this paper, we define the shared task and describe how the data sets were created. Furthermore, we report and analyze the results and describe the approaches of the participating systems. Following the successful approaches taken by the participants of the CoNLL-2008 shared task (Surdeanu et al, 2008) on monolingual syntactic and semantic dependency analysis, we designed and implemented our CoNLL-2009 SRL only system with pipeline architecture. We chose to use maximum entropy algorithm in this step because of its success in the CoNLL-2008 shared task (Surdeanu et al, 2008). Our first attempt is to directly apply the state of-art SRL system (Meza-Ruiz and Riedel, 2009) that trained on the CoNLL 08 shared task dataset (Surdeanu et al, 2008), hereafter called SRL-BS, to news tweets. Domain-oriented semantic structures are valuable assets because their representation suits information needs in the domain; however, the extraction of such structures is difficult due to the large gap between the text and these structures. On the other hand, the extraction of linguistically oriented semantics from text has long been studied in computational linguistics, and has recently been formalized as Semantic Role Labeling (Gildea and Jurafsky, 2002), and semantic structure extraction (Baker et al, 2007) (Surdeanu et al, 2008). CoNLL 2008 shared task (Surdeanu et al, 2008) first introduced the predicate classification task, which can be regarded as the predicate sense disambiguation. Perhaps the most immediately promising resource is the CoNLL shared task data from 2008 (Surdeanu et al, 2008) which has syntactic dependency annotations, named-entity boundaries and the semantic dependencies model roles of both verbal and nominal predicates. These resources exist on a large scale spearheading the SRL research in the associated languages (Carreras and Marquez, 2005), Surdeanu et al (2008). In 2008, the shared task (Surdeanu et al, 2008) used a unified dependency based formalism, which modeled both syntactic dependencies and semantic roles for English. In all sections, we will mention some of the differences between last year's and this year's tasks while keeping the text self-contained whenever possible; for details and observations on the English data, please refer to the overview paper of the CoNLL-2008 Shared Task (Surdeanu et al, 2008) and to the references mentioned in the sections describing the other languages. (Surdeanu et al, 2008), (Burchardt et al, 2006) and (Kawahara et al, 2002). The English corpus is almost identical to the corpus used in the closed challenge in the CoNLL-2008 shared task evaluation (Surdeanu et al, 2008). The complete merging process and the conversion from the constituent representation to dependencies is detailed in (Surdeanu et al, 2008). LGS denotes a logical subject in a passive construction (Surdeanu et al, 2008). Table 1 shows SRL performance for the local model described above, and the full global CCG-system described by Boxwell et al (2009). We use the method for calculating the accuracy of Propbank verbal semantic roles described in the CoNLL-2008 shared task on semantic role labeling (Surdeanu et al, 2008). In a second experiment, we applied the feature discovery procedure to the English corpus from CoNLL 2008 (Surdeanu et al, 2008), a dependency corpus converted from the Penn Tree bank and the Brown corpus. The CoNLL 2008 shared task (Surdeanu et al, 2008) was on joint parsing and semantic role labeling, but the best systems (Johansson and Nugues, 2008) were the ones which completely decoupled the tasks. This is a purely syntactic resource, but we can also include this tree bank in the category of multistratal resources since the PropBank (Palmer et al, 2005 )andNomBank (Meyers et al, 2004) projects have an notated shallow semantic structures on top of it. Dependency-converted versions of the Penn Tree bank, PropBank and NomBank were used in the CoNLL-2008 Shared Task (Surdeanu et al, 2008), in which the task of the participants was to produce a bistratal dependency structure consisting of surface syntax and shallow semantics. We applied the bistratal search method in Algorithm 3 on the data from the CoNLL-2008 Shared Task (Surdeanu et al, 2008). Here A0 represents the seller, and A1 represents the things sold (CoNLL 2008 shared task, Surdeanu et al, 2008). The submitted parser is simpler than the submission in which I participated at the CoNLL 2008 shared task on joint learning of syntactic and semantic dependencies (Surdeanu et al., 2008), in which we used a more complex committee based approach to both syntax and semantics (Samuelsson et al, 2008).
A Simple Similarity-based Model for Selectional Preferences We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics. Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles. In evaluations the similarity-based model shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model, but has coverage problems. We build on a recent selectional preference model (Erk, 2007) that bases its generalisations on word similarity in a vector space. Our model builds on the architecture of Erk (2007). Erk (2007) extracted the set of seen head words from corpora with semantic role annotation, and used only a single vector space representation. In addition, we discuss in detail which properties of the vector space are crucial for the prediction of plausibility ratings, a much more fine-grained task than the pseudo-word disambiguation task presented in Erk (2007) that is more closely related to semantic role labelling. We have demonstrated that the successful evaluation of the model in Erk (2007) on the coarse-grained pseudo-word disambiguation task carries over to the prediction of human plausibility judgments which requires relatively fine-grained, relation-based distinctions. Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). Erk (2007) and Erk et al (2010) modeled the contexts of a word as the distribution of words that co-occur with it.  Selectional preferences are computed as in Erk (2007). In (Erk, 2007) a distributional similarity based model for selectional preferences is introduced, reminiscent of that of Pantel and Lin (2000). Bergsma et al. (2008) test pairs that fall below a mutual information threshold (might include some seen pairs), and Erk (2007) selects a subset of roles in FrameNet (Baker et al, 1998) to test and uses all labeled instances within this subset (unclear what portion of subset of data is seen). We implemented the current state-of-the-art smoothing model of Erk (2007). The Train size is approximately the same size used in Erk (2007), although on a different corpus. These results appear consistent with Erk (2007) because that work used the BNC corpus (the same size as one year of our data) and Erk chose confounders randomly within a broad frequency range. Similar to Erk (2007), we used an adapted version which we computed for semantic roles by means of the FN database rather than for verb argument positions. Erk et al (2010) propose the Exemplar-Based Model of Selectional Preferences, in turn based on Erk (2007). In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL.   The notion of selectional preference is not restricted to surface-level predicates such as verbs and modifiers, but also extends to semantic frames (Erk, 2007) and inference rules (Pantel et al, 2007).
Refining Event Extraction through Cross-Document Inference We apply the hypothesis of “One Sense Per Discourse” (Yarowsky, 1995) to information extraction (IE), and extend the scope of “discourse” from one single document to a cluster of topically-related documents. We employ a similar approach to propagate consistent event arguments across sentences and documents. Combining global evidence from related documents with local decisions, we design a simple scheme to conduct cross-document inference for improving the ACE event ex- Without using any additional labeled data this new approach obtained 7.6% higher F-Measure in trigger labeling and 6% higher F-Measure in argument labeling over a state-of-the-art IE system which extracts events independently for each sentence. Ji and Grishman (2008) extended the one sense per discourse idea (Yarowsky, 1995) to multiple topically related documents and propagate consistent event arguments across sentences and documents. We then apply the cross-document inference techniques as described in (Ji and Grishman, 2008) to improve trigger and argument labeling performance by favoring interpretation consistency across the test events and background events. Later, Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic-related documents. (Ji and Grishman 2008) have observed that the events in a cluster of documents on the same topics as documents in the training corpus can be tagged more confidently. We followed Ji and Grishman (2008)'s approach and used the INDRI retrieval system (Strohman et al, 2005) to obtain the top N related documents for each annotated document in the training corpus.  There are two common assumptions within a cluster of related documents (Ji and Grishman 2008): Trigger Consistency Per Cluster: if one instance of a word triggers an event, other instances of the same word will trigger events of the same type. For ST_IR and ST_GI, we retrieved the best N (using N=25, which (Ji and Grishman 2008) found to work best) related texts for each training document from the English TDT5 corpus consisting of 278,108 news texts (from April to September of 2003). The most salient work for event extraction is Grishman et al (2005) and Jiand and Grishman (2008). Ji and Grishman (2008) further exploit a correlation between senses of verbs (that are triggers for events) and topics of documents. Our work shares some similarities. Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic related documents. Although this figure is very low, it is not surprising: the results on the English ACE 2005 corpus show that the inter-annotator agreement on trigger identification is only about 40% (Ji and Grishman, 2008). The second reason is that it is hard to identify an event mention due to the failure of following specified annotation guidelines, as mentioned in Ji and Grishman (2008). We use a state-of-the-art IE system (Ji and Grishman, 2008) developed for the Automatic Content Extraction (ACE) program to process texts and automatic speech recognition output. Our work can also be considered as an extension of global back ground inference (e.g. Ji and Grishman, 2008) to cross-media paradigm. Ji and Grishman (2008) enforce event role consistency across different documents. Finkel et al (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. A typical event extraction/discovery system (Ji and Grishman, 2008) fails to discover the war event due to the lack of context information (Benson et al, 2011), and thus fails to shed light on the users focus/interests. We have also attempted using the results from Dependency Parsing, Relation Extraction and Event Extraction tools (Ji and Grishman, 2008) to enrich the link types. Ji and Grishman (2008) extracts event mentions (belonging to a predefined list of target event types) and their associated arguments.
Bilingually-Constrained (Monolingual) Shift-Reduce Parsing Jointly parsing two languages has been shown to improve accuracies on either or both sides. However, its search space is much bigger than the monolingual case, forcing existing approaches to employ complicated modeling and crude approximations. Here we propose a much simpler monowhere a source-language parser learns to exploit reorderings as adobservation, but to build the target-side tree as well. We show specifically how to enhance a shift-reduce dependency parser with alignment features to resolve shift-reduce conflicts. Experiments on the bilingual portion of Chinese Treebank show that, with just 3 bilingual features, we can improve parsing accuracies by 0.6% (absolute) for both English and Chinese over a state-of-the-art with negligible efficiency overhead, thus much faster than biparsing. Finally, Huang et al (2009) use features, somewhat like QG configurations, on the shift-reduce actions in a monolingual, target language parser. The first consists of a joint segmentation and POS-tagging model (Zhang and Clark, 2010) and a word-based dependency parsing model using the arc-standard algorithm (Huang et al, 2009).     Please refer to page 573 of (Huang et al, 2009b) for more details about how to convert tree-to-string rules to SCFG rules. We follow Huang et al (2009b) to keep the probabilities of a natural rule unchanged and set those of a virtual rule to 1.  Huang et al (2009) presented a method to train a source-language parser by using the reordering information on words between the sentences on two sides. Huang et al (2009) proposed features based on reordering between languages for a shift-reduce parser.  Table 7 lists the results, where Huang 2009 refers to the result of Huang et al (2009), Chen2010BI refers to the result of using bilingual features in Chen et al (2010), and Chen2010ALL refers to the result of using all of the features in Chen et al (2010).  For the transition-based parsers, we used the arc-eager (ARCE) variant of the freely available MALT parser (Nivre et al,2006), and our own implementation of an arc standard parser (ARCS) as described in (Huang et al., 2009). The semantics of the system is described in (Huang et al, 2009). Fossum and Knight (2008) and Huang et al (2009) improve English prepositional phrase attachment using features from an unparsed Chinese sentence. In fact, it is worse than the deterministic parser of Huang et al (2009), which uses (almost) the same set of features.  Then we parse the English sentences to generate a string-to-dependency word-aligned corpus using the parser (Huang et al, 2009).
The Infinite PCFG Using Hierarchical Dirichlet Processes We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process (HDP). Our HDP-PCFG model allows the complexity of the grammar to grow as more training data is available. In addition to presenting a fully Bayesianmodel for the PCFG, we also develop an ef ficient variational inference procedure. Onsynthetic data, we recover the correct grammar without having to specify its complexity in advance. We also show that our tech niques can be applied to full-scale parsingapplications by demonstrating its effective ness in learning state-split grammars. We could use other word-based dependency trees such as trees by the infinite PCFG model (Liang et al, 2007) and syntactic-head or semantic-head dependency trees in Nakazawa and Kurohashi (2012), although it is not our major focus. However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al, 2007), or the number of adapted productions in the adaptor grammar (Johnson et al, 2007)) was not very large. For example, we can infer the number of nonterminals with a nonparametric Bayesian model (Liang et al., 2007), infer the model more robustly based on a Markov chain Monte Carlo inference (Johnson et al, 2007), and use probabilistic grammar models other than PCFGs.  While Liang et al (2007) demonstrated empirical gains on a synthetic corpus, our experiments focus on unsupervised category refinement on real language data. (General grammars with infinite numbers of nonterminals were studied by (Liang et al, 2007b)). In addition to the block sampler used by Bhattacharya and Getoor (2006), we are investigating general-purpose split merge samplers (Jain and Neal, 2000) and the permutation sampler (Liang et al, 2007a). However, because the latent variable grammars are not explicitly regularized, EM keeps fitting the training data and eventually begins over fitting (Liang et al, 2007).  We'd like to learn the number of paradigm classes from the data, but doing this would probably require extending adaptor grammars to incorporate the kind of adaptive state splitting found in the iHMM and iPCFG (Liang et al., 2007). It was introduced for Dirichlet process mixtures by Blei and Jordan (2005) and applied to infinite grammars by Liang et al (2007). First, we can let the number of nonterminals grow unboundedly, as in the Infinite PCFG, where the nonterminals of the grammar can be indefinitely refined versions of a base PCFG (Liang et al, 2007). Future work will involve a broader exploration of the parameter space of the adaptor grammars, in particular the number of topics and the value of alpha; a look at other non-parametric extensions of PCFGs, such as infinite PCFGs (Liang et al2007) for finding a set of non-terminals permitting more fine-grained topics.
A Probabilistic Earley Parser As A Psycholinguistic Model In human sentence processing, cognitive load can be defined many ways. This report considers a definition of cognitive load in terms of the total probability of structural options that have been disconfirmed at point in a sentence: the surprisal of word its prefix on a phrase-structural language model. These loads can be efficiently calculated using a probabilistic Earley parser (Stolcke, 1995) which is interpreted as generating predictions about reading time on a word-by-word basis. Under grammatical assumptions supported by corpusfrequency data, the operation of Stolcke’s probabilistic Earley parser correctly predicts processing phenomena associated with garden path structural ambiguity and with the subject/object relative asymmetry. Following Hale (2001) and Levy (2008), among others, the syntactic processor uses an incremental probabilistic Earley parser to compute a metric which correlates with increased reading difficulty. Using the prefix probability, we can compute the word-by-word Surprisal (Hale, 2001), by taking the log ratio of the previous word's prefix probability against this word's prefix probability. Since the introduction of a parser-based calculation for surprisal by Hale (2001), statistical techniques have been become common as models of reading difficulty and linguistic complexity. Surprisal (Hale, 2001) is then a straightforward calculation from the prefix probability. Both Hale (2001) and Levy (2008) used a Probabilistic Context Free Grammar (PCFG) as a language model in their implementations of surprisal theory. Hale (2001) pointed out that the ratio of the prefix probabilities. Hale (2001) suggests this quantity as an index of psycholinguistic difficulty. Probably the best known measure of syntactic expectation is surprisal (Hale 2001) which can be coarsely defined as the negative log probability of word wt given the preceding words, typically computed using a probabilistic context-free grammar. The original formulation of surprisal (Hale 2001) used a probabilistic parser to calculate these probabilities, as the emphasis was on the processing costs incurred when parsing structurally ambiguous garden path sentences. Hale (2001) reported that a probabilistic Earley parser can make correct predictions of garden-path effects and the subject/object relative asymmetry. Hale (2001) introduced the surprisal metric for probabilistic parsers, which measures the log ratio of the total probability mass at word t1 and word t. Hale (2001) showed that surprisal calculated from a probabilistic Earley parser correctly predicts well 356 known processing phenomena that were believed to emerge from structural ambiguities (e.g., garden paths) and Levy (2008) further demonstrated the relevance of surprisal to human sentence processing difficulty on a range of syntactic processing difficulty phenomena. The surprisal (Hale, 2001) at word wi refers to the negative log probability of wi given the preceding words, computed using the prefix probabilities of the parser. Hale (2001) demonstrated that surprisal thus predicts strong garden-pathing effects in the classic sentence. An alternative is to keep track of all derivations, and predict difficulty at points where there is a large change in the shape of the probability distribution across adjacent parsing states (Hale, 2001). This conclusion is strengthened when we turn to consider the performance of the parser on the standard Penn Treebank test set: the Within model showed a small increase in F-score over the PCFG baseline, while the copy model showed no such advantage.5All the models we proposed offer a broad coverage account of human parsing, not just a limited model on a hand-selected set of examples, such as the models proposed by Jurafsky (1996) and Hale (2001) (but see Crocker and Brants 2000). Although this does not match the interpretation of the experimental results followed in this paper, it leaves open the possibility that the feature could model the data according to a different measure of parsing difficulty, such as surprisal (Hale, 2001).
Three New Probabi l is t ic  Mode ls for Dependency  Parsing: An  Exploration* J ason  M.  E i sner CIS  Depar tment ,  Un ivers i ty  of  Pe lmsy lva i f ia .  Structures and rules for parsing with the (Eisner, 1996) algorithm. In that work, as here, inference is simply the Eisner first-order parsing model (Eisner, 1996) shown in Figure 2. In order to score higher-order features, each chart item maintains a list of signatures, which represent subtrees consistent with the chart item. context-free rules Charniak (1996) Collins (1996), Eisner (1996) context-free rules, headwords Charniak (1997) context-free rules, headwords, grandparent nodes Collins (2000) context-free rules, headwords, grandparent nodes/rules, bi grams, two-level rules, two-level bi grams, non headwords Bod (1992) all fragments within parse trees Scope of Statistical Dependencies Model Figure 4. Viterbi decoding is done using Eisner's algorithm (Eisner, 1996). 1 of these edges, using an O (n3) dynamic programming algorithm (Eisner, 1996) for projective trees. But the denominator Zi is a normalizing constant that sums over all parses; it is found by a dependency-parsing variant of the inside algorithm, following (Eisner, 1996). In a slightly more general formulation, it was first published by Eisner (1996). For MST Parser, we use 1st order features and a projective decoder (Eisner, 1996). It combines online Peceptron learning (Collins, 2002) with a parsing model based on the Eisner algorithm (Eisner, 1996), extended so as to jointly assign syntactic and semantic labels. The child nodes for a given parent are represented in a head-outward fashion such that the left and right children are separate lists, with the left and right-most elements as the last members of their respective lists, as in most generative dependency models (Eisner, 1996). Eisner (1996) algorithm with non-projective rewriting and second order features. Examples of this include McDonald and Pereira's (2006) rewriting of projective trees produced by the Eisner (1996) algorithm, and Nivre and Nilsson's (2005) pseudo projective approach that creates projective trees with specially marked arcs that are later transformed into non-projective dependencies. Descriptive dependency labels. The best projective parse tree is obtained using the Eisner algorithm (Eisner, 1996) with the scores, and the best non-projective one is obtained using the Chu Liu-Edmonds (CLE) algorithm (McDonald et al, 2005b). Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O (n3) time. Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al (2005). This formalization generalizes standard projective parsing models based on the Eisner algorithm (Eisner, 1996) to yield efficient O (n2) exact parsing methods for non projective languages like Czech. It is well known that projective dependency parsing using edge based factorization can be handled with the Eisner algorithm (Eisner, 1996). The (Eisner, 1996) algorithm is typically used for projective parsing. In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The Eisner (1996) algorithm and its variants are commonly used in data-driven dependency parsing.
Arabic Preprocessing Schemes For Statistical Machine Translation In this paper, we study the effect of different word-level preprocessing decisions for Arabic on SMT quality. Our results show that given large amounts of training data, splitting off only proclitics performs best. However, for small amounts of training data, it is best to apply English-like tokenization using part-of-speech tags, and sophisticated morphological analysis and disambiguation. Moreover, choosing the appropriate preprocessing produces a significant increase in BLEU score if there is a change in genre between training and test data. Similarly to Habash and Sadat (2006), the set of schemes we explore are all word-level. This preprocessing technique we use here is the best performer amongst other explored techniques presented in Habash and Sadat (2006). Morphological segmentation has been shown to benefit Arabic-to-English (Habash and Sadat, 2006) and English-to-Arabic (Badr et al, 2008) translation, although the gains tend to decrease with increasing training data size. Similarly, for Arabic-to-English, Lee (2004), and Habash and Sadat (2006) show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size. Furthermore, it is not necessarily optimal in that (i) manually engineered segmentation schemes can outperform a straightforward linguistic morphological segmentation, e.g., (Habash and Sadat, 2006), and (ii) it may result in even worse performance than a word-based system, e.g., (Durgar El-Kahlout and Oflazer, 2006).  Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. The MADA toolkit (Habash and Sadat, 2006) was used to perform Arabic morphological word decomposition and part-of-speech tagging. We use tokenisation scheme, which splits certain prefixes and has been reported to improve machine translation performance (Habash and Sadat, 2006). More aggressive segmentation results in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006). Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes.  For preprocessing, a similar approach to that shown in Habash and Sadat (2006) was employed, and the MADA+TOKAN system for disambiguation and tokenization was used. Recent research on handling rich morphology has largely focused on translating from rich morphology languages, such as Arabic, into English (Habash and Sadat, 2006). A morphological analysis of the Arabic text is then done using the Arabic morphological analyzer and disambiguation tool MADA (Nizar Habash and Roth, 2009), with the MADA-D2 since it seems to be the most efficient scheme for large data (Habash and Sadat, 2006). On different language pairs, (Koehn and Knight, 2003) and (Habash and Sadat, 2006) showed that data-driven methods for splitting and preprocessing can improve Arabic-English and German-English MT. Only for the Machine Translation task, (Habash and Sadat, 2006) report several results using different Arabic segmentation schemes. The Arabic text was preprocessed according to the D2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. The BLEU score improves uniformly, although the improvements are most dramatic for smaller datasets, which is consistent with previous work (Habash and Sadat, 2006).
A New Dataset and Method for Automatically Grading ESOL Texts We demonstrate how supervised discriminative machine learning techniques can be used to automate the assessment of ‘English as a Second or Other Language’ (ESOL) examination scripts. In particular, we use rank preference learning to explicitly model the grade relationships between scripts. A number of different features are extracted and ablation tests are used to investigate their contribution to overall performance. A comparison between regression and rank preference models further supports our method. Experimental results on the first publically available dataset show that our system can achieve levels of performance close to the upper bound for the task, as defined by the agreement between human examiners on the same corpus. Finally, using a set of ‘outlier’ texts, we test the validity of our model and identify cases where the model’s scores diverge from that of a human examiner. A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al, 2003), and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones (Yannakoudakis et al, 2011). Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in Yannakoudakis et al. (2011). Also, in Yannakoudakis et al (2011), experiments are presented that test the validity of the system using a number of automatically-created 'outlier' texts. We use the First Certificate in English (FCE) ESOL examinationscripts2 (upper-intermediate level assessment) described in detail in Yannakoudakis et al (2011), extracted from the Cambridge Learner Corpus3 (CLC). As in Yannakoudakis et al (2011), we analyze all texts using the RASP toolkit (Briscoe et al, 2006). Among the features used in Yannakoudakis et al (2011), none explicitly captures coherence and none models inter sentential relationships. In the following experiments, we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in Yannakoudakis et al (2011) to report results of the final best system.  See Yannakoudakis et al (2011) for details. Their correction detection algorithm relies on a set of heuristics developed from one single data collection (the FCE corpus (Yannakoudakis et al, 2011)). We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al, 2011), NUCLE corpus (Dahlmeier et al, 2013), UIUCcor pus 2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). The CLC-FCE sub corpus was extracted, anonymized, and made available as a set of XML files by Yannakoudakis et al (2011). Errors have been shown to have a significant impact on predicting learner level (Yannakoudakis et al, 2011). By running ablation studies - i.e., removing one or more sets of features (cf. e.g. (Yannakoudakis et al., 2011)) - we can determine their relative importance and usefulness. We extend our n-gram-based data-driven prediction approach from the Helping Our Own (HOO) 2011 Shared Task (Boyd and Meurers, 2011) to identify determiner and preposition errors in non-native English essays from the Cambridge Learner Corpus FCE Dataset (Yannakoudakis et al, 2011) as part of the HOO 2012 Shared Task. The documents are a subset of the 1,244 document sin the Cambridge Learner Corpus FCE (First Certificate in English) data set (Yannakoudakis et al, 2011). A suitable corpus for developing this program is the Cambridge Learner Corpus (CLC) (Yannakoudakis et al, 2011). The HOO development dataset consists of 1000 exam scripts drawn from a subset of the CLC FCE Dataset (Yannakoudakis et al, 2011). For the shared task, we made use of data drawn from the CLC FCE Dataset, a set of 1,244 exam scripts written by candidates sitting the Cambridge ESOL First Certificate in English (FCE) examination in 2000 and 2001, and made available by Cambridge Universiy Press; see (Yannakoudakis et al, 2011). We also apply our visualiser to a set of 1,244 publically available FCE ESOL texts (Yannakoudakis et al, 2011) and make it available as a web service to other researchers.
Class-Based Probability Estimation Using A Semantic Hierarchy This article concerns the estimation of a particular kind of probability, namely, the probability of a noun sense appearing as a particular argument of a predicate. In order to overcome the accompanying sparse-data problem, the proposal here is to define the probabilities in terms of senses from a semantic hierarchy and exploit the fact that the senses can be grouped into classes consisting of semantically similar senses. There is a particular focus on the problem of how to determine a suitable class for a given sense, or, alternatively, how to determine a suitable level of generalization in the hierarchy. A procedure is developed that uses a chi-square test to determine a suitable level of generalization. In order to test the performance of the estimation method, a pseudo-disambiguation task is used, together with two alternative estimation methods. Each method uses a different generalization procedure; the first alternative uses the minimum description length principle, and the second uses Resnik’s measure of selectional preference. In addition, the performance of our method is investigated using both the standard Pearson chisquare statistic and the log-likelihood chi-square statistic. Determining the appropriate level of generalization for a noun is an open problem (e.g., Clark and Weir, 2002). Clark and Weir (2002) investigate the task of generalizing a single relation concept pair. Clark and Weir (2002) also find an appropriate set of concept nodes to represent the selectional preferences for a verb, but do so using a test over corpus frequencies mapped to concepts to determine when to generalize from a node to its parent. Other models also relying on the WordNet resource include Abe and Li (1996) and Clark and Weir (2002). Calling the generalization problem a case of engineering in the face of sparse data, Clark and Weir (2002) looked at a number of previous methods, one conclusion being that the approach of Li and Abe appears to over-generalize. Since we wish to evaluate the strength of our method alone without any additional NLP effort, we bypass the issue of approximating the true distribution of the concepts via word sense disambiguation or class based approximation methods, such as those by Li and Abe (1998) and Clark and Weir (2002). Finding a generalization of a profile is explored in the works of Clark and Weir (2002) and Li and Abe (1998). Methods for the induction of semantically inspired word clusters have been widely used in language modeling and lexical acquisition tasks (e.g. (Clark and Weir, 2002. There is scope for experimenting with other approaches such as (Clark and Weir, 2002), however, we feel a type-based approach is worthwhile to avoid the noise introduced from frequent but polysemous arguments and bias from highly frequent arguments which might be part of a multiword rather than a prototypical argument of the predicate in question, for example eat hat. Further comparison of WNPROTOs and DSPROTOs to other WordNet models are warranted to contrast the effect of our proposal for disambiguation using word types with iterative approaches, particularly those of Clark and Weir (2002). Clark and Weir (2002) present a model that, while not explicitly described as cut-based, likewise seeks to find the right level of generalisation for an observation. In order to compare against previously proposed selectional preference approaches based on WordNet we also reimplemented the methods that performed best in the evaluation of Brockmann and Lapata (2003): Resnik (1993) and Clark and Weir (2002). Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001). Pseudo-disambiguation was introduced by Clark and Weir (2002) to evaluate models of selectional preference. We follow the approach by Clark and Weir (2002) to create the test data. Briefly, Clark and Weir (2002) populate the WordNet hierarchy based on corpus frequencies (of all nouns for a verb/slot pair), and then determine the appropriate probability estimate at each node in the hierarchy by using chi square to determine whether to generalize an estimate to a parent node in the hierarchy. It is worth noting that the method of Clark and Weir (2002) does not yield a tree cut, but instead generally populates the WordNet hierarchy with non-zero probabilities. We evaluate the SPD method on sense profiles created using the method of Clark and Weir (2002), with comparison to the other distance measures (skew and cos) as explained above. In an approach inspired by the works of Li and Abe (1998) and Clark and Weir (2002), McCarthy and Carroll use grammatically connected words from a corpus to induce a distribution of senses over subtrees in the WordNet hierarchy. SPs can help resolve syntactic, word sense, and reference ambiguity (Clark and Weir, 2002), and so gathering them has received a lot of attention in the NLP community.
Machine Translation Divergences: A Formal Description and Proposed Solution Bonnie J. Dorr* University of Maryland There are many cases in which the natural translation of one language into another esults in a very different form than that of the original.  A similar approach as been adopted in generation (Bateman, 1997), (Bateman et al., 1991) and in machine translation most notably in (Dorr, 1994). Using these simple, language agnostic measures allows one to look for divergence types such as those described by Dorr (1994). However, real bitexts generally do not exhibit parse-tree isomorphism, whether because of systematic differences between how languages express a concept syntactically (Dorr, 1994). We suggest that alignment constraints such as this one can be used to define most of the possible syntactic divergences between languages (Dorr, 1994),. Theoretically, it is well-known that two languages often do not express the same meaning in the same way (Dorr, 1994). One of the reasons for this difference is due to the different language pairs under study; (Meyers et al, 1998) deals with two languages that are closely related syntactically (Spanish and English) while we are dealing with languages that syntactically are quite divergent, Korean and English (Dorr, 1994). We suggest that alignment constraints such as this one can be used to define most of the possible syntactic divergences between languages (Dorr, 1994), and that only a handful of them are necessary for two given languages (we have identified 11 general alignment constraints necessary for Korean to English transfer so far). Dorr (1994) categorizes sources of syntactic divergence between languages. Yet, we can not preclude divergence from translational correspondence; on the contrary, it occurs routinely and to a certain extent systematically (Dorr, 1994). However, structural divergences between languages (Dorr, 1994) which are due to either systematic differences between languages or loose translations in real corpora pose a major challenge to syntax-based statistical MT. Dorr (1994) presents some major lexical-semantic divergence problems applicable in this scenario: (a) Thematic Divergence In some cases, although there exists semantic parallelism, the theme of the English sentence captured in the subject changes into an object in the Urdu sentence. Of particular relevance to MT is the issue of structural divergence (Dorr, 1994). English to French Lexical-Structural Transfer Rule with Verb Modifier ALMOST More details on how the structural divergences described in (Dorr, 1994) can be accounted for using our formalism can be found in (Nasr et al., 1998). The syntax based statistical approaches have been faced with the major problem of pervasive structural divergence between languages, due to both systematic differences between languages (Dorr, 1994) and the vagaries of loose translations in real corpora. However, previous work in machine translation leads us to believe that transferring the correlations between syntax and semantics across languages would be problematic due to argument structure divergences (Dorr, 1994).
Inducing Multilingual POS Taggers And NP Bracketers Via Robust Projection Across Aligned Corpora E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of tagging. Linguistics, (Yarowsky and Ngai, 2001) aim at pos tagging a target language corpus using English pos tags as well as estimation of lexical priors. Having said this, we follow in principle the algorithm proposed by (Yarowsky and Ngai, 2001) to estimate lexical priors. However, as was noted by (Yarowsky and Ngai, 2001), most words tend to have at most two pos. (Yarowsky and Ngai, 2001) propose the same algorithm as the one proposed here for their estimation of lexical priors, with the exception that they use automatic word alignments rather than our extraction algorithm for finding corresponding words. As for (Yarowsky and Ngai, 2001) estimating lexical priors is merely an intermediate step, they do not report evaluation results for this step. Moreover, the success of joint bilingual learning may lend itself to many inherent multilingual NLP tasks such as POS tagging (Yarowsky and Ngai, 2001), name entity recognition (Yarowsky et al, 2001). Our work is closest to that of Yarowsky and Ngai (2001), but differs in two important ways. This can be seen as a rough approximation of Yarowsky and Ngai (2001). Early studies of cross-lingual annotation projection were accomplished for lexically-based tasks; for example part-of-speech tagging (Yarowsky and Ngai, 2001). The first to explore the idea were Yarowsky and Ngai (2001), who induced a part-of-speech tagger for French and base noun phrase detectors for French and Chinese via transfer from English resources. Yarowsky and Ngai (2001) were the first to propose the use of parallel texts to bootstrap the creation of taggers.  In this case alignments such as English laws (NNS) to Frenchles (DT )lois (NNS) would be expected (Yarowsky and Ngai, 2001).  Given that we have a parallel corpus where the German side overtly realizes T and V, this is a classical case of annotation projection (Yarowsky and Ngai, 2001). A technique known as annotation projection (Yarowsky and Ngai, 2001) provides a means to relax this resource bottleneck to some extent. Previous research on resource projection attempts to address these problems by redistributing the parameter values (Yarowsky and Ngai, 2001) or by applying transformation rules (Hwa et al, 851 2002). Following the work of Yarowsky and Ngai (2001) we focus on the task of training a Part-of-Speech (POS) tagger, but we conduct our experiments with the more dissimilar language pair of English Chinese instead of English-French. One method of acquiring a large corpus of automatically POS tagged Chinese data is by projection (Yarowsky and Ngai, 2001). Following Yarowsky and Ngai (2001), we define 12 equivalence classes over the 47 Penn-English Treebank POS tags.
Synchronous Binarization For Machine Translation Systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine translation output, but are often very computationally intensive. The complexity is exponential in the size of individual grammar rules due to arbitrary re-orderings between the two languages, and rules extracted from parallel corpora can be quite large. We devise a linear-time algorithm for factoring syntactic re-orderings by binarizing synchronous rules when possible and show that the resulting rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system. The decoder uses a binarized representation of the rules, which is obtained via a syncronous binarization procedure (Zhang et al., 2006). For arbitrary SCFGs, this is typically accomplished via the synchronous binarization technique of (Zhang et al, 2006). We show that this approach gives us better practical performance than a mature system that binarizes using the technique of (Zhang et al, 2006). For SCFG grammars, (Zhang et al, 2006) provide a scope reduction method called synchronous binarization with quantifiable loss. Their principal objective is to provide a scope reduction method for SCFG that introduces fewer postconditions than (Zhang et al, 2006). However unlike (Zhang et al, 2006), their method only addresses simple grammars. Binarizing the grammars (Zhang et al, 2006) further increases the size of these sets, due to the introduction of virtual nonterminals. Rule size and lexicalization affect parsing complexity whether the grammar is binarized explicitly (Zhang et al, 2006) or implicitly binarized using Early-style intermediate symbols (Zollmann et al, 2006). Synchronous binarization (Zhang et al, 2006) solves this problem by simultaneously binarizing both source and target-sides of a synchronous rule, making sure of contiguous spans on both sides whenever possible. Intuitively speaking, the gaps on the target-side will lead to exponential complexity in decoding with integrated language models (see Section 3), as well as synchronous parsing (Zhang et al, 2006). This representation, being contiguous on both sides, successfully reduces the decoding complexity to a low polynomial and significantly improved the search quality (Zhang et al, 2006). Although according to Zhang et al (2006), the vast majority (99.7%) of rules in their Chinese-English dataset are binarizable, there do exist some interesting cases that are not (see Figure 2 for a real-data example). We used a bottom-up, CKY-style decoder that works with binary xRs rules obtained via a synchronous binarization procedure (Zhang et al, 2006). Most practical non-binary SCFGs can be binarized using the synchronous binarization technique by Zhang et al (2006). Zhang et al (2006) discuss methods for binarizing SCFGs, ignoring the nonbinarizable grammars. A CYK-style decoder has to rely on binarization to preprocess the grammar as did in (Zhang et al, 2006) to handle multi-nonterminal rules. Many solutions to the reordering problem have been proposed, e.g. syntax-based models (Chiang, 2005), lexicalized reordering (Och et al, 2004), and tree-to-string methods (Zhang et al, 2006). Compared with its string-based counterparts, tree-based decoding is simpler and faster: there is no need for synchronous binarization (Huang et al, 2009b; Zhang et al, 2006) and tree parsing generally runs in linear time (Huang et al, 2006). As tree-to-string rules usually have multiple non-terminals that make decoding complexity generally exponential, synchronous binarization (Huang et al, 2009b; Zhang et al, 2006) is a key technique for applying the CKY algorithm to parsing with tree-to-string rules. In our string-to-tree model, for efficient decoding with integrated n-gram LM, we follow (Zhang et al, 2006) and inversely binarize all translation rules into Chomsky Normal Forms that contain at most two variables and can be incrementally scored by LM.
A Trainable Rule-Based Algorithm For Word Segmentation This paper presents a trainable rule-based algorithm for performing word segmentation. The algorithm provides a simple, language-independent alternative to large-scale lexical-based segmenters requiring large amounts of knowledge engineering. As a stand-alone segmenter, we show our algorithm to produce high performance Chinese segmentation. In addition, we show the transformation-based algorithm to be effective in improving the output of several existing word segmentation algorithms in three different languages. Palmer (1997) conducted a Chinese segmenter which merely made use of a manually segmented corpus (without referring to any lexicon). In general, the word segmentation program utilizes the word entries, part-of-speech (POS) information (Chen and Liu, 1992) in a monolingual dictionary, segmentation rules (Palmer, 1997), and some statistical information (Sproat, et al, 1994).  This approach was used by Palmer (1997) for word segmentation. Our work adds an existing system to improve the rules learned, while Palmer (1997) adds rules to improve an existing system's performance. One may note that the error reductions here are smaller than Palmer (1997)'s error reductions. In Palmer (1997), the baseline is how well an existing system performs before the rules are run. If we were to use the same baseline as Palmer (1997), our baseline would be an F of 37.5% for IaB and 52.6% for IaC. For example, (Palmer, 1997) developed a Chinese word segmenter using a manually segmented corpus. Palmer (1997) uses transform-based learning (TBL) to correct an initial segmentation. In a later paper, Palmer (1997) presents a transformation-based algorithm, which requires pre-segmented training data. The use of TBL for Chinese word segmentation was first suggested in Palmer (1997).
Simple Coreference Resolution with Rich Syntactic and Semantic Features Coreference systems are driven by syntactic, semantic, and discourse constraints. We present a simple approach which completely modularizes these three aspects. In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus. Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones. Primary contributions include (1) the presentation of a simpleto-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems). In general, coreference errors in state-of-the art systems are frequently due to poor models of semantic compatibility (Haghighi and Klein, 2009). In addition to the above, if a mention is in a deterministic coreference configuration, as defined in Haghighi and Klein (2009), we force it to take the required antecedent. For r, we use a deterministic entity assignment Zr, similar to the Haghighi and Klein (2009)'s SYN-CONSTR setting: each referring mention is coreferent with any past mention with the same head or in a deterministic syntactic configuration (appositives or predicative nominatives constructions).  All systems except Haghighi and Klein (2009) and current work are fully supervised. We also compared to the strong deterministic system of Haghighi and Klein (2009). Haghighi and Klein (2009) reports on true mentions; here, we report performance on automatically detected mentions. We compared our output to the deterministic system of Haghighi and Klein (2009). The news have been processed with a tokenizer, a sentence splitter (Gillick and Favre, 2009), a part-of-speech tagger and dependency parser (Nivre, 2006), a co-reference resolution module (Haghighi and Klein, 2009) and an entity linker based on Wikipedia and Freebase (Milneand Witten, 2008). Specifically, when searching for an antecedent for mk, its candidate antecedents are visited in an order determined by their positions in the associated parse tree (Haghighi and Klein, 2009). For Chinese, we handle role appositives as introduced by Haghighi and Klein (2009) analogously.  This was what (Haghighi and Klein, 2009) did and we did this in training with the REUTERS corpus (Hasler et al, 2006) in which syntactic roles are annotated.  The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009).  We start by preprocessing all the news in the news collections with a standard NLP pipeline: tokenization and sentence boundary detection (Gillick, 2009), part-of-speech tagging, dependency parsing (Nivre, 2006), coreference resolution (Haghighi and Klein, 2009) and entity linking based on Wikipedia and Freebase. ARKref is a syntactically rich, rule-based within-document coreference system very similar to (the syntactic components of) Haghighi and Klein (2009). We implemented an algorithm for the task described above which was inspired by the work of Haghighi and Klein (2009). F1-scores could range between 39.8 and 67.3 for various methods and test sets (Haghighi and Klein, 2009).
VerbOcean: Mining The Web For Fine-Grained Semantic Verb Relations Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks. We present a semi-automatic method for extracting fine-grained semantic relations between verbs. We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web. On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5% accuracy. Analysis of types shows that on the relation achieved 75% accuracy. We provide the called for download at LSA Match: v and M (v) are distributionally similar according to a freely available Latent Semantic Indexing package,2 or for verbs similar according to VerbOcean (Chklovski and Pantel, 2004). We will consider to use a supervised learning approach, as well as the similar features employed for temporal relation classification task, in addition to lexical information (e.g. WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004)) and the existing causal signals. For event terms, we first find the root verbs of deverbal nouns and then measure verb similarity by using the fine-grained relations provided by VerbOcean (Chklovski and Pantel, 2004), which has proved useful in summarization (Liu et al, 2007). In particular, we could emulate the approach used in VerbOcean (Chklovski and Pantel 2004). The work on VerbOcean is similar to our research in the use of the Web for acquiring relationships (Chklovski and Pantel, 2004). Work on semantic similarity learning such as Chklovski and Pantel (2004) also automatically learns relations between verbs. For example, Chklovski and Pantel (2004) loosely define ENABLEMENT as a relation that holds between two verbs V1 and V2 when the pair can be glossed as V1 is accomplished by V2 and gives two examples: assess: :review and accomplish: :complete. Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based classifiers for hy ponyms (Hearst, 1992) ,meronyms (Girju, 2003), synonyms (Lin et al, 2003), a variety of verb relations (Chklovski and Pantel, 2004), and general purpose analogy relations (Turney et al, 2003). Chklovski and Pantel (2004) used patterns like 'x-ed by y-ing' ('obtained by borrowing') to get co-occurrence data on candidate pairs from the Web. Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. This metric allows to match synonym predicates by using verb ontologies such as VerbNet (Schuler, 2006) and VerbOcean (Chklovski and Pantel, 2004) and distributional semantics similarity metrics, such as Dekang Lin's thesaurus (Lin, 1998), where previous semantic metrics only perform exact match of predicate structures and arguments. Work was also done on relations be tween verbs (Chklovski and Pantel, 2004). Hence, when exploring very specific relation ship types or very generic, but not widely accepted, types (like verb strength), many researchers resort to manual human-based evaluation (Chklovski and Pantel, 2004). 30 relations are noun compound relationships as proposed in the (Nastase and Szpakowicz, 2003 ) classification scheme, and 5 relations are verb-verb relations proposed by (Chklovski and Pantel, 2004). Other types of relations that have been studied by pattern-based approaches include question answer relations (such as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al, 2003), general purpose analogy (Turney et al, 2003), verb relations (including similarity, strength, antonym, enable ment and temporal) (Chklovski and Pantel, 2004), entailment (Szpektor et al, 2004), and more specific relations, such as purpose, creation (Cimiano and Wenderoth, 2007), LivesIn, and EmployedBy (Bunescu and Mooney, 2007). We introduce VerbOcean (Chklovski and Pantel, 2004), a broad-coverage repository of semantic verb relations, into event-based summarization. Chklovski and Pantel (2004) address the automatic acquisition of verb-verb pairs and their relations from the web. An ablation study that formed part of the official RTE 5 evaluation attempted to evaluate the contribution of publicly available knowledge resources such as WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), and DIRT (Lin and Pantel, 2001) used by many of the systems. The latter utilized several resources for matching hypothesis terms with text terms: WordNet, VerbOcean (Chklovski and Pantel, 2004), utilizing two of its relations, as well as an acronym database ,number matching module, co-reference resolution and named entity recognition tools. Chklovski and Pantel (2004) used patterns to extract a set of relations between verbs, such as similarity, strength and antonymy.
Automatic Rule Induction For Unknown-Word Guessing Words unknown to the lexicon present a substantial problem to NLP modules that rely on morphosyntactic information, such as part-of-speech taggers or syntactic parsers. In this paper we present a technique for fully automatic acquisition of rules that guess possible part-of-speech tags for unknown words using their starting and ending segments. The learning is performed from a general-purpose lexicon and word frequencies collected from a raw corpus. Three complimentary sets of word-guessing rules are statistically induced: prefix morphological rules, suffix morphological rules and ending-guessing rules. Using the proposed technique, unknown-word-guessing rule sets were induced and integrated into a stochastic tagger and a rule-based tagger, which were then applied to texts with unknown words. One could also expand the lexicon, by adapting algorithms for analyzing unknown words (e.g., Mikheev, 1997). We have used LTPOS (Mikheev, 1997), which performed the task almost error less. Step 6 contains a call to the other main LT TTT program, LTOPS (Mikheev, 1997), which performs both sentence identification and POS tagging. The search for such rules has previously been conducted in the context of supervised part-of-speech tagging (Mikheev, 1997). Tagging and chunking is done by a standard tagger and chunker, LTPos (Mikheev,1997). The other main LT TTT program is ltpos, a statistical combined part-of-speech (POS) tagger and sentence boundary disambiguation module (Mikheev, 1997). The other main LT TTT program is ltpos, a statistical combined part-of-speech (POS) tagger and sentence identifier (Mikheev, 1997). PoS tagging can be performed using LTPOS (Mikheev, 1997). A common approach is to extract word-internal features from unknown words, for example suffix, capitalization, or punctuation features (Mikheev, 1997). the possible part(s)-of-speech of unknown words (Mikheev,1997). The other main LT TTT program is ltpos, a statistical combined part-of-speech (POS) tagger and sentence boundary disambiguation module (Mikheev, 1997). The next stage in the linguistic analysis module performs noun group and verb group chunking using fsg match with the specialised hand-written rule sets which were the core part of LT CHUNK (Finch and Mikheev, 1997). The stripping-recoding rules could be manually encoded, mined from a monolingual corpus usinga learning method such as (Mikheev, 1997), or supplied by a source terminology extraction system that handles morphological variations. Following Mikheev (1997), we therefore adjust reliability using lower confidence limit statistics. The identification of sentence boundaries, mark-up of sentence elements and POS tagging is done by the statistical program lt pos (Mikheev, 1997). Taggers based on Hidden Markoff Model (HMM) technology currently appear to be in the lead. The prime public domain examples of such implementations include the Trigrams'n'Tags tagger (Brandts 2000), Xerox tagger (Cutting et al. 1992) and LT POS tagger (Mikheev 1997). The LT POS tagger is reported to perform at 93.6-94.3% accuracy on known words and at 87.7-88.7% on unknown words using a cascading unknown word 'guesser' (Mikheev, 1997). Mikheev (1997) suggested a guessing-rule technique, based on prefix morphological rules ,suffix morphological rules, and ending-guessing rules. Transitions to the normal and pivotal stage occur when an estimator of the relative frequency is high enough, for example by taking the lower bound of the confidence interval (Mikheev,1997). Lexical knowledge (for unknown words) and the word lemma (for known words) provide, w.h.p, one sided error (Mikheev, 1997).
Semantic Taxonomy Induction From Heterogenous Evidence We propose a novel algorithm for inducing semantic taxonomies. Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns. By contrast, our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy, using knowledge of a word’s coordinate terms to help in determining its hypernyms, and vice versa. We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition, where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantaxonomy (WordNet 2.1). We add to WordNet 2.1 at a relaerror reduction of a non-joint algorithm using the same component classifiers. Finally, we show that a taxonomy built using our algorithm shows a 23% relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs. Recently, Snow, Jurafsky and Ng (2005) generated tens of thousands of hypernym patterns and combined these with noun clusters to generate high-precision suggestions for unknown noun insertion into WordNet (Snow et al, 2006). Following Snow et al (2006), we derive two types of evidence from these patterns: H is a hypernym of A, B and C, A, B and C are siblings of each other.  Obviously, all these semantic resources have been acquired using a very different set of processes (Snow et al, 2006), tools and corpora. We also compare ASIA on twelve additional benchmarks to the extended Wordnet 2.1 produced by Snow et al (Snow et al, 2006), and show that for these twelve sets, ASIA produces more than five times as many set instances with much higher precision (98% versus 70%). Snow et al (Snow et al, 2006) use known hypernym / hyponym pairs to generate training data for a machine-learning system, which then learns many lexico-syntactic patterns. Snow (Snow et al, 2006) has extended the Word Net 2.1 by adding thousands of entries (synsets) at a relatively high precision.  Snow et al (2006) add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Options for identifying interesting classes include manually created methods (WordNet (Miller et al, 1990)), textual patterns (Hearst, 1992), automated clustering (Lin and Pantel, 2002), and combinations (Snow et al, 2006). The work by Snow et al (2006) is the most similar to ours because they also took an incremental approach to construct taxonomies. We compare system performance between (Snow et al, 2006) and our framework in Section 5. To have a fair comparison, for PR, we estimate the conditional probability of a relation given the evidence P (Rij|Eij), as in (Snow et al 2006), by using the same set of features as in ME. An extension to WordNet was presented by (Snow et al, 2006). Snow et al (2006) use syntactic path patterns as features for supervised hyponymy and synonymy classifiers, whose training examples are derived automatically from WordNet. Following the spirit of the fine-grained human evaluation in (Snow et al, 2006), we randomly sampled 800 rules from our rule-base and presented them to an annotator who judged them for correctness, according to the lexical reference notion specified above. We observed that the likelihood of nouns mentioned in a definition to be referred by the concept title depends greatly on the syntactic path connecting them (which was exploited also in (Snow et al, 2006)). For example, (Snow et al 2006) proposed to estimate taxonomic structure via maximizing the overall likelihood of a taxonomy. More recently, Snow et al (2005) and Snow et al (2006) have described a method of hypernymy extraction using machine learning of 53 patterns. Due to the importance of WN for NLP tasks, substantial research was done on direct or indirect automated extension of the English WN (e.g., (Snow et al, 2006)) or WN in other languages (e.g., (Vintar and Fiser, 2008)).
Learning to Extract Relations from the Web using Minimal Supervision We present a new approach to relation extraction that requires only a handful of training examples. Given a few pairs of named entities known to exhibit or not exhibit a particular relation, bags of sentences containing the pairs are extracted from the web. We extend an existing relation extraction method to handle this weaker form of supervision, and present experimental results demonstrating that our approach can reliably extract relations from web documents. In this section, we show that many relationships are consistently expressed using a compact set of relation-independent lexico-syntactic patterns, and quantify their frequency based on a sample of 500 sentences selected at random from an IE training corpus developed by (Bunescu and Mooney, 2007). The first two datasets were collected from the Web, and made available by Bunescu and Mooney (2007). To resolve this problem, Bunescu and Mooney (2007), Riedel et al (2010) and Yao et al (2010) relaxed the DS assumption to the at least-one assumption and employed multi-instance learning techniques to identify wrongly labeled instances. Such data sets have been utilized successfully for relation extraction from the web (Bunescu and Mooney, 2007). Bunescu and Mooney (2007) follow a classification-based approach to RE. One approach for taxonomy deduction is to use explicit expressions (Iwaska et al, 2000) or lexical and semantic patterns such as is a (Snow et al, 2004), similar usage (Kozareva et al, 2008), synonyms and antonyms (Lin et al, 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. We used the dataset by Bunescu and Mooney (2007), which we selected because it contains multiple realizations of an entity pair in a target semantic relation, unlike similar datasets such as the one by Roth and Yih (2002). One heuristic is to assume that each candidate mention tuple of a training fact is indeed expressing the corresponding relation (Bunescu and Mooney, 2007). Notable exceptions include Rosario and Hearst (2005) and Bunescu and Mooney (2007), who tackle relation classification and extraction tasks by considering the set of contexts in which the members of a candidate relation argument pair co-occur. The dataset was built following the approach of Bunescu and Mooney (Bunescu and Mooney, 2007). Bunescu and Mooney (2007) presented an approach to extract relations from the Web using minimal supervision. Bunescu and Mooney (2007) connect weak supervision with multi-instance learning and extend their relational extraction kernel to this context.  Bunescu and Mooney (2007) and Riedel et al (2010) model distant supervision for relation extraction as a multi-instance single-label problem, which allows multiple mentions for the same tuple but disallows more than one label per object. As pointed out by (Bunescu and Mooney, 2007), even though the same entities co-occur in multiple sentences, they are not necessarily linked by the same relationship in all of them. One means of combating this is suggested by (Bunescu and Mooney, 2007).
Understanding the Value of Features for Coreference Resolution In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model. This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models – which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features.  To achieve roughly state-of-theart performance, RECONCILEACL09 employs a fairly comprehensive set of 61 features introduced in previous coreference resolution systems (see Bengtson and Roth (2008)). Bengtson and Roth (2008) simply discard twinless CEs, but this solution is likely too lenient - it doles no punishment for mistakes on twinless annotated or extracted CEs and it would be tricked, for example, by a system that extracts only the CEs about which it is most confident.   Inaddition, the architecture and system components of Reconcile (including a comprehensive set of features that draw on the expertise of state-of-the-art supervised learning approaches, such as Bengtson and Roth (2008)) result in performance closer to the state-of-the-art.  Building on elements of the coreference system described in Bengtson and Roth (2008), we design an end-to-end system (Sec. 2) that identifies candidate mentions and then applies one of two inference protocols, Best-Link and All-Link (Sec. 2.3), to disambiguate and cluster them. Illinois-Coref follows the architecture used in Bengtson and Roth (2008). For the ACE 2004 coreference task, a good performance in mention detection is typically achieved by training a classifier e.g., (Bengtson and Roth, 2008). We use the same features as Bengtson and Roth (2008), with the knowledge extracted from the OntoNotes-4.0 annotation. Although its strategy is simple, Bengtson and Roth (2008) show that with a careful design, it can achieve highly competitive performance. For example, a memorization feature is a word pair composed of the head nouns of the two NPs involved in an instance (Bengtson and Roth, 2008). For an empirical evaluation of the contribution of a subset of these features to the mention-pair model, see Bengtson and Roth (2008).  We used the state-of-the-art coreference resolution system of (Bengtson and Roth, 2008) to identify the canonical entities for pronouns and extract features accordingly. Note that we solve the above Best-Link inference using an efficient algorithm (Bengtson and Roth, 2008) which runs in time quadratic in the number of mentions. The baseline system applies the strategy in (Bengtson and Roth, 2008, Section 2.2) to learn the pairwise scoring functions using the Averaged Perceptron algorithm. Among them the mention pair model (McCarthy and Lehnert, 1995) is one of the most influential ones and can achieve the state of-the-art performance (Bengtson and Roth, 2008). Best-first clustering has been previously studied by Ng and Cardie (2002) and Bengtson and Roth (2008) and found to be effective.
Learning Entailment Rules for Unary Templates Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules betweentemplates with a single variable. In this paper we investigate two approaches for unsupervised learning of such rules and com pare the proposed methods with a binary rule learning method. The results show that the learned unary rule-sets outperform the binary rule-set. In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure. Since we acquire verb entailment pairs based on unary templates (Szpektor and Dagan, 2008) we used the Lin formula to acquire unary templates directly rather than using the DIRT formula, which is the arithmetic-geometric mean of Lin's similarities for two slots in a binary template. Szpektor and Dagan (2008) proposed a directional similarity measure called BInc (Balanced Inclusion) that consists of Lin and Precision, as BInc (l, r)=? Lin (l, r)? Precision (l, r) 1173where l and r are the target templates. Szpektor and Dagan (2008) also proposed a unary template, which is defined as a template consisting of one argument slot and one predicate phrase. We define a unary template as a template consisting of one argument slot and one predicate, following Szpektor and Dagan (2008). DC follows the double conditioned contextualized similarity measure according to Equation 4, as implemented by (Ritter et al, 2010), while SC follows the single conditioned one at Equation 5, as implemented by (Dinu and Lapata, 2010b; Dinu and Lapata, 2010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rule sets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpek tor and Dagan, 2008). argument mapping by decomposing templates with several arguments into unary ones (Szpektor and Dagan, 2008). ArgumentMappedWordNet (AmWN): A resource for entailment rules between verbal and nominal predicates (Szpektor and Dagan, 2009), including their argument mapping, based on WordNet and NomLex plus (Meyers et al, 2004), verified statistically through intersection with the unary-DIRT algorithm (Szpektor and Dagan, 2008). Szpektor and Dagan (2008) use the distributional similarity of arguments to detect unary template entailment, whilst Berant et al (2010) apply it to binary relations in focused entailment graphs.  We follow here the experimental setup presented in (Szpektor and Dagan, 2008), testing the generated rules on the ACE 2005 event dataset 6. This. Adjuncts (time and 6http: //projects.ldc.upenn.edu/ace/ 7 Only 26 frequent event types that correspond to a unique predicate were tested, following (Szpektor and Dagan, 2008). Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including in formation extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al, 2009). apply BInc (Szpektor and Dagan, 2008), to compute for every verb pair a similarity score between each of the five count vectors. We consider two similarity functions: The Lin (2001) similarity measure, and the Balanced Inclusion (BInc) similarity measure (Szpektor and Dagan, 2008). In addition, we obtained similarity lists learned by Linand Pantel (2001), and replicated 3 similarity measures learned by Szpektor and Dagan (2008), over the RCV1corpus7. In (Szpektor and Dagan, 2008), two approaches for unsupervised learning of unary rules (i.e. between templates with a single variable) are investigated. In (Zhao et al, 2009), a pivot approach for extracting paraphrase patterns from bilingual parallel corpora is presented, while in (Callison-Burch,2008) the quality of paraphrase extraction from parallel corpora is improved by requiring that phrases and their paraphrases have the same syntactic type. Our approach is different from theirs in many respects: their goal is paraphrase extraction, while we are extracting directional entailment rules; as textual resources for pattern extraction they use parallel corpora (using patterns in another language as pivots), while we rely on monolingual Wikipedia revisions (taking benefit from its increasing size); the para phrases they extract are more similar to DIRT, while our approach allows to focus on the acquisition of rules for specific phenomena frequent in entailment pairs, and not covered by other resources. Recently, (Szpektor and Dagan, 2008) tried identifying the entailment relation between lexical-syntactic templates using WeedsPrec, but observed that it tends to promote unreliable relations involving infrequent templates. Finally, we adopt the balancing approach in (Szpektor and Dagan, 2008), which, as explained in Section 2, penalizes similarity for infrequent words having fewer features (4 th property) (in our version, we truncated LIN similarity lists after top 1000 words). Last, a richer form of representation, termed unary, has been suggested where a different predicate is defined for each argument (Szpektor and Dagan, 2008).
SOME COMPUTATIONAL PROPERTISS OF  TREE ADJO IN ING GRAMM.~.S* K.  V i jay -Shank~"  and  Arav ind  K .  Schabes and Joshi (1988) and Vijay-Shanker and Joshi (1985) provide parsing algorithms for TAGs that could serve to parse the base formalism of a synchronous TAG. Vijay-Shanker and Joshi (1985) introduced the first TAG parser in a CYK-like algorithm. In particular, the algorithm presented in [Vijay-Shanker and Joshi, 1985] can be seen to corresponds to the approach involving the use of cfg to encode derivations, whereas, the algorithm of [Vijay-Shanker and Weir, in pressb] uses lig in this role.  As an example, we introduce a CYK-based algorithm (Vijay-Shanker and Joshi, 1985) for TAG. In this section, we make a comparison of several different TAG parsing algorithms - the CYK based algorithm described at (Vijay-Shanker and Joshi, 1985), Earley-based algorithms with (Alonso et al, 1999) and without (Schabes, 1994) the valid prefix property (VPP), and Nederhof's algorithm (Nederhof, 1999) - on the XTAG English grammar (release 2.24.2001), by using our system and the ideas we have explained.
A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers There is growing interest in applying Bayesian techniques to NLP problems. There are a number of different estimators for Bayesian models, and it is useful to know what kinds of tasks each does well on. This paper compares a variety of different Bayesian estimators for Hidden Markov Model POS taggers with various numbers of hidden states on data sets of different sizes. Recent papers have given contradictory results when comparing Bayesian estimators to Expectation Maximization (EM) for unsupervised HMM POS tagging, and we show that the difference in reported results is largely due to differences in the size of the training data and the number of states in the HMM. We invesigate a variety of samplers for HMMs, including some that these earlier papers did not study. We find that all of Gibbs samplers do well with small data sets and few states, and that Variational Bayes does well on large data sets and is competitive with the Gibbs samplers. In terms of times of convergence, we find that Variational Bayes was the fastest of all the estimators, especially on large data sets, and that explicit Gibbs sampler (both pointwise and sentence-blocked) were generally faster than their collapsed counterparts on large data sets. Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging. We use a Gibbs sampler (Gao and Johnson, 2008) to learn the parameters of this and all other models under consideration. All of these methods maintain distributions over (or settings of) the latent variables of the model and update the representation iteratively (see Gao and Johnson (2008) for an overview in the context of POS induction). Gao and Johnson (2008) employed blocked sampling for POS tagging, and the approach works nicely for arbitrary derivation lattices. The approximation can be corrected using the Metropolis-Hastings algorithm, in which the sample drawn from the proposal lattice is accepted only with a certain probability a; but Gao and Johnson (2008) report that a > 0.99, so we skip this step. [cross val]: Cross-validation accuracy (Gao and Johnson, 2008) is intended to address the problem with many-to-one accuracy which is that assigning each word to its own class yields a perfect score.  We selected the three evaluation criteria of Gao and Johnson (2008): M-to-1, 1-to-1, and VI. M-to-1 and 1-to-1 are the tagging accuracies under the best many-to-one map and the greedy one-to-one map respectively; VI is a map-free information theoretic criterion - see Gao and Johnson (2008) for details. To further gain insight into how successful current models are at disambiguating when they have the power to do so, we examined a collection of HMM-VB runs (Gao and Johnson 2008) and asked how the accuracy scores would change if, after training was completed, the model were forced to assign the same label to all tokens of the same type.  We tuned the prior using the same set of 8 value pairs suggested by Gao and Johnson (2008), using a held out set of POS-tagged CDS to evaluate final performance. They are also competitive with Bayesian estimators, on larger data sets, with cross-validation (Gao and Johnson, 2008). We experimented with the following models: ARR10 (Abend et al,2010), Clark03 (Clark, 2003), GG07 (Goldwater and Griffiths, 2007), GJ08 (Gao and Johnson, 2008), and GVG09 (Van Gael et al, 2009) (three models). For example, Gao and Johnson (2008) proposed to induce a many-to-one mapping of state identifiers to PoS tags from one half of the corpus and evaluate on the second half, which is referred to as cross-validation accuracy.   Note that LDC significantly outperforms all HMMs (Gao and Johnson, 2008) in every case except PTB45 under the OTO mapping. Gao and Johnson (2008) compared EM, VB and GS in English against the Penn Treebank Wall Street Journal (WSJ) text. Each of these techniques provide significant improvements over the standard HMM model: for example Gao and Johnson (2008) show that sparse priors can gain from 4% (.62 to .66 with a 1M word corpus) in cross-validated many to-one accuracy.
Coreference Resolution in a Modular Entity-Centered Model Coreference resolution is governed by syntactic, semantic, and discourse constraints. We present a generative, model-based approach in which each of these factors is modularly encapsulated and learned in a primarily unsupervised manner. Our semantic representation first hypothesizes an underlying set of latent which generate specific entities that in turn render individual mentions. By sharing lexical statistics at the level of abstract entity types, our model is able to substantially reduce semantic compatibility errors, resulting in the best results to date on the complete end-to-end coreference task. These abstract notions (lexical association, proximity, tendencies towards few or many relations, and allowing for unassociated items) play an important role in many relation-detection tasks (e.g., co-reference resolution, Haghighi and Klein 2010). Many other existing systems applied supervised or unsupervised (Haghighi and Klein, 2010) learning models. However, such structured knowledge bases are of limited scope, and, while Haghighi and Klein (2010) self-acquires knowledge about coreference, it does so only via reference constructions and on a limited scale. Altogether, our final system produces the best numbers reported to date on end-to-end coreference resolution (with automatically detected system mentions) on multiple data sets (ACE 2004 and ACE2005) and metrics (MUC and B3), achieving significant improvements over the Reconcile DT baseline and over the state-of-the-art results of Haghighi and Klein (2010). In ACE04 and ACE05, we have only the newswire portion (of the original ACE 2004 and 2005 training sets) and use the standard train/test splits reported in Stoyanov et al (2009) and Haghighi and Klein (2010). In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010).  Our main comparison is against Haghighi and Klein (2010), a mostly-unsupervised generative approach that models latent entity types, which generate specific entities that in turn render individual mentions. For the ACE05 and ACE05-ALL datasets, we revert to the 'AllPairs' (AP) setting of Reconcile because this gives us baselines competitive with Haghighi and Klein (2010). Our submission was a reduced version of the system described in Haghighi and Klein (2010), with extensions to improve mention detection to suit the OntoNotes annotation scheme. Unlike previous work, we did not use the Bllip or Wikipedia data described in Haghighi and Klein (2010). The specific update methods vary for each set of parameters; for details see Section 4 of Haghighi and Klein (2010). Unlike Haghighi and Klein (2010), no extra data from Wikipedia or Bllip was used, a restriction that was necessary to be eligible for the closed part of the task. Generative models are also used in unsupervised coreference (Haghighi and Klein, 2010). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al, 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). This knowledge could be especially helpful for cross document coreference resolution systems (Haghighi and Klein, 2010), which actually represent concepts and track mentions of them across documents.  For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Daume III and Marcu (2005) propose a generative approach to supervised clustering, and Haghighi and Klein (2010) use entity profiles to assist within-document coreference.
Unsupervised Personal Name Disambiguation This paper presents a set of algorithms for distinguishing personal names with multiple real referents in text, based on little or no supervision. The approach utilizes an unsupervised clustering technique over a rich feature space of biographic facts, which are automatically extracted via a language-independent bootstrapping process. The induced clustering of named entities are then partitioned and linked to their real referents via the automatically extracted biographic data. Performance is evaluated based on both a test set of handlabeled multi-referent personal names and via automatically generated pseudonames. This approach is practical for geographic names, for which broad-coverage gazetteers exist, though less so for personal names (Mann and Yarowsky, 2003). Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). The training set contains the top 100 web search results of 49 names from the Web03 corpus (Mann and Yarowsky, 2003), Wikipedia and European Conference on Digital Library (ECDL) participants; the test data are comprised of the top 100 documents of 30 names from Wikipedia, US Census and ACL participants. Mann and Yarowsky (2003) have proposed a Web based clustering technique relying on a feature space combining biographic facts and associated names, whereas Bagga and Baldwin (1998) have looked for co reference chains within each document, take the context of these chains for creating summaries about each entity and convert these summaries into a bag of words. (Mann and Yarowsky, 2003) first extract biographical information, such as birthdates, birthplaces ,occupations, and so on. (Wan et al, 2005) employ an approach similar to that of (Mann and Yarowsky,2003), and have developed a system called Web Hawk. Clustering approaches (e.g. hierarchical agglomerative clustering (Mann and Yarowsky, 2003)) have been commonly used for CDC due to the variety of data distributions of different names. Our work goes beyond the simple co-occurrence features (Bagga and Baldwin, 1998) and the limited extracted information (e.g. biographical information in (Mann and Yarowsky, 2003) that is relatively scarce in web data) using the broad range of relational information with the support of information extraction tools. This final similarity function will then be embedded into a normal HAC algorithm to group the web pages into different namesakes where we compute the centroid-based distance between clusters (Mann and Yarowsky, 2003). This is in line with Mann and Yarowsky (2003)'s modification, consisting in replacing all numbers in the patterns with the symbol ####. Bagga and Baldwin (1998) used a Bag of Words (BOW) model to resolve ambiguities among people. Mann and Yarowsky (2003) improved the performance of personal names disambiguation by adding biographic features. Mann and Yarowsky (Mann and Yarowsky, 2003) examined the same problem but they treated it as a clustering task. Mann and Yarowsky (2003) and Niu et al (2004) extended the vector representation with extracted biographic facts. Similarly, approaches in unstructured data (e.g., text) have involved using clustering techniques over biographical facts (Mann and Yarowsky, 2003), within-document resolution (Blume, 2005), and discriminative unsupervised generative models (Lietal., 2005). 277 Motivated by ambiguity in personal name search, Mann and Yarowsky (2003) disambiguate person names using biographic facts, like birth year, occupation and affiliation. An alternative approach by Mann and Yarowsky (2003) is based on a rich feature space of automatically extracted biographic information. To model these characteristics, Bunescu and Pasca (2006) and Cucerzan (2007) incorporate information from Wikipedia articles, Artiles et al (2007) use Webpage content, Mann and Yarowsky (2003) extract biographic facts. Mann and Yarowsky (2003) extract biographic facts such as date or place of birth, occupation, relatives among others to help resolve ambiguous names of people. In the future, we would like to model the biographical fact extraction approach of (Mann and Yarowsky, 2003) in our LDA model. On the other hand, Mann and Yarowsky (2003) proposes a richer document representation involving automatically extracted features.
Intention-Based Segmentation: Human Reliability And Correlation With Linguistic Cues Certain spans of utterances in a discourse, referred here as widely assumed to form coherent units. Further, the segmental structure of discourse has been claimed to constrain and be constrained by many phenomena. However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them. We present quantitative results of a two part study using a corpus of spontaneous, narrative monologues. The first part evaluates the statistical reliability of human segmentation of our corpus, where speaker intention is the segmentation criterion. We then use the subjects' segmentations to evaluate the correlation of discourse segmentation with three linguistic cues (referential noun phrases, cue words, and pauses), using information retrieval metrics. To attempt to overcome this issue, both Passonneau and Litman (1993) and Hearst (1993) conflated multiple manual segmentations into one that contained only those boundaries which the majority of coders agreed upon. Additionally, the definition of what constitutes a majority is subjective (e.g., Passonneau and Litman (1993, p. 150), Litman and Passonneau (1995), Hearst (1993, p. 6) each used 4/7, 3/7, and > 50%, respectively). Similar to (Passonneau and Litman, 1993), we adopt a flat model of topic segmentation for our gold standard based on discourse segment purpose, where a shift in topic corresponds to a shift in purpose that is acknowledged and acted upon by both conversational agents. However, implementing the NM in a new domain re quires little expertise as previous work has shown that native users can reliably annotate the information needed for the NM (Passonneau and Litman, 1993). With these goals in mind, we adopted a definition of "topic" that builds upon Passonneau and Litman's seminal work on segmentation of monologue (Passonneau and Litman, 1993). Passonneau and Litman (1993) were the first to investigate the relationship between cue phrases and linear segmentation.
Discourse Segmentation Of Multi-Party Conversation We present a domain-independent topic segmentation algorithm for multi-party speech. Our feature-based algorithm comknowledge about a text-based algorithm as a feature and linguistic and acoustic cues about topic shifts extracted from speech. This segmentation algorithm uses automatically induced decision rules to combine the different features. The embedded text-based algorithm builds on lexical cohesion and has performance comparable to state-of-the-art algorithms based on lexical information. A significant error reduction is obtained by combining the two knowledge sources. We use an automatic topic segmentation tool, LCSeg (Galley et al, 2003) setting parameters so that the derived segments are of the approximate desired length.  The gold standard for thematic segmentations has been kindly provided by (Galley et al., 2003) and has been chosen by considering the agreement between at least three human annotations. We evaluated WLM's performance on the ICSI meeting corpus (Janin et al 2003) by comparing our segmentation results to the results obtained by implementing LCSeg (Galley et al, 2003). For example, lexical cohesion-based algorithms, such as LCSEG (Galley et al, 2003), or its word frequency-based predecessor TextTile (Hearst, 1997) capture topic shifts by modeling the similarity of word repetition in adjacent windows. Previous work has shown that training a segmentation model with features that are extracted from knowledge sources other than words, such as speaker interaction (e.g., overlap rate, pause, and speaker change) (Galley et al, 2003), or participant behaviors, e.g., note taking cues (Banerjee and Rudnicky, 2006), can outperform LCSEG on similar tasks. Adapting the standard definition of topic (Galley et al, 2003) to conversations/emails, we consider a topic is something about which the participant(s) discuss or argue or express their opinions.  Moving to the task of segmenting dialogs, (Galley et al, 2003) first proposed the lexical chain based unsupervised segmenter (LCSeg) and a supervised segmenter for segmenting meeting transcripts. For the topic level, they achieve similar results as (Galley et al, 2003), with the supervised approach outperforming LCSeg.  Our second model is the lexical chain based segmenter LCSeg, (Galley et al, 2003). However, Galley et al, (Galley et al, 2003) uses only repetition relation as previous research results (e.g., (Choi, 2000)) account only for repetition. The first dataset is a subset of the ICSI-MR corpus (Janin et al, 2004), where the gold standard for thematic segmentations has been provided by taking into account the agreement of at least three human annotators (Galley et al, 2003). The LCseg system (Galley et al, 2003), labeled here as G03, is to our knowledge the only word distribution based system evaluated on ICSI meeting data. Therefore, we replicate the results reported by (Galley et al, 2003) when evaluation of LCseg was done on ICSI data. The so-labeled G03* algorithm indicates the error rates obtained by (Galley et al, 2003) when extra (meeting specific) features have been adopted in a decision tree classifier. The work of (Galley et al, 2003) shows that the G03* algorithm is better than G03 by approximately 10%, which indicates that on meeting data the performance of our word-distribution based approach could possibly be increased by using other meeting-specific features. Our feature set incorporates information which has proven useful in meeting segmentation (Galley et al, 2003) and the task of detecting addressees of a specific utterance in a meeting (Jovanovic et al, 2006). 
Error-Driven Pruning of Treebank Grammars for Base Noun Phrase Identification Finding simple, non-recursive, base noun phrases is an important subtask for many natural language processing applications. While previous empirical methods for base NP identification have been rather complex, this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task. In particular, we present a corpus-based approach for finding base NPs by matching part-ofspeech tag sequences. The training phase of the algorithm is based on two successful techniques: first the base NP grammar is read from a &quot;treebank&quot; corpus; then the grammar is improved by selecting rules with high &quot;benefit&quot; scores. Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the While early approaches to the NP-chunking task (Cardie and Pierce, 1998) relied on part-of-speech information alone, it is widely accepted that lexical information (word forms) is crucial for building accurate systems for these tasks. The approaches tested were Error Driven Pruning (EDP) (Cardie and Pierce, 1998) and Transformational Based Learning of IOB tagging (TBL) (Ramshaw and Marcus, 1995). Conjunctions are a major source of errors for English chunking as well (Ramshaw and Marcus, 1995, Cardie and Pierce, 1998), and we plan to address them in future work. Our implementation of the NP-based QA system uses the Empire noun phrase finder, which is described in detail in Cardie and Pierce (1998). (Cardie and Pierce, 1998, 1999) applied a scoring method to select new rules and a naive heuristic for matching rules to evaluate the results and accuracy. (Cardie and Pierce, 1998) present an approach to chunking based on a mixture of finite state and context-free techniques. (Cardie and Pierce, 1998) store POS tag sequences that make up complete chunks and use these sequences as rules for classifying unseen data. We tested this hypothesis by training the Error-Driven Pruning (EDP) method of (Cardie and Pierce, 1998) with an extended set of features.
Using a maximum entropy model to build segmentation lattices for MT Recent work has shown that translating segmentation lattices (lattices that encode alternative ways of breaking the input to an MT system into words), rather than text in any particular segmentation, improves translation quality of languages whose orthography does not mark morpheme boundaries. However, much of this work has relied on multiple segmenters that perform differently on the same input to generate sufficiently diverse source segmentation lattices. In this work, we describe a maximum entropy model of compound word splitting that relies on a few general features that can be used to generate segmentation lattices for most languages with productive compounding. Using a model optimized for German translation, we present results showing significant improvements in translation quality in German-English, Hungarian-English, and Turkish-English translation over state-ofthe-art baselines. Dyer (2009) also employed a segmentation lattice, which represents ambiguities of compound word segmentation in German, Hungarian and Turkish translation. To construct a segmentation dictionary, I used the 1-best segmentations from a supervised MaxEnt compound splitter (Dyer, 2009) run on all token types in bitext. Despite their simplicity, unigram weights have been shown as an effective feature in segmentation models (Dyer, 2009). The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). These lattices serve to encode alternative ways of segmenting compound words, and as such, when presented as the input to the system allow the decoder to automatically choose which segmentation is best for translation, leading to markedly improved results (Dyer, 2009). We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner. Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. Since German is a language that makes productive use of "closed" compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). These lattices encode alternative ways of segmenting compound words, and allow the decoder to automatically choose which segmentation is best for translation, leading to significantly improved results (Dyer, 2009). All data was tokenized and lowercased; German compounds were split (Dyer, 2009). Segmentation also improves translation of compounding languages such as German (Dyer, 2009) and Finnish (Macherey et al, 2011). An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where a maximum entropy model is used to assign probabilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model. Dyer (2009) applied this to German using a lattice encoding different segmentations of German words. Dyer (2009) applies a maximum entropy model of compound splitting to generate segmentation lattices that serve as input to a translation system. Dyer (2009) introduces a maximum entropy model for compound word splitting, which he uses to create word lattices for translation input.
Introduction To The CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition  The shared task of CoNLL-2002 dealt with named entity recognition for Spanish and Dutch (Tjong Kim Sang, 2002). We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al, 2005). Empirical evidence for this argument can be seen from the result of the CoNLL shared tasks (Tjong Kim Sang, 2002) (Tjong Kim Sang and Meulder, 2003), where the ranking of the participating systems changes with the test corpora. Similarly to classical NLP tasks such as base noun phrase chunking (Ramshaw and Marcus, 1994), text chunking (Ramshaw and Marcus, 1995) or named entity recognition (Tjong Kim Sang, 2002), we formulate the mention detection problem as a classification problem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. It is especially challenging to extract the named entities from the text sources written in languages other than English which, in practice, is supported by the results of the shared tasks on the named entity recognition (Tjong Kim Sang, 2002). In addition, several competitions have been organized, with a focus on multilingual NER (Tjong Kim Sang, 2002). Various state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost (Carreras et al., 2002), Hidden Markov Models (Bikel et al,), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used. This scheme was initially introduced in CoNLL's (Tjong Kim Sang, 2002a) and (Tjong Kim Sang and De Meulder, 2003) NER competitions, and we decided to adapt it for our experimental work. Similarly to classical NLP tasks, such as Base Phrase Chunking (Ramshaw and Marcus, 1999) (BPC) or NER (Tjong Kim Sang, 2002), we formulate the MD task as a sequence classification problem, i.e. the classifier assigns to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. We used three data sets: the English and German data for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003) and the Dutch data for the CoNLL 2002 shared task (Tjong Kim Sang, 2002). We use the Dutch data set from the CoNLL 2002 shared task (Tjong Kim Sang, 2002). The fourth type, called miscellaneous, was introduced in the CoNLL NER tasks in 2002 (Tjong Kim Sang, 2002) and 2003 (Tjong Kim Sang and De Meulder, 2003), and includes proper names falling outside the three classic types.
The Rhetorical Parsing Of Unrestricted Natural Language Texts We derive the rhetorical structures of texts by means of two new, surface-form-based algorithms: one that identifies discourse usages of cue phrases and breaks sentences into clauses, and one that produces valid rhetorical structure trees for unrestricted natural language texts. The algorithms use information that was derived from a corpus analysis of cue phrases. Percent agreement with the majonty opinion for each text We took then the RS-trees built by the analysts and used our formalizaUon of RST (Marcu, 1996, Marcu, 1997b) to assocmte with each. The mathematical foundations of the rhetorical parsing algorithm rely on a first order formatioon of valid text structures (Marcu, 1997b). Document structure (WD) is another important clue in determining which elements of a text are important enough to include in a summary (Marcu, 1997). Most recently, Marcu (1997) has described a method for text summarization based on nuclearity and selective retention of hierarchical fragments.  Since discourse markers, such as because and and, have been shown to play a major role in rhetorical parsing (Marcu, 1997), we also consider a list of features that specify whether a lexeme found within the local contextual window is a potential discourse marker. Our task is similar to the concept of discourse parsing (Marcu (1997)), where discourse structures are extracted from the text.  Argumentation belongs to discourse analysis, with fairly complex computational models such as the implementation of the rhetorical structure theory proposed by (Marcu, 1997), which proposes dozens of rhetorical classes. Marcu (1997) describes a rhetorical parsing approach which takes unrestricted text as input and derives the rhetorical structure tree. One exception is Marcu's work (Marcu, 1997, 1999) (see also Soricut and Marcu (2003) for constructing discourse structures for individual sentences). Our segmentation is linear, rather than hierarchical (Marcu 1997 and Yaari 1997), i.e. the input article is divided into a linear sequence of adjacent segments. Considering the problem from another angle, discourse approaches have focused on shorter units than multi-paragraph segments, but Rhetorical Structure Theory (Marcu 1997 and others) may be able to scale up to associate rhetorical functions with segments. In the approach proposed in (Marcu, 1997), for example, the presence of discourse markers is used to hypothesize individual textual units and relations holding between them. Marcu (1997) parsed documents as rhetorical trees and identified important sentences based on the trees.
The Ups and Downs of Preposition Error Detection in ESL Writing In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-native English speakers. Our system performs at 84% precision andclose to 19% recall on a large set of stu dent essays. In addition, we address the problem of annotation and evaluation inthis domain by showing how current ap proaches of using only one rater can skew system evaluation. We present a sampling approach to circumvent some of the issuesthat complicate evaluation of error detec tion systems. The accuracy of the choice classifier, the part of the system to which the work at hand is most similar, is 62.32% when tested on text from Encarta and Reuters news. Tetreault and Chodorow (2008a) present a system for detecting preposition errors in learner text. (Tetreault and Chodorow, 2008b) challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability. This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are 1 (Tetreault and Chodorow, 2008b) report that it would take 80hrs for one of their trained raters to find and mark 1,000 preposition errors. Examples include the Cambridge Learners Corpus2 used in (Felice and Pullman, 2009), and TOEFL data, used in (Tetreault and Chodorow, 2008a). (Tetreault and Chodorow, 2008b) showed that trained human raters can achieve very high agreement (78%) on this task. For example, (Tetreault and Chodorow, 2008b) found kappa between two raters averaged 0.630. Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three trained annotators. There is already existing work that addresses specific problems in this area (see, for ex ample, (Tetreault and Chodorow, 2008)), but to be genuinely useful, we require a solution to the writing problem as a whole, integrating existing solutions to sub-problems with new solutions for problems as yet unexplored. In example 1, the context requires a definite article, and the definite article, in turn, calls for the 4Our error classification was inspired by the classification developed for the annotation of preposition errors (Tetreault and Chodorow, 2008a). Tetreault and Chodorow (Tetreault and Chodorow, 2008a) show that agreement between two native speakers on a cloze test targeting prepositions is about 76%, which demonstrates that there are many contexts that license multiple prepositions. The latter is complicated by the fact that native speakers differ widely with respect to what constitutes acceptable usage (Tetreault and Chodorow, 2008a). To date, a common approach to annotating non native text has been to use one rater (Gamon et al, Source Errors Errors Mistakes by error type language total per 100 Repl. Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b). The usage feature detects errors related to articles (Han et al, 2006), prepositions (Tetreault and Chodorow, 2008) and collocations (Futagi et al, 2008). Some researchers (Tetreault and Chodorow, 2008) exploited syntactic information and n-gram features to represent verb usage context. Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (Gamon et al 2008, 2010, DeFelice and Pulman 2007, 2008, Han et al 2006, Chodorow et al 2007, Tetreault and Chodorow 2008).  For example Tetreault and Chodorow (2008) use a maximum entropy classifier to build a model of correct preposition usage, with 7 million instances in their training set, and Lee and Knutsson (2008) use memory-based learning ,with10 million sentences in their training set. For further perspective on these results, note Chodorow et al (2007) achieved 69% with 7M training examples, while Tetreault and Chodorow (2008) found the human performance was around 75%. We adopt the features from previous work by Han et al (2006), Tetreault and Chodorow (2008), and Rozovskaya et al (2011) for our system. We recreate a state-of-the-art preposition usage system (Tetreault and Chodorow (2008), henceforth T& amp; C08) originally trained with lexical features and augment it with parser output features. The prepositions were judged by two trained annotators and checked by the authors using the preposition annotation scheme described in Tetreault and Chodorow (2008b).
Morphological Analysis For Statistical Machine Translation We present a novel morphological analysis technique which induces a morphological and syntactic symmetry between two languages with highly asymmetrical morphological structures to improve statistical machine translation qualities. The technique pre-supposes fine-grained segmentation of a word in the morphologically rich language into sequence of and part-of-speech tagging of the parallel corpus. The algorithm to be the morphologically rich language to induce the desired morphological and syntactic symmetry. The technique improves Arabic-to-English translation qualities significantly when applied to IBM Model 1 and Phrase Translation Models trained on the training corpus size ranging from 3,500 to 3.3 million sentence pairs. Another project utilizing morphological analysis for statistical machine translation is described by Lee (2004). Lee (2004), for example, showed that morphological analysis can improve the quality of statistical machine translation for Arabic. We found that our approach of using lemmatization improved both the word alignment and the quality of SMT with a small amounts of training data, and, while much work indicates that MA is useless in training large amounts of data (Lee, 2004), our intensive experiments proved that the chance to get a better MT quality using lemmatization is higher than that without it for large amounts of training data. Often the choice of directionality is motivated by this restriction, and the choice of tokenization style may be designed (Lee, 2004) to reduce this problem. Lee (2004) uses a trigram language model to segment Arabic words.  For example, it has been shown (Lee, 2004) that determiner segmentation and deletion in Arabic sentences in an Arabic-to-English translation system improves sentence alignment, thus leading to improved overall translation quality. Lee (2004) only changes the word segmentation of the morphologically complex language (Arabic) to induce morphological and syntactic symmetry between the parallel sentences. Another way for determiner deletion is described in (Lee, 2004). For languages with more or less regular inflectional morphology like Arabic or Turkish, another good idea is to segment words into morpheme sequences, e.g., prefix(es)-stem-suffix(es), which can be used instead of the original words (Lee, 2004) or in addition to them. Similarly, for Arabic-to-English, Lee (2004), and Habash and Sadat (2006) show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size. In Lee (2004), the goal is to match the lexical granularities of the two languages by starting with a fine-grained segmentation of the Arabic side of the corpus and then merging or deleting Arabic morphemes using alignments with a part-of-speech tagged English corpus. Specifically considering Arabic, Lee (2004) investigated the use of automatic alignment of POS tagged English and affix-stem segmented Arabic to determine appropriate tokenizations. Lee (2004) and Zolmann et al (2006) have exploited morphology in Arabic English SMT. While comprehensive Arabic preprocessing schemes have been widely adopted for handling Arabic morphology in SMT (e.g., Sadat and Habash (2006), Zollmann et al (2006), Lee (2004)), syntactic issues have not received as much attention by comparison (Green et al). Lee (2004) uses a morphologically analyzed and tagged parallel corpus for Arabic English SMT. Specially for Arabic-English translation, Lee (2004) used the Arabic part of speech and English parts of speech (POS) alignment probabilities to retain an Arabic affix, drop it from the corpus or merge it back to a stem. Specifically considering Arabic, Lee (2004) investigated the use of automatic alignment of POS-tagged English and affix-stem segmented Arabic to determine appropriate tokenizations. It is identical to the initial tokenization used by Lee (2004). We do not use any additional information to remove specific features using alignments or syntax (unlike, e.g. removing all but one Al+ in noun phrases (Lee, 2004)).
Principle-Based Parsing Without Overgeneration Overgeneration is the main source of computational complexity in previous principle-based parsers. This paper presents a message passing algorithm for principle-based parsing that avoids the overgeneration problem. This algorithm has been implemented in C++ and successfully tested with example sen This also distinguishes our approach from another major stream of object-oriented natural anguage parsing which is almost entirely concerned with implementational aspects of object-oriented programming, e.g., Habert (1991), Lin (1993) or Yonezawa& amp; Ohsawa (1994). Throughout this paper we used syntactic features generated by the Minipar depend ency parser (Lin, 1993). We obtain candidate properties by parsing a large textual corpus with the Minipar parser (Lin 1993). We used the Minipar parser (Lin 1993) to analyze each sentence and we collected the frequency counts of the grammatical contexts output by Minipar and used them to compute the probability and point wise mutual information values from Sections 4.1 and 4.2. We use Minipar (Lin, 1993), which produces functional relations for the components in a sentence, including subject and object relations with respect to a verb. The dataset is identical to the one used by TDP and has been constructed in the same wayas the dataset used by E&P: it contains those gold standard instances of verbs that have according to the analyses produced by the MiniPar parser (Lin, 1993) an overtly realized subject and object. We build the space from a Minipar-parsed version of the British National Corpus with dependency parses obtained from Minipar (Lin, 1993). For the current paper, we constructed a new subset of LexSub we call LEXSUB-PARA by parsing LexSub with Minipar (Lin, 1993) and extracting all 177 sentences with transitive verbs that had overtly realized subjects and objects, regardless of voice. English documents were parsed with the syntactic analyzer (Lin, 1993). This space was built from BNC dependency parses obtained from Minipar (Lin, 1993). Verbs and possible senses in our corpus Both corpora were lemmatized and part-of-speech (POS) tagged using Minipar (Lin, 1993) and Mxpost (Ratnaparkhi, 1996) ,respectivelly. We used Minipar (Lin, 1993), a dependency-based parser, in order to identify verbs, nouns, verb dependencies and noun dependencies. We investigated four individual approaches for the syntax-features, a regular-expression-based quasi-parser, a system based on Dekang Lin's Mini Par (Lin, 1993), a system based on the Collins parser (Collins, 1999), and one based on the CMU Link Grammar Parser (Sleator and Temperley, 1993), as well as a family of voting-based combination schemes. The syntactic relations are generated by the Minipar dependency parser (Lin, 1993). The experiment was conducted using an 18 million tokens subset of the Reuters RCV1corpus, parsed by Lin's Minipar dependency parser (Lin, 1993). We used the dv package1 to compute type vectors from a Minipar (Lin, 1993) parse of the BNC. Specifically, we assume MINIPAR-style (Lin, 1993) dependency trees where nodes represent text expressions and edges represent the syntactic relations between them. Three parsers fulfilled all the requirements: Link Grammar (Sleator and Temperley, 1993), Minipar (Lin, 1993) and (Carroll and Briscoe, 2001). We used the Mini par parser (Lin 1993) to match DIRT patterns in the text. The syntactic relations were extracted using the Minipar parser (Lin, 1993).
Using The Web To Obtain Frequencies For Unseen Bigrams This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task. Following the methodology in Keller and Lapata (2003), we divide the verbs into four frequency bands, frequency being absolute number of annotated sentences: low (5), medium-low (12), medium-high (22), and high (38). Keller and Lapata (2003) showed that web frequencies correlate reliably with standard corpus frequencies. We used the log of the web counts returned, as recommended in previous work (Keller and Lapata, 2003). The Web has already been used successfully for a series of NLP tasks such as Mt (Grefenstette, 1999), word sense disambiguation (Agirre and Martinez, 2000), synonym recognition (Turney, 2001), anaphora resolution (Modjeska et al, 2003) and determining frequencies for unseen bi-grams (Keller and Lapata, 2003). Thus, the frequency of appearance of n-grams on the Web (via the Google search engine) appears as a good indicator of the n-gram popularity/soundness (Keller and Lapata, 2003). More recently, Keller and Lapata (2003) evaluate the utility of using Web search engines for obtaining frequencies for unseen bigrams. See Keller and Lapata (2003) for more issues. The findings described in (Keller and Lapata, 2003) seem to suggest that count estimations we need in the present study over Subject-Verb bigrams are highly correlated to corpus counts. The performance is similar to other published results like those by Keller and Lapata (2003), who adopted a similar feature set and reported around 75% success rates on the ACE data set. It has been shown that web documents (as Wikipedia) are reliable samples of language (Keller and Lapata, 2003). We also use Keller and Lapata (2003)'s approach to obtaining web-counts. Also, the Keller and Lapata (2003) approach will be undefined if the pair is unobserved on the web.  Recall that even the Keller and Lapata (2003) system, built on the world's largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5).  The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). It is believed that the size of the web is thousands of times larger than normal large corpora, and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicate-argument bi-grams (Keller and Lapata, 2003). However, previous study (Keller and Lapata, 2003) reveals that the large amount of data available for the web counts could outweigh the noisy problems.  NC analysis has benefited from the recent trend of using web-derived features rather than corpus based counts (Keller and Lapata, 2003).
Selectional Preference And Sense Disambiguation absence of is a real problem for corpus-based approaches to sense disambiguation, one that is unlikely to be solved soon. Selectional preference is traditionally connected with sense ambiguity; this paper explores how a statistical model of selectional preference, requiring neither manual annotation of selection restrictions nor supervised training, can be used in sense disambiguation. WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky 1995) (Resnik 1997). Previous work on selectional preferences has used them primarily for natural language analytic tasks such as word sense disambiguation (Resnik, 1997), dependency parsing (Zhou et al 2011), and semantic role labeling (Gildea and Jurafsky, 2002). I followed (Resnik, 1993)/ (Resnik, 1997) who defined selectional preference as the amount of information a verb provides about its semantic argument classes. The annotation of word senses such as used by machine-learning based word sense disambiguation (WSD) tools corresponds to the task of selecting the correct semantic class or concept for a word from an underlying ontology such as WordNet (Resnik, 1997). Resnik (1997) described a method to acquire a set of conceptual classes for word senses, employing selectional preferences, based on the idea that certain linguistic predicates constraint the semantic interpretation of underlying words into certain classes.  These promising results enable a number of future researches: (1) larger scale experiments with different measures and semantic similarity models (e.g. (Resnik, 1997)); (2) improvement of the overall efficiency by exploring feature selection methods over the SK, and (3) the extension of the semantic similarity by a general (i.e. non binary) application of the conceptual density model. Techniques for automatically detecting selections preferences have been discussed in (McCarthy and Carrol, 2003) and (Resnik, 1997). A large, high-quality database of preferences has the potential to improve the performance of a wide range of NLP tasks including semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al, 2008), textual inference (Pantel et al, 2007), word-sense disambiguation (Resnik,1997), and many more. We adopted the association measure proposed by Resnik (1993) and successfully applied to a number of tasks in NLP including word sense disambiguation (Resnik, 1997). To add the necessary context, ISP (Pantel et al, 2007) learned selectional preferences (Resnik, 1997) for DIRT's rules. In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al (2007). We used the training sets, test sets, and evaluation method described in (Resnik, 1997). Automatically or semi automatically acquired selectional preferences, as means for constraining the number of possible senses that a word might have, based on the relation it has with other words in context (Resnik, 1997).
Word Association Norms Mutual Information And Lexicography rs Sunday, calling for greater economic reforms to mtniasion asserted that &quot;the Postal Service could Then, she said, the family hopes to e out-of-work steelworker. &quot;because that doesn't &quot;We suspend reality when we say we'll scientists has won the first round in an effort to about three children in a mining town who plot to GM executives say the shutdowns will rtment as receiver, instructed officials to try to The package, which is to newly enhanced image as the moderate who moved to million offer from chairman Victor Posner to help after telling a delivery-room doctor not to try to h birthday Tuesday, cheered by those who fought to at he had formed an alliance with Moslem rebels to &quot; Basically we could We worked for a year to their expensive mirrors, just like in wartime, to ard of many who risked their own lives in order to We must increase the amount Americans save China from poverty. save enormous sums of money in contracting out individual c save enough for a down payment on a home. save jobs, that costs jobs. &quot; save money by spending $10,000 in wages for a public works save one of Egypt's great treasures, the decaying tomb of R save the &quot;pit ponies &quot;doomed to be slaughtered. save the automaker $500 million a year in operating costs a save the company rather than liquidate it and then declared save the country nearly $2 billion, also includes a program save the country. save the financially troubled company, but said Posner stil save the infant by inserting a tube in its throat to help i save the majestic Beaux Arts architectural masterpiece. save the nation from communism. save the operating costs of the Pershings and pound-launch save the site at enormous expense to us, &quot;said Leveillee. save them from drunken Yankee brawlers, &quot;Tess said. save those who were passengers. &quot; save. &quot; Figure 2: Some AP 1987 Concordance lines to 'save ... from,' roughly sorted into categories save X from Y (65 concordance lines) 1 save PERSON from Y (23 concordance lines) 1.1 save PERSON from BAD (19 concordance lines) ( Robert DeNiro ) to &quot;We wanted to Murphy was sacrificed to &quot;God sent this man to Pope John Paul II to&quot; save Indian tribes(PERSON] from genocide[DESTRUCT[BAD]] at the hands of save hirn(PERSON] from undue trouble(BADI and loss[BAD] of money, &quot; save more powerful Democrats[PERSON] from harm(BAD] . save my five diikiren[PERSON] from being burned to death[DESTRUCT(BAD]] and save us(PERSON1 from sin[BAD] . &quot; 1.2 save PERSON from (BAD) LOC(ATION) (4 concordance lines) rescuers who helped save the toddler(PERSON] from an abandoned well[LOC] will be feted with a parade while attempting to save two drowning boys(PERSON] from a turbulent(BAD] creek[LOC] in Ohio(LOC) 2. save INST(ITUTION) from (ECON) BAD (27 concordance lines) member states to help should be sought &quot;to law was necessary to operation &quot;to were not needed to his efforts to save the BEC[INST] from possible bankruptcy[ECONPAD] this year. save the company[CORP[DIST]] from bankruptcy(ECON)(BAD] . save the country[NATIONINSTD from dismter(BAD1 • save the nation[NATION[INST]J from Conununism[BADNPOLITICAL] save the system from bankruptcy[ECONHBAD] . save the world(INST] from the likes of Lothar and the Spider Woman 3. save ANIMAL from DESTRUCT(ION) (5 concordance lines) give them the money to save the dogs[ANIMAL] from being destroyed(DESTRUC11 , program intended to save the giant birds[ANIMAL] from extinction[DESTRUCT] , UNCLASSIFIED (10 concordance lines) walnut and ash trees to save them from the axes and saws of a logging company. after the attack to save the ship from a terrible[BAD] fire , Navy reports concluded Thursday. Church and Hanks (1989) discussed the use of the mutual information statistic in order to identify a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntacfic co-occurrence onstraints between verbs and prepositions (content word/function word). We use Pointwise Mutual Information (PMI) (Church and Hanks, 1989) to weight the contexts, and select the top 1000 PMI contexts for each adjective. Some of these are Frequency, Mutual Information (Church and Hanks, 1989), distributed frequency of object (Tapanainen et al, 1998) and LSA model (Baldwin et al, 2003) (Schutze, 1998). (Breidt, 1995) has evaluated the usefulness of the Point-wise Mutual Information measure (as suggested by (Church and Hanks, 1989)) for the extraction of V-N collocations from German text corpora. We encode the semantic compatibility between a noun and its parse tree parent (and grammatical relationship with the parent) using mutual information (MI) (Church and Hanks, 1989).  We weigh each context f using point wise mutual information (Church and Hanks 1989). Z (2) d:: l j= i (d -1 )dmax: max distance used wl: the i-th letter in the sentence w g (d): a certain weight for iV// concerning distance between letters The information between two remote words has less nmaning in a sentence when it comes to the semantic analysis (Church and Hanks, 1989). Mutual information (first introduced to computational linguistics by Church and Hanks (1989)) is one of many measures that seems to be roughly correlated to the degree of semantic relatedness be tween words. I also experimented with the co-occurence frequency c (s, w) and point-wise mutual information (Church and Hanks, 1989) as similarity functions. Beginning with (Church and Hanks, 1989), numerous authors have used the point wise mutual in formation between pairs of words to analyze word co-locations and associations. Point-wise mutual information (PMI, Church and Hanks (1989)) is used to capture the semantic relatedness of the candidate to the topic of the document. Early approaches to identifying MWEs concentrated on their collocational behavior (Church and Hanks, 1989). Specifically, the measure Snom (vt ,vh) is derived from point-wise mutual information (Church and Hanks, 1989): Snom (vt ,vh)= log p (vt, vh|nom) p (vt) p (vh|pers) (3) where nom is the event of having a nominalized textual entailment pattern and pers is the event of having an agentive nominalization of verbs. Some of these are mutual information (Church and Hanks, 1989), distributed frequency (Tapanainen et al, 1998) and Latent Semantic Analysis (LSA) model (Baldwin et al, 2003). Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al, 1998), Distributed frequency of object using verb information (Venkatapathyand Joshi, 2005), Similarity of object in verb object pair using the LSA model (Baldwin et al,2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006). Mutual information is commonly used to measure the association strength between two words (Church and Hanks 1989). Church and Hanks (1989) proposed a measure of association called Mutual Information. We have used POINTWISE MUTUAL INFORMATION (PMI, Church and Hanks (1989)) to account for the differences in information value between the several headwords and attributes. The explanation for such a behavior is: since we are not throwing away any infrequent word pairs, PMI will rank pairs with low frequency counts higher (Church and Hanks, 1989).
On Coreferring: Coreference In MUC And Related Annotation Schemes paper, it is argued that &quot;coreference&quot; annotations, as performed in the MUC community for example, go well beyond annotation of the relation of coreference proper. As a result, it is not always clear what semantic relation these annotations are encoding. The paper discusses a number of problems with these annotations and concludes that rethinking of the coreference task is needed before the task is expanded. In particular, it suggests a division of labor whereby annotation of the coreference relation proper is separated from other tasks such as annotation of bound anaphora and of the relation between a subject and a predicative NP. With such a concept hierarchy as well as semantic relations with a precisely defined signature, we can for example overcome annotation problems of intensionality and predication as discussed in (van Deemter and Kibble, 2000). In line with (Krahmer and Piwek, 2000) and (van Deemter and Kibble, 2000) this is in our view a too strict definition of anaphora so that we propose a more relation-based classification of anaphoric and bridging relations. The core scheme is in principle identical with the MUC coreference scheme and is restricted to the annotation of coreference in the sense of (van Deemter and Kibble, 2000). Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continues to treat nominal predicates and appositive phrases as coreferential. In contrast, they do not fall under the identity relation in OntoNotes, which follows the linguistic understanding of coreference according to which nominal predicates and appositives express properties of an entity rather than refer to a second (coreferent) entity (van Deemter and Kibble, 2000). In order to be linguistically accurate (van Deemter and Kibble, 2000), we distinguish between referring and attributive NPs: while the first point to an entity, the latter express some of its properties. Proposals for annotating coreference such as (Hirschman, 1998) have been motivated by work on Information Extraction, hence the notion of coreference used is very difficult to relate to traditional ideas about anaphora (van Deemter and Kibble, 2000). Although often treated together, anaphoric pronoun resolution differs from coreference resolution (van Deemter and Kibble, 2000). As van Deemter and Kibble (2000) point out, however, the result is rather ad hoc; the IDENT relation as defined by the instructions doesn't capture any coherent definition of coreference. see (van Deemter and Kibble, 2000) for some problems with the choices made in MUCCS. Coreference has been defined by (van Deemter and Kibble, 2000) as the relation holding between linguistic expressions that refer to the same extra linguistic entity. It suffers however from a number of problems (van Deemter and Kibble, 2000), chief among which is the fact that the one semantic relation expressed by the scheme, ident, conflates a number of relations that semanticists view as distinct: besides COREFERENCE proper, there are IDENTITY ANAPHORA, BOUND ANAPHORA, and even PREDICATION. Following van Deemter and Kibble (2000), we define a coreference relation to hold between two NPs just in case they refer to the same extra-linguistic referent in the real world. These arguments give support to van Deemter and Kibble (2000), who argue that it is problematic that the annotation strategy assumed in MUC-6 and MUC-7 goes beyond a mark-up of coreference. NP coreference is related to the task of anaphora resolution, whose goal is to identify an antecedent for an anaphoric NP (i.e., an NP that depends on another NP, specifically its antecedent, for its interpretation) [see van Deemter and Kibble (2000) for a detailed discussion of the difference between the two tasks].
Tense As Discourse Anaphor In this paper, I consider a range of English expressions and show that their context-dependency can be characterized in terms of two properties: 1. They specify entities in an evolving model of the discourse that the listener is constructing; 2. The particular entity specified depends on another entity in that part of the evolving &quot;discourse model&quot; that the listener is currently attending to. expressions have been called show how tensed clauses share these characteristics, usually just attributed to anaphoric noun phrases. This not only allows us to capture in a simple way the but difficult-to-prove intuition that is anaphoric, also contributes to our knowledge of what is needed for understanding narrative text. More independent of this approach are suggestions made by Moens and Steedmalt (1988) and by Webber (1988). We define a temporal segment to be a fragment of text that does not exhibit abrupt changes in temporal focus (Webber, 1988).    However, previous research has found that demonstrative anaphors rarely refer to NPs, while it rarely refers to discourse segments (Webber (1988a)).  Webber (Webber, 1988) improved upon the above work by specifying rules for how events are related to one another in a discourse and Sing and Sing defined semantic constraints through which events can be related (Sing, 1997). Consider discourse (1), from (Webber 1988). 
Statistical Significance Tests For Machine Translation Evaluation If two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling methods to compute statistical significance of test results, and validate them on the concrete example of the Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real. (Koehn, 2004) gives empirical evidence that these give accurate estimates for Bleu statistics. The significance test on translation performance was per formed by the bootstrap method (Koehn, 2004) with a 5% significance level. Our BLEU score improvements of 1.2 to 1.9 points are statistically significant according to the paired bootstrap re sampling method (Koehn, 2004) with n= 1000 and p=0.01. The 95% confidence intervals as calculated by bootstrap re sampling (Koehn, 2004) are shown for each of the results. The 95% confidence intervals of our scores, computed by bootstrap re sampling (Koehn, 2004), indicate that a score increase of more than 1 BLEU is statistically significant. The improvement in BLEU is statistically significant (p= 0.01) using the paired bootstrap re sampling significance test (Koehn, 2004). Similarly to the distortion penalty in the conventional phrase based decoder (Koehn 2004b), the distortion cost of jumping from a word at position i to another word at position j, d (i, j), is proportional to the distance between i and j ,e.g., |i-j|. Decoding is based on a beam search algorithm similar to that of the phrase-based MT decoder (Koehn 2004b).  Using bootstrap re sampling (Koehn, 2004), the improvements in BLEU, TER, as well as the linear combination used in tuning are statistically significant at at least p =.05. Statistical significance in BLEU differences was tested by paired bootstrap re-sampling (Koehn, 2004). These intervals were computed following the boot strap technique described in (Koehn, 2004). Confidence intervals at 95% confidence level following (Koehn, 2004). We perform a bootstrap re sampling significance test (Koehn, 2004) on the output predictions of the local classifiers with and without the inference model. To see whether an improvement is statistically significant, we also conduct significance tests using the paired bootstrap approach (Koehn, 2004). To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrap approach (Koehn, 2004). All improvements on two test sets are statistically significant by the bootstrap resampling (Koehn, 2004). To measure whether the difference between system performance is statistically significant, we use bootstrap re sampling with 100 samples with the t test (Koehn, 2004). Statistical significance on the BLEU scores was tested using pairwise bootstrap sampling (Koehn, 2004). The marked systems produce statistically significant improvements as measured by bootstrap re sampling method (Koehn, 2004) on BLEU over the baseline system.
Exploring Content Models for Multi-Document Summarization We present an exploration of generative probabilistic models for multi-document summarization. Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way. Our model, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions. At the task of producing generic DUC-style summaries, state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al. (2007)’s state-of-the-art discriminative system. We explore capacity to produce multiple ‘topical summaries’ in order to facilitate content discovery and navigation. In TOPICSUM (Haghighi and Vanderwende, 2009), each word is generated by a single topic which can be a corpus-wide background distribution over common words, a distribution of document-specific words or a distribution of the core content of a given cluster. Models that use more structure in the representation of documents have also been proposed for generating more coherent and less redundant summaries, such as HIERSUM (Haghighi and Vanderwende, 2009) and TTM (Celikyilmaz and Hakkani-Tur, 2011). In our experiments, we follow the same approach as in (Haghighi and Vanderwende, 2009) by greedily adding sentences to a summary so long as they decrease KL divergence. Once this is done, one of the learned collections can be used to generate the summary that best approximates this collection, using the greedy algorithm described by Haghighiand Vanderwende (2009). The original implementations of SUMBASIC (Nenkova and Vanderwende, 2005) and TOPICSUM (Haghighiand Vanderwende, 2009) were defined over single words (unigrams). This model is very similar to the one used by Haghighi and Vanderwende (2009) in the context of text summarization. Effective ways of representing content and ensuring coverage are the subject of ongoing research in the field (e.g., Gillick et al 2009, Haghighi and Vanderwende 2009). The most relevant work is by (Haghighi and Vanderwende, 2009) on exploring content models for multi-document summarization. Entity-aspect model is similar with 'HIERSUM' content model proposed by Haghighi and Vanderwende (2009). In this baseline, we directly compare our method with "HIERSUM" proposed by (Haghighi and Vanderwende, 2009). One could also think of this as a version of the KLSum summarization system (Haghighi and Vanderwende, 2009) that stops after one sentence. In more recent work, Haghighi and Vanderwende (2009) built a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt. Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al, 2008). The following models are used as benchmark: (i) PYTHY (Toutanova et al, 2007): Utilizes human generated summaries to train a sentence ranking system using a classifier model; (ii) HIERSUM (Haghighi and Vanderwende, 2009): Based on hierarchical topic models. Haghighi and Vanderwende (2009) demonstrated that these models can improve the quality of generic multi-document summaries over simpler surface models. We re-implement the HIERSUM system from Haghighi and Vanderwende (2009), and show that using our objective dramatically improves the content of extracted summaries. This idea was first presented by Daume and Marcu (2006) for their BAYESUM system for query-focused summarization, and later adapted for non-query summarization in the TOPICSUM system by Haghighi and Vanderwende (2009). Haghighi and Vanderwende (2009) presented a version of HIERSUM that models documents as a bag of bigrams, and provides results comparable to PYTHY. These are based on the manual evaluation questions from DUC 2007, and are the same questions asked in Haghighi and Vanderwende (2009). Such models also provide a framework for adding additional structure to a summarization model (Haghighi and Vanderwende, 2009).
Using Restriction To Extend Parsing Algorithms For Complex-Feature-Based Formalisms  An answer to these problems was presented by Shieber (1985) who proposed to do Earley prediction on the basis of some finite quotient of all constituent DAGs which can be specified by the grammar writer. Change in the citation purpose of Shieber (1985) paper. Similar analysis can be applied to the change in citation purpose of Shieber (1985) as illustrated in Figure 1. Shieber (1985) proposes a more efficient approach to gaps in the PATR-II formalism, extending Earley's algorithm by using. PATR-II (Shieber, 1985) into context-free grammars (CFG). The abstraction is specified by means of a restrict or (Shieber, 1985), the so-called lexicon restrict or. Pereira and Warren (1983) and Shieber (1985) present versions of Earley's algorithm for unification grammars, in which unification is the sole operation responsible for attribute valuation. After establishing a correspondence b tween attribute and unification grammar (UG), we may see that the technique of 'restrictions'; used by Shieber (1985) in his extended algorithm is related to finite partitioning on attribute domains, in fact a particular case which takes advantage of the more structured attribute domains of UG. We can use the technique of restriction (Shieber 1985) to remove these features from our feature structures. [Shieber, 1985] therefore proposes a modified version of the Earley-parser, using restricted top down prediction.  For UGs which lack a so-called context-free back-bone, such as CUG, the top-down prediction step can only be guaranteed to terminate if we make use of restriction, as defined in Shieber (1985).  The performance of the parsing algorithms discussed in the preceding sections (a bottom-up parser for UG (BU), a top-down parser for UG (of Shieber, 1985) (TD), a top-down parser operating on an instantiated grammar (TD/1), and a bottom-up parser with top down filtering operating on an instantiated grammar (BU/LC)) were tested on two experimental CUGs, one implementing the morphosyntactic features of German NPs, and one implementing the syntax of WH-questions in Dutch by means of a gap-threading mechanism. Comparing our results with those of Shieber (1985) and Haas (1989), we see that in all cases top-down filtering may reduce the size of the chart significantly. However, such an adaptation of CF algorithms involves their extension to possibly infinite nonterminal domains, which, as Shieber (1985) and Haas (1989) have shown, is nontrivial. Shieber (1985, 1992) follows established terminology in speaking of top-down filtering in connection with the prediction step of the Earley algorithm. We view the linking relation not simply as a filter to increase fficiency within the domain of syntactic analysis - this aspect is stressed by Shieber (1985) and other investigators such as Bouma (1991) - but rather as a device for the top-down predictive instantiation of information, as Shieber et al (1990) have shown for semantic-head-driven generation.  This amounts to the simplest case of the restriction technique of Shieber (1985).
The Web As A Parallel Corpus Parallel corpora have become an essential resource for work in multilingual natural language processing. In this article, we report on our work using the STRAND system for mining parallel text on the World Wide Web,first reviewing the original algorithm and results and then presenting a set of significant enhancements. These enhancements include the use of supervised learning based on structural features of documents to improve classification performance, a new contentbased measure of translational equivalence, and adaptation of the system to take advantage of the Internet Archive for mining parallel text from the Web on a large scale. Finally, the value of these techniques is demonstrated in the construction of a significant parallel corpus for a low-density language pair. Approaches such as harvesting parallel corpora from the web (Resnik and Smith, 2003) address the creation of data. Besides, there are also some other types of methods for mining parallel corpora from the web such as the work in (Resnik, 1998), (Resnik and Smith, 2003) and (Zhang et al, 2006). Unfortunately, parallel corpora are not readily available in large quantities, except for a small subset of the world's languages (see Resnik and Smith (2003) for discussion), therefore limiting the potential use of current SMT systems. For example, Resnik and Smith (2003) propose mining the web to collect parallel corpora for low-density language pairs. Most existing studies, such as Nie (1999), Resnik and Smith (2003) and Shi (2006), mine parallel web documents within bilingual web sites first and then extract bilingual sentences from mined parallel documents using sentence alignment method. Resnik and Smith (2003) exploit the similarities in URL structure, document structure and other clues for mining the Web for parallel documents. Earlier work on corpus collection from the web (e.g. (Resnik and Smith, 2003)) gave some hope that reasonably large quantities of parallel text could be found on the web, so that a bitext collection could be built for interesting language pairs(with one member of the pair usually being English) relatively cheaply. At the other end of the spectrum, Resnik and Smith (2003) search the Web to detect web pages that are translations of each other. Resnik and Smith (2003) use a similar idea of candidates and filters in their STRAND system. These approaches are in essence a combination of the usage of lexico-syntactic pattens conveying a certain relation of interest such as in (Hearst, 1992), (Charniak and Berland, 1999), (Iwanska et al, 2000) or (Poesio et al, 2002) with the idea of using the web as a big corpus (Resnik and Smith, 2003), (Grefenstette, 1999), (Keller et al, 2002). STRAND (Resnik and Smith, 2003) is a system that acquires document pairs in parallel translation automatically from the Web. Due to its significant growth, the WWW has become an attractive database for different systems applications as, machine translation (Resnik and Smith, 2003), question answering (Kwok et al, 2001), commonsense retrieval (Matuszek et al, 2005), and so forth. Another approach to retrieving relevant documents involves the collection of relevant document URLs from the WWW (Resnik and Smith, 2003). This is for instance the case of PTMINER (Chen and Nie, 2000) and STRAND (Resnik and Smith, 2003), two systems that are intended to mine parallel documents over the Web. Resnik and Smith (2003) develop a method for gathering parallel corpora from the web. Resnik and Smith (2003) employ the Web as parallel corpora to provide bilingual sentences for translation models. (Resnik and Smith, 2003) show that parallel corpora for a variety of languages can be harvested on the Internet. Resnik and Smith (2003) extract bilingual sentences from the Web to create parallel corpora for machine translation. Starting from nothing other than a set of language codes, our extension of the STRAND algorithm (Resnik and Smith,2003) identifies potentially parallel documents using cues from URLs and document content. Our system is based on the STRAND algorithm (Resnik and Smith, 2003).
Multi-Engine Machine Translation Guided By Explicit Word Matching We describe a new approach for synthetically combining the output of several different Machine Translation (MT) engines operating on the same input. The goal is to produce a synthetic combination that surpasses all of the original systems in translation quality. Our approach uses the individual MT engines as “black boxes” and does not require any explicit cooperation from the original MT systems. A decoding algorithm uses explicit word matches, in conjunction with confidence estimates for the various engines and a trigram language model in order to score and rank a collection of sentence hypotheses that are synthetic combinations of words from the various original engines. The highest scoring sentence hypothesis is selected as the final output of our system. Experiments, using several Arabicto-English systems of similar quality, show a substantial improvement in the quality of the translation output. Because of the fundamental nature of the semantic similarity problem, there are close connections with other areas of human language technologies such as information retrieval (Salton and Lesk, 1971), text alignment in machine translation (Jayaraman and Lavie, 2005), text summarization (Mani and Maybury, 1999), and textual coherence (Foltz et al, 1998).  (Jayaraman and Lavie, 2005) tried to overcome this problem by using confidence scores and language models in order to rank a collection of synthetic combinations of words extracted from the original translation hypotheses. Combination techniques have earlier been applied to various applications including machine translation (Jayaraman and Lavie, 2005), part-of-speech tagging (Brill and Wu, 1998) and base noun phrase identification (Sang et al., 2000). Our system is an enhancement of our previous work (Jayaraman and Lavie, 2005). The algorithm is described fully by Jayaraman and Lavie (2005). Also, a more heuristic alignment method has been proposed in a different system combination approach (Jayaraman and Lavie, 2005). Other than confusion-network-based algorithms, work most closely related to ours is the method of MT system combination proposed in (Jayaraman and Lavie 2005), which we will refer to as J&L. Jayaraman and Lavie (2005) proposed a heuristic-based matching algorithm which allows non monotonic alignments to align the words between the hypotheses. (Jayaraman and Lavie, 2005) proposed another black-box system combination strategy. Karakos, et al (2008) proposed an ITG based method for hypothesis alignment, Rosti et al (2008) proposed an incremental alignment method, and a heuristic-based matching algorithm was proposed by Jayaraman and Lavie (2005).
Using Semantic Preferences To Identify Verbal Participation In Role Switching Alternations We propose a method for identifying diathesis alternations where a particular argument type is seen in slots which have different grammatical roles in the alternating forms. The method uses selectional preferences acquired as probability distributions over WordNet. Preferences for the target slots are compared using a measure of distributional similarity. The method is evaluated on the causative and conative alternations, but is generally applicable and does not require a priori knowledge specific to the alternation. In a sense this is encouraging, as it motivates our most exciting future work: augmenting this simple model to explicitly capture complementary information such as distributional semantics (Blei et al, 2003), diathesis alternations (McCarthy, 2000) and selectional preferences (OSeaghdha, 2010). In contrast to comparing head nouns directly, McCarthy (2000) instead compares the selectional preferences for each of the two slots (captured by a probability distribution over WordNet). The approach of McCarthy (2000), on the other hand, addresses the generalization problem by comparing probability distributions over WordNet. As mentioned above, McCarthy (2000) suggested the use of selectional profiles to capture generalizations over argument slots, so that two argument slots could be effectively compared for detecting alternations.  In the first method (that of McCarthy, 2000), the two profiles become identical. We evaluate our method on the causative alternation in order for comparison to the earlier methods of McCarthy (2000) and Merlo and Stevenson (2001). This means that the kind of straightforward propagation method used by McCarthy (2000) is not applicable to selectional profiles of this type. The method addresses conceptual problems of an earlier measure proposed by McCarthy (2000), which was limited to tree cut models (Li and Abe, 1998) and failed to distinguish detailed semantic differences between them. By comparison, McCarthy (2000) attained 73% accuracy on her set of hand-selected test verbs in a similar task; however, when applied to our various sets of randomly selected verbs, our replication of her method performed very poorly, rarely reaching above chance performance. The mapping for the dependents in the alternation can be taken from existing lexical resources (Dorr, 1997), learned from corpora (McCarthy, 2000) or learned from existing lexicons (Bond et al, 2002). As in McCarthy (2000), we cast argument alternation detection as a comparison of sense profiles across two different argument positions of a verb. Similarly, McCarthy (2000) uses skew divergence (a variant of KL divergence proposed by Lee, 1999) to compare the sense profile of one argument of a verb (e.g., the subject position of the intransitive) to another argument of the same verb (e.g., the object position of the transitive), to determine if the verb participates in an argument alternation involving the two positions. Because we demonstrate our new SPD measure on the same problem as McCarthy (2000), we provide more detail of her method here, for comparison. In McCarthy (2000), an error analysis reveals that the best method has more false positives than false negatives - some slots are considered overly similar because the sense profiles are compared at a coarse-grained level, losing fine semantic distinctions. In McCarthy (2000), the distributions are propagated to the lowest common subsumers (i.e., the nodes labelled B, C, and D). In the first method (that of McCarthy, 2000), the two profiles become identical. We evaluate our method on the causative alternation in order for comparison to the earlier method of McCarthy (2000). This means that the kind of straightforward propagation method used by McCarthy (2000) is not applicable to sense profiles of this type. The method addresses conceptual problems of an earlier measure proposed by McCarthy (2000), which was limited to tree cut models (Li and Abe, 1998) and failed to distinguish detailed semantic differences between them.
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp.  We perform term disambiguation on each document using an entity extractor (Cucerzan, 2007). More similarity features were added by Cucerzan (2007) who realized that topical coherence between a candidate entity and other entities in the context will improve NED accuracy and by Milne and Witten (2008) who built on Cucerzan's work. Cucerzan (2007) employed context vectors consisting of phrases and categories extracted from Wikipedia. This entity disambiguation data set was introduced by Cucerzan (2007). we implemented the approach brought by Cucerzan (2007) based on our best understanding. e. To model these characteristics, Bunescu and Pasca (2006) and Cucerzan (2007) incorporate information from Wikipedia articles, Artiles et al (2007) use Webpage content, Mann and Yarowsky (2003) extract biographic facts.  Cucerzan (2007) disambiguated the names by combining the BOW model with the Wikipedia category information. Cucerzan (2007) and Bunescu (2006) used Wikipedia's category information in the disambiguation process.  Cucerzan (2007), by contrast to the above, used Wikipedia primarily for Named Entity Disambiguation, following the path of Bunescu and Pasca (2006). As in our paper, and unlike the above mentioned works, Cucerzan (2007) made use of the explicit Category information found within Wikipedia. Cucerzan (2007) did not make use of the Category information in identifying the class of a given entity.  Bunescu and Pasca (2006) and Cucerzan (2007) presented important pioneering work in this area, but suffer from several limitations including Wikipedia specific dependencies, scale, and the assumption of a KB entry for each entity. Previous work by Bunescu and Pasca (2006) and Cucerzan (2007) aims to link entity mentions to their corresponding topic pages in Wikipedia but the authors differ in their approaches. We evaluated our system on two datasets: the Text Analysis Conference (TAC) track on Knowl edge Base Population (TAC-KBP) (McNamee and Dang, 2009) and the newswire data used by Cucerzan (2007) (Microsoft News Data). We downloaded the evaluation data used in Cucerzan (2007): 20 news stories from MSNBC with 642 entity mentions manually linked to Wikipedia and another 113 mentions not having any corresponding link to Wikipedia. 
Stochastic Inversion Transduction Grammars And Bilingual Parsing Of Parallel Corpora Technology introduce (1) a novel inversion transduction formalism bilingual modeling of sentence-pairs, and (2) the concept of parsing a variety of parallel corpus analysis applications. Aside from the bilingual orientation, three major features distinguish the formalism from the finite-state transducers more traditionally found in computational linguistics: it skips directly to a context-free rather than finite-state base, it permits a minimal extra degree of ordering flexibility, and its probabilistic formulation admits an efficient maximum-likelihood bilingual parsing algorithm. A convenient normal form is shown to exist. Analysis of the formalism's expressiveness suggests that it is particularly well suited to modeling ordering shifts between languages, balancing needed flexibility against complexity constraints. We discuss a number of examples of how stochastic inversion transduction grammars bring bilingual constraints to bear upon problematic corpus analysis tasks such as segmentation, bracketing, phrasal alignment, and parsing. The work reported in Wu (1997), which uses an inside-outside type of training algorithm to learn statistical context free transduction. The empirical adequacy of synchronous context-free grammars of rank two (2-SCFGs) (Satta and Peserico, 2005), used in syntax based machine translation systems such as Wu (1997). In this paper it is shown that the synchronous grammars used in Wu (1997), Zhang et al (2006) and Chiang (2007) are not expressive enough to do that.  Bilingual Bracketing [Wu 1997] is one of the bilingual shallow parsing approaches studied for Chinese-English word alignment. In [Wu 1997], the Bilingual Bracketing PCFG was introduced, which can be simplified as the following production rules. More suitable ways could be bilingual chunk parsing, and refining the bracketing grammar as described in [Wu 1997]. Among the grammar formalisms successfully put into use in syntax based SMT are synchronous context-free grammars (SCFG) (Wu, 1997). Parsing optimally relative to a synchronous grammar using a dynamic program requires time O(n6) in the length of the sentence (Wu, 1997). Although this is less than the O (n6) complexity of exact ITG (In version Transduction Grammar) model (Wu, 1997), a quintic algorithm is often quite slow. In this respect it resembles Wu's bilingual bracketer (Wu, 1997), but ours uses a different extraction method that allows more than one lexical item in a rule, in keeping with the phrase based philosophy. Bilingual bracketing methods were used to produce a word alignment in (Wu, 1997). We present a new method that exploits a novel application of Inversion Transduction Grammar or ITG expressiveness constraints (Wu 1995 [1], Wu 1997 [2]) for mining monolingual data to obtain tight sentence translation pairs, yielding accuracy significantly higher than previous known methods. (Wu, 1997) introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. Joint parsing with a simplest synchronous context-free grammar (Wu, 1997) is O (n6) as opposed to the monolingual O (n3) time. The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages. The ITG we apply in our experiments has more structural labels than the primitive bracketing grammar: it has a start symbol S, a single preterminal C, and two intermediate nonterminals A and B used to ensure that only one parse can generate any given word-level alignment, as discussed by Wu (1997) and Zens and Ney (2003). Wu (1997) demonstrated that for pairs of sentences that are less than 16 words, the ITG alignment space has a good coverage over all possibilities. Besides, our model, as being linguistically motivated, is also more expressive than the formally syntax-based models of Chiang (2005) and Wu (1997). One way around this difficulty is to stipulate that all rules must be binary from the outset, as in inversion-transduction grammar (ITG) (Wu, 1997).
A Joint Model of Text and Aspect Ratings for Sentiment Summarization Online reviews are often accompanied with numerical ratings provided by users for a set of service or product aspects. We propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings – a fundamental problem in aspect-based sentiment summarization (Hu and Liu, 2004a). Our model achieves high accuracy, without any explicitly labeled data except the user provided opinion ratings. The proposed approach is general and can be used for segmentation in other applications where sequential data is accompanied with correlated signals.  However, these two models either require post processing to calculate the positive/negative coverage in a document for polarity identification (Mei et al, 2007) or require some kind of supervised setting in which review text should contain ratings for aspects of interest (Titov and McDonald, 2008a). The multi-aspect sentiment (MAS) model (Titov and McDonald, 2008a), which is extended from the multi-grain latent Dirichlet allocation (MG-LDA) model (Titov and McDonald, 2008b), allows sentiment text aggregation for sentiment summary of each rating aspect extracted from MG-LDA. Unlike some of the previous work (e.g., (Titov and McDonald, 2008a)), we do not constrain aspect specific sentiment to be the same across the document. In contrast, MLSLDA draws on techniques that view sentiment as a regression problem based on the topics used in a document, as in supervised latent Dirichlet allocation (SLDA) (Blei and McAuliffe, 2007) or in finer-grained parts of a document (Titov and McDonald, 2008). Aspect-based sentiment analysis summarizes sentiments with diverse attributes, so that customers may have to look more closely into analyzed sentiments (Titov and McDonald, 2008). There have been various extensions to multi-grain (Titov and McDonald, 2008a), labeled (Ramage et al, 2009), partially-labeled (Ramage et al, 2011), constrained (Andrzejewski et al, 2009) models, etc. Titov and McDonald (2008b) proposed a joint model of text and aspect ratings which utilizes a modified LDA topic model to build topics that are representative of ratable aspects, and builds a set of sentiment predictors.
OntoNotes: The 90% Solution hovy mitch martha.palmer lance.ramshaw weischedel @isi.edu @cis.upenn.edu @colorado.edu @bbn.com @bbn.com We describe the OntoNotes methodology and its result, a large multilingual richly-annotated corpus constructed at 90% interannotator agreement. An initial portion (300K words of English newswire and 250K words of Chinese newswire) will be made available to the community during 2007. The data we used for our experiments are developed as part of the OntoNotes project (Hovy et al, 2006) and they come from a variety of sources. However, the CoNLL data sets come from OntoNotes (Hovy et al, 2006), where singleton entities are not annotated, and BLANC has a wider dynamic range on data sets with singletons (Recasens and Hovy, 2011). The second set is radically different as it comprised 750 pairs of glosses from OntoNotes 4.0 (Hovy et al, 2006) and WordNet 3.1 (Fellbaum, 1998) senses. OntoNotes (Hovy et al, 2006) is a project that has annotated several layers of semantic information including word senses, at a high inter-annotator agreement of over 90%. Overall, however, this data indicates that the approach suggested by (Palmer, 2000) and that is being adopted in the ongoing OntoNotes project (Hovyet al, 2006) does result in higher system performance. In this work we focus on two datasets of hand-labeled sense groupings for WordNet: first, a dataset of sense groupings over nouns, verbs, and adjectives provided as part of the SENSEVAL-2 English lexical sample WSD task (Kilgarriff, 2001), and second, a corpus-driven mapping of nouns and verbs in WordNet 2.1 to the Omega Ontology (Philpot et al, 2005), produced as part of the ONTONOTES project (Hovy et al, 2006). We then merged the word alignment annotation with the TreeBank and PropBank annotation of Ontonotes4.0 (Hovy et al, 2006), which includes a wide array of data sources like broadcast news, news wire, magazine, web text, etc. Suggestions came from the previous named entity annotation of PERSONs, organizations (GROUP), and LOCATIONs, as well as heuristic lookup in lexical resources - Arabic WordNet entries (Elkateb et al, 2006) mapped to English WordNet, and named entities in OntoNotes (Hovy et al, 2006). Arabic is no exception: the publicly available NER corpora - ACE (Walker et al 2006), ANER (Benajiba et al 2008), and OntoNotes (Hovy et al 2006) - all are in the news domain. In some projects (e.g. OntoNotes (Hovy et al 2006)), the percentage of agreements between two annotators is used, but a number of more complex measures are available (for a comprehensive survey see (Artstein and Poesio, 2008)). The OntoNotes 90% solution (Hovy et al 2006) actually means such a degree of granularity that enables a 90% IAA. The corpus consists of texts of the Wall Street Journal corpus, and is hand-tagged with OntoNotes senses (Hovy et al, 2006). However, the performance of our model, trained using the OntoNotes corpus (Hovy et al, 2006), fell short of separate parsing and named entity models trained on larger corpora, annotated with only one type of information. We used OntoNotes 3.0 (Hovy et al, 2006), and made the same data modifications as (Finkel and Manning, 2009b) to ensure consistency between the parsing and named entity annotations. Elsewhere, similar, iterative annotation processes have yielded significant improvements in agreement for word sense and coreference (Hovy et al, 2006). We use the English part of the SemEval-2010 CR task data set, a subset of OntoNotes 2.0 (Hovy et al, 2006). Turkers were presented sentences from the test portion of the word sense induction task of SemEval-2007 (Agirre and Soroa, 2007), covering 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al, 2006) senses. This latter convention recently seems to be gaining ground in data sets like the Google 1T n-gram corpus (LDC# 2006T13) and OntoNotes (Hovy et al, 2006). In the OntoNotes project (Hovy et al., 2006), annotators use small-scale corpus analysis to create sense inventories derived by grouping together WordNet senses, with the procedure restricted to maintain 90% inter-annotator agreement. For experiments with PropBank, we used the Ontonotes corpus (Hovy et al, 2006), version 4.0, and only made use of the Wall Street Journal documents; we used sections 221 for training, section 24 for development and section 23 for testing.
Online Large-Margin Training for Statistical Machine Translation We achieved a state of the art performance in statistical machine translation by using a large number of features with an onlinelarge-margin training algorithm. The mil lions of parameters were tuned only on a small development set consisting of less than1K sentences. Experiments on Arabic-to English translation indicated that a modeltrained with sparse binary features outper formed a conventional SMT system with a small number of features. Sentence-level approximations to BLUE exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform BLUE computations in the context of a set O of previously-translated sentences, following Watanabe et al (2007). Work has been done to investigate a perceptron-like online margin training for statisitical machine translation (Watanabe et al, 2007). MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). The incorporation of a large number of sparse feature functions is described in (Watanabe et al, 2007). Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al, 2007), MERT is the most widely used algorithm for system optimization. Sparse features used in reranking are extracted according to (Watanabe et al, 2007). (Watanabe et al, 2007) also reports the possibility of overfitting in their dataset (Arabic-English newswire translation), especially when domain differences are present. In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al, 2005) and MT (Watanabe et al, 2007) uses iterative sets of N-best lists in its training process. The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al 2006), and extended to use large-margin training in an online frame work (Watanabe et al 2007).
An Algorithm For Pronominal Anaphora Resolution This paper presents an algorithm for identifying the noun phrase antecedents of third person pronouns and lexical anaphors (reflexives and reciprocals). The algorithm applies to the syntactic representations generated by McCord's Slot Grammar parser and relies on salience measures derived from syntactic structure and a simple dynamic model of attentional state. Like the parser, the algorithm is implemented in Prolog. The authors have tested it extensively on computer manual texts and conducted a blind test on manual text containing 360 pronoun occurrences. The algorithm successfully identifies the antecedent of the pronoun for 86% of these pronoun occurrences. The relative contributions of the algorithm's components to its overall success rate in this blind test are examined. Experiments were conducted with an enhancement of the algorithm that contributes statistically modelled information concerning semantic and real-world relations to the algorithm's decision procedure. Interestingly, this enhancement only marginally improves the algorithm's performance (by 2%). The algorithm is compared with other approaches to anaphora resolution that have been proposed in the literature. In particular, the search procedure of Hobbs' algorithm was implemented in the Slot Grammar framework and applied to the sentences in the blind test set. The authors' algorithm achieves a higher rate of success (4%) than Hobbs' algorithm. The relation of the algorithm to the centering approach is discussed, as well as to models of anaphora resolution that invoke a variety of informational factors in ranking antecedent candidates. our model incorporates the text-level of anaphora resolution, a shortcoming of the original SG approach that has recently been removed (Lappin and Leass, 1994), but still is a source of lots of problems. We have used the same weights, listed in table 2, proposed by Lappin and Leass (1994).  Lappin and Leass (1994) extracted rules from the output of the English Slot Grammar (ESG) (McCord, 1993). Only one of the four is explicitly aimed at personal-pronoun anaphora RAP (Resolution of Anaphora Procedure) (Lappin and Leass, 1994). In the heuristic salience-based algorithm for pronoun resolution, Lappin and Leass (1994) introduce a procedure for identifying anaphorically linked NP as a cluster for which a global salience value is computed as the sum of the salience values of its elements. Lappin and Leass (1994), for example, use several heuristics to filter out expletive pronouns, including a check for patterns including modal adjectives. We implemented a coreference resolution tool using a shallow rule-based approach inspired by Lappin and Leass (1994) and Bontcheva et al (2002). The new tool combines Hobbs algorithm (Hobbs, 1978) and the Resolution of Anaphora Procedure (RAP) algorithm (Lappin and Leass, 1994). (Lappin and Leass, 1994) describe several syntactic heuristics for reflexive, reciprocal and pleonastic anaphora, among others. Definiteness (Lappin and Leass, 1994). Non-prepositional NP (Lappin and Leass, 1994). Pleonastic (Lappin and Leass, 1994). Syntactic Parallelism (Lappin and Leass, 1994).  Other works, however, distinguish between restrictions and preferences (e.g. Lappin and Leass (1994)). Lappin and Leass (1994) describe an algorithm for pronominal anaphora resolution that achieves a high rate of correct analyses (85%). More recently, Kennedy and Boguraev (1996) propose an algorithm for anaphora resolution that is actually a modified and extended version of the one developed by Lappin and Leass (1994). Lappin and Leass (1994) has also been implemented in our system and an accuracy of 64% was attained. Implementation of constraints and preference scan be based on empirical insight (Lappin and Leass, 1994).
A Maximum Entropy Approach To Identifying Sentence Boundaries We present a trainable model for identifying sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of., ?, and !as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Romanalphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains. finding sentence boundaries (Reynar and Ratnaparkhi, 1997). This data was sentence-segmented using MxTerminator (Reynar and Ratnaparkhi, 1997) and parsed with the Stanford Parser (Klein and Manning, 2003). We trained a publicly available sentence splitter (Reynar and Ratnaparkhi, 1997) on a small manually annotated sample (1,000 sentences per domain per language) and applied it to our corpora. We also used MXTerminator (Reynar and Ratnaparkhi, 1997) for sentence segmentation, MINIPAR (Lin, 1993) for lemmatization and dependency parsing, and MATLAB3 for SVD computation. Sentence segmentation Off-the-shelf sentence segmentators tend to be trained on newswire texts (Reynar and Ratnaparkhi, 1997), which significantly differ from the noisy text in our corpus. Another statistical system, mxTerminator (Reynar and Ratnaparkhi, 1997) employs simpler lexical features of the words to the left and right of the candidate period. One common objection to supervised SBD systems is an observation in (Reynar and Ratnaparkhi, 1997), that training data and test data must be a good match, limiting the applicability of a model trained from a specific genre. Many alternatives suggest themselves to expand the options, including maximum entropy models, which have been previously successfully applied to, inter alia, sentence boundary detection (Reynar and Ratnaparkhi, 1997), and transformation-based learning, as used in part-of-speech tagging and statistical parsing applications (Brill, 1995). The corpus was prepared using MXTerminator (Reynar and Ratnaparkhi,1997) for sentence segmentation, BBN Identifinder (Bikel et al, 1999) for named entity recognition, as well as the aforementioned ASSERT for identification of verb predicate-argument structures and PropBank-style semantic role labeling of the arguments. As the text part may consist of more than one sentence, we first perform sentence splitting using Mxterminator (Reynar and Ratnaparkhi, 1997), a maximum 83 entropy-based end of sentence classifier trained onthe Penn Treebank data. We have tokenized the text using the Grok-OpenNLP tokenizer (Morton, 2002) and split the sentences using MXTerminator (Reynar and Ratnaparkhi, 1997). Obtained by segmenting (Reynar and Ratnaparkhi, 1997) the interviewee turns, and discarding sentences with only one word. finding sentence boundaries (Reynar and Ratnaparkhi, 1997). Table 1 presents information about article length (measured in sentences, as determined by the sentence separator of Reynar and Ratnaparkhi (1997)), vocabulary size, and token/type ratio for each domain. Each abstract set was prepared for annotation as follows: the order of the abstracts was randomized and the abstracts were broken into sentences using Mxterminator (Reynar and Ratnaparkhi, 1997). It can be resolved fairly easily with rules in the form of regular expressions or in a machine-learning framework (Reynar and Ratnaparkhi, 1997). The contents of these URLs were collected and only distinct web pages were retained. We use an HTMLparser3 to extract the textual con tents, and perform sentence segmentation (Reynar and Ratnaparkhi, 1997) on the parsed web pages. To prepare this corpus for analysis, we extracted the body text from each of the 4.1 million entries in the corpus and applied a maximum-entropy algorithm to identify sentence boundaries (Reynar and Ratnaparkhi, 1997). Sentence splitting, using mxterminator (Reynar and Ratnaparkhi, 1997). To produce this, we segment sentences with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse the corpus with the self trained Charniak parser (McClosky et al, 2006).
Probabilistic Parsing For German Using Sister-Head Dependencies We present a probabilistic parsing model for German trained on the Negra treebank. We observe that existing lexicalized parsing models using head-head dependencies, while successful for English, fail to outperform an unlexicalized baseline model for German. Learning curves show that this effect is not due to lack of training data. We propose an alternative model that uses sister-head dependencies instead of head-head dependencies. This model outperforms the baseline, achieving a labeled precision and recall of up to 74%. This indicates that sister-head dependencies are more appropriate for treebanks with very flat structures such as Negra. Experiments described in Section 4.3 used the same development and test sets but files 200-959 of WSJ as a smaller training set; for NEGRA we followed Dubey and Keller (2003) in using the first 18,602 sentences for training, the last 1,000 for development, and the previous 1,000 for testing.  Additionally, we show a limited number of results on the Negra corpus, using the standard training/development/test splits, defined in (Dubey and Keller, 2003). However, Dubey and Keller (2003) have demonstrated that lexicalization does not help a Collins-style parser that is trained on this corpus, and Levy and Manning (2004) have shown that its context-free representation is a poor approximation to the underlying dependency structure. It turns out, however, that lexicalization is not unproblematic: First, there is evidence that full lexicalization does not carry over across different tree-banks for other languages, annotations or domains (Dubey and Keller, 2003). There are even studies showing that lexicalization can be harmful when parsing richly inflected languages like German (Dubey and Keller, 2003) and Turkish (Eryigit and Oflazer, 2006). Efforts have been made to adapt existing CFG models to German (Dubey and Keller, 2003), but the results still don't compare to state-of-the art parsing of English. Dubey and Keller (2003) analyze the difficulties that Germanim poses on parsing. Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negra treebank (Skut et al, 1997) reports that lexicalization of PCFGs decrease the parsing accuracy when parsing Negra's flat constituent structures. Relevant, in principle, to our discussion here, are also the results obtained with tree bank grammars for German: (Dubey and Keller, 2003) have trained a PCFG on the Negra corpus (Skut et al, 1998), reporting labelled precision and recall between 70 and 75%. This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). The learning curves over increasing training data (e.g., for German (Dubey and Keller, 2003)) show that tree bank size can not be the sole factor to account for the inferior performance.    The comparison of the experiments with (line 2) and without grammatical functions (line 1) confirms the findings of Dubeyand Keller (2003) that the task of assigning correct grammatical functions is harder than mere constituent-based parsing. To allow comparisons with earlier work on NEGRA parsing, we use the same split of training, development and testing data as used in Dubey and Keller (2003). Overall, the best-performing model, using Brants smoothing, achieves a labelled bracketing F-score of 76.2, higher than earlier results reported by Dubey and Keller (2003) and Schiehlen (2004).  able 4 lists the result of the best model presented here against the earlier work on NEGRA parsing described in Dubey and Keller (2003) and Schiehlen (2004).
Collecting Highly Parallel Data for Paraphrase Evaluation A lack of standard datasets and evaluation has prevented the field of paraphrasmaking the kind of rapid progress enjoyed by the machine translation community over the last 15 years. We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale. The highly parallel nature of this data allows us to use simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates. In addition to being simple and efficient to compute, experiments show that these metrics correlate highly with human judgments. We used the Multiple Video Description Corpus (Chen and Dolan, 2011) obtained from multiple descriptions of short videos. The dataset consists of 1,500 pairs of short video descriptions collected using crowd sourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al, 2012). Such corpora have also been created manually through crowd sourcing (Chen and Dolan, 2011). Figure 2: Definition and instructions for annotation to provide a one-sentence description of the main action or event in the video (Chen and Dolan, 2011). 3. video descriptions Descriptions of short YouTube videos obtained via Mechanical Turk (Chen and Dolan, 2011). The STS task data includes five subtasks with text pairs from different sources: the Microsoft Research Paraphrase Corpus (Dolan et al, 2004) (MSRpar), The Microsoft Research Video corpus (Chen and Dolan, 2011) (MSRvid), statistical machine translation output of parliament proceedings (Koehn, 2005) (SMT-eur). MSR video data (Chen and Dolan, 2011). To encourage quality contributions, we use a tiered payment structure (Chen and Dolan, 2011) that rewards the good workers. However, as pointed out by (Chenand Dolan, 2011), there is the lack of automatic metric that is capable to measure all the three criteria in paraphrase generation. Examples of groundings include pictures (Rashtchianu et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). Consider the following excerpts from a video description corpus (Chen and Dolan, 2011): A man is sliding a cat on the floor. Given a large set of videos and a number of descriptions for each video (Chen and Dolan, 2011), we build a system that can recognize fluent and accurate descriptions of videos.
Multipath Translation Lexicon Induction Via Bridge Languages 6: Multipath Translation Induction language word), so the system's performance is lower than the Section 3 results. Since all available dictionaries are incomplete, it is difficult to decide which set of English words to compare against. Table 6 presents results for different choices of word coverage: the subset of existing pairs for English-Spanish, the union over all languages, and the intersection of all languages. Trends across subsets are relatively consistent. As an illustration, Table 7 shows consensus formation on English-Norweigian and English-Portuguese translation mappings via multiple bridge languages. Note that the English-French dictionary used here has no entry for &quot;bait&quot;, preventing its use as a bridge language for this word. As can be seen in Table 6, the distance-based combination methods are more successful at combining the different proposals than the rank-N combinations. One possible explanation for this is that rankbased classifiers pick the candidate with the best allaround distance, while distance-based combinations choose the single best candidate. Choosing the best all-around performer is detrimental when cognates exist for some languages but not for others. English Bridge language Bridge Word Target Word Score Rank bay (NORWEGIAN) Danish German Dutch bugt bucht baai bukt bukt baug (bow) bukt 1 1 1 1 25 2 1.5 2.5 distance-based method: bukt 1 1 rank-based method: bukt 27 1 (PORTUGUESE) bait Italian esca isca .5 1 nada (nothing) 3 54 Spanish carnada corneta (trumpet) 2 1 nada 3 12 isca 3.5 153 Romanian nada, nada (nothing) 0.5 1 isca 3.5 153 French N/A N/A N/A N/A distance-based method: isca 0.5 1 nada 0.5 2 rank-based method: nada 67 1 isca 307 20 Table 7: End-to-End Multipath Translation Induction The performance of an oracle, if allowed to choose the correct translation if it appears within the top-N in any language, would provide an upper bound for the performance of the combination methods. Results for such oracles are also reported in Table 6. The methods corresponding to &quot;oracle-1&quot; and &quot;distance&quot; are choosing from the same set of proposed targets, and the &quot;distance&quot; method achieves performance close to that of the oracle (77 vs. 82.8). 6 Path Differences This section investigates the effect of different pathway configurations on the performance of the final multi-path system by examining the following situations: • English to Portuguese, using the other Romance languages as bridges. • English to Norwegian, using the Germanic languages as bridges. • English to Ukrainian, using the Slavic languages as bridges. • Portuguese to English, using the Germanic languages and French as bridges. The results of these experiments are shown in Taen=English, pt=Portuguese, fr=French, it=Italian, es=Spanish, ro=Romanian, du=Dutch, no=Norwegian, de=German, da=Danish, cz=Czech, uk=Ukrainian, po=Polish, sr=Serbian, ru=Russian The data sets used in these experiments were apthe same size as those used in the previous experiment 1100-1300 translation word Dictionaries for Russian and Ukrainian were converted into romanized pronunciation dictionaries. There are three observations which can be made from the multipath results. 1. Adding more pathways usually results in an accuracy improvement. When there is a drop in accuracy on the cognate vocabulary by adding an additional bridge language there tends to be an improvement in accuracy on the full vocabulary due to significantly more cognate pathways (yielding greater coverage). 2. It is difficult to substantially improve upon the performance of the single closest bridge language, especially when they are as close as enes-pt. Improvements on performance relative to the single best ranged from 2% to 20%. 3. Several mediocre pathways can be combined to improve performance. Though it is always better to find one high-performing pathway, it is often possible to get good performance from the combination of several, less well-performing pathways (e.g. en-[sr po]-uk vs. en-ru-uk). In Table 8 &quot;Cvg&quot; or cognate coverage is the percentage words in the source language for which any of the bridge languages contains a cognate to the target translation. Italian and French bridges, for example, offer additional translation pathways to Portuguese which augment the Spanish pathways. Path Accuracy on Full Vocab Accuracy Cvg Cognate Vocab en-es-pt 58.7 86.7 65.5 en-it-pt 44.0 85.4 31.9 en-fr-pt 30.6 74.3 24.8 en-[fr it]-pt 41.2 79.4 42.2 en-[fr it es]-pt 60.2 84.2 70.3 en-da-no 71.9 92.4 75.4 en-du-no 36.1 76.7 39.8 en-de-no 36.1 74.7 38.9 en-[du de]-no 42.3 72.2 54.3 en-[da du de]-no 77.0 87.5 87.4 en-ru-uk 48.8 89.0 44.7 en-po-uk 38.1 87.8 31.9 en-sr-uk 31.9 86.7 30.8 en-[sr po]-uk 45.0 82.0 50.3 en-[ru sr po]-uk 58.4 74.6 71.0 pt-du-en 29.1 69.0 38.4 pt-fr-en 28.1 84.0 24.2 pt-de-en 25.3 68.4 32.1 pt-[de fr]-en 36.5 72.5 48.5 pt-[de fr du]-en 47.0 69.7 66.6 Table 8: Translation Accuracy via Different Bridge Language Paths (using L-A model) Using all languages together improves coverage, although this often does not improve performance over using the best single bridge language. As a final note, Table 9 shows the cross-language translation rates for some of the investigated languages. When translating from English to one of the Romance languages, using Spanish as the bridge language achieves the highest accuracy; and using Russian as the bridge language achieves the best performance when translating from English to the Slavic languages. However, note that using English alone without a bridge language when translating to the Romance languages still achieves reasonable performance, due to the substantial French and Latinate presence in English vocabulary. 7 Related Work Probabilistic string edit distance learning techniques have been studied by Ristad and Yianilos (1998) for use in pronunciation modeling for speech recognition. Satta and Henderson (1997) propose a transformation learning method for generic string transduction. Brill and Moore (2000) propose an alternative string distance metric and learning algorithm. While early statistical machine translation models, such as Brown et al. (1993), did not use any cognate based information to seed their wordto-word translation probabilities, subsequent models (Chen, 1993 and Simard et al., 1992) incorporated some simple deterministic heuristics to increase the translation model probabilities for cognates. Other methods have been demonstrated for building bilingual dictionaries using simple heuristic rules includes Kirschner (1982) for English/Czech dictionaries and Chen (1998) for Chinese/English proper names. Tiedemann (1999) improves on these alignment seedings by learning all-or-nothing rules for detecting Swedish/English cognates. Hajie et al. (2000) has studied the exploitation of language similarity for use in machine translation in the case of the very closely related languages (Czech/Slovak). Covington (1998) uses an algorithm based on heuristic orthographic changes to find cognate words for purposes of historical comparison. Perhaps the most comprehensive study of word alignment via string transduction methods was pioneered by Knight and Graehl (1998). While restricted to single language transliteration, it very effectively used intermediary phonological models to bridge direct lexical borrowing across distant languages. 8 Conclusion The experiments reported in this paper extend prior research in a number of directions. The novel probabilistic paradigm for inducing translation lexicons for words from unaligned word lists is introduced. The set of languages on which we demonstrate these methods is broader than previously examined. Finally, the use of multiple bridge languages and of the high degree of intra-family language similarity for dictionary induction is new. There are a number of open questions. The first is whether there exists a better string transformation algorithm to use in the induction step. One possible area of investigation is to use larger dictionaries and assess how much better stochastic transducers, and distance metrics derived from them, perform with more training data. Another option is to investigate the use of multi-vowel or multi-consonant compounds which better reflect the underlying phonetic units, using an more sophisticated edit distance measure. In this paper, we explore ways of using cognate pairs to create translation lexicons. It is an interesting research question as to whether we can augment these methods with translation probabilities estimated from statistical frequency information gleaned from loosely aligned or unaligned bilingual corpora for non-cognate pairs. Various machine learning techniques, including co-training and mutual bootstrapping, could employ these additional measures in creating better estimates. The techniques presented here are useful for language pairs where an on-line translation lexicon does not already exist, including the large majority of the world's lower-density languages. For language pairs with existing translation lexicons, these methods can help improve coverage, especially for technical vocabulary and other more recent borrowings which are often cognate but frequently missing from existing dictionaries. In both cases, the great potential of English -x Romance Accuracy on Cognate Vocab (35-68%) TL Bridge Language pt it es fr ro 0 pt (100) 85.6 86.7 74.3 72.1 79.4 it 83.7 (100) 85.1 75.5 82.1 78.0 es 85.8 84.0 (100) 78.1 82.1 79.3 fr 73.9 75.5 76.7 (100) 75.2 78.7 ro 72.8 84.4 82.8 76.1 (100) 78.3 av 78.2 82.0 82.2 75.7 77.7 78.4 English -x Romance Accuracy on Full Vocab TL Bridge Language pt it es fr ro 0 pt (100) 42.6 58.7 29.8 28.4 23.1 it 42.0 (100) 45.6 33.8 34.8 21.3 es 57.5 44.3 (100) 31.8 29.7 22.5 fr 30.7 35.2 32.7 (100) 33.3 24.9 ro 28.5 35.7 30.5 35.0 (100) 23.9 av 39.2 39.0 41.2 32.0 31.0 22.6 English -x Slavic Accuracy on Cognate Vocab TL Bridge Language cz ru pl sr uk 0 cz (100) 70.3 81.4 81.0 81.4 75.0 ru 72.7 (100) 84.1 80.3 87.3 73.9 pl 81.2 85.7 (100) 84.5 88.2 78.2 sr 85.7 82.9 85.8 (100) 85.5 76.7 uk 83.6 89.1 87.9 86.0 (100) 73.9 av 80.2 81.5 84.2 82.7 85.2 75 English -x Slavic Accuracy on Full Vocab TL Bridge Language cz ru pl sr uk 0 cz (100) 20.5 25.5 27.3 25.4 12.0 ru 23.3 (100) 29.9 27.3 47.1 13.4 pl 27.6 30.3 (100) 27.8 36.8 15.0 sr 31.0 29.6 29.4 (100) 33.1 18.5 uk 27.0 48.7 38.0 31.4 (100) 15.7 av 27 31.7 30.2 28 35.2 14.6 Table 9: Accuracy of English to TL (Target Language) via One Bridge Language (using L-A model) (0 = direct mapping no bridge) this work is the ability to leverage a single bilingual dictionary into translation lexicons for its entire language family, without any additional resources beyond raw wordlists for the other languages in the family. 9 Acknowledgements The authors would like to thank the following people for their insightful comments and feedback on drafts of this work: Radu Florian, Jan Hajie, Ellen Riloff, Charles Schafer, and Richard Wicentowski. Thanks also to the Johns Hopkins NLP lab in general for the productive and stimulating environment. References E. Brill and R. Moore. 2000. An improved errorfor noisy channel spelling correction. ACL, 286-293. P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and R. Mercer. 1993. The mathematics of statistical translation. Linguistics, 19(2):263-311. Buck. 1949. A of Selected Synonyms in the Principal Indo-European Languages. Chicago:University of Chicago Press. H-H. Chen, S-J. Huang, Y-W. Ding, and S-C. Tsai. 1998. Proper name translation in cross-language retrieval. of ACL/COLING, pages 232-236. Chen. 1993. Aligning sentences in bilingual corusing lexical information. of ACL, pages 9-16. M. Covington. 1998. Aligning multiple languages historical comparison. of COLING- 275-280. J. Hajie, J. Hric, and V. Kubori. 2000. Cesilko : Machine translation between closely related lanof ANLP, 7-12. Jelinek. 1997. Methods for Speech Press. Z. Kirshner. 1982. A dependency based analysis of english for the purpose of machine translation. Explizite Beschreibung der Sprache und automa- Textbearbeitung, Knight and J. Graehl. 1998. Machine transliter- Linguistics, E. Ristad and P. Yianilos. 1998. Learning string distance. Trans. PAMI, G. Satta and J. Henderson. 1997. String transforlearning. of ACL/EACL, 444- 451. M. Simard, G.F. Foster, and P. Isabelle. 1992. Using cognates to align sentences in bilingual corpora. There is work on lexicon induction using string distance or other phonetic/orthographic comparison techniques, such as Mann and Yarowsky (2001). Moreover, while some techniques (e.g., Mann and Yarowsky (2001)) use multiple languages, the languages used have resources such as dictionaries between some language pairs. Mann and Yarowsky (2001) present a method for inducing translation lexicons based on trasduction modules of cognate pairs via bridge languages. Similarly to Mann and Yarowsky (2001), we show that languages are often close enough to others within their language family so that cognate pairs between the two are common, and significant portions of the translation lexicon can be induced with high accuracy where no bilingual dictionary or parallel corpora may exist. Mann and Yarowsky (2001) applied the stochastic transducer of Ristad and Yianilos (1998) for inducing translation lexicons between two languages, but found that in some cases it offered no improvement over Levenshtein distance. LLW stands for Levenshtein with learned weights, which is a modification of RY proposed by Mann and Yarowsky (2001). As mentioned in (Mann and Yarowsky, 2001), it appears that there are significant differences between the pronunciation task and the cognate identification task. Mann and Yarowsky (2001) saw little improvement over Edit Distance when applying this transducer to cognates, even when filtering the transducer's probabilities into different weight classes to better approximate Edit Distance. For example, Mann and Yarowsky (2001) define a word pair (e, f) to be cognate if they are a translation pair (same meaning) and their Edit Distance is less than three (same form). The marked difference in the availability of monolingual vs parallel corpora has led several researchers to develop methods for automatically learning bilingual lexicons, either by using monolingual corpora (Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Haghighi et al., 2008) or by exploiting the cross-language evidence of closely related "bridge" languages that have more resources (Mann and Yarowsky, 2001). Mann and Yarowsky (2001) investigated the induction of translation lexicons via bridge languages. Mann and Yarowsky (2001) developed yet another model, which outperformed all other similarity measures. The HMM model of (Mann and Yarowsky, 2001) is of distinctly different design than our PHMM model. Mann and Yarowsky (2001) present a method for inducing translation lexicons based on transduction models of cognate pairs via bridge languages. Our work is inspired by Mann and Yarowsky (2001). Mann and Yarowsky (2001) distinguish between static metrics, which are sufficiently general to be applied to any language pair, and adaptive metrics, which are adapted to a specific language pair. Mann and Yarowsky (2001) use variants of Levenshtein distance as a static metric, and a Hidden Markov Model (HMM) and a stochastic transducer trained with the Expectation-Maximisation (EM) algorithm as adaptive metrics. Mann and Yarowsky (2001) consider a word pair as cognate if the Levenshtein distance between the two words is less than 3. This finding is consistent with the results of Mann and Yarowsky (2001), although our experiments show more clear-cut differences. Using the same models, Mann and Yarowsky (2001) induced over 90% of the Spanish-Portuguese cognate vocabulary.
Improving Unsupervised Dependency Parsing with Richer Contexts and Smoothing Unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts. Traditionally, the unsupervised models have been kept simple due to tractability and data sparsity concerns. In this paper, we introduce basic valence frames and lexical information into an unsupervised dependency grammar inducer and show how this additional information can be leveraged via smoothing. Our model produces state-of-theart results on the task of unsupervised grammar induction, improving over the best previous work by almost 10 percentage points. Headden III et al (2009) showed that performance could be improved by including high frequency words as well as tags in their model. The final base distribution over CFG-DMV rules (Psh) is inspired by the skip-head smoothing model of Headden III et al (2009). On the |w| ≤ 10 test set all the TSG-DMVs are second only to the L-EVG model of Headden III et al. (2009).  This graph indicates that the improvements in the posterior probability of the model are correlated with the evaluation, though the correlation is not as high as we might require in order to use LLH as a model selection criteria similar to Headden III et al (2009). Headden III et al (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing. In DMV (Klein and Manning, 2004) and in the extended model EVG (Headden III et al., 2009), there is a STOP sign indicating that no more dependents in a given direction will be generated. We follow an approach similar to the widely-referenced DMV model (Klein and Manning, 2004), which forms the basis of the current state-of-the-art unsupervised grammar induction model (Headden III et al, 2009). An additional point of comparison is the lexicalized unsupervised parser of Headden III et al (2009), which yields the current state-of-the-art unsupervised accuracy on English at 68.8%.  As in the previous section, we find that the presence of linguistic rules greatly reduces this sensitivity: for HDP-DEP, the standard deviation over five randomly initialized runs with the English-specific rules is 1.5%, compared to 4.5% for the parser developed by Headden III et al. (2009) and 8.0% for DMV (Klein and Manning, 2004). In future work we intend to study ways to bridge this gap by 1) incorporating more sophisticated linguistically-driven grammar rule sets to guide induction, 2) lexicalizing the model, and 3) combining our constraint-based approach with richer unsupervised models (e.g., Headden III et al (2009)) to benefit from their complementary strengths.  In Headden III et al (2009), by using the lexical values with the frequency more than 100 and defining tied probabilistic context free grammar (PCFG) and Dirichlet priors, the accuracy is improved. For better comparison with previous work we implemented three model extensions, borrowed from Headden III et al (2009). We fix λ = 1/3, which is a crude approximation to the value learned by Headden III et al. (2009). Headden III et al (2009) also implement a sort of parameter tying for the E-DMV through a learning a back off distribution on child probabilities.  
An IR Approach for Translating New Words from Nonparallel Comparable Texts  Fung and Yee (1998) demonstrated that the associations between a word and its context seed words are preserved in comparable texts of different languages. The material for the present experiments consists of comparable medical corpora in French and English and a French-English medical lexicon (Fung and Yee (1998) call its words seed words?).  An additional difference with Fung and Yee (1998) is that they look for translational equivalents only among words that are unknown in both corpora. Comparable corpora have primarily been used to build bilingual lexical resources (Fung and Yee, 1998). One approach that can, in principle, better exploit both alignments from bitextsand make use of non-parallel corpora is the distributional collocational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). Beside simple co occurrence counts within sliding windows, other SoA measures include functions based on TF/IDF (Fung and Yee, 1998), mutual information (PMI) (Lin, 1998), conditional probabilities (Schuetzeand Pedersen, 1997), chi-square test, and the log likelihood ratio (Dunning, 1993). A variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (Fung and Yee, 1998) or by proposing other ways of measuring similarity beyond co-occurence within a context window.  Here, a standard technique of estimating bilingual term correspondences from comparable corpora (e.g., Fung and Yee (1998) and Rapp (1999)) is employed. For example, Rapp (1999) filtered out bilingual term pairs with low monolingual frequencies (those below 100 times), while Fung and Yee (1998) restricted candidate bilingual term pairs to be pairs of the most frequent 118 unknown words. Fung and Yee (1998) also use a vector space approach, but use TF/IDF values in the vector components and experiment with different vector similarity measures for ranking the translation candidates. Paststatistical methods for non-parallel corpora (Fung and Yee, 1998) are not valid for finding translations of words or expressions with low frequency. Following standard practice in bilingual lexicon extraction from comparable corpora, we rely on the approach proposed by Fung and Yee (1998).  Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999). Fung and Yee (1998), for example, proposed to represent the contexts of a word or phrase with a real-valued vector (e.g., a TF-IDF vector), in which one element corresponds to one word in the contexts. Fung and Yee (1998) point out that not only the number of common words in context gives some similarity clue to a word and its translation, but the actual ranking of the context word frequencies also provides important clue to the similarity between a bilingual word pair. This fact has motivated Fung and Yee (1998) to use tfidf weighting to compute the vectors.
Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems This paper describes Meteor 1.3, our submission to the 2011 EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks. New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words. We include Ranking and Adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced Tuning version shown to outperform BLEU in minimum error rate training for a phrase-based Urdu-English system. Table 2 lists the BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011) scores of both systems. To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonly used evaluation metrics: BLEU (Papineni et al, 2002), NIST (Doddington, 2002), TER (Snover et al, 2006), ROUGE-W (Lin, 2004), and three METEOR variants (Denkowski and Lavie, 2011): METEOR-ex (exact match), METEOR-st (+stemming) and METEOR-sy (+synonyms). For the current evaluation phase four automatic evaluation metrics have been employed, i.e. BLEU (Papineni et al, 2002), NIST (NIST 2002), Meteor (Denkowski and Lavie, 2011) and TER (Snover et al, 2006). Inverted Automatic Scores: For each Spanish system output sentence, we translate it to English and get its scores of BLEU and METEOR (Denkowski and Lavie, 2011). We report two translation measures: BLEU (Papineni et al 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). System performance is evaluated on newstest 2011 using BLEU (uncased and cased) (Papineni et al, 2002), Meteor (Denkowski and Lavie, 2011), and TER (Snover et al, 2006). The baseline results (non-factored model) under the standard evaluation metrics are shown in the first row of Table 3 in terms of BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011). The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009). The Meteor scoring tool (Denkowski and Lavie, 2011) for evaluating the output of statistical machine translation systems can be used to calculate the similarity of two sentences in the same language. We evaluate translation quality using BLEU score (Papineni et al, 2002), both on the word and character level (with n= 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level.  We used BLEU 4 (Papineni et al, 2002), METEOR (v.1.3) (Denkowski and Lavie, 2011) to evaluate the texts at document level.
Prepositional Phrase Attachment Through A Backed-Off Model Recent work has considered corpus-based or statistical approaches to the problem of prepositional attachment ambiguity. Typically, ambiguous verb phrases of the form v p np2 through a model which considers values of the four head words (v, nl, paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable. Results on Wall Street Journal data of 84.5% accuracy are obtained using this method. Brill and Resnik (1994) applied Error-Driven Transformation Based Learning, Ratnaparkhi, Reynar and Roukos (1994) applied a Maximum Entropy model, Franz (1996) used a Loglinear model, and Collins and Brooks (1995) obtained good results using a BackOff model. As we have argued in Zavrel and Daelemans (1997), this corresponds exactly to the behavior of the Back-Off algorithm of Collins and Brooks (1995), so that it comes as no surprise that the accuracy of both methods is the same. The results of Brill's method on the present benchmark were reconstructed by Collins and Brooks (1995). Collins and Brooks (1995) used a Back-Off model, which enables them to take low frequency effects into account on the Ratnaparkhi dataset (with good results). The perhaps underwhelming human performance is partially due to misclassifications by the Treebank assemblers who made these determinations by hand, and also unclear cases, which we discuss in the next section. Collins and Brooks (1995) introduced modifications to the Ratnaparkhi et al (1994) dataset meant to combat data sparsity and used the modified version to train their backed-off model. Abney, Schapire, and Singer (1999) used the dataset from Collins and Brooks (1995) with a boosting algorithm and achieved 85.4% accuracy. Their algorithm also was able to order the specific data points by how much weight they were assigned by the learning algorithm. Brill and Resnik (1994) trained a transformation-based learning algorithm on 12,766 quadruples from WSJ, with modifications similar to those by Collins and Brooks (1995). Stetina and Nagao (1997) trained on a version of the Ratnaparkhi et al (1994) dataset that contained modifications similar to those by Collins and Brooks (1995) and excluded forms not present in WordNet. Collins and Brooks (1995) used a supervised back-off model to achieve 84.5% precision on the Ratnaparkhi test set. However, the baseline is similarly high for the PP problem if the most likely attachment is chosen per preposition: 72.2% according to (Collins and Brooks, 1995). (Collins and Brooks, 1995) also present a model with multiple back offs. Later, Collins and Brooks (1995) achieved 84.5% accuracy by employing a backed-off model to smooth for unseen events. In our experiments, we only considered features that contained P since the preposition is the most important lexical item (Collins and Brooks, 1995). We describe the different classifiers below: cl base: the baseline described in Section 7.2clR1: uses a maximum entropy model (Ratnaparkhi et al, 1994) clBR5: uses transformation-based learning (Brill and Resnik, 1994) cl CB: uses a backed-off model (Collins and Brooks, 1995 )clSN: induces a decision tree with a sense-tagged corpus, using a semantic dictionary (Stetina and Nagao, 1997 )clHR6: uses lexical preference (Hindle and Rooth, 1993 )clR2: uses a heuristic extraction of unambiguous attachments (Ratnaparkhi, 1998) cl Pl: uses the algorithm described in this paper Our classifier outperforms all previous unsupervised techniques and approaches the performance of supervised algorithm. The accuracy is reported in (Collins and Brooks, 1995). p (Rjright; a; b) =# (R; right; a; b)# (right; a; b) (6) e.g. for the Verb-PP attachment relation pobj (following (Collins and Brooks, 1995) including the description noun 7) p (pobjjright; verb; prep ;desc: noun) =# (pobj; right; verb; prep ;desc: noun)# (right; verb; prep ;desc: noun) The distance (measured in chunks) between a head and a dependent is a limiting factor for the probability of a dependency between them. For example, the sentence Congress accused the president of peccadillos is classified according to the attachment site of the prepositional phrase: attachment toN: accused [the president of peccadillos] attachment to V: (4) accused [the president] [of peccadillos] The UPenn Treebank-II Parsed Wall Street Journal corpus includes PP-attachment information, and PP-attachment classifiers based on this data have been previously described in Ratnaparkhi, Reynar, Roukos (1994), Brill and Resnik (1994), and Collins and Brooks (1995). We used the same training and test data as Collins and Brooks (1995). Our approach can be seen as an extension of (Collins and Brooks, 1995) from PP-attachment to most dependency relations. Supervised methods are as varied as the Back off approach by Collins and Brooks (1995) and the Transformation-based approach by Brill and Resnik (1994).
Target-dependent Twitter Sentiment Classification Sentiment analysis on Twitter data has attracted much attention recently. In this paper, we focus on target-dependent Twitter sentiment classification; namely, given a query, we classify the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-ofthe-art approaches for solving this problem always adopt the target-independent strategy, which may assign irrelevant sentiments to the given target. Moreover, the state-of-the-art approaches only take the tweet to be classified into consideration when classifying the sentiment; they ignore its context (i.e., related tweets). However, because tweets are usually short and more ambiguous, sometimes it is not enough to consider only the current tweet for sentiment classification. In this paper, we propose to improve target-dependent Twitter sentiment classification by 1) incorporating target-dependent features; and 2) taking related tweets into consideration. According to the experimental results, our approach greatly improves the performance of target-dependent sentiment classification. The SA component is implemented according to Jiang et al (2011), which incorporates target-dependent features and considers related tweets by utilizing a graph-based optimization. For target-dependent sentiment classification, the manual evaluation of Jiang et al (2011). Jiang et al (2011) combine the target-independent features (content and lexicon) and target-dependent features (rules based on the dependency parsing results) together in subjectivity classification and polarity classification for tweets. These features are all target-independent. SVMdep: We re-implement the method proposed by Jiang et al (2011). This is caused by mismatch of the rules (Jiang et al,2011) used to extract the target-dependent features. However, labeled data is expensive to create, and examples of Twitter classifiers trained on hand-labeled data are few (Jiang et al 2011). Subjectivity classification of small units of text, such as individual micro blog posts (Jiang et al, 2011) and sentences (Riloff et al, 2003), has been shown to benefit from additional context.
Machine Transliteration It is challenging to translate names and technical terms across languages with different alphabets and sound inventories. These items are commonly transliterated, i.e., replaced with approximate phonetic equivalents. For example, &quot;computer&quot; in English comes out as &quot;konpyuutaa&quot; in Japanese. Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries. We describe and evaluate a method for performing backwards transliterations by machine. This method uses a generative model, incorporating several distinct stages in the transliteration process. Knight and Graehl (1998) proposed a Japanese-English transliteration method based on the mapping probability between English and Japanese katakana sounds. The first is machine transliteration (Knight and Graehl, 1998), in which names and technical terms are translated across languages with different sound systems. Knight and Graehl (1998) used a bilingual English-katakana dictionary, a katakana-to-English phoneme mapping, and the CMU Speech Pronunciation Dictionary to create a series of weighted finite-state transducers between English words and katakana that produce and rank transliteration candidates. Compared with the previous work on katakana to-English transliteration, these accuracies do not look particularly high: both Knight and Graehl (1998) and Bilac and Tanaka (2004) report accuracies above 60% for 1-best transliteration. We should emphasize that this is due to the difficulty of our test data, and that we have tested against a baseline that has been shown to outperform Knight and Graehl (1998). phoneme based method (Knight and Graehl, 1998) makes use of phonetic correspondence to generate the transliteration. Systematic relationships between pairs of strings are at the core of problems such as transliteration (Knight and Graehl, 1998), morphology (Dreyer and Eisner, 2011), cross-document coreference resolution (Bagga and Baldwin, 1998), canonicalization (Culotta et al2007), and paraphrasing (Barzilay and Lee, 2003). Our terminology is based on studies on machine transliteration (Knight and Graehl, 1998). Transliteration (Knight and Graehl, 1998) is a mapping from one system of writing into another, automation of which has been actively studied be tween English and other languages such as Arabic, Chinese, Korean, Thai, and Japanese. Knight and Graehl (1998) model the transliteration of Japanese syllabic katakana script into English with a sequence of finite-state transducers. Unlike the transducers proposed in (Stalls and Knight, 1998) and (Knight and Graehl, 1998) no attempt is made to model the pronunciation of words. The seen test is similar to tests run in (Knight and Graehl, 1998) and (Stalls and Knight, 1998) where the models could not produce any words not included in the language. Lee and Chang (2003) detect transliterations with a generative noisy channel transliteration model similar to the transducer presented in (Knight and Graehl, 1998). One area is the generative transliteration modeling (Knight and Graehl, 1998), which studies how to convert a word from one language to another using statistical models. For example Knight and Graehl (1998) use the lexicon frequency. If source and target languages have different phoneme sets, alignment between the different phonemes is also required (Knight and Graehl, 1998). Instead of using the lexicon frequency (Knight and Graehl, 1998) or Web statistics (Qu and Grefenstette, 2004), we propose validating transliteration pairs according to the alignment distance D between the aligned English graphemes and Chinese phonemes (see equations (2) and (5)). We use the problem formulation of Knight and Graehl (1998). phoneme based method (Knight and Graehl, 1998) makes use of phonetic correspondence to generate the transliteration. A straight forward supervised learning approach would require training data of name pairs between every pair of languages (Knight and Graehl, 1998).
Edit Detection And Parsing For Transcribed Speech We present a simple architecture for parsing transcribed speech in which an edited-word detector first removes such words from the sentence string, and then a standard statistical parser trained on transcribed speech parses the remaining words. The edit detector achieves a misclassification rate on edited words of 2.2%. which marks everything as not edited, has an error rate of 5.9%.) To evaluate our parsing results we introduce a new evaluation metric, the purpose of which is to make evaluation of a parse tree relatively indifferent the exact tree position of By this metric the parser achieves 85.3% precision and 86.5% recall. Work in statistically parsing conversational speech (Charniak and Johnson, 2001) has examined the performance of a parser that removes edit regions in an earlier step. A study by Charniak and Johnson (2001) shows that one can identify and remove edits from transcribed conversational speech with an F-score of about 78, with roughly 95 Precision and 67 recall. Readability studies have shown that disfluencies (fillers and speech repairs) may be deleted from transcripts without compromising meaning (Jones et al, 2003), and deleting repairs prior to parsing has been shown to improve its accuracy (Charniak and Johnson, 2001). Earlier work had already made this claim regarding speech repairs and argued that there was consequently little value in syntactically analyzing repairs or evaluating our ability to do so (Charniak and Johnson, 2001). Our division of the corpus follows that used in (Charniak and Johnson, 2001). These steps result in an improvement of 43.98% percent relative error reduction in F-score over an earlier best result in edited detection when punctuation is included in both training and testing data [Charniak and Johnson 2001].  These steps result in an improvement of 43.98% percent relative error reduction in F-score over an earlier best result in edited detection when punctuation is included in both training and testing data [Charniak and Johnson 2001]. We include the distributions with punctuation is to match with the baseline system reported in [Charniak and Johnson 2001], where punctuation is included to identify the edited regions. We take as our baseline system the work by [Charniak and Johnson 2001]. In the rest of the section, we first briefly introduce the boosting algorithm, then describe the method used in [Charniak and Johnson 2001], and finally we contrast our improvements with the baseline system. In [Charniak and Johnson 2001], identifying edited regions is considered as a classification problem, where each word is classified either as edited or normal. We relax the definition for rough copy, because more than 94% of all edits have both reparandum and repair, while the rough copy defined in [Charniak and Johnson 2001] only covers 77.66% of such instances. Descriptions of the 18 conditioning variables from [Charniak and Johnson 2001] 182 rough copy if their corresponding major categories match. Since the original code from [Charniak and Johnson 2001] is not available, we conducted our first experiment to replicate the result of their baseline system described in section 3. We used the exactly same training and testing data from the Switchboard corpus as in [Charniak and Johnson 2001]. Edit disfluency detection systems that rely exclusively on word-based information have been presented by Heeman et al (Heeman et al, 1996) and Charniak and Johnson (Charniak and Johnson, 2001). Based on the convention from Shriberg (1994) and Charniak and Johnson (2001), a disfluent spoken utterance is divided into three parts: the reparandum, the part that is repaired; the interregnum, which can be filler words or empty; and the repair/repeat, the part that replaces or repeats the reparandum. Charniak and Johnson (2001) and Kahn et al (2005) have shown that improved edit region identification leads to better parsing accuracy they observe a relative reduction in parsing f-score error of 14% (2% absolute) between automatic and oracle edit removal. The features used here are grouped according to variables, which define feature sub-spaces as in Charniak and Johnson (2001) and Zhang and Weng (2005).
Semantic Role Labeling Via Integer Linear Programming Inference We present a system for the semantic role la beling task. The system combines a machine learning technique with an inference procedurebased on integer linear programming that supports the incorporation of linguistic and struc tural constraints into the decision process. Thesystem is tested on the data provided in CoNLL 2004 shared task on semantic role labeling and achieves very competitive results. Finally an ILP (Integer Linear Programming) based method is adopted for post inference (Punyakanok et al, 2004). Punyakanok et al, (2004) further showed that constituent-by-constituent (C by-C) tagging is better than P-by-P. In this paper, we focus on phrase structure parsing with function labelling as a post-processing step. Integer linear programs have already been successfully used in related fields including semantic role labelling (Punyakanok et al, 2004), relation and entity classification (Roth and Yih, 2004), sentence compression (Clarke and Lapata, 2008) and dependency parsing (Martins et al, 2009). This basic architecture was introduced by Punyakanok et al (2004) for the task of semantic role labelling and since then has been applied to different NLP tasks without significant changes. We use the inference process introduced by (Punyakanok et al, 2004). Experiments performed combining the best and second output of the joint parser and enforcing domain constraints via ILP (Punyakanok et al, 2004) showed no significant improvements. Following (Punyakanok et al, 2004), we formulate SRL as a constituent-by-constituent (C-by-C) tagging problem. As we did in the last year's system (Cheetal., 2008), we use the ILP (Integer Linear Programming) (Punyakanok et al, 2004) to get the global optimization, which is satisfied with three constrains: C1: Each word should be labeled with one and only one label (including the virtual label? NULL?). However, Punyakanok et al (2004) showed that constituent-by-constituent (C-by-C) tagging is better than P-by-P. ILP models have been successfully applied in several natural language processing tasks, including relation extraction (Roth and Yih, 2004), semantic role labeling (Punyakanok et al, 2004) and the generation of route directions (Marciniak and Strube, 2005). Punyakanok et al (2004) formulated an Integer Linear Programming (ILP) model for SRL. ILP method was first applied to SRL in (Punyakanok et al, 2004). constraints (Punyakanok et al,2004)? without having to call out to ILP optimizers. ILP has been applied to various NLP problems including semantic role labeling (Punyakanok et al, 2004), which is similar to dependency labeling: both can benefit from verb specific information. Actually, (Punyakanok et al, 2004) take into account to some extent verb specific information. Some other work paid much attention to the robust SRL (Pradhan et al, 2005b) and post inference (Punyakanok et al, 2004). must have 3 arguments of a particular grammatical role. Among the approaches to overcome this restriction, i.e. that allow for global, theory based constraints, Integer Linear Programming (ILP) has been applied to NLP (Punyakanok et al, 2004). ILP has been applied to various NLP problems, including semantic role labeling (Punyakanok et al., 2004), extraction of predicates from parse trees (Klenner, 2005) and discourse ordering in generation (Althaus et al, 2004). Recently, the integer programming framework has been widely adopted by researchers to solve other NLP tasks besides POS tagging such as semantic role labeling (Punyakanok et al, 2004), sentence compression (Clarke and Lapata, 2008) ,decipherment (Ravi and Knight, 2008) and dependency parsing (Martins et al, 2009). In (Punyakanok et al, 2004), several more constraints are considered.
Improved Statistical Machine Translation Using Paraphrases Parallel corpora are crucial for training SMT systems. However, for many language pairs they are available only in very limited quantities. For these language pairs a huge portion of phrases encountered at run-time will be unknown. We show how techniques from paraphrasing can be used to deal with these otherwise unknown source language phrases. Our results show that augmenting a stateof-the-art SMT system with paraphrases leads to significantly improved coverage and translation quality. For a training corpus with 10,000 sentence pairs we increase the coverage of unique test set unigrams from 48% to 90%, with more than half of the newly covered items accurately translated, as opposed to none in current approaches. Although both methods have gained mainstream acceptance and have shown good correlations with human judgments, their deficiencies have become more evident and serious as research in MT and summarization progresses (Callison-Burch et al, 2006). Callison-Burch et al (2006) point out three prominent factors. This substitution technique has shown some improvement in translation quality (Callison-Burch et al, 2006). Typical examples are paraphrasing using bilingual (Callison-Burch et al, 2006) or monolingual (Quirket al, 2004) data. Other proposed methods include paraphrasing (Callison-Burch et al, 2006) and transliteration (Knight and Graehl, 1997) that uses the feature of phonetic similarity. Callison-Burch et al (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Although related to Callison-Burch et al (2006) our method is conceptually simpler and more general. We carried out experiments on small, medium and large scale English-Chinese translation tasks to compare against a baseline PBSMT system, the translation model augmentation of (Callison-Burch et al, 2006) method and the word-lattice-based method of (Du et al., 2010) to show the effectiveness of our novel approach. Compared with translation model augmentation with paraphrases (Callison-Burch et al, 2006), word-lattice-based paraphrasing for PBSMT is introduced in (Du et al, 2010). Callison-Burch et al (2006) used paraphrases of the trainig corpus for translating unseen phrases. Callison-Burch et al (2006) argue that limited amounts of parallel training data can lead to the problem of low coverage in that many phrases encountered at run-time are not observed in the training data and so their translations will not be learned. Callison-Burch et al (2006) proposed a novel method which substitutes a paraphrase for an unknown source word or phrase in the input sentence, and then proceeds to use the translation of that paraphrase in the production of the target-language result. to explain how difficult it is to translate the source-side sentence in three respects: The OOV rates of the source sentences in the test set (Callison-Burch et al, 2006). system performs slightly better (0.36 absolute BLEU points) than the baseline system on the 20K data set, but slightly worse (0.19 absolute BLEU points) than the baseline on the 200K data set, which indicates that the paraphrase substitution method used in (Callison-Burch et al, 2006) does not work on resource-sufficient data sets. Callison-Burch et al (2006) aim to improve MT quality by adding paraphrases in the translation table, while Madnani et al (2007) aim to improve the minimum error rate training by adding the automatically generated paraphrases into the English reference sets. Callison-Burch et al (2006) exploited the existence of multiple parallel corpora to learn paraphrases for Phrase-based MT.  For example, according to Callison-Burch et al (2006), a SMT system with a training corpus of 10,000 words learned only 10% of the vocabulary; the same system learned about 30% with a training corpus of 100,000 words; and even with a large training corpus of nearly 10,000,000 words it only reached about 90% coverage of the source vocabulary. The table augmentation idea is similar to Callison Burch et al (Callison-Burch et al, 2006), but our proposed paradigm does not require using a limited resource such as parallel texts in order to generate paraphrases. This work is most closely related to that of Callison-Burch et al (2006), who also translate source-side paraphrases of the OOV phrases.
Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately. MERT was done only for the baseline system; these same weights were used for all experiments to control for the effect of MERT instability. In the future, we plan to experiment with approach specific optimization and to use recent published suggestions on controlling for optimizer instability (Clark et al, 2011). Following Clark et al (2011), we report average scores over five random tuning replications to account for optimizer instability. We measure statistical significance using MultEval (Clark et al, 2011), which implements a stratified approximate randomization test to account for multiple tuning replications. Tuning of models used minimum error rate training (Och, 2003), repeated 3 times and averaged (Clark et al., 2011). In order to alleviate the impact of MERT (Och, 2003) instability, we followed the suggestion of Clark et al (2011) to run MERT three times and report average BLEU/NIST scores over the three runs for all our experiments. Following the recommendation of Clark et al (2011), we ran the optimization three times and repeated evaluation with each set of feature weights. Since both MERT and PRO tuning toolkits involve randomness in their implementations, all BLEU scores reported in the experiments are the average of five tuning runs, as suggested by Clark et al (2011) for fairer comparisons. Aware of the low stability of MERT (Clark et al, 2011), we run MERT three times and report the average BLEU score including the standard deviation. We tuned with minimum error rate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al, 2011). We account for optimizer instability by running 3 independent MERT runs per system, and performing significance testing with MultEval (Clark et al, 2011). The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al, 2011). In order to make the results more reliable, it is necessary to repeat the experiment several times (Clark et al, 2011). All boldfaced results were found to be significantly better than the baseline at the 95% confidence level using method described in (Clark et al, 2011) with 3 separate MERT tuning runs for each system. While is generally not useful to test experimental manipulations based on a single tuning run (Clark et al., 2011) and with different monolingual language modelling data, we note these figures simply to situate our results within the state of the art. We collected the BLEU, TER, and Meteor scores using MultEval (Clark et al, 2011), and the ROUGE-SU4 scores using the RELEASE-1.5.5.pl script.
A Machine Learning Approach To Pronoun Resolution In Spoken Dialogue We apply a decision tree based approach to pronoun resolution in spoken dialogue. Our system deals with pronouns with NPand non-NP-antecedents. We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features. We evaluate the system on twenty Switchboard dialogues and show that it compares well to Byron’s (2002) manually tuned system. Consider the following example (see Figure 1 for an illustration): In recent work dealing with pronoun resolution in spoken dialogue (Strube and Muller, 2003), different types of expressions (noun phrases, verb phrases, whole utterances and disfluencies) had to be annotated.   Xiaofeng et al. (2004) or Strube and Müller (2003) have shown the feasibility of decision trees for the domain of anaphora resolution; we have chosen this approach as it makes it possible to easily switch the information set for training and evaluation as opposed to e.g. rewriting rule sets.    Strube and Mu?ller (2003) propose a similar idea, but aim instead at finding a subset of the available features with which the resulting coreference classifier yields the best clustering-level accuracy on held-out data.      This suggests that a robust model of discourse structure could complement current robust interpretation systems, which tend to focus on only one aspect of the semantically ambiguous material, such as pronouns (e.g., Strube and Muller (2003)), definite descriptions (e.g., Vieira and Poesio (2000)), or temporal expressions (e.g., Wiebe et al (1998)).      
Semantic Roles for SMT: A Hybrid Two-Pass Model We present results on a novel hybrid semantic SMT model that incorporates the strengths of both semantic role labeling and phrase-based statistical machine translation. The approach avoids major complexity limitations via a two-pass architecture. The first pass is performed using a conventional phrase-based SMT model. The second pass is performed by a re-ordering strategy guided by shallow semantic parsers that produce both semantic frame and role labels. Evaluation on a Wall Street Journal newswire genre test set showed the hybrid model to yield an improvement of roughly half a point in BLEU score over a strong pure phrase-based SMT baseline – to our knowledge, the first successful application of semantic role labeling to SMT. However, little research has been done on how to effectively perform SRL on bi text, which has important applications including machine translation (Wu and Fung, 2009). Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. Wu and Fung (2009) demonstrated the promise of using features based on semantic predicate argument structure in machine translation, using these feature to re-rank machine translation output. Wu and Fung (2009) developed a framework to reorder the output using information from both the source and the target SRL labels. Wu and Fung (2009) used SRL labels for reordering the n-best output of phrase-based translation systems. Recently, Wu and Fung (2009a; 2009b) also show that semantic roles help in statistical machine translation , capitalising on a study of the correspondence between English and Chinese which indicates that 84% of roles transfer directly, for PropBank-style annotations. Recent years have witnessed increasing efforts towards integrating predicate-argument structures into statistical machine translation (SMT) (Wu and Fung, 2009b; Liu and Gildea, 2010). Unfortunately they are usually neither correctly translated nor translated at all in many SMT systems according to the error study by Wu and Fung (2009a). As PAS analysis widely employs global and sentence-wide features, it is computationally expensive to integrate target side predicate argument structures into the dynamic programming style of SMT decoding (Wu and Fung, 2009b). We also want to address another translation issue of arguments as shown in Table 7: arguments are wrongly translated into separate groups instead of a cohesive unit (Wuand Fung, 2009a). Wu and Fung (2009) present a two-pass model to incorporate semantic information to the phrase-based SMT pipeline. As the demand for semantically consistent machine translation rises (Wu and Fung, 2009a), the need for a comprehensive semantic mapping tool has become more apparent. With the current architecture of machine translation decoders, few ways of incorporating semantics in MT output include using word sense disambiguation to select the correct target translation (Carpuat and Wu, 2007) and reordering/reranking MT output based on semantic consistencies (Wu and Fung, 2009b) (Carpuat et al, 2010). Later, Wu and Fung (2009b) used parallel semantic roles to improve MT system outputs. Other statistical translation specific applications we would like to explore include extensions of MT output reordering (Wu and Fung, 2009b) and reranking using predicate-argument mapping, as well as predicate-argument projection onto the target language as an evaluation metric for MT output. (Wu and Fung, 2009) uses PropBank role labels (Palmer et al, 2005) as the basis of a second pass filter over an SMT system to improve the BLEU score from 42.99 to 43.51.  Our approach is inline with Wu and Fung (2009b) who demonstrated that on the one hand 84% of verb syntactic functions in a 50-sentence test corpus projected from Chinese to English, and that on the other hand about 15% of the subjects were not translated into subjects, but their semantic roles were preserved across language. One problem with using BLEU as an evaluation metric is that it is a precision-oriented metric and tends to reward fluency rather than adequacy (see (Wu and Fung, 2009a; Liu and Gildea, 2010)). Concerning the usage of SRL for SMT, Wu and Fung (2009) reported a first successful application of semantic role labels to improve translation quality.
Enhanced Sentiment Learning Using Twitter Hashtags and Smileys Automated identification of diverse sen timent types can be beneficial for manyNLP systems such as review summariza tion and public media analysis. In some ofthese systems there is an option of assign ing a sentiment value to a single sentence or a very short text. In this paper we propose a supervised sentiment classification framework whichis based on data from Twitter, a popu lar microblogging service. By utilizing50 Twitter tags and 15 smileys as sen timent labels, this framework avoids theneed for labor intensive manual annotation, allowing identification and classifi cation of diverse sentiment types of shorttexts. We evaluate the contribution of different feature types for sentiment classification and show that our framework successfully identifies sentiment types of untagged sentences. The quality of the senti ment identification was also confirmed byhuman judges. We also explore dependencies and overlap between different sen timent types represented by smileys and Twitter hashtags. (Davidov et al, 2010) used 50 hash tags and 15 emoticons as noisy labels to create a dataset for twitter sentiment classification.  Evaluation is performed on several datasets of tweets that have been annotated for polarity: the Stanford Twitter Sentiment set (Go et al, 2009), Davidov et al (2010) use 15 emoticons and 50 Twitter hash tags as proxies for sentiment in a similar manner, but their evaluation is indirect. Davidov et al (2010) propose utilizing twitter hash tag and smileys to learn enhanced sentiment types.
PCFG Models Of Linguistic Tree Representations The kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus. This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today. This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases. The particular independence assumptions implicit in a tree representation can be studied theoretically and investigated empirically by means of a tree transformation / detransformation process. Structural contexts like those captured by parent annotation (Johnson, 1998) are more subtle. Grandparent annotation was used previously by Charniak and Carroll (1994) and Johnson (1998). See Johnson (1998) for a presentation of the transform/de transform paradigm in parsing. First, it captures both P and its parent P, which predicts the distribution over child symbols far better than just P (Johnson, 1998).  These two requirements are related to grammar refinement by annotation (Johnson, 1998), where annotations must be bounded and monotonic. Johnson (1998) annotates each node by its parent category in a tree, and gets significant improvements compared with the original PCFGs on the Penn Treebank. It is well known that richer context representation gives rise to better parsing performance (Johnson, 1998). Johnson (1998) showed that refining tree bank categories with parent information leads to more accurate grammars.  Coming to this problem from the standpoint of tree transformation, we naturally view our work as a descendent of Johnson (1998).  The implementation we use was created by Mark Johnson and used for the research in (Johnson, 1998). In addition we also have some features that do not have a linguistic interpretation, but result in a good PCFG, such as a parent feature on some categories, following Johnson (1998). Parent Annotation Another common relabeling method in parsing is parent annotation (Johnson, 1998), in which a node is annotated with its parent's label. To deal with such problems ,weadopt a two-steps approach, the first being realized with Conditional Random Fields (CRF) (Lafferty et al 2001), the second with a Probabilistic Context-Free Grammar (PCFG) (Johnson, 1998). Once word shave been annotated with basic entity constituents, the tree structure of named entities is simple enough to be reconstructed with relatively simple model like PCFG (Johnson, 1998). A PCFG model (Johnson, 1998) is used to parse complete entity trees upon components. As discussed in (Johnson, 1998), an important point for a parsing algorithm is the representation of trees being parsed. Concerning the PCFG model, grammars, tree binarization and the different tree representations are created with our own scripts, while entity tree parsing is performed with the chart parsing algorithm described in (Johnson, 1998).
A Decision Tree Of Bigrams Is An Accurate Predictor Of Word Sense This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby. This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise. It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words. This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact, Pedersen (2001) found that bigrams alone can be effective features for word sense disambiguation. Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001). Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Pedersen, 2001). Such descriptions can be found in (Pedersen, 2001b) or (Pedersen, 2002). The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a). The supervised lexical sample system that participated in SENSEVAL-3 is the Duluth3 (English) or Duluth8 (Spanish) system as used in SENSEVAL 2 (Pedersen, 2001b). In (Pedersen, 2001a) we introduced the use of decision trees based strictly on bigram features. The former approach relies on previously acquired linguistic knowledge, and the latter uses techniques from statistics and machine learning to induce models of language usage from large samples of text (Pedersen, 2001). We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001). We also obtained salient bigrams in the context, with the methods and the software described in (Pedersen, 2001). Finally, Bag-of-words features are the lemmas of the content words in the whole context, plus the salient bigrams in the context (Pedersen, 2001). Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001). They are derived from the Duluth systems that participated in SENSEVAL-2, and which are more fully described in (Pedersen, 2001b). However, both (Pedersen, 2001a) and (Lee and Ng, 2002) show that different learning algorithms produce similar results and that the use of appropriate features may dramatically improve results. We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001). The recent work of Pedersen (2001a) and Zavrel et al (2000) evaluated a variety of learning algorithms on the SENSEVAL1 data set. In SENSEVAL-2, the various Duluth systems (Pedersen, 2001b) attempted to investigate whether features or learning algorithms are more important. It should be noted that the experiments for the SENSEVAL-2 and SENSEVAL-1 data using unigrams and bigrams are re-implementations of (Pedersen, 2001a), and that our results are comparable. (Pedersen, 2001b) compares decision trees, decision stumps and a Naive Bayesian classifier to show that bigrams are very useful in identifying the intended sense of a word. Finally, Bag-of-words features are the lemmas of the content words in the whole context, plus the salient bigrams in the context (Pedersen, 2001).
A Bayesian Hybrid Method For Context-Sensitive Spelling Correction Two classes of methods have been shown to be useful for resolving lexical ambiguity. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word. These methods have complementary coverage: the former captures the lexical &quot;atmosphere&quot; (discourse topic, tense, etc. ), while the latter captures local syntax. Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained taking into account not just the single strongest piece of evidence, but available evidence. A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated. For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995), Golding and Roth (1996) and Mangu and Brill (1997). The more recent set of techniques includes multiplicative weight update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation-based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al, 1993, Golding, 1995, Golding and Schabes, 1996). The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996). All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995). Table 6 shows 3An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al, 1993) for testing. A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) and performs similar to Golding and Schabes (1996). The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus. Golding (1995) builds a classifier based on a rich set of context features. Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from 140 common spelling confusions among sets such as there, their, and they &apos; re. We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).
Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation We present Minimum Bayes-Risk (MBR) decoding over translation lattices that compactly encode a huge number of translation hypotheses. We describe conditions on the loss function that will enable efficient implementation of MBR decoders on lattices. We introduce an approximation to the BLEU score (Papineni et al., 2001) that satisfies these conditions. The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata. Our experiments show that the Lattice MBR decoder yields moderate, consistent gains in translation performance over N-best MBR decoding on Arabicto-English, Chinese-to-English and Englishto-Chinese translation tasks. We conduct a range of experiments to understand why Lattice MBR improves upon N-best MBR and study the impact of various parameters on MBR performance. We also analytically show that interpolating these n-gram models for different n is similar to minimum risk decoding for BLEU (Tromble et al,2008). We geometrically interpolate the resulting approximations q with one another (and with the original distribution p), justifying this interpolation as similar to the minimum-risk decoding for BLEU proposed by Tromble et al (2008). We now observe that our variational decoding resembles the MBR decoding of Tromble et al (2008). Note that Tromble et al (2008) only consider MBR for a lattice without hidden structures, though their method can be in principle applied in a hyper graph with spurious ambiguity.  Pass 1: Lattice Pruning After generating phrase lattices using a phrase-based MT system, we prune lattice edges using forward-backward pruning (Sixtus and Ortmanns, 1999), which has also been used in previous work using phrase lattices (Tromble et al., 2008). Tromble et al (2008) proposed a linear approximation to BLEU score (log-BLEU) as a new loss function in MBR decoding and extended it from N-best lists to lattices, and Kumar et al (2009) presented more efficient algorithms for MBR decoding on both lattices and hyper graphs to alleviate the high computational cost problem in Tromble et al's work. For lattice MBR decoding, we optimized the lattice density and set the p and r parameters as per Tromble et al (2008). In the lattice MBR experiments of Tromble et al (2008), it is shown that this size of hypothesis set is sufficient. Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al (2008) instead. up to 1081 as per Tromble et al (2008). Also, since we maintain a probabilistic formulation across training and decoding, our approach does not require a grid-search for a scaling factor as in Tromble et al (2008). The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed lattice based MBR (Tromble et al, 2008). Other linear functions have been explored for MBR, including Taylor approximations to the logarithm of BLEU (Tromble et al, 2008) and counts of matching constituents (Zhang and Gildea, 2008), which are discussed further in Section 3.3. Tromble et al (2008) describe a similar approach using MBR with a linear similarity measure. Using G, Tromble et al (2008) extend MBR to word lattices, which improves performance over k-best list MBR. Our approach differs from Tromble et al (2008) primarily in that we propose decoding with an alternative to MBR using BLEU, while they propose decoding with MBR using a linear alternative to BLEU. The log-BLEU function must be modified slightly to yield a linear Taylor approximation: Tromble et al (2008) replace the clipped n-gram count with the product of an n gram count and an n-gram indicator function. Second, rather than use BLEU as a sentence level similarity measure directly, Tromble et al (2008) approximate corpus BLEU with G above. Tromble et al (2008) compute expected feature values by intersecting the translation lattice with a lattices for each n-gram t.
Robust Temporal Processing Of News We introduce an annotation scheme for temporal expressions, and describe a method for resolving temporal expressions in print and broadcast news. The system, which is based on both hand-crafted and machine-learnt rules, achieves an 83.2% accuracy (Fmeasure) against hand-annotated data. Some initial steps towards tagging event chronologies are also described. Related work by Mani and Wilson (2000) focuses only on the core temporal expressions neglecting the temporal information conveyed by prepositions (e.g. Friday vs. by Friday). The main part of the system is a temporal expression tagger that employs finite state transducers based on hand-written rules. A more complex set of temporal expressions as extracted by recent systems (e.g. (Mani and Wilson, 2000)) was tagged. The systems compared against are: GUTime (Mani and Wilson, 2000), a widely used, older rule-based system. Although Ahn et al (2007) compared their results with those presented by Mani and Wilson (2000), they went on to point out that, for a variety of reasons, the numbers they provided were not really comparable. Most closely relevant to the work described in the present paper are the approaches described in (Baldwin, 2002), (Jang et al, 2004) and (Mani and Wilson, 2000). In the system presented in (Mani and Wilson, 2000), weekday name interpretation is implemented as part of a sequence of interpretation rules for temporal expression interpretation more generally. GUTime (Mani and Wilson, 2000) presents an older but widely used baseline. As a result, many timex interpretation systems are a mixture of both rule-based and machine learning approaches (Mani and Wilson, 2000).  The GUTime tagger, developed at Georgetown University, extends the capabilities of the TempEx tagger (Mani and Wilson, 2000). The system of Mani and Wilson (2000) goes further in using separate sets of hand-crafted rules for recognition and normalization and in separating out several disambiguation tasks. Mani and Wilson (2000) and Ahn et al (2005b) also perform limited semantic class disambiguation. (Mani and Wilson, 2000) use a heuristic method for this task, while (Ahn et al, 2005b) use a machine learned classifier. The feature TEMPEX recorded the number of temporal expressions in each clause, as returned by a temporal expression tagger (Mani and Wilson, 2000). A natural way to go about update summarization would be extracting temporal tags (dates, elapsed times, temporal expressions...) (Mani and Wilson, 2000) or to automatically construct the timeline from documents (Swan and Allan, 2000). In terms of hand-coded approaches, (Mani and Wilson 2000) used a baseline method of blindly propagating TempEx time values to events based on proximity, obtaining 59.4% on a small sample of 8,505 words of text. The use of machine learning techniques — mainly statistical — for this task is a more recent development, either alongside the traditional hand-grammar approach to learn to distinguish specific difficult cases (Mani and Wilson, 2000), or on its own (Hacioglu et al., 2005). Mani and Wilson (2000) worked on news and introduced an annotation scheme for temporal expressions, and a method for using explicit temporal expressions to assign activity times to the entirety of an article. Mani and Wilson (2000) attribute over half the errors of their baseline method to propagation of an incorrect event time to neighboring events. It was cited in (Mani and Wilson 2000) as achieving a .83 F-measure against hand-annotated data.
Characterising Measures Of Lexical Distributional Similarity This work investigates the variation in a word?s dis tributionally nearest neighbours with respect to the similarity measure used. We identify one type ofvariation as being the relative frequency of the neighbour words with respect to the frequency of the target word. We then demonstrate a three-way connec tion between relative frequency of similar words, aconcept of distributional gnerality and the seman tic relation of hyponymy. Finally, we consider theimpact that this has on one application of distributional similarity methods (judging the composition ality of collocations). We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al, 2004). Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research. Abstracting from results for concrete test sets, Weeds et al (2004) try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends. Therefore to compensate this deficiency (i.e. to eliminate the bias discussed in (Weeds et al, 2004)) an edge length from a property to a ranked term e (pk ,vj) is weighted by the square root of its absolute frequency freq (vj). Approaches that rely on distributional data have two major drawbacks: they need a lot of data, generally syntactically parsed sentences, that is not always available for a given language (English is an exception), and they do not discriminate well among lexical relations (mainly hyponyms, antonyms, hypernyms) (Weeds et al, 2004).  Over recent years, many applications (Lin, 1998), (Lee, 1999), (Lee, 2001), (Weeds et al,2004), and (Weeds and Weir, 2006) have been investigating the distributional similarity of words. Our notion of entailment is 113 based on the concept of distributional generality (Weeds et al, 2004), a generalisation of the distributional hypothesis of Harris (1985), in which it is assumed that terms with a more general meaning will occur in a wider array of contexts, an idea later developed by Geffet and Dagan (2005). Weeds et al (2004) also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently. Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al, 2004). For details on DPs and distributional measures, see Weeds et al (2004) and Turney and Pantel (2010). The search of the corpus for paraphrase candidates is performed in the following manner:. Work on measuring distributional semantic distance: For one survey of this rich topic, see Weeds et al (2004) and Turney and Pantel (2010). As a result, researchers have proposed different approaches to produce transformed vectors using more sophisticated association statistics (see Dumais, 1991, Weeds et al, 2004, Turney and Pantel, 2010, inter alia). Using the framework of Weeds et al (2004), we found that the bias of lower frequency words for preferring high-frequency neighbours was higher for RFF (0.58 against 0.35 for Lin? s measure). of Weeds et al (2004), who analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures. Their analysis showed that there are three classes of measures ,i.e. those selecting distribution ally more general neighbours (e.g. cosine), those selecting distribution ally less general neighbours (e.g. AMCRM Precision (Weeds et al, 2004)) and those without abias towards the distributional generality of a neigh bour (e.g. Jaccard). Weeds et al (2004) attempted to refine the distributional similarity goal to predict whether one term is a generalization/specification of the other. Weeds et al (2004), Lenciand Benotto (2012) and Santus et al (2014) identified hypernyms in distributional spaces.
A Latent Variable Model for Geographic Lexical Variation The rapid growth of geotagged social media raises new computational possibilities for investigating geographic linguistic variation. In this paper, we present a multi-level generative model that reasons jointly about latent topics and geographical regions. High-level topics such as “sports” or “entertainment” are rendered differently in each geographic region, revealing topic-specific regional distinctions. Applied to a new dataset of geotagged microblogs, our model recovers coherent topics and their regional variants, while identifying geographic areas of linguistic consistency. The model also enables prediction of an author’s geographic location from raw text, outperforming both text regression and supervised topic models. Eisenstein et al (Eisenstein et al, 2010) show promising results in identifying an author's geographic location from micro-blogs, but the locations are coarse-grained and rely on a substantial message history per-source. For example, Eisenstein et al (2010) demonstrated a model that predicted where an author was located in order to analyze regional distinctions in communication. Our dataset is derived from prior work in which we gathered the text and geographical locations of 9,250 microbloggers on the website twitter.com (Eisenstein et al, 2010). Many of these terms are non-standard; while space does not permit a complete glossary, some are defined in Table 4 or in our earlier work (Eisenstein et al, 2010). Mangoville and M2 are clubs in New York; fasho and coo were previously found to be strongly associated with the West Coast (Eisenstein et al., 2010). Eisenstein et al (2010) collected about 380,000 tweets from Twitter's official API. While the above approaches discretize the continuous surface of the earth, Eisenstein et al (2010) predict locations based on Gaussian distributions over the earth's surface as part of a hierarchical Bayesian model. Following Eisenstein et al (2010), we consider all tweets of a user concatenated as a single document, and use the earliest collected GPS-assigned location as the gold location. Eisenstein et al (2010) use a latent variable model to predict geolocation information of Twitter users, and investigate geographic variations of language use. Eisenstein et al (2010) investigate questions of dialectal differences and variation in regional interests in Twitter users using a collection of geotagged tweets. Eisenstein et al (2010) evaluate their geographic topic model by geolocating USA-based Twitter users based on their tweet content. Performance is measured both on geo tagged Wikipedia articles (Overell, 2009) and tweets (Eisenstein et al, 2010). As a second evaluation corpus on a different domain, we use the corpus of geotagged tweets collected and used by Eisenstein et al (2010). For example, Eisenstein et al (2010) use Gaussian distributions to model the locations of Twitter users in the United States of America. For the Twitter dataset, an additional parameter is a threshold on the number of feeds each word occurs in: in the preprocessed splits of Eisenstein et al (2010), all vocabulary items that appear in fewer than 40 feeds are ignored. Eisenstein et al (2010) used a fixed Twitter threshold of 40. Geographical region knowledge has also been considered in topic models (Eisenstein et al, 2010).
Word Sense Disambiguation Improves Statistical Machine Translation Recent research presents conflicting evidence on whether word sense disambiguation (WSD) systems can help to improve the performance of statistical machine translation (MT) systems. In this paper, we successfully integrate a state-of-the-art WSD system into a state-of-the-art hierarchical phrase-based MT system, Hiero. We show for the first time that integrating a WSD system improves the performance of a state-ofthe-art statistical MT system on an actual translation task. Furthermore, the improvement is statistically significant.  A previous implementation of the IMS system, NUS-PT (Chan et al, 2007b), participated in SemEval-2007 English all-words tasks and ranked first and second in the coarse-grained and fine grained task, respectively. For example, (Chan et al, 2007) trained a discriminative model for WSD using local but also across-sentence unigram collocations of words in order to refine phrase pair selection dynamically by incorporating scores from the WSD classifier.  Chan et al (2007) use an SVM based classifier for disambiguating word senses which are directly incorporated in the decoder through additional features that are part of the log-linear combination of models.  A similar approach has been tried in the word-sense disambiguation (WSD) domain where local but also across-sentence unigram collocations of words are used to refine phrase pair selection dynamically by incorporating scores from the WSD classifier (Chan et al, 2007).  It has long been believed that being able to detect the correct sense of a word in a given context - performing word sense disambiguation (WSD) - will lead to improved performance of systems tackling high end applications such as machine translation (Chan et al, 2007) and summarization (Elhadad et al., 1997). To show the effect of our framework, we globally train millions of word level context features motivated by word sense disambiguation (Chan et al, 2007) together with the features used in traditional SMT system (Section 6). We use a set of word context features motivated by word sense disambiguation (Chan et al, 2007) to test scalability. Independent of these lexical substitution tasks, the connection between word senses and word translation has been explored in Chan et al (2007) and Carpuat and Wu (2007), who predict the probabilities of a target word being translated as an item in a sense inventory, where the sense inventory is a list of possible translations. Following this WSD reformulation for SMT, Chan et al (2007) integrate a state-of-the-artWSD system into a hierarchical phrase-based system (Chiang, 2005).  Chan et al (2007) incorporated a WSD system into the hierarchical SMT system, Hiero (Chiang, 2005), and reported statistically significant improvement.  This work extends several existing threads of research in statistical MT, including the use of context in example-based machine translation (Carl and Way, 2003) and the incorporation of word sense disambiguation into a translation model (Chan et al, 2007). See (Chan et al, 2007) for the relevance of word sense disambiguation and (Chiang et al, 2009) for the role of prepositions in MT.  We note that the best performing system (Chan et al, 2007b) of this task achieved a relatively high accuracy of 82.5%, highlighting the importance of having an appropriate level of sense granularity.
Incremental Parsing With The Perceptron Algorithm This paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm. A beam-search algorithm is used during both training and decoding phases of the method. The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank. We demonstrate that training a perceptron model to combine with the generative model during search provides a 2.1 percent F-measure improvement over the generative model alone, to 88.8 percent.   We used an early update version of averaged perceptron algorithm (Collins and Roark, 2004) for training of shift-reduce and top-down parsers. It is possible to prove that, provided the training set (xi ,zi) is separable with margin > 0, the algorithm is assured to converge after a finite number of iterations to a model with zero training errors (Collins and Roark, 2004). Although we have not discussed it to this point, (Collins and Roark, 2004) present a perceptron algorithm for use with the Roark architecture. Hence we use a beam-search decoder during training and testing; our idea is similar to that of Collins and Roark (2004) who used a beam-search decoder as part of a perceptron parsing model. Shen et al (2007) have further shown that better results (97.3% accuracy) can be obtained using guided learning, a framework for bidirectional sequence classification, which integrates token classification and inference order selection into a single learning task and uses a perceptron-like (Collins and Roark, 2004) passive-aggressive classifier to make the easiest decisions first. Early update was introduced by Collins and Roark (2004) for incremental parsing and adopted to forest re-ranking by Wang and Zong (2011). We also show, in Section 3.3, how perceptron training with early update (Collins and Roark, 2004) can be used in this setting. The normal-form model of Zhang and Clark (2011) uses an early update mechanism (Collins and Roark, 2004), where decoding is stopped to update model weights whenever the single gold action falls outside the beam.  Here, it might be useful to relax the strict linear control regime by exploring beam search strategies, e.g. along the lines of Collins and Roark (2004). We apply the early update strategy (Collins and Roark, 2004), stopping parsing for parameter updates when the gold standard state item falls off the agenda.  Strategy of Collins and Roark (2004) is used: when the correct state item falls out of the beam at any stage, parsing is stopped immediately, and the model is updated using the current best partial item. Collins and Roark (2004) proposed the early-update idea, and Huang et al (2012) later proved its convergence and formalized a general framework which includes it as a special case. This section gives a description of Collins and Roark's incremental parser (Collins and Roark, 2004) and discusses its problem.  The best results of Collins and Roark (2004) (LR=88.4%, LP=89.1% and F=88.8%) are achieved when the parser utilizes the information about the final punctuation and the look-ahead. The early-update strategy of Collins and Roark (2004) is used so as to improve accuracy and speed up the training.
CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes The CoNLL-2011 shared task involved predicting coreference using OntoNotes data. Resources in this field have tended to be limited to noun phrase coreference, often on a set of entities, such as entities. OntoNotes provides a large-scale corpus of general anaphoric coreference not restricted to noun phrases or to a specified set of entity types. OntoNotes also provides additional layers of integrated annotation, capturing additional shallow semantic structure. This paper briefly describes the OntoNotes annotation (coreference and other layers) and then describes the parameters of the shared task including the format, pre-processing information, and evaluation criteria, and presents and discusses the results achieved by the participating systems. Having a standard test set and evaluation parameters, all based on a new resource that provides multiple integrated annotation layers (parses, semantic roles, word senses, named entities and coreference) that could support joint models, should help to energize ongoing research in the task of entity and event coreference. This paper describes our entry to the 2011 CoNLL closed task (Pradhan et al, 2011) on modeling unrestricted coreference in OntoNotes. We have updated the publicly available CoNLL coreference scorer 1 with the proposed BLANC, and used it to compute the proposed BLANC scores for all the CoNLL 2011 (Pradhan et al, 2011) and 2012 (Pradhan et al, 2012) participant sin the official track, where participants had to automatically predict the mentions. The proposed BLANC is highly positively correlated with the 1http: //code.google.com/p/reference-coreference-scorers 2 The order is kept the same as in Pradhan et al (2011) and Pradhan et al (2012) for easy comparison. We follow the CoNLL2011 scheme to select TRAIN, DEV and TEST datasets (Pradhan et al,2011). (Pradhan et al, 2011) presents challenges that go beyond previous definitions of the task. An overview of all systems participating in the CONLL-2011 shared task and their results is provided by Pradhan et al (2011). In this paper we present SUCRE (Kobdani and Schutze, 2010) that is a modular coreference resolution system participating in theCoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNote (Pradhan et al., 2011). This paper describes our coreference resolution system participating in the close track of CoNLL 2011 shared task (Pradhan et al, 2011). The most frequent one is the constituent's head that solvers need then to extract using ad-hoc rules; see the CoNLL 2011 shared task (Pradhan et al, 2011), for instance. Our system builds on an earlier system that we evaluated in the CoNLL 2011 shared task (Pradhan et al, 2011), where we optimized significantly the solver code, most notably the mention detection step and the feature design. The CoNLL 2011 Shared Task (Pradhan et al,2011) is dedicated to modeling unrestricted coreference in OntoNotes. Both the CoNLL-2011 (Pradhan et al, 2011) and CoNLL 2012 (Pradhan et al, 2012) shared tasks focus on resolving coreference on the OntoNotes corpus. a) Non-anaphoric detection modules b) Pronominal resolution module The data used for training as well as testing was provided CoNLL-2001 shared task (Pradhan et al, 2011), (Pradhan et al, 2007) organizers. Plenty of machine learning algorithms such as Decision tree (Ng and Cardie,2002), maximum entropy model, logistic regression (Bjorkelund and Nugues, 2011), Support Vector Machines, have been used to solve this problem. Meanwhile, the CoNLL-2011 shared task on English language show that a well-designed rule-based approach can achieve a comparable performance as a statistical one (Pradhan et al, 2011). This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al, 2011). This was the official metric in the CoNLL-2011 shared task (Pradhan et al2011). We followed the CoNLL-2011 evaluation methodology, that is, we removed all singleton clusters, and apposition/copular relations before scoring. We evaluated the systems on three different settings: only on entity clusters, only on event clusters, and on the complete task, i.e., both entities and events. While models other than mention-pair have been proposed (Culotta et al, 2007), none performs clearly better as evidenced by recent shared evaluations such as SemEval 2010 (Recasens et al, 2010) and CoNLL 2011 (Pradhan et al, 2011). We applied this solver to the closed track of the CoNLL 2011 shared task (Pradhan et al,2011). This task (Pradhan et al, 2011) has set a harder challenge by only considering exact matches to be correct. In this paper, we present a learning approach to coreference resolution of named entities (NE), pronouns (PRP), noun phrases (NP) in unrestricted text according to the CoNLL-2011 shared task (Pradhan et al, 2011).
Learning to Translate with Source and Target Syntax Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a simple approach that uses both source and target syntax for significant improvements in translation accuracy. Chiang (2010) also obtained significant improvement over his hierarchical baseline by using syntactic parse trees on both source and target sides to induce fuzzy (not exact) tree-to-tree rules and by also allowing syntactically mismatched substitutions. Chiang (2010) also avoided hard constraints and took a soft alternative that directly models the cost of mismatched rule substitutions.  SAMT extension with source and target-side syntax described by Chiang (2010). In future work, the problem could be addressed by reconsidering our naming scheme for virtual nodes, by allowing fuzzy matching of labels at translation time (Chiang, 2010), or by other techniques aimed at reducing the size of the overall nonterminal set. Although MT systems that employ syntactic or hierarchical information have recently shown improvements over phrase-based approaches (Chiang, 2010), our initial investigation with syntactically driven approaches showed poorer performance on the text simplification task and were less robust to noise in the training data. The string-to-tree (Galleyet al 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as well as other formalisms such as Dependency Trees (Shen et al, 2008). The reported results show that while utilizing linguistic information helps, the coverage is more important (Chiang, 2010). Chiang (2010) extended SAMT-style labels to both source and target-side parses, also introducing a mechanism by which SCFG rules may apply at runtime even if their labels do not match. However, as discussed by Chiang (2010), while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained. Inspired by Chiang (2010), we adopt a fuzzy way to label every source string with the complex syntactic categories of SAMT (Zollmann and Venugopal, 2006). However, as noted by Lavie et al. (2008), Liu et al. (2009), and Chiang (2010), the integration of syntactic information on both sides tends to decrease translation quality because the systems be come too restrictive. Fuzzy constituency constraints can solve this problem with a combination of product categories and slash categories (Chiang, 2010). Using both source and target syntax, but relaxing on rule extraction and substitution enables HPBMT to produce more well-formed and syntactically richer derivations (Chiang, 2010). Chiang (2010) proposes a method for learning to translate with both source and target syntax in the framework of a hierarchical. Tellingly, in the entire proceedings of ACL 2010 (Hajic et al., 2010), only one paper describing a statistical MT system cited the use of MIRA for tuning (Chiang, 2010), while 15 used MERT.
Unsupervised Learning Of The Morphology Of A Natural Language This study reports the results of using minimum description length (MDL) analysis to model unsupervised learning of the morphological segmentation of European languages, using corpora ranging in size from 5,000 words to 500,000 words. We develop a set of heuristics that rapidly develop a probabilistic morphological grammar, and use MDL as our primary tool to determine whether the modifications proposed by the heuristics will be adopted or not. The resulting grammar matches well the analysis that would be developed by a human morphologist. In the final section, we discuss the relationship of this style of MDL grammatical analysis to the notion of evaluation metric in early generative grammar. Goldsmith (2001b; 2001a), inspired by de Marcken's (1995) thesis on minimum description length, attempts to provide both a list of morphemes and an analysis of each word in a corpus. (Goldsmith, 2001b), endeavor to learn to analyze the morphology of the language at hand in the manner of a linguist. Obviously the program needs to be systematically tested on multiple lexica from different languages, but these results strongly suggest that it is possible to model the acquisition of morphology as a component of learning to generate language directly, rather than to treat computational learning as the acquisition of linguistic theory as several current approaches do, e.g. (Goldsmith, 2001b). For example, the objective of the influential Linguistica program is 'to produce an output that matches as closely as possible the analysis that would be given by a human morphologist' (Goldsmith, 2001). We also experimented with Linguistica (Goldsmith, 2001), training on a large corpus, but results were worse than with Morfessor. Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwateret al, 2006), and as model of learning morphology in European languages (Goldsmith, 2001). In (Goldsmith, 2001) a recursive structure is proposed, such that stems can consist of a sub-stem and a suffix. Moreover, it performs better than a state-of-the-art morphology learning algorithm, Linguistica (Goldsmith, 2001), when evaluated on Finnish data. The new category-learning algorithm is compared to two other algorithms, namely the baseline segmentation algorithm presented in (Creutz, 2003), which was also utilized for initializing the segmentation in the category-learning algorithm, and the Linguistica algorithm (Goldsmith, 2001). Consider, for instance, the model of morphology described in Goldsmith (2001). Third, we use a morphological representation based on signatures, which are sets of affixes that represent a family of words sharing an inflectional or derivational morphology (Goldsmith, 2001). Then, for each stem, the set of all affixes with which it appears (its signature, (Goldsmith, 2001)) is collected. The dependence on frequency and length is motivated by the observation that less frequent and shorter affixes are more likely to be erroneous (see Goldsmith (2001)). The dependence on frequency and length is motivated by the observation that less frequent and shorter affixes are more likely to be erroneous (see Goldsmith (2001)). It is time consuming and, as Goldsmith (2001) explains, leaves difficult decisions of what constitutes a morpheme to on-the-fly subjective opinion. The evaluation should also be done with respect to well known systems like Linguistica (Goldsmith, 2001) or the morphological analyzer of Bernhard (2006). We compare our system with Linguistica (Goldsmith 2001), and discuss the advantages of the probabilistic paradigm over Linguistica's signature representation. These models include the signature of (Goldsmith 2001), the conflation set of (Schone and Jurafsky 2001), the paradigm of (Brent et. al. 2002), and the inflectional class of (Monson 2004). We compare the probabilistic paradigm to the signature model of (Goldsmith 2001). In this section, we compare our system with Linguistica (Goldsmith 2001), a freely available program for unsupervised discovery of morphological structure.
A Compar i son  of  A l ignment  Mode ls  for S ta t i s t i ca l  Mach ine Trans la t ion Franz Josef Och and Hermann Ney Lehrstuhl fiir Informatik VI, Comlmter Science Department RWTH Aachen - University of Technology D-52056 Aachen, Germany {och, ney}~inf ormat ik.  First, it has been shown that Model 4 produces a very good alignment quality in comparison to various other alignment models (Och and Ney, 2000b).  Alternative measures for the evaluation of one-to-one word links have been proposed in (Och and Ney, 2000a; Och and Ney, 2003). Furthermore, we do not split MWU links as proposed by (Och and Ney, 2000a). As in other uses of parallel corpora, good alignment is essential in order for the results to be meaningful (Och and Ney, 2000). Clearly we would benefit from better matching and alignment techniques, and we wonder if perhaps some of the alignment techniques used for parallel multi-lingual corpora (Och and Ney, 2000) could be adapted to help align our text-data corpora. We know that better alignment models have been proposed and extensively compared (Och and Ney, 2000). The data is from the Canadian Hansard, and reference alignments were originally produced by Franz Och and Hermann Ney (Och and Ney, 2000). Recent statistical machine translation (SMT) algorithms generate such a translation by incorporating an inventory of bilingual phrases (Och and Ney, 2000). Word alignment using GIZA++ toolkit (Och and Ney, 2000), the default configuration as available in training scripts for Moses. The classical approaches to word alignment are based on IBM models 1-5 (Brown et al, 1994) and the HMM based alignment model (Vogel et al, 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax based approaches (Zhang and Gildea, 2005) for word alignment are also studied. In this paper, we present improvements to the HMM based alignment model originally proposed by (Vogel et al., 1996, Och and Ney, 2000a). In order to improve transition models in the HMM based alignment, Och and Ney (2000a) extended the transition models to be word-class dependent. In the next section we briefly review modeling of transition probabilities in a conventional HMM alignment model (Vogel et al, 1996, Och and Ney, 2000a). We briefly review the HMM based word alignment models (Vogel, 1996, Och and Ney, 2000a). In (Och and Ney, 2000a), the word null is introduced to generate the French words that don &apos; t align to any English words. Tests sentence pairs were manually aligned and were marked with both sure and possible alignments (Och and Ney 2000a).  only presents AER results that are calculated after combination of word alignments of both E F and F E directions based on a set of heuristics proposed by Och and Ney (2000b). Alignments of both directions are generated and then are combined by heuristic rules described in (Och and Ney 2000b).
Classifying The Semantic Relations In Noun Compounds Via A Domain-Specific Lexical Hierarchy We are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems). In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds. We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves. In previous work (Rosario and Hearst, 2001), we demonstrated the utility of using a lexical hierarchy for assigning relations to two-word noun compounds. Finally, Rosario and Hearst (2001) make use of a domain-specific lexical resource to classify according to neural networks and decision trees. Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. This includes the seminal paper of (Gildea and Jurafsky, 2002), Senseval and Conll competitions on automatic labeling of semantic roles detection of noun compound semantics (Lapata, 2000), (Rosario and Hearst, 2001) and many others. Rosario and Hearst (2001) constructed feature vectors for each noun modifier pair using MeSH (Medical Subject Headings) and UMLS (Unified Medical Language System) as lexical resources. In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). This approach was used by Rosario and Hearst (2001) within a specific domain - medical texts.   (Rosario and Hearst 2001) focused on the medical domain making use of a lexical ontology and standard machine learning techniques.  Rosario and Hearst (2001) used a discriminative classifier to assign 18 relations for noun compounds from biomedical text, achieving 60% accuracy. For example, Rosario and Hearst (2001) propose 18 abstract relations for interpreting NCs in biomedical text, e.g., DEFECT, MATERIAL, PERSON AFFILIATED, ATTRIBUTE OF CLINICAL STUDY. While most of the noun compound research to date is not domain specific, Rosario and Hearst (2001) create and experiment with a taxonomy tailored to biomedical text. Rosario and Hearst (2001) utilize neural networks to classify compounds according to their domain-specific relation taxonomy. Rosario and Hearst (2001) used MeSH, a lexical hierarchy of medical terms. Domain noun compound semantics, including static relations, have been considered in studies by (Rosario and Hearst, 2001) and (Nakov et al, 2005), but in IE settings static relations tend to appear only implicitly, as in the RelEx causal RE system of (Fundel et al, 2007), or through the causal relations they imply: for example, in the AIMed corpus (Bunescu et al, 2005) statements such as NE1/NE2 complex are annotated as a binding relation between the two NEs, not Part Whole relations with the broader entity. Rosario and Hearst (2001) classify noun compounds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier.
Building A Discourse-Tagged Corpus In The Framework Of Rhetorical Structure Theory and Anne Anderson. 1997. The reliability of a dialogue structure coding Linguistics 13-32. Giacomo Ferrari. 1998. Preliminary steps toward the creation of a discourse and text In of the First International Conference on Language The additional two systems were: PD-EDU: Same as EDU except using the perfect discourse trees, available from the RST corpus (Carlson et al, 2001). We experimentally evaluated the test collection for single document summarization contained in the RST Discourse Treebank (RSTDTB) (Carlsonetal., 2001) distributed by the Linguistic Data Consortium (LDC). Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al, 2008a), which are both based on the Wall Street Journal (WSJ) corpus. Fortunately, RST Discourse Treebank (RSTDT) (Carlson et al, 2001) is an available resource to help with. In the Cause versus Contrast case, their reported performance exceeds ours significantly; however, in a subset of their experiments which test Cause versus Contrast on instances from the human annotated RSTBank corpus (Carlson et al., 2001) where no cue phrase is present, they report only 63% accuracy over a 56% baseline (the baseline is > 50% because the number of input examples is unbalanced).  For the first, the labelled/unlabelled relations f scores are 50.3% /73.0% and for the latter, they are 75.3% /84.0%: this is similar to the performance on other discourse annotation projects, e.g., Carlson et al (2001). The generator is informed by a corpus study of embedded discourse units on two discourse annotated corpora: the RST Discourse Treebank (Carlson et al., 2001) and the Penn Discourse Treebank. We evaluate DPLP on the RST Discourse Tree bank (Carlson et al, 2001), comparing against state-of-the-art results. To compare with previous works on RSTDT, we use the 18 coarse-grained relations defined in (Carlson et al, 2001). (Carlson et al 2001) reported relatively high levels of inter-annotator agreement, this was based on an annotation procedure where the annotators were allowed to iteratively revise the instructions based on joint discussion. To demonstrate the functionality of our system without relying on still imperfect discourse parsing, we use the RST parsed Wall Street Journal corpus as input (Carlson et al, 2001). In Discourse Tree Bank (Carlson et al, 2001) only 26% of Contrast relations were indicated by cue phrases while in NTC-7 about 70% of Contrast were indicated by cue phrases. They use the RST corpus (Carlson et al,2001), which contains 385 Wall Street Journal articles annotated following the Rhetorical Structure Theory (Mann and Thompson, 1988). (Soricut and Marcu, 2003) parsed the discourse structures of sentences on RST Bank data set (Carlson et al, 2001) which is annotated based on Rhetorical Structure Theory (Mann and Thompson, 1988). The RST Discourse Treebank (RST-DT) (Carlson et al, 2001), is a corpus annotated in the framework of RST. In the corpus of Rhetorical Structure trees built by Carlson et al (2001), for example, we have observed that only 61 of 238 CONTRAST relation sand 79 out of 307 EXPLANATION-EVIDENCE relations that hold between two adjacent clauses were marked by a cue phrase. However, empirical work of Marcu (2000) and Carlson et al (2001) suggests that the majority of occurrences of but, for example, do signal CONTRAST relations. To test this, we used the corpus of discourse trees built in the style of RST by Carlson et al (2001). If no cue phrases are used to signal the relation between two elementary discourse units, an automatic discourse labeler can at best guess that an ELABORATION relation holds between the units, because ELABORATION relations are the most frequently used relations (Carlson et al, 2001).
Regular Models Of Phonological Rule Systems This paper presents a set of mathematical and computational tools for manipulating and reasoning about regular languages and regular relations and argues that they provide a solid basis for computational phonology. It shows in detail how this framework applies to ordered sets of context-sensitive rewriting rules and also to grammars in Koskenniemi's two-level formalism. This analysis provides a common representation of phonological constraints that supports efficient generation and recognition by a single simple interpreter. see Kaplan and Kay (1994) for an exposition of the mathematical basis. We assume that the reader is familiar with the basic concepts of finite state transducers (FST hereafter), finite state devices that map between two regular languages U and L (Kaplan and Kay, 1994). Two-level formal lists based on that introduced by (Koskenniemi, 1983) (see also (Ritchie et al, 1992) and (Kaplan and Kay, 1994)) are widely used in practical NLP systems, and are deservedly regarded as something of a standard. Unlike arbitrary regularization, same-length regular n-relations are closed under intersection and complementation , because a theorem tells us that they correspond to regular languages over of symbols (Kaplan and Kay, 1994, p. 342). (Kaplan and Kay, 1994) express CR rules by the relation. This expression is an expansion of Restrict in (Kaplan and Kay, 1994, p. 371). The Xerox calculus includes the composition, ignore, and substitution operator discussed by Kaplan and Kay (1994) and the priority-union operator of Kaplan and Newman (1997). This can be seen as an application of the ignore operator of Kaplan and Kay (1994), where E* is being ignored. Rules are compiled into finite-state transducers and merged using transducer composition (Kaplan and Kay, 1994).  The context-dependent rewrite algorithm used is that of Mohri and Sproat (1996), and see also Kaplan and Kay (1994). In particular, constructing an OT grammar step-by-step as the composition of a set of transducers, akin to rewrite rule com position in (Kaplan and Kay, 1994), has offered the attractive possibility of simultaneously modeling OT parsing and generation as a natural consequence of the bi-directionality of finite-state transducers. An algorithm for compilation into transducers was provided by Kaplan and Kay (1994). Back referencing has been implicit in previous research, such as in the batch rules of Kaplan and Kay (1994). Previous algorithms for compiling rewrite rules into transducers have followed Kaplan and Kay (1994) by introducing special marker symbols (markers) into strings in order to mark off candidate regions for replacement. it will be helpful to have at our disposal a few general tools, most of which were described already in Kaplan and Kay (1994). For batch context-dependent rules, the context of the application for all rules is determined at once before their application (Kaplan and Kay, 1994). Phonological rewrite-rules (Kaplan and Kay, 1994), two-level rules (Koskenniemi 1983), syntactic disarnbiguation rules (Karlsson et al 1994, Koskenniemi, Tapanainen, and Voutilainen 1992), and part-of-speech assignment rules (Brill 1992, Roche and Schabes 1995) are examples of replacement in context of finite-state grammars. Kaplan and Kay (1994) describe a general method representing a replacement procedure as finite-state transduction. 
Lattice-based Minimum Error Rate Training for Statistical Machine Translation Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, in MERT are represented as lists which contain the probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm that allows for efficiently constructing and reprethe exact error surface of translations that are encoded in a phrase lattice. to MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experiments conducted on the NIST 2008 translation tasks show significant runtime improvements moderate BLEU score gains over MERT. As already observed in previous literature (Macherey et al, 2008), first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository. Recent efforts extended MERT to work on lattices (Macherey et al, 2008) and hypergraphs (Kumar et al, 2009). While this approach is similar in spirit to lattice-based MERT (Macherey et al, 2008), there is a crucial difference. A key property of the line optimisation is that it can consider a large set of hypotheses encoded as a weighted directed acyclic graph (Macherey et al, 2008), which is called a lattice. Note that the upper envelope is completely defined by hypotheses e4 ,e3, and e1, together with the intersection points ?1 and ?2 (after Macherey et al (2008), Fig. 1). Macherey et al (2008) use methods from computational geometry to compute the upper envelope. Macherey et al (2008) describe a procedure for conducting line optimisation directly over a word lattice encoding the hypotheses in Cs. The SweepLine algorithm (Bentley and Ottmann, 1979) is applied to the union to discard redundant linear functions and their associated hypotheses (Macherey et al, 2008). Macherey's theorem (Macherey et al, 2008) states that an upper bound for the number of linear functions in the upper envelope at the final state is equal to the number of edges in the lattice. We compare feature weight optimisation using k best MERT (Och, 2003), lattice MERT (Macherey et al, 2008), and tropical geometry MERT. Both TGMERT and LMERT converge to a small gain over MERT in fewer iterations, consistent with previous reports (Macherey et al, 2008). Weights on feature functions are found by lattice MERT (Macherey et al, 2008). In the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the lattice based MERT (Macherey et al, 2008). In all cases, the final log-linear models were optimized on the dev set using lattice-based Minimum Error Rate Training (Macherey et al, 2008). To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al, 2008). To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al, 2008). Recognizing this shortcoming, Macherey et al (2008) extended the MERT algorithm so as to use the whole set of candidate translations compactly represented in the search lattice produced by the decoder, instead of only a N-best list of candidates extracted from it. But the Down hill Simplex Algorithm loses its robustness as the dimension goes up by more than 10 (Machereyet al, 2008). Macherey et al (2008) propose a new variation of MERT where the algorithm is tuned to work on the whole phrase lattice instead of N-best list only. If we use either N-best lists or random samples to form the translation pool, and M is the size of the translation pool, then computing the envelope can be done in time O (M log M) using the SweepLine algorithm reproduced as Algorithm 1 in (Macherey et al, 2008).
Stochastic Lexicalized Tree-Adjoining Grammars Although stochastic techniques applied to syntax mod- eling have recently regained popularity, current lazl- guage models uffer from obvious inherent inadequacies. Early proposals uch as Markov Models, N-gram mod- els (Pratt, 1942; Shannon, 1948; Shannon, 1951) and tlidden Markov Models were very quickly shown to be linguistically not appropriate for natural language (e.g. Chomsky (1964, pages 13-18)) since they are unable to capture long distance dependencies or to describe hier- archically the syntax of natural anguages. Stochastic context-free granunar (Booth, 1969) is a hierarchical model more appropriate for natural languages, however none of such proposals (Lari and Young, 1990; Jelinek, Lafferty, and Mercer, 1990) perform as well as the sim- pler Markov Models because of the difficulty of captur- ing lexical information. The parameters of a stochas- tic context-free grammar do not correspond irectly to a distribution over words since distributional phenom- ena over words that are embodied by the application of *This work was partially supported by DARPA Grant N0014- 90-31863, ARO Grant DAAL03-89-C-0031 and NSF Grant 1RI90- 16592. We thank Aravind Joshi for suggesting the use of TAGs for statistical nalysis during a private discussion that followed a presentation bybS'ed Jdinek during the June 1990 meeting of the DARPA Speech and Natural Language Workshop. We are also grateful to Peter Braun, FYed Jelinek, Mark Liberman, Mitch Marcus, Robert Mercer, Fernando Pereira said Stuart Shieber for providing vMu~ble comments. more than one context-free rule cannot be captured un- der the context-freeness a sumption. This leads to the difficulty of maintaining a standard hierarchical model while capturing lexieal dependencies. This fact prompted researchers in natural language processing to give up hierarchical language models in the favor of non-hierarchical statistical models over words (such as word N-grams models). Probably for lack of a better language model, it has also been ar- gued that the phenomena that such devices cannot cap- ture occur relatively infrequently. Such argumentation is linguistically not sound. Lexicalized tree-adjoining grammars (LTAG) t com- bine hierarchical structures while being hxieany sensi- tive and are therefore more appropriate for statistical analysis of language. In fact, LTAGs are the simplest hierarchical formalism which can serve as the basis for lexicalizing context-free grammar (Schabes, 1990; Joshi and Sehabes, 1991). LTAG is a tree-rewriting system that combines trees of large domain with adjoining and substitution. The trees found in a TAG take advantage of the available x- tended domain of locality by localizing syntactic depen- dencies (such as finer-gap, subject-verb, verb-objeet) and most semantic dependencies ( uch as predicate- argument relationship). For example, the following trees can be found in a LTAG lexicon: S /k NP,L VIP VP A V NPI NP NP VP* ADV L I I I uts J~n p~nutJ hungrily Since the elementary trees of a LTAG are minimal syntactic and semantic units, distributional analysis of the combination of these elementary trees based on a training corpus will inform us about relevant statistical aspects of the language such as the classes of words appearing as arguments of a predicative lement, the distribution of the adverbs licensed by a specific verb, or the adjectives licensed by a specific noun. This kind of statistical analysis as independently sug- gested in (Resnik, 1991) can be made with LTAGs be- cause of their extended omain of locality but also be- cause of their lexiealized property. lWe attallnle familiarity throughout the paper with TAGs and its lexicallzed variant, See, for instance, (Joehl, 1987), (Schabes, Abeill~, and Joehi, 1988), (Schabes, 1990) or (Joslfi and Schabes, 1~1). ACTES DE COLING-92. NANTES, 23-28 AOUT 1992 4 2 6 PROC. OF COLING-92, NANTES, AUG. 23-28, 1992 In this paper, this intuition is made formally precise by defining the notion of a stochastic lexicalized tree- adjoining rammar (SLTAG). We present an algorithm for computing the probability of a sentence generated by a SLTAG, and finally we introduce an iterative algo- r ithm for estimathlg the parameters of a SLTAG given a training corpus of text. This algorithm can either be used for refining the parameters of a SLTAG or for inferring a tree-adjoining grammar frmn a training cor- pus. We also report preliminary experiments with this algorithm. Due to the lack of space, in this paper tim algorithms are described succinctly without proofs of correctness and more attention is given to tile concepts and tech- niques used for SLTAG. hfformally speaking, SLTAGs are defined by assigning a probability to tile event that an elementary tree is combined (by adjunction or substitution) on a specific node of another elementary tree. These events of com- bination are the stochastic processes considered. Since SLTAG are defined on the basis of the deriva- tion and since TAG allows for a notion of derivation independent from the trees that are derived, a precise mathematical definition of the SLTAG derivation must be given. For this purpose, we use stochastic linear in- dexed grammars (SLIG) to formally express SLTAGs derivations. Linear Indexed grammar (LIG) (Alto, 1968; Gazdar, 1985) is a rewriting system in which the non-terminal symbols are augmented with a stack, in addition to rewriting non-terminals, the rules of the grammar can have the effect of pushing or popping symbols on top of tile stacks that are associated with each non-terminal symbol. A specific rule is triggered by the non-termlnal on the left hand side of the rule and the top element of its associated stack. The productions of a LIG are restricted to copy the stack corresponding to tile non-terminal being rewrit- ten to at most one stack associated with a non-terminal symbol on tile right hand side of the production? In tile following, \[..p\] refers to a possibly unbounded stack whose top element is p and whose remaining part is schematically written as '..'. \[$\] represents a stack whose only element is the bottom of the stack. While it is possible to define SLIGs in general, we define them for the particular case where the rules are binary branching and where tile left hand sides are always incomparable. A stochastic linear indexed grammar, G, is denoted by (VN, VT, VI, S, Prod), where VN is a finite set of non- terminal symbols; VT is a finite set of terminal symbols; VI is a finite set of stack symbols; S E VN is the start symbol; Prod is a finite set of productions of the form: Xo\[$po\] --* a Xo\[..po\] --. x~\[..m\] x~\[$p~\] x0\[..po\] -~ Xl\[$pd x~\[-.p~\] Xo\[$Po\] --~ Xl\[$pl\] X2\[$p2\] where Xk E Vjv, a E VT and po ~. VI, Pl,P2 E V\[; P, a probability distribution which assigns a probability, 0 < P(X\[..z\] ~ A) < 1, to a rule, X\[..x\] -* A ~. Prodsuch 2LIGs have been shown to be weakly eqtfivalent to "Ibee- Adjoining Graramars (V~jay-Shanker, 1987). that tbe sum of the probabilities of all the rules that can be applied to any non-terminal nnotated with a stack is equal to one. More precisely if, VX E VN,Vp E VI: ~ p(xt..pl -~ A) = 1 A P(X \[..p\] --* A) should be interpreted as the probability that X\[..p\] is rewritten as A. A derivation starts from S associated with the empty stack (S\[$\]) and each level of the derivation must be validated by a production rule. The language of a SLIG is defined as follows: L = {w E VT~ \[ S\[$\]~w}. The probability of a derivation is defined as the prod- uct of tile probabilities of all individual rules involved (counting repetition) in the derivation, the derivation being validated by a correct configuration of the stack at each level. The probability of a sentence is then com- puted as the sum of the probabilities of all derivations of tile sentence. Following tile construction described in (Vijay- Shanker and Weir, 1991), given a LTAG, Glaa, we con- struct an equivalent LIG, G,ua. Tile constructed LIG generates tile same language as Gtag and each deriva- tion of Gtaa corresponds to a unique LIG derivation corresponds to a unique derivation in G,ua (and con- versely). In addition, a probability is assigned to each production of the LIG. For simplicity of explanation and without loss of generality we assume that each node in an elementary tree in Gt,9 is either a leaf node (i.e. either a foot node or a non-empty terminal node) or binary branching, a The construction of the equivalent SLIG follows. The non-terminal symbols of Gstia are the two sym- bols 'top' (t) and 'bottom' (b), tile set of terminal sym- bols is the same as the one of Gta9, the set of stack symbols is the set of nodes (not node labels) found in the elementary trees of Gla~ augmented with the bot- tom of tile stack ($), and tile start symbol is ' top' (t). For "all root nodes ~10 of an initial tree whose root is labeled by S, the following starting rules are added: t\[$\] ~ t\[$,t0\] (1) These rules state that a derivation must start from the top of the root node of some initial tree. P is the prob- ability that a derivation starts from the initial tree as- sociated with a lexical item and rooted by %. Then, for all node '/ in an elementary tree, the fol- lowing rules are generated. If rhT/2 are ttle 2 children of a node r/sucb that r/2 is on the spine (i.e. subsumes tile foot node), include: b\[..~l ~&' tI$n, lt\[-.,~l (2) Since (2) encodes an immediate domination link de- fined by the tree-adjoining rammar, its associated probability is one. Similarly, if thT/~ are the 2 children of a node r/such that r h is on the spine (i.e. subsumes the foot node), include: b\[..rt\] P=-*~ t\["rl~\]t\[$~\] (3) Since (3) encodes a~t immediate domination link de- fined by the tree-adjoining rammar, its associated probability is one. aThe algorlthnm explained ill this paper cart be generalized to lexicadized tree-adjoining granunars that need not be in Chottmky Normal Form using techniqu?~ similar the one found in (Schabet, 1991). ACIES DE COLING-92, NANTES, 23-28 AO~rf 1992 4 2 7 P~oc. OF COLING-92, NANTES, AUG. 23-28, 1992 * If ~/tT/2 are the 2 children of a node q such that none of them is on the spine, include: b\[$~\] p~l \]~\[$I~1\]t\[$i~2 \] (4) Since (4) also encodes an immediate domination link defined by the tree-adjoining grammar, its associated probability is one. If 7? is a node labeled by a non-terminal symbol and if it does not have an obligatory adjoining constraint, then we need to consider the case that adjunetion might not take place. In this ease, include: t\[..~\] L b\[..~\] (5) The probabil ity of rule (5) corresponds to the proba- bility that no adjunetion takes place at node q. o If t/ is an node on which the auxiliary tree fl can be adjoined, the adjunetiou of fl can be predicted, therefore (assuming that ~tr is the root node of fl) include: t\["0\] L t\[..rl,,\] (6) The probability of rule (6) corresponds to the proba- bility of adjoining the auxiliary tree whose root node is ~/~, say/3, on the node 0 belonging to some elemen- tary tree, say a.4 ? If r)! is tim foot node of an auxiliary tree fl that has been adjoined, then the derivation of the node below q\] must resume. In this case, include: b\["0l\] ,~1 b\[..\] (7) The above stochastic production is included with probabil ity one since the decision of adjunction has already been made in rules of the form (6). Finally, if r h is the root node of an initial tree that can be substituted on a node marked for substitution r), include: t\[$~\] L t\[S~t\] (g) Here, p is the probability that the initial tree rooted by ~/~ is substituted at node q. It corresponds to the probability of substituting the lexicalized initial tree whose root node is 71, say 6, at the node q of a lexicalized elementary tree, say a. 5 The SLIG constructed as above is well defined if the following equalities hold for all nodes ~l: P(t\[..~/\] ---* b\[..~/\]) + E P(t\[..~/\] --* t\[..q0~\] ) = 1 (9) P(t\[$~/\] ---* t\[$Ol\]) ---- 1 (10) E P(t\[$\] -~ t\[$O0\]) = 1 (11) 4Since the granmmr is lexicalized, both trees a and /3 are a~ sociated with lexical iter~s, mad the site node for adjtmction ~ correuponds to some syntactic modification. Such llde encapsu- lates S modifiers (e.g. s~tential adverbs as in "apparently John left"), VP modifiers (e.g. verb phr~e adverbs as in "John left abruptly}", NP modifiers (e.g. relative clauses as in "The man who left was happy"), N modifiers (e.g. adtieetive~ asin "prelty woman"), or even sententiM complements (e.g. John think8 that Harry is sick). s Among other cases, the probability of thi~ rule corr~ponds to the probability of filling some argument p(~ition by a lexiealized tree. It will encapsulate he distribution for Belectional restriction since the position of substitution is taken into account. A gramular satisfying (12) is called consistent. 6 E P ( t \ [$ \ ]~w)= 1 (12) wEZ* Beside the distributional phenomena that we mentioned earlier, SLTAG also captures the effect of adjoining con- straints (selective, obligatory or null adjoining) which are required for tree-adjoining rammar . 7 Probab i l i ty of a Sentence We now define an bottom-up algorithm for SLTAG which computes the probability of an input string. The algorithm is an extension of the CKY-type parser for tree-adjoining grammar (Vijay-Shanker, 1987). The ex- tended algorithm parses all spans of the input string and also computes tbelr probability in a bottom-up fashion. Since the string on the frontier of an auxiliary is bro- ken up into two substrings by the foot node, for the purpose of computing the probability of the sentence, we will consider the probability that a node derives two substrings of the input string. This entity will be called the inside probability. Its exact definition is given be- low. We will refer to the subsequenee of the input string w = ax "" aN from position i to j , w{'. It is defined as follows: w~/'~f { a i+t" .uj , i f i>_ j ' i f /< j Given a string w = at . . . a N and a SLTAG rewritten as in (1-8) the inside probability, F (pos , 71, i, j , k,l), is defined for all nodes 7/ contained in an elementary tree and for pos E {t,b}, and for all indices 0 < i < j < k < I < N as follows: (i) If the node 7/does not subsume the foot node of (~ (if there is one), then j and k are un- bound and: l~ (pos, ~, i , - , - , I) d~=l P(pos\[$@~ w~) (it) If the node y/subsumes the foot node 7/! of e, then: l~ (pos, rL i, j, k, l) a~l P ( pos \ [$@~ w{ b\[$o l lw~ ) In (ii), only the top element of the stack matters ince as a consequence of the eonstrnction of the SLIG, we have that if pos\[$tl\]~ w~b\[$rll\]w ~ then for all string 7 e V/~ we also have pos\[$Tr/\]~ w~b\[$7~l\]w~.S Initially, all inside probabilities are set to zero. Then, the computat ion goes bottom-up start ing from the pro- ductions introducing lexieal items: if r/ is a node such that b\[$7/\] --~ a, then: 1 i f l= i+ lAa=w~ +t (1~ IW(b 'T l ' i ' - ' - ' l ) = 0 otherwise. Then, the inside probabilities of larger substrings are computed bottom-up relying on the recurrence qua- ~We will not investigate im conditions under which (12) holds. We conjecture that the techniques used for dmcking the eolmis- tency of stochastic context-free grammars (Booth and Thomp6on, 1973) can be adapted to SLTAG. r For example, for a given node 0 setting to zero the probability o\[ all rules of the forts (6) ht~ the effect of blocking adjunction. 8Thls can be seen by obae~.ing that for any node on the path from the root node to the foot node of an auxiliary tree, the stack remains unchanged. ACRES DE COLING-92, NANTES. 23-28 AOt~T 1992 4 2 8 PROC. OF COLING-92, NANTES. AUG. 23-28, 1992 lions stated in Appendix A. This computation takes in the worst case O(IGl~N6)-time and O(IGINa)-space for a sentence of lengtb N. Once the inside probabilities cmnputed, we obtain the probability of the sentence flu follows: P(w)aJP(t\[$\]~,~) = Z~(t, $, 0 , - , - , Iwl) (14) Wc now consider the problem of re-estimating a SI,TAG. 4 Ins ide -Ous ide A lgor i thm for. 1%eest imat ing a SLTAG Given a set of positive example sentences, W = {wt ' "wK}, we would like to compute the probabil- ity of each rule of a given SLTAG in order to maximize thc probability that the corpus were generated by this SLTAG. An algorithm solving this problem can be used in two different ways. The first use is as a reestimation algorithm. In ttfis approach, the input SI,'1'A(~ derives structures that arc reasonable according to some criteria (such as a linguis- tic theory and some a priori kuowledge of the corpus) and the intended use of the algorithm is to refine the probability of each rule. The second use is as a learning algorithm. At the first iteration, a SLTAG which generates all possible struc- tures over a given set of nodes and terminal symbols is used. Initially the probability of each rule is randomly assigned and then tile algorithm will re-estimate tbese probabilities. Informally speaking, given a first estimate of the pa- rameters of a SLTAG, the algorithm re-estimates these parameters on the basis of the parses of each sentence in a training corpus obtained by a CKY-tyt)e parser. The algorithm is designed to derive a new estimate after each iteration such that the probability of the corpus is increased or equivalently such that tile cross entropy estimate (negative log probability) is decreased: log~(e(r0)) l t (W,G) - weW (15) wEW In order to derive a new estimate, the algorithm needs to compute for all seutences in W the in- side probabilities and the outside probabilities. Given a string w = al . . .aN, tbe outside probability, 0 ~ (pos, ~, i, j, k, It, is defined for all nodes r I contained in an elementary tree a and for pos E {t,b}, and for all indices 0 < i < j < k < l < N as follows: (it If the node r/does not subsume the foot node of a (if there is one), then j and k axe un- bound asld: ..de\] O'? (P os, O, i, - , - , t) - P(B"/ C V~ s.t. t\[$\]=~ Wio pos\[$Ttl\] w~) (ii) If the node ~/does ubsume the foot node ~/! of a then: 0 '~ (pos, O, i, j, k, l) aeJ- /'(37 ~ V~* s.t. t \ [$\]~ Wlo pos\[$Trl\] w~ and b\[$7~ll\]~w\]) Once the inside probabilities computed, the outside probabilities can be computed top-down by consider- ing smaller spans of the input string starting with O"( t ,$ ,O , - , - ,N ) = 1 (by definition). This is done by computing the recurrence quations tated in Ap- pendix B. In the following, we assume that r I subsumes the foot node r/l within a same elementary tree, and also that tll subsumes the foot node ~111 (within a same elementary tree). The other cases are handled similarly. Table 1 shows the reestimation formulae for the adjoining rules (16) and the null adjoining rules (17). (16) corresponds to the average number of time that tl . . . le L\[..T1\] .-* t\[..yqv\] is used, and (17) to th . . . age number of times no adjunction occnrred on T/. The denominators of (16) and of (17) estimate the average number of times that a derivation involves tlLe expan- sion oft\[-.~/\]. The numerator of(16) estimates the aver- age number of times that a derivation involves the rule t\[.-7/\] -~ t\[..Tirfl\]. Therefore, for example, (16) estimates the probability of using the rule/\['-~7\] ~ l\["rplt\]. The algorittun reiterates until H(W, G) is unchanged (within some epsilon) between two iterations. Each it- eration of the algoritbm requires at most O(IGIN e) time for each sentence of length N. 5 Grammar In ference w i th. SLTAG The reestimation algorithm explained in Section 4 can be used botll to reestimate the paramcters for a SI,TAG derived by some other mean or to infer a grammar from scratch. Ill the following, we investigate grammar In- ference from scratch. The initial grammar for the reestimation algoritiim consists of all SLIG rules for the tress ill Lexical- ized Normal I~brm (ill short LNF) over a given set = {aill .< i _< T} of terminal symbols, with suit- ably assigned non zero probability: 9 S 0 $4 s h t~ a i The above normal form is capable not only to de- rive any lexicalized tree-adjoining language, but also to impose ally binary bracketing over the strings of the language. The latter property is important as we would like to be able to use bracketing information in the ilL- put corpus as in (Pereira and Schabes, 1992). The worst case complexity of tim reestimation algo- r ithm given iu Section 4 with respect o the length of the input string (O(NS)) makes this approach in gen- eral impractical for LNF grammars. However, if only trees of the form fit a' and a~" (or only of tile form /~' and a~) , the language generated is a context-free language and can be handled more efficiently by the reestimation algorithnL 9Adjoining constraints can be u~d in tiffs normal form, They will be reflected in the SLIG eq~vaient grammar. Indices have been added on S nodes in order to be able to uniquely refer to each node in the granunar. AcrEs OE COLING-92, NANTES. 23-28 AOOT 1992 4 2 9 DROC. OF COLING-92, NANTES, AUG. 23-28, 1992 wwPW ) x QW(t\[..~/\] ~ t\[.-r/rp\]) P(t\[-.t/\] ---, t\[..~Tt/t\]) = 1 (16) ~wp--- ~ x \[R~0/) + ~_~O'~(t\[..O\] --, t\[..~/r/,\])\] 1 to~w /3(t\[..r/\] ---+ b\[..~/\]) = 1 (17) Ot?(t\["r/\] ~ t\["r/rY\]) = Z P(t\["O\]--*t\["O~Y\])?Iw(t'o/ ' i ' r 's ' l )xlW(b'o'r ' j 'k 's)xOW(t'~l' i ' j 'k ' l ) (18) i)r,j~k,t)l /~w(r/) = ~ P(t\[..r/\] ~ b\[..r/\]) x l~(t ,o, i , j ,k , l ) x O~?(b,)l,i,j,k,l) (19) i,j,k,I Table 1: Keestimation of adjoining rules (16) and null adjoining rules (17) It can be shown that if, only trees of the form ~a~ and ~a~ are considered, the reestimation algorithm requires in the worst case O(Na)-t ime) ? The system consisting of trees of the form ~' and c~ can be seen as a stochastic lexicalized conle~:t-free gram- mars since it generates exactly context-free languages while being lexically sensitive. In the following, due to the lack of space, we report only few experiments on grammar inference using these restricted forms of SLTAG and the reestimation algo- rithm given in Section 4. We compare the results of the TAG inside-outside algorithm with the results of the inside-outside algorithm for context-free grammars (Baker, 1979). These preliminary experiments suggest that SLTAG achieves faster convergence (and also to a better solu- tion) than stochastic ontext-free grmnmars. 5.1 In ferr ing the Language {a"b"\]n > 0}. We consider first an artificial language. The train- ing corpus consists of 100 sentences in the language L = {a"b'~ln > 0} randomly generated by a stochastic context-free grammar. The initial grammar consists of the trees ~' , fl~, c~ a and ab with random probability of adjoining and null adjoining. The inferred grammar models correctly the language L. Its rules of the form (I), (5) or (fi) with high prob- ability follow (any excluded rule of the same form has probability at least l0 -a3 times lower than the rules given below). The structural rules of the form (2), (3), (4) or (7) are not shown since their probability always remain 1. Z?This can be Been by ol~ervin g that, for exaanple in l(posji, i,j,k,I), it i~ nece~y the ea~ that k = l, nnd also by noting that k is superfluous. t\[$,Tg\] s:~4 t\[S,lg,78\] t\[$og\] o_~ t\[$,lg,lg\] t\[.-t/~\] z_~,o b\[,.~7~\] t \ [~\ ] ,..~o b\[,~\] t\[..~\] ~,? b \ [~\ ] t\[..o~\] 1~0 b\[..o~\] In the above grammar, a node S'k in a tree c~ a or /~ associated with the symbol a is referred as t/~, and a node S~ in a tree associated with b as r/~. We also conducted a similar experiment with the inside-outside algorithm for context-free grammar (Baker, 1979), starting with all pc~sible Chomsky Nor- mal Form rules over 4 non-terminals and the set of ter- minal symbols {a,b} (72 rules). The inferred grammar does not quite correctly model the language L. Fur- thermore, the algorithm does not converge as fast as in the case of SLTAG (See Figure 1). 1.8 1 .6 1,4 1.2 1 0 .8 0 .6 0.4 I I I I I I I I SLTAG - - SCFG . . . " \ 2 3 4 5 6 7 8 9 1 0 iteration Figure 1: Convergence for the Language {anb"ln > 0} 5.2 Exper iments on the ATIS Corpus. We consider the part-of-speech sequences of the spoken- language transcriptions in the Texas Instruments sub- ACT~ BE COIANG-92. NANTES, 23-28 AO~' 1992 4 3 0 PROC. OF COLING-92, NANTES, AUG. 23-28, 1992 set of the Air Travel hfformation System (ATIS) corpus (Hemphill, Godfrey, and Doddington, 1990). This cor- pus is of interest since it has been used for infcrring stochastic ontext-free grammars from partially brack- eted corpora (Pereira and Sehabes, 1992). We use the data given by Pereira and Schabes (1992) on raw text and compare with an inferred SLTAG. The initial grammar consists of all trees (96) of the form fl~, a ~ for all 48 terminal symbols for part-of- speech. As shown in Figure 2, the grannnar converges very rapidly to a lower value of the log probability than the stochastic ontext-free grammar eported by Pereira and Schabes (1992). 16 14 12 i0 SCFG ..... " i i i t 5 10 15 20 25 iteration Figure 2: Convergence for ATIS Corpus 6 Conc lus ion. A novel statistical language model and fundamental - gorithms for this model have been presented. SLTAGs provide a stochastic model both hierarchi- cal and sensitive to lexical information. They combiae the advantages of purely |exical models such ms N-gram distributions or Ilidden Markov Models and the one of ifierarchical modes as stochastic ontext-free gram- mars without their inhercnt limitations. The parame- ters of a SLTAG correspond to the probability of com- bining two structures each one associated with a word and therefore capture linguistically relevant distribu- tions over words. An algorithm for computing the probability of a sen- tence generated by a SLTAG was presented as well as an iterative algorithm for estimating the parameters of a SLTAG given a training corpus of raw text. Simi- larly to its context-free counterpart, he reestimation algorithm can be extended to handle partially parsed corpora (Pereira and Schabes, 1992). Preliminary experiments with a context-free subset of SLTAG confirms that SLTAG enables faster conver- gence than stochastic ontext-free grammars (SCFG). This is the case since SCFG are unable to represent lexieal influences on distribution except by a statisti- cally and eomputationally impractical proliferation of nonterminal symbols, whereas SLTAG allows for a lexi- eally sensitive distributional mmlysis while maintaining a hierarchical structure. Furthermore, the techniques explained in this paper apply to other grammatical formalisms uch as combi- natory categorial grammars and modified head gram- mars since they have been proven to be equivalent to tree-adjoining grammars and linear indexed grmnmars (Joshi, Vijay-Shanker, and Weir, 1991). Due to the lack of space, only few experiments with SLTAG were reported. A full version of tile paper will be available by tile time of the meeting and more exper- imental details will be reported uring the presentation of the paper. In collaboration with Aravind Joshi, Fernando Pereira and Stuart Slfieber, we are currently investigat- ing additional algorithnLs and applications for SLTAG, methods for lexical clustering and autonratic onstruc- tion of a SLTAG from a large training corpus. Aho, A. V. 1968. lndexed grammars - An extension to context free grammars. J ACM, 15:647-671. Baker, J.K. 1979. Trainable grammars tbr speech recognition. In Jared J. Wolf and Dennis H. Klatt, editors, Speech communication papers presentacd at the 97 ~h Meeting of the Acoustical Society of Amer- ica, MIT, Cambridge, MA, June. llooth, Taylor R. and Richard A. Thoml)son. 1973. Applying probability measures to abstract languages. IEEE 7)'aasactions on Computers, C-22(5):442-450, May. Booth, T. 1969. Probabilistic representation f formal languages. In Tenth Annual IEEE Symposium on Switching and Automata Theory, October. Chomsky, N., 1964. Syntactic Structures, chapter 2-3, pages 13-18. Mouton. Gazdar, G. 1985. Applicability of indexed gr,'unmars to natural anguages. Technical Report CSLI-85-34, Center for Study of Language and Information. tlempttill, Charles T., John J. Godfrey, and George IL Doddington. 1990. The ATIS spoken language sys- tems pilot corpus. In DARPA Speech and Natural Laaguage Workshop, Hidden Valley, Pennsylvania, June. Jelinek, F., J. D. Lafferty, and R. L. Mercer. 1990. Ba- sic methods of probabilistic ontext free grammars. Technical Report RC 16374 (72684), IBM, Yorktown Heights, New York 10598. Joshi, Aravind K. and Yves Schabes. 1991. Tree- adjoiuing grammars and lexiealized grammars. In Maurice Nivat and Andreas Podelski, editors, Defin- ability and Recognizability ofSets of Trees. Elsevier. Forthcoming. Joshi, Aravind K., K. Vijay-Simnker, and David Weir. 1991. The convergence of mildly context-sensitive gramnmtical formalisms, in Peter Sells, Stuart Shieber, and Tom Wasow, editors, Foundational Is- sues in Natural Language Processing. MIT Press, Cambridge MA. Joshi, Aravind K. 1987. An Introduction to Tree Ad- joining Grammars. In A. Manaster-Ramer, editor, Mathematics of Language. John Beujamins, Amster- dana. Lari, K. and S. J. Young. 1990. The estimation of stochastic ontext-free grmnmars using the Inside- Outside algorithm. Computer Speech and Language, 4:35-56. ACRES DE COL1NG-92, NANTES, 23-28 AO~r 1992 4 3 1 PROr'.. OI: COLING-92, NANTES, AUG. 23-28, 1992 Pereira, Fernando and Yves Schabes. 1992. Inside- outside reest imation from partial ly bracketed cor- pora. In 20 th Meeting of the Association for Compu- tational Linguistics (ACL '9~), Newark, Delaware. Prat t , Fletcher. 1942. Secret and urgent, the story of codes and ciphers. Blue Ribbon Books. Resnik, Philip. 1991. Lexicalized tree-adjoining ram- mar for distr ibutional analysis. In Penn Review of Linguistics, Spring. Schabes, Yves, Anne Abeill~, and Aravind K. Joshi. 1988. Pars ing strategies with ' lexicalized' grarnmars: Application to tree adjoining gra~mnars. In Proceed- ings of the 1~ lh International Conference on Compu- tational Linguistics (COLING'88}, Budapest, Hun- gary, August . Sehabes, Yves. 1990. Mathematical nd Computational Aspects of Lexicalized Grammars. Ph.D. thesis, Uni- versity of Pennsylvania, Philadelphia, PA, August. Available as technical report (MS-CIS-90-48, L INC LAB179) from the Department of Computer Science. Schabes, Yves. 1991. An inside-outside algor i thm for est imat ing the parameters of a hidden stochastic context-free grammar based on Earley's algorithm. Manuscript. Shannon, C. E. 1948. A mathemat ica l theory of communicat ion. The Bell System Technical Journal, 27(3):379-423. Shannon, C. E. 1951. Predict ion and entropy of printed english. The Bell System Technical Journal, 30:50- 64. Vi jay-Shanker, K. and David J. Weir. 1991. Pars ing constrained grammar formalisms. In preparation. Vi jay-Shanker, K. 1987. A Study of ?lbee Adjoining Grammars. Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvmfia. A Comput ing the Ins ide P rob- ab i l i t i es In the following, the inside and outside probabilities are re\]ative to the input string w. 3 t" stands for the the set of foot nodes, S for the set of nodes on which substitution can occur, ~ for the set of root nodes of initial trees, and ,4 for the set of non-terminal nodes of auxiliary trees. The inside probability can be computed bottom-up with the following recurrence quations. For all node v/found in an elementary tree, it can be shown that: 1. If b\[$r/\] ~ a, I(b,7, i , - , - , I ) = dl if / = i+ 1 and if. a = w~ +1, 0 otherwise. 2. \] f71 E3 c, l(b,7/,i,j,k,t)= l if i= j and if k = l, 0 otherwise. In stochastic tree-adjoining grammar (Schabes, 1992), this lack of context-sensitivity is overcome by assigning probabilities to larger structural units. In (Schabes, 1992) it is proposed to infer a stochastic TAG from a large training corpus using an inside-outside-like iterative algorithm. In (Pereira and Schabes, 1992), 90.36% bracketing accuracy was reported using a stochastic CFG trained on bracketings from the ATIS corpus. Our general estimation method also has practical applications in cases one uses a probabilistic context-free grammar to approximate strictly more powerful rewriting systems, as for instance probabilistic tree adjoining grammars (Schabes, 1992). In recent years, the statistical parsing community has begun to reach out; for syntactic formalisms that recognize the individuality of words, link grammars (Sleator and &apos; Pemperley, 1991) and lexicalized tree-adjoining grammars (Schabes, 1992) have now received stochastic treatments. Although it is beyond the scope of our research, we conjecture that there exists a Monte Carlo disambiguation algorithm for at least Stochastic Tree-Adjoining Grammar (Schabes, 1992). Examples of the first approach can be seen most clearly with the usage of CNF grammars by the Inside-Outside algorithm (Pereira and Schabes, 1992, Lari and Young, 1990).
Instance Weighting for Domain Adaptation in NLP Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains. In this paper, we study the domain adaptation problem from the instance weighting perspective. We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains. We then propose a general instance weighting framework for domain adaptation. Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective. For evaluation we selected two domain adaptation datasets: spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al, 2007). In a complimentary approach, Jiang and Zhai (2007) weighed training instances based on their similarity to unlabeled target domain data.  We show that, on the NER task, DAB outperforms supervised, transductive and standard bootstrapping algorithms, as well as a bootstrapping variant, called balanced bootstrapping (Jiang and Zhai, 2007), that has recently been proposed for domain adaptation. Jiang and Zhai (2007) proposed an instance re-weighting framework that handles both the [S+T+] and [S+T] settings. Jiang and Zhai (2007) recently proposed an instance re-weighting framework to take domain shift into account.  Balanced bootstrapping has been shown to be more effective for domain adaptation than standard bootstrapping (Jiang and Zhai, 2007) for named entity classification on a subset of the dataset used here. It also outperforms balanced bootstrapping, an approach designed for domain adaptation (Jiang and Zhai, 2007). This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). (Jiang and Zhai, 2007) used a small number of labeled data from target domain to weight source instances. Among the previously mentioned work, (Jiang and Zhai, 2007) is a special case given that it discusses both aspects of adaptation algorithms. This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiangand Zhai, 2007) to down weight domain-specific examples in OUT. We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (non discriminative) instance weighting. For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. The maintainer of the system may be notified that performance is suffering, labels can be obtained for a sample of instances from the stream for retraining, or large volumes of unlabeled instances can be used for instance reweighting (Jiang and Zhai, 2007).  Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation.  
Nouns are Vectors Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space We propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors. Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training. A small post-hoc analysis further suggests that, when the model-generated AN vector is not similar to the corpus-observed AN vector, this is due to anomalies in the latter. We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task. Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs: the simple additive and multiplicative models of Mitchell and Lapata (2008), and the linear-map-based models of Guevara (2010) and Baroni and Zamparelli (2010). The final approach we re-implement is the one proposed by Baroni and Zamparelli (2010), who treat attributive adjectives as functions from noun meanings to noun meanings. Baroni and Zamparelli (2010) show that their model significantly outperforms other vector composition methods, including addition, multiplication and Guevara's approach, in the task of approximating the correct vectors for previously unseen (but corpus-attested) ANs. Note that they follow very closely the procedure of Baroni and Zamparelli (2010), including choices of source corpus and parameter values, so that we expect their results on the quality of the various models in predicting ANs to also hold for our re-implementations. Confirming the results of Baroni and Zamparelli (2010), non-normalized versions of add and mult were also tested, but did not produce significant results (in the case of multiplication, normalization amounts to multiplying the composite vector by a scalar, so it only affects the length-dependent vector length measure). It is important to note that, as reported in Baroni and Zamparelli (2010), the mult method can be expected to perform better in the original, non reduced semantic space because the SVD dimensions can have negative values, leading to counter intuitive results with component-wise multiplication (multiplying large opposite-sign values results in large negative values instead of being cancelled out). Following Guevara, we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented in the Rpls package (MevikandWehrens, 2007), with the latent dimension parameter of PLSR set to 50, the same value used by Baroni and Zamparelli (2010). Finally, in the adjective-specific linear map (alm) method of Baroni and Zamparelli (2010), an AN is generated by multiplying an adjective weight matrix with a noun vector. In Baroni and Zamparelli (2010), the alm model performed far better than add and mult in approximating the correct vectors for unseen ANs, while on this (in a sense, more meta linguistic) task add and mult work better, while alm is successful only in the more sophisticated measure of neighbor density. Although, somewhat disappointingly, the model that has been shown in a previous study (Baroni and Zamparelli, 2010) to be the best at capturing the semantics of well-formed ANs turns out to be worse than simple addition and multiplication. In particular, Baroni and Zamparelli (2010) tackle adjective-noun compositions using a vector representation for nouns and learning a matrix representation for each adjective. Work by Baroni and Zamparelli (2010) models nouns as vectors in some semantic space and adjectives as matrices. Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators. Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p= Ab, where A is an adjective matrix and b is a vector for a noun. Relevant papers include O Seaghdha (2010), who evaluates several topic models adapted to learning selectional preference using co-occurence and Baroni and Zamparelli (2010), who represent nouns as vectors and adjectives as matrices, thus treating them as functions over noun meaning. The model by Baroni and Zamparelli (2010) emerges as a suitable model of adjectival composition, while multiplication and addition shed mixed results. Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. In the adjective-specific linear map (alm) model, proposed by Baroni and Zamparelli (2010), a different matrix B is learnt for each adjective.
A Generative Model for Parsing Natural Language to Meaning Representations In this paper, we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures. The model is applied to the task of mapping sentences to hierarchical representations of their underlying meaning. We introduce dynamic programming techniques for efficient training and decoding. In experiments, we demonstrate that the model, when coupled with a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models.  Like the hybrid tree semantic parser (Lu et al, 2008) and the synchronous grammar based WASP (Wong and Mooney, 2006), our model simultaneously generates the input MR tree and the output NL string. The hybrid tree model (Lu et al, 2008) takes a transformative perspective that is in some ways more similar to our model. WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system. A novel grammar induction algorithm: To automatically induce such synchronous grammar rules, we propose a novel generative model that establishes phrasal correspondences between logical sub-expressions and natural language word sequences, by extending a previous model proposed for parsing natural language into meaning representations (Lu et al, 2008). Of particular interest is our prior work Lu et al (2008), in which we presented a joint generative process that produces a hybrid tree structure containing words, syntactic structures, and meaning representations, where the meaning representations are in a variable-free tree-structured form. Hybrid Tree in Lu et al (2008), a generative model was presented to model the process that jointly generates both natural language sentences and their underlying meaning representations of a variable-free tree structured form. In this section, we present a novel hybrid tree model that provides the following extensions over the model of Lu et al (2008). Since we allow a packed meaning forest representation rather than a fixed tree structure, the MR model parameters in this work should be estimated with the inside-outside algorithm as well, rather than being estimated directly from the training data by simple counting, as was done in Lu et al (2008). Motivated by the limitations of these previous methods, we propose a new generative alignment model that includes a full semantic parsing model proposed by Lu et al (2008). Motivated by this prior research, our approach combines the generative alignment model of Liang et al (2009) with the generative semantic parsing model of Lu et al (2008) in order to fully exploit the NL syntax and its relationship to the MR semantics. Our model is built on top of the generative semantic parsing model developed by Lu et al (2008). Lu et al (2008) introduced a generative semantic parsing model using a hybrid-tree framework. We use Lu et al (2008)'s generative model for this step, in which: P (w|e)=?? T over (w, m) P (T ,w|m) (2) where m is the MR logical form defined by event e and T is a hybrid tree defined over the NL? MR pair (w, m). Lu et al (2008) propose 3 models for generative semantic parsing :unigram, bigram, and mix gram (interpolation between the two). Our model is built on top of Lu et al (2008)'s generative semantic parsing model, which is also trained in several steps in its best-performing version. The bigram model of Lu et al (2008), which is the one used in this paper, must be trained using parameters previously learned for the IBM Model 1 and unigram model in order to exhibit the best performance. In particular, our proposed model outperforms the generative alignment model of Liang et al (2009), indicating that the extra linguistic information and MR grammatical structure used by Lu et al (2008)'s generative language model make our overall model more effective than a simple Markov + bag-of-words model for language generation.  The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008).
Does Baum-Welch Re-Estimation Help Taggers? In part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammatical categories to words in a text. Early work in the field relied on a corpus which had been tagged by a human annotator to train the model. recently, Cutting al. suggest that training can be achieved with a minimal lexicon and a limited amount priori about probabilities, by using an Baum-Welch re-estimation to automatically refine the model. In this paper, I report two experiments designed to determine how much manual training information is needed. The first experiment suggests that initial biasing of either lexical or transition probabilities is essential to achieve a good accuracy. The second experiment reveals that there are three distinct patterns of Baum-Welch reestimation. In two of the patterns, the re-estimation ultimately reduces the accuracy of the tagging rather than improving it. The pattern which is applicable can be predicted from the quality of the initial model and the similarity between the tagged training corpus (if any) and the corpus to be tagged. Heuristics for deciding how to use re-estimation in an effective manner are given. The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the model. 1 Background Part-of-speech tagging is the process of assigning grammatical categories to individual words in a corpus. One widely used approach makes use of a statistical technique called a Hidden Markov Model (HMM). The model is defined by two collections of the probabilities, express the probability that a tag follows the preceding (or two for a second order model); and the the probability that a word has a given tag without regard to words on either side of it. To tag a text, the tags with non-zero probability are hypothesised for each word, and the most probable sequence of tags given the sequence of words is determined from the probabilities. Two algorithms are commonly used, known as the Forward-Backward (FB) and Viterbi algorithms. FB assigns a probability to every tag on every word, while Viterbi prunes tags which cannot be chosen because their probability is lower than the ones of competing hypotheses, with a corresponding gain in computational ef Elworthy (1994), in contrast, reports an accuracy of 75.49%, 80.87% and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags. While this observation confirms Elworthy (1994), the impact of error reduction is much less than reported there for English about 70% (79? 94). Experimental results confirming this wisdom have been presented ,e.g., by Elworthy (1994) and Pereira and Schabes (1992) for EM training of Hidden Markov Models and PCFGs. Although [Kupiec, 1992] presented a very sophisticated method of unsupervised learning, [Elworthy, 1994] reported that re-estimation is not always helpful.  However, Merialdo (1994) and Elworthy (1994) have criticized methods of estimation from an untagged corpus based on the maximum likelihood principle. Discuss ion and Future Work Merialdo (1994) and Elworthy (1994) have insisted, based on their experimental results, that the maximum likelihood training using an untagged corpus does not necessarily improve tagging accuracy. On this point, I agree with Merialdo (1994) and Elworthy (1994).  Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-of speech tagger when deployed in an unsupervised or semi-supervised setting.  Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available). Elworthy (1994), in contrast, reports accuracy of 75.49%, 80.87%, and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags. We considered three taggers: the El worthy bigram tagger (Elworthy, 1994) within the RASP parser (Briscoe et al, 2006), an enhanced. In [Elworthy, 1994], similar experiments were run. The advantage of combining unsupervised and supervised learning over using supervised in [Elworthy, 1994] quotes accuracy on ambiguous words, which we have converted to overall accuracy.
TAG Dynamic Programming and the Perceptron for Efficient Feature-Rich Parsing We describe a parsing approach that makes use of the perceptron algorithm, in conjunction with dynamic programming methods, to recover full constituent-based parse trees. The formalism allows a rich set of parse-tree features, including PCFGbased features, bigram and trigram dependency features, and surface features. A severe challenge in applying such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved. We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism. A lower-order dependency parsing model is used to restrict the search space of the full model, thereby making it efficient. Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy. This strategy is similar to the one employed by Carreras et al (2008) to prune the search space of the actual parser.  This approach can be seen as trade-off between phrase based reranking experiments (Collins, 2000) and the approach of Carreras et al (2008) where a discriminative model is used to score lexical features representing unlabelled dependencies in the Tree Adjoining Grammar formalism. Our system also compares favourably with the system of Carreras et al (2008) that relies on a more complex generative model, namely Tree Adjoining Grammars, and the system of Suzuki et al (2009) that makes use of external data (unannotated text). Carreras et al (2008) use coarse-to-fine pruning with dependency parsing, but in that case, a graph based dependency parser provides the coarse pass, with the fine pass being a far-more-expensive tree adjoining grammar.  2.2? Petrov& amp; Klein (2007) 6.2 Carreras et al (2008) Unk This Paper Baseline 100.7 Baseline+Padding 89.5 Baseline+Padding+Semi 46.8 Table 9: Comparison of running times on the English test set, where the time for loading model sis excluded.  Suzuki et al (2009) and phrase-structure annotations in the case of Carreras et al (2008).    (Carreras et al., 2008) and edge annotation (Huang, 2008). Many edges can be ruled out beforehand, either based on the distance in the sentence between the two words (Eisner and Smith, 2010), the predictions of a local ranker (Martins et al2009), or the marginals computed from a simpler parsing model (Carreras et al2008). The key idea in our approach is to allow highly flexible reordering operations, in combination with a discriminative model that can condition on rich features of the source-language input string. Our approach builds on a variant of tree adjoining grammar (TAG; (Joshi and Schabes, 1997)) (specifically, the formalism of (Carreras et al, 2008)). Our work builds on the variant of tree adjoining grammar (TAG) introduced by (Carreras et al., 2008). To continue our example, the resulting entry would be as follows: es gibt? S NP there VP is To give a more formal description of how syn tactic structures are derived for phrases, first note that each parse tree t is mapped to a TAG derivation using the method described in (Carreras et al, 2008). 
Clustering Polysemic Subcategorization Frame Distributions Semantically Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data. We describe a new approach which involves clustering subcategorizaframe distributions using the Information Bottleneck and nearest neighbour methods. In contrast to previous work, we particularly focus on clustering polysemic verbs. A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, offering us a good insight into the potential and limitations of semantically classifying Schulteim Walde and Brew (2002) used the k-Means (Forgy, 1965) algorithm to cluster SCF distributions for monose mous verbs while Korhonen et al (2003) applied other clustering methods to cluster polysemic SCF data. The most closely related work to our polysemy aware task of unsupervised verb class induction is the work of Korhonen et al (2003), who used distributions of sub categorization frames to cluster verbs. Korhonen et al (2003) evaluated hard clusterings based on a gold standard with multiple classes per verb. Table 1: An excerpt of the gold-standard verb classes for several verbs from Korhonen et al (2003). We first evaluate our induced verb classes on the test set created by Korhonen et al (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin's classes and the LCS database (Dorr, 1997). We first implemented a soft clustering method for verb class induction proposed by Korhonen et al (2003).  By following the method of Korhonen et al (2003), prepositional phrases (pp) are parameterized for two frequent sub categorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb. We evaluate the single-class output for each verb based on the predominant gold-standard classes, which are defined for each verb in the test set of Korhonen et al (2003). We evaluate these single-class outputs in the same manner as Korhonen et al (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations. For baselines, we once more adopt the Nearest Neighbor (NN) and Information Bottleneck (IB) methods proposed by Korhonen et al (2003), and LDA-frames proposed by Materna (2012). Korhonen et al (2003) reported that the highest modified purity was 49% against predominant classes and 60% against multiple classes.  There are a few exceptions to this tradition, such as Pereira et al (1993), Rooth et al (1999), Korhonen et al (2003), who used soft clustering methods for multiple assignment to verb semantic classes. In this paper, we extend an existing approach to lexical classification (Korhonen et al, 2003) and apply it (without any domain specific tuning) to the domain of biomedicine. We extended the system of Korhonen et al (2003) with additional clustering techniques (introduce din sections 3.2.2 and 3.2.4) and used it to obtain the classification for the biomedical domain. The closest possible comparison point is (Korhonen et al, 2003) which reported 50-59% mPUR and 15-19% APP on using IB to assign 110 polysemous (general language) verbs into 34 classes. Korhonen et al (2003) observed the opposite with general language data. This use of frame is different than that used for subcate gorization frames, which are also used to induce word classes (e.g., Korhonen et al, 2003).
Joint Parsing and Named Entity Recognition For many language technology applications, such as question answering, the overall system runs several independent processors over the data (such as a named entity recognizer, a coreference system, and a parser). This easily results in inconsistent annotations, which are harmful to the performance of the aggregate system. We begin to address this problem with a joint model of parsing and named entity recognition, based on a discriminative feature-based constituency parser. Our model produces a consistent output, where the named entity spans do not conflict with the phrasal spans of the parse tree. The joint representation also allows the information from each type of annotation to improve performance on the other, and, in experiments with the OntoNotes corpus, we found improvements of to absolute F1 for parsing, and up to F1 for named entity recognition. Compared with the disappointing results of joint learning on syntactic and semantic parsing, Miller et al (2000) and Finkel and Manning (2009) showed the effectiveness of joint learning on syntactic parsing and some simple NLP tasks, such as information extraction and name entity recognition. We built a joint model of parsing and named entity recognition (Finkel and Manning, 2009b), which had small gains on parse performance and moderate gains on named entity performance, when compared with single-task models trained on the same data.  Our base joint model for parsing and named entity recognition is the same as (Finkel and Manning, 2009b), which is also based on the discriminative parser discussed in the previous section. Our baseline experiments were modeled after those in (Finkel and Manning, 2009b), and while our results were not identical (we updated to a newer release of the data), we had similar results and found the same general trends with respect to how the joint model improved on the single models. We used OntoNotes 3.0 (Hovy et al, 2006), and made the same data modifications as (Finkel and Manning, 2009b) to ensure consistency between the parsing and named entity annotations. These approaches have been shown to be successful for tasks such as parsing and named entity recognition in newswire data (Finkel and Manning, 2009) or semantic role labeling in the Penn Treebank and Brown corpus (Toutanova et al, 2008). Finkel and Manning (2009) proposed a discriminative feature based constituency parser for joint named entity recognition and parsing. However, most of the mentioned approaches are task-specific (e.g., (Toutanova et al, 2008) for semantic role labeling, and (Finkel and Manning, 2009) for parsing and NER), and they can hardly be applicable to other NLP tasks. It has been used in some natural language processing tasks, such as joint parsing and named entity recognition (Finkel and Manning, 2009), and word sense disambiguation (Zhong et al, 2008). Alternative approaches are that of Finkel and Manning (2009) on joint parsing and named entity recognition and the work of (Wehrli et al, 2010) which uses collocation information to rank competing hypotheses in a symbolic parser. For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al, 2009), that couple chunking with (dependency) parsing. Finkel and Manning (2009) built a joint, discriminative model for parsing and named entity recognition (NER), addressing the problem of inconsistent annotations across the two tasks, and demonstrating that NER benefited considerably from the interaction with parsing. In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al, 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. Finkel and Manning (2009b) also proposed a parsing model for the extraction of nested named entity mentions, which, like this work, parses just the corresponding semantic annotations. For instance, performing named entity recognition (NER) jointly with constituent parsing has been shown to improve performance on both tasks, but the only aspect of the syntax which is leveraged by the NER component is the location of noun phrases (Finkel and Manning, 2009). Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. To avoid overfitting, we employed an implementation from previous literature (Finkel and Manning,2009). The spirit of this work more closely resembles that of Finkel and Manning (2009), which improves both parsing and named entity recognition by combining the two tasks. The idea of jointly training parsers to optimize multiple objectives is related to joint learning and inference for tasks like information extraction (Finkeland Manning, 2009) and machine translation (Burkett et al, 2010).
Translation By Structural Correspondences We sketch and illustrate an approach to machine translation that exploits the potential of simultaneous correspondences between separate levels of linguistic representation, as in the of codescriptions. The approach is illustrated with examples from English, German and French where the source and the target language sentence show noteworthy differences in linguistic analysis. In this section, we will outline the approach to the translation of non-local re-entrances proposed in Kaplan et al (1989). We have shown in this paper that the approach to transfer between feature structures introduced in Kaplan et al 1989 can be exploited to deal with the translation of anaphoric dependeneins. The first one applies semantic transfer model via the methodology similar to the lexical functional grammar (Kaplan et al., 1989) and it is develop with the intention of public use.  Kaplan et al (1989) present a framework for translation based on the description and correspondence concepts of Lexical Functional Grammar (Kaplan and Bresnan, 1982). Kaplan et al (1989) present a framework for translation based on the description and correspondence concepts of Lexical-Functional Grammar (Kaplan and Bresnan, 1982). Kaplan et al (1989) suggest that this architecture can provide a formal basis for specifying complex source-target translation relationships in a declarative fashion that builds on monolingual grammars and lexicons that are independently motivated and theoretically justified. Kaplan et al (1989) offer several examples to illustrate the effectiveness of this approach to translation. Kaplan et al (1989) discussed such differences in embedding and offered two alternative analyses that rely only on codescriptive specifications.
Using Mostly Native Data to Correct Errors in Learners&rsquo; Writing We present results from a range of experiments on article and preposition error correction for non-native speakers of English. We first compare a language model and errorspecific classifiers (all trained on large English corpora) with respect to their performance in error detection and correction. We then combine the language model and the classifiers in a meta-classification approach by combining evidence from the classifiers and the language model as input features to the metaclassifier. The meta-classifier in turn is trained on error-annotated learner data, optimizing the error detection and correction performance on this domain. The meta-classification approach results in substantial gains over the classifieronly and language-model-only scenario. Since the meta-classifier requires error-annotated data for training, we investigate how much training data is needed to improve results over the baseline of not using a meta-classifier. All evaluations are conducted on a large errorannotated corpus of learner English. Gamon (2010) proposes a hybrid system for preposition and article correction, by incorporating the scores of a language model and class probabilities of a maximum entropy model, both trained on native data, into a meta-classifier that is trained on a smaller amount of annotated ESL data. In contrast to Gamon (2010) and Han et al (2010) that use annotated data for training, the system is trained on native data, but the native data are transformed to be more like L1 data through artificial article errors that mimic the error rates and error patterns of non-native writers. Gamon (2010) shows precision/recall curves on the combined task of detecting missing, extraneous and confused prepositions. Gamon (2010) also considers missing and extraneous preposition errors. Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b). Gamon et al (2008) and Gamon (2010) used a language model in addition to a classifier and combined the classifier output and language model scores in a meta classifier. Note that this use of a host of language model features is substantially different from using a single language model score on hypothesized error and potential correction to filter out unlikely correction candidates as in Gamon et al (2008) and Gamon (2010). In Figure 4 we compare the sequence modeling results for prepositions with results from the preposition component of the current version of the system described in Gamon (2010) on the same test set.  The heuristics are based on those used in Gamon (2010) (personal communication). Obtaining better-quality training data is a major issue for machine learning applied to learner language, as the domain of writing is different from news-heavy training domains (Gamon, 2010). This is a baseline run that represents the language model approach proposed by Gamon (2010). Correcting preposition errors requires more data to achieve performance comparable to article error correction, due to the task complexity (Gamon, 2010). A third alternative, that of selectively removing or correcting errors, is something of a middle road, and has been used in other work using the CLC data: in particular, Gamon (2010) removes from the data sentences where some other error appears immediately next to a preposition or determiner error. Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al, 2010).
Monolingual Machine Translation For Paraphrase Generation We apply statistical machine translation (SMT) tools to generate novel paraphrases of input sentences in the same language. The system is trained on large volumes of sentence pairs automatically extracted from clustered news articles available on the World Wide Web. Alignment Error Rate (AER) is measured to gauge the quality of the resulting corpus. A monotone phrasal decoder generates contextual replacements. Human evaluation shows that this system outperforms baseline paraphrase generation techniques and, in a departure from previous work, offers better coverage and scalability than the current best-of-breed paraphrasing approaches. In particular, for this study we employ the MSR Paraphrase Corpus (Quirk et al, 2004). However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase pat terns are long or complicated (Quirk et al, 2004). Researchers employ the existing SMT models for PG (Quirk et al, 2004). Baseline-1 follows the method pro posed in (Quirk et al, 2004), which generates paraphrases using typical SMT tools. Quirk et al (2004) also generate sentential paraphrases using a monolingual corpus. A similar approach to our sentence reconstruction method has been developed by Quirk et al (2004) for paraphrase generation. As sentential paraphrasing is more likely to alter meaning, Quirk et al (Quirk et al, 2004) approached paraphrasing as a monotonous decoding by a phrase-based SMT system. Most paraphrase generation tools use some standard SMT decoding algorithms (Quirk et al., 2004) or some off-the-shelf decoding tools like Moses (Koehn et al, 2007). Quirk et al (2004) built a paraphrase generation model from a monolingual comparable corpus based on a statistical machine translation framework, where the language model assesses the grammaticality of the translations ,i.e., generated expressions. Most paraphrase generators use some standard SMT decoding algorithms (Quirk et al, 2004) or some off-the-shelf decoding tools like MOSES. Another piece of related work, (Quirk et al, 2004), starts off with parallel inputs and uses monolingual Statistical Machine Translation techniques to align them and generate novel sentences. Typical examples are paraphrasing using bilingual (Callison-Burch et al, 2006) or monolingual (Quirket al, 2004) data. Quirk et al (2004) build a monolingual translation system using a corpus of sentence pairs extracted from news articles describing same events. Quirk et al (2004) present an end-to-end paraphrasing system inspired by phrase-based machine translation that can both ac quire paraphrases and use them to generate new strings. Although there is a greater supply of paraphrasing corpora, such as the Multiple-Translation Chinese (MTC) corpus 1 and the Microsoft Research (MSR) Paraphrase Corpus (Quirk et al, 2004),. Outside of NLI, prior research has also explored the task of monolingual word alignment using extensions of statistical MT (Quirk et al, 2004) and multi-sequence alignment (Barzilay and Lee, 2002). For other SMT methods, see Quirk et al (2004) and Bannard and Callison-Burch (2005) among others. Paraphrase generation can be viewed as monolingual machine translation (Quirk et al, 2004), which typically includes a translation model and a language model. Quirk et al (2004) first recast paraphrase generation as monolingual SMT. The SMT-based paraphrasing model used by Quirk et al (2004) was the noisy channel model of Brown et al (1993), which identified the optimal paraphrase T of a sentence S by finding.
Using Syntactic Dependency As Local Context To Resolve Word Sense Ambiguity Most previous corpus-based algorithms disambiguate a word with a classifier trained from previous usages of the same word. Separate classifiers have to be trained for different words. We present an algorithm that uses the same knowledge sources to disambiguate different words. The algorithm does not require a sense-tagged corpus and exploits the fact that two different words are likely to have similar meanings if they occur in identical local contexts. Our similarity measure is based on a proposal in (Lin, 1997), where the similarity between two objects is defined to be the amount of information contained in the commonality between the objects divided by the amount of information in the descriptions of the objects. The similarity measure simwN is based on the proposal in (Lin, 1997). We determine closeness using two similarity measures Jiang and Conrath (1997) and Lin (1997) and two relatedness measures Lesk (Banerjee and Pedersen, 2003) and gloss vector overlap (Pedersen et al, 2004) from the Word Net Similarity package. On the other hand, (Lin, 1997) proposes a disambiguation algorithm that relies on the basic intuition that if two occurrences of the same word have identical meanings, then they should have similar local context. Dependencies and a Conceptual Network Similar to (Lin, 1997), we consider the syntactic dependency of words, but we also consider the conceptual hierarchy of a word obtained through the WordNet semantic network as a means for generalization, capable to handle unseen words. These methods acquire contextual information directly from unannotated raw text, and senses can be induced from text using some similarity measure (Lin, 1997). The same compound (or its variant) would be difficult to detect in a document talking about traveling to Java: the two words may appear at some distance or not in some specific syntactic structure as required in (Lin, 1997). (Lin, 1997) also tries to solve word ambiguity by adding syntactic dependency as context. In particular, this method has been used for word sense disambiguation (Lin, 1997) and thesaurus construction (Lin, 1998). We then compute the semantic similarity measure as the Jensen-Shannon (Lin, 1997) divergence JS (L (hi) ||L (h (i)))= 1 2 [D (L (hi) ||avg) +D (L (h (i)) ||avg)] where avg= (L (hi)+ L (h (i))) /2 is the average between the two distributions and D (L (hi) ||avg) is the Kullback Leiber divergence (Cover and Thomas, 2006). To do this, we use one information-content based measure (Lin, 1997), which is provided in Wordnet Similarity package (Pedersen et al, 2004) to evaluate the similarity between two concepts in Wordnet. The term selector comes from (Lin, 1997), and refers to a word which can take the place of another given word within the same local context. Differing from previous systems, the language model in ARE is based on dependency relations obtained from Minipar by Lin (1997). A consideration for future work to enhance para phrasal meaning preservation would be to explore other contextual representations, such as syntactic dependency parsing (Lin, 1997), mutual information between co-occurences of phrases Church and Hanks (1991), or increasing the number of neighboring words used in n-gram based repesentations.  Lin (Lin 1997) described a distributional hypothesis that if two words have similar set of collocations, they are probably similar. Selectors are words which take the place of a given target word within its local context (Lin, 1997). (Lin, 1997) takes a supervised approach that is unique as it did not create a classifier for every target word. We identify a third approach through the use of selectors, first introduced by (Lin, 1997), which help to disambiguate a word by comparing it to other words that may replace it within the same local context. We adopted the term selector from (Lin, 1997) to refer to a word which takes the place of another in the same local context.
TnT - A Statistical Part-Of-Speech Tagger Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger. Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework. A recent comparison has even shown that TnT performs significantly better for the tested corpora. We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words. Furthermore, we present evaluations on two corpora. The sentences in the DSO collection were tagged with parts of speech using TnT (Brants, 2000) trained on the Brown Corpus itself.  The English POS-tagging has been carried out using freely available TNT tagger (Brants, 2000). This proposition is quite viable as statistical POS taggers like TnT (Brants, 2000) are available. We use TnT (Brants, 2000), a second order Markov Model tagger. For PoS tagging and lemmatization, we combine GENIA (with its built-in, occasionally deviant to kenizer) and TnT (Brants, 2000), which operates on pre-tokenized inputs but in its default models trained on financial news from the Penn Tree bank. Tag the tokens with PTB-style POS tags using a tagger (Brants, 2000). For example, Petrov et al (2012) build supervised POS taggers for 22 languages using the TNT tagger (Brants, 2000), with an average accuracy of 95.2%. Forun aligned words, we simply assign a random POS and very low probability, which does not substantially affect transition probability estimates. In Step 6 we build a tagger by feeding the es ti mated emission and transition probabilities into the TNT tagger (Brants, 2000), an implementation of a trigram HMM tagger. based on tree-structures of various complexity in the tree-adjoining grammar model. Using such tags, Brants (2000) has achieved the automated tagging of a syntactic-structure-based set of grammatical function tags including phrase-chunk and syntactic-role modifiers trained in supervised mode from a tree bank of German. We also incorporated part-of speech tagging, using the TnT tagger (Brants, 2000) retrained on the GENIA corpus gold standard part of-speech tagging. POS Majority lexical type noun count-noun-le c-n-f verb trans-nerg-str-verb-le haben-auxf adj adj-non-prd-le adv intersect-adv-le Table 5: POS tags to lexical types mapping Again for comparison, we have built another simple baseline model using the TnT POS tagger (Brants, 2000). The texts were POS-tagged using TnT (Brants,2000). The freely-available POS lexicon from Sharoff et al (2008), specifically the file for the POS tagger TnT (Brants, 2000), contains full words (239,889 unique forms), with frequency information. We use a corpus of 5 million words automatically tagged by TnT (Brants, 2000) and freely available online (Sharoff et al, 2008). Because we want to make linguistically-informed corruptions, we corrupt only the words we have information for, identifying the words in the corpus which are found in the lexicon with the appropriate POS tag. We also select only words which have inflectional morphology: nouns, verbs, adjectives, pronouns, and numerals.7 4.2.1 Determining word properties (step 1) We use the POS tag to restrict the properties of a word, regardless of how exactly we corrupt it. To POS tag, we use the HMM tagger TnT (Brants, 2000) with the model from http: //corpus.leeds.ac.uk/mocky/. After finishing the corrections, we experimented with training and testing the TnT tagger (Brants,2000) on the& quot; old& quot; and on the& quot; corrected& quot; version of NEGRA?. To make them useful, the necessary preprocessing steps must have been done. The texts were first automatically segmented and tokenized and then they were part-of-speech tagged by TnT tagger (Brants, 2000), which was trained on the respective WILS training data. POS tags, on the other, represent more of a challenge with only 91.6% NORM LEMMA POS Agreed tokens (out of 57,845) 56,052 55,217 52,959 Accuracy (%) 96.9% 95.5% 91.6% Table 3: Inter-annotator agreement agreement between two annotators, which is cons id erably lower than the agreement level reported for annotating a corpus of modern German using STTS, at 98.6% (Brants, 2000a). We further plan to retrain state-of the-art POS taggers such as the TreeTagger and TnT Tagger (Brants, 2000b) on our data. Finally, we plan to investigate how linguistic annotations can be automatically integrated in the TEI annotated version of the corpus to produce TEI con formant output.
Statistical Machine Translation By Parsing In an ordinary syntactic parser, the input is a string, and the grammar ranges over strings. This paper explores generalizations of ordinary parsing algorithms that allow the input to consist of string tuples and/or the grammar to range over string tuples. Such algorithms can infer the synchronous structures hidden in parallel texts. It turns out that these generalized parsers can do most of the work required to train and apply a syntax-aware statistical machine translation system.   For example, requiring l not 0 and, if k not 0 then sk must be a child of sl in the source tree, we can implement a synchronous dependency grammar similar to (Melamed, 2004). Melamed (2004) establishes a theoretical framework for generalized synchronous parsing and translation. When a parser's grammar can have fewer dimensions than the parser's input, we call it a synchronizer (Melamed, 2004). Our synchronous parser is similar to the synchronous CKY parser presented at (Melamed, 2004). Recent work in machine translation has evolved from the traditional word (Brown et al, 1993) and phrase based (Koehn et al, 2003a) models to include hierarchical phrase models (Chiang, 2005) and bilingual synchronous grammars (Melamed, 2004). Melamed (2004) formalizes machine translation problem as synchronous parsing based on multi text grammars.  To handle syntactic differences, Melamed (2004) proposes methods based on tree-to-tree mappings. A representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar (Melamed, 2004), parse tree-to-string translation models (Yamada and Knight, 2001) and non isomorphic tree-to-tree mappings (Eisner, 2003). Melamed (2004) also used a similar way to integrate the language model. 
Automatic Evaluation Of Machine Translation Quality Using Longest Common Subsequence And Skip-Bigram Statistics GTM30 0.87 0.81 0.91 0.79 0.67 0.90 0.83 0.77 0.87 0.73 0.62 0.83 0.83 0.77 0.88 0.71 0.60 0.83 1. Pearson’s Spearman’s of automatic evaluation measures vs. 4, and 12 are maximum of 1, 4, and 12 grams, NIST is the NIST ROUGE-L is LCS-based F-measure 1), ROUGE-W is weighted LCS-based F-measure = 1). ROUGE-S* is skip-bigram-based co-occurrence statistics with any skip distance limit, ROUGEis skip-bigram-based F-measure 1) with maximum skip distance of N, PER is position independent word error rate, and WER is word error rate. GTM 10, 20, and 30 are general text matcher exponents of 1.0, 2.0, and 3.0. (Note, only 4, and 12 are shown here to preserve space.) limit and with skip distant limits of 0, 4, and 9. Correlation analysis based on two different correlastatistics, Pearson’s Spearman’s with respect to adequacy and fluency are shown in Table 1. Pearson’s correlation measures the and direction of a between any two variables, i.e. automatic metric score and human assigned mean coverage score in our case. It ranges from +1 to -1. A correlation of 1 means that there is a perfect positive linear relationship between the two variables, a correlation of -1 means that there is a perfect negative linear relationship between them, and a correlation of 0 means that there is no linear relationship between them. Since we would like to use automatic evaluation metric not only in comparing systems a quick overview of the Pearson’s coefficient, see: http://davidmlane.com/hyperstat/A34739.html. but also in in-house system development, a good linear correlation with human judgment would enable us to use automatic scores to predict corresponding human judgment scores. Therefore, Pearson’s correlation coefficient is a good measure to look at. correlation coefficient 6is also a measure of correlation between two variables. It is a non-parametric measure and is a special case of the Pearson’s correlation coefficient when the values of data are converted into ranks before computing the coefficient. Spearman’s correlation coefficient does not assume the correlation between the variables is linear. Therefore it is a useful correlation indicator even when good linear correlation, for example, according to Pearson’s correlation coefficient between two variables could a quick overview of the Spearman’s coefficient, see: http://davidmlane.com/hyperstat/A62436.html. not be found. It also suits the NIST MT evaluation scenario where multiple systems are ranked according to some performance metrics. To estimate the significance of these correlation statistics, we applied bootstrap resampling, generating random samples of the 919 different sentence segments. The lower and upper values of 95% confidence interval are also shown in the table. Dark (green) cells are the best correlation numbers in their categories and light gray cells are statistically equivalent to the best numbers in their categories. Analyzing all runs according to the adequacy and fluency table, we make the following observations: Applying the stemmer achieves higher correlation with adequacy but keeping case information achieves higher correlation with fluency except for (only is shown). For example, Pearson’s correlation of ROUGE-S* with adequacy increases from 0.85 (Case) to 0.95 while its Pearson’s with fluency drops from 0.84 (Case) to 0.78 (Stem). We will focus our discussions on the Stem set in adequacy and Case set in fluency. Pearson's values in the Stem set of the Adequacy Table, indicates that ROUGE- L and ROUGE-S with a skip distance longer than 0 correlate highly and linearly with adequacy and NIST. ROUGE-S* achieves best correlation with a Pearson’s 0.95. Measures favoring consecutive matches, i.e. and 12, ROUGE-W, GTM20 and 30, ROUGE-S0 (bigram), and WER have lower Pear- Among them WER (0.48) that tends to penalize small word movement is the worst performer. One interesting observation is that longer lower correlation with adequacy. generally agree with Pearhave more equivalents. Pearson's values in the Stem of the Fluency Table, indicates that has the highest correlation (0.93) with fluency. However, it is statistically indistinguishable with 95% confidence from all other metrics shown in the Case set of the Fluency Table except for WER and GTM10. GTM10 has good correlation with human judgments in adequacy but not fluency; while GTM20 and GTM30, i.e. GTM with exponent larger than 1.0, has good correlation with human judgment in fluency but not adequacy. ROUGE-L and ROUGE-S*, 4, and 9 are good automatic evaluation metric candidates since they as well as fluency correlation and outperform and 12 significantly in adequacy. Among them, ROUGE-L is the best metric in both adequacy and fluency correlation with human judgment according to Spearman’s correlation coefficient and is statistically indistinguishable from the best metrics in both adequacy and fluency correlation with human judgment according to Pearson’s correlation coefficient. In this paper we presented two new objective automatic evaluation methods for machine translation, ROUGE-L based on longest common subsequence (LCS) statistics between a candidate translation and a set of reference translations. Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in-sequence ngrams automatically while this is a free parameter To give proper credit to shorter common sequences that are ignored by LCS but still retain the flexibility of non-consecutive matches, we proposed counting skip bigram co-occurrence. The skip-bigram-based ROUGE-S* (without skip disrestriction) had the best Pearson's correlation of 0.95 in adequacy when all words were lower case and stemmed. ROUGE-L, ROUGE-W, ROUGE-S*, ROUGE-S4, and ROUGE-S9 were performers to measuring fluency. However, they have the advantage that we can apthem on sentence level while longer would not differentiate any sentences with length shorter than 12 words (i.e. no 12-gram matches). We plan to explore their correlation with human judgments on sentence-level in the future. We also confirmed empirically that adequacy and fluency focused on different aspects of machine translations. Adequacy placed more emphasis on terms co-occurred in candidate and reference translations as shown in the higher correlations in Stem set than Case set in Table 1; while the reverse was true in the terms of fluency. The evaluation results of ROUGE-L, ROUGE- W, and ROUGE-S in machine translation evaluation are very encouraging. However, these measures in their current forms are still only applying string-to-string matching. We have shown that better correlation with adequacy can be reached by applying stemmer. In the next step, we plan to extend them to accommodate synonyms and paraphrases. For example, we can use an existing thesaurus such as WordNet (Miller 1990) or creating a customized one by applying automated synonym set discovery methods (Pantel and Lin 2002) to identify potential synonyms. Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003). Once we have acquired synonym and paraphrase data, we then need to design a soft matching function that assigns partial credits to these approximate matches. In this scenario, statistically generated data has the advantage of being able to provide scores reflecting the strength of similarity between synonyms and paraphrased. ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004). In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement. According to the results reported in that paper, ROUGE-L, ROUGE-W, and ROUGE-S also outperformed NIST.  The other metric is ROUGE (Lin and Och, 2004), here named R. Lin and Och (2004) experimented, unlike previous works, with a wide set of metrics, including NIST, WER (Nießen et al, 2000), PER (Tillmann et al, 1997), and variants of ROUGE, BLEU and GTM. In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005).  Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L. Therefore, Lin and Och (2004) introduced skip-bigram statistics for the evaluation of machine translation.      Stemming is enabled (Lin and Och, 2004a). The optimal set is: { METEOR wnsyn, ROUGE w 1.2} which includes variants of METEOR, and ROUGE (Lin and Och, 2004). Furthermore, we attempt to achieve additional generalization by using skip n-grams (Lin and Och, 2004). ROUGE utilizes skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a).   Skip bigrams, generally speaking, are pairs of words in a sentence order with arbitrary gap (Lin and Och, 2004a). Different from the previous skip bigram statistics which compare sentence similarities through overlapping skip bigrams (Lin and Och, 2004a), the skip bigrams we used are weighted by a decaying factor of the skipping gap in a sentence, giving higher scores to closer occurrences of skip bigrams.
Never Look Back: An Alternative to Centering I propose a model for determining the hearer's attentional state which depends solely on a list of salient discourse entities (S-list). The ordering among the elements of the S-list covers also the of the center the centering model. The ranking criteria for the S-list based on the distinction between entities and incorporate preferences for interand intra-sentential anaphora. The model is the basis for an algorithm which operates incrementally, word by word.  According to Nissim et al, their definitions are built upon Prince? s (1981), and the categorization into old, new, and mediated entities resemble those of Strube (1998) and Eckert and Strube (2001). Our model of prominence is a simple local one similar to (Strube, 1998).  We have compared the obtained results with those obtained by testing bfp (Brennan et al, 1987) and str98 (Strube, 1998). Strube (1998)'s centering approach (whose sentence ordering is designated as SR2 in Table 2) also deals with and even prefers intra sentential anaphora, which raises the upper limit to a more acceptable 80.2%. Strube (1998) and Strube and Hahn (1999) argue that the information status of an antecedent is more important than the grammatical role in which it occurs. Like (Ge et al, 1998), Strube (1998) evaluates on ideal hand annotated data. Strube (1998)'s S-list algorithm is also restricted to the current and last sentence.   The POS properties could indicate whether a candidate refers to a hearer old entity that would have a higher preference to be selected as the antecedent (Strube, 1998). We now turn to our method of anaphora resolution, which extends the algorithm presented in Strube (1998), in order to be able to account for discourse deictic anaphora as well as individual anaphora. In addition to the S-List (Strube, 1998), which contains the referents of NPs available for anaphoric reference, our model includes an A-List for abstract objects. We have compared the obtained results with those obtained by testing bfp (Brennan et al, 1987) and str98 (Strube, 1998). Centering theory has also guided the development of pronoun resolution algorithms, such as the BFP algorithm (Brenan, Friedman and Pollard, 1987) and the S-list algorithm developed by Strube (Strube, 1998).
Exploiting Diverse Knowledge Sources Via Maximum Entropy In Named Entity Recognition This paper describes a novel statistical namedentity (i.e. &quot;proper name&quot;) recognition system built around a maximum entity framework. By working within the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions. These knowledge sources include capitalization features, lexical features, features indicating the current section of text (i.e. headline or main body), and dictionaries of single or multi-word terms. The purely statistical system contains no hand-generated patterns and achieves a result comparable with the best statistical systems. However, when combined with other handcoded systems, the system achieves scores that exceed the highest comparable scores thus-far published.  A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al 1997), Maximum Entropy methods (Borthwick et al 1998, Chieu and Ng 2002), Decision Trees (Sekine et al 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al 2002), Agent-based Approach (Ye et al 2002) and Support Vector Machines. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), named entity recognition (Borthwick et al, 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al, 2003).  For instance, maximum entropy may be used when a high diversity of knowledge sources are to be taken into account (Borthwick et al, 1998). It is straightforward to see that this problem may be resolved using dynamic programming, as did Borthwick et al (1998).
Using String-Kernels For Learning Semantic Parsers We present a new approach for mappingnatural language sentences to their formal meaning representations using string kernel-based classifiers. Our system learns these classifiers for every production in theformal language grammar. Meaning representations for novel natural language sen tences are obtained by finding the most probable semantic parse using these stringclassifiers. Our experiments on two real world data sets show that this approachcompares favorably to other existing sys tems and is particularly robust to noise. KRISP (Kate and Mooney, 2006) is a semantic parser learning system which uses word subsequence kernel based SVM (Cristianini and Shawe-Taylor, 2000) classifiers and was shown to be robust to noise compared to other semantic parser learners. For details please refer to (Kate and Mooney, 2006). Word subsequence kernel was employed in (Kate and Mooney, 2006) to compute the similarity between two substrings. We note that this use of multiple classifiers to determine the most probable parse is similar to the method used in the KRISP semantic parser (Kate and Mooney, 2006). We describe how these are applied in an error driven manner using the base semantic parsing learning algorithm presented in (Kate and Mooney, 2006) resulting in a better learned semantic parser. We very briefly describe the semantic parser learning system, KRISP (Kate and Mooney, 2006), which we will use as a base system for transforming MRGs, we however note that the MRG transformation methods presented in this paper are general enough to work with any system which learns semantic parser using MRGs. KRISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. KRISP (Kate and Mooney, 2006) uses string classifiers to label substrings of the NL with entities from the MR. The remaining refined landmarks plans are then treated as supervised training data for a semantic-parser learner, KRISP (Kate and Mooney, 2006). To train a semantic parser using KRISP (Kate and Mooney, 2006), they had to supply a MRG, a context-free grammar, for their formal navigation plan language. We modify KRISP, a supervised learning system for semantic parsing presented in (Kate and Mooney, 2006), to make a semi-supervised system we call SEMISUP-KRISP. KRISP (Kernel-based Robust Interpretation for Semantic Parsing) (Kate and Mooney, 2006) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data. Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data (Kate and Mooney, 2006). 
Using Semantic Roles to Improve Question Answering Shallow semantic parsing, the automaticidentification and labeling of sentential constituents, has recently received much attention. Our work examines whether seman tic role information is beneficial to questionanswering. We introduce a general frame work for answer extraction which exploits semantic role annotations in the FrameNetparadigm. We view semantic role assignment as an optimization problem in a bipartite graph and answer extraction as an instance of graph matching. Experimental results on the TREC datasets demonstrate im provements over state-of-the-art models. (Shen and Lapata, 2007) has shown that shallow semantic information in the form of predicate argument structures (PASs) improves the automatic detection of correct answers to a target question. For example, Shen and Lapata (2007) show the potential improvement that FrameNet can bring on the performance of a Question Answering (QA) system. The alignment of answers to question types as a semantic role labelling task using similar methods was explored by Shen and Lapata (2007). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures can lead to tangible performance gains in NLP applications such as information extraction (Surdeanu et al, 2003), question answering (Shen and Lapata, 2007) or recognising textual entailment (Burchardt and Frank, 2006). FrameNet has a shorter history in NLP applications than WordNet, but lately more and more researchers have been demonstrating its potential to improve the quality of question answering (Shen and Lapata, 2007) and recognizing textual entailment (Burchardt et al, 2009). Recently, predicate argument structure analysis has attracted the attention of researchers because this information can increase the precision of text processing tasks, such as machine translation, information extraction (Hirschman et al, 1999), question answering (Narayanan and Harabagiu, 2004) (Shen and Lapata, 2007), and summarization (Melli et al., 2005). Shen and Lapata (2007) developed an answer extraction module that incorporates FrameNet style semantic role information. Indeed, the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from information extraction (Surdeanu et al, 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al, 2005). Recent studies (e.g. Shen and Lapata (2007)) show that the use of FrameNet can potentially improve the performance of Question Answering systems. Yet, Shen and Lapata (2007) also point out that the low coverage of the current version of FrameNet significantly limits the expected boost in performance. Other work incorporating syntactic and linguistic information into IR includes early research by (Smeaton, O Donnell and Kelledy, 1995), who employed tree structured analytics (TSAs) resembling dependency trees, the use of syntax to detect paraphrases for question answering (QA) (Lin and Pantel, 2001), and semantic role labelling in QA (Shen and Lapata, 2007). Semantic role analysis has the potential of benefiting a wide spectrum of applications ranging from information extraction (Surdeanu et al, 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al, 2005). In particular, resources annotated with the surface realization of semantic roles, like FrameNet (Baker et al, 1998) or PropBank (Palmeret al, 2005) have shown to convey an improvement in several NLP tasks, from question answering (Shen and Lapata, 2007) to textual entailment (Burchardt et al, 2007) and shallow semantic parsing (Giuglea and Moschitti, 2006).
Verb Semantics And Lexical Selection This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentence as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection.   Note that Wu and Palmer (1994) designed their measure such that shallow nodes are less similar than nodes that are deeper in the WordNet hierarchy. This included similarity measures introduced by Wu and Palmer (1994) (wup), Leacock and Chodorow (1998) (lch), Resnik (1995) (res), Jiang and Conrath (1997) (jcn), Lin (1998) (lin), Banerjee and Pedersen (2003) (lesk), Hirst and St-Onge (1998) (hso) and Patwardhan and Pedersen (2006) (vector and vpairs) respectively. We here experiment with the Hirst-St.Onge and the Wu and Palmer (Wu and Palmer, 1994) measures, as they are pure taxonomic measures, i.e. they do not require any corpus statistics. The Wu and Palmer (Wu and Palmer, 1994) similarity metric measures the depth of the two concepts in the WordNet taxonomy, and the depth of the least common subsumer (LCS), and combines these figures into a similarity score. Figure 2: A bipartite graph representing patterns and tuples 504 measure described in (Wu and Palmer, 1994) which finds the path length to the root node from the least common subsumer (LCS) of the two word senses which is the most specific word sense they share as an ancestor. (4) reported by (Wu and Palmer, 1994). (Wu and Palmer, 1994) uses the path length from the root to the lowest common subsumer (LCS) of two concepts scaled by the distance from the LCS to each concept.  The synset similarity metric defined by Wu and Palmer (1994) combines the path length and synset depth intuitions into a single numeric score that is defined as follows: 2? depth (lca (synset1, synset2)) depth (synset1)+ depth (synset2) (12) In Equation 12 ,lca returns the lowest common ancestor of the two synsets within the WordNet is-a hierarchy. (Wu and Palmer, 1994) combines the depth of the LCS of two concepts into a similarity score. The semantic similarity between words is computed based on Wu and Palmer's measure (Wu and Palmer,1994) using WordNet (Fellbaum, 1998).  Wu and Palmer (1994) proposed to scale the depth of the two synset nodes (depthc1 and depthc2) by the depth of their LCS (depth(lcsc1,c2)). We used the Wu and Palmermea sure (Wu and Palmer, 1994) applied to Dutch EWN for computing the semantic similarity between two words. ATEC is equipped with these two modules as well, and furthermore, with two measures for word similarity, including a WordNet-based (Wu and Palmer, 1994) and a corpus-based measure (Landauer et al, 1998) for matching word pairs of similar meanings. Wu and Palmer (1994) proposed a concept similarity measure between two concepts c1 and c2 as :sim (c1 ,c2)= 2 ?dep (c )len (c1, c) +len (c2, c) +2 ?dep (c) (1) where c is the lowest common subsumer (LCS) of c1 and c2, and len (?,?) is the number of edges between two nodes. The two similarity measures we experiment with are that of Wu and Palmer (1994) and Jiang and Conrath (1997). 
Grammatical Category Disambiguation By Statistical Optimization three previous efforts directed specifically to this problem. The first published effort is that of Klein and Simmons (1963), a simple system using suffix lists and limited frame rules. The second approach to lexical disambiguation is and Rubin (1971)), a system of several thousand context-frame rules. This algorithm was used to assign initial tags to the Brown Corpus. Third is the CLAWS system develto tag the (or LOB) Coris a corpus of British written English, parallel to the Brown Corpus. Parsing systems always encounter the problem of category ambiguity; but usually the focus of such systems is at other levels, making their responses less relevant for our purposes here. 1.1 KLEIN AND SIMMONS Klein and Simmons (1963) describe a method directed primarily towards the task of initial categorial tagging rather than disambiguation. Its primary goal is avoiding &quot;the labor of constructing a very large dictionary&quot; (p. 335); a consideration of greater import then than now. The Klein and Simmons algorithm uses a palette of 30 categories, and claims an accuracy of 90% in tagging. The algorithm first seeks each word in dictionaries of about 400 function words, and of about 1500 words which &quot;are exceptions to the computational rules used&quot; (p. 339). The program then checks for suffixes and special characters as clues. of all, frame tests applied. These work on scopes bounded by unambiguous words, as do later algorithms. However, Klein and Simmons impose an explicit limit of three ambiguous words in a row. For such ambiguous words, the pair of unambiguous categories bounding it is mapped into a list. The list includes all known sequences of tags occurring between the particular bounding tags; all such sequences of the correct length become candidates. The program then matches the candidate sequences against the ambiguities remaining from earlier steps of the algorithm. When only one sequence is possible, disambiguation is successful. The samples used for calibration and testing were limited. First, Klein and Simmons (1963) performed &quot;hand analysis of a sample [size unspecified] of Golden Grammatical Category Disambiguation by Statistical Optimization Book Encyclopedia text&quot; (p. 342). Later, &quot;[w]hen it was run on several pages from that encyclopedia, it correctly and unambiguously tagged slightly over 90% of the words&quot; (p. 344). Further tests were run on small from the Americana from Scientific American. Klein and Simmons (1963) assert that &quot;[o]riginal fears that sequences of four or more unidentified parts of speech would occur with great frequency were not substantiated in fact&quot; (p. 3). This felicity, however, is an artifact. First, the relatively small set of categories reduces ambiguity. Second, a larger sample would reveal both (a) low-frequency ambiguities and (b) many long spans, as discussed below. 1.2 GREENE AND RUBIN (TAGGIT) Greene and Rubin (1971) developed TAGGIT for tagging the Brown Corpus. The palette of 86 tags that TAGGIT uses has, with some modifications, also been used in both CLAWS and VOLSUNGA. The rationale underlying the choice of tags is described on pages 3-21 of Greene and Rubin (1971). Francis and Kucera (1982) report that this algorithm correctly tagged approxithe million words in the Brown Corpus (the tagging was then completed by human post-editors). Although this accuracy is substantially lower than that reported by Klein and Simmons, it should be remembered that Greene and Rubin were the first to attempt so large and varied a sample. TAGGIT divides the task of category assignment into initial (potentially ambiguous) tagging, and disambiguation. Tagging is carried out as follows: first, the program consults an exception dictionary of about 3,000 words. Among other items, this contains all known closed-class words. It then handles various special cases, such as words with initial &quot;$&quot;, contractions, special symbols, and capitalized words. The word's ending is then checked against a suffix list of about 450 strings. The lists were derived from lexicostatistics of the Brown Corpus. If TAGGIT has not assigned some tag(s) after these several steps, &quot;the word is tagged NN, VB, or JJ [that is, as being three-ways ambiguous], in order that the disambiguation routine may have something to work with&quot; (Greene and Rubin (1971), p. 25). After tagging, TAGGIT applies a set of 3300 context frame rules. Each rule, when its context is satisfied, has the effect of deleting one or more candidates from the list of possible tags for one word. If the number of candidates is reduced to one, disambiguation is considered successful subject to human post-editing. Each rule can include a scope of up to two unambiguous words on each side of the ambiguous word to which the rule is being applied. This constraint was determined as follows: In order to create the original inventory of Context Frame Tests, a 900-sentence subset of the Brown University Corpus was tagged. . . and its ambiguities were resolved manually; then a program was run 32 Computational Linguistics, Volume 14, Number 1, Winter 1988 Steven J. DeRose Grammatical Category Disambiguation by Statistical Optimization which produced and sorted all possible Context Frame Rules which would have been necessary to perform this disambiguation automatically. The rules generated were able to handle up to three consecutive ambiguous words preceded and followed by two non-ambiguous words [a constraint similar to Klein and Simmons']. However, upon examination of these rules, it was found that a sequence of two or three ambiguities rarely occurred more than once in a given context. Consequently, a decision was made to examine only one ambiguity at a time with up to two unambiguously tagged words on either side. The first rules created were the results of informed intuition (Greene and Rubin (1972), p. 32). 1.3 CLAWS Marshall (1983, p. 139) describes the LOB Corpus tagging algorithm, later named CLAWS (Booth (1985)), as &quot;similar to those employed in the TAGGIT program&quot;. The tag set used is very similar, but somewhat larger, at about 130 tags. The dictionary used is derived from the tagged Brown Corpus, rather than from the untagged. It contains 7000 rather than 3000 entries, and 700 rather than 450 suffixes. CLAWS treats plural, possessive, and hyphenated words as special cases for purposes of initial tagging. The LOB researchers began by using TAGGIT on parts of the LOB Corpus. They noticed that While less than 25% of TAGGIT's context frame rules are concerned with only the immediately preceding or succeeding word. . . these rules were applied in about 80% of all attempts to apply rules. This relative overuse of minimally specified contexts indicated that exploitation of the relationship between successive tags, coupled with a mechanism that would be applied throughout a sequence of ambiguous words, would produce a more accurate and effective method of word disambiguation (Marshall (1983), p. 141). The main innovation of CLAWS is the use of a matrix probabilities, the relative likelihood of co-occurrence of all ordered pairs of tags. This matrix can be mechanically derived from any pre-tagged corpus. CLAWS used &quot;[a] large proportion of the Brown Corpus&quot;, 200,000 words (Marshall (1983), pp. 141, 150). The ambiguities contained within a span of ambiguous words define a precise number of complete sets of mappings from words to individual tags. Each such of tags is called a path is composed of a number of tag collocations, and each such collocation has a probability which may be obtained from the collocation matrix. One may thus approximate each path's probability by the product of the probabilities of all its collocations. Each path corresponds to a unique assignment of tags to all words within a span. paths constitute a network, the path of maximal probability may be taken to contain the &quot;best&quot; tags. (1983) states that CLAWS the most probable sequence of tags, and in the majority of cases the correct tag for each individual word corresponds to the associated tag in the most probable sequence of tags&quot; (p. 142). But a more detailed examination of the Pascal code for CLAWS revealed that CLAWS has a more complex definition of &quot;most probable sequence&quot; than one might expect. A probability called &quot;SUMSUCCPROBS&quot; is predicated of each word. SUMSUCCPROBS is calculated by looping through all tags for the words immediately preceding, at, and following a word; for each tag triple, an increment is added, defined by: DownGrade(GetSucc(Tag2, Tag3), TagMark) * Get3SeqFactor(Tag1, Tag2, Tag3) the collocational probability of a tag either 1, or a special value the tag-triple list described below. the value of accordance with RTPs as described below. The CLAWS documentation describes SUMSUCC- PROBS as &quot;the total value of all relationships between the tags associated with this word and the tags associated with the next word. . . [found by] simulating all accesses to SUCCESSORS and ORDER2VALS which will be made. . . .&quot; The probability of each node of the span network (or rather, tree) is then calculated in the following way as a tree representing all paths through which the span network is built: = currenttag), TagMark) * Get3SeqFactor(. . .)) = PROB * (predecessor's It appears that the goal is to make each tag's probabe the summed probability of passing through it. At the final word of a span, pointers are followed back up the chosen path, and tags are chosen en route. We will see below that a simpler definition of optimal path is possible; nevertheless, there are several advantages of this general approach over previous ones. First, spans of unlimited length can be handled (subject to machine resources). Although earlier researchers (Klein and Simmons, Greene and Rubin) have suggested that spans of length over 5 are rare enough to be of little concern, this is not the case. The number of spans of a given length is a function of that length and the corpus size; so long spans may be obtained merely by examining more text. The total numbers of spans in the Brown Corpus, for each length from 3 to 19, are: 397111, 143447, 60224, 26515, 11409,5128, 2161, 903, 382, 161, 58, 29, 14, 6, 1, 0, 1. Graphing the logarithms Computational Linguistics, Volume 14, Number 1, Winter 1988 33 Steven J. DeRose Grammatical Category Disambiguation by Statistical Optimization of these quantities versus the span length for each, produces a near-perfect straight line. Second, a precise mathematical definition is possible for the fundamental idea of CLAWS. Whereas earlier efforts were based primarily on ad hoc or subjectively determined sets of rules and descriptions, and employed substantial exception dictionaries, this algorithm requires no human intervention for set-up; it is a systematic process. Third, the algorithm is quantitative and analog, rather than artificially discrete. The various tests and employed by earlier algorithms enforced absolute constraints on particular tags or collocations of tags. Here relative probabilities are weighed, and a series of very likely assignments can make possible a particular, a priori unlikely assignment with which they are associated. In addition to collocational probabilities, CLAWS also takes into account one other empirical quantity: Tags associated with words. . . can be with a marker @ or %; @ indicates that the tag is infrequently the correct tag for the associated word(s) (less than 1 in 10 occasions), % indicates is highly improbable. . . (less than 1 in 100 oc- . . . The word disambiguation program currently uses these markers top devalue values when retrieving a value from the matrix, @ results in the value being halved, % in the value being divided by eight (Marshall (1983), p. 149). Thus, the independent probability of each possible tag for a given word influences the choice of an optimal Such probabilities will be referred to as Probabilities, Other features have been added to the basic algorithm. For example, a good deal of suffix analysis is used in initial tagging. Also, the program filters its output, considering itself to have failed if the optimal tag assignment for a span is not &quot;more than 90% probable&quot;. cases it reorders tags rather than actually disambiguating. On long spans this criterion is effectively more stringent than on short spans. A more significant addition to the algorithm is that a number of tag triples associated with a have been introduced which may either upgrade or downgrade values in the tree computed from the one-step matrix. For example, the triple [1] [2] adverb [3] past-tense-verb has been assigned a factor which downgrades a sequence containing this triple compared with a competing of [1] 'be' [2] adverb [3]-past-participle/adjective, on the basis that after a form of 'be', past participles and adjectives are more likely than a past tense verb (Marshall (1983), p. 146). A similar move was used near conjunctions, for which the words on either side, though separated, are more closely correlated to each other than either is to the conjunction itself (Marshall (1983), pp. 146-147). For example, a verb/noun ambiguity conjoined to a verb should probably be taken as a verb. Leech, Garside, and Atwell (1983, p. 23) describe &quot;IDIOMTAG&quot;, which is applied after initial tag assignment and before disambiguation. It was developed as a means of dealing with sequences which would otherwise cause diffifor the automatic tagging. . . . for example, that tagged as a single conjunction. . . . Tagging Program. . . can look at any combination of words and tags, with or without intervening words. It can delete tags, add tags, or change the probability of tags. Although this program might to be an hoc it is worth bearing in that any fully automatic language analysis syshas to come to with problems of lexical idiosyncrasy. IDIOMTAG also accounts for the fact that the probability of a verb being a past participle, and not simply past, is greater when the following word is &quot;by&quot;, as opposed to other prepositions. Certain cases of this sort may be soluble by making the collocational matrix distinguish classes of ambiguities—this question is being pursued. Approximately 1% of running text is tagged by IDIOMTAG (letter, G. N. Leech to Henry Kucera, June 7, 1985; letter, E. S. Atwell to Henry Kucera, June 20, 1985). Marshall notes the possibility of consulting a complete three-dimensional matrix of collocational probabilities. Such a matrix would map ordered triples of tags into the relative probability of occurrence of each such triple. Marshall points out that such a table would be too large for its probable usefulness. The author has proa table based upon more 85% of the Brown Corpus; it occupies about 2 megabytes (uncompressed). Also, the mean number of examples per triple is very low, thus decreasing accuracy. CLAWS has been applied to the entire LOB Corpus with an accuracy of &quot;between 96% and 97%&quot; (Booth (1985), p. 29). Without the idiom list, the algorithm was 94% accurate on a sample of 15,000 words (Marshall (1983)). Thus, the pre-processor tagging of 1% of all tokens resulted in a 3% change in accuracy; those particular assignments must therefore have had a substantial effect upon their context, resulting in changes of two other words for every one explicitly tagged. But CLAWS is timeand storage-inefficient in the extreme, and in some cases a fallback algorithm is employed to prevent running out of memory, as was discovered by examining the Pascal program code. How often the fallback is employed is not known, nor is it known what effect its use has on overall accuracy. Since CLAWS calculates the probability of every path, it operates in time and space proportional to the product of all the degrees of ambiguity of the words in the span. Thus, the time is exponential (and hence Non-Polynomial) in the span length. For the longest span in the Brown Corpus, of length 18, the number of paths examined would be 1,492,992. 34 Computational Linguistics, Volume 14, Number 1, Winter 1988 Steven J. DeRose Grammatical Category Disambiguation by Statistical Optimization LINEAR-TIME ALGORITHM The algorithm described here depends on a similar empirically-derived transitional probability matrix to that of CLAWS, and has a similar definition of &quot;optimal path&quot;. The tagset is larger than TAGGIT's, though smaller than CLAWS', containing 97 tags. The ultimate assignments of tags are much like those of CLAWS. However, it embodies several substantive changes. Those features that can be algorithmically defined have been used to the fullest extent. Other add-ons have been minimized. The major differences are outlined below. First, the optimal path is defined to be the one whose component collocations multiply out to the highest probability. The more complex definition applied by using the sum of all paths at of the network, is not used. Second, VOLSUNGA overcomes the Non-Polynomial complexity of CLAWS. Because of this change, it is never necessary to resort to a fallback algorithm, and the program is far smaller. Furthermore, testing the algorithm on extensive texts is not prohibitively costly. Third, VOLSUNGA implements Relative Tag Probabilities (RTPs) in a more quantitative manner, based upon counts from the Brown Corpus. Where CLAWS scales probabilities by 1/2 for RTP < 0.1 (i.e., where less than 10% of the tokens for an ambiguous word are in the category in question), and by 1/8 for p < 0.01, VOLSUNGA uses the RTP value itself as a factor in the equation which defines probability. Fourth, VOLSUNGA uses no tag triples and no idioms. Because of this, manually constructing specialcase lists is not necessary. These methods are useful in certain cases, as the accuracy figures for CLAWS show; but the goal here was to measure the accuracy of a wholly algorithmic tagger on a standard corpus. The algorithm is essentially the same as [DeRose, 1988].  This is well-described in for example (DeRose 1988).  As we said at the outset, we don't necessarily believe HunPos to be in any way better than TnT, and certainly the main ideas have been pioneered by DeRose (1988), Church (1988), and others long before this generation of HMM work. We have found that if we first tag every word in the corpus with a part of speech using a method such as Church (1988) or DeRose (1988), and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with a following preposition to/in and verbs associated with a following infinitive marker to/to. Kallgren (1996) gives a more covering description of how XPOST is used on the Swedish material and also sketches the major differences between this algorithm and some others used for tagging, such as PARTS (Church 1988) and VOLSUNGA (DeRose 1988). The latter approach was pioneered by Stolz et al (1965) and Bahl and Mercer (1976), and became widely known through the work of e.g. Church (1988) and DeRose (1988).
Classifier Combination for Improved Lexical Disambiguation One of the most exciting recent directions in machine learning is the discovery that the combination of multiple classifiers often results in significantly better performance than what can be achieved with a single classifier. In this paper, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary. Next, we show how this complementary behavior can be used to our advantage. By using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers. Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. Committee-based approaches to POS tagging have been in focus the last decade: Brill and Wu (1998) combined four different taggers for English using unweighted voting and by exploring contextual cues (essentially a variant of stacking). In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). The complementarity between two learners was defined by Brill and Wu (1998) in order to quantify the percentage of time when one system is wrong, that another system is correct, and therefore providing an upper bound on combination accuracy. The data used in the experiment was selected from the Penn Treebank Wall Street Journal, and is the same used by Brill and Wu (1998). The investigated MMVC model proves to be a very effective participant in classifier combination, with substantially different output to Naive Bayes (9.6% averaged complementary rate, as defined in Brill and Wu (1998)). We have experimented with various classifier combination methods, such as those described in (Brill and Wu, 1998) or (van Halteren et al., 2001), and got improved results, as expected. One opossibility is the example-based combiner in Brill and Wu (1998, Sec. 3.2). Related work includes learning ensembles of POS taggers, as in the work of Brill and Wu (1998), where an ensemble consisting of a unigram model, an N-gram model, a transformation-based model, and an MEMM for POS tagging achieves substantial results beyond the individual taggers. (Van Halteren et al, 1998) and (Brill and Wu, 1998) describe a series of successful experiments for improving the performance of part-of-speech taggers. This suggests that the final accuracy number presented here could be slightly improved upon by classifier combination, it is worth noting that not only is this tagger better than any previous single tagger, but it also appears to outperform Brill and Wu (1998), the best-known combination tagger (they report an accuracy of 97.16% over the same WSJ data, but using a larger training set, which should favor them). Brill and Wu (1998) call this complementary disagreement complementarity. Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc. Comparison of different taggers on the WSJ corpus TBL and ME (Brill and Wu, 1998). Combination techniques have earlier been applied to various applications including machine translation (Jayaraman and Lavie, 2005), part-of-speech tagging (Brill and Wu, 1998) and base noun phrase identification (Sang et al., 2000).
Transliteration Of Proper Names In Cross-Lingual Information Retrieval We address the problem of transliterating English names using Chinese orthography in support of cross-lingual speech and text processing applications. We demonstrate the application of statistical machine translation techniques to “translate” the phonemic representation of an English name, obtained by using an automatic text-to-speech system, to a sequence of initials and finals, commonly used subword units of pronunciation for Chinese. We then use another statistical translation model to map the initial/final sequence to Chinese characters. We also present an evaluation of this module in retrieval of Mandarin spoken documents from the TDT corpus using English text queries. Virga and Khudanpur (2003) reported 8.3% absolute accuracy drops when converting from Pinyin to Chinese character. Virga and Khudanpur (2003) and Kuo et al (2005) adopted the noisy channel modeling framework. Technologies developed for SMTare borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003). The proposed transliteration framework obtained significant improvements over a strong baseline transliteration approach similar to AbdulJaleel and Larkey (2003) and Virga and Khudanpur (2003). This result was comparable to other state-of-the-art statistical name transliteration systems (Virga and Khudanpur, 2003).  Virga and Khudanpur (2003) model this scoring function using a separate translation and language model, that is, s (e, f)= Pr (f |e) Pr (e). Past studies on phoneme-based E2C have reported their adverse effects (e.g. Virga and Khudanpur, 2003). Virga and Khudanpur (2003) reported 8.3% absolute accuracy drops when converting from Pinyin to Chinese characters, due to homophone confusion. In CLIR or multilingual corpus alignment (Virga and Khudanpur, 2003), N-best results will be very helpful to increase chances of correct hits. The reference data are extracted from Table 1 and 3 of (Virga and Khudanpur 2003).
A Uniform Approach to Analogies Synonyms Antonyms and Associations Recognizing analogies, synonyms, anto nyms, and associations appear to be fourdistinct tasks, requiring distinct NLP al gorithms. In the past, the four tasks have been treated independently, using a widevariety of algorithms. These four seman tic classes, however, are a tiny sample of the full range of semantic phenomena, andwe cannot afford to create ad hoc algo rithms for each semantic phenomenon; weneed to seek a unified approach. We propose to subsume a broad range of phenom ena under analogies. To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations. We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonymquestions, ESL synonym-antonym questions, and similar-associated-both ques tions from cognitive psychology. Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. An alternative embedding is that used by Turney (2008) in his PairClass system (see Section 6). Turney (2008) has recently proposed a simpler SVM-based algorithm for analogical classification called PairClass. Turney (2008) argues that many NLP tasks can be formulated in terms of analogical reasoning, and he applies his PairClass algorithm to a number of problems including SAT verbal analogy tests, synonym/antonym classification and distinction between semantically similar and semantically associated words. Finally, (Turney, 2008) proposes a supervised machine learning approach for discovering synonyms, antonyms, analogies and associations. For that purpose, feature vectors are based on frequencies of patterns and classified by a SVM. In particular, (Turney, 2008) tackled the problem of classifying different lexical information such as synonymy, antonymy, hypernymy and association by using context words. Turney (2008) proposed a supervised method to solve word analogy questions that require identifying synonyms, antonyms, hypernyms, and other lexical-semantic relations between word pairs. Turney (2008) recently advocated the need for a uniform approach to corpus-based semantic tasks. Such tasks will require an extension of the current framework of Turney (2008) beyond evidence from the direct co-occurrence of target word pairs. Turney (2008) presents a general approach for classifying word pairs into semantic relations by extracting the strings occurring between the two words of a pair (up to three words in-between, up to one word on either side) and using a frequency-based selection process to select sub-patterns where words from the extracted context pattern may have been replaced by a wild card. Building on a recent proposal in this direction by Turney (2008), we propose a generic method of this sort, and we test it on a set of unrelated tasks, reporting good performance across the board with very little task-specific tweaking. Turney (2008) is the first, to the best of our knowledge, to raise the issue of a unified approach. We adopt a similar approach to the one used in Turney (2008) and consider each question as a separate bi nary classification problem with one positive training instance and 5 unknown pairs. The algorithm proposed by Turney (2008) is labeled as Turney-PairClass. This type of similarity is reminiscent of relational analogies investigated in Turney (2008). Turney (2008) proposes a unified approach to handling analogies, synonyms, antonyms and associations by transforming the last three cases into cases of analogy.
Forest-based Translation Rule Extraction examples (partial) target tree-to-tree Ding and Palmer (2005) Translation rule extraction is a fundamental problem in machine translation, especially for syntax-based that need parse trees from either or both sides of the bitext. The current dominant practice only uses 1-best trees, which adversely affects the rule set quality due to parsing errors. So we propose a novel approach which extracts rules a forest compactly encodes exponentially many parses. Experiments show that this method improves translation quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30best parses. When combined with our previous work on forest-based decoding, it achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero by 0.7 points. Forest-based rule extractor (Mi and Huang 2008) is used with a pruning thresh old p=3.  This classification is inspired by and extends the Table 1 in (Mi and Huang, 2008).  Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang,2008), word lattices instead of 1-best segmentations (Dyer et al, 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al, 2009). The GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). We follow Mi and Huang (2008) to assign a fractional count to each well-formed structure. While Mi and Huang (2008) and we both use forests for rule extraction, there remain two major differences. Firstly, Mi and Huang (2008) use a packed forest, while we use a dependency forest. Secondly, the GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. Instead, it does top-down recursive matching from each node one-by-one with each translation rule in the rule set (Mi and Huang 2008). Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest. Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the nth best tree and the 1st best tree. Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string. As we know, the traditional tree-to-string rules can be easily extracted from ? using the algorithm of Mi and Huang (2008). Mi and Huang (2008) extend the tree-based rule extraction. Employ the forest-based tree rule extraction algorithm (Mi and Huang, 2008) to extract our rules from the non-complete forest. Then we can easily extract our rules from the CF using the tree rule extraction algorithm (Mi and Huang, 2008). Finally, to calculate rule feature probabilities for our model, we need to calculate the fractional counts (it is a kind of probability defined in Mi and Huang, 2008) of each translation rule in a parse forest.
