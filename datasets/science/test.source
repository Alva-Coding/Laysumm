Generation Of Word Graphs In Statistical Machine Translation graphs: An efficient interface between continous speech recognition and language understanding. International Conference on Acoustics, and Signal Processing, 2, pages 119-122, Minneapolis, MN, April. Stefan Ortmanns, Hermann Ney, and Xavier Aubert. 1997. A word graph algorithm for large vocabcontinuous speech recognition. and Language, January. Christoph Tillmann and Hermann Ney. 2000. Word re-ordering and DP-based search in statistical matranslation. In '00: The 18th Int. The generation of word graphs for a bottom-top search with the IBM constraints is described in (Ueffing et al, 2002). This is sometimes referred to as a word graph (Ueffing et al, 2002), although in our case the segmented phrase table also produces tokens that correspond to morphemes. A word graph is a weighted directed acyclic graph, in which each node represents a partial translation hypothesis and each edge is labelled with a word of the target sentence and is weighted according to the scores given by an SMT model (see (Ueffing et al, 2002) for more details). The decoder involves generating a phrase lattice (Ueffing et al, 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice. The idea of n-best list extraction from a word graph for SMT was presented in (Ueffing et al, 2002). Details regarding n-best list generation from decoder output can be found in (Ueffing et al, 2002). The machine translation system is a graph based decoder (Ueffing et al, 2002). The consistent N-best phrase alignment can be obtained by using A* search as described in (Ueffing et al, 2002). We implemented our own decoder based on the algorithm described in (Ueffing et al, 2002). The only publication, we are aware of, is (Ueffing et al, 2002).  The important point is that we compare the full path scores and not only partial scores as, for instance, in the beam pruning method in (Ueffing et al., 2002). (Ueffing et al, 2002) and (Mohri and Riley, 2002) both present an algorithm based on the same idea: use a modified A* algorithm with an optimal rest cost estimation. The algorithm in (Ueffing et al, 2002) has two disadvantages: it does not care about duplicates and the rest cost computation is suboptimal as the described algorithm has an exponential worst-case complexity. We will use the well-known graph word error rate (GWER), see also (Ueffing et al,2002). A lattice (Ueffing et al, 2002) can be viewed as a special hyper graph, in which the maximum arity is one. The objective measures used were the BLEU score (Papineni et al, 2001), the NIST score (Dod ding ton, 2002) and Multi-reference Word Error Rate (mWER) (Ueffing et al, 2002). In this paper we explore a different strategy to perform MBR decoding over Translation Lattices (Ueffing et al, 2002) that compactly encode a huge number of translation alternatives relative to an N -best list. For search, we use a dynamic programming beam-search algorithm to explore a subset of all possible translations (Och et al, 1999) and extract. best candidate translations using A* search (Ueffing et al, 2002). For a description on how to generate lattices, see (Ueffing et al, 2002).
Domain Adaptation via Pseudo In-Domain Data Selection We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical the in-domain data, we call them These subcorpora – 1% the size of the original – can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus. Performance is further improved when we use these domain-adapted models in combination with a true in-domain model. The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining inand general-domain systems during decoding. In Moore and Lewis (2010), the authors compare several approaches to selecting data for LM and Axelrod et al (2011) extend their ideas and apply them to MT. Axelrod et al (2011) improved the perplexity based approach and proposed bilingual cross entropy difference as a ranking function with in- and general-domain language models.  This would empirically provide more accurate lexical probabilities, and thus better match the translation task at hand (Axelrod et al., 2011). Axelrod et al (2011) proposed a bilingual cross-entropy difference to select data from parallel corpus for domain adaptation which captures the contextual information slightly, and outperformed monolingual cross-entropy difference (Moore and Lewis, 2010), which first shows the advantage of bilingual data selection. We experimented with two different types of sub sampling techniques - Model 1, similar to that used by Schwenk et al (2011), and modified Moore Lewis (Axelrod et al, 2011) - for the language pairs es-en, en-es, fr-en and en-fr.  
A fully Bayesian approach to unsupervised part-of-speech tagging Unsupervised learning of linguistic structure is a difficult problem. A common approach is to define a generative model and maximize the probability of the hidden structure given the observed data. Typically, this is done using maximum-likelihood estimation (MLE) of the model parameters. We show using part-of-speech tagging that a fully Bayesian approach can greatly improve performance. Rather than estimating a single set of parameters, the Bayesian approach integrates over all possible parameter values. This difference ensures that the learned structure will have high probability over a range of possible parameters, and permits the use of priors favoring the sparse distributions that are typical of natural language. Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE. We find improvements both when training from data alone, and using a tagging dictionary. The application of MCMC techniques, including Gibbs sampling, to HMM inference problems is relatively well-known: see Besag (2004) for a tutorial introduction and Goldwater and Griffiths (2007) for an application of Gibbs sampling to HMM inference for semi 300 supervised and unsupervised POS tagging. This model differs from other non-recursive computational models of grammar induction (e.g. Goldwater and Griffiths, 2007) since it is not based on Hidden Markov Models. Other work aims to do truly unsupervised learning of taggers, such as Goldwater and Griffiths (2007) and Johnson (2007). Following Goldwater and Griffiths (2007), the transition, emission and coupling parameters are governed by Dirichlet priors, and a token-level collapsed Gibbs sampler is used for inference. In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on "diluted dictionaries" in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ). Variation of Information is an information-theoretic measure summing mutual information between tags and states, proposed by (Meila?, 2002), and first used for Unsupervised Part of Speech in (Goldwater and Griffiths, 2007). Each integral can be computed in closed form using multinomial-Dirichlet conjugacy (and by making the above-mentioned simplifying assumption that all other tags were generated separately by their transition and super lingual 87 parameters), just as in the monolingual Bayesian HMM of (Goldwater and Griffiths, 2007). Results are given for a monolingual Bayesian HMM (Goldwater and Griffiths, 2007), a bilingual model (Snyder et al, 2008), and the multilingual model presented here. We implemented the Bayesian HMM model of Goldwater and Griffiths (2007) (BHMM1) as our mono lingual baseline. Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al, 1992) and the incorporation of morphological features (Clark, 2003). The first local Gibbs sampler (PYP-HMM) updates a single tag assignment at a time, in a similar fashion to Goldwater and Griffiths (2007). In the non-hierarchical model (Goldwater and Griffiths, 2007) these dependencies can easily be accounted for by incrementing customer counts when such a dependence occurs.  We start with a well-known application of Bayesian inference, unsupervised POS tagging (Goldwater and Griffiths, 2007). First, if we want to find the MAP derivations of the training strings, then following Goldwater and Griffiths (2007), we can use annealing: raise the probabilities in the sampling distribution to the 1T power, where T is a temperature parameter, decrease T to wards zero, and take a single sample. We use the same modeling approach as as Goldwater and Griffiths (2007), using a probabilistic tag bigram model in conjunction with a tag-to-word model. In the remainder of this paper, we first present the Bayesian Hidden Markov Model (BHMM; Goldwater and Griffiths (2007)) that is used as the baseline model of category acquisition, as well as our extensions to the model, which incorporate sentence type information. We use a Bayesian HMM (Goldwater and Griffiths, 2007) as our baseline model. In the experiments reported below, we use the Gibbs sampler described by (Goldwater and Griffiths, 2007) for the BHMM, and modify it as necessary for our own extended models. Slight corrections need to be made to Equation 5 to account for sampling tags from the middle of the sequence rather than from the end; these are given in (Goldwater and Griffiths, 2007) and are followed in our own samplers.
CORPUS-BASED STAT IST ICAL  SENSE RESOLUTION Claudia Leacock, 1 Geoffrey Towell, 2 Ellen Voorhees 2 1Princeton University, Cognitive Science Laboratory, Princeton, New Jersey 08542 2Siemens Corporate Research, Inc., Princeton, New Jersey 08540 ABSTRACT The three corpus-based statistical sense resolution methods studied here attempt o infer the correct sense of a polyse- mous word by using knowledge about patterns of word co- occurrences.  Some clusters of studies have used common test suites, most notably the 2094-word line data of Leacock et al (1993), shared by Lehman (1994) and Mooney (1996) and evaluated on the system of Gale, Church and Yarowsky (1992). For this preliminary experiment, we used the "line" dataset of a word sense disambiguation task (Leacock et al, 1993). Further details can be found in the Leacock et al (1993).  Others have used common test suites such as the 2094-word line data of Leacock et al (1993). In particular, we use subsets of the line data (Leacock et al, 1993) and the English lexical sample data from the SENSEVAL-2 comparative exercise among word sense disambiguation systems (Edmonds and Cotton, 2001).
More  accurate  tes ts  Ibr the  s ta t i s t i ca l  s ign i f i cance  of resu l t d i f ferences  * Alexander  Yeh Mitre Corp. 202 Burli l lgl;on Rd.  A statistical treatment of Question 1 is presented by Yeh (2000).  The others are always statistically significant at p? 0.005, calculated with approximate randomization (Yeh, 2000). advantage of test data with a label distribution similar to the training set. The best F score and WER are obtained using the combination of all three dictionaries, HB-dict+GHM-dict+S-dict. Furthermore, the difference between the results using HB-dict+GHM-dict+S-dict and HB-dict+GHMdict is statistically significant (p 0.01), based on the computationally-intensive Monte Carlo method of Yeh (2000), demonstrating the contribution of Sdict. We think that our tag-word pairs are effective because they are selected by a linguistically meaning Significance was measured with the randomized significance test described in (Yeh, 2000). The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Undirected graphical models, in particular Condi 3 We measured significance of all the experiments in this paper with the randomized significance test (Yeh, 2000). All statistical significance testing is done via the stratified shuffling test (Yeh, 2000). Table 5 shows a 0.4 percent F-score improvement over the baseline for that section, which is statistically significant at p &lt; 0.001, using the stratified shuffling test (Yeh, 2000). To determine the statistical significance of the difference in the performance of the systems we analyzed, we use the model described in (Yeh, 2000) as implemented in (Pado, 2006). It is also interesting to note that the best result on the validation set for estimation We measured significance of all the experiments in this paper with the randomized significance test (Yeh, 2000). All statistical significance tests in these experiments use the computationally-intensive randomisation test described in Yeh (2000), with p < 0.05. A stratified shuffling-based randomization test (Yeh, 2000) shows that the differences are statistically significant (p<0.05). We use the approximate randomization test (Yeh, 2000) for statistical significance of the difference between the basic sequential CRF and our second round CRF, which has additional features derived from the output of the first CRF. Statistical significance is tested using randomised estimation (Yeh, 2000) with p &lt; 0.05. We test the statistical significance of all above-baseline results using randomised estimation (p &lt; 0.05; Yeh (2000)), and present all such results in bold in our results tables. A wide range of exact and asymptotic tests as well as computationally intensive randomisation tests (Yeh, 2000) are available and add to the confusion about an appropriate choice. The aim of this paper is to formulate a statistical model that interprets the evaluation of ranking methods as a random experiment. Standard deviations for F scores were estimated with bootstrap re sampling (Yeh, 2000). We calculate statistical significance of performance differences using stratified shuffling (Yeh, 2000). However, when combined with word similarity features, context support improves over the basic method at a level of statistical significance (based on randomised estimation, p &lt; 0.05: Yeh (2000)), indicating the complementarity of the two methods, especially on Twitter data. Differences between models are tested for significance using stratified shuffling (Yeh, 2000), using a standard number of 10000 iterations.
Aligning Sentences In Bilingual Corpora Using Lexical Information In this paper, we describe a fast algorithm for aligning sentences with their translations in a bilingual corpus. Existing efficient algorithms ignore word identities and only consider sentence (Brown al., Gale and Church, 1991). Our algorithm constructs a simple statistical word-to-word translation model on the fly during alignment. We find the alignment that maximizes the probability of generating the corpus with this translation model. We have achieved an error rate of approximately 0.4% on Canadian Hansard data, which is a significant improvement over previous results. The algorithm is language independent.  As shown in Chen (1993) and Wu (1994), however, sentence length based methods suffer when the texts to be aligned contain small passages, or the languages involved share few cognates. Other translation-based alignments (Kay, 199l; Chen, 1993) show the difficulty in determining the word correspondence and are very complex. As Chen (1993) points out, dynamic programming is particularly susceptible to deletions occurring in one of the two languages. Simard and Plamondon (Simard and Plamondon, 1998) used a composite method in which the first pass does alignment at the level of characters asin (Church, 1993) (itself based on cognate matching) and the second pass uses IBM Model-1, following Chen (Chen, 1993). (The problem of aligning parallel corpora at the sentence level has been addressed by Meyers (1998b) Chen (1993) and others and is beyond the scope of this paper). By adopting the IBM model 1, (Chen 1993) used word translation probabilities, which he showed gives better accuracy than the sentence length based method. (Simard and Plamondon 1996) used a two-pass approach, where the first pass performs length-based alignment at the character level as in (Gale and Church 1993) and the second pass uses IBM Model 1, following (Chen 1993). Each type of the web page sentence aligner makes use of three conventional sentence alignment models, one is the length based model following (Brown 1991), one is the lexicon based model following (Chen 1993), and the other one is the hybrid model presented in (Zhao 2002).  Other examples of lexical methods are Warwick et al (1989), Mayers et al (1998), Chen (1993) and Haruno and Yamazaki (1996). Chen (1993) constructs a simple word-to-word translation model and then takes the alignment that maximizes the likelihood of generating the corpus given the translation model. Another case study of sentence alignment that we will present here is that of Chen (1993). EMACC finds only 1:1 textual units alignments in its present form but a document pair can be easily extended to a document bead following the example from (Chen, 1993). Asshown in (Chen, 1993) the accuracy of sentence length based methods decreases drastically when aligning texts containing small deletions or free translations. The approaches by (Chen, 1993), (Ceausu et al, 2006) or (Fattah et al, 2007) need manually aligned pairs of sentences in order to train the used alignment models. There are basically three kinds of approaches on sentence alignment: the length-based approach (Gale and Church 1991 and Brown et al 1991), the lexical approach (Roscheisen 1993), and the combination of them (Chen 1993, Wu 1994 and Langlais 1998, etc.). Chen (1993) combines the length-based approach and lexicon-based approach together.
A Bootstrapping Method For Learning Semantic Lexicons Using Extraction Pattern Contexts This paper describes a bootstrapping algorithm called Basilisk that learns highquality semantic lexicons for multiple categories. Basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category. Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts. We evaluate Basilisk on six semantic categories. The semantic lexicons produced by Basilisk have higher precision than those produced by previous techniques, with several categories showing substantial improvement. However, (Thelen and Riloff, 2002) did not focus on the issue of convergence, and on leveraging negative categories to achieve or improve convergence. Given the endless amount of data we have at our disposal, many efforts have focused on mining knowledge from structured or unstructured text, including ground facts (Etzioni et al, 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). (Thelen and Riloff, 2002) address this problem by learning multiple semantic categories simultaneously, relying on the often unrealistic assumption that a word cannot belong to more than one semantic category. Previous approaches to context pattern induction were described by Riloff and Jones (1999), Agichtein and Gravano (2000), Thelen and Riloff (2002), Lin et al (2003), and Etzioni et al (2005), among others. Over the years, researchers have successfully shown how to build ground facts (Etzioni et al., 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). Next, we applied the Basilisk bootstrapping algorithm (Thelen and Riloff, 2002) to learn PPVs. Similar approaches are used among others in (Thelen and Riloff, 2002) for learning semantic lexicons, in (Collins and Singer, 1999) for named entity recognition, and in (Fagni and Sebastiani, 2007) for hierarchical text categorization. Thelen and Riloff (2002)'s bootstrapping method iteratively performs feature selection and word selection for each class. In Section 5, we report experiments using syntactic features shown to be useful by the above studies, and compare performance with Thelen and Riloff (2002)'s bootstrapping method. We use the following algorithms as baseline: EM, co training, and co-EM, as established techniques for learning from unlabeled data in general; the bootstrapping method proposed by Thelen and Riloff (2002) (hereafter, TRB and TR) as a state-of-the-art bootstrapping method designed for semantic lexicon construction. The extraction of large sets of candidate facts opens the possibility of fast-growth iterative extraction, as opposed to the de-facto strategy of conservatively growing the seed set by as few as five items (Thelen and Riloff, 2002) after each iteration. Pattern generalization is disabled, and the ranking of patterns and facts follows strictly the criteria and scoring functions from (Thelen and Riloff, 2002), which are also used in slightly different form in (Lita and Carbonell, 2004) and (Agichtein and Gravano,2000). As a more realistic compromise over overly-cautious acquisition, the baseline run retains as many of the top candidate facts as the size of the current seed, whereas (Thelen and Riloff, 2002) only add the top five candidate facts to the seed set after each iteration. Multi-category algorithms out perform MLB (Thelen and Riloff, 2002), and we focus on these algorithms in our experiments. In BASILISK (Thelen and Riloff, 2002), candidate terms are ranked highly if they have strong evidence for a category and little or no evidence for other categories. To improve the seeds, the frequency of the potential seeds in the corpora is often considered, on the assumption that highly frequent seeds are better (Thelen and Riloff, 2002). This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). Seed-based supervision is closely related to the idea of seeding in the bootstrapping literature for learning semantic lexicons (Thelen and Riloff, 2002). Thelen and Riloff (2002) use a bootstrapping algorithm to learn semantic lexicons of nouns for six semantic categories, one of which is EVENTS. Multi-category bootstrapping algorithms, such as Basilisk (Thelen and Riloff, 2002), NOMEN (Yangarber et al, 2002), and WMEB (McIntosh and Curran, 2008), aim to reduce semantic drift by extracting multiple semantic categories simultaneously.
Phrase Dependency Parsing for Opinion Mining In this paper, we present a novel approach for mining opinions from product reviews, where it converts opinion mining task to identify product features, expressions of opinions and relations between them. By taking advantage of the observation that a lot of product features are phrases, a concept of phrase dependency parsing is introduced, which extends traditional dependency parsing to phrase level. This concept is then implemented for extracting relations between product features and expressions of opinions. Experimental evaluations show that the mining task can benefit from phrase dependency parsing. A previous work that used structured kernels in Sentiment Analysis is the approach of Wu et al (2009). The results showed by Wu et al (2009) suggest that tree kernels on dependency trees are a good approach but we also plan to employ string kernels on this task. We compared our aspect identification approach against two baselines: a) the method proposed by Hu and Liu (2004), which was based on the association rule mining, and b) the method proposed by Wu et al (2009), which was based on a dependency parser. Afterwards, Wu et al (2009) utilized the dependency parser to extract the noun phrases and verb phrases from the reviews as the aspect candidates. Phrase dependency grammars have recently been used by Wu et al (2009) for feature extraction for opinion mining. For a monolingual task, Wu et al (2009) used a shallow parser to convert lexical dependencies from a dependency parser into phrase dependencies. We compared our approach against two state-of-the art methods: a) the method proposed by Hu and Liu (2004), which is based on the association rule mining, and b) the method proposed by Wu et al (2009), which is based on the dependency parser. For example, Wu et al (2009) identified aspects based on the features explored by dependency parser. A wide spectrum of tasks have been studied under review mining, ranging from coarse-grained document-level polarity classification (Pang et al,2002) to fine-grained extraction of opinion expressions and their targets (Wu et al, 2009).  For MaxEnt training, we tried three labeled data sets: one that was taken from the restaurant data set and manually annotated by us, and two from the annotated data set used in (Wu et al., 2009). To test this hypothesis, we tried two quite different training data sets, one from the cell phone domain and the other from the DVD player domain, both used in (Wu et al, 2009). Wu et al (2009) proposed a phrase level dependency parsing for mining aspects and features of products.  In supervised approaches, various kinds of models were applied, such as HMM (Jin and Ho, 2009), SVM (Wu et al, 2009) and CRFs (Li et al, 2010). They regarded it as a sequence labeling task, where several classical models were used, such as CRFs (Li et al, 2010) and SVM (Wu et al, 2009).
Learning Dependency-Based Compositional Semantics Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two stansemantic parsing benchmarks our system obtains the highest published accuracies, despite requiring no annotated logical forms. Clarke et al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning. In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance. Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments. More recently, Liang et al (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins. GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011).  Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world. It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011). One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al 2011) or even a binary correct/incorrect signal (Clarke et al 2010). For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form. Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers. Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011). DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1). In (Liang et al, 2011) DCS trees are learned from QA pairs and database entries. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable. Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available. WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011). For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model. DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011). See Liang et al (2011) for work in representing lambda calculus expressions with trees.
A Maximum-Entropy-Inspired Parser We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.  As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000). Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniak parser (Charniak, 2000), also trained on the Switch board tree bank. We then use Charniak's parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus. We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997). After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF structure for each sentence in every article. The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003). In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000). We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus. The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 % unlabelled and 84 % labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank. The paper is organized as follows. Blaheta and Charniak (2000) presented the first method for assigning Penn functional tags to constituents identified by a parser. As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically. Thus, it is informative to compare our results with those reported in (Blaheta and Charniak, 2000) for this same task. First, because of the different definition of a correctly identified constituent in the parser's output, we apply our method to a greater portion of all labels produced by the parser (95% vs. 89% reported in (Blaheta and Charniak, 2000)). TiMBL performed well on tasks where structured, more complicated and task-specific statistical models have been used previously (Blaheta and Charniak, 2000). The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents. Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)). The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions. Note that the dependency figures of Dienes lag behind even the parsed results for Johnson's model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%. Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size.
Biographies Bollywood Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains. For evaluation we selected two domain adaptation datasets :spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al, 2007). Blitzer et al (2007) used structural correspondence learning to train a classifier on source data with new features induced from target unlabeled data. For these experiments we use the Multi-Domain Sentiment Dataset, introduced by Blitzer et al (2007). We split the labeled data 80/20 following Blitzer et al (2007) (cf. Chen et al. (2012) train on all "labeled" data and test on the "unlabeled" data). Empirical work on NLP domain shifts has focused on the former. For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. We continue in this tradition by making two assumptions about our setting. We selected three data sets commonly used in domain adaptation: spam (Jiang and Zhai, 2007), ACE 2005 named entity recognition (Jiang and Zhai, 2007), and sentiment (Blitzer et al, 2007). In a batch setting this corresponds to learning a linear classifier to discriminate the domains, and Blitzer et al (2007) showed correlations with the error from domain adaptation. However, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexica from a resource-rich language to the target language (Banea et al., 2008; Wan, 2008), or a domain that is "similar" enough to the target domain (Blitzer et al, 2007). We use five sentiment classification datasets, including the widely-used movie review dataset [MOV] (Pang et al, 2002) as well as four datasets containing reviews of four different types of products from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (Blitzer et al, 2007). We show that this constraint is effective on the sentiment classification task (Pang et al, 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al, 2007) without the need to engineer auxiliary tasks. We evaluate our approach on adapting sentiment classifiers on 4 domains: books, DVDs, electronics and kitchen appliances (Blitzer et al, 2007). Both the achieved error reduction and the absolute score match the results reported in (Blitzer et al, 2007) for the best version of the SCL method (SCL-MI, 36%), suggesting that our approach is a viable alternative to SCL. To evaluate our approach, we consider the same dataset as the one used to evaluate the SCL method (Blitzer et al, 2007). To evaluate our approach, we consider the same dataset as the one used to evaluate the SCLmethod (Blitzer et al, 2007). Instead of using the full set of bigram and unigram counts as features (Blitzer et al, 2007), we use a frequency cut-off of 30 to remove infrequent ngrams. In Table 1, we also compare the results of our method with the results of the best version of the SCL method (SCL-MI) reported in Blitzer et al (2007). Second, the absolute scores achieved in Blitzer et al (2007) are slightly worse than those demonstrated in our experiments both for supervised and semi-supervised methods. Our approach results in competitive domain adaptation performance on the sentiment classification task, rivalling that of the state-of-the-art SCLmethod (Blitzer et al, 2007). On a separate note, previous research has explicitly studied sentiment analysis as an application of transfer learning (Blitzer et al, 2007). We use the dataset from (Blitzer et al, 2007) for sentiment classification.
EM Can Find Pretty Good HMM POS-Taggers (When Given a Good Start) We address the task of unsupervised POS tagging. We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries. We present a family of algorithms to compute effective estimations We test the method on the task of full morphological disambiguation in Hebrew achieving an error reduction of 25% over a strong uniform distribution baseline. We also test the same method on the standard WSJ unsupervised POS tagging task and obtain results competitive with recent state-ofthe-art methods, while using simple and efficient learning methods. Thus, an orthogonal line of research can involve inducing classes for words which are more general than single categories, i.e., something akin to ambiguity classes (see, e.g., the discussion of ambiguity class guessers in Goldberg et al, 2008). This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008).  All of the methods to which we compare except Goldberg et al (2008) focus on learning and modeling techniques, while our method only addresses initialization. See Goldberg et al (2008) for details. Goldberg et al (2008) provide a linguistically-informed starting point for EM to achieve 91.4% accuracy. Goldberg et al (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. (Goldberg et al, 2008) extend the work of (Adler and Elhadad, 2006) by using an EM algorithm, and achieve an accuracy of 88% for full morphological analysis, but again, this does not include lemma IDs.  Despite the fact that HMM-EM has a poor reputation in POS literature (Goldberg et al, 2008) has shown that with good initialization together with some language specific features and language dependent constraints HMM-EM achieves 91.4% accuracy. Traditionally, such unsupervised EM-trained HMM taggers are thought to be inaccurate, but (Goldberg et al, 2008) showed that by feeding thee M process with sufficiently good initial probabilities, accurate taggers (> 91% accuracy) can be learned for both English and Hebrew, based on a (possibly incomplete) lexicon and large amount of raw text. As another baseline, we experimented with a pipeline system in which the input text is automatically segmented and tagged using a state-of-the-art HMMpos-tagger (Goldberg et al, 2008). Goldberg et al 2008 note that fixing noisy dictionaries by hand is actually quite feasible, and suggest that effort should focus on exploiting human knowledge rather than just algorithmic improvements. The probability of a lemma was defined as the sum of probabilities for all morphological analyses containing the lemma, using a morpho-lexical context-independent probabilities approximation (Goldberg et al, 2008). The latter had two important characteristics: The first is flexibility This tagger allows adapting the estimates of the prior (context-independent) probability of each morphological analysis in an unsupervised manner, from an unlabeled corpus of the target domain (Goldberg et al, 2008). The importance is underscored succinctly by Goldberg et al (2008). EM-HMM tagger provided with good initial conditions (Goldberg et al, 2008). Goldberg et al (2008) depart from the Bayesian framework and show how EM can be used to learn good POS taggers for Hebrew and English, when provided with good initial conditions. The system achieves a better accuracy than the 88.6% from Smith and Eisner (2005), and even surpasses the 91.4% achieved by Goldberg et al (2008) without using any additional linguistic constraints or manual cleaning of the dictionary. Their models are trained on the entire Penn Treebank data (instead of using only the 24,115-token test data), and so are the tagging models used by Goldberg et al (2008).
Multilingual Subjectivity Analysis Using Machine Translation Although research in other languages is increasing, much of the work in subjectivity analysis has been applied to English data, mainly due to the large body of electronic resources and tools that are available for this language. In this paper, we propose and evaluate methods that can be employed to transfer a repository of subjectivity resources across languages. Specifically, we attempt to leverage on the resources available for English and, by employing machine translation, generate resources for subjectivity analysis in other languages. Through comparative evaluations on two different languages (Romanian and Spanish), we show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language. Banea et al (2008) demonstrate that machine translation can perform quite well when extending the subjectivity analysis to multilingual environment, which makes it inspiring to replicate their work on lexicon-based sentiment analysis. Banea et al (2008) use machine translation for multilingual sentiment analysis. Another approach in obtaining subjectivity lexicons for other languages than English was explored in Banea et al (2008b).  Similar to the approach proposed in (Banea et al, 2008), Wan's method also uses machine translation to produced a labeled Chinese review corpus from the available labeled English review data. A primary question is whether such lexicons improve performance over a translate-to-English strategy (Banea et al, 2008). Banea et al (2008) use machine translation for multilingual sentiment analysis. Mihalcea et al, (2007) and Banea et al, (2008) used machine translation technique to leverage English resources for analysis in Romanian and Spanish languages. Another approach in obtaining subjectivity lexicons for other languages than English was explored by Banea et al (Banea et al,2008b).  Similarly, (Banea et al, 2008) propose a method based on machine translation to generate parallel texts, followed by a cross-lingual projection of subjectivity labels, which are used to train subjectivity annotation tools for Romanian and Spanish.  Figure 1: Examples of sentiments in multilingual text Banea et al (2008) have attributed the variations in the difficulty level of subjectivity learning to the differences in language construction. translating a source language training corpus into target language and creating a corpus based system in target language (Banea et al, 2008). Banea et al (2008) proposed several approaches for cross lingual subjectivity analysis by directly applying the translations of opinion corpus in source language to train the opinion classifier on target language. Various experiments of the first strategy are performed in (Banea et al,2008) for the subjective analysis task and an average 65 F1 score was reported. As stressed in some work (Banea et al., 2008), researchers have shown that in sentiment analysis, an approach in two steps is often beneficial, in which we first distinguish objective from subjective texts, and then classify subjective texts depending on their polarity (Kim and Hovy, 2006).  The above two methods have been used in (Banea et al, 2008) for Romanian subjectivity analysis, but the experimental results are not very promising. Although subjectivity tends to be preserved across languages - see the manual study in (Mihalcea et al, 2007), (Banea et al, 2008) hypothesize that subjectivity is expressed differently in various languages due to lexicalization, formal versus informal markers, etc.
A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes We propose a new hierarchical Bayesian model of natural languages. Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages. We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney, one of the best smoothmethods for language models. Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney. Smoothing techniques, such as Kneser-Ney and Witten-Bell back off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off to lower order distributions, thus providing an estimate for the probability of these unseen events. Similar considerations apply to other sophisticated language modeling techniques like Pitman-Yor processes (Teh, 2006), recurrent neural networks (Mikolov et al, 2010) and FLMs in their general, more powerful form.  The hyper parameters of our model are updated with the auxiliary variable technique (Teh, 2006a). Growing discounts of this sort were previously suggested by the model of Teh (2006). Although the PYP has no known analytical form, we can marginalise out the GX's and reason about individual rules directly using the process described by Teh (2006).    Conventional smoothing techniques, such as Kneser Ney and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events. We develop a Bayesian approach using a Pitman Yor process prior, which is capable of modellinga diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). Recent work has applied Bayesian non-parametric models to anaphora resolution (Haghighi and Klein, 2007), lexical acquisition (Goldwater, 2007) and language modeling (Teh, 2006) with good results. Nonparametric Bayesian modeling has recently become very popular in natural language processing (NLP), mostly because of its ability to provide priors that are especially suitable for tasks in NLP (Teh, 2006). While the Dirichlet process is simply the Pitman Yor process with d= 0, it has been shown that the discount parameter allows for more effective modeling of the long-tailed distributions that are often found in natural language (Teh, 2006). The Chinese Restaurant Process representation of Pt (Teh, 2006) lends itself to a natural and easily implementable solution to this problem. Similarly, if the seating dynamics are constrained such that each dish is only served once (tw= 1 for any w), a single discount level is affected, establishing direct correspondence to original interpolated Kneser-Ney smoothing (Teh, 2006). The hierarchical PYP (hPYP; Teh (2006)) is an extension of the PYP in which the base distribution G0 is itself a PYP distribution. There is not space for a complete treatment of the hPYP and the particulars of inference; we refer the interested reader to Teh (2006).  Among these, smoothing techniques, such as Good-Turing, Witten-Bell and Kneser-Ney smoothing schemes (see (Chen and Goodman, 1996) for an empirical overview and (Teh, 2006) for a Bayesian interpretation) are used to compute estimates for the probability of unseen events, which are needed to achieve state-of-the-art performance in large-scale settings.
Extracting Opinions Opinion Holders And Topics Expressed In Online News Media Text This paper presents a method for identifying an opinion with its holder and topic, given a sentence from online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using data from FrameNet. We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles. For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet. Our experimental results show that our system performs significantly better than the baseline. Kim and Hovy (2006) integrated verb information from FrameNet and incorporated it into semantic role labeling. Kim and Hovy (2006) and Bethard et al (2005) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al, 2003) for both opinion holder and opinion target extraction. Due to data sparseness, Kim and Hovy (2006) expand FrameNet data by using an unsupervised clustering algorithm. Kim and Hovy (2006 ) used a FrameNet-based semantic role labeler to deter mine holder and topic of opinions. Kim and Hovy (2006 )useda FrameNet-based semantic role labeler to deter mine holder and topic of opinions. Kim and Hovy (2006 ) used a FrameNet-based semantic role labeler to deter mine holder and topic of opinions. Kim and Hovy (2006) use structural features of the language to identify opinion entities. A more linguistically motivated approach was taken by Kim and Hovy (2006) through identifying opinion holders and targets with semantic role labeling. Also Framenet (Ruppenhofer et al (2010)) is used as a resource in opinion mining and sentiment analysis (Kim and Hovy (2006)).  Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words. Some work such as (Kim and Hovy, 2006) has explored the connection to role labeling. Kim and Hovy (2006) use a machine translation system and subsequently use a subjectivity analysis system that was developed for English.  A notable exception is the work of Kim and Hovy (2006). Opinion holder is usually an entity that holds an opinion, and opinion target is what the opinion is about (Kim and Hovy, 2006). Kim and Hovy (2006) proposed to map the semantic frames of FrameNet into opinion holder and target for only adjectives and verbs. This means that we are actually searching for all triples{ source, target, opinion} in this sentence (Kim and Hovy, 2006) and throughout each document in the corpus. Bethard et al (2004) and Kim and Hovy (2006) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al 2003). Bethard et al (2004) use this resource to acquire labeled training data while in (Kim and Hovy, 2006) FrameNet is used within a rule-based classifier mapping frame-elements of frames to opinion holders.
Accurate Unlexicalized Parsing Figure 3: Size and devset performance of the cumulatively annotated models, starting with the markovized baseline. The two columns show the change in the baseline for each annotation introduced, both cumulatively and for each single annotation applied to the baseline in isolation. history models similar in intent to those described in Ron et al. (1994). For variable horizontal histories, we did not split intermediate states below 10 occurrences of a symbol. For example, if the symbol too rare, we would colit to For vertical histories, we used a cutoff which included both frequency and mutual information between the history and the expansions (this was not appropriate for the horizontal because unreliable at such low counts). Figure 2 shows parsing accuracies as well as the number of symbols in each markovization. These symbol counts include all the intermediate states which represent partially completed constituents. The general trend is that, in the absence of further annotation, more vertical annotation is better – even exhaustive grandparent annotation. This is not true for horizontal markovization, where the variableorder second-order model was superior. The best has an 79.74, already a substantial improvement over the baseline. In the remaining sections, we discuss other annotations which increasingly split the symbol space. Since we expressly do not smooth the grammar, not all splits are guaranteed to be beneficial, and not all sets of useful splits are guaranteed to co-exist well. particular, while markovization is good on its own, it has a large number of states and does not tolerate further splitting well. Therefore, base all further exploration on the ROOT S&quot;ROOT 4: An error which can be resolved with the (incorrect baseline parse shown). grammar. Although it does not necessarily jump out of the grid at first glance, this point represents the best compromise between a compact grammar and useful markov histories. 3 External vs. Internal Annotation The two major previous annotation strategies, parent annotation and head lexicalization, can be seen as instances of external and internal annotation, respectively. Parent annotation lets us indicate an important feature of the external environment of a node which influences the internal expansion of that node. On the other hand, lexicalization is a (radical) method of marking a distinctive aspect of the otherwise hidden internal contents of a node which influence the external distribution. Both kinds of annotation can be useful. To identify split states, we suffixes of the form mark internal content and mark external features. To illustrate the difference, consider unary productions. In the raw grammar, there are many unaries, and once any major category is constructed over a span, most others become constructible as well using unary chains (see Klein and Manning (2001) for discussion). Such chains are rare in real treebank trees: unary rewrites only appear in very specific for example of verbs where an empty, controlled subject. Figure 4 shows an erroneous output of the parser, using the baseline markovized grammar. Intuitively, there are several reasons this parse should be ruled out, but is that the lower which is intended prifor of communication verbs, is not a unary rewrite position (such complements usually have subjects). It would therefore be natural to annotate the trees so as to confine unary productions to the contexts in which they are actually ap- We tried two annotations. First, . NP&quot;S VP&quot;S NP&quot;VP VBD , NN SˆVP . VPˆS QP , NP&quot;VP $ CD CD VBG 444.9 million including , CONJP NP&quot;NP NP&quot;NP $ QP JJ NN , RB RB IN net interest down slightly from CD $ 450.7 million was Revenue $ CD (with a any nonterminal node which has only one child. In isolation, this resulted in an absolute gain of 0.55% (see figure 3). The same sentence, parsed using only the baseline and is parsed correctly, because the in the incorrect parse ends with an very low marked nodes had no siblings with It was similar to solo benefit (0.01% worse), but provided far less marginal benefit on top of later features (none at all on top of our top models), and was One restricted place where external unary annotation was very useful, however, was at the preterminal level, where internal annotation was meaningless. One distributionally salient tag conflation in the Penn treebank is the identification of demonstraand regular determiners based on whether they were only captured this distinction. The same external unary annotation was even more efwhen applied to adverbs disfor example, well Beyond these cases, unary tag marking was detrimen- The 78.86%. 4 Tag Splitting The idea that part-of-speech tags are not fine-grained enough to abstract away from specific-word behaviour is a cornerstone of lexicalization. The for example, showed that the determiners which occur alone are usefully distinguished from those which occur with other nomimaterial. This marks the with a single bit about their immediate external context: whether there are sisters. Given the success of parent annotation for nonterminals, it makes sense to parent antags, as well In fact, as figure 3 shows, exhaustively marking all preterminals with their parent category was the most effective single annotation we tried. Why should this be useful? Most tags have a canonical category. For example, occur under (only 234 of 70855 do not, mostly mistakes). However, when a tag that when we show such trees, we generally only show one annotation on top of the baseline at a time. Moreover, we do not explicitly show the binarization implicit by the horizontal markovization. two are not equivalent even given infinite data. 5: An error resolved with the (of the (a) the incorrect baseline parse and (b) the correct resolves this error. somewhat regularly occurs in a non-canonical position, its distribution is usually distinct. For example, most common adverbs directly under and Under they are and Under and and so on. substantially, to 80.62%. In addition to the adverb case, the Penn tag set conflates various grammatical distinctions that are commonly made in traditional and generative grammar, and from which a parser could hope to get useful information. For example, subordinating conas, complementizers prepositions in, all get the tag of these distinctions are captured by conjunctions occur under under but are not (both subordinating conjunctions and complementizers appear Also, there are exclusively nounprepositions predominantly verbones and so on. The annotation a linguistically motivated 6-way split the and brought the total to 81.19%. Figure 5 shows an example error in the baseline is equally well fixed by either In this case, the more common nominal of preferred unless the is annoto allow prefer We also got value from three other annotations which subcategorized tags for specific lexemes. we split off auxiliary verbs with the which appends all forms all forms of More miconjunction tags to indicate is an extended uniform version of the partial auxiliary annotation of Charniak (1997), wherein all auxiliaries are as a added to gerund auxiliaries and VP&quot;S VP&quot;S TO VP&quot;VP TO&quot;VP VP&quot;VP to VB PP&quot;VP to VB&quot;VP SBAR&quot;VP see NP&quot;PP see IN&quot;SBAR S&quot;SBAR IN (a) (b) NNS NN if VP&quot;S VBZ&quot;VP works works NN&quot;NP advertising advertising or not they were the strings &, each of which have distinctly different distributions from other conjunctions. Finally, we gave the percent sign (%) its own tag, in line with the dollar sign ($) already having its own. Together these three anbrought the 81.81%. 5 What is an Unlexicalized Grammar? Around this point, we must address exactly what we by an To the extent that go about subcategorizing many of them might come to represent a single word. One might thus feel that the approach of this paper is to walk down a slippery slope, and that we are merely arguing degrees. However, we believe that there is a fundamental qualitative distinction, grounded in linguistic practice, between what we see as permitted an unlexicalized against what one finds hopes to exploit in lexicalized The division rests on the traditional distinction between words closed-class words) and open class or lexical words). It is standard practice in linguistics, dating back decades, to annotate phrasal nodes with important functiondistinctions, for example to have a a whereas content words are not part of grammatical structure, and one would not have sperules or constraints for an for example. We follow this approach in our model: various closed classes are subcategorized to better represent important distinctions, and important features commonly expressed by function words are annotated phrasal nodes (such as whether a finite, or a participle, or an infinitive clause). However, no use is made of lexical class words, to provide either or bilexical At any rate, we have kept ourselves honest by estimating our models exclusively by maximum likelihood estimation over our subcategorized grammar, without any form of interpolation or shrinkage to unsubcategorized categories (although we do rules, as explained above). This effecshould be noted that we started with four tags in the Penn tagset that rewrite as a single word: and some of the punctuation tags, which rewrite as barely more. To the extent that we subcategorize tags, there will be more such cases, but many of them already exist in other tag sets. For instance, many tag sets, such as the Brown and tagsets give a separate sets of tags to each form of verbal auxiliaries and most of which rewrite as only a single word (and any corresponding contractions). 6: An error resolved with the (a) incorrect baseline parse and (b) the correct tively means that the subcategories that we break off must themselves be very frequent in the language. In such a framework, if we try to annotate categories with any detailed lexical information, many sentences either entirely fail to parse, or have only extremely weird parses. The resulting battle against sparsity means that we can only afford to make a few distinctions which have major distributional impact. Even with the individual-lexeme annotations in this section, the grammar still has only 9255 states compared to the 7619 of the baseline model. 6 Annotations Already in the Treebank At this point, one might wonder as to the wisdom of stripping off all treebank functional tags, only to heuristically add other such markings back in to the grammar. By and large, the treebank out-of-the tags, such as have negative utility. Recall that the raw treebank gramwith no annotation or markovization, had an of 72.62% on our development set. With the functional annotation left in, this drops to 71.49%. The v markovization baseline of 77.77% dropped even further, all the way to 72.87%, when these annotations were included. Nonetheless, some distinctions present in the raw trees were valuable. For example, an an could be either a temporal a For the annotation we retained the on and, furthermore, propathe tag down to the tag of the head of the This is illustrated in figure 6, which also shows an of its utility, clarifying that last night is not a plausible compound and facilitating the othunusual high attachment of the smaller the cumulative 82.25%. Note that this technique of pushing the functional tags down to preterminals might be useful more generfor example, locative expand roughly the VPˆVP VPˆVP VB to NPˆVP appear NNˆTMP NPˆNP PPˆNP JJ PPˆNP NPˆNP appear NPˆVP VB NP-TMPˆVP to IN NNS last night JJ times three NNP NN on CD on times three last CNN night NPˆPP NNP CNN NPˆPP IN NNS CD (a) (b) ROOT Distance SˆROOT SˆROOT 7: An error resolved with the (a) incorrect baseline parse and (b) the correct way as all other (usually as but do tend to have different prepositions below A second kind of information in the original trees is the presence of empty elements. Following (1999), the annotation nodes which have an empty subject (i.e., raising and constructions). This brought 82.28%. 7 Head Annotation The notion that the head word of a constituent can affect its behavior is a useful one. However, often the head tag is as good (or better) an indicator of how constituent will We found several head annotations to be particularly effective. First, poshave a very different distribution than – in particular, are only used in the treebank when the leftmost child is possessive (as opposed to other imaginable uses like for York which is left flat). To address this, all possessive This brought total 83.06%. Second, the is very overloaded in the Penn treebank, most severely in that there is no distinction between finite and in- An example of the damage this conflation can do is given in figure 7, where one needs to capture the fact that present-tense verbs do not take bare infinitive To allow the finite/non-finite distinction, and other verb distinctions, all with their head tag, merging all finite forms to a sintag In particular, this also accomplished This was extremely bringing the cumulative 85.72%, 2.66% absolute improvement (more than its solo improvement over the baseline). is part of the explanation of why (Charniak, 2000) finds that early generation of head tags as in (Collins, 1999) is so beneficial. The rest of the benefit is presumably in the availability of the tags for smoothing purposes. Error analysis at this point suggested that many remaining errors were attachment level and conjunction scope. While these kinds of errors are undoubtedly profitable targets for lexical preference, most attachment mistakes were overly high attachments, indicating that the overall right-branching tendency of English was not being captured. Indeed, this tenis a difficult trend to capture in a because often the high and low attachments involve the very same rules. Even if not, attachment height is modeled by a it is somehow explicitly encoded into category labels. More complex parsing models have indirectly overcome this by modeling distance (rather than height). distance is difficult to encode in a – marking nodes with the size of their yields masmultiplies the state Therefore, we wish to find indirect indicators that distinguish high from low ones. In the case of two a with the question of whether the a second modifier of the leftmost should attach lower, inside the first the important distinction is usually that the lower site is a base Collins (1999) captures this by introducing the notion of a base in any dominates only preterminals is with a Further, if an not have non-base it is given one with a unary production. This was helpful, but substantially less than marking base the unary, whose presence actually erased a useful indicator – base are more frequent in subject position than object position, for example. In isolation, the Collins method actually hurt the base- (absolute cost to 0.37%), while skipping the unary insertion added an absolute 0.73% to the and brought the cumulative 86.04%. the case of attachment of a an eiabove or inside a relative clause, the high is distinct from the low one in that the already modified one contains a verb (and the low one may be base well). This is a partial explanation of the utility of verbal distance in Collins (1999). To inability to encode distance naturally in a naive somewhat ironic. In the heart of any the fundamental table entry or chart item is a label over a span, for exan position 0 to position 5. The concrete use of a grammar rule is to take two adjacent span-marked labels and them (for example and into Yet, only the labels are used to score the combination. “ DT “ NPˆS VBZ VPˆVP VPˆS ! . ” ” “ “ NPˆS DT NPˆVP ” . ! ” VPˆS-VBF VBZ (a) (b) NPˆVP buying This is This NN NN panic buying LP LR Exact CB 0 CB Magerman (1995) 84.9 84.6 1.26 56.6 Collins (1996) 86.3 85.8 1.14 59.9 this paper 86.9 85.7 86.3 30.9 1.10 60.3 Charniak (1997) 87.4 87.5 1.00 62.1 Collins (1999) 88.7 88.6 0.90 67.1 Figure 8: Results of the final model on the test set (section 23). this, all nodes which any verbal node with a This the cumulative 86.91%. We also tried marking nodes which dominated prepositions and/or conjunctions, but these features did not help the cumulative hill-climb. The final distance/depth feature we used was an explicit attempt to model depth, rather than use distance and linear intervention as a proxy. With we marked all which contained their right periphery (i.e., as a rightmost descendant). This captured some further attachment trends, and brought us to a final develop- 87.04%. 9 Final Results We took the final model and used it to parse section 23 of the treebank. Figure 8 shows the re- The test set 86.32% for words, already higher than early lexicalized models, though of course lower than the state-of-the-art parsers. 10 Conclusion The advantages of unlexicalized grammars are clear enough – easy to estimate, easy to parse with, and timeand space-efficient. However, the dismal performance of basic unannotated unlexicalized grammars has generally rendered those advantages irrelevant. Here, we have shown that, surprisingly, the maximum-likelihood estimate of a compact unlexiparse on par with early lexicalized parsers. We do not want to argue that lexical selection is not a worthwhile component of a state-ofthe-art parser – certain attachments, at least, require it – though perhaps its necessity has been overstated. Rather, we have shown ways to improve parsing, some easier than lexicalization, and others of which are orthogonal to it, and could presumably be used to benefit lexicalized parsers as well. We fully parsed the training and testing data using the Stanford Parser of (Klein and Manning, 2003) operating on the TnT part-of-speech tagging. At present, the Stanford Parser (Klein and Manning, 2003) is used.  Duringthe NER extraction, we also employ phrase analysis based on our phrase utility extraction method using Standford dependency parser ((Klein and Manning, 2003)). The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task. We follow a standard procedure to extract statements, as similarly adopted by Nakashole et al (2012), using Stanford CoreNLP (Klein and Manning, 2003) to lemmatize and parse sentences. We parsed each sentence using the Stanford Parser (Klein and Manning, 2003) and used heuristics to identify cases where the main verb is transitive, where the subject is a nominalization (e.g. running), or whether the sentence is passive. We use the Stanford Parser (Klein and Manning, 2003b) for all experiments. As the grammar becomes sparser, there are limited opportunities for the lexical dependencies to correct the output of the PCFG grammar under the factored parsing model of Klein and Manning (2003b). We parsed the documents into typed dependencies with the Stanford Parser (Klein and Manning, 2003). To obtain dependency structures, we apply the Stanford parser (Klein and Manning, 2003) on the target side of the training material.    To relieve the negative effect of SRL errors, we get the multiple SRL results by providing the SRL system with 3-best parse trees of Berkeley parser (Petrov and Klein, 2007), 1 best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003).  Klein and Manning presented an unlexicalized PCFG parser that eliminated all the lexicalized parameters (Klein and Manning, 2003). In STAN-ANNOTATION, we annotate thetreebank symbols with annotations from the Stanford parser (Klein and Manning, 2003). For the following ensemble experiments we make use of both (Charniak and Johnson, 2005) and Stanford's (Klein and Manning, 2003) constituent parsers. 
Towards History-Based Grammars: Using Richer Models For Probabilistic Parsing While a different order for these predictions is possible, we only experimented with this one. Parameter Estimation We only have built a decision tree to the rule probability component (3) of the model. For the mowe are using with the usual interpolation smoothing for the other four components of the model. We have assigned bit strings to the syntactic and semantic categories and to the rules manually. Our intention is that bit strings differing in the least significant bit positions correspond to categories of non-terminals or rules that are similar. We also have assigned bitstrings for the words in the vocabulary (the lexical heads) using automatic clustering algorithms using the bigram mutual information clustering algorithm (see (5)). Given the bitsting of a history, we then designed a decision tree for modeling the probability that a rule will be used for rewriting a node in the parse tree. Since the grammar produces parses which may be more detailed than the Treebank, the decision tree was built using a training set constructed in the following manner. Using the grammar with the P-CFG model we determined the most likely parse that is consistent with the Treebank and considered the resulting sentence-tree pair as an event. Note that the grammar parse will also provide the lexical head structure of the parse. Then, we extracted using leftmost derivation order tuples of a history (truncated to the definition of a history in the HBG model) and the corresponding rule used in expanding a node. Using the resulting data set we built a decision tree by classifying histories to locally minimize the entropy of the rule template. With a training set of about 9000 sentencetree pairs, we had about 240,000 tuples and we grew a tree with about 40,000 nodes. This required 18 hours on a 25 MIPS RISC-based machine and the resulting decision tree was nearly 100 megabytes. Immediate vs. Functional Parents model employs two types of parents, the and the The a list Figure 3: Sample representation of &quot;with a list&quot; in HBG model. R: PP1 Syn: PP H1: list with R: NBAR4 Syn: NP Sem: Data H1: list H2: a R: N1 Syn: N Sem: Data H1: list H2: * 35 immediate parent is the constituent that immediately dominates the constituent being predicted. If the immediate parent of a constituent has a different syntactic type from that of the constituent, then the immediate parent is also the functional parent; otherwise, the functional parent is the functional parent of the immediate parent. The distinction between functional parents and immediate parents arises primarily to cope with unit productions. When unit productions of the form XP2 ---> XP1 occur, the immediate parent of XP1 is XP2. But, in general, the constituent XP2 does not contain enough useful information for ambiguity resolution. In particular, when considering only immediate parents, unit rules such as NP2 —■ NP1 prevent the probabilistic model from allowing the NP1 constituent to interact with the VP rule which is the functional parent of NP1. When the two parents are identical as it often happens, the duplicate information will be ignored. However, when they differ, the decision tree will select that parental context which best resolves ambiguities. Figure 3 shows an example of the representation of a history in HBG for the prepositional phrase &quot;with a list.&quot; In this example, the immediate parent of the Ni node is the NBAR4 node and the functional parent of Ni is the PP1 node. Results We compared the performance of HBG to the &quot;broad-coverage&quot; probabilistic context-free gram- P-CFG. The of the grammar is 90% on test sentences of 7 to 17 words. The of P-CFG is 60% on the same test corpus of 760 sentences used in our experiments. On the same test sentences, the HBG model has a of 75%. This is a reduction of 37% in error rate. Accuracy P-CFG 59.8% HBG 74.6% Error Reduction 36.8% Figure 4: Parsing accuracy: P-CFG vs. HBG In developing HBG, we experimented with similar models of varying complexity. One discovery made during this experimentation is that models which incorporated more context than HBG performed slightly worse than HBG. This suggests that the current training corpus may not contain enough sentences to estimate richer models. Based on the results of these experiments, it appears likely that significantly increasing the size of the training corpus should result in a corresponding improvement in the accuracy of HBG and richer HBG-like models. To check the value of the above detailed history, we tried the simpler model: 1. 2. 3. p(Syn p(Sem ISyn, p(R ISyn, Sem, This model corresponds to a P-CFG with NTs that are the crude syntax and semantic categories with the lexical heads. The in this case was 66%, a small improvement over the P-CFG model indicating the value of using more context from the derivation tree. Conclusions The success of the HBG model encourages future development of general history-based grammars as a more promising approach than the usual P-CFG. More experimentation is needed with a larger Treebank than was used in this study and with different aspects of the derivation history. In addition, this paper illustrates a new approach to grammar development where the parsing problem is divided (and hopefully conquered) into two subproblems: one of grammar coverage for the grammarian to address and the other of statistical modeling to increase the probability of picking the correct parse of a sentence. They train a statistical parser on trees with only semantic labels on the nodes; however, they do not integrate syntactic and semantic parsing. History-based models of parsing were first introduced in (Black et al, 1993). In recent years, re-ranking techniques have been successfully used in statistical parsers to rerank the output of history-based models (Black et al, 1993). The major alternative to PCFG-based approaches are so-called history-based parsers (Black et al, 1993). Instead, the dependency tree is built stepwise and the decision about what step to take next (e.g. which dependency to insert) can be based on information about, in theory all, previous steps and their results (in the context of generative probabilistic parsing, Black et al (1993) call this the history).
NLTK: The Natural Language Toolkit NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset. If evaluated against the requirements for teaching environments discussed in (Loper and Bird, 2002), GATE covers them all quite well. However, other such modules, e.g., those from NLTK (Loper and Bird, 2002), can be used for such assignments. We use the Punkt sentence splitter from NLTK (Loper and Bird, 2002) to perform both sentence and word segmentation on each text chunk. NLTK, the Natural Language Toolkit, is a suite of Python modules providing many NLP data types, processing tasks, corpus samples and readers, together with animated algorithms, tutorials, and problem sets (Loper and Bird, 2002). For English and German documents in all experiments, we removed stop words (Loper and Bird, 2002), stemmed words (Porter and Boulton, 1970), and created a vocabulary of the most frequent 5000 words per language (this vocabulary limit was mostly done to ensure that the dictionary-based bridge was of manageable size). We have not yet used the Natural Language Toolkit (Loper and Bird, 2002) (see Section 3.1) in this course. Finally, all texts were lemmatized using Porter's stemmer (1980) for English and Snowballstemmers for other languages using an implementation provided by the NLTK (Loper and Bird, 2002). We strip unnecessary HTML tags and Wiki templates with mwlib5 and split sentences with NLTK (Loper and Bird, 2002). Some popular options include the NLTK (Loper and Bird, 2002), CSLU (Cole, 1999), Trindi (Larsson and Traum, 2000) and Regulus (Rayner et al, 2003) toolkits. We use a simple path distance similarity measure, as implemented in NLTK (Loper and Bird, 2002). Our word pairs are lemmatized using the Wordnet based lemmatizer of NLTK (Loper and Bird, 2002). The Natural Language Toolkit, or NLTK, was developed to give a broad range of students access to the core knowledge and skills of NLP (Loper and Bird, 2002). Tokenization ,lemmatization, and stop word removal was performed using the Natural Language Toolkit (Loper and Bird, 2002). Systems like NLTK (Loper and Bird, 2002) and Gate (Cunningham, 2002) do not offer functionality for Lexical Resource Management. To identify content words, we used the NLTK-Lite tagger to assign a part of speech to each word (Loper and Bird, 2002). For the NL processing, the Natural Language Toolkit (NL Toolkit or NLTK), developed at the University of Pennsylvania by Loper and Bird (2002), and available for download from Source Forge at http: //nltk.sourceforge.net/ was used.  In the end I decided to require the students to learn python because I wanted to use NLTK, the Natural Language Toolkit (Loper and Bird, 2002).  The Natural Language Toolkit (NLTK) was developed in conjunction with a computational linguistics course at the University of Pennsylvania in 2001 (Loper and Bird, 2002).
Manual And Automatic Evaluation Of Machine Translation Between European Languages Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy . 0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006). We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English. For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference. The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006). For the bitext-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and German-English (De) (Koehn and Monz, 2006). Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator. For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001). We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3). We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006).  We use the same method described in (Koehn and Monz, 2006) to perform the significance test. We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006). The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002). Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006). We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006). A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006). described in (Koehn and Monz, 2006). For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006). The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008).
Anaphora For Everyone: Pronominal Anaphora Resolution Without A Parser We present an algorithm for anaphora res- olutkm which is a modified and extended version of that developed by (Lappin and Leass,/994). In contrast to that work, our al- gorithm does not require in-depth, full, syn.. tactic parsing of text. Instead, with minimal compromise in output quality, the modifica- tions enable the resolution process to work from tile output of a part of speech tag- ge~; enriched only with annotations of gram- matica\] functkm of lexical items in the in- put text stream. Evaluation of the results of our in-tplementation demonstrates that ac- curate anaphora resolution can be realized within natural anguage processing fl'ame- works which do not--~,)r cannot- employ ro- bust and rcqiable parsing components. But (Kennedy and Boguraev, 1996a) show that the Lappin and Leass algorithm still provides good results (75%) even without complete parse. They suggest also (Kennedy and Boguraev, 1996b) that anaphora resolution is part of the discourse referents resolution. This module is based partly on the system described by Kennedy and Boguraev 1996, with the various weighting factors based on theirs, but designed so that the weights can be trained given appropriate data. In addition, Dagan and Itai (1991) undertook additional pre-editing such as the removal of sentences for which the parser failed to produce a reasonable parse, cases where the antecedent was not an NP etc.; Kennedy and Boguraev (1996) manually removed 30 occurrences of pleonastic pronouns (which could not be recognised by their pleonastic recogniser) as well as 6 occurrences of it which referred to a VP or prepositional constituent. Kennedy and Boguraev (1996), for example, need additional information about whether a certain discourse referent is embedded or not, plus a pointer to the COREF class associated to the referent, while Mitkov's approach needs a score associated to each noun phrase. RAP (Kennedy and Boguraev, 1996), Baldwin's pronoun resolution method (Baldwin, 1997) and Mitkov's knowledge-poor pronoun resolution approach (Mitkov, 1998b). As expected, the results reported in Table 1 do not match the original results published by Kennedy and Boguraev (1996), Baldwin (1997) and Mitkov (1998b) where the algorithms were tested on different data, employed different pre-processing tools, resorted to different degrees of manual intervention and thus provided no common ground for any reliable comparison. In Kennedy and Boguraev (1996) it is proposed an algorithm for anaphor resolution which is a modified and extended version of that developed by Lappin and Leass (1994). Our framework will allow us a similar approach to that of Kennedy and Boguraev (1996), but we will automatically get syntactic information from partial parsing. The grammar in Figure 4 will only parse coordinated prepositional phrases (pp), coordinated noun phrases (np), pronouns (p), conjunctions (conj) and verbs (verb) in whatever order that they appear in the text and it will allow us to work in a similar way that the algorithm mentioned in Kennedy and Boguraev (1996). improved the accuracy (83%) in pronominal references to the work of Kennedy and Boguraev (1996) (75%), but we have also improved that approach since we automatically. It presupposes fine grained methods for the identification of cohesive ties 76 between (sentence) units in a text; describing the computational basis for developing such methods is outside of the scope of this paper (howeveb see (Kennedy and Boguraev, 1996), (Fellbaum, 1999), (Kelleb 1994)), as is the complete framework for lexical cohesion analysis we have developed. More recently, Kennedy and Boguraev (1996) propose an algorithm for anaphora resolution that is actually a modified and extended version of the one developed by Lappin and Leass (1994). L&L demonstrated with a system called RAP that a (manually-tuned) weight-based scheme for integrating pronoun interpretation preferences can achieve high performance on real data, in their case, 86% accuracy on a corpus of computer training manuals. Dagan et al (1995) then developed a post processor based on predicate-argument statistics that was used to override RAP's decision when it failed to express a clear preference between two or more antecedents, which resulted in a modest rise in per Kennedy and Boguraev (1996, henceforth, K&B) adapted L&L's algorithm to rely on far less syntactic analysis (noun phrase identification and rudimentary grammatical role marking), with performance in the 75% range on mixed genres. Kennedy and Boguraev (1996) then report a 75% accuracy for an algorithm that approximates Lappin and Leass's with more robust and coarse-grained syntactic input. Part of the pronoun resolution performance here enables a preliminary comparison with the results reported in (1) Lappin and Leass (1994) and (2) Kennedy and Boguraev (1996). preferred candidate antecedents Kennedy and Boguraev (1996) approximated the above components with a poorer syntactic input, which is an output of a part-of-speech tagger with grammatical function information, plus NPs recognized by finite-state patterns and NPs &apos; adjunct and subordination contexts recognized by heuristics. For example, we anticipate that sentences with quotation marks will be problematic, as other researchers have observed that quoted text requires special handling for pronoun resolution (Kennedy and Boguraev, 1996).  
Long-Distance Dependency Resolution In Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations This paper shows how finite approximations of long distance dependency (LDD) resolution can be obtained automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks. We extract LFG subcategorisation frames and paths linking LDDreentrancies from f-structures generated automati cally for the Penn-II treebank trees and use them in an LDD resolution algorithm to parse new text.Unlike (Collins, 1999; Johnson, 2002), in our ap proach resolution of LDDs is done at f-structure (attribute-value structure representations of basicpredicate-argument or dependency structure) with out empty productions, traces and coindexation in CFG parse trees. Currently our best automaticallyinduced grammars achieve 80.97% f-score for f structures parsing section 23 of the WSJ part of the Penn-II treebank and evaluating against the DCU1051 and 80.24% against the PARC 700 Depen dency Bank (King et al, 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al, 2004).  For this experiment, we choose the C&C parser (Clark and Curran, 2003) for CCG, Enju parser (Miyao and Tsujii, 2008) for HPSG and pipeline automatic annotator (Cahill et al, 2004) with Charniak parser for LFG.  The research presented in this paper forms part of an ongoing effort to develop methods to induce wide-coverage multilingual Lexical Functional Grammar (LFG) (Bresnan, 2001) resources from tree banks by means of automatically associating LFG f-structure information with constituency trees produced by probabilistic parsers (Cahill et al, 2004). The f-structure annotation algorithm used for inducing LFG resources from the Penn-II treebank for English (Cahill et al, 2004) uses configurational, categorial, function tag and trace information. A methodology for automatically obtaining LFG f-structures from trees output by probabilistic parsers trained on the Penn-II tree bank has been described by Cahill et al (2004). Some properties of Spanish and the encoding of syntactic information in the Cast3LB treebank make it non-trivial to apply the method of automatically mapping c-structures to f-structures used by Cahill et al (2004), which assigns grammatical functions to tree nodes based on their phrasal category, the category of the mother node and their position relative to the local head. Cahill et al (2004), in their presentation of LFG parsing resources, distinguish 32 types of dependencies, divided into two major groups: a group of predicate-only dependencies and non predicate dependencies. The translation and reference files are analyzed by a tree bank-based, probabilistic Lexical-Functional Grammar (LFG) parser (Cahill et al, 2004), which produces a set of dependency triples for each input. Cahill et al (2004) presents Penn-II Treebank based LFG parsing resources. In this paper, we use the parser developed by Cahill et al (2004), which automatically annotates input text with c-structure trees and f-structure dependencies, reaching high precision and recall rates. We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations (Cahill et al, 2004) automatically extracted from tree banks, maximising the probability of a tree given an f-structure. In this paper we present a novel PCFG-based architecture for probabilistic generation based onwide-coverage, robust Lexical Functional Grammar (LFG) approximations automatically extracted from tree banks (Cahill et al, 2004). Cahill et al (2004) present two parsing architectures: the pipeline and the integrated parsing architecture. The generation architecture presented here builds on the integrated parsing architecture resources of Cahill et al (2004).   This conditioning effectively turns the f-structure annotated PCFGs of Cahill et al (2004) into probabilistic generation grammars. Our back off uses the built-in lexical macros of the automatic f-structure annotation algorithm of Cahill et al (2004) to identify potential part-of-speech categories corresponding to a particular set of features. The feasibility of such post-parse deepening (for a statistical parser) is demonstrated by Cahill et al (2004).
Empirical Lower Bounds On The Complexity Of Translational Equivalence This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts. The study found that the complexity of these patterns in every bitext was higher than suggested in the literature. These findings shed new light on why “syntactic” constraints have not helped to improve statistical translation models, including finitestate phrase-based models, tree-to-string models, and tree-to-tree models. The paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations, even in relatively simple real bitexts in syntactically similar languages with rigid word order. Instructions for replicating our experiments are at Wellington et al (2006), in a more systematic study, find that, of sentences where the tree-to-tree constraint blocks rule extraction, the majority are due to parser errors. Wellington et al. (2006) reports that discontinuities are very useful for translational equivalence analysis using binary branching structures under word alignment and parse tree constraints while they are almost of no use if under word alignment constraints only. In this sense, our model behaves like a phrase based model, less sensitive to discontinuous phrases (Wellington et al, 2006). We propose SG-ITG that follows Wellington et al (2006)'s suggestion to model at most one gap. Grammar rules extracted from large parallel corpora by systems such as Galley et al (2004) can be quite large, and Wellington et al (2006) argue that complex rules are necessary by analyzing the coverage of gold-standard word alignments from different language pairs by various grammars. The methodology in Wellington et al (2006) measures the complexity of word alignment using the number of gaps that are necessary for their synchronous parser which allows discontinuous spans to succeed in parsing. Wellington et al (2006) indicate the necessity of introducing discontinuous spans for synchronous parsing to match up with human-annotated word alignment data. Wellington et al (2006) treat many-to-one word links disjunctively in their synchronous parser. We use the same alignment data for the five language pairs Chinese/English, Romanian/English, Hindi/English, Spanish/English, and French/English (Wellington et al, 2006). Wellington et al (2006) did a similar analysis on the English-English bitext. Translational equivalence is a mathematical relation that holds between linguistic expressions with the same meaning (Wellington et al, 2006). Wellington et al (2006) describes their study of the patterns of translational equivalence exhibited by a variety of bilingual/monolingual bitexts. Fox (2002), Galley et al (2004) and Wellington et al (2006) examine TEM only. Thus, it may not suffer from the issues of non-isomorphic structure alignment and non-syntactic phrase usage heavily (Wellington et al, 2006). (Wellington et al, 2006) argue that these restrictions reduce our ability to model translation equivalence effectively. In Wellington et al (2006), hand-aligned data are used to compare the standard ITG constraints to ITGs that allow gaps. In particular, Wellington et al (2006) find that the coverage of a translation model can increase dramatically when one allows a bilingual phrase to stretch out over three rather than two continuous substrings. Wellington et al (2006) argue for the necessity of discontinuous spans (i.e., for a formalism beyond Synchronous CFG) in order for synchronous parsing to cover human-annotated word alignment data under the constraint that rules have a rank of no more than two. We use the same alignment data for the five language pairs Chinese-English, Romanian-English, Hindi-English, Spanish-English, and French-English as Wellington et al (2006). The empirical adequacy of synchronous context-free grammars of rank two (2-SCFGs) (Satta and Peserico, 2005), used in syntax based machine translation systems such as Wu (1997), Zhang et al (2006) and Chiang (2007), in terms of what alignments they induce, has been discussed in Wu (1997) and Wellington et al (2006), but with a one-sided focus on so-called inside-out alignments.
Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition. We first propose a simple yet powerful semi-supervised discriminative model appropriate for handling large scale unlabeled data. Then, we describe experiments performed on widely used test collections, namely, PTB III data, CoNLL’00 and ’03 shared task data for the above three NLP tasks, respectively. We incorporate up to 1G-words (one billion tokens) of unlabeled data, which is the largest amount of unlabeled data ever used for these tasks, to investigate the performance improvement. In addition, our results are superior to the best reported results for all of the above test collections. Recent research indicates that using labeled and unlabeled data in semi-supervised learning (SSL) environment, with an emphasis on graph-based methods, can improve the performance of information extraction from data for tasks such as question classification (Tri et al, 2006), web classification (Liu et al., 2006), relation extraction (Chen et al, 2006), passage-retrieval (Otterbacher et al, 2009), various natural language processing tasks such as part of-speech tagging, and named-entity recognition (Suzuki and Isozaki, 2008), word-sense disambiguation (Niu et al, 2005), etc.  Suzuki and Isozaki (2008) provided evidence that the use of more unlabeled data in semi supervised learning could improve the performance of NLP tasks, such as POS tagging, syntactic chunking, and named entities recognition. We describe an extension of semi supervised structured conditional models (SS-SCMs) to the dependency parsing problem, whose framework is originally proposed in (Suzuki and Isozaki, 2008). Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008). Note that it is possible to iterate the method steps 2 and 3 can be repeated multiple times (Suzuki and Isozaki, 2008) but in our experiments we only performed these steps once. We follow a similar approach to that of (Suzuki and Isozaki, 2008) in partitioning f (x, y), where the k different feature vectors correspond to different feature types or feature templates. This paper has described an extension of the semi-supervised learning approach of (Suzuki and Isozaki, 2008) to the dependency parsing problem. Suzuki and Isozaki (2008) introduce a semi-supervised extension of conditional random fields that combines supervised and unsupervised probability models by so-called MDF parameter estimation, which reduces error on Wall Street Journal (WSJ) standard splits by about 7% relative to their supervised baseline. 22-24 was 4.2%, which is comparable to related work in the literature, e.g. Suzuki and Isozaki (2008) (7%) and Spoustova et al (2009) (4-5%). In comparison, there are 79 templates in (Suzuki and Isozaki, 2008).  Wong and Ng (2007) and Suzuki and Isozaki (2008) are similar in that they run a baseline discriminative classifier on unlabeled data to generate pseudo examples, which are then used to train a different type of classifier for the same problem. Suzuki and Isozaki (2008), on the other hand, used the automatically labeled corpus to train HMMs. Although the method in (Suzuki and Isozaki 2008) is quite general, it is hard to see how it can be applied to the query classification problem. Suzuki and Isozaki (2008) also found a log linear relationship between unlabeled data (up to a billion words) and performance on three NLP tasks. Another approach (Suzuki and Isozaki, 2008) embeds a joint probability model.  Incorporating binary and real features yields a rough approximation of generative models in semi supervised CRFs (Suzuki and Isozaki, 2008). Suzuki and Isozaki (2008) is one such example.
An Efficient Implementation Of A New DOP Model Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank. This paper proposes an integration of the two models which outperforms each of them separately. Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence.  Data-Oriented Parsing (DOP)'s methodology is to calculate weighted derivations, but as noted in (Bod, 2003), it is the highest ranking parse, not derivation, that is desired. Goodman's transform, in combination with a range of heuristics, allowed Bod (2003) to run the DOP model on the Penn Treebank WSJ benchmark and obtain some of the best results obtained with a generative model. Zuidema (2006a) shows that also the estimator (Bod, 2003) uses is biased and inconsistent, and will, even in the limit of infinite data, not correctly identify many possible distributions over trees. Second, we compare against a composed-rule system, which is analogous to the Data Oriented Parsing (DOP) approach in parsing (Bod, 2003). Our best performing model is more accurate than all these previous models except (Bod, 2003). Performance of the latter model on the standard test set achieves 90.1% F-measure on constituents, which is the second best current accuracy level, and only 0.6% below the current best (Bod, 2003).   Similarly, (Bod, 2003) changes the way frequencies fi are counted, with a similar effect.  My approach is closely related to work in statistical parsing known as Data-Oriented Parsing (DOP), an empirically highly successful approach with labeled recall and precision scores on the Penn Tree Bank that are among the best currently obtained (Bod, 2003). We approximated the most probable parse as follows (following (Bod, 2003)). This result is only slightly higher than the highest reported result for this test-set, Bod's (.907) (Bod,2003).  This assumption is in consonance with the principle of simplicity, but there are also empirical reasons for the shortest derivation assumption: in Bod (2003) and Hearne and Way (2006), it is shown that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well. But equally important is the fact that this new DOP* model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003). Of course, it is well-known that a supervised parser's f-score decreases if it is transferred to another domain: for example, the (non-binarized) WSJ-trained DOP model in Bod (2003) decreases from around 91% to 85.5% f score if tested on the Brown corpus. A moderately larger vocabulary version (4215 tag-word pairs) of this parser achieves 89.8% F-measure on section 0, where the best current result on the testing set is 90.7% (Bod, 2003). This subtree probability is redressed by a simple correction factor discussed in Goodman (2003: 136) and Bod (2003).
Better Word Alignments with Supervised ITG Models This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints. We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations. Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives. For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments. This directional model has been shown produce state-of-the art results with this setup (Haghighi et al, 2009). In order to reduce spurious derivations, Wu (1997), Haghighi et al (2009), Liu et al (2010) propose different variations of the grammar. Haghighi et al (2009) give some restrictions on null-aligned word attachment. Four grammars were used to parse these alignments, namely LG (Wu, 1997), HaG (Haghighi et al, 2009), LiuG (Liu et al, 2010) and LGFN (Section 3.3). To further study how spurious ambiguity affects the discriminative learning, we implemented a frame work following Haghighi et al (2009). Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). However, the space of block ITG alignments is expressive enough to include the vast majority of patterns observed in hand annotated parallel corpora (Haghighi et al, 2009). MIRA has been used successfully in MT to estimate both alignment models (Haghighi et al, 2009) and translation models (Chiang et al, 2008). We also include indicator features on lexical templates for the 50 most common words in each language, as in Haghighi et al (2009).  This supervised base line is a reimplementation of the MIRA-trained model of Haghighi et al (2009). This training regimen on this data set has provided state-of-the-art unsupervised results that outperform IBM Model 4 (Haghighi et al., 2009). The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al, 2009). When evaluated on parsing and word alignment, this model significantly improves over independently trained baselines: the monolingual parser of Petrov and Klein (2007) and the discriminative word aligner of Haghighi et al (2009). Although this assumption does limit the space of possible word-level alignments, for the domain we consider (Chinese-English word alignment), the reduced space still contains almost all empirically observed alignments (Haghighi et al, 2009). We begin with the same set of alignment features as Haghighi et al (2009), which are defined only for terminal bi spans. Because of this, given a particular word alignment w, we maximize the marginal probability of the set of derivations A (w) that are consistent with w (Haghighi et al., 2009). We prune our ITG forests using the same basic idea as Haghighi et al (2009), but we employ a technique that allows us to be more aggressive.
Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models This article describes a number of log-linear parsing models for an automatically extracted lexicalized grammar. The models are “full” parsing models in the sense that probabilities are defined for complete parses, rather than for independent events derived by decomposing the parse tree. Discriminative training is used to estimate the models, which requires incorrect parses for each sentence in the training data as well as the correct parse. The lexicalized grammar formalism used is Combinatory Categorial Grammar (CCG), and the grammar is automatically extracted from CCGbank, a CCG version of the Penn Treebank. The combination of discriminative training and an automatically extracted grammar leads to a significant memory requirement (up to 25 GB), which is satisfied using a parallel implementation of the BFGS optimization algorithm running on a Beowulf cluster. Dynamic programming over a packed chart, in combination with the parallel implementation, allows us to solve one of the largest-scale estimation problems in the statistical parsing literature in under three hours. A key component of the parsing system, for both training and testing, is a Maximum Entropy supertagger which assigns CCG lexical categories to words in a sentence. The supertagger makes the discriminative training feasible, and also leads to a highly efficient parser. Surprisingly, given CCG’s “spurious ambiguity,” the parsing speeds are significantly higher than those reported for comparable parsers in the literature. We also extend the existing parsing techniques for CCG by developing a new model and efficient parsing algorithm which exploits all derivations, including CCG’s nonstandard derivations. This model and parsing algorithm, when combined with normal-form constraints, give state-of-the-art accuracy for the recovery of predicate–argument dependencies from CCGbank. The parser is also evaluated on DepBank and compared against the RASP parser, outperforming RASP overall and on the majority of relation types. The evaluation on DepBank raises a number of issues regarding parser evaluation. This article provides a comprehensive blueprint for building a wide-coverage CCG parser. We demonstrate that both accurate and highly efficient parsing is possible with CCG. Clark and Curran (2007) show that both accurate and highly efficient parsing is possible using a CCG. From a parsing perspective, the C & C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrase structure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al, 2009). However, the parsing work by Clark and Curran (2007), and also Hockenmaier (2003) and Fowler and Penn (2010), has only considered chart-parsing. Following Clark and Curran (2007), we assume that each input word has been assigned a POS-tag (from the Penn Treebank tag set) and a set of CCG lexical categories. Clark and Curran (2007) gives a more precise definition. We ran the C & C parser using the normal-form model (we reproduced the numbers reported in Clark and Cur ran (2007)), and copied the results of the hybrid model from Clark and Curran (2007), since the hybrid model is not part of the public release. The numbers for C & C are for the hybrid model, copied from Clark and Curran (2007). The numbers for the normal-form model are evaluated by running the publicly available parser, while those for the hybrid dependency model are from Clark and Curran (2007). Following Hockenmaier (2003), we extract the grammar by reading rule instances directly from the derivations in CCGbank (Hockenmaier and Steedman, 2007), rather than defining the combinatory rule schema manually as in Clark and Curran (2007). When it is reasonable to assume that the input sentence for the grammaticality improvement system is sufficiently fluent, a list of candidate lexical categories can be assigned automatically to each word via super tagging (Clark and Curran, 2007) on the input sequence. The average number of lexical categories per word drops to 1.3 when equals 0.075, which is the value used for parsing newspaper text in Clark and Curran (2007). However, compared to the 93% lexical category accuracy of a CCG parser (Clark and Curran, 2007), which also uses a level of 0.075 for the majority of sentences, the accuracy of our grammaticality improvement system is much lower. We parsed both corpora using the C & C parser (Clark and Curran, 2007) as we employ both GR and POS information in our learning method. We compare the CCG parser of Clark and Curran (2007) with a state-of-the-art PennTreebank (PTB) parser. Examples of this approach include Riezler et al (2002), Miyao and Tsujii (2005), Briscoe and Carroll (2006), and Clark and Curran (2007). The formalism-based parser we use is the CCG parser of Clark and Curran (2007), which is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. The CCG parser has been extensively evaluated elsewhere (Clark and Curran, 2007), and arguably GRs or predicate-argument structures provide a more suitable test set for the CCG parser than PTB phrase-structure trees. Since this short paper reports a small, focused research contribution, we refer readers to Clark and Curran (2007) and Petrov and Klein (2007) for details of the two parsers. The schemas were developed by manual inspection using section 00 of CCGbank and the PTB as a development set, following the oracle methodology of Clark and Curran (2007), in which gold standard derivations from CCGbank are converted to the new representation and compared with the gold standard for that representation. While high quality syntactic parsers are able to efficiently annotate large quantities of English text (Clark and Curran, 2007), existing approaches to query do not work on the same scale.
Formalism-Independent Parser Evaluation with CCG and DepBank A key question facing the parsing community is how to compare parsers which use different grammar formalisms and produce different output. Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance. this paper we evaluate a on DepBank, and demonstrate the difficulties in converting the parser output into Dep- Bank grammatical relations. In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy. The obtains an F-score of 81.9% on labelled dependencies, against an upper of 84.8%. We compare the against the outperformover 5% overall and on the majority of dependency types. The evaluation in this paper was based solely on CCGbank, but we have shown in Clark and Curran (2007) that the CCG parser gives state-of-the-art performance, outperforming the RASP parser (Briscoe et al, 2006) by over 5% on DepBank. CCG and HPSG parsers also favor the dependency based metrics for evaluation (Clark and Curran, 2007b; Miyao and Tsujii, 2008). For example, Clark and Curran (2007) developed a set of mapping rules from the output of a Combinatorial Categorial grammar parser to the Grammatical Relations (GR) (Carroll et al, 1998). Our system used the C & C parser (Clark and Curran, 2007a), which uses the Combinatory Categorial Grammar formalism (CCG, Steedman, 2000). However, cross framework parser evaluation is a difficult problem: previous attempts to evaluate the C & C parser on grammatical relations (Clark and Curran, 2007b) and Penn Treebank-trees (Clark and Curran, 2009) have also produced upper bounds between 80 and 90% F-score. Clark and Curran (2007a) demonstrate the use of techniques like adaptive super tagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C & C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al, 2009). While this is not ideal, we note that previous efforts at cross-parser evaluation have shown that it is a difficult problem (Clark and Curran (2007b) and Clark and Curran (2009)). More detailed discussions of the obstacles to directly comparing syntactic structures include Preiss (2003), Clark and Curran (2007), and most recently Sagae et al (2008). We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii,2008), which have been successfully used in applications (Miyao et al, 2008). EDM Fscores of 90% and 83% over in-domain data compare well with dependency-based scores from other parsers, although a direct comparison is very difficult to do (Clark and Curran, 2007a; Miyao et al,2007). SCF and DR: These more linguistically informed features are constructed based on the grammatical relations generated by the C & C CCG parser (Clark and Curran, 2007). The focus on labeled dependencies also provides a direct link to recent work on dependency-based evaluation (e.g., Clark and Curran, 2007) and dependency parsing (e.g., CoNLL shared tasks 2006, 2007). However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalism independent evaluation of parsers (e.g., Clark and Curran, 2007). A labeled dependency evaluation based on grammatical relations, which links this work to current work on formalism-independent parser evaluation (e.g., Clark and Curran, 2007), shows that the parsing performance for Negra and Tu Ba-D/Z is comparable. Conceptually, this conversion is similar to the conversions from deeper structures to GR reprsentations reported by Clark and Curran (2007) and Miyao et al (2007). The CCG parser we use (Clark and Curran, 2007b) makes use of three levels of representation: one, a POS tag level based on the fairly coarse-grained POS tags in the Penn Treebank; two, a lexical category level based on the more fine-grained CCG lexical categories, which are assigned to words by a CCG super tagger; and three, a hierarchical level consisting of CCG derivations. The CCG parser is described in detail in Clark and Curran (2007b) and so we provide only a brief description. Supertagging was originally developed for Lexicalized Tree Adjoining Grammar (Bangalore and Joshi, 1999), but has been particularly successful for wide-coverage CCG parsing (Clark and Curran, 2007b). The CCG super tagger is not able to assign a single category to each word with extremely high accuracy - hence the need for it to operate as a multi-tagger - but even in multi-tagger mode it dramatically reduces the ambiguity passed through to the parser (Clark and Curran, 2007b). The parser has been evaluated on DepBank (King et al., 2003), using the GR scheme of Briscoe et al. (2006), and it scores 82.4% labelled precision and 81.2% labelled recall overall (Clark and Curran, 2007a).
Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars One of the reasons nonparametric Bayesian inference is attracting attention in computational linguistics is because it provides a principled way of learning the units of generalization together with their probabilities. Adaptor grammars are a framework for defining a variety of hierarchical nonparametric Bayesian models. This paper investigates some of the choices that arise in formulating adaptor grammars and associated inference procedures, and shows that they can have a dramatic impact on performance in an unsupervised word segmentation task. With appropriate adaptor grammars and inference procedures we achieve an 87% word token f-score on the standard Brent version of the Bernstein- Ratner corpus, which is an error reduction of over 35% over the best previously reported results for this corpus. One way to solve the mixing problem is for the sampler to make more global moves, e.g., with table label resampling (Johnson and Goldwater, 2009) or split-merge (Jain and Neal, 2000). Johnson and Goldwater (2009) showed that word segmentation accuracy improves further if the model learns a nested hierarchy of collocations. The starting point and baseline for our extension is the adaptor grammar with syllable structure phonotactic constraints and three levels of collocational structure (5-21), as prior work has found that this yields the highest word segmentation token f-score (Johnson and Goldwater, 2009). Adaptor Grammar model described in section 2.3 and compare it to the baseline grammar with collocations and phonotactics from Johnson and Goldwater (2009). We use the standard hyperparameters values α = 1.0, β = 0.01 and τ = 1.0 and run the sampler for 1000 iterations, but one can use techniques like slice sampling to estimate the hyperparameters (Johnson and Goldwater, 2009). The Dirichlet parameters α are drawn independently from a Γ(1, 1) distribution, and are resampled using slice sampling at frequent intervals throughout the sampling process (Johnson and Goldwater, 2009). Hierarchical Bayes methods have been mainstream in unsupervised word segmentation since the dawn of hierarchical Dirichlet process (Goldwater et al, 2009) and adaptors grammar (Johnsonand Goldwater, 2009). For the runs where adaptation was used we set the initial Pitman-Yor a and b parameters to 0.01 and 10 respectively, then slice sample (Johnson and Goldwater, 2009).  This is the same set-up used by Liang and Klein (2009), Goldwater et al (2006), and Johnson and Goldwater (2009). It is substantially simpler than the non-parametric Bayesian models proposed by Johnson et al (2007), which require sampling procedures to perform inference and achieve an F1 of 87 (Johnson and Goldwater, 2009). While in principle, increasing the number of rejuvenation steps and particles will make this gap smaller and smaller, we believe the existence of the gap to be interesting in its own right, suggesting a general difference in learning behaviour between batch and incremental learners, especially given the similar results in Johnson and Goldwater (2009). Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as morphology analysis, word segmentation (Johnson and Goldwater, 2009), and dependency grammar induction (Cohen et al., 2010), rather than constituent syntax parsing. We follow the experimental setting of Johnson and Goldwater (2009), who present state-of-the-art results for inference with adaptor grammars using Gibbs sampling on a segmentation problem. We use the standard Brent corpus (Brent and Cartwright,1996), which includes 9,790 unsegmented phonemic representations of utterances of child-directed speech from the Bernstein-Ratner (1987) corpus. Johnson and Goldwater (2009) test three grammars for this segmentation task. GUnigram and GSyllable can be found in Johnson and Goldwater (2009). For example, with GUnigram convergence typically takes 40 iterations with variational inference, while Johnson and Goldwater (2009) ran their sampler for 2,000 iterations, for which 1,000 were for burning in. This is a new application of adaptor grammars, which have so far been used in segmentation (Johnson and Goldwater, 2009) and named entity recognition (Elsner et al, 2009). Johnson and Goldwater (2009) have proposed a novel method based on adaptor grammars, whose accuracy surpasses the aforementioned methods by a large margin, when appropriate assumptions are made regarding the structural units of a language. Modeling the corpus using hierarchical grammars that can model the input at varying levels (word collocations, words, syllables, onsets, etc.) provide the learner the most flexibility, allowing the learner to build structure from the individual phonemes and apply distributions at each level of abstraction (Johnson and Goldwater, 2009).
A Word-To-Word Model Of Translational Equivalence Many multilingual NLP applications need to translate words between different languages, but cannot afford the computational expense of inducing or applying a full translation model. For these applications, we have designed a fast algorithm for estimating a partial translation model, which accounts for translational equivalence only at the word level . The model's precision/recall trade-off can be directly controlled via one threshold parameter. This feature makes the model more suitable for applications that are not fully statistical. The model's hidden parameters can be easily conditioned on information extrinsic to the model, providing an easy way to integrate pre-existing knowledge such as partof-speech, dictionaries, word order, etc.. Our model can link word tokens in parallel texts as well as other translation models in the literature. Unlike other translation models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy. All translation models were induced using the method of Melamed (1997). SWAT considers only those translations c that has been linked with w based the Competitive Linking Algorithm (Melamed 1997) and logarithmic likelihood ratio (Dunning 1993). In its basic form the Competitive Linking algorithm (Melamed, 1997) allows for only up to one link per word. The selection order is similar to that in the competitive linking algorithm (Melamed, 1997). The first algorithm is similar to Competitive Linking (Melamed, 1997). In this paper, we describe the key idea behind this model and connect it with the competitive linking algorithm (Melamed, 1997) which was developed for word-to-word alignment. The competitive linking algorithm (CLA) (Melamed, 1997) is a greedy word alignment algorithm. (Melamed,1997) has proposed the Competitive Linking Algorithm for linking the word pairs and a method which calculates the optimized correspondence level of the word pairs by hill climbing. Every English word has either 0 or 1 alignments (Melamed, 1997). The problem, which was described in (Melamed, 1997) in a word-to-word alignment context, is as follows: if e1 is the translation of f1 and f2 has a strong monolingual association with f1, e1 and f2 will also have a strong correlation. EM is then performed by first discovering an initial phrasal alignments using a greedy algorithm similar to the competitive linking algorithm (Melamed, 1997).  This can be seen as a generalization of the one-to-one assumption for word-to-word translation used by Melamed (1997a) and is exploited for the same purpose, i.e. to exclude large numbers of candidate alignments, when good initial alignments have been found. The basic algorithm combines the K-vec approach, described by Fung and Church (1993), with the greedy word-to-word algorithm of Melamed (1997a). Equivalents in Bilingual Corpus When accurate instances are obtained from bilingual corpus, we continue to integrate the statistical word-alignment techniques (Melamed, 1997) and dictionaries to find the translation candidates for each of the two collocates.
Evaluating WordNet-based Measures of Lexical Semantic Relatedness Alexander Budanitsky?  Its applications include word sense disambiguation, text summarization and information retrieval (Budanitsky and Hirst, 2006). In addition, those methods that are based on hierarchical, taxonomically structured resources are generally better suited for measuring semantic similarity than relatedness (Budanitsky and Hirst, 2006). In particular, WordNet based measures are well known to be better suited to measure similarity than relatedness due to its hierarchical, taxonomic structure (Budanitsky and Hirst, 2006). The semantic distance can be got by the usage of lexicon, such as WordNet (Budanitsky and Hirst, 2006). Semantic relatedness can denote every possible relation between two concepts, unlike semantic similarity, which typically denotes only certain hierarchical relations (like hypernymy and synonymy) and is often computed using hierarchical networks like WordNet (Budanitsky and Hirst, 2006). Budanitsky and Hirst (2006) provide an extensive survey of these measures. Budanitsky and Hirst (2006) also point out an important distinction, between relatedness and similarity. Following Budanitsky and Hirst (2006), we consider two frames as similar if they are linked via is-a like relations (e.g. GETTING and COMMERCE BUY), while as related if any relation stands between them (e.g. causation between KILLING and DEATH). We also experiment with the Jiang and Conrath's (Jiang and Conrath, 1997) measure which relies only on the is-a hierarchy, but proved to be the best WordNet-based measure in the task of ranking words (Budanitsky and Hirst, 2006). A direct comparison to the word ranking task, suggests that ranking frames is harder than words, not only for humans (as reported in Section 3.2), but also for machines: Budanitsky and Hirst (2006) show that measures for ranking words get much closer to the human upper-bound than our measures do, confirming that frame relatedness is a fairly complex notion to model. Following Budanitsky and Hirst (2006), we estimate the WordNet sense similarity using the method proposed by Jiang and Conrath (1997). For example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and Budanitsky and Hirst (2006) highlight other problems that stem from the imbalance and sparseness of the corpora. In the experiments by Budanitsky and Hirst (2006), the measure by (Jiang and Conrath, 1997) yields the best results. Budanitsky and Hirst (2006) used a characteristic gap in the standard evaluation dataset by Rubenstein and Goodenough (1965) that separates unrelated from related word pairs.  SCM and SPE capture the two most important parameters of measuring semantic relatedness between terms (Budanitsky and Hirst, 2006), namely path length and senses depth in the used thesaurus. The reader can consult Budanitsky and Hirst (2006) to confirm that all the other measures of semantic relatedness we compare to, do not follow the same pattern as the human ratings, as closely as our measure of relatedness does (low y values for small x values and high y values for high x). There are two main approaches. Methods based on manually built lexical knowledge bases, such as WordNet, model semantic relatedness by computing the shortest path between two concepts in the knowledge base and/or by looking at word overlap in the glosses (see Budanitsky and Hirst (2006) for an overview). There is a large body of work on using WordNet to compute measures of lexical similarity (Budanitsky and Hirst, 2006). The ability to determine semantic relatedness between terms is useful for a variety of nlp applications, including word sense disambiguation, information extraction and retrieval, and text summarisation (Budanitsky and Hirst, 2006).
Named Entity Transliteration And Discovery From Multilingual Comparable Corpora Named Entity recognition (NER) is an important part of many natural language processing tasks. Most current approaches employ machine learning techniques and require supervised data. However, many languages lack such resources. This paper presents an algorithm to automatically discover Named Entities (NEs) in a resource free language, given a bilingual corpora in which it is weakly temporally aligned with a resource rich language. We observe that NEs have similar time distributions across such corpora, and that they are often transliterated, and develop an algorithm that exploits both iteratively. The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration. We evaluate the algorithm on an English-Russian corpus, and show high level of NEs discovery in Russian. We follow the recent work of (Klementiev and Roth 2006) who addressed the problem of discovery of transliterated named entities from comparable corpora and suggested that alignment may not be necessary for transliteration.      We compared our algorithm to two models described in (Klementiev and Roth, 2006b) one uses only phonetic similarity and the second also considers temporal cooccurrence similarity when ranking the transliteration candidates. This configuration is equivalent to the model used in (Klementiev and Roth, 2006b). We adopt a methodology parallel to that of [Klementiev and Roth, 2006], but we focus instead on mining parallel named entity transliteration pairs, using a well-trained linear classifier to identify transliteration pairs. Recently, [Klementiev and Roth, 2006] outlined an approach by leveraging the availability of article aligned news corpora between English and Russian, and tools in English, for discovering transliteration pairs between the two languages, and progressively refining the discovery process. As in [Klementiev and Roth, 2006] no language specific knowledge was used to refine our mining process, making the approach broadly applicable. In this section, we outline briefly the methodology presented in [Klementiev and Roth, 2006], and refer interested readers to the source for details. We start with comparable corpora in English and Tamil, similar in size to that used in [Klementiev and Roth, 2006], and using the English side of this corpora, first, we extract all the NEs that occur more than a given threshold parameter, FE, using a standard NER tool. While we adopted a methodology similar to that in [Klementiev and Roth, 2006], our focus was on mining parallel NE transliteration pairs, leveraging the availability of comparable corpora and a well-trained linear classifier to identify transliteration pairs. However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth, 2009), (Goldwasser and Roth, 2008), (Klementiev and Roth, 2006), (Sproat et al, 2006), (Udupa et al, 2009b). Character unigrams and bigrams were used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model (Klementiev and Roth, 2006). Identifying transliteration pairs is an important component in many linguistic applications which require identifying out-of-vocabulary words, such as machine translation and multilingual information retrieval (Klementiev and Roth, 2006b; Hermjakob et al, 2008). The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al, 2006) and train a discriminative classifier. We then use a method similar to (Klementiev and Roth, 2006a; Goldwasser and Roth, 2008) in order to discriminatively train a better weight vector for the objective function. Our initial feature extraction method follows the one presented in (Klementiev and Roth, 2006a), in which the feature space consists of n-gram pairs from the two languages.
Minimally Supervised Morphological Analysis By Multimodal Alignment with genetic algorithms. Workshop on Empirical Learning of NLP Tasks. K. Koskenniemi, 1983. A general computation model word-form recognition and production. 11, of General Linguistics. of Helsinki. C.X. Ling, 1994. Learning the past tense of English verbs: The symbolic pattern associator vs. connecmodels. Art. Intel. Res., R. Mooney and M. Califf, 1995. Induction of firstorder decision lists: Results on learning the past of English verbs. Art. Intel. Res., K. Oflazer and S. Nirenburg, 1999. Practical bootof morphological analyzers. of the Conference on Natural Language Learning. D. Rumelhart and J. McClelland, 1986. On learning the past tense of English verbs. In J. McClel- D. Rumelhart, and the Group, Parallel distributed processing: Explorations in the of cognition, 2. MIT Press. P. Theron and I. Cloete, 1997. Automatic acquisition two-level morphological rules. of the Fifth Conference on Applied Natural Language Propages 103-110. A. Voutilainen, 1995. Morphological disambiguation. In F. Karlsson, A. Voutilainen, J. Heikkila, and A. (eds.) grammar - A language independent system for parsing unrestricted text, pages 165-284. The Hague: Mouton de Gruyter. Also, Yarowsky and Wicentowski (2000) obtained outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language's inflectional parts of speech, and the canonical suffixes for each part of speech. MED has been applied to the morphology induction problem by other researchers (such as Yarowsky and Wicentowski, 2000). For the future, we expect improvements could be derived by coupling this work, which focuses primarily on inducing regular morphology, with that of Yarowsky and Wicentowski (2000), who assume some information about regular morphology in order to induce irregular morphology.  Yarowsky and Wicentowski (2000) propose an algorithm that extracts morphological rules relating roots and inflected forms of verbs (but the algorithm can be extended to other morphological relations). Yarowsky and Wicentowski (2000) report an accuracy of over 99% for their best model and a test set of 3888 pairs. From the point of view of the evaluation of the algorithm, we should design an assessment scheme that would make our experimental results more directly comparable to those of Yarowsky and Wicentowski (2000), Schone and Jurafsky (2000) and others. The development of the WordFrame model was motivated by work originally presented in Yarowsky and Wicentowski (2000). The supervised morphological learner presented in Yarowsky and Wicentowski (2000) modeled lemmatization as a word-final stem change plus a suffix taken from a (possibly empty) list of potential suffixes. In Yarowsky and Wicentowski (2000), the end-of-string model is trained from inflection-root pairs acquired through unsupervised methods. We propose no new solutions to handling irregular verb forms, but suggest using non-string-based techniques, such as those presented in (Yarowsky and Wicentowski, 2000), (Baroni et al, 2002) and (Wicentowski, 2002). Yarowsky and Wicentowski (2000) have developed a system that learns such rules given a preliminary morphological hypothesis and part of speech tags. Similarly as Yarowsky and Wicentowski (2000), we assume that, in any language, vowels are more mutable in inflection than consonants, thus for example replacing a for i is cheaper that replacing s by r. Yarowsky and Wicentowski (2000) use similar statistics to identify words related by inflection, but they gather their counts from a much smaller corpus. Yarowsky and Wicentowski (2000) propose an interesting algorithm that employs four similarity measures to successfully identify the most probable root of a highly irregular word. Lemmatization was performed using an existing trie-based supervised models for English, and a combination of supervised and unsupervised methods (Yarowsky and Wicentowski, 2000) for all the other languages. Yarowsky and Wicentowski (2000) present a corpus-based approach for morphological analysis of both regular and irregular forms based on four models including: relative corpus frequency, context similarity, weighted string similarity and incremental retraining of inflectional transduction probabilities.
Inference In DATR a declarative language for representing a restricted class of inheritance networks, permitting both multiple and default inheritance. The principal intended area of application is the representation of lexical entries for natural language processing, and we use examples from this domain throughout. In this paper we present the syntax and inference mechanisms for language. The goal of the is the design of a simple language that (i) has the necessary expressive power to encode the lexical entries presupposed by contemporary work in the unification grammar tradition, (ii) can express all the evident generalizations about such entries, (iii) has an explicit theory of inference, (iv) is computationally tractable, and (v) has an explicit declarative semantics. The present paper is primarily concerned with (iii), though the examples used may hint at our strategy in respect of (i) and (ii). Several lexical representation formalisms addressing these desiderata have been proposed, e.g. DATR [Evans and Gazdar 1989a, 1989b, 1990]; LRL [Copestake, 1992]; [Russell et al 1991]. Analyses in Network Morphology are implemented in DATR, a formal language for representing lexical knowledge designed and implemented by Roger Evans and Gerald Gazdar (Evans and Gazdar, 1989). A well-known formalism following this approach is DATR [Evans and Gazdar, 1989]. DATR was originally introduced by Evans and Gazdar (1989a; 1989b) as a simple, non monotonic language for representing lexical inheritance hierarchies. The original publications on DATR sought to provide the language with (1) a formal theory of inference (Evans and Gazdar, 1989a) and (2) a model-theoretic semantics (Evans and Gazdar, 1989b). The problem of constructing an explicit theory of infhrence for DATR was originally addressed in (Evans and Gazdar, 1989a). Consider for example the following rule of inherence, adapted from (Evans and Gazdar, 1989a). This fulfills one of the original objectives of the DATR programme, as set out in (Evans and Gazdar, 1989a; Evans and Gazdar, 1989b), to provide the language with an explicit theory of inference.
Shared Logistic Normal Distributions for Soft Parameter Tying in Unsupervised Grammar Induction We present a family of priors over probabilistic grammar weights, called the shared logistic normal distribution. This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learnand with a non-parallel, multilingual corpus. Cohen et al (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags. Cohen and Smith (2009) use more complicated algorithms (variational EM and MBR decoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data).  Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others.   Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts. While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods. While this progression of model structure is similar to that explored in Cohen and Smith (2009), Cohen and Smith saw their largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing. Evaluating our LINGUISTIC model on the same test sets as (Cohen and Smith, 2009), sentences of length 10 or less in section 23 of PTB and sections 271 - 300 of CTB, we achieved an accuracy of 56.6 for Chinese and 60.3 for English. The best models of Cohen and Smith (2009) achieved accuracies of 52.0 and 62.0 respectively on these same test sets. These are the same training, development, and test sets used by Cohen and Smith (2009). Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results.  This has been done successfully in multilingual settings (Cohen and Smith, 2009). Today's best unsupervised dependency parsers, which are rooted in this model, train on short sentences only: both Headen et al., (2009) and Cohen and Smith (2009) train on WSJ10 even when the test set includes longer sentences. Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al, 2008, and Cohen and Smith, 2009). The performance of Cohen and Smith (2009), like the performance of Headden et al (2009), is greater than what we report, but those developments are orthogonal to the contributions of this paper. Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009).
Corpus Variation And Parser Performance Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank. While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might affect parser performance, and how portable parsing models are across corpora. We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained. This leads us to a technique for pruning parameters to reduce the size of the parsing model. Another approach to this topic, examining the effect of using lexical bigram information, which is very corpus specific, appears in (Gildea, 2001). Lexicalization allows us to capture bi-lexical relationships along dependency arcs, but it has been previously shown that these add only marginal benefit to Collins's model anyway (Gildea, 2001). Gildea (2001) and Bacchiani et al (2006) show that out-of-domain training data can improve parsing accuracy. A statistical syntactic parser is known to perform badly if a text to be parsed belongs to a domain which differs from a domain on which the parser is trained (Gildea, 2001). Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001). When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001). See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here. We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-of domain training data. Lexical information has a lot of promise for parse selection in theory, but there are practical problems such as sparse data and genre effects (Gildea, 2001). Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001). Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. For the Brown corpus, the test set was formed from every tenth sentence in the corpus (Gildea, 2001). The genre dependency of parsers is an accepted fact and has been described by, among others, Sekine (1997) and Gildea (2001). See Gildea (2001) for the exact setup. For the lexicalized approach, the annotation of symbols with lexical head is known to be rarely fully used in practice (Gildea, 2001), what is really used being the category of the lexical head. For this reason we do not run experiments on the task considered in (Gildea, 2001) and (Roark and Bacchiani, 2003), where they are porting from the restricted domain of the WSJ corpus to the more varied domain of the Brown corpus as a whole. In addition to portability experiments with the parsing model of (Collins, 1997), (Gildea, 2001) provided a comprehensive analysis of parser portability. Gildea (2001) only reports results on sentences of 40 or less words on all the Brown corpus sections combined, for which he reports 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus. They achieved very good improvement over their baseline and over (Gildea, 2001), but the absolute accuracies were still relatively low (as discussed above). Past research in a variety of NLP tasks, like parsing (Gildea, 2001) and chunking (Huang and Yates, 2009), has shown that systems suffer from a drop-off in performance on out-of-domain tests.
THE PENN TREEBANK:  ANNOTATING PREDICATE ARGUMENT STRUCTURE Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, Britta Schasberger Department of Computer and Information Science University of Pennsylvania Philadelphia, PA, USA ABSTRACT The Penn Treebank has recently implemented a new syn- tactic annotation scheme, designed to highlight aspects of predicate-argument structure.  In this study, training and test sets marked with two different types of chunk structure were derived algorithmically from the parsed data in the Penn Treebank corpus of Wall Street Journal text (Marcus et al, 1994). The label sets for German have been adopted from (Beuck and Menzel, 2013), while the sets for English have been obtained by manually analyzing the PTB (Marcus et al, 1994) for predictability. The Switchboard (Marcus et al, 1994) corpus contains transcriptions of spoken, spontaneous conversation annotated with phrase-structure trees. Few hand-crafted, deep unification grammars have in fact achieved the coverage and robustness required to parse a corpus of say the size and complexity of the Penn treebank: (Riezler et al., 2002) show how a deep, carefully hand-crafted LFG is successfully scaled to parse the Penn-II treebank (Marcus et al, 1994) with discriminative (log linear) parameter estimation techniques. Figure 1: The tree structure for an imperative sentence part-of-speech is the WP in Penn Treebank (Marcus et al, 1994) POS tag set) or is an adverb (WRB). We will be reporting on results using PropBank (Kingsbury et al, 2002), a 300k-word corpus in which predicate argument relations are marked for part of the verbs in the Wall Street Journal (WSJ) part of the Penn Tree Bank (Marcus et al, 1994). We applied our approaches to parsing errors given by the HPSG parser Enju, which was trained on the Penn Treebank (Marcus et al, 1994) section 2-21. Note that the tag set used by ltpos is the Penn Treebank tag set (Marcus et al, 1994). The use of PCFG is tied to the annotation principles of popular tree banks, such as the Penn Treebank (PTB) (Marcus et al, 1994), which are used as a data source for grammar extraction. A second approach involves querying gold standard treebanks such as the Penn Treebank (Marcus et al, 1994) and Tiger Treebank (Brantset al, 2004) to determine the frequency of certain phenomena. In this study we use PropBanked versions of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1994) and part of the Brown portion of the Penn Treebank. Bangalore et al (2001) investigate the effect of training size on performance while using grammars automatically extracted from the Penn II Treebank (Marcus et al, 1994) for generation. In the Penn Treebank (PTB) (Marcus et al, 1994), e.g., this mechanism is a combination of special labels and empty nodes, establishing implicit additional edges. This results in several aspects that distinguish the MH treebank from, e.g., the WSJ Penn tree bank annotation scheme (Marcus et al, 1994). Also, the MH tree bank is much smaller than the ones for, e.g., English (Marcus et al, 1994) and Arabic (Maamouri and Bies, 2004), making it hard to apply data-intensive methods such as the all subtrees approach (Bod, 1992) or full lexicalization (Collins, 2003). The idea is to augment the Penn Treebank (Marcus et al, 1994) constituent labels with the semantic role labels from the PropBank (Palmer et al, 2005), and generate a rich training corpus. Note that the Berkeley parser is trained on the Penn treebank (Marcus et al, 1994) yet the HPSG parser is trained on the HPSG tree bank (Miyao and Tsujii, 2008). We conducted two experiments on Penn Treebank II corpus (Marcus et al, 1994). The PennTreebank II (Marcus et al, 1994) marks subjects (SBJ), logical objects of passives (LGS), some reduced relative clauses (RRC), as well as other grammatical information, but does not mark each constituent with a grammatical role. The same idea was used by Magerman (1995), who developed the first "head table" for the Penn Treebank (Marcus et al, 1994), and Collins (1996), whose constituent parser is internally based on probabilities of bilexical dependencies, i.e. dependencies between two words.
Unsupervised Modeling of Twitter Conversations We propose the first unsupervised approach to the problem of modeling dialogue acts in an open domain. Trained on a corpus of noisy Twitter conversations, our method discovers dialogue acts by clustering raw utterances. Because it accounts for the sequential behaviour of these acts, the learned model can provide insight into the shape of communication in a new medium. We address the challenge of evaluating the emergent model with a qualitative visualization and an intrinsic conversation ordering task. This work is inspired by a corpus of 1.3 million Twitter conversations, which will be made publicly available. This huge amount of data, available only because Twitter blurs the line between chatting and publishing, highlights the need to be able to adapt quickly to a new medium. Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al, 2010). Ritter et al (2010) extends LMHMM to allow words to be emitted from two additional sources: the topic of current dialogue, or a background LM shared across all dialogues. Ritter et al (2010) finds these alternate sources are important in non-task-oriented domains, where events are diffuse and fleeting. Ritter et al (2010) proposes an evaluation based on rank correlation coefficient, which measures the degree of similarity between any two orderings over sequential data. Ritter et al (2010) limits their dataset by choosing Twitter dialogues containing 3 to 6 posts (utterances), making it tractable to enumerate all permutations. To overcome this, we plan to use an unsupervised learning approach to discover dialogue acts (Ritter et al, 2010). To date, our paper most closely relates to works on semantic role labeling (SRL) on social media (Liu et al., 2010) and conversation modeling (Ritter et al, 2010). On a different ground, Ritter et al (2010) propose a probabilistic model to discover dialogue acts in Twitter conversations and to classify tweets in a conversation according to those acts. Twitter also provides a wealth of user dialog, and a variety of dialog acts have been observed (Ritter et al, 2010) and predicted (Ritter et al, 2011). Examples of such approaches include methods for conversation structure analysis (Ritter et al, 2010) and exploration of geographic language variation (Eisenstein et al, 2010) from Twitter messages. Second, we employ a MIRA-based binary classifier (Ritter et al, 2010) to predict whether a message mentions a concert event. Twitter has previously been proposed as a candidate for modeling conversations, see for example Ritter et al (2010). In the context of Twitter conversations, Ritter et al (2010) suggests using dialogue act tags as a middle layer towards conversation reconstruction. Specifically, we build off the Bayesian block HMMs used by Ritter et al (2010) for modeling Twitter conversations, which will be our primary baseline. We show that M4 increases thread reconstruction accuracy by up to 15% compared to the HMM of Ritter et al (2010), and we reduce variation of information against speech act annotations by an average of 18% from HMM and LDA baselines. Under the block HMM, as utilized by Ritter et al (2010), messages in a conversation flow according to a Markov process, where the words of messages are generated according to language models associated with a state in a hidden Markov model. If it is desired to have a large number of latent topics as is common in LDA, this model could be combined with a standard topic model without sequential dependencies, as explored by Ritter et al (2010). Unsupervised HMMs were applied to conversational data by Ritter et al (2010) who experimented with Twitter conversations. Second, we use the Twitter data set created by Ritter et al (2010). Our work is motivated by the Bayesian HMM approach of Ritter et al (2010) - the model we refer to as the block HMM (BHMM) - and we consider this our primary baseline.
Bayesian Learning of Non-Compositional Phrases with Synchronous Parsing We combine the strengths of Bayesian modeling and synchronous grammar in unsupervised learning of basic translation phrase pairs. The structured space of a synchronous grammar is a natural fit for phrase pair probability estimation, though the search space can be prohibitively large. Therefore we explore efficient algorithms for pruning this space that lead to empirically effective results. Incorporating a sparse prior using Variational Bayes, biases the models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment. This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches. This is in line with earlier work on consistent estimation for similar models (Zollmann and Sima? an, 2006), and agrees with the most up-to-date work that employs Bayesian priors over the estimates (Zhang et al., 2008). Also most up-to-date, (Zhang et al, 2008) report on a multi-stage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator and concentrating the efforts on pruning both the space of phrase pairs and the space of (ITG) analyses. It differs from (Zhang et al, 2008) in that it does postulate a latent segmentation variable and puts the prior directly over that variable rather than over the ITG synchronous rule estimates. As well as smoothing, we find (in the same vein as (Zhang et al, 2008)) that setting effective priors/smoothing is crucial for EM to arrive at better estimates. The samplers are initialised with trees created from GIZA++ Model 4 alignments, altered such that they are consistent with our ternary grammar. This is achieved by using the factorisation algorithm of Zhang et al (2008a) to first create initial trees. Zhang et al (2008) suggest tic-tac-toe pruning, which uses Model 1 posteriors to exclude ranges of cells from being computed.   We used a variant of the phrasal ITG described by Zhang et al (2008). Since many widely used SCFGs meet these criteria, including hierarchical phrase-based translation grammars (Chiang, 2007), SAMT grammars (Zollmann and Venugopal, 2006), and phrasal ITGs (Zhang et al, 2008), a detailed analysis of  containing and higher rank grammars is left to future work. We opt for an alternative alignment technique, similar to the word-aligner described by Zhang et al (2008). Cherry and Lin (2007) and Zhang et al (2008) used synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem. The block ITG family permits multiple links to be on (aij 6= off) for a particular word ei via terminal block productions, but ensures that every word is 32 in at most one such terminal production, and that the full set of terminal block productions is consistent with ITG reordering patterns (Zhang et al, 2008). Zhang et al (2008b) and Chang et al (2008) show that get ting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. Johnson (2007) and Zhang et al (2008a) show having small helps to control over fitting. Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al, 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al, 2009). Similarly, the work in (Zhang et al, 2008) reports on a multistage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator. In (Zhang et al, 2008), Bayesian learning was applied for estimating word-alignments within a synchronous grammar. Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2, compared with the n3 amortized time of the tic-tac-toe pruning algorithm described by (Zhang et al, 2008a). Zhang et al (2008) and others propose dealing with this problem by putting a prior probability P (? x,? t) on the parameters.
Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis Determining the polarity of a sentimentbearing expression requires more than a simple bag-of-words approach. In particular, words or constituents within the expression can interact with each other to yield a particular overall polarity. In this paper, we view such interactions in light of composiand present a novel learningbased approach that incorporates structural inference motivated by compositional semantics into the learning procedure. Our experiments show that (1) simple heuristics based on compositional semantics can perform better than learning-based methods that do not incorporate compositional semantics (accuracy of 89.7% vs. 89.1%), but (2) a method that integrates compositional semantics into learning performs better than all other alternatives (90.7%). We also find that “contentword negators”, not widely employed in previous work, play an important role in determining expression-level polarity. Finally, in contrast to conventional wisdom, we find that expression-level classification accuracy additional, potentially disambiguating, context is considered.  Content-word negators are words that are not function words, but act semantically as negators (Choi and Cardie, 2008).  For the general-purpose polarity lexicon, we expand the polarity lexicon of Wilson et al (2005) with General Inquirer dictionary as suggested by Choi and Cardie (2008). According to Choi and Cardie (2008), voting algorithms that recognize content-word negators achieve a competitive performance, so we will use a variant of it for simplicity. Because none of the algorithms proposed by Choi and Cardie (2008) is designed to handle the neutral polarity, we invent our own version as shown in Figure 2. Choi and Cardie (2008) also focus on the expression-level polarity classification, but their evaluation setting is not as practical as ours in that they assume the inputs are guaranteed to be either strongly positive or negative. Choi and Cardie (2008) proposed a learning-based framework. Choi and Cardie (2008) present a more lightweight approach using compositional semantics towards classifying the polarity of expressions. The rules presented by Choi and Cardie (2008) are, however, much more specific, as they define syntactic contexts of the polar expressions. Unlike Choi and Cardie (2008), these rules require a proper parse and reflect grammatical relationships between different constituents. In this work we focus on explicit negation mentions, also called functional negation by Choi and Cardie (2008). Choi and Cardie (2008) combine different kinds of negators with lexical polarity items through various compositional semantic models, both heuristic and machine learned, to improve phrasal sentiment analysis. Here, the verbs prevent and ease act as content-word negators (Choi and Cardie, 2008) in that they modify the negative sentiment of their direct object arguments so that the phrase as a whole is perceived as somewhat positive. Choi and Cardie (2008), for example, propose an algorithm for phrase-based sentiment analysis that learns proper assignments of intermediate sentiment analysis decision variables given the a priori (i.e., out of context) polarity of the words in the phrase and the (correct) phrase-level polarity. Choi and Cardie (2008) hand-code compositional rules in order to model compositional effects of combining different words in the phrase. We extract all sentences containing strong (i.e. intensity is medium or higher), sentiment-bearing (i.e. polarity is positive or negative) expressions following Choi and Cardie (2008). An English polarity reversing word dictionary was constructed from the General Inquirer dictionary in the same way as Choi and Cardie (2008), by collecting words which belong to either NOTLW or DECREAS categories (The dictionary contains 121 polarity reversing words). Choi and Cardie (2008) categorized polarity reversing words into two categories: function-word negators such as not and content-word negators such as eliminate. Choi and Cardie (2008) proposed a method to classify the sentiment polarity of a sentence basing on compositional semantics.
Base Noun Phrase Translation Using Web Data And The EM Algorithm We consider here the problem of Base Noun Phrase translation. We propose a new method to perform the task. For a given Base NP, we first search its translation candidates from the web. We next determine the possible translation(s) from among the candidates using one of the two methods that we have developed. In one method, we employ an ensemble of Na?ve Bayesian Classifiers constructed with the EM Algorithm. In the other method, we use TF-IDF vectors also constructed with the EM Algorithm. Experimental results indicate that the coverage and accuracy of our method are significantly better than those of the baseline methods relying on existing technologies. We integrate both empirical and symbolic knowledge sources as features into our system which outperforms the best known methods in statistical machine translation. Previous work on defining subtasks within statistical machine translation has been performed on ,e.g., noun-noun pair (Cao and Li, 2002) and named entity translation (Al-Onaizan and Knight, 2002). Cao and Li (2002) and Ismail and Manandhar (2010) the context of text units is used to identify term mappings. So far as we are aware, only Cao and Li (2002), who treat only base noun phrase (NP) mapping, consider the problem this way. Our work differs from that of Cao and Li (2002) in several ways. First they consider only terms consisting of noun noun pairs. Cao and Li (2002) have achieved 91% accuracy for the top three candidates using the Web as a comparable corpus. Cao and Li (2002) restricted candidate bilingual compound term pairs by consulting a seed bilingual lexicon and requiring their constituent words to be translation of each other across languages. On the other hand, in the framework of bilingual term correspondences estimation of this paper, the computational complexity of enumerating translation candidates can be easily avoided with the help of cross-language retrieval of relevant news texts. Furthermore, unlike Cao and Li (2002), bilingual term correspondences for compound terms are not restricted to compositional translation. In the first approach, translation candidates are validated through the search engine (Cao and Li, 2002). The scoring function? I is intended to evaluate the approach proposed in (Cao and Li, 2002). (Cao and Li, 2002) also proposed a method of compositional translation estimation for compounds. In the method of (Cao and Li, 2002), the translation candidates of a term are composition ally generated by concatenating the translation of the constituents of the term and are validated directly through the search engine. In this paper, we evaluate the approach proposed in (Cao and Li, 2002) by introducing a total scoring function 17 that is based on validating translation candidates directly through the search engine. Another important difference is that in (Fujii and Ishikawa, 2001), they evaluate only the performance of cross-language information retrieval but not that of translation estimation. (Cao and Li, 2002) proposed a method of compositional translation estimation for compounds. In the proposed method of (Cao and Li, 2002), translation candidates of a term are compositionally generated by concatenating the translation of the constituents of the term and are re-ranked by measuring contextual similarity against the source language term. One of the major differences of the technique of (Cao and Li, 2002) and the one proposed in this paper is that in (Cao and Li, 2002), they do not use the domain/topic specific corpus. Cao and Li (2002) propose a new method to translate base noun phrases. (Cao and Li, 2002) acquire noun phrase translations by making use of web data. Cao and Li (2002) described a new method for base noun phrase translation by using Web data. We translate NN compounds by way of a two-phase procedure, incorporating generation and selection (similarly to Cao and Li (2002) and Langkilde and Knight (1998)). One piece of research relatively closely related to our method is that of Cao and Li (2002), who use bilingual bootstrapping over Chinese and English web data in various forms to translate Chinese NN compounds into English.
Robust Sentiment Detection on Twitter from Biased and Noisy Data In this paper, we propose an approach toautomatically detect sentiments on Twit ter messages (tweets) that explores some characteristics of how tweets are written and meta-information of the words that compose these messages. Moreover, we leverage sources of noisy labels as our training data. These noisy labels were provided by a few sentiment detectionwebsites over twitter data. In our experi ments, we show that since our features areable to capture a more abstract representation of tweets, our solution is more ef fective than previous ones and also more robust regarding biased and noisy data, which is the kind of data provided by these sources. Recent examples of this approach are Barbosa and Feng (2010) and Pak and Paroubek (2010).  The size of our hand-labeled data allows us to perform cross validation experiments and check for the variance in performance of the classifier across folds. Another significant effort for sentiment classification on Twitter data is by Barbosa and Feng (2010). The state-of-the-art approaches for solving this problem, such as (Go et al, 20095; Barbosa and Feng, 2010), basically follow (Pang et al, 2002), who utilize machine learning based classifiers for the sentiment classification of texts. In contrast, (Barbosa and Feng, 2010) propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features. We also re-implemented the method proposed in (Barbosa and Feng, 2010) for comparison. From Table 1, we can see that all our systems perform better than (Barbosa and Feng, 2010) on our data set. One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. The results show that our system using both content features and sentiment lexicon features performs slightly better than (Barbosa and Feng, 2010). Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010).
Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters author: Online conversational text, typified by microblogs, and text is a challenge for natural language processing. Unlike the highly edited genres that conventional NLP tools have been developed for, conversational text contains many nonstandard lexical items and syntactic patterns. These are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013). An example is shown in Fig. 1. As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate. There Abstract We consider the problem of part-of-speech tagging for informal, online conversational text. We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy. With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90% to 93% accuracy (more than 3% absolute). Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre. Additionally, we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines. Tagging software, annotation guidelines, and large-scale word clusters are available at: POS tagger in this work, we note that taggers optimized specifically for social media are now available and would likely have resulted in higher tagging accuracy (e.g. Owoputi et al (2013)). To test this, we train a CRF model (Lafferty et al, 2001) with simple orthographic features and word clusters (Owoputi et al, 2013) on the annotated Twitter data described in Gimpel et al (2011). For NER, we use standard features, including POS tags (from the previous experiments), indicators for hyphens, digits, single quotes, upper/lowercase, 3-character prefix and suffix information, and Brown word cluster features 6 with 2,4,8,16 bit string prefixes estimated from a large Twitter corpus (Owoputi et al, 2013). We use the CMU Twitter Part-of-Speech Tagger (Owoputi et al, 2013) to select only instances in the verb sense. Part-of-speech tags are assigned based on Owoputi et al's tweet POS system (Owoputi et al, 2013).
A Discriminative Latent Variable Model for Statistical Machine Translation Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems. We argue that a principle reason for this failure is not dealing with multiple, equivalent translations. We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised. Results show that accounting for multiple derivations does indeed improve performance. Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions. Coming up-to-date, (Blunsom et al, 2008) attempt a related estimation problem to (Marcu and Wong, 2002), using the expanded phrase pair set of (Chiang, 2005a), working with an exponential model and concentrating on marginalizing out the latent segmentation variable.  Secondly, as (Blunsom et al, 2008) show, marginalizing out the different segmentations during decoding leads to improved performance. The version presented in (Blunsom et al., 2008) scales to more than a hundred thousand short training sentences, but does not integrate a language model and thus has performance that improves upon Hiero without a language model only. In practice, this problem can be circumvented by discarding the training sentence pairs with unreachable reference translations, but this may mean a significant reduction in the amount of training data (24% in (Blunsom et al., 2008)). Instead of using expected BLEU as a training objective, Blunsom et al (2008) trained their model to directly maximise the log-likelihood of the discriminative model, estimating feature expectations from a packed chart. This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al, 2008). In recent years, great research has shown the strength of latent variable models for natural language processing (Blunsom et al, 2008). We use the forest to train a log-linear model with a latent variable as describe in Blunsom et al (2008). Of ten, there are many derivations that are distinct yet produce the same translation. Blunsom et al (2008) present a latent variable model that describes the relationship between translation and derivation clearly. (6) as max-derivation decoding, which are first termed by Blunsom et al (2008). Blunsom et al (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. Both Mi et al (2008) and Blunsom et al (2008) use a translation hyper graph to represent search space. For the first model, which includes the sparse parse features, we learn weights in order to optimize penalized conditional log likelihood (Blunsom et al, 2008). See Blunsom et al (2008) for more information. We might also want to calculate the total probability of all possible derivations, which is useful for parameter estimation (Blunsom et al, 2008). Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al, 2008). For the hierarchical phrase-based approach, (Blunsom et al, 2008) present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations. The first method maximizes data likelihood as is standard in EM, while the second method maximizes conditional likelihood for a log linear model following Blunsom et al (2008). Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al, 2008).
Rule Writing Or Annotation: Cost-Efficient Resource Usage For Base Noun Phrase Chunking This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive realtime human annotation. Several novel variations on active learning are investigated, and underlying cost models for cross-modal machine learning comparison are presented and explored. Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment. (Ngai and Yarowsky, 2000) and (Ngai, 2001) provide a thorough description of many experiments involving rule-based systems and statistical learners for NP bracketing. Ngai and Yarowsky (2000) investigated the effectiveness of rule-writing versus annotation (using active learning) for chunking, and found the latter to be far more effective. Another relatively early work in our field along these lines was the work of Ngai and Yarowsky (2000), which measured actual times of annotation to compare the efficacy of rule writing versus annotation with AL for the task of BaseNP chunking. One empirical study (Ngai and Yarowsky, 2000) found that it also required more annotation time than active learning. The f-complement has been suggested for active learning in the context of NP chunking as a structural comparison between the different analyses of a committee (Ngai and Yarowsky, 2000). One exception is (Ngai and Yarowsky, 2000) (discussed later) which compares the cost of manual rule writing with AL-based annotation for noun phrase chunking. In contrast to the model presented by Ngai and Yarowsky (2000), which predicts monetary cost given time spent, this model estimates time spent from characteristics of a sentence. Ngai and Yarowsky (2000) used an ensemble based on bagging and partitioning for active learning for base NP chunking. In our annotation experiments, we measure the exact time taken to annotate each example by each annotator and use this as the cost metric, inspired by Ngai and Yarowsky (2000). AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). These measures are convenient for performing active learning simulations, but awareness has grown that they are not truly representative measures of the actual cost of annotation (Haertel et al, 2008a; Settles et al, 2008), with Ngai and Yarowsky (2000) being an early exception to the unit-cost approach. In their study on AL for base noun phrase chunking, Ngai and Yarowsky (2000) compare the costs of rule-writing with (AL-driven) annotation to compile a base noun phrase chunker.
Improving Data Driven Wordclass Tagging by System Combination In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system. We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging. Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers. All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger. We consider three voting strategies suggested by van Halteren et al (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair-wise voting. In the weighted voting method, we can assign different weights to the results of the individual system (van Halteren et al, 1998). In that paper, pairwise voting (van Halteren et al, 1998) has been used to combine the results of two super taggers that scan the input in the opposite directions. In the experiments presented in van Halteren et al (1998), this method was the best performer among the presented methods. And finally, TAGPAIR uses classification pair weights based on the probability of a classification for some predicted classification pair (van Halteren et al, 1998). Parallel to (van Halteren et al, 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based. In all experiments, the TotPrecision voting scheme of (van Halteren et al, 1998) has been used.
Effective Self-Training For Parsing We present a simple, but surprisingly effective, method of self-training a twophase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. improved model achieves an of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon. We should keep in mind that (1) a tree bank PCFG is not state-of-the-art: its performance is mediocre compared to e.g. Bod (2003) or McClosky et al (2006), and (2) that our tree bank PCFG is binarized as in Klein and Manning (2005) to make results comparable.  For instance, McClosky et al (2006) improved a statistical parser by self-training. Its success stories range from parsing (McClosky et al, 2006) to machine translation (Ueffing, 2006). previously used for self-training of parsers (McClosky et al, 2006).  We note that the performance is on the same level as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the combination of reranking and self-training. In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). The NANC corpus contains approximately 2 million WSJ sentences that do not overlap with Penn's WSJ and has been previously used by McClosky et al (2006) in improving a supervised parser by self training. McClosky et al (2006) presented a very effective method for self-training a two-stage parsing system consisting of a first-stage generative lexicalized parser and a second-stage discriminative reranker. Although our models are based on purely generative PCFG grammars, our best product model performs competitively to the self-trained two-step discriminative reranking parser of McClosky et al (2006), which makes use of many non-local reranking features. We expect that replacing the first-step generative parsing model in McClosky et al (2006) with a product of latent variable grammars would give even higher parsing accuracies. Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al, 2006). Our work is related to self-training (McClosky et al, 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. SELF-CRF: Following the self-training paradigm (e.g., (McClosky et al, 2006b; McClosky et al, 2006a)), we train our baseline first on the training set, then apply it to the test set, then retrain it on the training set plus the automatically labeled test set. To produce this, we segment sentences with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse the corpus with the self trained Charniak parser (McClosky et al, 2006). A noteable exception in constituent-based parsing is the work of McClosky et al (2006) who show that self-training is possible if a reranker is used to inform the underlying parser. Of these, McClosky et al (2006) deal specifically with self training for data-driven statistical parsing. Self-training can suffer from over-fitting, in which errors in the original model are repeated and amplified in the new model (McClosky et al, 2006).
Tailoring Word Alignments to Syntactic Machine Translation Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that violate syntactic correspondences. We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our model’s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments. Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein (2007), and building a syntax-based word alignment model Mayand Knight (2007) with TTS templates. We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al, 2006), combined word-to-word posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-of the-art unsupervised baseline. When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein (2007), including the bilingual dictionary. We also trained an HMM aligner as described in DeNero and Klein (2007) and used the posteriors of this model as features.  This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007). DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and using agreement training for the HMM (Liang et al, 2006). DeNero and Klein (2007) use a syntax based distance in an HMM word alignment model to favor syntax-friendly alignments.  We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus. 
Creating Speech and Language Data With Amazon&rsquo;s Mechanical Turk In this paper we give an introduction to using Amazon?s Mechanical Turk crowdsourc ing platform for the purpose of collecting data for human language technologies. Wesurvey the papers published in the NAACL 2010 Workshop. 24 researchers participated in the workshop?s shared task to create data for speech and language applications with $100. Crowdsourcing services, such as Amazon Mechanical Turk (MTurk) and CrowdFlower, have been recently used with success for a variety of NLP applications (Callison-Burch and Dredze, 2010). Corpus creation using AMT has numerous precedents now; see i.e. Callison-Burch and Dredze (2010) and Heilman and Smith (2010b). A number of researchers have recently experimented with the use of Amazon Mechanical Turk (AMT) to create and evaluate human language data (Callison-Burch and Dredze, 2010). Over the last several of years, Mechanical Turk, introduced by Amazon as artificial artificial intelligence, has been used successfully for a number of NLP tasks, including robust evaluation of machine translation systems by reading comprehension (Callison-Burch, 2009), and other tasks explored in the recent NAACL workshop (Callison-Burch and Dredze, 2010b). A number of mechanisms for quality control have been proposed for use with Mechanical Turk annotation (Callison-Burch and Dredze, 2010a). That paper appeared in a NAACL 2010 workshop organized by Callison-Burch and Dredze (2010), focusing on MTurk as a source of data for speech and language tasks. Callison-Burch and Dredze (2010) provide an overview of various tasks for which MTurk has been used, and offer a set of best practices for ensuring high-quality data.
A Non-Projective Dependency Parser We describe a practical parser for unrestricted dependencies. The parser creates links between words and names the links according to their syntactic functions. We first describe the older Constraint Grammar parser where many of the ideas come from. Then we proceed to describe the central ideas of our new parser. Finally, the parser is evaluated. To identify associative constructions, we first process our texts using Conexor's FDG parser (Tapanainen and Jarvinen, 1997). For English texts, these trees were first provided by the Connexor parser at UMIACS (Tapanainen and Jarvinen, 1997), and then corrected by one of the team PIs. It is developped at the Xerox Research Centre Europe (XRCE) and shares the same computationnal paradigm as the PNLPL approach (Jensen, 1992) and the FDGP approach (Tapanainen and Jarvinen, 1997).
Discriminative Training Of A Neural Network Statistical Parser Discriminative methods have shown significant improvements over traditional generative methods in many machine learning applications, but there has been difficulty in extending them to natural language parsing. One problem is that much of the work on discriminative methods conflates changes to the learning method with changes to the parameterization of the problem. We show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model. We present three methods for training a neural network to estimate the probabilities for a statistical parser, one generative, one discriminative, and one where the probability model is generative but the training criteria is discriminative. The latter model outperforms the previous two, achieving state-ofthe-art levels of performance (90.1% F-measure on constituents). For example, see (Henderson, 2004) for a discussion of why generative models are better than models parameterized to estimate the a posteriori probability directly. There certainly exist competitive parsers that internally represent lexical items as real-valued vectors, such as the neural network-based parser of Henderson (2004), and even parsers which use pre-trained word embeddings to represent the lexicon, such as Socher et al. First, these parsers are among the best in the literature, with a test performance of 90.7 F1 for the baseline Berkeley parser on the Wall Street Journal corpus (compared to 90.4 for Socher et al (2013) and 90.1 for Henderson (2004)). Even though some parsers effectively exhibit linear behavior in sentence length (Ratnaparkhi, 1997), fast statistical parsers such as (Henderson, 2004) still take around 1.5 seconds for sentences of length 35 in tests that we made. For example, the discriminative training techniques successfully applied in (Henderson, 2004) to the feed-forward neural network model can be directly applied to the mean field model proposed in this paper. Other techniques are also possible; Henderson (2004) uses neural networks to induce latent left-corner parser states.   Stochastic optimization methods have proven to be extremely efficient for the training of models involving computationally expensive objective functions like those encountered with our task (Vishwanathan et al, 2006) and, in fact, the on-line backpropagation learning used in the neural network parser of Henderson (2004) is a form of stochastic gradient descent. In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. Also, as with any generative model, it may be easy to improve the parser's accuracy by using discriminative retraining techniques (Henderson, 2004) or data-defined kernels (Henderson and Titov, 2005), with or even with out introduction of any additional linguistic features. We would expect further improvement of ISBN results if we applied discriminative retraining (Henderson, 2004) or reranking with data-defined kernels (Henderson and Titov, 2005), even without introduction of any additional features. Henderson (2004) finds that discriminative training was too slow, and reports accuracy higher than generative models by discriminatively reranking the output of his generative model.  Also, as with any generative model, it should be easy to improve the parser's accuracy with discriminative reranking, such as discriminative retraining techniques (Henderson, 2004) or data-defined kernels (Henderson and Titov, 2005), with or even without the introduction of any additional linguistic features. Other related work includes (Henderson, 2004), who discriminatively trains a parser based on synchrony networks and (Titov and Henderson, 2006), who use an SVM to adapt a generative parser to different domains.
Transductive learning for statistical machine translation Statistical machine translation systems are usually trained on large amounts of bilingual text and monolingual text in the target language. In this paper we explore the use of transductive semi-supervised methods for the effective use of monolingual data from the source language in order to improve translation quality. We propose several algorithms with this aim, and present the strengths and weaknesses of each one. We present detailed experimental evaluations on the French–English EuroParl data set and on data from the NIST Chinese–English largedata track. We show a significant improvement in translation quality on both tasks. Transductive learning method (Ueffing et al, 2007) which repeatedly re-trains the generated source target N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+in-domain language model) and obtain 1-best translation for each source-side sentence. A different form of semi-supervised learning (self-training) has been applied to MT by (Ueffing et al, 2007). Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). Such self-translation techniques have been introduced by Ueffing et al (2007). In follow up work, this approach was refined (Ueffing et al, 2007). Also, this method has been shown empirically to be more effective (Ueffing et al., 2007b) than (1) using the weighted combination of the two phrase tables from L and U+, or (2) combining the two sets of data and training from the bitext. We measure the similarity using weighted n-gram coverage (Ueffing et al, 2007b). To make the confidence score for sentences with different lengths comparable, we normalize using the sentence length (Ueffing et al, 2007b). The SMT system we applied in our experiments is PORTAGE (Ueffing et al, 2007a). From machine learning perspective, both proposed methods can be viewed as certain form of transductive learning applied to the SMT task (Ueffing et al, 2007).
Dual Decomposition for Parsing with Non-Projective Head Automata This paper introduces algorithms for nonparsing based on decomposi- We focus on parsing algorithms for nonhead a generalization of head-automata models to non-projective structures. The dual decomposition algorithms are simple and efficient, relying on standard dynamic programming and minimum spanning tree algorithms. They provably solve an LP relaxation of the non-projective parsing problem. Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98% of test sentences. The accuracy of our models is higher than previous work on a broad range of datasets. For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Recent work has shown that a relaxation based on dual decomposition often produces an exact solution for such problems (Koo et al 2010). In our dual decomposition inference algorithm, we use K =200 maximum iterations and tune the decay rate following the protocol described by Koo et al (2010). First, we follow Koo et al (2010) and use lazy decoding as part of dual decomposition. Approximate parsers have there fore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). While AD requires solving quadratic subproblems as an intermediate step, recent results (Martins et al, 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al, 2010). This opens the door for larger subproblems (such as the combination of trees and head automata in Koo et al, 2010) instead of a many-components approach (Martins et al, 2011), while still enjoying faster convergence. Koo et al (2010) used an identical automaton for their second-order model, but leaving out the grand-sibling scores.  Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation of the third-order model of Koo et al (2010). Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata,2006), and agreement constraints between word alignment directions (Ganchev et al, 2008) or various parsing models (Koo et al, 2010). DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al, 2010), bilingual sequence tagging (Wang et al, 2013) and word alignment (DeNero and Macherey, 2011). We adopt a learning rate update rule from Koo et al (2010) where t is defined as 1/N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t.  For the third-order features (e.g., grand-siblings and tri-siblings) described in (Koo et al, 2010), we will discuss it in future work.  Although we have seen more than a handful of recent papers that apply the dual decomposition method for joint inference problems, all of the past work deals with cases where the various model components have the same inference output space (e.g., dependency parsing (Koo et al, 2010), POS tagging (Rush et al., 2012), etc.). There is a lot of flexibility about how to decompose the model into S components: each set Rs can correspond to a single factor in a factor graph (Smith and Eisner, 2008), or to a entire subgraph enclosing several factors (Koo et al, 2010), or even to a formula in Markov logic (Richardson and Domingos, 2006). 
Lexical Semantic Relatedness with Random Graph Walks Many systems for tasks such as question answering, multi-document summarization, and infor mation retrieval need robust numerical measures of lexical relatedness. Standard thesaurus-based measures of word pair similarity are based on only a single path between those words in the thesaurus graph. By contrast, we propose a newmodel of lexical semantic relatedness that incorporates information from every explicit or implicit path connecting the two words in the en tire graph. Our model uses a random walk over nodes and edges derived from WordNet links and corpus statistics. We treat the graph as aMarkov chain and compute a word-specific sta tionary distribution via a generalized PageRank algorithm. Semantic relatedness of a word pair is scored by a novel divergence measure, ZKL, that outperforms existing measures on certain classes of distributions. In our experiments, the resultingrelatedness measure is the WordNet-based measure most highly correlated with human similar ity judgments by rank ordering at ? = .90. We further note that our results are different from that of (Hughes and Ramage, 2007) as they use extensive feature engineering and weight tuning during the graph generation process that we have not been able to reproduce. Hughes and Ramage (2007) find that PPR+cos has high correlation with human similarity judgments on WordNet-based graphs.     Hughes and Ramage (2007) use random walks over WordNet, incorporating information such as meronymy and dictionary glosses.  Our similarity method is similar, but simpler, to that used by (Hughes and Ramage, 2007), which report very good results on similarity datasets.  For example, (Hughes and Ramage, 2007) used random walks on Wordnet graph to measure lexical semantic relatedness between words. As for Personalized PageRank, it was used for word sense disambiguation (Agirre and Soroa, 2009), and for measuring lexical relatedness of words in a graph built from WordNet (Hughes and Ramage, 2007). As can be seen from the Table, our approach with the Weighted Overlap signature comparison improves over the similar approach of Hughes and Ramage (2007) which, however, does not involve the disambiguation step and considers a word as a whole unit as represented by the set of its senses.  Unlike some approach like (Hughes and Ramage, 2007), which performs well on some datasets but poorly on others, combing the VSMs from heterogeneous sources is more robust.  Graph-based methods have been successfully applied to evaluate word similarity using available ontologies, where the underlying graph included word senses and semantic relationships between them (Hughes and Ramage, 2007). For instance, Hughes and Ramage (2007) constructed a graph which represented various types of word relations from WordNet, and compared random-walk similarity to similarity assessments from human subject trials.
Nymble: A High-Performance Learning Name-Finder This paper presents a statistical, learned approach to finding names and other nonrecursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach. Bikel et al (Bikel et al, 1997) report on Nymble, an HMM-based name tagging system operating in English and Spanish. Most commonly, feature-based classifiers use a set of capitalisation features and a sentence-initial feature (Bikel et al, 1997). Nymble (Bikel et al, 1997) uses statistical learning to acquire a Hidden Markov Model (HMM) that recognises NEs in text. (Bikelet al, 1997) are other examples of the use of HMMs. Our chunk-based system takes the last word of the chunk as its head word for the purposes of predicting roles, but does not make use of the identities of the chunk's other words or the intervening words between a chunk and the predicate, unlike Hidden Markov Model-like systems such as Bikel et al (1997), McCallum et al (2000) and Laerty et al (2001). We were already using a generative statistical model for part-of-speech tagging (Weischedel et al 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al 1997). The HMM tagger generally follows the Nymble model (Bikel et al 1997), and uses best-first search to generate N-Best hypotheses for each input sentence. The base system is an HMM based tagger, similar to (Bikel et al, 1997). The alternative to true casing text is to destroy case information in the training material SNORIFY procedure in (Bikel et al, 1997). The typical machine learning approaches for English NE are transformation-based learning [Aberdeen et al 1995], hidden Markov model [Bikel et al. 1997], maximum entropy model [Borthwick, 1999], support vector machine learning [Eunji Yi et al 2004], unsupervised model [Collins et al 1999] and etc. Many of the previous studies of Bio-NER tasks have been based on machine learning techniques including Hidden Markov Models (HMMs) (Bikel et al, 1997), the dictionary HMM model (Kouetal., 2005) and Maximum Entropy Markov Mod els (MEMMs) (Finkel et al, 2004). These include rule-based systems [Krupka 1998], Hidden Markov Models (HMM) [Bikel et al 1997] and Maximum Entropy Models (MaxEnt) [Borthwick 1998]. constrained HMM Our original HMM is similar to the Nymble [Bikel et al 1997] system that is based on bigram statistics. In addition, an automatic named entity tagger (Bikel et al, 1997) was run on the sentences to map proper nouns to a small set of semantic classes. A. wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al 1997), Maximum Entropy methods (Borthwick et al 1998, Chieu and Ng 2002), Decision Trees (Sekine et al 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al 2002), Agent-based Approach (Ye et al 2002) and Support Vector Machines. Another related work is (Bikel et al, 1997) which used HMMs as part of its modelling for the name finding problem in information extraction. A common approach is to extract word-internal features from unknown words, for example suffix, capitalization, or punctuation features (Mikheev, 1997, Wacholder et al, 1997, Bikel et al, 1997). Our baseline name tagger is based on an HMM that generally follows the Nymble model (Bikel et al 1997). As hidden Markov models have been used both for name finding (Bikel et al (1997)) and tokenization (Cutting et al. In addition, an automatic named entity tagger (Bikel et al, 1997) was run on the sentences to map proper nouns to a small set of semantic classes.
Termight: Identifying And Translating Technical Terminology 1993). We that part of speech tagging and word alignment could have an important role in glossary construction for translation. Glossaries are extremely important for translation. How would Microsoft, or some other software vendor, want the term &quot;Character menu&quot; to be translated in their manuals? Technical terms are difficult for translators because they are generally not as familiar with the subject domain as either the author of the source text or the reader of the target text. In many cases, there may be a number of acceptable translations, but it is important for the sake of consistency to standardize on a single one. It would be unacceptable for a manual to use a variety of synonyms for a particular menu or button. Customarily, translation houses make extensive job-specific glossaries to ensure consistency and correctness of technical terminology for large jobs. A glossary is a list of terms and their translations.' We will subdivide the task of constructing a glossary into two subtasks: (1) generating a list of terms, and (2) finding the translation equivalents. The first task will be referred to as the monolingual task and the second as the bilingual task. How should a glossary be constructed? Translation schools teach their students to read as much background material as possible in both the source and target languages, an extremely time-consuming On the one hand, the linguistically-based or rule-based approaches use linguistic information such as PoS tags, chunk information, etc. to filter out stop words and restrict candidate terms to predefined syntactic patterns (Ananiadou, 1994), (Dagan and Church, 1994). Such a tool has proved extremely useful not only for translators, but also for bilingual lexicographers (Langlois, 1996) and terminologists (Dagan and Church, 1994). Our system is most similar to Termight (Dagan and Church, 1994) and TransSearch (Macklovitch et al, 2000).
Forest-Based Translation Among syntax-based translation models, the which takes as input a parse tree of the source sentence, is a promising direction being faster and simpler than its string-based counterpart. However, current tree-based systems suffer from a major drawback: they only use the 1-best parse to direct the translation, which potentially introduces translation mistakes due to parsing errors. We a that translates a packed forest of exponentially many parses, which encodes many more alternatives standard lists. Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. However, they are promising because the search space of translations is much larger than the typical N-best list (Mi et al, 2008). In addition, Mi et al (2008) have proposed a method for forest-to-string (F2S) translation using packed forests to encode many possible sentence interpretations. Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al, 2008), but we build ambiguity into the translation rule. Thus, high quality parsers are unavailable for many source languages of interest. Parse forests can be used to mitigate the accuracy problem, allowing the decoder to choose from many alternative parses, (Mi et al, 2008). In terms of formal similarity, Mi et al (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1 best parsing errors (as such, all derivations yield the same source string). An example derivation of tree-to-string translation (much simplified from Mi et al (2008)). At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule pattern matching (Mi et al, 2008). Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al 2008). For example, Quirk et al (2005) use features involving phrases and source side dependency trees and Mi et al (2008) use features from a forest of parses of the source sentence. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern matching (Mi et al, 2008).  Mi et al (2008) applied statistical machine translation to a source language parse forest, rather than to the 1-best parse. For example, Mi et al (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al, 2002) by including bilingual syntactic phrases in their forest-based system. We implemented the forest-to-string decoder described in (Mi et al, 2008) that makes use of forest based translation rules (Mi and Huang, 2008) as the baseline system for translating English HPSG forests into Japanese sentences. Mi et al (2008) give a detailed description of the two-step decoding process. Given a source parse forest and an STSG grammar G, we first apply the conversion algorithm proposed by Mi et al (2008) to produce a translation forest. However, when a pack forest encodes over 1M parses per sentence, the improvements are less significant, which echoes the results in (Mi et al, 2008). The first direct use of packed forest is proposed by Mi et al (2008). Following Mi et al (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al (2008), and then the decoder finds the best derivation on the lattice translation forest. For more detail, we refer to the algorithms of Mi et al (2008).
The Senseval-3 English Lexical Sample Task This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was orgaas part of the evaluation exercise. The task drew the participation of 27 teams from around the world, with a total of 47 systems. This was mainly because of their attested strength at earlier Senseval evaluations (Edmonds et al 2002, Mihalcea et al 2004) and mutual complementarity discovered by us (Saarikoski et al., 2007). In addition, it has been Senseval practice (Edmonds et al 2002, Mihalcea et al 2004) that words with great number of test instances tend to have an equally great number of training instances. The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al, 2004) (hereafter SE-3) and SemCor (Miller et al, 1993). At Senseval-3 (Mihalcea et al, 2004) the top systems were considered to have reached a ceiling, in terms of performance, at 72% for fine grained disambiguation and 80% for coarse grained. We employ supervised WSD systems ,since Senseval results have amply demonstrated that supervised models significantly outperform unsupervised approaches (see for instance the English lexical sample tasks results described by Mihalcea et al (2004)).  S3LS-best stands for the the winner of S3LS (Mihalcea et al, 2004), which is 8.3 points over our method. Results from the last edition of the Senseval competition (Mihalcea et al, 2004) have shown that, for supervised learning, the best accuracies are obtained with a combination of various types of features, together with traditional machine learning algorithms based on feature-value vectors, such as Support Vector Machines (SVMs) and Naive Bayes. In this paper we use an automatic method to map the induced senses to WordNet using hand-tagged corpora, enabling the automatic evaluation against available gold standards (Senseval 3 English Lexical Sample S3LS (Mihalcea et al, 2004)) and the automatic optimization of the free parameters of the method. We include three supervised systems, the winner of S3LS (Mihalcea et al, 2004), an in-house system (kn N-all, CITATION OMITTED) which uses optimized kn N, and the same in-house system restricted to bag-of-words features only (kn N-bow) ,i.e. discarding other local features like bi grams or trigrams (which is what most unsupervised systems do). Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). These approaches have shown good results; particularly those using supervised learning (see Mihalcea et al, 2004 for an overview of state-of the-art systems). WSD systems have generally been more successful in the disambiguation of nouns than other grammatical categories (Mihalcea et al, 2004). The experiments are performed on the set of ambiguous nouns from the SENSEVAL-3 English lexical sample evaluation (Mihalcea et al, 2004). Since the test data for the nouns of SENSEVAL-3 English lexical sample task (Mihalcea et al, 2004) were also drawn from BNC and represented a difference in domain from the parallel texts we used, we also expanded our evaluation to these SENSEVAL-3 nouns. Extracting Senses Preliminary experiments on 10 nouns of SensEval-3 English lexical-sample task (Mihalcea et al, 2004) (S3LS), suggested that our hyper graphs 415 are small-world networks, since they exhibited a high clustering coefficient and a small average path length. In this paper the relevance feedback approach described by Stevenson et al (2008a) is evaluated using three data sets: the NLM-WSD corpus (Weeber et al, 2001) which Stevenson et al (2008a) used for their experiments, the Senseval-3 lexical sample task (Mihalcea et al, 2004) and the coarse grained version of the SemEval English lexical sample task (Pradhan et al, 2007). In detail, we first mapped senses of ambiguous words, as defined in the gold-standard TWA (Mihalcea, 2003) and Senseval-3 lexical sample (Mihalcea et al, 2004) datasets (which we use for evaluation) onto their corresponding Chinese translations. For fine grained evaluation, we used Senseval-3 English lexical sample dataset (Mihalcea et al, 2004), which comprises 7,860 sense-tagged instances for training and 3,944 for testing, on 57 words (nouns, verbs and adjectives). Still, the performance is significantly lower than the score achieved by supervised systems, which can reach above 72% recall (Mihalcea et al, 2004).
Incremental Integer Linear Programming For Non-Projective Dependency Parsing Integer Linear Programming has recently been used for decoding in a number of probabilistic models in order to enforce global constraints. However, in certain applications, such as non-projective dependency parsing and machine translation, the complete formulation of the decoding problem as an integer linear program renders solving intractable. We present an approach which solves the problem incrementally, thus we avoid creating intractable integer linear programs. This approach is applied to Dutch dependency parsing and we show how the addition of linguistically motivated constraints can yield a significant improvement over stateof-the-art. For dependency parsing, McDonald and Pereira (2006) proposed a method which can incorporate some types of global features, and Riedel and Clarke (2006) studied a method using integer linear programming which can incorporate global linguistic constraints. However, work in dependency parsing (Riedel and Clarke, 2006) has demonstrated that it is possible to use ILP to perform efficient inference for very large programs when used in an incremental manner. However, recent work (Riedel and Clarke, 2006) has shown that even exponentially large decoding problems may be solved efficiently using ILP solvers if a Cutting-Plane Algorithm (Dantzig et al, 1954) is used. Riedel and Clarke (2006) describe ILP methods for the problem; Martins et al (2009) recently introduced alternative LP and ILP formulations. Riedel and Clarke (2006) tackled the MAP problem for dependency parsing by an incremental approach that starts with a relaxation of the problem, solves it, and adds additional constraints only if they are violated. ILPs have since been used successfully in many NLP applications involving complex structures Punyakanok et al (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al (2009) for dependency parsing and several others. Another attempt to overcome the problem of complexity with ILP models is described in (Riedel and Clarke, 2006) (dependency parsing). In contrast, generic NP-hard solution techniques like Integer Linear Programming (Riedel and Clarke, 2006) know nothing about optimal substructure. Riedel and Clarke (2006) showed that dependency parsing can be framed as Integer Linear Program (ILP), and efficiently solved using an off-the shelf optimizer if a cutting plane approach is used. Compared to the representation Riedel and Clarke (2006), this bound has the benefit a small polynomial number of constraints. Our formulation is inspired by Martins et al 2009, and hence uses fewer constraints than Riedel and Clarke (2006). We suggest scaling techniques that allow to optimally learn such graphs over a large set of typed predicates by first decomposing nodes into components and then applying incremental ILP (Riedel and Clarke, 2006). Another solution for scaling ILP is to employ incremental ILP, which has been used in dependency parsing (Riedel and Clarke, 2006). For instance, to improve the accuracy further, more global constrains capturing the subcategorization correct could be integrated as in Riedel and Clarke (2006). Riedel and Clarke (2006) cast dependency parsing as an ILP, but efficient formulations remain an open problem. If it is extended to labeled parsing (a straightforward extension), our formulation fully subsumes that of Riedel and Clarke (2006), since it allows using the same hard constraints and features while keeping the ILP polynomial in size. Rather than adding exponentially many constraints, one for each potential cycle (like Riedel and Clarke, 2006), we equivalently replace condition 3 by 3. all of them state-of-the-art parsers based on non-arc-factored models: the second order model of McDonald and Pereira (2006), the hybrid model of Nivre and McDonald (2008), which combines a (labeled) transition-based and a graph based parser, and a refinement of the latter, due to Martins et al (2008), which attempts to approximate non-local features. We did not reproduce the model of Riedel and Clarke (2006) since the latter is tailored for labeled dependency parsing; however, experiments reported in that paper for Dutch (and extended to other languages in the CoNLL-X task) suggest that their model performs worse than our three baselines. The average runtime (across all languages) is 0.632 seconds per sentence, which is in line with existing higher-order parsers and is much faster than the runtimes reported by Riedel and Clarke (2006). The standard approach to framing dependency parsing as an integer linear program was introduced by (Riedel and Clarke, 2006), who converted the MST parser of (McDonald et al 2005) to use ILP for inference. The key idea is to build a complete graph consisting of tokens of the sentence where each edge is weighted by a learned scoring function.
KenLM: Faster and Smaller Language Model Queries We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs. The structure uses linear probing hash tables and is designed for speed. Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline. Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems. This paper describes the several performance techniques used and presents benchmarks against alternative implementations. We used common tools for phrase-based translation Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments. The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run. While this increases the number of LM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states. Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference. The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011). Inference was carried out using the language modeling library described by Heafield (2011). We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011). The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua. Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets. With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002). This was used to create a KenLM (Heafield, 2011). In the Opinum system we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application. Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights. For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011). Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011). n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3. Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011). For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011). For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing.
Automatic Error Detection In The Japanese Learners' English Spoken Data This paper describes a method of detecting grammatical and lexical errors made by Japanese learners of English and other techniques that improve the accuracy of error detection with a limited amount of training data. In this paper, we demonstrate to what extent the proposed methods hold promise by conducting experiments using our learner corpus, which contains information on learners’ errors. Izumi et al (2003) and (2004) used error annotated transcripts of Japanese speakers in an interview-based test of spoken English to train a maximum entropy classifier (Ratnaparkhi, 1998) to recognize 13 different types of grammatical and lexical errors, including errors involving prepositions. For example, (Izumi et al, 2003) reported error rates for English prepositions that were as high as 10% in a Japanese learner corpus. (Izumi et al., 2003) and (Izumi et al, 2004) used an ME approach to classify different grammatical errors in transcripts of Japanese interviews. For example, in the Japanese Learners of English corpus (Izumi et al., 2003), errors related to verbs are among the most frequent categories. A maximum entropy model, using lexical and POS features, is trained in (Izumi et al, 2003) to recognize a variety of errors. Izumi et al (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. False starts and disfluencies were then cleaned up, and grammatical mistakes tagged (Izumi et al, 2003). The usage of articles has been found to be the most frequent error class in the JLE corpus (Izumi et al, 2003). In the future, we would like to search for more salient features through a careful study of non-native errors, using error-tagged corpora such as (Izumi et al., 2003).  We based our error annotation scheme on that used in the NICT JLE corpus (Izumi et al, 2003a), whose detailed description is readily available, for example, in Izumi et al (2005).  The method (Izumi et al, 2003) aims to detect omission-type and replacement-type errors and transformation-based leaning is employed in (Shi and Zhou, 2005) to learn rules to detect errors for speech recognition outputs. Izumi et al (2003) train a maximum entropy model on error-tagged data from the Japanese Learners of English corpus (JLE, (Izumi et al., 2004)) to detect 8 error types in the same corpus.
Parsing The WSJ Using CCG And Log-Linear Models This paper describes and evaluates log-linear parsing models for Combinatory Categorial A parallel implementation of algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation. We also develop a new efficient parsing for maximises expected recall of dependencies. We compare models use all including nonstandard derivations, with normal-form models. The performances of the two models are comparable and the results are competitive with ex The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. The parser used in this paper is described in Clark and Curran (2004b).  In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. The parsing results in Clark and Curran (2004b) rely on a super tagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). However, the scores in Clark and Curran (2004b) give an indication of how super tagging accuracy corresponds to overall dependency recovery.  In order to access categorial and structural information, we used the C&C toolkit (Clark and Curran, 2004).  Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train. Clark and Curran (2004b) describes the CCG parser.   In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. We use the same feature representation (x, y) as in Clark and Curran (2004b), to allow comparison with the log-linear model.  We applied the same normal-form restrictions used in Clark and Curran (2004b): categories can only combine if they have been seen to combine in Sections 2-21 of CCGbank, and only if they do not violate the Eisner (1996a) normal-form constraints. In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of BFGS, to solve this problem, but need up to 20 GB of RAM. Following Clark and Curran (2004b), accuracy is measured using F-score over the gold standard predicate-argument dependencies in CCG bank. 
Offline Strategies For Online Question Answering: Answering Questions Before They Are Asked Recent work in Question Answering has focused on web-based systems that answers using simple lexicosyntactic patterns. We present an alternative strategy in which patterns are used to extract highly precise relational information offline, creating a data repository that is used to efficiently answer questions. We evaluate our strategy on a challenging subset of questions, i.e. “Who is ...” questions, against a state of the art web-based Question Answering system. Results indicate that the extracted relations answer 25% more questions correctly and do so three orders of magnitude faster than the state of the art system. (Fleischman et al., 2003) also propose a supervised algorithm that uses part of speech patterns and a large corpus to extract semantic relations for Who-is type questions. We also made use of the person-name/instance pairs automatically extracted by Fleischman et al (2003). Mann (2002) and Fleischman et al (2003) used part of speech patterns to extract a subset of hyponym relations involving proper nouns. Some of these patterns are similar to the ones discovered by Hearst (1992) while other patterns are similar to the ones used by Fleischman et al (2003). Following Fleischman et al (2003), we select the 50 definition questions from the TREC2003 (Voorhees 2003) question set.  We compared our system with the concepts in WordNet and Fleischman et al's instance/concept relations (Fleischman et al 2003). This approach is similar in spirit to the work reported by Fleischman et al (2003) and Mann (2002), except that our system benefits from a greater variety of patterns and answers a broader range of questions. The precision of the extracted information can be improved significantly by using machine learning methods to filter out noise (Fleischman et al, 2003). The recall problem is usually addressed by increasing the amount of text data for extraction (taking larger collections (Fleischman et al, 2003)) or by developing more surface patterns (Soubbotin and Soubbotin, 2002). Fleischman et al (2003) focus on the precision of the information extracted using simple part-of-speech patterns. To get a clear picture of the impact of using different information extraction methods for the offline construction of knowledge bases, similarly to (Fleischman et al, 2003), we focused only on questions about persons, taken from the TREC8 through TREC 2003 question sets. This confirms the results of Fleischman et al (2003): shallow methods may benefit significantly from the post-processing. In our future work we plan to investigate the effect of more sophisticated and, probably, more accurate filtering methods (Fleischman et al, 2003) on the QA results. After that, several million instances of people, locations, and other facts were added (Fleischman et al, 2003). In particular, we use the name/instance lists described by (Fleischman et al., 2003) and available on Fleischman's web page to generate features between names and nominals (this list contains noU pairs mined from pI GBs of news data). Fleischman et al (2003) describe a dataset of concept-instance pairs extracted automatically from a very large corpus of newspaper articles. The goal of this study has been to automatically extract a large set of hyponymy relations, which play a critical role in many NLP applications, such as Q&A systems (Fleischman et al, 2003).
Fully Automatic Lexicon Expansion For Domain-Oriented Sentiment Analysis This paper proposes an unsupervised lexicon building method for the detecof which convey positive or negative aspects in a specific domain. The lexical entries to be acare called the minimum human-understandable syntactic structures that specify the polarity of clauses. As a clue to obtain candidate atoms, we use the tendency for same polarities to appear successively in contexts. Using the overall density and precision of coherency in the corpus, the statistical estimation picks up appropriate polar atoms among candidates, without any manual tuning of the threshold values. The experimental results show that the precision of polarity assignment with the automatically acquired lexicon was 94% on average, and our method is robust for corpora in diverse domains and for the size of the initial lexicon. Various attempts have been made to incorporate discourse relations into sentiment analysis: Pang and Lee (2004) explored the consistency of subjectivity between neighboring sentences; Mao and Lebanon (2007), McDonald et al (2007), and Tackstrom and McDonald (2011a) developed structured learning models to capture sentiment dependencies between adjacent sentences; Kanayama and Nasukawa (2006) and Zhou et al (2011) use discourse relations to constrain two text segments to have either the same polarity or opposite polarities; Trivedi and Eisenstein (2013) and Lazaridou et al (2013) encode the discourse connectors as model features in supervised classifiers. Kanayama and Nasukawa (2006) posited that polar clauses with the same polarity tend to appear successively in contexts. As mentioned in Section 2, Kanayama and Nasukawa (2006) validated that polar text units with the same polarity tend to appear together to make contexts coherent. Kanayama and Nasukawa (2006) bootstrap subjectivity lexicons for Japanese by generating subjectivity candidates based on word co-occurrence patterns. In (Kanayama and Nasukawa, 2006), the authors propose an algorithm to automatically expand an initial opinion lexicon based on context coherency, the tendency for same polarities to appear successively in contexts. Similarly, Andreevskaia and Bergler (2006) used WordNet to expand seed lists with fuzzy sentiment categories, in which words could be more central to one category than the other. Finally, Kanayama and Nasukawa (2006) used syntactic features and context coherency, defined as the tendency for same polarities to appear successively, to acquire polar atoms. The second type of relations are word-to-expression relations: e.g., some words appear in expressions that take on a variety of polarities, while other words are associated with expressions of one polarity class or another. In relation to previous research, analyzing word-to-word (intra-expression) relations is most related to techniques that determine expression-level polarity in context (e.g., Wilson et al (2005)), while exploring word-to-expression (inter-expression) relations has connections to techniques that employ more of a global-view of corpus statistics (e.g., Kanayama and Nasukawa (2006)). While most previous research exploits only one or the other type of relation, we propose a unified method that can exploit both types of semantic relation, while adapting a general purpose polarity lexicon into a domain specific one. For example, Hatzivassiloglou (Hatzivassiloglou and McKeown, 1997) and Kanayama (Kanayama and Nasukawa, 2006) used conjunction rules to solve this problem from large domain corpora. (Kanayama and Nasukawa, 2006) reported that it was appropriate in 72.2% of cases. Kanayama and Nasukawa used both intra and inter-sentential co-occurrence to learn polarity of words and phrases (Kanayama and Nasukawa,2006). Kanayama and Nasukawa (2006) use syntactic features and context coherency, the tendency for same polarities to appear successively, to acquire polar atoms. Other related work is concerned with subjectivity analysis. Kanayama and Nasukawa (2006) used syntactic features and context coherency, defined as the tendency for same polarities to appear successively, to acquire polar atoms. Kanayama and Nasukawa (2006) improved this work by using the idea of coherency. In addition to individual seed words, Kanayama and Nasukawa (2006) used more complicated syntactic patterns that were manually created.  Next, the target-specific polarity of adjectives is determined using Hearst-like patterns. Kanayama and Nasukawa (2006) introduce polar atoms: minimal human-understandable syntactic structures that specify polarity of clauses. More advanced methods such as (Kanayama and Nasukawa, 2006) adopt domain knowledge by extracting sentiment words from the domain-specific corpus.
Learning Subjective Language Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article. Wiebe et al (2004) show that low-frequency words and some collocations are a good indicators of subjectivity. Hedging is sometimes classed under the umbrella concept of subjectivity, which covers a variety of linguistic phenomena used to express differing forms of authorial opinion (Wiebe et al, 2004). This phenomenon, together with others used to express forms of authorial opinion, is often classified under the notion of subjectivity (Wiebe et al, 2004), (Shanahan et al, 2005). In contrast to the findings of Wiebe et al ((Wiebe et al, 2004)), who addressed the broader task of subjectivity learning and found that the density of other potentially subjective cues in the context benefits classification accuracy, we observed that the co-occurence of speculative cues in a sentence does not help in classifying a term as speculative or not. Following Wiebe et al (2004) we apply a unique feature. On the other hand, Wiebe et al (2004) have noted that hap ax legomena (terms that only appear once in a collection of texts) are good signs for detecting subjectivity. Accurate automatic analysis of these aspects of language will augment existing research in the fields of sentiment (Pang et al, 2002) and subjectivity analysis (Wiebe et al, 2004), but assessing the usefulness of analysis algorithms leveraging the Appraisal framework will require test data. We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al, 2004), the Polarity data set created by (Pang and Lee, 2004), and the MPQA data set created by (Wiebe et al, 2005). This paper is also not concerned with subjectivity (Wiebe et al, 2004), the nature of the proposition p (statement about interior world or external world) is not of interest, only whether the writer wants the reader to believe the writer believes p. Subjectivity lexicon (Wiebe et al, 2004) is a resource that annotates words with tags like parts-of-speech, prior polarity, magnitude of prior polarity (weak/strong), etc. For example, detecting subjective sentences, expressions, and other opinionated items in documents representing certain press categories (Wiebe et al, 2004) and measuring strength of subjective clauses (Wilson et al, 2004). Wiebe et al (2004) focused on the detection of subjective language such as opinions, evaluations, or emotions in text. More generally, (Wiebe et al 2004) and subsequent work focused on the analysis of subjective language in narrative text, primarily news. There have been attempts on tackling this so-called document-level subjectivity classification task, with very encouraging results (see Yu and Hatzivassiloglou (2003) and Wiebe et al (2004) for details). Wiebe et al (2004) proposed that whether a sentence is subjective or objective should be discriminated according to the adjectives in it. We believe that this relaxation can be done in that particular case, as adjectives are much more likely to convey opinions a priori than verbs (Wiebe et al 2004).  First, we investigate the utility of applying a UNIQUE feature (Wiebe et al, 2004) where low frequency words below a threshold are replaced with the token UNIQUE. This group includes two features that have been employed in various SSA studies. Unique: Following Wiebe et al (2004), we apply a UNIQUE (Q) feature: We replace low frequency words with the token UNIQUE. In addition to Appraisal Theory, subjectivity annotation of text in context has also been performed in Yu and Hatzivassiloglou (2003), Bruce and Wiebe (1999), and Wiebe et al (2004).
Automatic Tagging Of Arabic Text: From Raw Text To Base Phrase Chunks  Both Diab et al (2004) and Habash and Rambow (2005) use support-vector machines with local features; the former for tokenization, POS tagging, and base phrase chunking; the latter for full morphological disambiguation. We use AMIRA (Diab et al, 2004) to annotate Arabic and Tree Tagger (Schmid, 1994) to annotate German. Arabic text is then segmented with AMIRA (Diab et al, 2004) according to the ATB scheme. This scheme is compatible with the chunker we use (Diab et al, 2004). For chunking Arabic, we use the AMIRA (ASVMT) toolkit (Diab et al, 2004). In supervised POS tagging, (Diab et al, 2004) achieves high accuracy on MSA with the direct application of SVM classifiers. A comparable work was done by (Diab et al, 2004), where a POS tagging method for Arabic is also discussed. In (Lee et al, 2003), (Diab et al, 2004) and (Habash and Rambow, 2005) three supervised segmentation methods are introduced. In the next subsections we will shortly describe the method of (Diabetal., 2004). (Diab et al, 2004) propose solutions to word segmentation and POS Tagging of Arabic text. L1 uses the simple POS tags advocated by Habash and Rambow (2005) (15 tags); while L2 uses the reduced tag set used by Diab et al (2004) (24 tags). Khoja (2001) first introduced a tagger for Arabic, which has 131 tags, but subsequent work has collapsed the tag set to simplify tagging (Diab et al, 2004). Recently, Diab et al (2004) used SVM based approach for Arabic text chunking. It was also successfully used in Arabic (Diab et al, 2004). The data facilitates machine learned part-of-speech taggers, tokenizers, and shallow parsing units such as chunkers, as exemplified by Diab et al (2004). Diab et al (2004) describe a part-of-speech tagger based on support vector machines that is trained on tokenized data (clitics are separate tokens), reporting a tagging accuracy of 95.5%. Mansour et al (2007) combine a lexicon-based tagger (such as MorphTagger (Bar-Haim et al, 2005)), and a character-based tagger (such as the data-driven ArabicSVM (Diab et al, 2004)), which includes character features as part of its classification model, in order to extend the set of analyses suggested by the analyzer. Diab et al. (2004) used a Support Vector Machine, SVM-based tagger, trained on the Arabic Penn Treebank 1 to tokenize, POS tag, and annotate Arabic base phrases. The tokenization was done using the ASVM Toolkit (Diab et al, 2004). As reported by Habash and Rambow, the first work on Arabic tagging which used a corpus for training and evaluation was the work of Diab et al (2004).
Thumbs Up? Sentiment Classification Using Machine Learning Techniques We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging. For example, (Pang et al, 2002) collected reviews from a movie database and rated them as positive, negative, or neutral based on the rating (e.g., number of stars) given by the reviewer. It is the case in many sentiment analysis corpora that only positive and negative instances are included, e.g., (Pang et al, 2002). Sentiment classification is a special task of text classification whose objective is to classify a text according to the sentimental polarities of opinions it contains (Pang et al, 2002), e.g., favorable or unfavorable, positive or negative. For evaluation, we use five sentiment classification datasets, including the widely-used movie review dataset [MOV] (Pang et al, 2002) as well as four datasets that contain reviews of four different types of product from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (Blitzer et al, 2007). Automatic identification of subjective content of ten relies on word indicators, such as unigrams (Pang et al, 2002) or predetermined sentiment lexica (Wilson et al, 2005).  For the purpose of this work, we follow the definition of Pang et al (2002) and Turney (2002) and consider a binary classification task for output labels as positive and negative. The low recall for adjective POS based synsets can be detrimental to classification since adjectives are known to express direct sentiment (Pang et al, 2002). We choose to use SVM since it performs the best for sentiment classification (Pang et al, 2002). Following standard practice in sentiment analysis (Pang et al, 2002), the input to SVMlight consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors. Much of this research explores sentiment classification, a text categorization task in which the goal is to classify a document as having positive or negative polarity (e.g., Das and Chen (2001), Pang et al (2002), Turney (2002), Dave et al (2003), Pang and Lee (2004)). Each has proven to be effective in previous sentiment analysis studies (Pang et al, 2002), so as this experiment is rooted in sentiment classification, these methods were also assumed to perform well in this cross-discourse setting. These results support experiments carried out for topic based classification using Bayesianclassifiers by McCallum and Nigam (1998), but differs from sentiment classification results from Pang et al (2002) that suggest that term-based models perform better than the frequency-based alternative. Pang et al (2002) applied these classifiers to the movie review domain, which produced good results.  Previouswork has used machine learning techniques to identify positive and negative movie reviews (Pang et al., 2002). Pang et al (2002) and Turney (2002) classified sentiment polarity of reviews at the document level. Although the discriminative model (e.g., SVM) is proven to be more effective on unigrams (Pang et al, 2002) for its ability of capturing the complexity of more relevant features, WR features are more inclined to work better in the generative model (e.g., NB) since the feature independence assumption holds well in this case. We do this because we already have enough training data, so there is no need to resort to cross-validation (Pang et al, 2002). Applications of text categorization, such as sentiment classification (Pang et al, 2002), are now required to run on multiple languages.
cdec: A Decoder Alignment and Learning Framework for Finite-State and Context-Free Translation Models present an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms. From this unified representation, the decoder can not only the 1or translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. For translation experiments, we used cdec (Dyer et al, 2010), a fast implementation of hierarchical phrase-based translation models (Chiang, 2005), which represents a state-of-the-art translation system. Our implementation is mostly in Python on top of the cdec system (Dyer et al, 2010) via the pycdec interface (Chahuneau et al, 2012). The weights of the log-linear translation models were tuned towards the BLEU metric on development data using cdec's (Dyer et al, 2010) implementation of MERT (Och, 2003). We implement Linear CP (LCP) on top of Cdec (Dyer et al, 2010), a widely-used hierarchical MT system that includes implementations of standard CP and FCP algorithms. In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG). Finally, the cdec decoder (Dyer et al, 2010) includes a grammar extractor that performs well only when all rules can be held in memory. We use the cdec decoder (Dyer et al, 2010) with default settings for this purpose. Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al, 2010).  We implemented UD on top of a widely-used HMT open-source system, cdec (Dyer et al, 2010). We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner.  In our work, we use hierarchical phrase-based translation (Chiang, 2007), as implemented in the cdec framework (Dyer et al, 2010). The work reported in this paper was carried out while the author was at the University of Cambridge.It has been noted that line optimisation over a lattice can be implemented as a semi-ring of sets of linear functions (Dyer et al, 2010). Grammars were extracted from the resulting parallel text and used in our hierarchical phrase-based system using cdec (Dyer et al, 2010) as the decoder. We have evaluated the one-translation-per-discourse feature using the cdecMT system (Dyer et al, 2010). An efficient implementation that integrates well with the open source cdec SMT system (Dyer et al., 2010). We used cdec (Dyer et al, 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al, 2002) on the NIST MT06 corpus. In all experiments, our MT system learned a synchronous context-free grammar (Chiang, 2007), using GIZA++ for word alignments, MIRA for parameter tuning (Crammer et al, 2006) ,cdec for decoding (Dyer et al, 2010), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation. To date, several open-source SMT systems (based on either phrase based models or syntax-based models) have been developed, such as Moses (Koehn et al, 2007), Joshua (Li et al, 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al, 2010), cdec (Dyer et al, 2010), Jane (Vilar et al, 2010) and SilkRoad, and offer good references for the development of the NiuTrans toolkit.
Two-Level Morphology with Composition Lauri Karttunen, Ronald M. Kaplan, and Annie Zaenen Xerox Palo Alto Research Center Center for the Study of language and Information StanJbrd University 1.  Currently, the most attractive architecture is as in Figure 1 (Karttunen et al (1992)): 1284 String alternation rules Formative lexicon] I~ Compilation &apos; 1 Compilation (Lexicon FSA) (Rule FSTs) Lexicalransducel2, j Figure l: Architecture of Two-Level Morphology. Earlier examples of such approaches include lexc (Karttunen et al, 1992), FASTR (Jacquemin,2001), HABIL (Alegria et al, 2004), and Mul ti flex discussed below. See also (Savary, 2008) for a detailed contrastive analysis of Multiflex with respect to 10 other systems for a lexical description of MWUs in different languages such as (Karttunen et al, 1992), (Jacquemin, 2001), and (Alegria et al, 2004). A second important advance in computational morphology was the recognition by Karttunen et al (1992) that a cascade of composed FSTs could implement the two-level model. related forms of the word pair using a lexical transducer for English (Karttunen et al, 1992). It was observed somewhere around 1990 at Xerox that the rule sets may be composed with the lexicon transducers in an efficient way and that the resulting transducer was roughly similar in size as the lexicon transducer itself (Karttunen et al, 1992).
Introduction To The CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition  For comparison, we also computed two baselines: one in which each character is labeled with its most frequent label (Baseline1 in Table 2), and one in which each entity that was seen in training data is labeled with its most frequent classification (Baseline2 in Table 2 this baseline is computed using the software provided with the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003)).  In addition to the conceptual simplicity of this approach, it also seems to perform better experimentally (Tjong Kim Sang and De Meulder, 2003). The first source is the CoNLL 2003 shared task date (Tjong Kim Sang and De Meulder, 2003) and the second source is the 2004 NIST Automatic Content Extraction (Weischedel, 2004). Named entities with (Chieu and Ng, 2003), based on Maximum-Entropy classifiers, and following the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003). Supervised NE Tagging has been studied extensively over the past decade (Bikel et al 1999, Baluja et. al. 1999, Tjong Kim Sang and De Meulder 2003). The annotation is distributed in the standard column based BIO format applied for e.g. CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) and JNLPBA (Kim et al, 2004) data, among other established datasets. The IOB2 strategy, which is very popular, having been used in public challenges such as those of CoNLL (Tjong Kim Sang and De Meulder, 2003) and JNLPBA (Kim et al, 2004), has been found to be indeed the best of all established tagging strategies. Named Entities predicted with the Maximum Entropy based tagger of Chieu and Ng (2003). The tagger follows the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003), and thus is not developed with WSJ data. We benchmark the performance of our baseline MaxEnt classifier using the feature set from Section 5.1 (MaxEnt-A henceforth) on the CoNLL 2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003), the de-facto standard for evaluating coarse-grained NERC systems. This scheme was initially introduced in CoNLL's (Tjong Kim Sang, 2002a) and (Tjong Kim Sang and De Meulder, 2003) NER competitions, and we decided to adapt it for our experimental work. Entity tagging has been thoroughly addressed by many statistical machine learning techniques, obtaining greater than 90% F1 on many datasets (Tjong Kim Sang and De Meulder, 2003). We consider the problem of named-entity recognition (NER) and use the English data from the CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003). The ACE data was morphologically annotated with a tokenizer based on manual rules adapted from the one used in CoNLL (Tjong Kim Sang and De Meulder, 2003), with TnT 2.2, a trigram POS tagger based on Markov models (Brants, 2000), and with the built-in WordNet lemmatizer (Fellbaum, 1998). Finaly, combining models has been a successful way of achieving good results, such as those of Florian et al (2003) who had the top performance in the named entity recognition shared task of CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003). Tokenisation and sentence splitting is followed by part-of speech tagging with the Maximum Entropy Markov Model (MEMM) tagger developed by Curran and Clark (2003) (here after referred to as C&C) for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003), trained on the MedPost data (Smith et al, 2004). All our results for NER are reported on the CoNLL-2003 shared task dataset (Tjong Kim Sangand De Meulder, 2003). language newspaper domain (English data set of the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003)).  The fourth type, called miscellaneous, was introduced in the CoNLL NER tasks in 2002 (Tjong Kim Sang, 2002) and 2003 (Tjong Kim Sang and De Meulder, 2003), and includes proper names falling outside the three classic types.
An Algorithm For Finding Noun Phrase Correspondences In Bilingual Corpora The paper describes an algorithm that employs English and French text taggers to associate noun phrases in an aligned bilingual corpus. The taggers provide part-of-speech categories which are used by finite-state recognizers to extract simple noun phrases for both languages. Noun phrases are then mapped to each other using an iterative re-estimation algorithm that bears similarities to the Baum-Welch algorithm which is used for training the taggers. The algorithm provides an alternative to other approaches for finding word correspondences, with the advantage that linguistic structure is incorporated. Improvements to the basic algorithm are described, which enable context to be accounted for when constructing the noun phrase mappings.  For instance, Kupiec (1993) uses statistical techniques and extracts bilingual noun phrases from parallel corpora tagged with terms. There is a long tradition of research into bilingual terminology extraction (Kupiec, 1993), (Gaussier, 1998). Kupiec (1993) applied finite state transducer in his noun phrases recogniser for both English and French. Kupiec (1993) attempted to find noun phrase correspondences in parallel corpora using part-of-speech tagging and noun phrase recognition methods. Kupiec (1993) also briefly mentions the use of finite state NP recognizers for both English and French to prepare the input for a program that identified the correspondences between NPs in bilingual corpora, but he does not directly discuss their performance. For example, Kupiec (1993) presented a method for finding translations of whole noun phrases. Bound-length N-gram correspondences include (Kupiec, 1993) where NP recognizers are used to extract translation units and (Smadja et al, 1996) which uses the Extract system to extract collocations. We informally evaluated the MWE extraction tool following Kupiec (1993) by manually inspecting the mapping of the 100 most frequent terms.  Kupiec proposes an Mgorithm for finding noun phrases in bilingual corpora (Kupiec, 1993). The plausible hypothesis that parallel sentences containing corresponding linguistic expressions is the major premise in Kupiec (1993). In Kupiec (1993) and Yamamoto (1993), term and phrase extraction is applied to both of parallel texts. Previous works that focus on multi-word translation correspondences from parallel corpora include noun phrase correspondences (Kupiec, 1993), fixed/flexiblecollocations (Smadja et al, 1996), n-gram word sequences of arbitrary length (Kitamura and Matsumoto, 1996), non-compositional compounds (Melamed, 2001), captoids (Moore, 2001), and named entities. Kupiec (1993) focuses on noun-phrase translations only, Smadja et al (1996) limits to find French translation of English collocation identified by his Xtract system, and Kitamura and Matsumoto (1996) can exhaustively enumerate only rigid word sequences. Most bilingual terminology extraction systems first identify candidate terms in the source language based on predefined source patterns, and then select translation candidates for these terms in the target language (Kupiec, 1993). Part-of-speech taggers are used in a few applications, such as speech synthesis (Sproat et al, 1992) and question answering (Kupiec, 1993b).  
Automatic Acquisition of Domain Knowledge for Information Extraction Roman Yangarber, Ralph Grishman Past Tapanainen Courant  Inst i tute of Conexor oy Mathemat ica l  Sciences Helsinki, F in land New York University {roman [ grishman}@cs, nyu.  On the one hand machine learning is used to automate as much as possible the tasks an IE expert would perform in application development (Cardie 1997) (Yangarber et al 2000). learned, otherwise go to step 4 Previous algorithms which use this approach include those described by Yangarber et al (2000) and Stevenson and Greenwood (2005). The extraction patterns used by both Yangarber et al (2000) and Stevenson and Greenwood (2005) were based on SVO tuples extracted from dependency trees. Yangarber et al (2000) suggested a method where patterns were compared based on their distribution across documents in a corpus. Yangarber et al (2000) proposed an algorithm for learning extraction patterns for a small number of examples which greatly reduced the burden on the application developer and reduced the knowledge acquisition bottleneck. Yangarber et al (2000) chose an approach motivated by the assumption that documents containing a large number of patterns already identified as relevant to a particular IE scenario are likely to contain further relevant patterns. This approach has been shown to successfully acquire useful extraction patterns which, when added to an IE system, improved its performance (Yangarber et al, 2000). Architecture This architecture has been inspired by several existing seed-oriented minimally supervised ma chine learning systems, in particular by Snowball (Agichtein and Gravano, 2000) and ExDisco (Yangarber et al, 2000). ExDisco (Yangarber et al,2000) uses a bootstrapping mechanism to find new extraction patterns using unannotated texts and some seed patterns as the initial input. For example, the AutoSlog system (Riloff, 1993) uses pat terns which match certain grammatical categories, mainly nouns and verbs, in phrase chunked text while Yangarber et al (2000) use subject-verb object tuples derived from a dependency parse. To reduce the knowledge engineering burden on the user in constructing and porting an IE system, unsupervised learning has been utilized ,e.g. Riloff (1996), Yangarber et al (2000), and Sekine (2006). Bootstrapping approaches are employed in (Riloff, 1996), (Yangarber et al, 2000), (Yangarber, 2003), and (Stevenson and Greenwood, 2005) in order to find IE patterns for domain-specific event extraction. For example, EXDISCO (Yangarber et al., 2000) used Wall Street Journal articles for training. meets the density criterion (as defined in (Yangarber et al, 2000)). AutoSlog TS, does not require a pre-annotated corpus, but does require one that has been split into subsets that are relevant vs. non-relevant subsets to the scenario. (Yangarber et al, 2000) attempts to find extraction patterns, without a pre-classified corpus, starting from a set of seed patterns. We first present the basic algorithm for pattern acquisition, similar to that presented in (Yangarber et al., 2000).  For an indirect evaluation of the quality of the learned patterns, we employ the text-filtering evaluation strategy, as in (Yangarber et al, 2000). Information Extraction (IE) systems typically use extraction patterns (e.g., Soderland et al (1995), Riloff (1996), Yangarber et al (2000), Califf and Mooney (2003)) or classifiers (e.g., Freitag (1998), Freitag and McCallum (2000), Chieu et al (2003), Bunescu and Mooney (2004)) to extract role fillers for events. We have chosen this evaluation strategy because this indirect approach was shown to correlate well with a direct evaluation, where the learned patterns were used to customize an IE system (Yangarberet al, 2000).
Trainable Methods For Surface Natural Language Generation We present three systems for surface natural language generation that are trainable from annotated corpora. The first two systems, called NLG1 and NLG2, require a corpus marked only with domainspecific semantic attributes, while the last system, called NLG3, requires a corpus marked with both semantic attributes and syntactic dependency information. All systems attempt to produce a grammatical natural language phrase from a domain-specific semantic representation. NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase. The systems NLG2 and NLG3 learn to determine both the word choice and the word order of the phrase. We present experiments in which we generate phrases to describe flights in the air travel domain. More recently, context-based models of disambiguation have been shown to represent significant improvements over the baseline (Bangalore and Rambow, 2000), (Ratnaparkhi, 2000). In the case of Ratnaparkhi's generator for flight information in the air travel domain (Ratnaparkhi, 2000), the transformation algorithm is trivial as the generator uses the corpus itself (annotated with semantic information such as destination or flight number) as input to a surface realizer with an n-gram model of the domain, along with a maximum entropy probability model for selecting when to use which phrase. The likelihood of realisations given concepts or semantic representations has been modeled directly, but is probably limited to small-scale and specialised applications: summarisation construed as term selection and ordering [Witbrock and Mittal, 1999], grammar-free stochastic surface realisation [Oh and Rudnicky, 2000], and surface realisation construed as attribute selection and lexical choice [Ratnaparkhi, 2000]. Some of the above papers compare the purely statistical methods to other machine learning methods such as memory-based learning and reinforcement learning. [Ratnaparkhi, 2000] describes a sentence realizer that had been trained on a domain-specific corpus (in the air travel domain) augmented with semantic attribute value pairs. Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. Our work is more related to Ratnaparkhi (2000) in the sense that we also use a large collection of generation templates for surface realization, but still distinct in that we intend to generate text from minimal input. An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain. Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). A number of statistical surface realizers have been described, notably the FERGUS (BangaloreandRambow, 2000) and HALogen systems (LangkildeGeary, 2002), as well as experiments in (Ratnaparkhi, 2000). More generally, the NLG problem of non-deterministic decision making has been addressed from many different angles, including PENMAN-style choosers (Mann and Matthiessen,1983), corpus-based statistical knowledge (Langkilde and Knight, 1998), tree-based stochastic models (Bangalore and Rambow, 2000), maximum entropy based ranking (Ratnaparkhi, 2000), combinatorial pattern discovery (Duboue and McKeown, 2001), instance-based ranking (Varges, 2003), chart generation (White, 2004), planning (Koller and Stone, 2007), or probabilistic generation spaces (Belz, 2008) to name just a few.
A Semantic-Head-Driven Generation Algorithm For Unification-Based Formalisms We present an algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applicable. In particular, unlike an Earley deduction generator (Shieber, 1988), it allows use of semantically nonmonotonic grammars, yet unlike topdown methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion. The infinite application of grammar rules is a common problem of the existing top-down unification-based generators (Shieber et al, 1989). One of the termination problems Shieber et al (1989) pointed out is in the left-recursive rules. Though the restriction can not be applied to languages like Dutch (Shieber et al, 1989), the limitation is irrelevant to our purpose (translation between Japanese and English). The actual realization of the component is based on a constraint-based inheritance algorithm that follows the example of PATR-II (Shieber et al, 1989). The lexicon match is not based on direct unification of the target phrase's semantics with that of its head, a fundamental requirement of the bottom-up head-driven algorithm of Shieber et al (1989) and Van Noord (1990).
An Unsupervised Method For Detecting Grammatical Errors We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora. The system was developed and tested using essay-length responses to prompts on the Test of English as a Foreign Language (TOEFL). The errorrecognition system, ALEK, performs with about 80% precision and 20% recall. For instance, Chodorow and Leacock (2000) point out that the word concentrate is usually used as a noun in a general corpus whereas it is a verb 91% of the time in essays written by non-native learners of English. Chodorow and Leacock (2000) try to identify errors on the basis of context, as we do here, and more specifically a 2 word window around the word of interest, from which they consider function words and POS tags. N-gram-based approaches to the problem of error detection have been proposed and implemented in various forms by Atwell (1987), Bigert and Knutsson (2002), and Chodorow and Leacock (2000) amongst others. Chodorow and Leacock (2000) use a mutual information measure in addition to raw frequency of n grams. Among unsupervised checkers, Chodorow and Leacock (2000) exploits negative evidence from edited textual corpora achieving high precision but low recall, while Tsao and Wible (2009) uses general corpus only. For example, Chodorow and Leacock (2000) exploit bigrams and trigrams of function words and part-of-speech (PoS) tags, while Sun et al (2007) use labeled sequential patterns of function, time expression, and part-of-speech tags. For example, unsupervised systems of (Chodorow and Leacock, 2000) and (Tsao and Wible, 2009) leverage word distributions in general and/or word-specific corpus for detecting erroneous usages while (Hermet et al, 2008) and (Gamon and Leacock, 2010) use Web as a corpus. Chodorow and Leacock (2000) and Chodorow et al (2007) argue that precision-oriented is better, but they do not give any concrete reason. The grammar feature covers errors such as sentence fragments, verb form errors and pronoun errors (Chodorow and Leacock, 2000). An example is the error detection method (Chodorow and Leacock, 2000), which identifies unnatural sequences of POSs as grammatical errors in the writing of learners. Our method outperforms Microsoft Word03 and ALEK (Chodorow and Leacock, 2000) from Educational Testing Service (ETS) in some cases. An unsupervised method (Chodorow and Leacock, 2000) is employed to detect grammatical errors by inferring negative evidence from TOEFL administrated by ETS. In addition, we compared our technique with two other methods of checking errors, Microsoft Word03 and ALEK method (Chodorow and Leacock, 2000). In this paper, we compare our technique with the grammar checker of Microsoft Word03 and the ALEK (Chodorow and Leacock, 2000) method used by ETS. Chodorow and Leacock (2000) utilized mutual information and chi-square statistics to identify typical contexts for a small set of targeted words from a large well-formed corpus. The filter-based system combines unsupervised detection of a set of possible errors (Chodorow and Leacock, 2000) with hand-crafted filters designed to reduce this set to the largest subset of correctly flagged errors and the smallest possible number of false positives. Chodorow and Leacock (2000) found that low-frequency bigrams (sequences of two lexical categories with a negative log-likelihood) are quite reliable predictors of grammatical errors.
Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages. We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010). Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm. Subramanya et al's model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers. To this end, we use a variant of the quadratic cost criterion of Bengio et al (2006), also used by Subramanya et al (2010) and Das and Petrov (2011). Fortunately, some recently proposed POS taggers, such as the POS tagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach. Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al, 2010), unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith,2011). Following Das and Petrov (2011) and Subramanya et al (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics. Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type is unavailable (e.g., Das and Petrov, 2011). Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters. MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007)). For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 induced, but it assumes access to a labeled dataset of a different language. (Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfers to induce POS tags between two languages. Recent work by Das and Petrov (2011) builds a dictionary for a particular language by transferring annotated data from a resource-rich language through the use of word alignments in parallel text. These approaches build a dictionary by transferring labeled data from a resource rich language (English) to a resource poor language (Das and Petrov, 2011). In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages, in fact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactico semantic (Peirsman and Pado?, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al, 2011). Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of speech tags from the projected part-of-speech tagger of Das and Petrov (2011). In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011). Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language.  We have proposed a method for unsupervised POS tagging that performs on par with the current state of-the-art (Das and Petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM).
Unsupervised Multilingual Learning for Morphological Segmentation For centuries, the deep connection between languages has brought about major discoveries about human communication. In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning. In particular, we study the task of morphological segmentation of multiple languages. We present a nonparametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies cross-lingual morpheme pator We apply our model to three Semitic languages: Arabic, Hebrew, Aramaic, as well as to English. Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models. Furthermore, we provide evidence that our joint model achieves better performance when applied to languages from the same family. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008).  (Snyder and Barzilay, 2008) use multilingual data to compute segmentations of Arabic, Hebrew, Aramaic, and English. Snyder and Barzilay (2008) uses a set of aligned phrases across related languages to learn how to segment words with a Bayesian model and is otherwise fully unsupervised. The very interesting study by Snyder and Barzilay (2008) on multilingual approaches to morphological segmentation was difficult to classify. Snyder and Barzilay (2008a; 2008b) consider learning morphological segmentation with non parametric Bayesian model from multilingual data.  Snyder and Barzilay (2008) study the task of unsupervised morphological segmentation of multiple languages. For a majority of our testing we borrow the parallel phrases corpus used in previous work (Snyder and Barzilay, 2008), which we refer to as S&B.  Our work also has connections to multilingual tokenization (Snyder and Barzilay, 2008). Snyder and Barzilay (2008) use bilingual information but the segmentation is learned independently from translation modeling.
Identifying Agreement And Disagreement In Conversational Speech: Use Of Bayesian Networks To Model Pragmatic Dependencies We describe a statistical approach for modeling agreements and disagreements in conversational interaction. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse. We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance. Our approach achieves 86.9% accuracy, a 4.9% increase over previous work. Galley et al (2004) describe a system that identifies agreement and disagreement occurring in human-to-human multi-party conversations. An adjacent pair is said to consist of two parts that are ordered, adjacent, and produced by different speakers (Galley et al, 2004). Using them, Galley et al (2004) report an 8% increase in speaker identification. Galley et al 2004 show the value of using durational and structural features for identifying agreement and disagreement in spoken conversational speech. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentiment polarity indicators within speech segments (Galley et al, 2004). Classifying agree/disagree opinions in conversational debates using Bayesian networks was presented in (Galley et al, 2004). Other researchers have developed models for detecting agreement and disagreement in meetings, using models that combine lexical features with prosodic features (e.g., pause, duration, F0, speech rate) (Hillard et al, 2003) and structural information (e.g., the previous and following speaker) (Galley et al, 2004). This research has tackled issues such as the automatic detection of agreement and disagreement (Galley et al, 2004), and of the level of involvement of conversational participants (Gatica-Perez et al, 2005).   The contrast classifier is also competitive with the best case result in (Galley et al, 2004) (last entry), which adds speaker change, segment duration, and adjacency pair sequence dependency features using a dynamic Bayesian network. The experiments here kept the feature set fixed, but results of (Galley et al, 2004) suggest that further gains can be achieved by augmenting the feature set. Galley et al (2004) proposed the use of Bayesian networks to model pragmatic dependencies of previous agreement or disagreement on the current utterance. It is to be expected that the a-part provides a useful cue for identification of addressee of the b-part (Galley et al, 2004). Identification of this fine-grained structure of an interaction has been studied in prior work, with applications in agreement detection (Galley et al, 2004), addressee detection (op den Akker and Traum, 2009), and real-world applications, such as customer service conversations (Kim et al, 2010). To find these pairs automatically, we trained a non-sequential log-linear model that achieves a .902 accuracy (Galley et al, 2004). 
TINA: A Natural Language System For Spoken Language Applications new natural language system, been developed for applications involving spoken tasks. key ideas from context free grammars, Augmented Transition (ATN's), and the unification concept. a seamless interface between syntactic and semantic analysis, and also produces a highly constraining probabilistic language model to improve recognition performance. An initial set of context-free rewrite rules provided by hand is first converted to a network structure. Probability assignments on all arcs in the network are obtained automatically from a set of example sentences. The parser uses a stack decoding search strategy, with a top-down control flow, and includes a feature-passing mechanism to deal long-distance movement, agreement, and semantic constraints. an automatic sentence generation capability that has been effective for identifying overgeneralization problems as well as in producing a word-pair language model for a recognizer. The parser is currently with MIT's for use in two application domains, with the parser screening recognizer outputs either at the sentential level or to filter partial theories during the active search process. The parsing model is a probabilistic recursive transition network similar to those described in (Miller et ai. 1994) and (Seneff 1992). The resulting N-best hypotheses are processed by the TINA language understanding component (Seneff, 1992). As another way of bringing contextual information to bear in the process of predicting the meaning the following stochastic models, of unparsed inspired in Miller et al (1994) and Seneff (1992), and collectively referred to as hidden understanding model (HUM), are employed. Speech recognition results were parsed by the TINA parser (Seneff, 1992) using a hand-crafted grammar. The problem of over-generalization of speech grammars and related issues is well discussed by Seneff (1992). Example of WFST for LUcepts from user utterances by keyword spotting or heuristic rules has also been proposed (Seneff, 1992) where utterances can be transformed into concepts without major modifications to the rules. In our case, the log files include the output of the TINA Natural Language Understanding module, meaning that all semantically relevant units present in an input sentence are marked explicitly in the output parse frame (Seneff, 1992). We utilized a parser (Seneff, 1992) that is based on an enhanced probabilistic context-free grammar (PCFG), which captures dependencies beyond context-free rules by conditioning on the external left-context parse categories when predicting the first child of each parent node. The input utterance is processed through the speech recognizer and language under standing (Seneff, 1992) components, to achieve a simple encoding of its meaning. The language understanding system, TINA, described at length in (Seneff, 1992), integrates key ideas context free grammar, augmented transition network and unification concepts. Based on the Galaxyarchitecture (Goddeau et al, 1994), Jupiter recognizes user question over the phone, parses the question with the TINA language understanding system (Seneff,1992).
Word-Sense Disambiguation Using Decomposable Models is composed of interdependent variables. The test used to evaluate a model gives preference to those that have the fewest number of interdependencies, thereby selecting models expressing only the most systematic variable interactions. To summarize the method, one first identifies informative contextual features (where &quot;informative&quot; is a well-defined notion, discussed in Section 2). Then, out of all possible decomposable models characterizing interdependency relationships among the selected variables, those that are found to produce good approximations to the data are identified (using the test mentioned above) and one of those models is used to perform disambiguation. Thus, we are able to use multiple contextual features without the need for untested assumptions regarding the form of the model. Further, approximating the joint distribution of all variables with a model identifying only the most important systematic interactions among variables limits the number of parameters to be estimated, supports computational efficiency, and provides an understanding of the data. The biggest limitation associated with this method is the need for large amounts of sense-tagged data. Because asymptotic distributions of the test statistics are used, the validity of the results obtained using this approach are compromised when it is applied to sparse data (this point is discussed further in Section 2). To test the method of model selection presented in this paper, a case study of the disambiguation of the performed. selected because it has been shown in previous studies to be a difficult word to disambiguate. We selected as the set of tags all non-idiomatic noun senses of defined in the electronic version of Longman's Dictionary of Contemporary English (LDOCE) ([23]). Using the models produced in this study, we are able to assign an sense tag to every usage of a heldout test set with 78% accuracy. Although it is difficult to compare our results to those reported for previous disambiguation experiments, as will be discussed later, we feel these results are encouraging. The remainder of the paper is organized as follows. Section 2 provides a more complete definition of the Abstract Most probabilistic classifiers used for word-sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features. In this paper, a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguation of the noun describe a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguation, without requiring untested assumptions regarding the form of the model. Using this approach, the joint distribution of all variables is described by only the most systematic variable interactions, thereby limiting the number of parameters to be estimated, supporting computational efficiency, and providing an understanding of the data. [Interest] is a binary version of the word sense disambiguation data from (Bruce and Wiebe, 1994). One set was extracted from a hand-tagged corpus (Bruce and Wiebe, 1994) and the other by our algorithm.  We also make use of these properties in formulating the empirical classifiers as described in (Bruce and Wiebe, 1994). An example of the type of feature used is the part-of-speech of the word to the right; see (Bruce and Wiebe, 1994) for the other ones we use. The Interest data set developed by Bruce and Wiebe (1994) has been previously used for WSD (Ng and Lee, 1996). The next significant hand tagging task was reported in (Bruce and Wiebe, 1994), where 2,476 usages of interest were manually assigned with sense tags from the Longman Dictionary of Contemporary English (LDOCE).   The Interest data set developed by Bruce and Wiebe (1994) has been previously used for WSD (Ng and Lee, 1996).
Automatic Grammar Induction And Parsing Free Text: A Transformation-Based Approach In this paper we describe a new technique for parsing free text: a transformational grammar' is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled. The algorithm works by beginning in a very naive state of knowledge about phrase structure. By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce error. After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction. Second, there is a class of techniques for learning rules from text, a recent example being Brill 1993. Other works describe systems that induce structures from corpora, but they use tagged corpora (Brill, 1993), or grammatical informations (Brent, 1993), or work with artificial samples (Elman, 1990). Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. This idea is not new, but as far as we know it has been implemented in rule-based taggers and parsers, such as (Brill, 1993a), (Brill, 1993b), (Brill, 1993c) and (Ribarov, 1996), but not in models based on probability distributions. 
Linguistic Regularities in Continuous Space Word Representations Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems. These vector representations capture interesting linear relationships (up to some accuracy), such as king-man+woman=queen (Mikolov et al, 2013). However, not only are these techniques intractable to train with high-order context vectors, they also lack the neural network's ability to semantically generalize (Mikolov et al, 2013) and learn nonlinear relationships. Continuous space models have also been used for generating translations for new words (Mikolov et al 2013a) and ITG reordering (Li et al 2013). While all the previous data sets are relatively standard in the DSM field to test traditional count models, our last benchmark was introduced in Mikolov et al (2013a) specifically to test predict models. Mikolov et al (2013a) reach top accuracy on the syntactic subset (an syn) with a CBOW predict model akin to ours (but trained on a corpus twice as large). Top accuracy on the entire data set (an) and on the semantic subset (ansem) was reached by Mikolov et al (2013c) using a skip-gram predict model. Note however that, because of the way the task is framed, performance also depends on the size of the vocabulary to be searched: Mikolov et al (2013a) pick the nearest neighbour among vectors for 1M words, Mikolov et al (2013c) among 700K words, and we among 300K words.  Word embeddings have been empirically shown to preserve linguistic regularities, such as the semantic relationship between words (Mikolov et al, 2013b). More recently, Mikolov et al (2013a) propose two log-linear models, namely the Skip-gram and CBOW model, to efficiently induce word embeddings. Mikolov et al (2013b) observe that word embeddings preserve interesting linguistic regularities, capturing a considerable amount of syntactic/semantic relations. The word vectors we use are from Mikolov et al (2013) 13 (Mikolov13), and additional results are also shown using Turian et al (2010) 14 (Turian10). Mikolov et al (2013b) show that pre-trained embeddings can capture interesting semantic and syntactic information such as king-man+woman=queen on English data. These are precisely the kinds of distinctions between determiners that state-splitting in the Berkeley parser has shown to be useful (Petrov and Klein, 2007), and existing work (Mikolov et al., 2013b) has observed that such regular embedding structure extends to many other parts of speech. Among the state-of-the-art word embedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by Mikolov et al (2013b) and implemented in the word2vec software. Our departure point is the skip-gram neural embedding model introduced in (Mikolov et al, 2013a) trained using the negative-sampling procedure presented in (Mikolov et al, 2013b). We also tried using the sub sampling option (Mikolov et al, 2013b) with BOW contexts (not shown). In particular, we found that DEPS perform dramatically worse than BOW contexts on analogy tasks as in (Mikolov et al, 2013c; Levy and Goldberg, 2014). Furthermore, their text-based vectors encode very rich information, such as king-man+woman=queen (Mikolov et al, 2013c). This method is similar to the one introduced by Mikolov et al (2013a) for estimating a translation matrix, only solved analytically.
Translating Collocations For Bilingual Lexicons: A Statistical Approach Collocations are notoriously difficult for non-native speakers to translate, primarily because they are opaque and cannot be translated on a word-by-word basis. We describe a program named given a pair of parallel corpora in two different languages and a list of collocations in one of them, automatically produces their translations. Our goal is to provide a tool for compiling bilingual lexical information above the word level in multiple languages, for different domains. The algorithm we use is based on statistical methods and produces p-word translations of collocations in which not be the same. For example, . . . decision, employment equity, market . . . decision, equite matiere d'emploi, Testing three years' worth of the Hansards corpus yielded the French translations of 300 collocations for each year, evaluated at 73% accuracy on average. In this paper, we describe the statistical measures used, the algorithm, the implementation of our results and evaluation. This score is used, for instance, in the collocation compiler XTract (Smadja, 1993) and in the lexicon extraction system Champollion (Smadja et al, 1996). Consequently, if we compare our approach to the problem of collocation identification, we may say that we are more interested in precision than recall (Smadja et al, 1996). This highlights the need for finding multi-word translation correspondences. Previous works that focus on multi-word translation correspondences from parallel corpora include noun phrase correspondences (Kupiec, 1993), fixed flexible collocations (Smadja et al, 1996). Our preliminary finding supports the work on collocation by Smadja et al (1996) in that gapped sequences are also an important class of multi-word translations. Smadja et al (1996) limits to find French translation of English collocation identified by his Xtract system. The relationship between pointwise Mutual Information and the Dice coefficient is also discussed in (Smadja et al, 1996). A number of corpus-based methods to extract bilingual lexicons have been proposed (Smadja et al,1996).  Commonly used association measures are the Mutual Information (Fano, 1961) and the Dice factor (Smadja et al 1996).  Our method is similar to that of Smadja et al (1996), except that we incorporate lexical-level information into the association-based method. Our MWE translation extraction method is similar to the two-phase approach proposed by Smadja et al (1996). As noted by Smadja et al (1996), this two-step approach drastically reduces the search space. However, translations of collocated context words in the source word sequence create noisy candidate words, which might cause incorrect extraction of target translations by naive statistical correlation measures, such as the Dice coefficient used by Smadja et al (1996). Bound-length N-gram correspondences include (Kupiec, 1993) where NP recognizers are used to extract translation units and (Smadja et al, 1996) which uses the Extract system to extract collocations. Aligning parallel text, i.e. automatically setting the sentences or words in one text into correspondence with their equivalents in a translation, is a very useful preprocessing step for a range of applications, including but not limited to machine translation (Brown et al, 1993), cross-language information retrieval (Hiemstra, 1996), dictionary creation (Smadja et al, 1996) and induction of NLP-tools (Kuhn, 2004). Estimated clues are derived from the parallel data using, for example, measures of co-occurrence (e.g. the Dice coefficient (Smadja et al, 1996)). Smadja et al (1996) proposed a statistical association measure of the Dice coefficient to deal with the problem of collocation translation. Some methods make alignment suggestions at an intermediate level between sentence and word 271 and word (Smadja, 1992; Smadja et al, 1996).
A SEMANTIC CONCORDANCE George A. Miller, Claudia Leacock, Randee Tengi, Ross T. Bunker Cogn i t ive  Sc ience Laboratory Pr inceton Un ivers i ty Pr inceton,  NJ  08542 ABSTRACT A semantic oncordance is a textual corpus and a lexicon So com- bined that every substantive word in the text is linked to its appropriate ~nse in the lexicon.  This is due, in part, to the availability of public hand-tagged material, e.g. SemCor (Miller et al, 1993) and the DSO collection (Ng & Lee, 1996). Typically, word frequency distributions are estimated with respect to a sense-tagged corpus such as SemCor (Miller et al, 1993), a 220,000 word corpus tagged with WordNet (Fellbaum, 1998) senses. We base our experiments on SemCor (Miller et al, 1993), a balanced, semantically annotated dataset, with all content words manually tagged by trained lexicographers. The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al, 2004) (hereafter SE-3) and SemCor (Miller et al, 1993). In the experiments reported in this section, we use a parallel corpus consisting of 107 documents from the SemCor corpus (Miller et al, 1993) and their manual translations into Romanian. Existing hand annotated corpora like SemCor (Miller et al, 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all words track of the last Senseval competition (Snyder and Palmer, 2004). To train the classifiers for the all-words task we just used Semcor (Miller et al, 1993). This is roughly comparable with most frequent sense figures in standard annotated corpora such as Semcor (Miller et al, 1993) and the Senseval/Semeval data sets, which suggests that diversity may not play a major role in the current Google ranking algorithm. Cor text collection (Miller et al, 1993), a subset of the Brown corpus manually tagged with WordNet senses (37,176 sentences in 352 newspaper articles). To this extent, we cast the supersense tagging problem as a sequence labeling task and train a discriminative Hidden Markov Model (HMM), based on that of Collins (2002), on the manually annotated Semcor corpus (Miller et al, 1993). The Semcor corpus (Miller et al, 1993), a fraction of the Brown corpus (Kucera and Francis, 1967) which has been manually annotated with Wordnet synset labels. Most of current all-words generic supervised WSD systems take SemCor (Miller et al, 1993) as their source corpus, i.e. they are trained on SemCor examples and then applied to new examples. This includes sense ranks in WordNet, SemCor statistics (Miller et al, 1993), and similarity scores and rankings in Lin's resources. Unless specified otherwise, we use WordNet 1.7.1 (Milleret al, 1990) and the associated sense annotated SemCor corpus (Miller et al, 1993) (translated to WordNet 1.7.1 by Rada Mihalcea). ImCor dataset by associating images from the Corel database with text from the SemCor corpus (Miller et al, 1993). Coarse-grained English All-Words LexPar and SynWSD were trained on an 1 million words corpus comprising the George Orwell's 1984 novel and the SemCor corpus (Miller et al, 1993). Then, we present a detailed comparison of their performance on SemCor (Miller et al, 1993). A Times and SemCor corpora (Milleretal., 1993), and used to generate a training corpus, with manually-annotated positive and negative examples of part-whole relations. We are using a subset of the SemCor texts (Miller et al, 1993) - five randomly selected files covering different topics: news, sports, entertainment, law, and debates - as well as the data set provided for the English all words task during SENSEVAL-2. We contrast the performance of first sense heuristics i) from SemCor (Miller et al, 1993) and ii) derived automatically from the BNC following (McCarthy et al, 2004) and also iii) an upper-bound first sense heuristic extracted from the test data.
Statistical Machine Translation for Query Expansion in Answer Retrieval We present an approach to query expansion in answer retrieval that uses Statistical Machine Translation (SMT) techniques to bridge the lexical gap between questions and answers. SMT-based query expansion is done by i) using a full-sentence paraphraser to introduce synonyms in context of the entire query, and ii) by translating query terms into answer terms using a full-sentence SMT model trained on question-answer pairs. We evaluate these global, context-aware query expansion techon from 10 million question-answer pairs extracted from FAQ pages. Experimental results show that SMTbased expansion improves retrieval performance over local expansion and over retrieval without expansion. Recently (Riezler et al, 2007) used statistical machine translation for query expansion and took a step towards bridging the lexical gap between questions and answers. Riezler et al (2007) define the problem of answer retrieval from FAQ and social Q/A websites as a query expansion problem. With the exception of the query expansion approaches (Riezler et al, 2007), all works discussed here use some form of noisy-channel model (translation model and target language model) but do not perform the decoding part of the SMT process to generate translations, nor use the rich set of features of a full SMT. Furthermore, we plan to include the Level 1 translations into the candidate answer generation module in order to do query expansion in the style of Riezler et al (2007). Similar work has also been performed in the area of query expansion using training data consisting of FAQ pages (Riezler et al, 2007) or queries and clicked snippets from query logs (Riezler et al, 2008). (Riezler et al, 2007) adopted an SMT-based method to query expansion in answer retrieval. Our work is also related to that of Riezler et al (2007) where SMT-based query expansion methods are used on data from FAQ pages. Riezler et al (2007) demonstrate the advantages of translation-based approach to answer retrieval by utilizing a more complex translation model also trained from a large amount of data extracted from FAQs on the Web. Besides, Riezler et al (2007) and Zhou et al (2011) proposed the phrase-based translation models for question and answer retrieval.
Learning A Translation Lexicon From Monolingual Corpora This paper presents work on the task of constructing a word-level translation lexicon purely from unrelated monolingual corpora. We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency. Experimental results for the construction of a German-English noun lexicon are reported. Noun translation accuracy of 39% scored against a parallel test corpus could be achieved. Koehn and Knight (2002) combine a vector-space approach with other clues such as orthographic similarity and frequency. Unlike the noun-only test sets used in other studies, (e.g., Koehn and Knight (2002), Haghighi et al (2008)), TS1000 also contains adjectives and verbs. Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts. This setting has been considered before, most notably in Koehn and Knight (2002) and Fung (1995), but the current paper is the first to use a probabilistic model and present results across a variety of language pairs and data conditions. Following Koehn and Knight (2002), we consider lexicons over only noun word types, although this is not a fundamental limitation of our model. Also, as in Koehn and Knight (2002), we make use of a seed lexicon, which consists of a small, and perhaps incorrect, set of initial translation pairs. The second method is to heuristically induce, where applicable, a seed lexicon using edit distance, as is done in Koehn and Knight (2002). In order to explore system robustness to heuristically chosen seed lexicons, we automatically extracted a seed lexicon similarly to Koehn and Knight (2002): we ran EDITDIST on EN-ES-D and took the top 100 most confident translation pairs. However, we attempted to run an experiment as similar as possible in setup to Koehn and Knight (2002), using English Gigaword and German Europarl. In this setting, our MCCA system yielded 61.7% accuracy on the 186 most confident predictions compared to 39% reported in Koehn and Knight (2002). Koehn and Knight (2002) describe few potential clues that may help in extracting bilingual lexicon from two monolingual corpora such as identical words, similar spelling, and similar context features. Koehn and Knight (2002) map 976 identical word pairs that are found in their two monolingual German-English corpora and report that 88.0 percent of them are correct. Koehn and Knight (2002) mention few related works that use different measurement to compute the similarity, such as longest common subsequence ratio (Melamed, 1995) and string edit distance (Mann 10 and Yarowski, 2001). However, Koehn and Knight (2002) point out that majority of their word pairs do not show much resemblance at all since they use German-English language pair. Koehn and Knight (2002) present one interesting idea of using extracted cognate pairs from corpus as the seed words in order to alleviate the need of huge, initial bilingual lexicon.  Koehn and Knight (2002) derived such a seed lexicon from German-English cognates which were selected by using string similarity criteria. Other methods such as (Koehn and Knight, 2002) try to design a bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences. Following Koehn and Knight (2002), we have also employed simple transformation rules for the adoption of words from one language to another. The previous approach relying on work from Koehn and Knight (2002) has been outperformed in terms of precision and coverage.
Automatic Acquisition Of A Large Sub Categorization Dictionary From Corpora This paper presents a new method for producing a dictionary of subcategorization frames from unlabelled text corpora. It is shown that statistical filtering of the results of a finite state parser running on the output of a stochastic tagger produces high quality results, despite the error rates of the tagger and the parser. Further, it is argued that this method can be used to learn all subcategorization frames, whereas previous methods are not extensible to a general solution to the problem. Following Manning (1993), we empirically determined the value of p. Our local language model approach also bears some resemblance to statistical approaches to modeling sub categorization frames (Manning, 1993). Our work differs from corpus-based work such as Manning (1993) or Kawahara and Kurohashi (2001) in that we are using existing lexical resources rather than a corpus.  Techniques for the automatic acquisition of subcategofization dictionaries have been developed by Manning (1993), Bfiscoe and Carroll (1997) and Carroll and Rooth (1998). This can be done automatically with unparsed corpora (Briscoe and Carroll 1997, Manning 1993, Ushioda et al 1993), from parsed corpora such as Marcus et al's (1993) Treebank (Merlo 1994, Framis 1994) or manually as was done for COMLEX (Macleod and Grishman 1994). Our eventual goal is to develop a set of regular expressions that work on fiat tagged corpora instead of TreeBank parsed structures to allow us to gather information from larger corpora than have been done by the TreeBank project (see Manning 1993 and Gahl 1998). This shows to which extent the range of arguments is fine grained, in contrast to other works where the range is at the categorial level, such as NP or PP (M. Brent 1993, C. Manning 1993, P. Merlo & M. Leybold 2001). C. Manning (1993) presents the acquisition of sub categorization frames from unlabelled text corpora.  The same year, (Manning, 1993) used 4 million words of the New York Times (Sandhaus,), selected only clauses with auxiliary verbs and automatically analyzed them with a finite-state parser. Increasingly, tools are also becoming available for acquiring sub categorization information from corpora, i.e. for inferring the sub categorization frames of a given lemma (e.g. Manning 1993). Both Brcnt (1993) and Manning (1993), who attempt to induce a lexicon of sub categorization features do so by completely discarding all preexisting knowledge; both systems are stand-ahmc, without a parsing engine to test or use the "learned" information.  (Manning, 1993) observes that Brent's recognition technique is a rather simplistic and inadequate approach to verb detection, with a very high error rate. (Briscoe and Carroll, 1997) observe that in the work of (Brent, 1993), (Manning, 1993) and (Ushioda et al, 1993), the maximum number of distinct sub categorization classes recognized is sixteen, and only Ushioda et al attempt to derive relative sub cat egorization frequency for individual predicates.
A Phrase-Based Joint Probability Model For Statistical Machine Translation We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora. Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4. 1 Motivation Most of the noisy-channel-based models used in statistical machine translation (MT) (Brown et al., 1993) are conditional probability models. In the noisy-channel framework, each source sentence e in a parallel corpus is assumed to “generate” a target sentence f by means of a stochastic process, whose parameters are estimated using traditional EM techniques (Dempster et al., 1977). The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences. A variety of methods are used to account for the re-ordering stage: word-based (Brown et al., 1993), templatebased (Och et al., 1999), and syntax-based (Yamada and Knight, 2001), to name just a few. Although these models use different generative processes to explain how translated words are re-ordered in a target language, at the lexical level they are quite similar; all these models assume that source words are into target individual words may contain a non-existent element, called NULL. We suspect that MT researchers have so far chosen to automatically learn translation lexicons defined only over words for primarily pragmatic reasons. Large scale bilingual corpora with vocabularies in the range of hundreds of thousands yield very large translation lexicons. Tuning the probabilities associated with these large lexicons is a difficult enough task to deter one from trying to scale up to learning phrase-based lexicons. Unfortunately, trading space requirements and efficiency for explanatory power often yields non-intuitive results. Consider, for example, the parallel corpus of three sentence pairs shown in Figure 1. Intuitively, if we allow any Source words to be aligned to any Target words, the best alignment that we can come up with is the one in Figure 1.c. Sentence pair (S2, T2) offers strong evidence that “b c” in language S means the same thing as “x” in language T. On the basis of this evidence, we expect the system to also learn from sentence pair (S1, T1) that “a” in language S means the same thing as “y” in language T. Unfortunately, if one works with translation models that do not allow Target words to be aligned to more than one Source word — as it is the case in the IBM models (Brown et al., 1993) — it is impossible to learn that the phrase “b c” in language S means the same thing as word “x” in language T. The IBM Model 4 (Brown et al., 1993), for example, converges to the word alignments shown in Figure 1.b and learns the probabilities shown in Figure Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure train the IBM-4 model, we used Giza (Al-Onaizan et al., 1999). IBM−4 T−Table IBM−4 Intuitive Joint Joint T−Table p(y  |a) = 1 p(x  |c) = 1 p(z  |b) = 0.98 p(x  |b) = 0.02 S1: a b c T1: x y S2: b c S1: a b c T1: x y S2: b c S1: a b c T1: x y S2: b c p(x y, a b c) = 0.32 p(x, b c) = 0.34 p(y, a) = 0.01 p(z, b) = 0.33 Corresponding Conditional Table T2: x T2: x T2: x p(x y  |a b c ) = 1 p(x  |b c) = 1 p(y  |a) = 1 p(z  |b) = 1 S3: b S3: b S3: b T3: z T3: z T3: z a) b) c) d) e) Figure 1: Alignments and probability distributions in IBM Model 4 and our joint phrase-based model. yields unintuitive translation probabilities. (Note that another good word-for-word model is one that assigns high probability to p(xb) and p(zb) and low probability to p(xc).) In this paper, we describe a translation model that assumes that lexical correspondences can be established not only at the word level, but at the phrase level as well. In constrast with many previous approaches (Brown et al., 1993; Och et al., 1999; Yamada and Knight, 2001), our model does not try to capture how Source sentences can be mapped into Target sentences, but rather how Source and Target sentences can be generated simultaneously. In other words, in the style of Melamed (2001), we estimate a joint probability model that can be easily marginalized in order to yield conditional probability models for both source-to-target and target-tosource machine translation applications. The main difference between our work and that of Melamed is that we learn joint probability models of translation equivalence not only between words but also between phrases and we show that these models can be used not only for the extraction of bilingual lexicons but also for the automatic translation of unseen sentences. In the rest of the paper, we first describe our model (Section 2) and explain how it can be implemented/trained (Section 3). We briefly describe a decoding algorithm that works in conjunction with our model (Section 4) and evaluate the performance of a translation system that uses the joint-probability model (Section 5). We end with a discussion of the strengths and weaknesses of our model as compared to other models proposed in the literature. 2 A Phrase-Based Joint Probability Model 2.1 Model 1 In developing our joint probability model, we started out with a very simple generative story. We assume that each sentence pair in our corpus is generated by the following stochastic process: 1. Generate a bag of concepts. 2. For each concept , generate a pair of phrases , according to the distribution contain at least one word. 3. Order the phrases generated in each language so as to create two linear sequences of phrases; these sequences correspond to the sentence pairs in a bilingual corpus. For simplicity, we initially assume that the bag of concepts and the ordering of the generated phrases are modeled by uniform distributions. We do not assume that is a hidden variable that generates pair , but rather that . Under these assumptions, it follows that the probability of generating a sentence pair (E, F) using concepts given by the product of all phrase-tophrase translation probabilities, yield bags of phrases that can be ordered linearly so as to obtain the sentences E and F. For example, the sentence pair “a b c” — “x y” can be generated using two concepts, (“a b” : “y”) and (“c” : “x”); or one concept, (“a b c” : “x y”), because in both cases the phrases in each language can be arranged in a sequence that would yield the original sentence pair. However, the same sentence pair cannot be generated using the concepts (“a b” : “y”) and (“c” : “y”) because the sequence “x y” cannot be recreated from the two phrases “y” and “y”. Similarly, the pair cannot be generated using concepts (“a c” : “x”) and (“b” : “y”) because the sequence “a b c” cannot be created by catenating the phrases “a c” and “b”. We say that a set of concepts can be linearized into a sentence pair (E, F) if E and F can be obtained by permuting the phrasesandthat characterize concepts . We denote this property using the predicate . Under this model, the probability of a given sentence pair (E, F) can then be obtained by summing up over all possible ways of generating bags of concepts that can linearized to (E, F). (1) 2.2 Model 2 Although Model 1 is fairly unsophisticated, we have found that it produces in practice fairly good alignments. However, this model is clearly unsuited for translating unseen sentences as it imposes no constraints on the ordering of the phrases associated with a given concept. In order to account for this, we modify slightly the generative process in Model 1 so as to account for distortions. The generative story of Model 2 is this: 1. Generate a bag of concepts. 2. Initialize E and F to empty sequences. 3. Randomly take a concept and generate a pair of phrases , according to the distribution , whereandeach contain at least one word. Remove then from. 4. Append phraseat the end of F. Letbe the start position ofin F. 5. Insert phraseat positionin E provided that no other phrase occupies any of the positions betweenand , wheregives length of the phrase. We hence create the alignment between the two phrasesand with probability is a position-based distortion distribution. 6. Repeat steps 3 to 5 untilis empty. In Model 2, the probability to generate a sentence pair (E, F) is given by formula (2), where the position of wordof phrasein sen- F and denotes the position in tence E of the center of mass of phrase. (2) Model 2 implements an absolute position-based distortion model, in the style of IBM Model 3. We have tried many types of distortion models. We eventually settled for the model discussed here because it produces better translations during decoding. Since the number of factors involved in computing the probability of an alignment does not vary with the size of the Target phrases into which Source phrases are translated, this model is not predisposed to produce translations that are shorter than the Source sentences given as input. 3 Training Training the models described in Section 2 is computationally challenging. Since there is an exponential number of alignments that can generate a sentence pair (E, F), it is clear that we cannot apply the 1. Determine high-frequency ngrams in the bilingual corpus. 2. Initialize the t-distribution table. 3. Apply EM training on the Viterbi alignments, while using smoothing. 4. Generate conditional model probabilities. Figure 2: Training algorithm for the phrase-based joint probability model. EM training algorithm exhaustively. To estimate the parameters of our model, we apply the algorithm in Figure 2, whose steps are motivated and described below. 3.1 Determine high-frequency n-grams in E and F If one assumes from the outset that any phrases can be generated from a cept , one would need a supercomputer in order to store in the memory a table that models the distribution. Since we don’t have access to computers with unlimited memory, we initially learn t distribution entries only for the phrases that occur often in the corpus and for unigrams. Then, through smoothing, we learn t distribution entries for the phrases that occur rarely as well. In order to be considered in step 2 of the algorithm, a phrase has to occur at least five times in the corpus. 3.2 Initialize the t-distribution table Before the EM training procedure starts, one has no idea what word/phrase pairs are likely to share the same meaning. In other words, all alignments that can generate a sentence pair (E, F) can be assumed to have the same probability. Under these conditions, the evidence that a sentence pair (E, F) contributes to fact that are generated by the same cept is given by the number of alignments that can be built between (E, F) that have a concept that is linked to phrasein sentence E and phrase sentence F divided by the total number of alignments that can be built between the two sentences. Both these numbers can be easily approximated. Given a sentence E ofwords, there are ways in which thewords can be partitioned into sets/concepts, where is the ling number of second kind. There are also ways in which the words a sentence F can be partitioned into nonempty sets. Given that any words in E can be mapped to any words in F, it follows that there are alignments that can be built between two sentences (E, F) of lengthsand , respectively. When a concept generates two phrases of lengthand, respectively, there are only and words left to link. Hence, the absence of any other information, the probability that phrasesandare generated by the same concept is given by formula (4). Note that the fractional counts returned by equation (4) are only an approximation of the t distribution that we are interested in because the Stirling numbers of the second kind do not impose any restriction on the words that are associated with a given concept be consecutive. However, since formula (4) overestimates the numerator and denominator equally, the approximation works well in practice. In the second step of the algorithm, we apply equation (4) to collect fractional counts for all unigram and high-frequency n-gram pairs in the cartesian product defined over the phrases in each sentence pair (E, F) in a corpus. We sum over all these t-counts and we normalize to obtain an initial joint distribution. This step amounts to running the EM algorithm for one step over all possible alignments in the corpus. 3.3 EM training on Viterbi alignments Given a non-uniform t distribution, phrase-to-phrase alignments have different weights and there are no other tricks one can apply to collect fractional counts over all possible alignments in polynomial time. Starting with step 3 of the algorithm in Figure 2, for each sentence pair in a corpus, we greedily produce an initial alignment by linking together phrases so as to create concepts that have high t probabilities. We then hillclimb towards the Viterbi alignment of highest probability by breaking and merging concepts, swapping words between concepts, and moving words across concepts. We compute the probabilities associated with all the alignments we generate during the hillclimbing process and collect t counts over all concepts in these alignments. We apply this Viterbi-based EM training procedure for a few iterations. The first iterations estimate the alignment probabilities using Model 1. The rest of the iterations estimate the alignment probabilities using Model 2. During training, we apply smoothing so we can associate non-zero values to phrase-pairs that do not occur often in the corpus. 3.4 Derivation of conditional probability model At the end of the training procedure, we take marginals on the joint probability distributionsand . This yields conditional probability distributions and which we use for decoding. 3.5 Discussion When we run the training procedure in Figure 2 on the corpus in Figure 1, after four Model 1 iterations we obtain the alignments in Figure 1.d and the joint and conditional probability distributions shown in Figure 1.e. At prima facie, the Viterbi alignment for the first sentence pair appears incorrect because we, as humans, have a natural tendency to build alignments between the smallest phrases possible. However, note that the choice made by our model is quite reasonable. After all, in the absence of additional information, the model can either assume that “a” and “y” mean the same thing or that phrases “a b c” and “x y” mean the same thing. The model chose to give more weight to the second hypothesis, while preserving some probability mass for the first one. Also note that although the joint distribution puts the second hypothesis at an advantage, the conditional distribution does not. The conditional distribution in Figure 1.e is consistent with our intuitions that tell us that it is reasonable both to translate “a b c” into “x y”, as well as “a” into “y”. The conditional distribution mirrors perfectly our intuitions. 4 Decoding For decoding, we have implemented a greedy procedure similar to that proposed by Germann et al. (2001). Given a Foreign sentence F, we first produce a gloss of it by selecting phrases inthat the probability . We then tively hillclimb by modifying E and the alignment between E and F so as to maximize the formula . We hillclimb by modifying an existing alignment/translation through a set of operations that modify locally the aligment/translation built until a given time. These operations replace the English side of an alignment with phrases of different probabilities, merge and break existing concepts, and swap words across concepts. The probability p(E) is computed using a simple trigram language model that was trained using the CMU Language Modeling Toolkit (Clarkson and Rosenfeld, 1997). The language model is estimated at the word (not phrase) level. Figure 3 shows the steps taken by our decoder in order to find the translation of sentence “je vais me arrˆeter l`a .” Each intermediate translation in Figure 3 is preceded by its probability and succeded by the operation that changes it to yield a translation of higher probability. 5 Evaluation To evaluate our system, we trained both Giza (IBM Model 4) (Al-Onaizan et al., 1999) and our joint probability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus. The sentences in the corpus were at most 20 words long. The English side had a total of 1,073,480 words (21,484 unique tokens). The French side had a total of 1,177,143 words (28,132 unique tokens). We translated 500 unseen sentences, which were uniformly distributed across lengths 6, 8, 10, 15, and 20. For each group of 100 sentences, we manually determined the number of sentences translated perfectly by the IBM model decoder of Germann et (2001) and the decoder that uses the joint prob- Model Percent perfect translations IBM Bleu score Sentence length Sentence length 6 8 10 15 20 Avg. 6 8 10 15 20 Avg. IBM 36 26 35 11 2 22 0.2076 0.2040 0.2414 0.2248 0.2011 0.2158 Phrase-based 43 37 33 19 6 28 0.2574 0.2181 0.2435 0.2407 0.2028 0.2325 Table 1: Comparison of IBM and Phrase-Based, Joint Probability Models on a translation task. je vais me arreter la . je vais me arreter la . 9.46e−08 i am going to stop there . Figure 3: Example of phrase-based greedy decoding. ability model. We also evaluated the translations automatically, using the IBM-Bleu metric (Papineni et al., 2002). The results in Table 1 show that the phrased-based translation model proposed in this paper significantly outperforms IBM Model 4 on both the subjective and objective metrics. 6 Discussion 6.1 Limitations The main shortcoming of the phrase-based model in this paper concerns the size of the t-table and the cost of the training procedure we currently apply. To keep the memory requirements manageable, we arbitrarily restricted the system to learning phrase translations of at most six words on each side. Also, the swap, break, and merge operations used during the Viterbi training are computationally expensive. We are currently investigating the applicability of dynamic programming techniques to increase the speed of the training procedure. Clearly, there are language pairs for which it would be helpful to allow concepts to be realized as non-contiguous phrases. The English word “not”, for example, is often translated into two French words, “ne” and “pas”. But “ne” and “pas” almost never occur in adjacent positions in French texts. At the outset of this work, we attempted to develop a translation model that enables concepts to be mapped into non-contiguous phrases. But we were not able to scale and train it on large amounts of data. The model described in this paper cannot learn that the English word “not” corresponds to the French words “ne” and “pas”. However, our model learns to deal with negation by memorizing longer phrase translation equivalents, such as (“ne est pas”, “is not”); (“est inadmissible”, “is not good enough”); and (“ne est pas ici”, “is not here”). 6.2 Comparison with other work A number of researchers have already gone beyond word-level translations in various MT settings. For example, Melamed (2001) uses wordlevel alignments in order to learn translations of noncompositional compounds. Och and Ney (1999) learn phrase-to-phrase mappings involving word classes, which they call “templates”, and exploit them in a statistical machine translation system. And Marcu (2001) extracts phrase translations from automatically aligned corpora and uses them in conjunction with a word-for-word statistical translation system. However, none of these approaches learn simultaneously the translation of phrases/templates and the translation of words. As a consequence, there is a chance that the learning procedure will not discover phrase-level patterns that occur often in the je vais me arreter la . 7.50e−11 FuseAndChangeTrans(&quot;la .&quot;, &quot;there .&quot;) i want me to that . je vais me arreter la . 2.97e−10 ChangeWordTrans(&quot;arreter&quot;,&quot;stop&quot;) 7.75e−10 1.09e−09 i want me to there . je vais me arreter la . i want me stop there . je vais me arreter la . let me to stop there . FuseAndChange(&quot;je vais&quot;,&quot;let me&quot;) FuseAndChange(&quot;je vais me&quot;, &quot;i am going to&quot;) 1.28e−14 changeWordTrans(&quot;vais&quot;, &quot;want&quot;) i . me to that . data. In our approach, phrases are not treated differently from individual words, and as a consequence the likelihood of the EM algorithm converging to a better local maximum is increased. Working with phrase translations that are learned independent of a translation model can also affect the decoder performance. For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993). The phrases in the translation memory were automatically extracted from the Viterbi alignments produced by Giza (Al-Onaizan et al., 1999) and reused in decoding. The decoder described in (Marcu, 2001) starts from a gloss that uses the translations in the translation memory and then tries to improve on the gloss translation by modifying it incrementally, in the style described in Section 4. However, because the decoder hill-climbs on a word-forword translation model probability, it often discards good phrasal translations in favour of word-for-word translations of higher probability. The decoder in Section 4 does not have this problem because it hillclimbs on translation model probabilities in which phrases play a crucial role. posed is a joint model as in (Marcu and Wong, 2002), since target and source phrases are generated jointly. Marcu and Wong (2002) use a joint probability model for blocks where the clumps are contiguous phrases as in this paper. In (Marcu and Wong, 2002), a joint probability phrase model is presented. The joint model by (Marcu and Wong, 2002) is refined by (Birch et al, 2006) who use high-confidence word alignments to constrain the search space in training. The work by (DeNero et al, 2008) describes a method to train the joint model described in (Marcu and Wong, 2002) with a Gibbs sampler.  Marcu and Wong (2002) propose a joint probability model which searches the phrase alignment space, simultaneously learning translations lexicons for words and phrases without consideration of potentially suboptimal word alignments and heuristic for phrase extraction. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model to handle multi-word units. A joint probability model, proposed by Marcu and Wong (2002), is a kind of phrase based one. Marcu and Wong (2002) proposed a phrase-based alignment model which suffered from a massive parameter space and intractable inference using expectation maximisation. The advent of Statistical Machine Translation, and most recently phrase-based approaches (PBMT, see Marcu and Wong (2002), Koehn et al (2003)) into the commercial arena seems to hold the promise of a solution to this problem: because the MT system learns directly from existing translations, it can be automatically customized to new domains and tasks. For example, (Marcu and Wong, 2002) for a joint phrase based model, (Huang et al, 2003) for a translation memory system; and (Watanabe et al., 2003) for a complex model of insertion, deletion and head-word driven chunk reordering. Marcu and Wong (2002) propose a joint probability model. However, for more permissive models such as Marcu and Wong (2002) and DeNero et al (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. Indeed, Marcu and Wong (2002) conjectures that none exist.  For the space A of bijective alignments, problems E and O have long been suspected of being NP-hard, first asserted but not proven in Marcu and Wong (2002). Marcu and Wong (2002) describes an approximation to O. Other phrase-based models model the joint distribution P (e, f) (Marcu and Wong, 2002) or made P (e) and P (f| e) into features of a log-linear model (Och and Ney, 2002). The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT).
Named Entity Recognition With Character-Level Models We discuss two named-entity recognition models which use characters and character -grams either exclusively or as an important part of their data representation. The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features. Our best model achieves an overall F of 86.07% on the English test data (92.31% on the development data). This number represents a 25% error reduction over the same model without word-internal (substring) features. Klein et al (2003) also applied the related Conditional Markov Models for combining classifiers. Klein et al (2003) employed a stacked learning system which contains Hidden Markov Models, Maximum Entropy Models and Conditional Markov Models. Here there is no significant difference between them and the systems of Klein et al (2003) and Zhang and Johnson (2003). The performance of the system of Chieu et al (2003) was not significantly different from the best performance for English and the method of Klein et al (2003) and the approach of Zhang and Johnson (2003) were not significantly worse than the best result for German.   Character n-gram based approach (Klein et al, 2003) using generative models, was experimented on English language and it proved to be useful over the word based models. We plan to experiment with the character n-gram approach (Klein et al, 2003) and include gazetteer information. We experimented with a conditional Markov model tagger that performed well on language-independent NER (Klein et al, 2003) and the identification of gene and protein names (Finkel et al, 2005). Where the best results for English with Stanford NER CRF gave a precision of 86.1 percent, a recall of 86.5 percent and F-score of 86.3 percent, for German the best results had a precision of 80.4 percent, a recall of 65.0 percent and an F-score of 71.9 percent, (Klein et al, 2003). Another approach we will also focus is dividing words into characters and applying character-level models (Klein et al, 2003). Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic, morphological, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). Our system is a Maximum Entropy Markov Model, which further develops a system earlier used for the CoNLL 2003 shared task (Klein et al, 2003) and the 2004 BioCreative critical assessment of information. Sometimes, these types of features are referred to as word-external and word-internal (Klein et al, 2003). In addition, because of data sparsity (out-of-vocabulary) problem due to the long-tailed distribution of words in natural language, sophisticated unknown word models are generally needed for good performance (Klein et al 2003). The use of char N-gram (N-gram substring) features was inspired by the work of (Klein et al 2003), where the introduction of such features has been shown to improve the overall F1 score by over 20%.
A Memory-Based Approach to Learning Shallow Natural Language Patterns Recognizing shallow linguistic patterns, such as basic syntactic relationships between words, is a common task in applied natural language and text processing. The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata. This paper presents a novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus. The training data are stored as-is, in efficient suffix-tree data structures. Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus. This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training. The paper presents experimental results for recognizing noun phrase, subject-verb and verb-object patterns in English. Since the learning approach enables easy porting to new domains, we plan to apply it to syntactic patterns in other languages and to sub-language patterns for information extraction. Of the several slightly different definitions of a base NP in the literature we use for the purposes of this work the definition presented in (Ramshaw and Marcus, 1995) and used also by (Argamon et al, 1998) and others. SV phrases, following the definition suggested in (Argamon et al, 1998), are word phrases starting with the subject of the sentence and ending with the first verb, excluding modal verbs. As reported in (Argamon et al, 1998), most base NPs present in the data are less or equal than 4 words long. Argamon et al (1998) segmented the POS sequence of a multi-word into small POS tiles, counted tile frequency in the new word and non-new-word on the training set respectively, and detected new words using these counts. (Argamon et al, 1998) use Memory-Based Sequence Learning for recognizing both NP chunks and VP chunks. Ramshaw and Marcus (1995), Munoz et al (1999), Argamon et al (1998), Daelemans et al (1999a) find NP chunks, using Wall Street Journal training material of about 9000 sentences.
Online Large-Margin Training of Syntactic and Structural Translation Features Minimum-error-rate training (MERT) is a bottleneck for current development in statistical machine translation because it is limited in the number of weights it can reliably optimize. Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT. We first show that by parallel processing and exploiting more of the parse forest, we can obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost. We then test the method on two classes of features that address deficiencies in the Hiero hierarchical phrasebased model: first, we simultaneously train a large number of Marton and Resnik’s soft syntactic constraints, and, second, we introduce a novel structural distortion model. In both cases we obtain significant improvements in translation performance. Optimizing them in combination, for a total of 56 feature weights, improve performance by 2.6 a subset of the NIST 2006 Arabic-English evaluation data. To remedy this problem, Chiang et al (2008) introduce a structural distortion model, which we include in our experiment. As proposed by Haddow et al (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentence wise score (Chiang et al, 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. An alternative way of accounting for phrase size is presented by Chiang et al (2008), who introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length. MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). We incorporate all our new features into a linear model and learn weights for each using the online averaged perceptron algorithm (Collins, 2002) with a few modifications for structured outputs inspired by Chiang et al (2008). The results are especially notable for the basic feature setting - up to 1.2 BLEU and 4.6 TER improvement over MERT - since MERT has been shown to be competitive with small numbers of features compared to high-dimensional optimizers such as MIRA (Chiang et al, 2008). We conjecture both these issues will be ameliorated with syntactic features such as those in Chiang et al (2008). In future work we also intend to explore using additional sparse features that are known to be useful in translation, e.g. syntactic features explored by Chiang et al (2008).  The oracle is created, analogously Chiang et al (2008), by choosing e+j? N to maximise the sum of gain (calculated on the batch) and model score. For example, the features introduced by Chiang et al (2008) and Chiang et al (2009) for an SCFG model for Chinese/English translation are of two types: The first type explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals. The perceptron algorithm itself compares favorably to related learning techniques such as the MIRA adaptation of Chiang et al (2008). The algorithms described below can be straightforwardly generalized to compute oracle hypotheses under combined metrics mixing model scores and quality measures (Chiang et al 2008), by weighting each edge with its model score and by using these weights down the pipe. Building on this paper, the most recent work to our knowledge has been done by Chiang et al (2008). We parse the English side of our parallel corpus with the Berkeley parser (Petrov et al, 2006), and tune parameters of them T system with MIRA (Chiang et al, 2008). Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (Macherey et al, 2008) and large-margin training (Chiang et al, 2008). Chiang et al (2008) added structure distortion features into their decoder and showed improvements in their Chinese-English experiment. The definition of the loss function here is similar to the one used in (Chiang et al, 2008) where only the top-1 translation candidate (i.e. k= 1) is taken into account. In addition, MERT would not be an appropriate optimizer when the number of features increases a certain amount (Chiang et al, 2008). 
A Statistical Approach To Anaphora Resolution This paper presents an algorithm for identifying pronominal anaphora and two experiments based upon this algorithm. We incorporate multiple anaphora resolution factors into a statistical framework — specifically the distance between the pronoun and the proposed antecedent, gender/number/animaticity of the proposed antecedent, governing head information and noun phrase repetition. We combine into a single probability that enables to identify the referent. Our first experiment shows the relative contribution of each source of information and demonstrates a success rate 82.9% for all sources combined. experiment investigates a method for unsupervised learning of gender/number/animaticity information. We present some experiments illustrating the accuracy of the method and note that with this information added, our pronoun resolution method achieves 84.2% accuracy. Treebank texts contain complete structural parsers, POS tags, and annotation of the antecedents of definite pronouns (added by Ge et al 1998). Ge et al (1998) implement a Hobbs distance feature, which encodes the rank assigned to a candidate antecedent for a pronoun by Hobbs's (1978) seminal syntax-based pronoun resolution algorithm. Ge et al [1998] also present a statistical algorithm based on the study of statistical data in a large corpus and the application of a naive Bayes model. Ge et al (1998) uses a non-parametrized statistical model to find the antecedent from a list of candidates generated by applying the Hobbs algorithm to the English Penn Treebank. Compared with these work, our work uses machine-generated parse trees from which trainable features are extracted in a maximum-entropy coreference system, while (Ge et al, 1998) assumes that correct parse trees are given. In other cases, these modules are integrated by means of statistical (Ge et al, 1998) or uncertainty reasoning techniques (Mitkov, 1997). (Ge et al 1998) incorporate gender, number, and animaticity information into a statistical model for anaphora resolution by gathering coreference statistics on particular nominal-pronoun pairs. Thus the size of the annotated data (3,115 personal pronouns, 2,198 possessive pronouns, 928 demonstrative pronouns) compares favourably with the size of evaluation data in other proposals (619 German pronouns in (Strube and Hahn, 1999), 2,477 English pronouns in (Ge et al, 1998), about 5,400 English coreferential expressions in (Ng and Cardie, 2002)). Exceptions are existant but few (2.5%): abstract pronouns (such as that in English) referring to non neuter or plural NPs, plural pronouns co-referring with singular collective NPs (Ge et al, 1998), antecedent and anaphor matching in natural gender rather than grammatical gender. Ge et al (1998) try to factorize the same principle by counting the number of times a discourse entities has been mentioned in the discourse already. Like (Ge et al, 1998), Strube (1998) evaluates on ideal hand annotated data.  Ge et al (1998)'s probabilistic approach combines three factors (aside from the agreement filter): the result of the Hobbs algorithm, Mention Count dependent on the position of the sentence in the article, and the probability of the antecedent occur ring in the local context of the pronoun. The choice of entities may reasonably be considered to be independent given the mixing weights, but how we realize an entity is strongly dependent on context (Ge et al, 1998). Ge et al (1998) exploit a similar idea to assign gender to proper mentions. Implementation of constraints and preferences can be based on empirical insight (Lappin and Leass, 1994), or machine learning from a reference annotated corpus (Ge et al, 1998). There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al, 1998), or contextual role-knowledge (Bean and Riloff, 2004). Incorporating context only through the governing constituent was also done in (Ge et al, 1998). Ge et al (1998) describe a supervised probabilistic pronoun resolution algorithm which is based on complete syntactic information. Their factors are taken from Ge et al (1998), with two exceptions.
Methods For Using Textual Entailment In Open-Domain Question Answering Work on the semantics of questions has argued that the relation between a question and its answer(s) can be cast in terms of logical entailment. In this paper, we demonstrate how computational systems to recognize entailment can be used to enhance the accuracy of current open-domain automatic question answering (Q/A) systems. In our experiments, we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system, accuracy can be increased by as much as 20% overall. For more information on the TE system described in this section, please see (Hickl et al, 2006b) and (Harabagiu and Hickl, 2006). Following (Harabagiu and Hickl, 2006), we used TE information in order to filter answers identified by the Q/A system that were not entailed by the user's original question. While it has been shown that paraphrasing methods are useful for question answering (Harabagiu and Hickl, 2006) and relation extraction (Romano et al, 2006), this is, to the best of our knowledge, the first paper to perform semantic parsing through paraphrasing. In order to improve QA systems' performance many research focus on different structures such as question processing (Huang et al., 2008), information retrieval (Clarke et al., 2006), information extraction (Saggion and Gaizauskas, 2006), textual entailment (TE) (Harabagiu and Hickl, 2006) for ranking, answer extraction, etc. Implementation of different TE models has previously shown to improve the QA task using supervised learning methods (Harabagiu and Hickl, 2006). Instead of matching headline and first sentence of the document as in (Harabagiu and Hickl, 2006), we followed a different approach. In cases where simple question formulation is not satisfactory, many advanced QA systems implement more sophisticated syntactic, semantic and contextual processing such as named-entity recognition (Molla et al, 2006), coreference resolution (Vicedo and Ferrandez, 2000), logical inferences (abduction or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. Recent work on textual entailment has shown improvements on QA results (Harabagiu and Hickl, 2006), (Celikyilmaz et al, 2009), when used for filtering and ranking answers. For the task of Question Answering, (Harabagiu and Hickl, 2006) applied a TE component to rerank candidate answers returned by a retrieval step. Techniques developed for RTE have now been successfully applied in the domains of Question Answering (Harabagiu and Hickl, 2006) and Machine Translation (Pado et al, 2009), (Mirkin et al, 2009). This includes finding question answer pairs (Cong et al, 2008) from online forums, auto-answering queries on a technical forum (Feng et al, 2006), ranking answers (Harabagiu and Hickl, 2006) etc. Being a challenging task, it has been shown that it is helpful to applications like question answering (Harabagiu and Hickl, 2006). The great potential of integrating (monolingual) TE recognition components into NLP architectures has been reported in several works, such as question answering (Harabagiu and Hickl, 2006), information retrieval (Clinchant et al, 2006), information extraction (Romano et al, 2006), and document summarization (Lloret et al, 2008). TE has been successfully applied to a variety of natural language processing applications, including information extraction (Romano et al, 2006) and question answering (Harabagiu and Hickl, 2006). Knowledge about entailment is beneficial for NLP tasks such as Question Answering (Harabagiu and Hickl, 2006). Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al, 2009).
Machine Translation by Triangulation: Making Effective Use of Multi-Parallel Corpora Current phrase-based SMT systems perform poorly when using small training sets. This is a consequence of unreliable translation estimates and low coverage over source and target phrases. This paper presents a method which alleviates this problem by exploiting multiple translations of the same source Central to our approach is triangulathe process of translating from a source to a target language via an intermediate third language. This allows the use of a much wider range of parallel corpora for training, and can be combined with a standard phrase-table using conventional smoothing methods. Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase-based system. A similar idea exists in machine translation where English is frequently used to pivot between other languages (Cohn and Lapata, 2007). In statistical machine translation (SMT), methods that incorporate translations from other languages (Cohn and Lapata, 2007) have proven effective in low-resource situations: when phrase translations are unavailable for a certain language, one can look at other languages where the translation is available and then translate from that language. Pivoting can finally be used to fix or improve the translation model: (Cohn and Lapata, 2007) augments the phrase table for a baseline bilingual system with supplementary phrases obtained by pivoting into a third language. One of them is often called triangulation and usually refers to the combination of phrase tables (Cohn and Lapata, 2007). For instance, (Cohn and Lapata, 2007) explore the use of triangulation for machine translation, where multiple translation models are learned using multilingual parallel corpora. This finding might also carry over to phrase-table triangulation (Cohn and Lapata, 2007), where multi-parallel data is used in training to augment a standard translation system. Cohn and Lapata (2007) explores how to utilize multilingual parallel data (rather than pivot data) to improve translation performance.
What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA This paper presents a syntax-driven ap proach to question answering, specifically the answer-sentence selection problem forshort-answer questions. Rather than using syntactic features to augment exist ing statistical classifiers (as in previouswork), we build on the idea that ques tions and their (correct) answers relate toeach other via loose but predictable syntactic transformations. We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust non lexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model. Our model learns soft alignments as a hidden variable in discriminative training. Experimental results using the TREC dataset are shown to significantly outperform strong state-of-the-art baselines. More recently, Wang et al (2007) explored the use a formalism called quasi synchronous grammar (Smith and Eisner, 2006) in order to find a more explicit model for matching the set of dependencies, and yet still allow for looseness in the matching. Wang et al (2007) use quasi-synchronous translation to map all parent-child paths in a question to any path in an answer. Finally, in (Wang et al 2007), a quasi-synchronous grammar (Smith and Eisner,2006) is used to model relations between questions and answer sentences. Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010).  We followed the same experimental setup as Wang et al (2007) and Heilman and Smith (2010). Results of replicated systems for the last two were reported by Wang et al (2007), with lexical-semantic augmentation from WordNet. Results in Table 3 show that our model gives the same level of performance as Wang et al (2007), with no statistically significant difference (p > 5 in sign test).  Experiments were conducted to evaluate tree edit models for three tasks: recognizing textual entailment (Giampiccolo et al, 2007), paraphrase identification (Dolan et al, 2004), and an answer selection task (Wang et al, 2007) for question answering (Voorhees, 2004).  For a given set of questions, the task here is to rank candidate answers (Wang et al, 2007). The experimental setup is the same as in Wang et al (2007). We compare our tree edit model to three other systems as they are reported by Wang et al (2007). The results for the tree edit model are statistically significantly different (sign test, p < 0.01) from the results for all except the Wang et al (2007) system with WordNet (p > 0.05). This includes work on question answering (Wang et al, 2007), sentiment analysis (Nakagawa et al, 2010), MT reordering (Xu et al, 2009), and many other tasks.  In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al, 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009).
Maximum Entropy Based Phrase Reordering Model For Statistical Machine Translation We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs). The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext. We present an algorithm to extract all reordering events of neighbor blocks from bilingual data. In our experiments on Chineseto-English translation, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks. We also ran a phrase-based system (PB) with a distortion reordering model (Xiong et al, 2006) on the same corpus. Some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al, 2006). In addition, a few models employed the collocation information to improve the performance of the ITG constraints (Xiong et al, 2006). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The phrase-based SMT system proposed by Xiong et al 2006) is used as the baseline system, with a MaxEnt principle-based lexicalized reordering model integrated, which is used to handle reorderings in decoding. Xiong et al (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. Another extension would try to reorder not words but phrases, following (Xiong et al, 2006), or segment choice models (Kuhn et al, 2006), which assume a single segmentation of the words into phrases. For distortion modeling, Li et al (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al, 2006). Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al, 2006). The second one (SYS2) is a phrase-based system (Xiong et al, 2006) based on Bracketing Transduction Grammar (Wu, 1997) with a lex icalized reordering model (Zhang et al, 2007) under maximum entropy framework, where the phrasal translation rules are exactly the same with that of SYS1. Xiong et al (2006) is an enhanced bracket transduction grammar with a maximum entropy-based reordering model (MEBTG). In MEBTG (Xiong et al, 2006), three rules are used to derive the translation of each sub sentence: lexical rule, straight rule and inverted rule.    For example, the MaxEnt reordering model described in (Xiong et al, 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder.   Without loss of generality, we evaluate our models in a phrase-based SMT system which adapts bracketing transduction grammars to phrasal translation (Xiong et al, 2006). The reordering model MR predicts the merging order (straight or inverted) by using discriminative contextual features (Xiong et al, 2006).
Aligning Sentences In Parallel Corpora In this paper we describe a statistical technique for aligning sentences with their translations in two parallel corpora. In addition to certain anchor points that are available in our data, the only information about the sentences that we use for calculating alignments is the number of tokens that they contain. Because we make no use of the lexical details of the sentence, the alignment computation is fast and therefore practical for application to very large collections of text. We have used this technique to align several million sentences in the English-French Hansard corpora and have achieved an accuracy in excess of 99% in a random selected set of 1000 sentence pairs that we checked by hand. We show that even without the benefit of anchor points the correlation between the lengths of aligned sentences is strong enough that we should expect to achieve an accuracy of between 96% and 97%. Thus, the technique may be applicable to a wider variety of texts than we have yet tried.  Statistical alignment methods at sentence level have been thoroughly investigated Gale and Church, 1991a/ 1991b; Brown et al, 1991. Early work by Peter Brown et al (1991) and William Gale and Kenneth Church (1991) aligned sentences which had a proportional number of words and characters, respectively. There are basically three kinds of approaches on sentence alignment: the length-based approach (Gale and Church 1991 and Brown et al 1991), the lexical approach (key and Roscheisen 1993), and the combination of them (Chen 1993, Wu 1994 and Langlais 1998, etc.). Brown, Lai and Mercer (Brown et al, 1991) used word count as the sentence length, whereas Gale and Church (Gale and Church, 1991) used character count. Using this search method meant that no prior segmentation of the corpora was needed (Moore, 2002), either in terms of aligned paragraphs (Gale and Church, 1991), or some aligned sentences as anchors (Brown et al, 1991). This model is trained on approximately 5 million sentence pairs of Hansard (Canadian parliamentary) and UN proceedings which have been aligned on a sentence-by-sentence basis by the methods of (Brown et al, 1991), and then further aligned on a word-by-word basis by methods similar to (Brown et al, 1993). Brown et al (1991) and Gale and Church (1993) are amongst the most cited works in text alignment work. The method used by Brown et al (1991) measures sentence length in number of words. Given that lexical methods can be computationally expensive, our idea was to try a simple length-based approach similar to that of Brown et al (1991) for sentence alignment and then use lexical methods to align words within aligned sentences. Thus, dynamic programming based sentence alignment algorithms rely on paragraph anchors (Brown et al 1991) or lexical information, such as cognates (Simard 1992), to maintain a high accuracy rate. For instance, to produce sentence alignments, Brown et al (1991) and Gale and Church (1991) both proposed methods that completely ignored the lexical content of the texts and both reported accuracy levels exceeding 98%. The first length based algorithm was proposed in (Brown et al, 1991). Algorithm Length-based sentence alignment algorithm was first proposed in (Brown et al, 1991).  In (Gale and Church, 1991) and (Brown et al, 1991), the authors start from the fact that the length of a source text sentence is highly correlated with the length of its target text translation: short sentences tend to have short translations, and long sentences tend to have long translations. Among approaches that are unsupervised and language independent, (Brown et al, 1991) and (Gale and Church, 1993) use sentence-length statistics in order to model the relationship between groups of sentences that are translations of each other. Several automatic sentence alignment approaches have been proposed based on sentence length (Brown et al, 1991) and lexical information (Kay and Roscheisen, 1993). A bilingual sentence alignment program (Gale and Church, 1991, and Brown et al, 1991) is the crucial part in this adaptation procedure, in that it collects bilingual document pairs from the Inter net, and identifies sentence pairs, which should have a high likelihood of being correct translations of each other. The obtained accuracy is around 96% and was computed indirectly by checking disagreement with the Brown sentence aligner (Brown et al, 1991) on randomly selected 500 disagreement cases.
Articles: Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis Many approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity (also called semantic orientation). However, the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word’s prior polarity. Positive words are used in phrases expressing negative sentiments, or vice versa. Also, quite often words that are positive or negative out of context are neutral in context, meaning they are not even being used to express a sentiment. The goal of this work is to automatically distinguish between prior and contextual polarity, with a focus on understanding which features are important for this task. Because an important aspect of the problem is identifying when polar terms are being used in neutral contexts, features for distinguishing between neutral and polar instances are evaluated, as well as features for distinguishing between positive and negative contextual polarity. The evaluation includes assessing the performance of features across multiple machine learning algorithms. For all learning algorithms except one, the combination of all features together gives the best performance. Another facet of the evaluation considers how the presence of neutral instances affects the performance offeatures for distinguishing between positive and negative polarity. These experiments show that the presence of neutral instances greatly degrades the performance of these features, and that perhaps the best way to improve performance across all polarity classes is to improve the system’s ability to identify when an instance is neutral. The authors later explored the difference between prior and contextual polarity (Wilson et al, 2009): words that lose polarity in context, or whose polarity is reversed because of context. Other studies such as Na et al (2004), Ding et al (2008), and Wilson et al (2009) also explore negation shifting and achieve some improvements. Wilson et al (2009) use conjunctive and dependency relations among polarity words. In the research on recognizing contextual polarity done by Wilson et al (2009) a rich prior-polarity lexicon and dependency parsing technique were employed to detect and analyze subjectivity on phrasal level, taking into account all the power of context, captured through such features as negation, polarity modification and polarity shifters. Wilson et al (2009) proposed a two-step approach to classify word polarity out of context firstly, and then to classify word polarity in context with a wide variety of features. Domain: Following (Wilson et al, 2009), we apply a feature indicating the domain of the document to which a sentence belongs. This is significantly different from the previous input structure methods, which consider the linguistic structure as heuristic rules (Ding and Liu, 2007) or input features for classification (Wilson et al 2009). Currently, our input sentiment list exists only of prior sentiment values, however work by Wilson et al (2009) has advanced the notion of contextual polarity lists. For the task of classifying the polarity of a given expression, there has been fairly extensive work on suitable classification features (Wilson et al, 2009). The first part of each pipeline extracts opinion expressions, and this is followed by a multiclass classifier assigning a polarity to a given opinion expression, similar to that described by Wilson et al (2009). The problem of polarity classification has been studied in detail by Wilson et al (2009), who used a set of carefully devised linguistic features.  Wilson et al (2009) show that modalities as well as negations are good cues for opinion identification. Wilson et al (2009) also consider negators and in addition distinguish between positive polarity shifters and negative polarity shifters since they only reverse a particular polarity type. Among the few research efforts in this direction, Wilson et al (2009) use a list of modal words. Our treatment of negation goes beyond the approaches of (Wilson et al, 2009) (Taboada et al., 2011) and (Liu and Seneff, 2009) since we propose a specific treatment for negative polarity items and for multiple negatives. Features previously found to be useful for detecting phrase-level contextual polarity (Wilson et al, 2009) are also included. The polarity of each word in arguments is derived from Multi-perspective Question Answering Opinion Corpus (MPQA) (Wilson et al, 2009). The task has been extended to allow sentences to be annotated as displaying both positive and negative sentiment (Wilson et al, 2009) or indicating the degree of intensity (Thelwall et al, 2010). More recently, Wilson et al (2009) distinguish prior and contextual polarity, and thus describe a method to phrase-level sentiment analysis.
Using Lexical Chains for Text Summarization Reg ina  Barz i lay Mathematics and Computer S~nence Dept Ben Gunon University m the Negev Beer-Sheva, 84105 Israel regana@cs.bEu ac.  Term frequency (Luhn, 1958), lexical chains (Barzilay and Elhadad, 1997), location of the sentences (Edmundson, 1969) and the cue phrases (Teufel et al, 1997) are used to determine the important lexical units. Lexical chains, which capture relationships between related terms in a document, have shown promise as an intermediate representation for producing summaries (Barzilay and Elhadad, 1997). Various applications in Natural Language Processing, such as Question Answering (Novischi and Moldovan, 2006), Topic Detection (Carthy, 2004), and Text Summarization (Barzilay and Elhadad, 1997), rely on semantic relatedness (similarity or distance) measures either based on word nets and/or corpus statistics as a resource. Ever since Morris and Hirst (1991)'s ground breaking paper, topic segmentation has been a steadily growing research area in computational linguistics, with applications in summarization (Barzilay and Elhadad, 1997), information retrieval (Salton and Allan, 1994), and text understanding (Kozima, 1993). For each top scored chain, Barzilay and Elhadad (1997) extract econometrics statistsical methods economic analysis case studies methods measurement evaluation statistical data data analysis cartography data collection surveys censures Figure 2. Barzilay and Elhadad (1997) segment the original text and construct lexical chains that sentence which contains the first appearance of a chain member. Conceptual units can also be defined out of more basic conceptual units, based on the co-occurrence of important concepts (Barzilay and Elhadad, 1997) or syntactic constraints between representations of concepts (Hatzivassiloglou et al, 2001). A number of techniques for choosing the right sentences to extract have been proposed in the literature, ranging from word counts (Luhn, 1958), key phrases (Edmundson, 1969), naive Bayesian classification (Kupiec et al, 1995), lexical chains (Barzilay and Elhadad, 1997), topic signatures (Hovy and Lin, 1999) and cluster centroids (Radev et al, 2000). In particular, in the biomedical domain Reeve et al (2007) adapt the lexical chaining approach (Barzilay and Elhadad, 1997) to work with UMLS concepts, using the MetaMap Transfer Tool to annotate these concepts. In particular, they have successfully been used in the field of Automatic Text Summarization (Barzilay and Elhadad, 1997). But, as Barzilay and Elhadad (1997) point at, the use of a part-of-speech tagger could eliminate wrong inclusions of words such as read, which has both noun and verb entries in WordNet. So, Barzilay and Elhadad (1997) propose the first dynamic method to compute Lexical Chains. As a consequence, Silber and McCoy (2002) propose a linear time version of (Barzilay and Elhadad, 1997) lexical chaining algorithm. Their evaluation shows that their algorithm is more accurate than (Barzilay and Elhadad, 1997) and (Silber and McCoy, 2002) ones. Like in (Barzilay and Elhadad, 1997), we define a chain score which is defined in Equation 16 where |chain| is the number of words in the chain. A more fine-grained model of coherence might include proper anaphora resolution (Lee et al, 2011), which is still an unsolved task for scientific texts, and also include models of lexical coherence such as lexical chains (Barzilay and Elhadad, 1997) and entity coherence (Barzilay and Lapata, 2008). Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). Examples include summarization (Barzilay and Elhadad, 1997), question answering (Ramakrishnan et al, 2003) and machine translation (Chan and Ng, 2007). Cohesion is achieved through the use in the text of semantically related terms, reference, ellipse and conjunctions (Barzilay and Elhadad, 1997).
Feature Structures Based Tree Adjoining Grammars 1 K. Vijay-Shanker Department of Computer and Information Sciences University of Delaware Newark, DE 19711 U.S.A A. K. Joshi Del)artment of Computer and Information Science University of Pennsylvania Philadelphia, PA 19104 U.S.A Abstract  We have embedded Tree Adjoining Grammars (TAG) in a fea- ture structure based unification system.  A Feature-based TAG (Vijay-Shanker and Joshi,1988) consists of a set of (auxiliary or initial) elementary trees and of two tree-composition operations: substitution and adjunction. A Feature-based TAG (FTAG, (Vijay-Shanker and Joshi, 1988)) consists of a set of (auxiliary or initial) elementary trees and of two tree composition operations: substitution and adjunction. They arise from top-bottom feature identifications parallel to the unifications performed in FTAG (Vijay-Shanker and Joshi, 1988) and from identifications of global features. A Feature based TAG (FTAG, (Vijay-Shanker and Joshi,1988)) consists of a set of (auxiliary or initial) elementary trees and of two tree composition operations: substitution and adjunction.  The equations of top and bottom features linked to specific node positions in the elementary trees are parallel to the syntactic unifications in FTAG (Vijay-Shanker and Joshi, 1988). A Feature based TAG (FTAG, (Vijay-Shanker and Joshi,1988)) consists of a set of (auxiliary or initial) elementary trees and of two tree composition operations: substitution and adjunction.
Estimating Upper And Lower Bounds On The Performance Of Word-Sense Disambiguation Programs We have recently reported on two new word-sense disambiguation systems, one trained on bilingual material (the Canadian Hansards) and the other trained on monolingual material (Roget's Thesaurus and Grolier's Encyclopedia). After using both the monolingual and bilingual classifiers for a few months, we have convinced ourselves that the performance is remarkably good. Nevertheless, we would really like to be able to make a stronger statement, and therefore, we decided to try to develop some more objective evaluation measures. Although there has been a fair amount of literature on sense-disambiguation, the literature does not offer much guidance in how we might establish the success or failure of a proposed solution such as the two systems mentioned in the previous paragraph. Many papers avoid quantitative evaluations altogether, because it is so difficult to come up with credible estimates of performance. This paper will attempt to establish upper and lower bounds on the level of performance that can be expected in an evaluation. An estimate of the lower bound of 75% (averaged over ambiguous types) is obtained by measuring the performance produced by a baseline system that ignores context and simply assigns the most likely sense in all cases. An estimate of the upper bound is obtained by assuming that our ability to measure performance is largely limited by our ability obtain reliable judgments from human informants. Not surprisingly, the upper bound is very dependent on the instructions given to the judges. Jorgensen, for example, suspected that lexicographers tend to depend too much on judgments by a single informant and found considerable variation over judgments (only 68% agreement), as she had suspected. In our own experiments, we have set out to find word-sense disambiguation tasks where the judges can agree often that we could show that they were outperforming the baseline system. Under quite different conditions, we have found 96.8% agreement over judges. Large In (Gale et al, 1992), it was argued that any wide coverage WSD program must be able to perform significantly better than the most-frequent-sense classifier to be worthy of serious consideration. Hearst (1997, pp. 53-54) attempted to adapt π∗ to award partial credit for near misses by using the percentage agreement metric of Gale et al (1992, p. 254) to compute actual agreement which conflates multiple manual segmentations together according to whether a majority of coders agree upon a boundary or not.  The lower bound as Gale et al (1992c) suggested should be very low and it is more difficult to disambiguate if there are more senses. (Gale et al, 1992) reports that word sense disambiguation would be at least 75% correct if a system assigns the most frequently occurring sense. This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. To address the last problem, (Gale et al 1992) argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges. The need to ascertain the agreement and reliability between coders for segmentation was recognized by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al (1992, p. 254) for usage in segmentation.
Accurate Methods For The Statistics Of Surprise And Coincidence Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results. This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text. However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical. The problem of low counts (i.e. linguistic patterns that were never, or rarely found) has not been analyzed appropriately inmost papers, as convincingly demonstrated in [Dunning, 1993]. Since then, a variety of statistical methods have been proposed to measure bi-gram association, such as Log-likelihood (Dunning, 1993). The likelihood ratio tests (Dunning, 1993) is used for this purpose. Each element of the resulting vector was replaced with its log-likelihood value (see Definition 10 in Section 2.3) which can be considered as an estimate of how surprising or distinctive a co-occurrence pair is (Dunning, 1993). Many previous studies have shown that the log-likelihood ratio is well suited for this purpose (Dunning, 1993). It can be expected that the log-likelihood ratio produces an accurate ranking of word pairs that highly correlates with human judgment (Dunning, 1993), although there are other measures which come close in performance (e.g. Rapp, 1998). It is known that PMI gives undue importance to low frequency events (Dunning, 1993), therefore the evaluation considers only pairs of genes that occur at least 5 times in the whole corpus. The measures2 - Mutual Information (Church and Hanks, 1989), the log-likelihood ratio test (Dunning, 1993), two statistical tests: t-test and chi square-test, and co-occurrence frequency - are applied to two sets of data: adjective-noun (AdjN) pairs and preposition-noun-verb (PNV) triples, where the AMs are applied to (PN,V) pairs. For instance, there is a widely held belief that and are inferior to other measures because they overestimate the collocativity of low-frequency candidates (cf. the remarks on the chi square measure in (Dunning, 1993)). We tried two feature reduction methods: a simple count cutoff, and selection of the top n features in terms of log likelihood ratio (Dunning, 1993) with the target values. The LLR measurement measures stochastic dependency between two such random variables (Dunning, 1993), and is known to be equal to Mutual Information that is linearly scaled by the size of the corpus (Moore, 2004). As an alternative for determining the probability of a positive association using P (PMI& gt; 0), we calculate LLR and assume that approximately LLR with one degree of freedom (Dunning, 1993). Many statistical metrics have been proposed, including point wise mutual information (MI) (Church et al 1990), mean and variance, hypothesis testing (t-test, chi square test, etc.), log-likelihood ratio (LR) (Dunning, 1993),statistic language model (Tomokiyo, et al 2003), and so on. Given a contextual word cw that occurs in the paragraphs of bc, a log-likelihood ratio (G2) test is employed (Dunning, 1993), which checks if the distribution of cw in bc is similar to the distribution of cw in rc. Since it was first introduced to the NLPcommunity by Dunning (1993), the G2 log-likelihood-ratio statistic has been widely used in statistical NLP as a measure of strength of association, particularly lexical associations. Dunning (1993) gives the formula for the statistic we are calling G2 in a form that is very compact, but not necessarily the most illuminating. As the strength of relevance between a target compound noun t and its co-occurring word r, the feature value of r, w (t, r) is defined by the log likelihood ratio (Dunning, 1993) as follows. We then ranked the collected query pairs using log likelihood ratio (LLR) (Dunning, 1993), which measures the dependence between q1 and q2 within the context of web queries (Jones et al, 2006b). The starting point is the log likelihood ratio (Dunning 1993). Although log identifies collocations much better than competing approaches (Dunning 1993) in terms of its recall, it suffers from its relatively poor precision rates.
A Fully Statistical Approach To Natural Language Interfaces We present a natural language interface system which is based entirely on trained statistical models. The system consists of three stages of processing: parsing, semantic interpretation, and discourse. Each of these stages is modeled as a statistical process. The models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames. The OVIS annotations are in contrast with other corpora and systems (e.g. Miller et al. 1996), in that our annotation convention exploits the Principle of Compositionality of Meaning. This use of model-theoretic interpretation represents an important extension to thesemantic grammars used in existing statistical spoken language interfaces, which rely on co-occurrences among lexically-determined semantic classes and slot fillers (Miller et al, 1996), in that the probability of an analysis is now also conditioned on the existence of denoted entities and relations in the world model. Examples include an early statistical method for learning to fill slot-value representations (Miller et al, 1996) and a more recent approach for recovering semantic parse trees (Ge & Mooney, 2006). An influential precursor to this integration is the system described in (Miller et al, 1996). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al, 1996). The Miller et al (1996) approach is fully supervised and produces a final meaning representation in SQL. Evaluation Metrics Miller et al (1996) report accuracy rates for recovering correct SQL annotations on the test set. In the domain of the Air Traveler Information System (Miller et al, 1996) the authors apply statistical methods to compute the probability that a constituent can fill in a semantic slot within a semantic frame. For example, in the context of the Air Traveler Information System (ATIS) for spoken dialogue, Miller et al (1996) computed the probability that a constituent such as Atlanta filled a semantic slot such as Destination in a semantic frame for air travel.
Machine Learning Of Temporal Relations This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts. To address data sparseness, we used temporal reasoning as an oversampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93% using a Maximum Entropy classifier on human annotated data. This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions.   Our approach for labelling temporal relations (or TLINKs) is based on NLTK's maximum entropy classifier, using the feature sets initially proposed in Mani et al (2006). Thus, the features in Mani et al (2006) are augmented with those used to describe signals detailed in Derczynski and Gaizauskas (2010), with some slight changes. The performance of classifier based approaches to temporal link labelling seems to be levelling off - the 60% - 70% relation labelling accuracy of work such as Mani et al (2006) has not been greatly exceeded.  While machine learning approaches attempt to improve classification accuracy through feature engineering, Mani et al (2006) introduced a temporal reasoning component to greatly expand the training data. Recently, extensions of Mani et al (2006)'s research is briefly described in (Mani et al, 2007). This technical report addresses two problems found in (Mani et al, 2006): (1) feature vector duplication caused by the data normalization process (once fixed, the accuracy drops to 76.56% and 83.23%) and (2) a somewhat unrealistic evaluation scheme (we describe Mani et al (2007)'s results in Section 4.1).  Although Mani et al (2006) use the links introduced by closure to boost the amount of training data for a tlink classifier, this technique is not suitable for our learning task since the closure might easily propagate errors in the automatic annotations. Following (Mani et al, 2006), prior approaches exploit temporal inferences to enrich the set of training in stances used for learning.   These differences are likely to come from the fact that: (i) (Mani et al, 2006) perform a 6-way classification, and not a 13-way classification, and (ii) (Chambers and Jurafsky, 2008) use a relation set that is even more restrictive than TempEval's. As such, it emphasizes robustness at Web scale, without taking advantage of existing specification languages for representing events and temporal expressions occurring in text (Pustejovsky et al, 2003), and forgoing the potential benefits of more complex methods that extract temporal relations from relatively clean text collections (Mani et al, 2006). Taking a cue from Mani et al (2006), we also increased Timebank's size by applying transitivity rules to the hand labeled data.  Mani et al (2006) introduced a temporal reasoning component that greatly expands the available training data. In order to connect the event graph, we draw on work from (Mani et al, 2006) and apply transitive closure to our documents.
Findings of the 2012 Workshop on Statistical Machine Translation This paper presents the results of the WMT12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality. We conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics. We introduced a new quality estimation task this year, and evaluated submissions from 11 teams. This contribution has been built based on the data released for the Quality Estimation task of the Workshop on Machine Translation (WMT) 2012 (Callison-Burch et al, 2012). The two-scale scoring for adequacy and fluency used in NIST evaluation has been abandoned by some evaluation campaigns, most notably the WMT shared task series, see Koehn and Monz (2006) through Callison-Burch et al (2012) 1. Callison-Burch et al (2012) report for several automatic metrics on the whole WMT12 English-to-Czech dataset, the best of which correlates at?= 0.18. In line with our observation, Czech-to-English correlations reported by Callison-Burch et al (2012) are higher: the best metric achieves 0.28 and aver ages 0.25 across four source languages. On the other hand, it is quite possible that the WMT-style rankings taken as the gold standard are of a disputable quality themselves, see Section 3.1 or the detailed report on inter annotator agreement and a long discussion on interpreting the rankings in Callison-Burch et al (2012). It is possible that Callison-Burch et al (2012) use some what different METEOR settings apart from the different subset of the data. Nevertheless, in the context of this Quality Evaluation Shared task (see (Callison-Burch et al, 2012) for a detailed description) we have also used supervised learning as a final stage, in order to submit results which can be compared to other methods (see? 4). We investigate the use of various similarity measures for evaluating the quality of machine translated sentences. Table 2 shows the best results among the configurations we have tested (expressed using the official evaluation measures, see (Callison-Burch et al, 2012) for details). The TCD-M5P-resources-only submission ranked 5th (among 17) in the ranking task, and 5th among 19 (tied with two other systems) in the scoring task (Callison-Burch et al, 2012). Unfortunately the TCD-M5P-all submission contained an error.13 Below are the official results for TCD-M5P-resources-only and the corrected results for TCD-M5P-all: In four cases in which Google n-grams formed the reference data, the scores were computed using the wrong language (Spanish instead of English) as the reference. The WMT 2012 shared task on QE for MT (Callison-Burch et al, 2012) required participants to score and rank a set of automatic English to Spanish translations output by a state of-the-art phrase based machine translation system. An analysis of the Pearson correlation of the baseline features (Callison-Burch et al, 2012) with human quality assessments shows that the two strongest individual predictors of post-editing effort are the n-gram language model perplexities estimated on source and target sentences.
A Corpus-Based Investigation Of Definite Description Use We present the results of a study of the use of definite descriptions in written texts aimed at assessing the feasibility of annotating corpora with information about definite description interpretation. We ran two experiments, in which subjects were asked to classify the uses of definite descriptions in a corpus of 33 newspaper articles, containing a total of 1,412 definite descriptions. We measured the agreement among annotators about the classes assigned to definite descriptions, as well as the agreement about the antecedent assigned to those definites that the annotators classified as being related to an antecedent in the text. The most interesting result of this study a corpus annotation perspective was the rather low agreement = 0.63) we obtained versions of Hawkins's and Prince's classification schemes; better results = 0.76) obtained using the simplified scheme proposed by Fraurud that includes only two classes, firstmention and subsequent-mention. The agreement about antecedents was also not complete. These findings raise questions concerning the strategy of evaluating systems for definite description inby comparing their results with a standardized annotation. a linguistic of view, the most interesting observations were the great number of discourse-new definites in our corpus (in one of our experiments, about 50% of the definites in the collection were classified as discourse-new, 30% as anaphoric, and 18% as associative/bridging) and the presence of definites that did not seem to require a complete disambiguation. Such items have also been called bridging anaphors (Poesio and Vieira, 1998). Bridging anaphora, as in (2a) above, have received much attention, see e.g. Asher and Lascarides (1998) or Poesio and Vieira (1998). Associative anaphora (e.g., Poesio and Vieira, 1998) and indirect anaphora (e.g., Murata and Nagao, 2000) are virtually the same phenomena that this paper is concerned with. A serious problem when working with bridging reference sis the fact that subjects, when asked for judgments about bridging references in general, have a great deal of difficulty in agreeing on which expressions in the corpus are bridging references, and what their anchors are (Poesio and Vieira, 1998). Things are different for associative anaphora, see (Poesio and Vieira, 1998). Poesio and Vieira (1998) carried out corpus studies indicating that in corpora like the Wall Street Journal portion of the Penn Treebank. Index1 refers to the sequential numbering of definite descriptions; Index2 refers to the sequential numbering of noun phrases; and Code refers to their classification, according to discourse status (Poesio and Vieira, 1998). Poesio and Vieira (1998) found that of the 1,400 definite descriptions in their corpus, only about 50% were subsequent mention or bridging references, whereas 50% were first mentions. this was confirmed by Poesio and Vieira (1998). The annotation scheme proposed by Poesio et al (Poesio and Vieira, 1998) is a product of a corpus based analysis of definite description (DD) use showing that more than 50% of the DDs in their corpus are discourse new or unfamiliar. In fact, it has been shown that the agreement of subjects annotating bridging (Poesio and Vieira, 1998) or discourse (Cimiano,2003) relations can be too low for tentative conclusion to be drawn (Carletta, 1996). Poesio and Vieira (1998) report lower human agreement on more fine-grained classifications of definite descriptions. the results of Poesio and Vieira (1998) indicated that this type of an notation could be highly unreliable. One of our aims was to continue the work on bridging references annotation and interpretation in (Poesio and Vieira, 1998), which showed that marking up bridging references is quite hard. Only perhaps one in four or five NPs are markable (Poesio and Vieira, 1998). as Poesio and Vieira (1998) show, only about 2/3 of definite descriptions which are anaphoric have the same head as their antecedent. In previous work (Poesio and Vieira, 1998) we reported the results of corpus annotation experiments in which the subjects were asked to classify the uses of definite descriptions in Wall Stree Journal articles. The final configuration of the system was arrived at on the basis of an extensive evaluation of the heuristics using the corpus annotated in our previous work (Poesio and Vieira, 1998). In our corpus study (Poesio and Vieira, 1998) we found that our subjects did better at ideutifying discourse-new descriptions all together. This model aims to resolve both coreferent and associative (also called bridging (Poesio and Vieira, 1998)) cases of nonpronominal anaphora.
Domain Adaptation for Statistical Machine Translation with Monolingual Resources Domain adaptation has recently gained interest in statistical machine translation to cope with the performance drop observed when testing conditions deviate from training conditions. The basic idea is that in-domain training data can be exploited to adapt all components of an already developed system. Previous work showed small performance gains by adapting from limited in-domain bilingual data. Here, we aim instead at significant performance gains by exploiting large but cheap monolingual in-domain data, either in the source or in the target language. We propose to synthesize a bilingual corpus by translating the monolingual adaptation data into the counterpart language. Investigations were conducted on a stateof-the-art phrase-based system trained on the Spanish–English part of the UN corpus, and adapted on the corresponding Europarl data. Translation, re-ordering, and language models were estimated after translating in-domain texts with the baseline. By optimizing the interpolation of these models on a development set the BLEU score was improved from 22.60% to 28.10% on a test set. See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009). In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+indomain language model) and obtain 1-best translation for each source-side sentence. Bertoldi and Federico (2009) used monolingual data for adapting existing translation models to translation of data from different domains. In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but considering only additional source data. Pivoting can also intervene earlier in the process, for instance as a means to automatically generate the missing parallel resource, an idea that has also been considered to adapt an existing translation systems to new domains (Bertoldi and Federico, 2009).  For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). On the other hand, Bertoldi and Federico (2009) adapted an SMT system with automatic translations and trained the translation and reordering models on the word alignment used by moses. Further approaches to domain adaptation for SMT include adaptation using in-domain language mod els (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007).  Three experiments involving the Twitter language model confirm Bertoldi and Federico (2009)'s findings that the language model was most helpful. As has been observed before by Bertoldi and Federico (2009), it did not matter whether the synthetic data were used on their own or in addition to the original training data.
A Program For Aligning Sentences In Bilingual Corpora Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, bodies of text such as the Canadian Hansards (parliamentary proceedings), which are available in multiple languages (such as French and English). One useful step is to align the sentences, that is, to identify correspondences between sentences in one language and sentences in the other language. This paper will describe a method and a program (align) for aligning sentences based on a simple statistical model of character lengths. The program uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences. A probabilistic score is assigned to each proposed correspondence of sentences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference. This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences. It is remarkable that such a simple approach works as well as it does. An evaluation was performed based on a trilingual corpus of economic reports issued by the Union Bank of Switzerland (UBS) in English, French, and German. The method correctly aligned all but 4% of the sentences. Moreover, it is possible to extract a large subcorpus that has a much smaller error rate. By selecting the best-scoring 80% of the alignments, the error rate is reduced from 4% to 0.7%. There were more errors on the English—French subcorpus than on the English—German subcorpus, showing that error rates will depend on the corpus considered; however, both were small enough to hope that the method will be useful for many language pairs. To further research on bilingual corpora, a much larger sample of Canadian Hansards (approximately 90 million words, half in English and and half in French) has been aligned with the and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI). In addition, in order to facilitate replication of the an appendix is provided with detailed c-code of the more difficult core of the program. Labeling of sentence boundaries is a necessary prerequisite for many natural language processing (NLP) tasks, including part-of-speech tagging (Church, 1988), (Cutting et al, 1991), and sentence alignment (Gale and Church, 1993), (Kay and RSscheisen, 1993). For each aligned pair of text chunks, perform the sentence alignment method of Gale and Church (1993). For each aligned text chunk pair, we perform sentence alignment using the algorithm of Gale and Church (1993). Gale and Church (1993) based their align program on the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences. Previous investigations can be found in works such as (Gale and Church, 1993). Sentences have been aligned using the length-based dynamic programming approach of Gale and Church (1993) enhanced with a small number of lexical and non-alphabetic anchors. The algorithm used to align English-Inuktitut sentences is an extension of that presented in Gale and Church (1993). Following a suggestion in Gale and Church (1993), the alignment was aided by the use of additional anchors that were available for the language pair. All alignments that occurred in the first two sentences of each paragraph were marked as hard boundaries for the Gale and Church (1993) program as provided in their paper. For comparison, the Gale and Church (1993) program, which did not make use of additional anchors, had poorer results over our corpus. We ran a sentence alignment algorithm (Gale and Church, 1993) for each pair of English and Chinese stories. Among approaches that are unsupervised and language independent, (Brown et al, 1991) and (Gale and Church, 1993) use sentence-length statistics in order to model the relationship between groups of sentences that are translations of each other. A hybrid approach is presented in (Gale and Church, 1993) whose basic hypothesis is that longer sentences in one language tend to be translated into longer sentences in the other language, and shorter sentences tend to be translated into shorter sentences. We performed robust alignment based on sentence lengths as in (Gale and Church, 1993). Gale and Church (1993) proposed a dynamic programming algorithm for the sentence-level alignment of translations that exploited two facts: the length of translated sentences roughly corresponds to the length of the original sentences and the sequence of sentences in translated text largely corresponds to the original order of sentences. we used a slightly modified version of CharAlign described by Gale and Church (1993). The alignment of sentences can be done sufficiently well using cues such as sentence length (Gale and Church, 1993) or cognates (Simardetal., 1992). A second pass aligns the sentences in a way similar to the algorithm described by Gale and Church (1993), but where the search space is constrained to be close to the one delimited by the word alignment. In our case, the score we seek to globally maximize by dynamic programming is not only taking into account the length criteria described in (Gale and Church, 1993) but also a cognate-based one similar to (Simard et al, 1992). This is in line with previous findings that length difference can help predict alignment (cf. ,e.g., Gale and Church, 1993).
SemEval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems The goal of this task is to allow for comparison across sense-induction and discrim ination systems, and also to compare thesesystems to other supervised and knowledgebased systems. In total there were 6 participating systems. We reused the SemEval 2007 English lexical sample subtask of task17, and set up both clustering-style unsuper vised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping). We provide a comparison to the results ofthe systems participating in the lexical sam ple subtask of task 17. The object of the sense induction task of SENSEVAL-4 (Agirre and Soroa, 2007) was to cluster 27,132 instances of 100 different words (35 nouns and 65 verbs) into senses or classes. The supervised evaluation in the SEMEVAL-2010WSI/WSD task follows the scheme of the SEMEVAL 2007 WSI task (Agirre and Soroa, 2007), with some modifications. The evaluation of the collocational-graph method in the SemEval-2007 sense induction task (Agirre and Soroa, 2007) showed promising results. Our definition of context is equivalent to an instance of the target word in the SemEval-2007 sense induction task dataset (Agirre and Soroa, 2007). We followed the setting of SemEval-2007 sense induction task (Agirre and Soroa, 2007). We evaluate our method on the nouns of the SemEval-2007 word sense induction task (Agirre and Soroa, 2007) under the second evaluation setting of that task ,i.e. supervised evaluation. We followed the same sense mapping method as in the SemEval-2007 sense induction task (Agirre and Soroa, 2007). We evaluate our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art. The remainder of this paper is structured as follows. For evaluation, we used the Semeval-2007 benchmark dataset released as part of the sense induction and discrimination task (Agirre and Soroa, 2007). Evaluation Methodology Agirre and Soroa (2007) present two evaluation schemes for assessing sense induction methods. The SemEval-2007 word sense induction task (Agirre and Soroa, 2007) already allows for evaluation of automatic sense induction systems, but compares output to gold-standard senses from Onto Notes. While word senses have been studied extensively in lexical semantics, research has focused on word sense disambiguation, the task of disambiguating words in context given a predefined sense inventory (e.g., Agirre and Edmonds (2006)), and word sense induction, the task of learning sense inventories from text (e.g., Agirre and Soroa (2007)). The SemEval-2007 WSI task (SWSI) participating systems UOY and UBC-AS used labeled data for parameter estimation (Agirre and Soroa, 2007a), while the authors of I2R, UPV SI and UMND2 have empirically chosen values for their parameters. The collocational WSI approach was evaluated under the framework and corpus of SemEval-2007 WSI task (Agirre and Soroa, 2007a). Thus, inducing a number of clusters similar to the number of senses is not a requirement for good results (Agirre and Soroa, 2007a). High supervised recall means high purity and entropy, as in I2R, but not vice versa, as in UOY. Context vectors are clustered and the resulting clusters represent the induced senses. Recently, graph-based methods have been employed for word sense induction (Agirre and Soroa, 2007). The second type of evaluation, supervised evaluation, follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007). This evaluation follows the supervised evaluation of SemEval-2007WSI task (Agirre and Soroa, 2007), with the difference that the reported results are an average of 5 random splits. Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval 20071 (Agirre and Soroa, 2007). The evaluation data comes from the WSI task of SemEval-2007 (Agirre and Soroa, 2007).
Arabic Tokenization Part-Of-Speech Tagging And Morphological Disambiguation In One Fell Swoop We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including partof-speech tagging) Arabic words in one process. We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer. We obtain accuracy rates on all tasks in the high nineties. Both Diab et al (2004) and Habash and Rambow (2005) use support-vector machines with local features; the former for tokenization, POS tagging, and base phrase chunking; the latter for full morphological disambiguation. Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes. Most available Arabic NLP tools and resources model morphology using surface inflectional features and do not mark rationality; this includes the PATB (Maamouri et al, 2004), the Buckwalter morphological analyzer (BAMA) (Buckwalter, 2004) and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) system (Habash and Rambow, 2005). MADA is an SVM based system that disambiguates among different morphological analyses produced by BAMA (Habash and Rambow, 2005). In a previous publication, we described the Morphological Analysis and Disambiguation of Arabic (MADA) system (Habash and Rambow, 2005). The algorithm we proposed in (Habash and Rambow, 2005) for choosing the best BAMA analysis simply counts the number of predicted values for the set of linguistic features in each candidate analysis. To create these schemes, we use MADA, an off-the-shelf resource for Arabic morphological disambiguation (Habash and Rambow, 2005), and TOKAN, a general Arabic tokenizer (Habash and Sadat, 2006). We tokenize using the MADA morphological disambiguation system (Habash and Rambow, 2005), and TOKAN, a general Arabic tokenizer (Sadat and Habash, 2006). MADA (Habash and Rambow, 2005) is used to pre-process the Arabic text for the translation model and 5-gram language model (LM). Our data is gold tokenized; however, all of the features we use are predicted using MADA (Habash and Rambow, 2005) following the work of Marton et al (2010). For predicting morphological features, we use the MADA system (Habash and Rambow, 2005). We compare our results with the form-based features from the state-of-the-art morphological analyzer MADA (Habash and Rambow, 2005). They use a trigram language model and the Arabic morphological analyzer MADA (Habash and Rambow, 2005) respectively, to segment the Arabic side of their corpora. We use the Morphological Analyzer MADA (Habash and Rambow, 2005) to decompose the Arabic source. The Arabic side is segmented according to the Arabic Treebank tokenization scheme (Maamouri et al, 2004) using the MADA + TOKAN morphological analyzer and tokenizer (Habash and Rambow, 2005). Past approaches include rule-based morphological analyzers (Buckwalter, 2004) and supervised learning (Habash and Rambow, 2005). The Morphological Analysis and Disambiguation of Arabic (MADA) system is described in (Habash and Rambow, 2005). Habash and Rambow (2005) use SVM-classifiers for individual morphological features and a simple combining scheme for choosing among competing analyses proposed by the dictionary. To make our results more comparable to those by Habash and Rambow (2005), we converted the test set with the POS tags from the whole word tagger to their tokenization and to a reduced tag set of 15 tags. Therefore, we repeated the experiments above with POS tags predicted by the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow, 2005).
MindNet: Acquiring and Structuring Semantic Information from Text As a lexical knowledge base constructed automatically from the definitions and example sentences in two machine-readable dictionaries (MRDs), MindNet embodies several features that distinguish it from prior work with MRDs. It is, however, more than this static resource alone. MindNet represents a general methodology for acquiring, structuring, accessing, and exploiting semantic information from natural language text. This paper provides an overview of the distinguishing characteristics of MindNet, the steps involved in its creation, and its extension beyond dictionary text. Instead, we parse them into a default analysis, which can then be expanded and disambiguatcd at later stages of processing using a large semantic knowledge base (Richardson 1997, Richardson et al 1998). Others have automatically extracted attribute relations from dictionary definitions (Richardson et al, 1998), structured online sources such as Wikipedia info boxes, (Wu and Weld, 2007) and large-scale collections of high-quality tabular web data (Cafarella et al, 2008). The Mindnet is a general-purpose database of semantic information (Richardson et al 1998) that has been repurposed as the primary repository of translation information for MT applications. MindNet (Richardson et al, 1998) is both an extraction methodology and a lexical ontology different from a word net since it was created automatically from a dictionary and its structure is based on such resources.  Such an approach was taken by the MindNet project (Richardson et al, 1998). One early example was MindNet (Richardson et al, 1998), which was based on collecting 24 semantic role relations from MRDs such as the American Heritage Dictionary. Richardson et al (1998) describes how MindNet began as a lexical knowledge base containing LF-like structures that were produced automatically from the definitions and example sentences in machine-readable dictionaries. Synonymy, hypernymy or meronymy fall clearly in this latter category, and well known resources like WordNet (Miller, 1995), EuroWordNet (Vossen, 1998) or MindNet (Richardson et al, 1998) contain them. Graphs also can be 69 generated from dictionaries, and used to produce knowledge bases (Richardson et al., 1998) or proximity information (Gaume et al, 2006).
Integrating Multiple Knowledge Sources To Disambiguate Word Sense: An Exemplar-Based Approach In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation. We tested our WSD program, named LEXAS, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed. LEXAS achieves a higher accuracy on the common data set, and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WORDNET. We used the method of extraction by Ng and Lee (1996) and encoded all keywords in a binary bag of words model. Likewise, (Ng and Lee, 1996) report overall accuracy for the noun interest of 87%, and find that that when their feature set only consists of co-occurrence features the accuracy only drops to 80%.   Previous experiments (Ng and Lee, 1996) have explored the relative contribution of different knowledge sources to WSD and have concluded that collocational information is more important than syntactic information. The DSO collection (Ng and Lee, 1996) focuses on 191 frequent and polysemous words (nouns and verbs), and contains around 1,000 sentences per word. Exemplar-based method makes use of typical contexts (exemplars) of a word sense, e.g., verb noun collocations or adjective-noun collocations, and identifies the correct sense of a word in a particular context by comparing the context with the exemplars (Ng and Lee, 1996). Moreover, the effectiveness of this method on disambiguating words in large-scale corpora into fine-grained sense distinctions needs to be further investigated (Ng and Lee, 1996). Hence, besides gathering examples from the widely usedSEMCOR corpus, we also gathered training examples from 6 English-Chinese parallel corpora and the DSO corpus (Ng and Lee, 1996). Besides SEMCOR, the DSO corpus (Ng and Lee, 1996) also contains manually annotated examples for WSD. Our approach to memory-based all-words WSD follows the memory based approach of (Ng and Lee, 1996), and the work by (Veenstra et al, 2000) on a memory based approach to the English lexical sample task of SENSEVAL-1. The keywords were selected through a selection method suggested by (Ng and Lee, 1996) within three sentences around the ambiguous word; only content words were used as candidates. In the following testing phase, a word is classified into senses (Mihalcea, 2002) (Ng and Lee, 1996). This feature set is similar to the one used by (Ngand Lee, 1996), as well as by a number of state-of the-art word sense disambiguation systems participating in the SENSEVAL-2 and SENSEVAL-3evaluations. The high accuracy of the LEXAS system (Ng and Lee, 1996) is due in part to the use of large corpora. The disambiguation is then done using k-NN (Ng and Lee, 1996) where the k nearest neighbors of the test sentence are identified using this scoring function. The idea of using supervised machine learning for WSD is not new and was used for example in (Ng and Lee, 1996). We report results of comparing our lexicon with theWordNet cousins as well as the inter-annotator disagreement observed between two semantically an notated corpora: WordNet Semcor (Landes et al, 1998) and DSO (Ng and Lee, 1996). To test if the sense partitions in our lexicon constitute an appropriate (or useful) level of granularity, we applied it to the inter-annotator disagreement observed in two semantically annotated corpora: WordNet Semcor (Landes et al, 1998) and DSO (Ng and Lee, 1996). The set of features needed for the training of the system is described in figure 1, and is based on the feature selection made by Ng and Lee (1996) and Escudero et al (2000).
Weakly Supervised Learning for Hedge Classification in Scientific Literature We investigate automatic classification of speculative language (‘hedging’), in biomedical text using weakly supervised machine learning. Our contributions include a precise description of the task with annotation guidelines, analysis and discussion, a probabilistic weakly supervised learning model, and experimental evaluation of the methods presented. We show that hedge classification is feasible using weakly supervised ML, and point toward avenues for future research. Medlock and Briscoe (2007) proposed a weakly supervised setting for hedge classification in scientific texts where the aim is to minimise human supervision needed to obtain an adequate amount of training data. The authors are only aware of the following related corpora: the Hedge classification corpus (Medlock and Briscoe, 2007), which has been annotated for hedge cues (at the sentence level) and consists of five full biological research papers (1537 sentences). 5 articles from FlyBase (the same data were used by Medlock and Briscoe (2007) for evaluating sentence-level hedge classifiers) and 4 articles from the open access BMC Bioinformatics website were downloaded and annotated for negations, uncertainty and their scopes. Solving the sentence level task, Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles as speculative or non speculative. A misinterpretation of the BioScope paper (Szarvas et al, 2008) led us to believe that five of the nine full articles in the training data were annotated using the guidelines of Medlock and Briscoe (2007). Medlock and Briscoe (2007) extended the work of Light et al (2004) by refining their annotation guidelines and creating a publicly available data set (FlyBase data set) for speculative sentence classification. Szarvas (2008) extended the weakly supervised machine learning methodology of Medlock and Briscoe (2007) by applying feature selection to reduce the number of candidate keywords, by using limited manual supervision to filter the features, and by extending the feature representation with bigrams and trigrams. In addition, by following the annotation guidelines of Medlock and Briscoe (2007), Szarvas (2008) made available the BMC Bioinformatics data set, by annotating four full text papers from the open access BMC Bioinformatics website. In related work, Szarvas (2008) extended the methodology of Medlock and Briscoe (2007), and presented a hedge detection method in biomedical texts with a weakly supervised selection of keywords. For speculative sentences detection, Medlock and Briscoe (2007) report their approach based on weakly supervised learning. Medlock and Briscoe (2007) also used single words as input features in order to classify sentences from scientific articles in biomedical domain as speculative or non-speculative. Medlock and Briscoe (2007) use a similar baseline as the one adopted by Light et al (2004), i.e. a naive algorithm based on substring matching, but with a different list of terms to match against. However Medlock and Briscoe (2007) note that their model is unsuccessful in identifying assertive statements of knowledge paucity which are generally marked rather syntactically than lexically. Kilicoglu and Bergler (2008) did experiments on the same dataset as Medlock and Briscoe (2007) and their experimental results proved that the classification accuracy can be improved by approximately 9% (from an F-score of 76% to an F-score of 85%) if syntactic and semantic information are incorporated. The experiments run by Medlock (2008) on the same dataset as Medlock and Briscoe (2007) show that adding features based on part-of speech tags to a bag-of-words input representation can slightly improve the accuracy, but the improvements are marginal and not statistically significant. Other early work focused on semi supervised learning due to a lack of annotated datasets (Medlock and Briscoe, 2007). We can then use the large amounts of unannotated sentences that are available to extract n-gram features that have high uncertainty class conditional probability and add them to our training set with those features labeled as hedges as described in Medlock and Briscoe (2007). Medlock and Briscoe (2007) used single words as input feature sin order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-automatically collected training examples. Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features. Early work on speculative language detection tried to classify a sentence either as speculative or non-speculative (see, for example, Medlock and Briscoe (2007)).
A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging. With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly. Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging. On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline. Following Jiang et al (2008), we describe segmentation and Joint S&T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2. As described in Ng and Low (2004) and Jiang et al (2008), we use s indicating a single character word, while b, m and e indicating the begin, middle and end of a word respectively. Templates called lexical-target in the column below are introduced by Jiang et al (2008). For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j. Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging. We use the feature templates the same as Jiang et al, (2008) to extract features form E model.  The first is the "character-based" approach, where basic processing units are characters which compose words (Jiang et al., 2008a).  We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results. We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm. However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper). Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0]). As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle. Inspired by (Jiang et al, 2008), we set the real-value of C0 to be 2.0, the value of C-1C0 and C0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model.  Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004).   
Language Model Adaptation For Statistical Machine Translation Via Structured Query Models We explore unsupervised language model adaptation techniques for Statistical Machine Translation. The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection. Specific language models are then build from the retrieved data and interpolated with a general background model. Experiments show significant improvements when translating with these adapted language models. Zhao et al (2004) apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then finding similar sentences in a monolingual target language corpus. Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al, 2004), though no previous research has examined the language model domain adaptation problem for text simplification. individual target hypotheses (Zhao et al, 2004). Zhao et al (2004) construct a baseline SMT system using a large background language model and use it to retrieve relevant documents from large monolingual corpora and subsequently interpolate the resulting small domain-specific language model with the background language model. (Zhao et al, 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. Zhao et al (2004) converted initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection. Refinements of this approach are described in (Zhao et al., 2004). These schemes are overall limited by the quality of the translation hypotheses (Tam et al2007 and 2008), and better initial translation hypotheses lead to better selected sentences (Zhao et al., 2004). This adaptation technique was first proposed by Zhao et al (2004). Zhao et al (2004) and Eck et al (2004) introduce information retrieval method for language model adaptation.
D-Theory: Talking About Talking About Trees Linguists, including computational linguists, have always been fond of talking about trees. In this paper, we outline a theory of linguistic structure which talks about talking about trees; we call theory theory While important issues must be resolved before a complete picture of D-theory emerges (and also before we can build programs which utilize it), we believe that this theory will ultimately provide a framework for explaining the syntax and semantics of natural in a manner which is This paper will focus primarily on one set of motivations for this theory, those engendered by attempts to handle certain syntactic phenomena within the framework of deterministic parsing. Description theory (henceforth, D-theory) (Marcus et al (1983)). This model is interesting in that it does not allow the parser to employ delay tactics, such as using a lookahead buffer (Marcus (1980), Marcus et al (1983)), or waiting for the head of a phrase to appear in the input before constructing that phrase (Abney (1987, 1989), Pritehett (1992)). The original D-theory model (Marcus et al (1983)) is also more powerful, because it allows the right-most daughter of a node to be lowered under a sibling node. More on Dominance Links Dominance links are quite common in tree description formalisms, where they were already in use in D-theory (Marcus et al, 1983) and in quasi-tree semantics for fb TAGs (Vijay-Shanker, 1992). S /npvp Mary/ V S th inks /npvp^ John M&C suggest various possibilities for packing the partial syntax trees, including using Tree Adjoining Grammar (Joshi 1987) or Description Theory (Marcus et al 1983).
Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure. While previous work has focused primarily on English, we extend these results to other languages along two dimensions. First, we show that these results hold true for a number of languages across families. Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction. Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%. When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.    Recently, Tackstrom et al (2012) tested the incorporation of cluster features from unlabeled corpora in a multilingual setting, giving an algorithm for inducing cross-lingual clusters. Specifically, we extend the method recently proposed by Tackstrom et al (2012), which is based on cross-lingual word cluster features. Specifically, we extend the direct transfer method proposed by Tackstrom et al (2012) in two ways. Recently, Tackstrom et al (2012) developed an algorithm for inducing cross-lingual word clusters and proposed to use these clusters to enrich the feature space of direct transfer systems. Tackstrom et al (2012) showed that this is, at least to some degree, achievable by coupling monolingual class-based language models, via word alignments. This is due to limitations in the sequence labeling software used and gives slightly lower results, across the board, than those reported by Tackstrom et al (2012).  A later extension of Tackstrom et al (2012) enriches this representation with cross-lingual word clusters, considerably improving the performance.      Unfortunately, the models presented in the previous work, such as Zeman and Resnik (2008), McDonald et al (2011) and Tackstrom et al (2012), were not made available, so we reproduced the direct transfer algorithm of McDonald et al (2011), using Malt parser (Nivre, 2008) and the same set of features.   
Syntax Annotation for the GENIA Corpus Linguistically annotated corpus based on texts in biomedical domain has been constructed to tune natural language processing (NLP) tools for bio textmining. As the focus of information extraction is shifting from "nominal" information such as named entity to "verbal" information such as function and interaction of substances, applica tion of parsers has become one of the key technologies and thus the corpus annotated for syntactic structure of sen tences is in demand. A subset of the GENIA corpus consisting of 500 MEDLINE abstracts has been annotated for syntactic structure in an XML based format based on Penn Treebank II (PTB) scheme. Inter-annotator agreement test indicated that the writ ing style rather than the contents of the research abstracts is the source of the difficulty in tree annotation, and that annotation can be stably done by linguists without much knowledge of bi ology with appropriate guidelines regarding to linguistic phenomena par ticular to scientific texts. The Penn-style treebank for GENIA, created by Tateisi et al (2005), currently contains 500 abstracts. More detail, the tokenized text was done by GENIA tools, and the syntactic analyses was created by the McClosky-Charinak parser (McClosky and Charniak, 2008), trained on the GENIA Treebank corpus (Tateisi et al, 2005), which is one of the most accurate parsers for biomedical documents. Table 1 lists three corpora in the biomedical domain that are annotated with deep syntactic structures; CRAFT (described below), GENIA (Tateisi et al, 2005), and Penn BIOIE (Bies et al, 2005). We used the Stanford parser (Klein and Manning, 2003), and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank (Tateisi et al, 2005) for biomedical text. We created the data set by building on the annotation of the GENIA Event corpus (Kim et al, 2008), making use of the rich set of annotations already contained in the corpus: term annotation for NEs and other entities (Ohta et al, 2002), annotation of events between these terms, and treebank structure closely following the Penn Treebank scheme (Tateisi et al, 2005). We first converted the gold standard annotation of the GENIA treebank (Tateisi et al, 2005) into a dependency representation using the Stanford parser tools (de Marneffe et al, 2006) and then determined the shortest paths in the dependency analyses connecting each relevant entity with each NE. Further, all of the biomedical domain models have been created with reference and for many parsers with direct training on the data of (a subset of) the GENIA treebank (Tateisi et al., 2005). The resulting parser was tested on a test corpus of hand-parsed sentences from the Genia Treebank (Tateisi et al, 2005). These expected benefits drive the development of domain-specific resources, such as the GENIA treebank (Tateisi et al., 2005), and parser domain adaption (Hara et al, 2007), which are of clear importance in parsing research, but of largely unconfirmed impact on practical systems. The parser was trained using 8,000 sentences from the GENIA Treebank (Tateisi et al, 2005), which contains abstracts of papers taken from MEDLINE, annotated with syntactic structures. For preprocessing, all the sentences in the Bio scope corpus are tokenized and then parsed using the Berkeley parser (Petrov and Klein, 2007) trained on the GENIA TreeBank (GTB) 1.0 (Tateisi et al, 2005), which is a bracketed corpus in (almost) PTB style. Our biomedical data comes from the GENIA treebank (Tateisi et al, 2005), a corpus of abstracts from the Medline database. The task dataset consists of new annotations for the GENIA corpus (Kim et al, 2008), building on the existing biomedical term annotation (Ohta et al., 2002), the gene and gene product name annotation (Ohta et al, 2009) and the syntactic annotation (Tateisi et al, 2005) of the corpus. Because our target is biomedical texts, we re-trained a parser (Hara et al., 2005) with the GENIA tree bank (Tateisi et al., 2005), and also applied a bidirectional part-of speech tagger (Tsuruoka and Tsujii, 2005) trained with the GENIA tree bank as a preprocessor. The BioNLP-09 shared task involved documents contained also in the GENIA treebank (Tateisi et al., 2005), creating an opportunity for direct study of intrinsic and task-oriented evaluation results. For comparison and evaluation, the texts in the GENIA treebank (Tateisi et al, 2005) are converted to the various formats as follows. The Penn-style treebank for GENIA, created by Tateisi et al (2005), currently contains 500 abstracts. The native dependency parsers were re-trained on the GENIA Treebank (Tateisi et al, 2005) conversions. The data sets for the COREF task are produced based on three resources: MedCO coreference annotation (Su et al, 2008), Genia event annotation (Kim et al, 2008), and Genia Treebank (Tateisi et al., 2005). In contrast, the GENIA Treebank Corpus (Tateisi et al, 2005) is estimated to have no imperative sentences and only seven interrogative sentences (see Section 5.2.2).
A Semantic Approach To IE Pattern Induction This paper presents a novel algorithm for the acquisition of Information Extraction patterns. The approach makes the assumption that useful patterns will have similar meanings to those already identified as relevant. Patterns are compared using a variation of the standard vector space model in which information from an ontology is used to capture semantic similarity. Evaluation shows this algorithm performs well when compared with a previously reported document-centric approach. It has also been shown that these techniques prove useful for tasks such as word sense disambiguation (Patwardhan et al, 2003), real-word spelling correction (Budanitsky and Hirst, 2001) and information extraction (Stevenson and Greenwood, 2005), among others.  Stevenson and Greenwood (2005) evaluated their method through document and sentence filtering at the scenario level. Many of the adaptive IE systems rely on the existing part-of-speech (POS) taggers (Debnath and Giles, 2005) and/or syntactic parsers (Stevenson and Greenwood, 2005) for analysing and annotating text corpora. Stevenson and Greenwood (2005) propose a weakly supervised approach to sentence filtering that uses semantic similarity and bootstrapping to acquire IE patterns. Our method is modeled on the approach developed by Stevenson and Greenwood (2005) but uses a different technique for ranking candidate patterns. Stevenson and Greenwood (2005) use subject-verb-object triples for their features. This is a significant improvement over the 0.58 F-measure score reported by Stevenson and Greenwood (2005) for the same task.    Stevenson and Greenwood (2005) suggested an alternative method for ranking the candidate patterns.   Stevenson and Greenwood (2005) suggested an alternative method for ranking the candidate patterns by lexical similarities. To our knowledge, the only previous study that embeds similarities into the acquisition of extraction patterns is (Stevenson and Greenwood, 2005). We adapted the method of matrix similarity given by Stevenson and Greenwood (2005).
Multilingual Authoring using Feedback Texts There are obvious reasons for trying to automate the production of multilingual documentation, especially for routine subject-matter in restricted domains (e.g. technical instructions). Two approaches have been adopted: Machine Translation (MT) of a source text, and Multilingual Natural Language Generation (M-NLG) from a knowledge base. For MT, information extraction is a major difficulty, since the meaning must be derived by analysis of the source text; M-NLG avoids this difficulty but seems at first sight to require an expensive phase of knowledge engineering in order to encode the meaning. We introduce here a new technique which employs M-NLG during the phase of knowledge editing. A 'feedback text', generated from a possibly incomplete knowledge base, describes in natural language the knowledge encoded so far, and the options for extending it. This method allows anyone speaking one of the supported languages to produce texts in all of them, requiring from the author only expertise in the subject-matter, not expertise in knowledge engineering. Paper will be implemented in ICONOCLAST, an authoring tool which enables domain experts to create a knowledge base through a sequence of interactive choice and generates hiexarchitally structured text according to various tylistic constraints (See Power and Scott 1998). One important source of inspiration for GF was the WYSIWYM system (Power and Scott 1998), which used domain-specific interlinguas and produced excellent quality in multilingual generation. Figure 1: Architecture of a MDA system creation to systems presenting the user with the evolving text of the document (often called the feedback or control text) in her language, following from the WYSIWYM (What You See Is What You Meant) approach (Power and Scott, 1998). The MDA (Multilingual Document Authoring) system [Brun et al2000] is an instance (descended from Ranta's Grammatical Framework [Ranta 2002]) of a text-mediated interactive natural language generation system, a notion introduced by [Power and Scott 1998] under the name of WYSIWYM. querying Conceptual authoring through WYSIWYM editing alleviates the need for expensive syntactic and semantic processing of the queries by providing the users with an interface for editing the conceptual meaning of a query instead of the surface text (Power and Scott, 1998). The "WYSIWYM" approach was proposed ([Power and Scott, 1998], [Paris and Vander Linden, 1996]) as a system design methodology where users author and manipulate an underlying logical form through a user interface that provides feedback in natural language text. In an influential series of papers [Power and Scott, 1998], WYSIWYM (What You See Is What You Mean) was proposed as a method for the authoring of semantic information through direct manipulation of structures rendered in natural language text. The system belongs to the family of WYSIWYM (What You See Is What You Mean) (Power and Scott, 1998) text generation systems: logical forms are entered interactively and the corresponding linguistic realization of the expressions is generated in several languages. The method was invented to meet the needs of applications using WYSIWYM editing (Power and Scott, 1998), which allow an author to control the content of an automatically generated text without prior training in knowledge engineering. Wysiwym (What You See Is What You Meant) is a user-interface technology through which a domain expert can formally encode knowledge by structured editing of an automatically generated feedback text (Power and Scott, 1998).
A Centering Approach To Pronouns In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns. As described in [GJW86], the process of centering attention on entities in the discourse gives rise to the intersentential states of, retaining shiftpropose an extension to these states which handles some additional cases of multiple ambiguous pronouns. The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application.   Centering models local coherence rather generally and has been applied to the generation of referring expressions (Kibble and Power, 2004), to resolve pronouns (Brennan et al, 1987, inter alia), to score essays (Miltsakaki and Kukich, 2004), to arrange sentences in the correct order (Karamanis et al, 2009), and to many other tasks. To our knowledge, there are only two focus-based pronoun resolution algorithms that are specified in enough detail to work on unrestricted naturally occurring text: Brennan et al (1987) using the definition of utterance according to Kameyama (1998), and Struhe (1998). In Section 4, I compare the results of my algorithm with the results of the centering algorithm (Brennan et al, 1987) with and without specifications for complex sentences (Kameyama, 1998). For their centering algorithm, Brennan et al (1987, henceforth BFP-algorithm) extend the notion of centering transition relations, which hold across adjacent utterances, to differentiate types of shift. To illustrate this algorithm, we consider example (1) (Brennan et al, 1987) which has two different final utterances (ld) and (ld~). Table 1 shows the most common classification into the four types CONTINUE, RETAIN, SMOOTH-SHIFT, and ROUGH-SHIFT, which are predicted to be less and less coherent in this order (Brennan et al, 1987). Finally, the measure M.BFP (Brennan et al, 1987) uses a lexicographic ordering on 4-tupleswhich indicate whether the transition is a CONTINUE, RETAIN, SMOOTH-SHIFT, or ROUGH SHIFT. Note that common centering algorithms (e.g., the one by Brennan et al (1987)) are specified only for the resolution of anaphors in Ui-1. The rules are applied locally, across adjacent sequences of utterances (Brennan et al., 1987). The ranking of the CFs other than the CP is defined according to the following preference on their gf (Brennan et al, 1987): obj>iobj>other. We also make note of the preference between these transitions, known as Centering Rule 2 (Brennan et al, 1987). The main assumptions of the theory as presented by (Gross et al 1995 (GJW), Brennan et al1987). For instance Passoneau (1998) refers to two variants of CT: Version A, based on Brennan et al (1987) and Version B, taken from Kameyama et al (1993). The first strategy is clearly appropriate for interpretation (cf. Brennan et al 1987) but for generation the issue is less clear-cut. Brennan et al [1987] propose an algorithm for pronoun resolution based on centering theory. The latter proposes the ranking subject, direct object, indirect object (Brennan et al 1987) and noun phrases which are parts of prepositional phrases are usually indirect objects. Hard-core centering approaches only deal with the last sentence (Brennan et al, 1987). PF.BFP which is based on PF as well as the original formulation of CT in [Brennan et al, 1987].
A Statistical Approach To Machine Translation this paper, we present a statistical to machine translation. We describe the application of our approach to translation from French to English and give preliminary results. Parallel texts have recently received considerable attention in machine translation (e.g., Brown et al 1990). They are used in many applications, e.g. word prediction (Bickel et al, 2005), speech recognition (Rabiner and Juang, 1993), machine translation (Brown et al, 1990). probability P (tw|dw) can be learned from the training corpus using statistical translation model (Brown et al, 1990). Systems for automatic translation between languages have been divided into transfer-based approaches, which rely on interpreting the source string into an abstract semantic representation from which text is generated in the target language, and statistical approaches, pioneered by Brown et al (1990), which estimate parameters for a model of word-to-word correspondences and word re-orderings directly from large corpora of parallel bilingual text. This decomposition into two knowledge sources is known as the source-channel approach to statistical machine translation (Brown et al, 1990). Giza++ (Och and Ney,2003) is used to induce, based on statistical principles (Brown et al, 1990), an automatic word alignment of SMS tokens with their normalized counterparts. The noisy channel model approach is being successfully applied to various natural language processing (NLP) tasks, such as speech recognition (Jelinek, 1985), spelling correction (Kernighan et al, 1990), machine translation (Brown et al, 1990). This approach is a generalization of the source channel approach (Brown et al, 1990). However, we can learn to attribute some similarity between (Brown et al, 1990) and the second publication using the text in (Marcu and Wong, 2002). Now we can attribute some similarity between the (Brown et al, 1990) and (Marcu and Wong, 2002) publication since they contain similar keywords. In such translation, given a source language text, S, the translated text, T, in the target language that maximizes the probability P (T |S) is selected as the most appropriate translation, T best, which is represented as (Brown et al, 1990). This approach is a generalization of the source channel approach (Brown et al, 1990). Akhmatova and Dras (2007) described a two-fold probabilistic approach to recognizing entailment, that in its turn was based on the well-known noisy channel model from Statistical Machine Translation (Brown et al., 1990). The second method proposed by RALI is based on a dynamic programming scheme which uses a score function derived from a translation model similar to that of (Brown et al, 1990). The decomposition into two knowledge sources in Equation 2 is known as the source-channel approach to statistical machine translation (Brown et al, 1990). But for other tasks, such as machine translation (Brown et al, 1990), the chief merit of unlabeled data is simply that nothing else is available. Brown et al (1990) gradually increased learning difficulty using a series of increasingly complex models for machine translation. This approach is a generalization of the source channel approach (Brown et al, 1990). For example, when considering whether to align two words in the IBM models (Brown et al, 1990), one can not easily include information about such features as orthographic similarity (for detecting cognates), presence of the pair in various dictionaries, similarity of the frequency of the two words, choices made by other alignment systems on this sentence pair, and so on. Statistical machine translation was introduced by work at IBM [Brown et al, 1990, 1993].
A General Framework For Distributional Similarity We present a general framework for distributional similarity based on the concepts of precision and recall. Different parameter settings within this framework approximate different existing similarity measures as well as many more which have, until now, been unexplored. We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns.  (Weeds and Weir, 2003)) measure of Lin (1998) as a representative case, and utilized it for our analysis and as a starting point for improvement.  However, it is not at all obvious that one universally best measure exists for all applications (Weeds and Weir, 2003).  Weeds and Weir (2003) proposed a general framework for distributional similarity that mainly consists of the notions of what they call Precision and Recall. title=Textual_Entailment_Resource_Pool 69 To date, most distributional similarity research concentrated on symmetric measures, such as the widely cited and competitive (as shown in (Weeds and Weir, 2003)) LIN measure (Lin, 1998): LIN (u, v)=? f? FV u? FV v [w u (f)+ w v (f)]? f? FV u w u (f)+? f? FV v w v (f) where FV x is the feature vector of a word x and w x (f) is the weight of the feature f in that word? s vector, set to their point wise mutual information.  For this reason, a new approach could be envisaged for this task, in the direction of the work by (Weeds and Weir, 2003), by building rankings of similarity for each verb. As a case study, we used our evaluation methodology to compare four methods for learning entailment rules between predicates: DIRT (Lin and Pantel,2001), Cover (Weeds and Weir, 2003), BInc (Szpek tor and Dagan, 2008) and Berant et al (2010).
Automatic sense prediction for implicit discourse relations in text We present a series of experiments on auidentifying the sense of imrelations, i.e. relations that are not marked with a discourse connective such as “but” or “because”. We work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses. We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features. In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications. Our best combination of features outperforms the baseline from data intensive approaches by 4% for comparison and 16% for contingency. Another research line is to exploit various linguistically informed features under the framework of supervised models, (Pitler et al, 2009a) and (Lin et al, 2009), e.g., polarity features, semantic classes, tense, production rules of parse trees of arguments, etc. Our study on PDTB test data shows that the average f-score for the most general 4 senses can reach 91.8% when we simply mapped the ground truth implicit connective of each test instance to its most frequent sense. In this paper, we include 9 types of features in our system due to their superior performance in previous studies, e.g., polarity features, semantic classes of verbs, contextual sense, modality, inquirer tags of words, first-last words of arguments, cross-argument word pairs, ever used in (Pitler et al, 2009a), production rules of parse trees of arguments used in (Lin et al, 2009), and intra-argument word pairs inspired by the work of (Saito et al, 2006). Here we provide the details of the 9 features, shown as follows: Verbs: Similar to the work in (Pitler et al, 2009a), the verb features consist of the number of pairs of verbs in Arg1 and Arg2 if they are from the same class based on their highest Levin verb class level (Dorr, 2001). Following the work of (Pitler et al, 2009a), we used sections 2-20 as training set, sections 21-22 as test set, and sections 0-1 as development set for parameter optimization. Here the numbers of training and test instances for Expansion relation are different from those in (Pitler et al, 2009a). Table 2 summarizes the best performance achieved by the baseline system in comparison with previous state-of-the-art performance achieved in (Pitler et al, 2009a). Table 2: Performance comparison of the baseline system with the system of (Pitler et al, 2009a) on test set.  Although on Comparison relation there is only as light improvement (+1.07%), our two best systems both got around 10% improvements of f score over a state-of-the-art system in (Pitler et al, 2009a). This also encourages our future work on finding the most suitable connectives for implicit relation recognition. From this table, we found that, using only predicted implicit connectives achieved an comparable performance to (Pitler et al, 2009a), although it was still a bit lower than our best baseline. Since (Pitler et al, 2009a) used different selection of instances for Expansion sense, we cannot make a direct comparison. Specifically, the model for the Comparison relation achieves an f-score of 26.02% (5% over the previous work in (Pitler et al, 2009a)). Furthermore, the models for Contingency and Temporal relation achieve 35.72% and 13.76% f-score respectively, which are comparable to the previous work in (Pitler et al, 2009a). (Pitler et al, 2009a) performed implicit relation classification on the second version of the PDTB. Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a realistic setting were Pitler et al (2009). However, the approach taken by Pitler et al (2009) and repeated in more recent work (training directly on PDTB) is problematic as well: when training a model with so many sparse features on a dataset the size of PDTB (there are 22, 141 non-explicit relations overall), it is likely that many important word pairs will not be seen in training. An analysis in (Pitler et al, 2009) also shows that the top word pairs (ranked by information gain) all contain common functional words, and are not at all the semantically-related content words that were imagined. Our word pair features outperform the previous formulation (represented by the results reported by (Pitler et al, 2009), but used by virtually all previous work on this task).  
Intelligent Selection of Language Model Training Data We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation. Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model. We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods. We find that the best selection technique is the recently proposed cross-entropy difference method (Moore and Lewis, 2010). In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010).This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010). We found that even for our small amount of in-domain data, the recently proposed cross-entropy difference method was consistently the best (Moore and Lewis, 2010). One of the most widely used sentence-selection approaches is that of Moore and Lewis (2010). Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data. In Moore and Lewis (2010), the authors compare several approaches to selecting data for LMand Axelrod et al (2011) extend their ideas and apply them to MT. For the English and German language models, we applied the data selection method proposed in (Moore and Lewis, 2010). Moore and Lewis (2010) propose a method for filtering large quantities of out-of-domain language model training data by comparing the cross-entropy of an in-domain language model and an out-of-domain language model trained on a random sampling of the data. This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010). Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy. Difference Moore and Lewis (2010) also start with a language model LMI over the in-domain corpus, but then further construct a language model LMO of similar size over the general-domain corpus. Again, the vocabulary of the language model trained on a subset of the general domain corpus was restricted to only cover those tokens found in the in-domain corpus, following Moore and Lewis (2010). We consider three methods for extracting domain targeted parallel data from a general corpus: source side cross-entropy (Cross-Ent), source-side cross entropy difference (Moore-Lewis) from (Moore and Lewis, 2010), and bilingual cross-entropy difference (b Ml), which is novel. For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis,2010). For the 109 French-English, UN and LDC Gigaword corpora we apply the data selection technique described in (Moore and Lewis, 2010). We employ the data selection method of (Axelrod et al, 2011), which builds upon (Moore and Lewis, 2010). These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010). We compare our method with the method of (Moore and Lewis, 2010) (Cross-Entropy). It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010).
Sentence Fusion For Multidocument News Summarization A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading. In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents. Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence. Sentence fusion moves the summarization field from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and can synthesize information across sources. To evaluate the grammaticality of our generated summaries, following common practice (Barzilay and McKeown, 2005), we randomly selected 50 sentences from original conversations and system generated abstracts, for each dataset. Finally, recent research on analyzing online social media shown a growing interest in mining news stories and headlines because of its broad applications ranging from "meme" tracking and spike detection (Leskovec et al., 2009) to text summarization (Barzilay and McKeown, 2005). The pioneering work on fusion is Barzilay and McKeown (2005), which introduces the frame work used by subsequent projects: they represent the inputs by dependency trees, align some words to merge the input trees into a lattice, and then extract a single, connected dependency tree as the output. Barzilay and McKeown (2005) proposed an idea called sentence fusion that integrates information in overlapping sentences to produce a non-overlapping summary sentence. This is the other way around compared to the English dependency such as in Barzilay and McKeown (2005). Either a sentences from the cluster is selected (Aliguliyev, 2006) or a new sentence is regenerated from all/some sentences in a cluster (Barzilay and McKeown, 2005). Recent abstractive approaches, such as sentence compression (Knight and Marcu, 2000) (Cohn and Lapata, 2009) and sentence fusion (Barzilay and McKeown, 2005) or revision (Tanaka et al, 2009) have focused on rewriting techniques, without consideration for a complete model which would include a transition to an abstract representation for content selection. The work of (Barzilay and McKeown, 2005) on sentence fusion shows an example of re-using the same syntactical structure of a source sentence to create a new one with a slightly different meaning. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sentence. In our experiments, dependency parsing is accomplished with Minipar (Lin, 1998) and alignment is done using a bottom-up tree alignment algorithm (Barzilay and McKeown, 2005) modified to account for the shallow semantic role labels produced by the parser. Abstractive summarization has been explored to some extent in recent years: sentence compression (Knight and Marcu, 2000) (Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005) or revision (Tanaka et al, 2009), and a generation based approach that could be called sentence splitting (Genest and Lapalme, 2011). Sentence fusion is a text-to-text generation application, which given two related sentences, outputs a single sentence expressing the information shared by the two input sentences (Barzilay and McKeown 2005). Barzilay and McKeown (2005) argue convincingly that employing such a fusion strategy in a multidocument summarization system can result in more informative and more coherent summaries. In contrast to these approaches, sentence fusion was introduced to combine fragments of sentences with common information for multi-document summarization (Barzilay and McKeown, 2005).
  V. Ng and C. Cardieanalysed in (Ng and Cardie, 2002) the impact of such a prefiltering on their co reference resolution engine. Various natural language processing (NLP) tasks benefit from the identification of elliptical subjects, primarily anaphora resolution (Mitkov, 2002) and co-reference resolution (Ng and Cardie, 2002). As an improvement, Ng and Cardie (2002a) and Ng (2004) train a separate model to classify an anaphor as either anaphoric or non-anaphoric. The output of this classifier can be used either as a pre-filter (Ng and Cardie,2002a) so that non-anaphoric anaphors will not be precessed in the co reference system, or as a set of features in the co reference model (Ng, 2004). This is different from (Ng and Cardie, 2002a; Ng, 2004) where their anaphoricty models are trained independently of the co reference model, and it is either used as a pre-filter, or its output is used as features in the co reference model.  Ng and Cardie (2002a) trains a separate anaphoricity classifier in addition to a coreference model. Ng and Cardie (2002) and Poesio et al (2005) have tested the impact of such a detector on the overall co reference resolution performance with encouraging results. Ng and Cardie (2002) and Uryupina (2003) do not limit to definite NPs but deal with all types of NPs. Notice the confusing use of the term anaphoric in (Ng and Cardie, 2002) for describing their chain-starting filtering module. Other partially capture the differential preferences between different anaphorsvia different sample selection strategies during training (Ng and Cardie, 2002b; Uryupina, 2004). For classifiers, we replicate the procedures of Ng and Cardie (2002b). This is very similar to the approach of Ng and Cardie (2002a).   In fact, Ng and Cardie (2002a) challenged the motivation for the inclusion of such detectors, reporting no improvements, or even worse performance. In fact, Ng and Cardie (2002a) challenged the motivation for the inclusion of such detectors, reporting no improvements or even worse performance. Ng and Cardie (2002a) directly investigate the question of whether employing a discourse-new prediction component improves the performance of a Method R P Baseline 100 72.2 Syntactic Heuristics 43 93.1 Synt. Heuristics+ S1+ EHP+ DO+ V 79.1 84.5 Table 2: Discourse-new prediction results by Bean and Riloffcoreference resolution system (specifically, the system discussed in (Ng and Cardie, 2002b)). Traditional learning-based co reference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al (2001), Ng and Cardie (2002b), Kehler et al (2004), Ponzetto and Strube (2006)).
An Empirical Model Of Multiword Expression Decomposability This paper presents a constructioninspecific model of multiword expression decomposability based on latent semantic analysis. We use latent semantic analysis to determine the similarity between a multiword expression and its constituent words, and claim that higher similarities indicate greater decomposability. We test the model over English noun-noun compounds and verb-particles, and evaluate its correlation with similarities and hyponymy values in WordNet. Based on mean hyponymy over partitions of data ranked on similarity, we furnish evidence for the calculated similarities being correlated with the semantic relational content of WordNet. Baldwin et al (2003) use LSA as a technique foranalysing the compositionality (or decomposability) of a given MWE. Some of these are mutual information (Church and Hanks, 1989), distributed frequency (Tapanainen et al, 1998) and Latent Semantic Analysis (LSA) model (Baldwin et al, 2003). Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al, 1998), Distributed frequency of object using verb information (Venkatapathyand Joshi, 2005), Similarity of object in verb object pair using the LSA model (Baldwin et al,2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006). For example, Baldwin et al (2003) studied vector extraction for phrases because they were interested in the decomposability of multi word expressions. According to Baldwin et al (2003), divergences in VPC and head verb semantics are often reflected in differing selectional preferences, as manifested in patterns of noun co-occurrence. Prior work in discovering non-compositional phrases has been carried out by Lin (1999) and Baldwin et al (2003), who also used LSAto distinguish between compositional and non compositional verb-particle constructions and noun noun compounds. Some of these are Frequency, Mutual Information (Church and Hanks, 1989), distributed frequency of object (Tapanainen et al, 1998) and LSA model (Baldwin et al, 2003) (Schutze, 1998). An interesting way of quantifying the relative compositionality of a MWE is proposed by Baldwin, Bannard, Tanaka and Widdows (Baldwin et al, 2003). They evaluate their model on English NN compounds and verb-particles, and showed that the model correlated moderately well with the Word net based decomposability theory (Baldwin et al, 2003). The LSA model we built is similar to that described in (Schutze, 1998) and (Baldwin et al, 2003).  Katz and Giesbrecht (2006) and Baldwin et al (2003) use Latent Semantic Analysis for this purpose. (Baldwin et al, 2003) use WordNet: :Similarity to provide an evaluation tool for multi word expressions that are identified via Latent Semantic Analysis. Baldwin et al (2003) proposed a LSA-based model for measuring the decomposability of MWEs by examining the similarity between them and their constituent words, with higher similarity indicating the greater decomposability. Baldwin et al (2003) investigate semantic decomposability of noun-noun compounds and verb constructions. In the above model, if a=0 and b=1, the resulting model is similar to that of Baldwin et al (2003). Baldwin et al, (2003) focus more narrowly on distinguishing English noun-noun compound sand verb-particle constructions which are compositional from those which are not compositional. To compare our method with that proposed by Baldwin et al (2003), we applied their method to our materials, generating LSA vectors for the component content words in our candidate MWEs and comparing their semantic similarity to theMWEs LSA vector as a whole, with the expectation being that low similarity between the MWE as a whole and its component words is indication of the non-compositionality of the MWE. There is some evidence (Baldwin et al, 2003) that part of speech tagging might improve results in this kind of task. Other approaches use Latent Semantic Analysis (LSA) to determine the similarity between a potential idiom and its components (Baldwin et al, 2003).
Representing Text Chunks Dividing sentences in chunks of words is a useful preprocessing step for parsing, information extraction and information retrieval. (Ramshaw and Marcus, 1995) have introduced a &quot;convenient&quot; data representation for chunking by converting it to a tagging task. In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks. We will show that the the data representation choice has a minor influence on chunking performance. However, equipped with the most suitable data representation, our memory-based learning chunker was able to improve the best published chunking results for a standard data set. (Tjong Kim Sang and Veenstra, 1999) have presented three variants of this tagging representation.  (Tjong Kim Sang and Veenstra, 1999) compare different data representations for this task. We encoded the opinionated expression brackets using the IOB2 encoding scheme (Tjong Kim Sang and Veenstra, 1999). There are four kinds of chunk tags in the CoNLL-1999 dataset, namely IOB1, IOB2, IOE1, and IOE2 (Tjong Kim Sang and Veenstra, 1999). The mention encoding is the IOB2 encoding presented in (Tjong Kim Sang and Veenstra, 1999) and introduced by (Ramshaw and Marcus, 1994) for base noun phrase chunking. Tjong Kim Sang and Veenstra (1999) describes in detail the IOB schemes. We used the JNLPBA-2004 training data, which is a set of tokenized word sequences with IOB2 (Tjong Kim Sang and Veenstra, 1999) protein labels. From now on, we shall refer to the Chunk tag of a word as its IOB value (IOB was named by Tjong Kim Sang and Jorn Veeenstra (Tjong Kim Sang and Veenstra, 1999) after Ratnaparkhi (Ratnaparkhi, 1998)). The training data was converted to use the IOB2 phrase model (Tjong Kim Sang and Veenstra, 1999). The mention encoding is the IOB2 encoding presented in (Tjong Kim Sang and Veenstra, 1999) and introduced by (Ramshaw and Marcus, 1994) for the task of base noun phrase chunking. Tjong Kim Sang calls this method as IOB1 representation, and introduces three alternative versions - IOB2, IOE1 and IOE2 (Tjong Kim Sang and Veenstra, 1999). To transform the problem into a classification task, we use the IOB2 classification scheme (Tjong Kim Sang and Veenstra, 1999). We encoded the opinionated expression brackets using the IOB2 encoding scheme (Tjong Kim Sang and Veenstra, 1999) and trained the model using the metod by Collins (2002).
COGEX: A Logic Prover For Question Answering Recent TREC results have demonstrated the need for deeper text understanding methods. This paper introduces the idea of automated reasoning applied to question answering and shows the feasibility of integrating a logic prover into a Question Answering system. The approach is to transform questions and answer passages into logic representations. World knowledge axioms as well as linguistic axioms are supplied to the prover which renders a deep understanding of the relationship between question text and answer text. Moreover, the trace of the proofs provide answer justifications. The results show that the prover boosts the performance of the QA system on TREC questions by 30%. A third approach, exemplified by Moldovan et al (2003) and Raina et al (2005), is to translate dependency parses into neo-Davidsonian-style quasi logical forms, and to perform weighted abductive theorem proving in the tradition of (Hobbs et al, 1988). The two models become distinct when there is a good supply of additional linguistic and world knowledge axioms - as in Moldovan et al (2003). Most of existing systems on the web produce a set of answers to a question in the form of hyper links or page extracts, ranked according to a relevance score (for example, COGEX [Moldovan et al, 2003]). Our system uses COGEX (Moldovan et al, 2003), a natural language prover originating from OT TER (McCune, 1994). This assumption, also made by other recent abductive approaches (Moldovanetal., 2003), does not hold for several classes of examples. Analysis of results on some RTE examples along without guesses and confidence probabilities inference of (Moldovan et al, 2003) and have proposed a way to capture common cases of this phenomenon. In COGEX (Moldovan et al, 2003), a recent QA system, authors used automated reasoning for QA and showed that it is feasible, effective and scalable. Moldovan et al (2003) describe a method similar to ours. Continuing this work Moldovan et al (Moldovan et al, 2003) built a logic prover for Question Answering. Wordnets and ontologies are very common resources and are employed in a wide variety of direct and indirect QA tasks, such as reasoning based on axioms extracted from WordNet (Moldovanetal., 2003). COGEX (Moldovan et al, 2003) uses its logic prover to extract lexical relationships between the question and its candidate answers. a combination of language processes that transform questions and candidate answers in logic representations such that reasoning systems can select the correct answer based on their proofs (cf. (Moldovan et al., 2003)). WordNet (Fellbaum, 1998) is perhaps the most popular resource and has been employed in a variety of QA-related tasks ranging from query expansion, to axiom-based reasoning (Moldovan et al., 2003), passage scoring (Paranjpe et al, 2003), and answer filtering (Leidner et al, 2004). Scenario knowledge was also included in the form of axiomatic logic transformation developed in (Moldovan et al, 2003). 
Parameter Estimation For Probabilistic Finite-State Transducers algebraic path problem (shortest paths; matrix inver- 34(3):191–219. Richard Sproat and Michael Riley. 1996. Compilation of weighted finite-state transducers from decision trees. of the 34th Annual Meeting of the Andreas Stolcke and Stephen M. Omohundro. 1994. Best-first model merging for hidden Markov model induction. Tech. Report ICSI TR-94-003, Berkeley, CA. Robert Endre Tarjan. 1981a. A unified approach to path of the 28(3):577–593, July. Robert Endre Tarjan. 1981b. Fast algorithms for solving problems. of the 28(3):594–614, July. G. van Noord and D. Gerdemann. 2001. An extendible regular expression compiler for finite-state approaches natural language processing. In Impleno. 22 in Springer Lecture Notes in CS. The calculation of expected counts can be formulated using the expectation semiring frame work of Eisner (2002), though that work does not show how to compute expected products of counts which are needed for our gradient calculations. Concurrently with this work, Li and Eisner (2009) have generalized Eisner (2002) to compute expected products of counts on translation forests. We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Li and Eisner, 2009). In some special cases only a linear solver is needed: e.g., for unary rule cycles (Stolcke, 1995), or epsilon-cycles in FSMs (Eisner, 2002). We use standard algorithms (Eisner, 2002) to compute the path sums as well as their gradients with respect to theta for optimization (section 4.1). We used the OpenFST library (Allauzen et al, 2007) to implement all finite-state computations, using the expectation semiring (Eisner, 2002) for training. Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hyper graphs). In this paper, we apply the expectation semiring (Eisner, 2002) to a hyper graph (or packed forest) rather than just a lattice. Eisner (2002) uses closed semirings that are also equipped with a Kleene closure operator.  However, Eisner (2002, section 5) observes that this is inefficient when n is large. This follows Eisner (2002), who similarly generalized the forward-backward algorithm. For example, Eisner (2002) uses finite-state operations such as composition, which do combine weights entirely within the expectation semiring before their result is passed to the forward-backward algorithm. We presented first-order expectation semirings and inside-outside computation in more detail than (Eisner, 2002), and developed extensions to higher-order expectation semirings. This logic can be used with the expectation semiring (Eisner, 2002) to find the maximum likelihood estimates of the parameters of a word-to-word translation model. Eisner (2002) has claimed that parsing under an expectation semiring is equivalent to the Inside-Outside algorithm for PCFGs. To compute this, intersect the WFSA and the lattice, obtaining a new acyclic WFSA, and sum the u-scores of all its paths (Eisner, 2002) using a simple dynamic programming algorithm akin to the forward algorithm.  Eisner (2002) gives a general EM algorithm for parameter estimation in probabilistic finite-state transducers. Eisner (2002) describes the expectation semiring for parameter learning.
Bayesian Inference for PCFGs via Markov Chain Monte Carlo This paper presents two Markov chain Monte Carlo (MCMC) algorithms forBayesian inference of probabilistic context free grammars (PCFGs) from ter minal strings, providing an alternative to maximum-likelihood estimation usingthe Inside-Outside algorithm. We illus trate these methods by estimating a sparse grammar describing the morphology ofthe Bantu language Sesotho, demonstrat ing that with suitable priors Bayesian techniques can infer linguistic structure in situations where maximum likelihoodmethods such as the Inside-Outside algo rithm only produce a trivial grammar. However, as noted by Johnson et al (2007), this choice of beta leads to difficulties with MAP estimation. Adaptor grammars are a framework for Bayesian inference of a certain class of hierarchical nonparametric models (Johnson et al, 2007b). Adaptor Grammars are formally defined in Johnson et al (2007b), which should be consulted for technical details. There are several different procedures for inferring the parse trees and the rule probabilities given a corpus of strings: Johnson et al (2007b) describe a MCMC sampler and Cohen et al (2010) describe a Variational Bayes procedure. Hidden Markov Models (HMMs), a special case of DBNs, are a classical method for important NLP applications such as unsupervised part-of-speech tagging (Gael et al., 2009) and grammar induction (Johnson et al, 2007) as well as for ASR. Our model is similar to the Adaptor Grammar model of Johnson et al (2007b), which is also a kind of Bayesian nonparametric tree-substitution grammar. Our model is similar in this way to the Adaptor Grammar model of Johnson et al (2007a). Given a sample, we can reason over the space of possible trees using a Metropolis-Hastings sampler (Johnson et al, 2007a) coupled with a Monte Carlo integral (Bod, 2003). A natural proposal distribution, p(d|w), is the maximum a posterior (MAP) grammar given the elementary tree analysis of our training set (analogous to the PCFG approximation used in Johnson et al (2007a)). To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al (2007a). This section informally introduces adaptor grammars using unsupervised word segmentation as a motivating application; see Johnson et al (2007b) for a formal definition of adaptor grammars. In adaptor grammars the base distributions HX are determined by the PCFG rules expanding X and the other adapted distributions, as explained in Johnson et al (2007b). Johnson et al (2007a) describe Gibbs samplers for Bayesian inference of PCFG rule probabilities, and these techniques can be used directly with adaptor grammars as well.  The adaptor grammar algorithm described in Johnson et al (2007b) repeatedly resamples parses for the sentences of the training data. This is closely related to adaptor grammars (Johnson et al., 2007a), which also generate full tree rewrites in a monolingual setting. This work was inspired by adaptor grammars (Johnson et al, 2007a), a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree. The sampling algorithm closely follows the process for sampling derivations from Bayesian PCFGs (Johnson et al, 2007b). In the monolingual setting, there is a well known tree sampling algorithm (Johnson et al,2007). However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al, 2007), or the number of adapted productions in the adaptor grammar (Johnson et al, 2007)) was not very large.
Automatic Retrieval and Clustering of Similar Words Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.  Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research. This scheme utilizes the symmetric similarity measure of (Lin, 1998) to induce improved feature weights via bootstrapping. We will take advantage of the flexibility provided by our framework and use syntax based measure of similarity in the computation of the verb vectors, following (Lin, 1998). Chantree et al (2005) applied the distributional similarity proposed by Lin (1998) to coordination disambiguation. Accurate measurement of semantic similarity between lexical units such as words or phrases is important for numerous tasks in natural language processing such as word sense disambiguation (Resnik, 1995), synonym extraction (Lin, 1998a), and automatic thesauri generation (Curran, 2002).   Lin (1998) created a thesaurus using syntactic relationships with other words. Like McCarthy et al (2004) we use k= 50 and obtain our thesaurus using the distributional similarity metric described by Lin (1998). The thesaurus was acquired using the method described by Lin (1998). For every pair of nouns, where each noun had a total frequency in the triple data of 10 or more, we computed their distributional similarity using the measure given by Lin (1998).  As in (Lin, 1998) or (Cur ran and Moens, 2002a), this building is based on the definition of a semantic similarity measure from a corpus. This seems to be a reasonable compromise between the approach of (Freitag et al, 2005), in which none normalization of words is done, and the more widespread use of syntactic parsers in work such as (Lin, 1998). Finally, the results of Table 2 are compatible with those of (Lin, 1998) for instance (R-prec. = 11.6 and MAP = 8.1 with WM as reference for all entries of the thesaurus at http://webdocs.cs.ualberta.ca/lindek/Downloads/sim.tgz) if we take into account the fact that the thesaurus of Lin was built from a much larger corpus and with syntactic co-occurrences. For example, one of the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym of person (Lin, 1998). For instance, Lin (1998) used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes. One of the most important approaches is Lin (1998). 
Minimum Cut Model For Spoken Lecture Segmentation We consider the task of unsupervised lecture segmentation. We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion. Our approach moves beyond localized comparisons and takes into account longrange cohesion dependencies. Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors.    Our model for disentanglement fits into the general class of graph partitioning algorithms (Roth and Yih, 2004) which have been used for a variety of tasks in NLP, including the related task of meeting segmentation (Malioutov and Barzilay, 2006). In our problem, however, the solution is constrained by the linearity of segmentation on transcripts, similar to that in (Malioutov and Barzilay, 2006). Malioutov and Barzilay (2006) describe a dynamic programming algorithm to conduct topic segmentation for spoken documents. Malioutov and Barzilay (2006) optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences. This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006). We use the evaluation source code provided by Malioutov and Barzilay (2006). Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006). We evaluate the performance of APS on three tasks: finding topical boundaries in transcripts of course lectures (Malioutov and Barzilay, 2006), identifying sections in medical textbooks (Eisen stein and Barzilay, 2008) and identifying chapter breaks in novels. We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). Malioutov and Barzilay (2006) show that the knowledge about long-range similarities between sentences improves segmentation quality. The first, compiled by Malioutov and Barzilay (2006), consists of manually transcribed and segmented lectures on Artificial Intelligence, 3 development files and 19 test files. We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (EisensteinandBarzilay, 2008). In situations where the document boundaries are unavailable or when finer segmentation is desired, automatic techniques for document segmentation may be applied (Malioutov and Barzilay, 2006).  (Malioutov and Barzilay, 2006) uses the minimum cut model to segment spoken lectures (i.e., monologue). Segmentation may be particularly beneficial when working with documents without overt structure: speech transcripts (Malioutov and Barzilay, 2006), newswire (Misra et al, 2011) or novels (Kazantseva and Szpakowicz, 2011). Malioutov and Barzilay (2006) created a corpus of course lectures segmented by four annotators, noting that the annotators operated at different levels of granularity.
Tuning as Ranking We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999). Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features. Moreover, unlike recent approaches built upon the MIRA algorithm of Crammer and Singer (2003) (Watanabe et al., 2007; Chiang et al., 2008b), PRO is easy to implement. It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours. We establish PRO’s scalability and effectiveness by comparing it to MERT and MIRA and demonstrate parity on both phrase-based and syntax-based systems in a variety of language pairs, using large scale data scenarios.  RAMPION settings were as described in (Gimpel and Smith, 2012), and PRO settings as described in (Hopkins and May, 2011), with PRO requiring regularization tuning in order to be competitive with the other optimizers. We found similar fluctuations for the cdec implementations of PRO (Hopkins and May, 2011) or hyper graph-MERT (Kumar et al, 2009) both of which depend on hyper graph sampling. Additionally, we present Joshua's implementation of the pairwise ranking optimization (Hopkins and May, 2011) approach to translation model tuning. Pairwise ranking optimization (PRO) proposed by (Hopkins and May, 2011) is a new method for discriminative parameter tuning in statistical machine translation.  It optimizes a logistic objective identical to that of PRO (Hopkins and May, 2011) with stochastic gradient descent, although other objectives are possible. We cast MT tuning as pairwise ranking (Herbrich et al, 1999, inter alia), which Hopkins and May (2011) applied to MT. Introduced by Hopkins and May (2011), Pairwise Ranking Optimization (PRO) aims to handle large feature sets inside the traditional MERT architecture. Hopkins and May (2011) advocate a maximum-entropy version of PRO, which is what we evaluate in our empirical comparison. We used sparse feature templates that are equivalent to the PBMT set described in (Hopkins and May, 2011). Feature weights were re-tuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline. Hopkins and May (2011) presented a method that uses a binary classifier. Hopkins and May (2011) introduced the method of pairwise ranking optimization (PRO), which casts the problem of tuning as a ranking problem between pairs of translation candidates. Following Hopkins and May (2011), we used the following parameters for the sampling task: For each sentence, the decoder generates the 1500 best candidate translations (k= 1500), and the sampler samples 5000 pairs (n= 5000).  Like Hopkins and May (2011), we optimize ranking in n-best lists, but learn parameters in an online fashion. Unlike Hopkins and May (2011), we do not randomly sample from all the pairs in the n-best translations, but extract pairs by selecting one oracle translation and one other translation in the n-bests other than those in ORACLE. Hopkins and May (2011) applied a MERT-like procedure in Alg 1 in which Equation 4 was solved to obtain new parameters in each iteration. Hopkins and May (2011) minimized logistic loss sampled from the merged n-bests, and sentence-BLEU was used for determining ranks.
Using Syntax to Disambiguate Explicit Discourse Connectives in Text Discourse connectives are words or such as and contrary explicitly signal the presence of a discourse relation. There are two types of ambiguity that need to be resolved during discourse processing. First, a word can be ambiguous between discourse or non-discourse usage. For be either a temporal discourse connective or a simply a word meaning “formerly”. Secondly, some connectives are ambiguous in terms of the they mark. For example can serve as either a temporal or causal connective. We demonstrate that syntactic features improve performance in both disambiguation tasks. We report state-ofthe-art results for identifying discourse vs. non-discourse usage and human-level performance on sense disambiguation. The same assumption underlies methods for automatically identifying DRMs (Pitler and Nenkova, 2009). Such semantic information is based on external lists of lexical items and on the output of the add Discourse tagger (Pitler and Nenkova, 2009). The handling of explicit connectives can be split into three tasks (Pitler and Nenkova, 2009). The syntactic features (Syn) are inspired by (Pitler and Nenkova, 2009). Models M2-M4 do not rely on gold standard annotation or parsing (in contrast to the models for English in (Pitler and Nenkova, 2009)). Prior work (Pitler and Nenkova, 2009) showed that where explicit markers exist, the class of the relation can be disambiguated with f-scores higher than 90%. Predicting the class of implicit discourse relations, however, is much more difficult. We identify discourse connectives and their senses (TEMPORAL, COMPARISON, CONTINGENCY or EXPANSION) in each reference segment using the system in Pitler and Nenkova (2009). The state of-the-art for recognizing all types of explicit connectives in English is therefore already high, at 97% accuracy for disambiguating discourse vs. non discourse uses (Lin et al, 2010) and 94% for disambiguating the four main senses from the PDTB hierarchy (Pitler and Nenkova, 2009). The sense of the connective feature (F2) extracted from PDTB for the base system, though for the fully automatic one (Ghosh et al, 2011b) it needs the PTB (Penn TreeBank)-style syntactic parse trees as input (Pitler and Nenkova, 2009). The labeling of the four main senses from the PDTB sense hierarchy (temporal, contingency, comparison, expansion) reaches 94% ac curacy (Pitler and Nenkova, 2009) however, the baseline accuracy is already around 85% when using only the connective token as a feature. The state of the art for recognizing explicit connectives in English is therefore already high, at a level of 94% for disambiguating the four main senses on the first level of the PDTB sense hierarchy (Pitler and Nenkova, 2009). For instance, Pitler and Nenkova (2009) report an accuracy of 85.86% for correctly classified connectives (with the 4 main senses), when using the connective token as the only feature.
Factored Language Models And Generalized Parallel Backoff We introduce factored language models (FLMs) and generalized parallel backoff (GPB). An FLM represents words as bundles of features (e.g., morphological classes, stems, data-driven clusters, etc. ), and induces a probability model covering sequences of bundles rather than just words. GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types, where no obvious natural (temporal) backoff order exists, and where multiple dynamic backoff strategies are allowed. These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit. This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles. Significantly, FLMs with GPB can produce bigrams with significantly lower perplexity, sometimes lower than highly-optimized baseline trigrams. In a multi-pass speech recognition context, where bigrams are used to create first-pass bigram lattices or N-best lists, these results are highly relevant. Different back-off strategies, including different back-off paths as well as combination methods (Bilmes and Kirchhoff, 2003), were tried and here we present the best results. We compare an optimized four-gram, a three gram baseline, and various numbers of cluster sizes using our MCMI method and generalized back off (Bilmes and Kirchhoff, 2003), which, (again) with 500 clusters, achieves an 8.9% relative improvement over the trigram. In (Bilmes and Kirchhoff, 2003), it is shown that factored language models are able to outperform standard n-gram techniques in terms of perplexity. Conditional probability distributions are represented as factored language models smoothed using Witten-Bell interpolated back off smoothing (Bilmes and Kirchhoff, 2003), according to the backoff graphs in Fig. Class-based LMs (Brown et al, 1992) or factored LMs (Bilmes and Kirchhoff, 2003) are very similar to our T+C scenario. In (Ji and Bilmes, 2005), for example, an analysis of DA tagging using DBNs is performed, where the models avoid label bias by structural changes and avoid data sparseness by using a generalized backoff procedures (Bilmes and Kirchhoff, 2003). We therefore have developed a procedure that allows us to train generalized backoff models (Bilmes and Kirchhoff, 2003), even when some or all of the variables involved in the model are hidden. Because all variables are observed when training our baseline, we use the SRILM toolkit (Stolcke, 2002), modified Kneser-Ney smoothing (Chen and Goodman, 1998), and factored extensions (Bilmesand Kirchhoff, 2003). We use the SRILM toolkit with extensions (Bilmes and Kirchhoff, 2003) to train, and use GMTK (Bilmes and Zweig, 2002) for decoding. The Factored Language Model (FLM) (Bilmes and Kirchhoff, 2003) offers a convenient view of the input data: it represents every word in a sentence as a tuple of factors. Both approaches are essentially a simple form of a factored language model (FLM) (Bilmes and Kirchhoff, 2003). A factored language model (FLM) (Bilmes and Kirchhoff, 2003) is based on a representation of words as feature vectors and can utilize a variety of additional information sources in addition to words, such as part-of-speech (POS) information, morphological information, or semantic features, in a unified and principled framework. This work is related to several existing directions: generative factored language model, discriminative language models, online passive-aggressive learning and confidence-weighted learning. Generative factored language models are proposed by (Bilmes and Kirchhoff, 2003). A more powerful back off strategy is used in factored language models (FLMs) (Bilmes and Kirchhoff, 2003), which view a word as a vector of word features or factors. Another approach is to use the factored language models (FLMs) which are powerful models that combine multiple sources of information and efficiently integrate them via a complex back off mechanism (Bilmes and Kirchhoff, 2003). In addition, the framework exists to integrate language models, such as those described in (Bilmes and Kirchhoff 2003), which takes advantage of the factored representation within Moses. In COMIC, the OpenCC Grealiser uses factored language models (Bilmes and Kirchhoff, 2003) over words and multi modal co articulations to select the highest-scoring realisation licensed by the grammar that satisfies the specification given by the fission module. They used factored language models introduced by Bilmes and Kirchhoff (2003) to integrate different word factors into the translation process. Different backoff paths are possible, and it would be interesting but prohibitively slow to apply a strategy similar to generalised parallel back off (Bilmesand Kirchhoff, 2003) which is used in factored language models. Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006).
Seeing Stars When There Aren’t Many Stars: Graph-Based Semi-Supervised Learning For Sentiment Categorization We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference. Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., “4 stars”), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text. In particular, we are interested in the situation where labeled data is scarce. We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve ratinginference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training. (Goldberg and Zhu, 2006) adapt semi-supervised graph-based methods for sentiment analysis but do not incorporate lexical prior knowledge in the form of labeled features. Some of the work is not related to discourse at all (e.g., lexical similarities (Takamura et al, 2007), morphosyntactic similarities (Popescu and Etzioni, 2005) and word-based measures like TF IDF (Goldberg and Zhu, 2006)). Thus, instead of directly learning a classification function, we learn a regression function - similar to (Goldberg and Zhu, 2006) - that is then used for ranking the hypotheses. To perform rating inference on reviews, Goldberg and Zhu (2006) created a graph on both labeled and unlabeled reviews, and then solved an optimization problem to obtain a smooth rating function over the whole graph. Compared with methods which do not exploit the relationship be tween samples, experiments showing advantages of graph-based learning methods can be found 1210 in (Rao and Ravichandran, 2009), (Goldberg and Zhu, 2006), (Tong et al, 2005), (Wan and Xiao,2009), (Zhu and Ghahramani, 2002) etc. When labeled data are scarce, such graph-based transductive learning methods are especially useful. Our approach accounts for intercategory relationships from the outset of classifier design, rather than addressing this issue with later adjustments. Goldberg and Zhu (2006) proposed a semisupervised learning approach to the rating inference problem in scenarios where labeled training data is scarce. First, we compare two graph-based algorithms in cross-domain SC settings: the algorithm exploited in (Goldberg and Zhu, 2006), which seeks document sentiments as an output of an optimisation problem (OPTIM) and the algorithm adopted by (Wu et al2009), that uses ranking to assign sentiment scores (RANK). The RANK algorithm (Wu et al 2009) is based on node ranking, while OPTIM (Goldberg and Zhu, 2006) determines solution of graph optimisation problem. For more details on the problem solution see (Goldberg and Zhu, 2006). Following (Goldberg and Zhu, 2006) and (Pang and Lee, 2005) we consider 2 types of document representations: feature-based: this involves weighted document features. In NLP, label propagation has been used for word sense disambiguation (Niu et al, 2005), document classification (Zhu, 2005), sentiment analysis (Goldberg and Zhu, 2006), and relation extraction (Chen et al, 2006). nodes (Goldberg and Zhu, 2006). However, similar approaches have been proven rather efficient on other tasks such as document level sentiment classification (Goldberg and Zhu, 2006) and word sense disambiguation (Agirre et al, 2006).
Shallow Semantic Parsing Using Support Vector Machines In this paper, we propose a machine learning algorithm for shallow semantic parsing, extend ing the work of Gildea and Jurafsky (2002),Surdeanu et al (2003) and others. Our al gorithm is based on Support Vector Machineswhich we show give an improvement in performance over earlier classifiers. We show perfor mance improvements through a number of newfeatures and measure their ability to general ize to a new test set drawn from the AQUAINT corpus. Note that the result here is not comparable with the best in this domain (Pradhan et al., 2004) where the full parse tree is assumed given.  Using this intuition, state-of-the-art systems first build a parse tree, and syntactic constituents are then labeled by feeding hand-built features extracted from the parse tree to a machine learning system, e.g. the ASSERT system (Pradhan et al, 2004).  We compared our system to the freely available Assert system (Pradhan et al, 2004). Because ASSERT uses a parser, and because PropBank was built by labeling the nodes of a hand-annotated parse tree, per node accuracy is usually reported in papers such as (Pradhan et al, 2004). We measured the argument classification accuracy of our network, assuming the correct segmentation is given to our system, as in (Pradhan et al, 2004), by post-processing our per-word tags to form a majority vote over each segment. For these reasons, we use a semantic role labeler (Pradhan et al, 2004) to provide and delimit the text spans that contain the semantic arguments of a predicate. To compute the semantic roles for the source trees, we use an in-house max-ent classifier with features following Xue and Palmer (2004) and Pradhan et al (2004). Examples are linearly interpolated relative frequency models (Gildea and Jurafsky, 2002), SVMs (Pradhan et al, 2004), decision trees (Surdeanu et al, 2003), and log-linear models (Xue and Palmer, 2004).    We adopted the ASSERT English SRL labeler (Pradhan et al, 2004), which was trained on PropBank data using SVM classifier. They use ASSERT (Pradhan et al, 2004), a publicly available shallow semantic parser trained on PropBank, to generate predicate-argument structures which subsequently form the basis of comparison between question and answer sentences. For semantic analysis, we used the ASSERT toolkit (Pradhan et al, 2004) that produces shallow semantic parses using the PropBank conventions. As mentioned by (Pradhan et al, 2004), argument identification plays a bottleneck role in improving the performance of a SRL system. A crucial difference from similar approaches, such as SRL with PropBank roles (Pradhan et al, 2004) is that by identifying relations as part of a frame, you have identified a gestalt of relations that enables far more inference, and sentences from the same passage that use other words from the same frame will be easier to link together. A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al, 2004)), to label predicate argument tuples. 
Learning Surface Text Patterns For A Question Answering System In this paper we explore the power of surface text patterns for open-domain question answering systems. In order to obtain an optimal set of patterns, we have developed a method for learning such patterns automatically. A tagged corpus is built from the Internet in a bootstrapping process by providing a few hand-crafted examples of each question type to Altavista. Patterns are then automatically extracted from the returned documents and standardized. We calculate the precision of each pattern, and the average precision for each question type. These patterns are then applied to find answers to new questions. Using the TREC-10 question set, we report results for two cases: answers determined from the TREC-10 corpus and from the web. Ravichandran and Hovy (2002) present an alternative ontology for type preference and describe a method for using this alternative ontology to extract particular answers using surface text patterns. The benefit of utilizing template-based inference rules between predicates was demonstrated in NLP tasks such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). Automatic pattern derivation is more appealing (Ravichandran and Hovy, 2002). Ravichandran and Hovy (2002) presents a method that learns patterns from online data using some seed questions and answer anchors. Indeed, many researchers have recently tapped the Web as a data-source for improving performance on NLP tasks (e.g., Resnik (1999), Ravichandran and Hovy (2002), Keller and Lapata (2003)). Other work such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. To do so, Espresso uses a slight modification of the state of the art algorithm described in (Ravichandran and Hovy, 2002). Ravichandran and Hovy (2002) proposed automatically learning surface text patterns for answer extraction. But it is almost impossible to learn such surface text patterns following (Ravichandran and Hovy, 2002). For the special case of Rote extractors, a more attractive alternative has been proposed by Brin (1998), Agichtein and Gravano (2000), and Ravichandran and Hovy (2002).  Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy (2002) report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting one text of a relation instance can be sufficient, rather than detecting all texts.  Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). Ravichandran and Hovy (2002) focus on scaling relation extraction to the Web. We chose the state of the art algorithm described in (Ravichandran and Hovy 2002) with the following slight modification. In (Ravichandran and Hovy 2002), a frequency threshold on the patterns in P is set to select the final patterns. RH02: The algorithm by Ravichandran and Hovy (2002) described in Section 2. These patterns could be manually generated, such as the ones described here, or learned from text, as described in Ravichandran and Hovy (2002). Ravichandran and Hovy (2002) used seed instances of a relation to automatically obtain surface patterns by querying the web.
Pseudo-Projective Dependency Parsing In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures. We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures. Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy. This leads to the best reported performance for robust non-projective parsing of Czech. Recent work by Nivre and Nilsson introduces a technique where the projectivization transformation is encoded in the non-terminals of constituents during parsing (Nivre and Nilsson, 2005). Sagae and Tsujii (2007)'s dependency parser, based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. For tree banks with non-projective trees we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005). It uses graph transformation to handle non-projective trees (Nivre and Nilsson, 2005). To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson, 2005), in which non projective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the non projective links at parse time. Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to preprocess training data and post-process parser output, so-called pseudo-projective parsing. For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective. The most popular strategy for capturing non projective structures in data-driven dependency parsing is to apply some kind of post-processing to the output of a strictly projective dependency parser, as in pseudo-projective parsing (Nivre and Nilsson, 2005), corrective modeling (Hall and Novak, 2005), or approximate non-projective parsing (McDonald and Pereira, 2006). We adopt the pseudo-projective approach introduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English. However, just as it has been noted that most non-projective structures appearing in practice are only 'slightly' non projective (Nivre and Nilsson, 2005), we characterise a sense in which the structures appearing in tree banks can be viewed as being only 'slightly' ill-nested. In order to avoid losing the benefits of higher-order parsing, we considered applying pseudo-projective transformation (Nivre and Nilsson, 2005). Pseudo-projective parsing for recovering non projective structures (Nivre and Nilsson, 2005). Although the parser only derives projective graphs, the fact that these graphs are labeled allows non-projective dependencies to be captured using the pseudo-projective approach of Nivre and Nilsson (2005) (section 3.4). Pseudo-projective parsing was proposed by Nivreand Nilsson (2005) as a way of dealing with non projective structures in a projective data-driven parser. We projectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label r-h, where r is the original label and h is the label of the original head in the non-projective dependency graph. For tree banks with non-projective trees we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005). Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing for all cases. It should be noted that the proportion of lost dependencies is about twice as high as the proportion of dependencies that are non-projective in themselves (Nivre and Nilsson, 2005). The resulting algorithm is projective, and nonprojectivity is handled by pseudo-projective transformations as described in (Nivre and Nilsson, 2005).
Global Thresholding And Multiple-Pass Parsing We present a variation on classic beam thresholding techniques that is up to an order of magnitude faster than the traditional method, at the same performance level. We also present a new thresholding technique, global thresholding, which, combined with the new beam thresholding, gives an additional factor of two improvement, and a novel technique, multiple pass parsing, that can be combined with the others to yield yet another 50% improvement. We use a new search algorithm to simultaneously optimize the thresholding parameters of the various algorithms. For purposes of pruning, and only for purposes of pruning, the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman, 1997). For example, Goodman (1997) suggests using a coarse grammar consisting of regular non-terminals, such as NP and VP, and then non-terminals augmented with head-word information for the more accurate second-pass grammar. In best-first parsing, this priority is called a figure-of-merit (FOM), and is based on various approximations to P (e|s), the fraction of parses of a sentence s which include an edge e (though see Goodman (1997) for an alternative notion of FOM). As in most other statistical parsing systems we therefore use the pruning technique described in Goodman (1997) and Collins (1999: 263-264) which assigns a score to each item in the chart equal to the product of the inside probability of the item and its prior probability. We first applied beam thresholding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997), and iterative parsing (Tsuruoka and Tsujii, 2005b). Beam thresholding (Goodman, 1997) is a simple and effective technique for pruning edges during parsing. For the experiments reported in this paper, we use as parser P, our in-house implementation of the Collins parser (Collins, 2003), to which various 893 speed-related enhancements (Goodman, 1997) have been applied. We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al, 2005) and quick check (Malouf et al, 2000). The terms alpha and beta are the thresholds of the number and the beam width of lexical entries, and theta is the beam width for global thresholding (Goodman, 1997). The terms alpha and beta are the thresholds of the number and the beam width of lexical entries, and theta is the beam width for global thresholding (Goodman, 1997). We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al, 2005) and other techniques for deep parsing. However, if many iterations are required to obtain a parse, the utility of starting with a low beam and iterating becomes questionable (Goodman, 1997). A paper closely related to ours is Goodman (1997). (Solsona et al, 2002)) or to prune the search space by adjusting a beam width during parsing itself (Goodman, 1997). A prime example of this idea is from Goodman (1997), who describes a method for producing a simple but crude approximate grammar of a standard context-free grammar. However, M1 is usually not preferred in practice (Goodman, 1997). However, if combined with other inexact pruning techniques like beam-pruning (Goodman, 1997) or coarse-to-fine parsing (Charniak et al, 2006), binarization may interact with those pruning methods in a complicated way to affect parsing accuracy.
A Decoder For Syntax-Based Statistical MT This paper describes a decoding algorithm for a syntax-based translation model (Yamada and Knight, 2001). The model has been extended to incorporate phrasal translations as presented here. In contrast to a conventional word-to-word statistical model, a decoder for the syntaxbased model builds up an English parse tree given a sentence in a foreign language. As the model size becomes huge in a practical setting, and the decoder considers multiple syntactic structures for each word alignment, several pruning techniques are necessary. We tested our decoder in a Chinese-to-English translation system, and obtained better results than IBM Model 4. We also discuss issues concerning the relation between this decoder and a language model. In comparison, in (Yamada and Knight, 2002), which was a phrasal structure based statistical MT system for Chinese to English translation, the Bleu score reported for short sentences (less than 14 words) is 0.099 to 0.102. Here, we used a model defined by Yamada and Knight (2001) and Yamada and Knight (2002). Internally, the model performs three types of operations on each node of a parse tree. The model is further extended to incorporate phrasal translations performed at each node of the input parse tree (Yamada and Knight, 2002). While (Yamada and Knight, 2002) represent syntactical information in the decoding process through a series of transformation operations, we operate directly at the phrase level. The syntactically supervised model has been found to outperform the IBM word-level alignment models of Brown et al (1993) for translation by Yamada and Knight (2002). (Yamada and Knight, 2002) propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence. Yamada and Knight (2002) presents a decoder for syntax-based MT that uses so-called phrasal translation units that correspond to blocks. As an alternative option to our verb-modifier experiments, structured language models (Chelba and Jelinek, 1998) might be considered to improve clause coherence, until full-featured syntax-based MT models (Yamada and Knight (2002), Eisner (2003), Chiang (2005) among many others) are tested when translating to morphologically rich languages. It follows the decoding-as-parsing idea exemplified by Wu (1996) and Yamada and Knight (2002). This model is then decoded as described in (Yamada and Knight, 2002). For reasons of speed, Yamada and Knight (2002) limited training to sentences of length 30, and were able to use only one fifth of the available Chinese-English parallel corpus. One way to approach reordering is by extending the translation model, either by adding extra models, such as lexicalized (Koehn et al, 2005) or discriminative (Zens and Ney, 2006) reordering models or by directly modelling reordering in hierarchical (Chiang, 2007) or syntactical translation models (Yamada and Knight, 2002).
Determining The Sentiment Of Opinions Identifying sentiments (the affective parts of opinions) is a challenging problem. We present a system that, given a topic, automatically finds the people who hold opinions about that topic and the sentiment of each opinion. The system contains a module for determining word sentiment and another for combining sentiments within a sentence. We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results. This idea is similar to that of (Kim and Hovy, 2004) and (Hu and Liu, 2004), but instead of using a window of size k or the output of a noun phrase chunker, OPINE takes advantage of the syntactic dependencies computed by the MINIPAR parser. (signed integers representing positive and negative feelings) (Kim and Hovy, 2004). In particular, they have been an essential ingredient for fine grained sentiment analysis (e.g., Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al (2005)). Kim and Hovy (2004) try to determine the final sentiment orientation of a given sentence by combining sentiment words within it.   In separate qualitative experiments done by Pang et al (2002), 97 Wilson et al (2005) and Kim and Hovy (2004), the agreement between human judges when given a list of sentiment-bearing words is as low as 58% and no higher than 76%. Kim and Hovy (2004) start with two lists of positive and negative seed words. Kim and Hovy (2004) found the polarity of subjective expressions.  The system of Kim and Hovy (2004) tackles orientation detection by attributing, to each term, a positivity score and a negativity score; interestingly, terms may thus be deemed to have both a positive and a negative correlation, maybe with different degrees, and some terms may be deemed to carry a stronger positive (or negative) orientation than others. This hypothesis is confirmed by an experiment performed by Kim and Hovy (2004) on testing the agreement of two human coders at tagging words with the Positive, Negative, and Objective labels. Kim and Hovy (2004) select candidate sentiment sentences and use word-based sentiment classifiers to classify unseen words into a negative or positive class. The lexicons are generated from manually selected seeds for a broad domain such as Health or Business, following an approach similar to (Kim and Hovy, 2004). Kim and Hovy (2004), among others, have combined the two tasks, identifying subjective text and detecting its sentiment polarity. Kim and Hovy (Kim and Hovy, 2004) used WordNet synonyms and antonyms to expand two lists of positive and negative seed words. However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) also address the classification into subjective/objective words and show this to be a potentially harder task than polarity classification with lower human agreement and automatic performance. There are only two prior approaches addressing word sense subjectivity or polarity classification. Kim and Hovy proposed two probabilistic models to estimate the strength of polarity (Kim and Hovy, 2004). However, Kim and Hovy (2004) and Andreevskaiaand Bergler (2006) show that subjectivity recognition might be the harder problem with lower human agreement and automatic performance. 
Extracting Aspect-Evaluation and Aspect-Of Relations in Opinion Mining The technology of opinion extraction allowsusers to retrieve and analyze people?s opinions scattered over Web documents. We define an opinion unit as a quadruple consist ing of the opinion holder, the subject being evaluated, the part or the attribute in which the subject is evaluated, and the value of theevaluation that expresses a positive or neg ative assessment. We use this definition as the basis for our opinion extraction task. We focus on two important subtasks of opinion extraction: (a) extracting aspect-evaluationrelations, and (b) extracting aspect-of re lations, and we approach each task usingmethods which combine contextual and sta tistical clues. Our experiments on Japaneseweblog posts show that the use of contex tual clues improve the performance for both tasks. We also analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains, which is identical with the conclusion presented by Kobayashiet al (2007).  Kobayashi et al (2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation.   In (Kobayashi et al 2007), a pattern mining method was used. Kobayashi et al (2007) adopted a supervised learning technique to search for useful syntactic patterns as contextual clues. Kobayashi et al (2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation.
Combining Outputs from Multiple Machine Translation Systems Currently there are several approaches to machine translation (MT) based on different paradigms; e.g., phrasal, hierarchical and syntax-based. These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge. The availability of such a variety of systems has led to a growing interest toward finding better translations by combining outputs from multiple systems. This paper describes three different approaches to MT system combination. These combination methods operate on sentence, phrase and word level exploiting information from -best lists, system scores and target-to-source phrase alignments. The word-level combination provides the most robust gains but the best results on the development test sets (NIST MT05 and the newsgroup portion of GALE 2006 dry-run) were achieved by combining all three methods. This work was extended in (Rosti et al, 2007) by introducing system weights for word confidences. The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al, 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Compared to the baseline from (Rosti et al, 2007), the new method improves the BLEU scores significantly. The subnetworks in the latter approach may be weighted by prior probabilities estimated from the alignment statistics (Rosti et al, 2007a). Confusion network based system combination for machine translation has shown promising advantage compared with other techniques based system combination, such as sentence level hypothesis selection by voting and source sentence re-decoding using the phrases or translation models that are learned from the source sentences and target hypotheses pairs (Rosti et al, 2007a; Huang and Papineni, 2007). Among the four steps, the hypothesis alignment presents the biggest challenge to the method due to the varying word orders between outputs from different MT systems (Rosti et al 2007). Sim et al (2007), Rosti et al (2007a), and Rosti et al (2007b) used minimum Translation Error Rate (TER) (Snover et al, 2006) alignment to build the confusion network. Similar to (Rosti et al, 2007a), each word in the hypothesis is assigned with a rank-based score of 1/(1+r), where r is the rank of the hypothesis. The system used in this paper is a variant of the one proposed in Rosti et al (2007a), which we now describe in detail. Meanwhile, we also use a word-level combination framework (Rosti et al, 2007) to combine the multiple translation hypotheses and employ a new rescoring model to generate the final result. We also implemented the word-level system combination (Rosti et al, 2007) and the hypothesis selection method (Hildebrand and Vogel, 2008).  The current state-of-the-art is confusion-network-based MT system combination as described by Rosti and colleagues (Rosti et al., 2007a, Rosti et al., 2007b). Bangalore et al (2001) used a multiple string matching algorithm based on Levenshtein edit distance, and later Sim et al (2007) and Rosti et al (2007) extended it to a TER-based method for hypothesis alignment. Similar to (Rosti et al, 2007), each word in the confusion network is associated with a word posterior probability. Various techniques include hypothesis selection from different systems using sentence-level scores, re-decoding source sentences using phrases that are used by individual systems (Rosti et al., 2007a; Huang and Papineni, 2007) and word-based combination techniques using confusion networks (Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b). Rosti et al (2007a) collect source-to-target correspondences from the input systems, create a new translation option table using only these phrases, and re-decode the source sentence to generate better translations. For selecting the best skeleton, two common methods are choosing the hypothesis with the Minimum Bayes Risk with translation error rate (TER) (Snover et al, 2006) (i.e., the hypothesis with the minimum TER score when it is used as the reference against the other hypotheses) (Sim et al, 2007) or choosing the best hypotheses from each system and using each of those as a skeleton in multiple confusion networks (Rosti et al, 2007b). Rosti et al (2007) look at sentence-level combinations (as well as word and phrase-level), using reranking of n-best lists and confidence scores derived from generalised linear models with probabilistic features from n-best lists. A new search space is constructed from these backbone-aligned outputs, and then a voting procedure or feature-based model predicts a final consensus translation (Rosti et al, 2007).
Functional Centering Grounding Referential Coherence In Information Structure Considering empirical evidence from a free-word-order language (German) we propose a revision of the principles guiding the ordering of discourse entities in the forward-looking center list within the centering model. We claim that grammatical role criteria should be replaced by criteria that reflect the functional information structure of the utterances. These new criteria are based on the distinction between hearer-old and hearer-new discourse entities. We demonstrate that such a functional model of centering can be successfully applied to the analysis of several forms of referential text phenomena, viz. pronominal, nominal, and functional anaphora. Our methodological and empirical claims are substantiated by two evaluation studies. In the first one, we compare success rates for the resolution of pronominal anaphora that result from a grammaticalrole-driven centering algorithm and from a functional centering algorithm. The second study deals with a new cost-based evaluation methodology for the assessment of centering data, one which can be directly derived from and justified by the cognitive load premises of the centering model. Using functional centering (Strube and Hahn, 1999) to rank the CFs led to no improvements, because of the almost perfect correlation in our domain be tween subject hood and being discourse-old. For example, Strube and Hahn (1999) introduce Functional Centering, a variant of Centering Theory which utilizes information status distinctions between hearer-old and hearer-new entities. Violations of CHEAPNESS (Strube and Hahn, 1999), COHERENCE and SALIENCE (Kibble and Power, 2000). work has also been done on adapting the centering model to other, freer word order languages such as German (Strube and Hahn, 1999). Finally, the scripts determine whether CBn is the same as CPn, known as the principle of cheapness (Strube and Hahn, 1999).  Arguably, the free word-order of German arguably leads to a clearer distinction between grammatical function, surface order, and in formation status (Strube and Hahn, 1999). Thus the size of the annotated data (3,115 personal pronouns1, 2,198 possessive pronouns, 928 demonstrative pronouns) compares favourably with the size of evaluation data in other proposals (619 German pronouns in (Strube and Hahn, 1999), 2,477 English pronouns in (Ge et al, 1998), about 5,400 English co referential expressions in (Ng and Cardie, 2002)) .In the experiments, systems only looked for single NP antecedents. In general, however, the required knowledge sources are lacking, so they must be hand-coded and can only be applied in restricted domains (Strube and Hahn, 1999). Strube and Hahn (1999) extend the context to more than the last sentence, but switch preference order between the last and the current sentence so that an antecedentes determined in the last sentence, whenever possible. Strube (1998) and Strube and Hahn (1999) argue that the information status of an antecedent is more important than the grammatical role in which it occurs. Moreover, in order to compare our proposal with Centering approach, Functional Centering by Strube and Hahn (1999) has also been implemented. Finally, a transition is considered to satisfy the CHEAPNESS constraint (Strube and Hahn, 1999) if Cb (ui)= Cp (ui).
Coarse-To-Fine N-Best Parsing And MaxEnt Discriminative Reranking Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000). A discriminative reranker requires a source of candidate parses for each sentence. This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000). This method generates 50-best lists that are of substantially higher quality than previously obtainable. We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less. Niu et al (2009) also use the reranker (RP) of Charniak and Johnson (2005) as a stronger baseline, but the results are missing. Features extracted from the output of three probabilistic parsers of English (Charniak and Johnson, 2005), one trained on Wall Street Journal trees (Marcus et al, 1993), one trained on a distorted version of the tree bank obtained by automatically creating grammatical error and adjusting the parse trees, and the third trained on the union of the original and distorted versions. Reranking has been used in many tasks to find better global solutions, such as machine translation (Wang et al, 2007), parsing (Charniak and Johnson, 2005), and disfluency detection (Zwarts and Johnson, 2011).   We parse the English sentences with the Charniak Parser (Charniak and Johnson, 2005), and tag the Chinese sentences with a POS tagger implemented faithfully according to (Collins, 2002) and trained on the Penn Chinese Treebank 5.0 (Xue et al., 2005). This is similar to the pruning described in Charniak and Johnson (2005) where edges in a coarse-grained parse forest are pruned to allow full evaluation with fine grained categories.  Standard state-of-the-art parsing systems (e.g., Charniak and Johnson, 2005) typically involve two passes. We experimented with three scenarios; in two of them we trained using the gold standard trees and then tested on gold standard parse trees (GoldGold), and text annotated using a state-of-the-art statistical parser (Charniak and Johnson, 2005) (Gold Charniak), respectively.  Charniak and Johnson (2005) showed accuracy improvements from composed local tree features on top of a lexicalized base parser.  We adapt the maximum entropy reranker from Charniak and Johnson (2005) by creating a customized feature extractor for event structures - in all other ways, the reranker model is unchanged. To improve performance and robustness, features are pruned as in Charniak and Johnson (2005): selected features must distinguish a parse with the highest F1 score in a n-best list, from a parse with a suboptimal F1 score at least five times. To train the classifiers, we used parse trees from the Charniak and Johnson (2005) parser with the same feature representation as in the original system. Other training algorithms include perceptron-style algorithms (Liang et al, 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al, 2005). We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and HPSG parser (Miyao and Tsujii, 2005). Given a tree pair (f, c), whose respective parses (pif ,pic) were generated by the parser described in (Charniak and Johnson, 2005), the goal is to transform the tree pair into SCFG derivations, in order to build relative frequency estimates for our Markovized models from observed SCFG productions. The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measure on sentences of all lengths), which are consistent with these studies.
Inducing History Representations For Broad Coverage Statistical Parsing We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser. The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge. Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions. we take a corpus-based approach to this empirical investigation, using a previously defined statistical parser (Henderson, 2003). Of the previous work on using neural net works for parsing natural language, by far the most empirically successful has been the work using Simple Synchrony Networks (Henderson,2003). This provides the neural network with a linguistically appropriate inductive bias when it learns the history representations, as explained in more detail in (Henderson, 2003). The input features for these log linear models are the real-valued vectors computed by h (d1, ... ,di), as explained in more detail in (Henderson, 2003). In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. Instead we use a pruning strategy similar to that described in (Henderson, 2003), where it was applied to a considerably harder search problem: constituent parsing with a left-corner parsing order. We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output rich information comprising both a parse tree and semantic role labels robustly, that is without any significant degradation of the parser's accuracy on the original parsing task. The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. Henderson's parsing model (Henderson, 2003) has a similar motivation as ours in that a derivation history of a parse tree is compactly represented by induced hidden variables (hidden layer activation of a neural network), although the details of his approach is quite different from ours. Briefly, our method consists in augmenting a state-of-the-art statistical parser (Henderson, 2003), whose architecture and properties make it particularly adaptive to new tasks. Our approach maintains state-of-the-art results in parsing, while also reaching state-of-the-art result sin function labelling, by suitably extending a Simple Synchrony Network (SSN) parser (Henderson, 2003) into a single integrated system. We use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which crucially do not make any explicit independence assumptions, and learn to smooth across rare feature combinations. SSN parsers, on the other hand, do not state any explicit independence assumptions: they induce a finite history representation of an unbounded sequence of moves, so that the representation of a move i − 1 is included in the inputs to the represention of the next move i, as explained in more detail in (Henderson, 2003). H03 indicates the model illustrated in (Henderson, 2003). (Henderson, 2003) tested the effect of larger input vocabulary on SSN performance by changing the frequency cut-off that selects the input tag-word pairs. In this they are similar to the class of neural networks proposed in (Henderson, 2003) for constituent parsing. Unlike (Titov and Henderson, 2007b), in the shared task we used only the simplest feed-forward approximation, which replicates the computation of a neural network of the type proposed in (Henderson, 2003). We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output richer information robustly, that is without any significant degradation of the parser's accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics. To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which do not make any explicit independence assumptions, an dare therefore likely to adapt without much modification to the current problem. (Henderson, 2003) exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step.
A Statistical Model For Multilingual Entity Detection And Tracking Entity detection and tracking is a relatively new addition to the repertoire of natural language tasks. In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text. Both the mention detection model and the novel entity tracking model can use arbitrary feature types, being able to integrate a wide array of lexical, syntactic and semantic features. In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers. The proposed framework is evaluated with several experiments run in Arabic, Chinese and English texts; a system based on the approach described here and submitted to the latest Automatic Content Extraction (ACE) evaluation achieved top-tier results in all three evaluation languages. Details of the mention detection and coreference system can be found in (Florian et al, 2004). However, Florian et al (2006) used some gazetteers and the output of other Information Extraction (IE) models as additional features, which provided significant gains ((Florian et al, 2004)). We use an information extraction toolkit (Florian et al, 2004) to analyze each event argument. In the case when some in-domain labeled training data is available, we show how to use SCL together with the classifier combination techniques of Florian et al (2004) to achieve even greater performance. In this case, we use classifiers as features as described in Florian et al (2004). In this case, we make use of the out-of-domain data by using features of the source domain tagger's predictions in training and testing the target domain tagger (Florian et al, 2004). Aside from Florian et al (2004), several authors have also given techniques for adapting classification to new domains. This is because, similar to many NLP tasks, good performance has been shown to depend heavily on integrating many sources of information (Florian et al, 2004). Initially, the corpus is automatically annotated with NE types in the source and target languages using NE identifiers similar to the systems described in (Florian et al, 2004) for NE detection. In addition, feature-based integration has been used by Taskar et al (2005), who trained a discriminative word alignment model using features derived from the IBM models, and by Florian et al (2004), who trained classifiers on auxiliary data to guide named entity classifiers. The performance of many natural language processing tasks, such as shallow parsing (Zhang et al., 2002) and named entity recognition (Florianet al, 2004), has been shown to depend on integrating many sources of information. Good performance in many natural language processing tasks has been shown to depend heavily on integrating many sources of information (Florian et al., 2004). These features were described in (Florian et al, 2004), and are not discussed here. We also note that while Florian et al (2004) and Blitzer et al (2006) observe that including the label of a source classifier as a feature on small amounts of target data tends to improve over using either the source alone or the target alone, we did not observe that for our data. Finally we note that while Blitzer et al (2006) did combine SCL with labeled target domain data, they only compared using the label of SCL or non-SCL source classifiers as features, following the work of Florian et al (2004). Florian et al (2004) first train a NE tagger on the source domain, and then use the tagger's predictions as features for training and testing on the target domain. Each instance represents w i, the token under consideration, and consists of 29 linguistic features, many of which are modeled after the systems of Bikel et al (1999) and Florian et al (2004), as described below. For event coreference, we follow the approach to entity coreference detailed in (Florian et al,2004). Florian et al (2004) reports good results on the 2003 ACE task. These features were described in (Florian et al, 2004), and are not discussed here.
Correcting ESL Errors Using Phrasal SMT Techniques This paper presents a pilot study of the use of phrasal Statistical Machine Translation (SMT) techniques to identify and correct writing errors made by learners of English as a Second Language (ESL). Using examples of mass noun errors in the Learner Error Cor- (CLEC) guide creation of an engineered training set, we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers. Our system was able to correct 61.81% of mistakes in a set of naturallyoccurring examples of mass noun errors found on the World Wide Web, suggesting that efforts to collect alignable corpora of preand post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners. The phrasal Statistical Machine Translation (SMT) technique is employed to identify and correct writing errors (Brockett et al, 2006). Recent work by Brockett et al (2006) utilized phrasal Statistical Machine Translation (SMT) techniques to correct ESL writing errors and demonstrated that this data-intensive SMT approach is very promising, but they also pointed out SMT approach relies on the availability of large amount of training data.  Brockett et al (2006) introduce errors involving mass/count noun confusions into English newswire text and then use the resulting parallel corpus to train a phrasal SMT system to perform error correction.  Brockett et al (2006) showed that phrase-based statistical MT can help to correct mistakes made on mass nouns. Similar to our approach, Brockettet al (2006) view error correction as a Machine Translation problem. Note that our approach is different from that of Brockett et al (2006), as we do make use of a truly multi-lingual translation model. Brockett et al (2006) uses phrasal SMT techniques to identify and correct mass noun errors of ESL students. Some researchers explicitly focus on individual classes of errors, e.g., mass vs count nouns in (Brockett et al, 2006) and (Nagata et al, 2006). Brockett et al (2006) employed phrasal Statistical Machine Translation (SMT) techniques to correct countability errors. Brockett et al (2006) propose the use of the phrasal statistical machine translation (SMT) technique to identify and correct ESL errors. To construct the training corpus, we followed the idea in Brockett et al (2006), and applied a similar strategy described in section 3.4 to the SRL system's training data to generate aligned pairs. Because of the great flexibility of the log-linear model, researchers have used the framework for other tasks outside SMT, including grammatical error correction (Brockett et al, 2006).
CoNLL-X Shared Task On Multilingual Dependency Parsing Each year the Conference on Computational Natural Language Learning features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems. The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing. In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured. We also give an overview of the parsing approaches that participants took and the results that they achieved. Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser? Acknowledgement Many thanks to Amit Dubey and Yuval Krymolowski, the other two organizers of the shared task, for discussions, converting treebanks, writing and helping with the also to Alexander Yeh for additional help with the paper reviews. His work was made possible by the MITRE Cor The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions, whereby each correctly identified pair of head-dependent words is counted towards the success of the parser (Buchholz and Marsi, 2006). For this paper since we are primarily concerned with the merging of tree structures we only evaluate UAS (Buchholz and Marsi, 2006). Recently dependency parsing has received renewed interest, both in the parsing literature (Buchholz and Marsi, 2006) and in applications like translation (Quirk et al, 2005) and information extraction (Culotta and Sorensen, 2004). 19.6% of the sentences in the corpus contain non-projective edges and 1.8% of the edges are non-projective, which is almost 5 times more frequent than in English and is the same as the Czech non-projectivity level (Buchholz and Marsi, 2006). The standard procedure for this purpose would be cross-validation. However, the popular data sets used for bench marking parsers, such as those that emerged 1176 from the CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006), are typically based on monolingual text. The treebank data used to train the German parser is the Tiger Treebank (Brants et al, 2002), in the version released with the CoNLL-X shared task (Buchholz and Marsi, 2006). We have used the 10 smallest data sets from CoNNL-X (Buchholz and Marsi, 2006) in our experiments. Currently there are about a dozen input/output conversion filters available, covering various existing data formats including the TigerXML format, the for mats of the Penn Treebank (Marcus et al, 1994), the CoNLL-X shared task format (Buchholz and Marsi, 2006), and the formats of the Latin Dependency (Bamman and Crane, 2006), Sinica (Chu Ren et al, 2000), Slovene Dependency (D? zero ski et al, 2006) (SDT), and Alpino (van der Beek et al., 2002) tree banks.  We evaluate all constraints and measures described in the previous section on 12 languages, whose treebanks were made available in the CoNLL-X shared task on dependency parsing (Buchholz and Marsi,2006). Additionally, we converted the annotation about scope of negation into a token-per-token representation, following the standard format of the 2006 CoNLL Shared Task (Buchholz and Marsi, 2006), where sentences are separated by a blank line and fields are separated by a single tab character. Datasets and Evaluation Our experiments are run on five different languages: Chinese (ch), Danish (da), Dutch (nl), Portuguese (pt) and Swedish (sv) (da ,nl, pt and sv are free data sets distributed for the 2006 CoNLL Shared Tasks (Buchholz and Marsi, 2006)). Multilingual parsers of participants in the CoNLL 2006 shared task (Buchholz and Marsi, 2006) can handle Japanese sentences. We find that partial correspondence projection gives rise to parsers that outperform parsers trained on aggressively filtered data sets, and achieve unlabeled attachment scores that are only 5% behind the aver age UAS for Dutch in the CoNLL-X Shared Task on supervised parsing (Buchholz and Marsi, 2006). Despite its simplicity, the partial correspondence approach proves very effective and leads to parsers that achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We use the CoNLL-X data format for dependency trees (Buchholz and Marsi, 2006) to encode partial structures. Penn Treebank (Marcus et al, 1993) the HPSG LinGo Redwoods Treebank (Oepen et al, 2002), and a smaller dependency treebank (Buchholz and Marsi, 2006). The CoNLL-X (Buchholz and Marsi, 2006) and CoNLL 2007 (Nivre et al, 2007) shared tasks focused on multilingual dependency parsing. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). The Spanish corpus was parsed using the MST dependency parser (McDonald et al, 2005) trained using dependency trees generated from the the English Penn Treebank (Marcus et al, 1993) and Spanish CoNLL-X data (Buchholz and Marsi, 2006). So that we could directly compare against statistical translation models, our Spanish and English monolingual corpora were drawn from the Europarl parallel corpus (Koehn, 2005).
First-Order Probabilistic Models for Coreference Resolution Traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases. In this paper, we propose a machine learning method enables features over noun phrases, resulting in a first-order probabilistic model for coreference. We outline a set of approximations that make this approach practical, and apply our method to the ACE coreference dataset, achieving a 45% error reduction over a comparable method that only considers features of pairs of noun phrases. This result demonstrates an example of how a firstorder logic representation can be incorporated into a probabilistic model and scaled efficiently. Culotta et al (2007) present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not. In our study, we also tested the "Most-X" strategy for the first-order features as in (Culotta et al., 2007), but got similar results without much difference (±0.5% F-measure) in performance. The one exception is the size of a cluster (Culotta et al, 2007). Culotta et al (2007) also apply online learning in a first-order logic framework that enables non-local features, though using a greedy search algorithm. Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions and thus operates directly on entities. However, with the advent of the ACE data, many systems either evaluated only true mentions, i.e. mentions which are included in the annotation, the so-called key, or even received true information for mention boundaries, heads of mentions and mention type (Culotta et al, 2007, inter alia). Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions.  As we show, this combination of a pairwise model and strong features produces a 1.5 percentage point increase in B-Cubed F-Score over a complex model in the state-of-the-art system by Culotta et al. (2007), although their system uses a complex, non-pairwise model, computing features over partial clusters of mentions. Culotta et al (2007) introduced a model that predicts whether a pair of equivalence classes should be merged, using features computed over all the mentions in both classes. For our experiments in Section 5, we use gold mention types as is done by Culotta et al (2007) and Luo and Zitouni (2005). Many of our features are similar to those described in Culotta et al (2007). Our test set contains the same 107 documents as Culotta et al (2007). For the experiments in Section 5, following Culotta et al (2007), to make experiments more comparable across systems, we assume that perfect mention boundaries and mention type labels are given. Culotta et al (2007) is the best comparable system of which we are aware. Note that since we report results on Dev-Eval, the results in Table 6 are not directly comparable with Culotta et al (2007). Motivated in part by Culotta et al (2007), we create cluster-level features from the relational features in our feature set using four predicates: NONE, MOST FALSE, MOST-TRUE, and ALL.   We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from Culotta et al (2007) and Bengston and Roth (2008).
Deterministic Parsing Of Syntactic Non-Fluencies 128 24% 161 29% 47 9% 148 27% 32 6% 17 3% 11 2% 6. Discussion the rules for Fidditch are written as deterministic pattern-action rules of the same sort as the rules in the parsing grammar, their operation is in a sense isolable. The patterns of the self-correction rules are checked first, before any of the grammar rule patterns are checked, at each step in the parse. Despite this independence in terms of rule ordering, the operation of the self-corr,:ction component is closely tied to the grammar of the parser; for it is the parsing grammar that specifies what sort of constituents count as the same for copying. example, if the grammar did not treat a noun phrase when it is subject of a sentence, the self-correction rules could not properly resolve a sentence like People-a people from Kennsington the editing rules would never recognize that the same sort of element. (Note (13) treated as a Restart because lexical trigger is not present.) Thus, the observed pattern of self-correction introduces empirical constraints on the set of features that are available for syntactic rules. The self-correction rules impose constraints not only on what linguistic elements must count as the same, but also on what must count as different. For example, in sentence be recognized as different sorts of elements in the grammar for the AUX node to be correctly the grammar assigned the words exactly the same part of speech, then the Category C;.7y Editor necessarily apply, incorrectly expunging (14) Kid could-be a brain in school. It appears therefore that the pattern of self-corrections that occur represents a potentially rich source of evidence about the nature of syntactic categories. the patterns of self-correction count as about the nature of categories for the linguist, then this data must be equally available to the language learner. This would suggest that, far from being an impediment to language learning, non-fluencies may in fact facilitate language acquisition by highlighting equivalent classes. expunction of edit signal only surface copy category copy stack copy restart failures remaining unclear and ungrammatical 127 This raises the general question of how children can acquire a language in the face of unrestrained non-fluency. How can a language learner sort out the grammatical from the ungrammatical strings? (The non-fluencies of speech are of course but one aspect of the degeneracy of input that makes language acquisition a puzzle.) The self-correction system I have described suggests that many non-fluent strings can be resolved with little detailed linguistic knowledge. As Table 1 shows, about a quarter of the editing signals result in expunction of only non-linguistic material. This requires only an ability to distinguish linguistic from nonlinguistic stuff, and it introduces the idea that edit signals signal an expunction site. Almost a third are resolved by the Surface Copying rule, which can be viewed simply as an instance of the general non-linguistic rule that multiple instances of the same thing count as a single instance. The category copying rules are generalizations of simple copying, applied to a knowledge of linguistic categories. Making the transition from surface copies to category copies is aided by the fact that there is considerable overlap in coverage, defining a path of expanding generalization. Thus at the earliest stages of learning, only the simplest, non-linguistic self-correction rules would come into play, and gradually the more syntactically integrated would be acquired. Contrast this self-correction system to an approach that handles non-fluencies by some general problem solving routines, for example Granger (1982), who proposes reasoning from what a speaker might be expected to say. Besides the obvious inefficiencies of general problem solving approaches, it is worth giving special emphasis to the problem with learnability. A general problem solving approach depends crucially on evaluating the likelihood of possible deviations from the norms. But a language learner has by definition only partial and possibly incorrect knowledge of the syntax, and is therefore unable to consistently identify deviations from the grammatical system. With the editing system I describe, the learner need not have the ability to recognize deviations from grammatical norms, but merely the non-linguistic ability to recognize copies of the same thing. far, I have considered the selfcorrection component from the standpoint of parsing. However, it is clear that the origins are in the process of generation. The mechanism for editing self-corrections that I have proposed has as its essential operation expunging one two identical is unable to expunge a sequence of two elements. (The Surface Copy Editor might be viewed as a counterexample to this claim, but see below.) Consider expunction now from the standpoint of the generator. Suppose self-correction bears a one-to-one relationship to a possible action of the generator (initiated by some monitoring component) which could be called ABANDON CONSTRUCT X. And suppose that this action can be initiated at any time up until CONSTRUCT X is completed, when a signal is returned that the construction is complete. Further suppose that ABANDON CONSTRUCT X causes an editing signal. When the speaker decides in the middle of some linguistic element to abandon it and start again, an editing signal is produced. If this is an appropriate model, then the elements which are self-corrected should be exactly those elements that xist at some stage in the generation process. Thus, we should be able to find evidence for the units involved in generation by looking at the data of self-correction. And indeed, such evidence should be available to the language learner as well. Summary I have described the nature of self-corrected speech (which is a major source of spoken non-fluencies) and how it can be resolved by simple editing rules within the context of a deterministic parser. Two features are essential to the self-correction system: 1) every self-correction site (whether it results in the expunction of words or not) is marked by a phonetically identifiable signal placed at the right edge of the potential expunction site; and 2) the expunged part is the left-hand member of a pair of copies, one on each side of the editing signal. The copies may be of three types: 1) identical surface strings, which are edited by a matching rule that applies before syntactic analysis begins; 2) complete constituents, when two constituents of the same type appear in the parser's buffer; or 3) incomplete constituents, when the parser finds itself trying to complete a constituent of the same type as a constituent it has just completed. Whenever two such copies appear in such a configuration, and the first one ends with an editing signal, the first is expunged from further analysis. This editing system has been implemented as part of a deterministic parser, and tested on a wide range of sentences from transcribed speech. Further study of the self-correction system promises to provide insights into the units of production and the nature of linguistic categories. Fidditch is one such deterministic parser, designed to provide a syntactic analysis of text as a tool for locating examples of various linguistically interesting structures (Hindle 1983). Hindle (1983) addressed the problem of correcting self repairs by adding rules to a deterministic parser that would remove the necessary text.  Example: I think that you get - it is more strict in Catholic schools. (Hindle 1983). (Hindle, 1983) and (Bear et al., 1992) performed speech repair identification in their parsers, and removed the corrected material (reparandum) from consideration. (Hindle, 1983) states that repairs are available for semantic analysis but provides no details on the representation to be used. RIM builds upon Labov (1966) and Hindle (1983) by conceptually extending the EDIT SIGNAL HYPOTHESIS that repairs are acoustically or phonetically marked at the point of interruption of fluent speech. One proposal for repair processing that lends itself to both incremental processing and the integration of speech cues into repair detection is that of Hindle (1983), who defines a typology of repairs and associated correction strategies in terms of extensions to a deterministic parser. An hypothesized acoustic phonetic edit signal, "a markedly abrupt cut-off of the speech signal" (Hindle, 1983 ,p.123), is assumed to mark the interruption of fluent speech (cf. (Labov, 1966)). Importantly, Hindle's system allows for non surface-based corrections and sequential application of correction rules (Hindle, 1983, p. 123). RIM incorporates two main assumptions of Hindle (1983): (1) correction strategies are linguistically rule-governed, and (2) linguistic ues must be available to signal when a disfluency has occurred and to 'trigger' correction strategies. Other than syntactic knowledge includes grammar specific recovery rules such as recta-rules (Weishedel and Sondheimer, 1983), semantic or pragmatic knowledge which may depend on a particular domain (Carbonell and Hayes, 1983) or the characteristics of the ill-formed utterances observed in human discourse (Hindle, 1983). 
Learning Multilingual Subjective Language via Cross-Lingual Projections This paper explores methods for generating subjectivity analysis resources in a new language by leveraging on the tools and resources available in English. Given a bridge between English and the selected target language (e.g., a bilingual dictionary or a parallel corpus), the methods can be used to rapidly create tools for subjectivity analysis in the new language. To date, the most closely related work is Mihalcea et al (2007), which explores cross-lingual projections to generate subjectivity analysis resources in Romanian by leveraging on the tools and resources available in English. In addition to the above methods for using English resources, the lexicon-based method investigated in Mihalcea et al (2007) can also use English resources by directly projecting English lexicons into Chinese lexicons. Mihalcea et al (2007) learn multilingual subjectivity via cross-lingual projections. Mihalcea et al (2007) propose a method to learn multilingual subjective language via cross-language projections.  Mihalcea et al (2007) propose two methods for translating sentiment lexicons. Some CLOA works used bilingual dictionaries (Mihalcea et al, 2007), or aligned corpus (Kim and Hovy, 2006) to align the expressions between source and target languages.  There is a growing number of methods that use data available in one language to build text processing tools for another language, for diverse tasks such as word sense disambiguation (Ng et al, 2003), syntactic parsing (Hwa et al, 2005), information retrieval (Monz and Dorr, 2005), subjectivity analysis (Mihalcea et al, 2007), and others. For instance, (Mihalcea et al, 2007) use an English corpus annotated for subjectivity along with parallel text to build a subjectivity classifier for Romanian. Mihalcea et al (2007) and Banea et al (2008) proposed a number of approaches exploiting a bilingual dictionary, a parallel corpus, and an MT system to port the resources and systems available in English to languages with limited resources. For subjectivity lexicons translation, Mihalcea et al (2007) and Wan (2008) used the first sense in a bilingual dictionary. Our simple approach produces moderate-sized lexicons (3,808, 3,980, 3,027 for Korean, Chinese, and Japanese) compared to Mihalcea et al (2007)'s complicated translation approach (4,983 Romanian words). In (Mihalcea et al, 2007), different shortcomings of lexicon-based translation scheme was discussed for the more semantic-oriented task subjective analysis, instead the authors proposed to use a parallel-corpus, apply the classifier in the source language and use the corresponding sentences in the target language to train a new classifier. MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007)). (Mihalcea et al, 2007) describes experiments with subjectivity classification for Romanian. The lexicon obtained after 5 iterations of the method was used for sentence level sentiment classification, indicating an 18% improvement over the lexicon of (Mihalcea et al, 2007). Mihalcea et al, (2007) and Banea et al, (2008) used machine translation technique to leverage English resources for analysis in Romanian and Spanish languages. Mihalcea et al (2007), for example, generate subjectivity analysis resources in a new language from English sentiment resources by leveraging a bilingual dictionary or a parallel corpus. In (Mihalcea et al, 2007), a bilingual lexicon and a manually translated parallel corpus are used to generate a sentence classifier according to their level of subjectivity for Romanian.
A Classifier-Based Parser With Linear Run-Time Complexity We present a classifier-based parser that produces constituent trees in linear time. The parser uses a basic bottom-up shiftreduce algorithm, but employs a classifier to determine parser actions instead of a grammar. This can be seen as an extension of the deterministic dependency parser of Nivre and Scholz (2004) to full constituent parsing. We show that, with an appropriate feature set used in classification, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers. We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively. This approach differs from that of Yamada and Matsumoto (2003) and Sagae and Lavie (2005), who parallelize according to the POS tag of one of the child items. Like Yamada and Matsumoto (2003) and Sagae and Lavie (2005), our parser is driven by classifiers. Although training time is still a concern in our setup, the situation is ameliorated by generating training examples in advance and inducing one-vs-all classifiers in parallel, a technique similar in spirit to the POS-tag parallelization in Yamada and Matsumoto (2003) and Sagae and Lavie (2005). We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. A Shift-Reduce Algorithm for Deterministic Constituent Parsing In its deterministic form, our parsing algorithm is the same single-pass shift-reduce algorithm as the one used in the classifer-based parser of Sagae and Lavie (2005). Sagae and Lavie (2005) built two deterministic parsers this way, one using support vector machines, and one using k-nearest neighbors. Features used for classification, with features 1 to 13 taken from Sagae and Lavie (2005). Training the maximum entropy classifier with such a large number (1.9 million) of training instances and features required more memory than was available (the maximum training set size we were able to train with 2GB of RAM was about 200,000 instances), so we employed the training set splitting idea used by Yamada and Matsumoto (2003) and Sagae and Lavie (2005). For comparison, Sagae and Lavie (2005) report that training support vector machines for one-against-all multi-class classification on the same set of features for their deterministic parser took 62 hours, and training a k-nearest neighbors classifier took 11 minutes. More interestingly, it parses all 2,416 sentences (more than 50,000 words) in only 46 seconds, 10 times faster than the deterministic SVM parser of Sagae and Lavie (2005). Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples. One extrapolation is to a very fast stochastic parser by Sagae and Lavie (2005).  In the deterministic setting there is only one correct path, so example generation is identical to that of Sagae and Lavie (2005). Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples. This model is inspired by Sagae and Lavie (2005), in which a stack-based representation of monolingual parsing trees is used. Sagae and Lavie (2005) propose a constituency based parsing method to determine sentence dependency structures. In our approach, which is based on the shift-reduce parser for English reported in (Sagae and Lavie,2005), the parsing task is transformed into a succession of classification tasks. A simple transformation process as described in (Sagae and Lavie, 2005) is employed to convert between arbitrary branching trees and binary trees. Sagae and Lavie (2005) have shown that this algorithm has linear time complexity, assuming that classification takes constant time.
Monotonic Semantic Interpretation hiyan@cam. sri . corn ralcam. sri. corn ABSTRACT Aspects of semantic interpretation, such as quantifier scoping and reference resolution, are often realised computationally by non-monotonic operations involving loss of information and destructive manipulation of semantic representations. The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation. Semantics for QLF are presented in which the denotations of formulas are extended monotonically as QLF expressions are resolved. Figure 1: Lexical entries and a sample derivation in LUD Logical Form, QLF which also is a monotonic representation language for compositional semantics as discussed in (Alshawi and Crouch, 1992). This led to the development of underspecified semantic representations (e.g. QLF, Alshawi and Crouch (1992) and MRS, Copestake et al (2005)) which are easier to produce from text without contextual inference but which can be further specified as necessary for the task being performed. In contrast to statements of a fully specified logic in which denotations are typically taken to be functions from possible worlds to truth values (Montague, 1973), denotations of a statement in an under specified logic are typically taken to be relations between possible worlds and truth values (Alshawi and Crouch (1992), Alshawi (1996)). Secondly, monotonicity guarantees that interpretation algorithms can proceed incrementally, combining information from various sources in a nondestructive way (Alshawi and Crouch, 1992). First, a quasi logical form allows the under-specification of several types of information, such as anaphoric references, ellipsis and semantic relations (Alshawi and Crouch, 1992).
A Maximum Entropy Word Aligner For Arabic-English Machine Translation This paper presents a maximum entropyword alignment algorithm for Arabic English based on supervised training data.We demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of su pervised and unsupervised methods yields superior performance. The probabilisticmodel used in the alignment directly models the link decisions. Significant improvement over traditional word alignment tech niques is shown as well as improvement onseveral machine translation tests. Perfor mance of the algorithm is contrasted with human annotation performance. This model was significantly better than the MaxEnt aligner (Ittycheriah and Roukos, 2005) and is also flexible in the sense that it allows for arbitrary features to be introduced while still keeping training and decoding tractable by using a greedy decoding algorithm that explores potential alignments in a small neighborhood of the current alignment. The model thus needs a reasonably good initial alignment to start with for which we use the MaxEnt aligner (Ittycheriah and Roukos, 2005) as in McCarley et al (2011). We experimented with two different supervised aligners: a maximum entropy aligner (Ittycheriah and Roukos, 2005) and an improved correction model that corrects the maximum entropy alignments (McCarley et al, 2011). In each language, the rule extraction was performed using approximately 1.2M sentence pairs aligned using a maxent aligner (Ittycheriah and Roukos, 2005) trained using a variety of domains (Europarl, computer manuals) and a maximum entropy parser for English (Ratnaparkhi, 1999). To quantify this effect, we learn reordering rules using three sets of alignments: HMM alignments, alignments from a supervised MaxEnt aligner (Ittycheriah and Roukos, 2005), and hand alignments. Parallel sentences were first word-aligned using a MaxEnt aligner (Ittycheriah and Roukos, 2005). As our word aligner (Ittycheriah and Roukos, 2005) can introduce errors in extracting Tree-to-String rules, we use a small hand-aligned data set CE16K, which consists of 16K sentence pairs, to get relatively clean rules, free from alignment errors. A discriminatively trained 1-to-N model with feature functions specifically designed for Arabic was presented in (Ittycheriah and Roukos, 2005). Ittycheriah and Roukos (2005) trained a discriminative model on a corpus of ten thousand word aligned Arabic-English sentence pairs that outperformed a GIZA++ baseline. The gold standard alignments we use here are part of the IBM Arabic-English aligned corpus (IBMAC) (Ittycheriah and Roukos, 2005). Since the IBMAC gold alignments we use are not marked as such, AER reduces to 1 F-score (Ittycheriah and Roukos, 2005): Pr= |A? S||A| Rc= |A? S| |S| AER= 1? 2PrRc Pr+Rc where A links are proposed and S links are gold. Ittycheriah and Roukos (2005) used only the top 50 sentences in IBMAC test data. The two results are not comparable because: (a) Ittycheriah and Roukos (2005) used additional gold aligned data that was not released and (b) they use an additional 500K sentences from the LDC UN corpus for Giza training that was created by adapting to the source side of the test set - the details of such adaptation were not provided and thus it is not clear how to replicate them to compare fairly. For word alignments we use the Maximum Entropy aligner described in (Ittycheriah and Roukos, 2005) that is trained using hand aligned training data. To find the most likely alignment we use the same algorithm as in (Ittycheriah and Roukos, 2005) since the structure of the model is unchanged. The data in this corpus is automatically aligned using a technique presented in (Ittycheriah and Roukos, 2005). Ittycheriah and Roukos (2005) used a maximum entropy classifier to train an alignment model using hand-labeled data. On our test set, (Tillmann and Zhang, 2005) reports a BLEU score of 37.8 and (Ittycheriah and Roukos, 2005) reports a BLEU score of 48.0. We assume that each sentence pair in the training corpus is word-aligned (e.g. using a MaxEnt aligner (Ittycheriah and Roukos, 2005) or an HMM aligner (Ge, 2004)). 
Discriminative Sentence Compression With Soft Syntactic Evidence We present a model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers. The parsers are trained out-of-domain and contain a significant amount of noise. We argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly. This differs from current state-of-the-art models (Knight and Marcu, 2000) that treat noisy parse trees, for both compressed and uncompressed sentences, as gold standard when calculating model parameters. McDonald (McDonald, 2006) independently proposed a new machine learning approach. However, while the former problem can be solved efficiently using the dynamic programming approach of McDonald (2006), there are no efficient algorithms to recover maximum weighted non projective subtrees in a general directed graph. McDonald (2006) provides a Viterbi-like dynamic programming algorithm to recover the highest scoring sequence of order-preserving bigrams from a lattice, either in unconstrained form or with a specific length constraint. For consistent comparisons with the other systems, our re-implementation does not include the k-best inference strategy presented in McDonald (2006) for learning with MIRA. The third one is the bigram model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference.  Some utilize a parser to identify and later keep certain important relations but do not require a complete parse (Clarke & Lapata, 2008), or use a syntactic representation to extract features (McDonald, 2006).  One successful recent approach (McDonald, 2006) combines a discriminative framework with a set of features that capture information similar to the K&M model. This phenomenon has been noted and discussed in the task of pairwise sentence fusion (Daume III and Marcu, 2004) and also in sentence compression (McDonald, 2006).  Better statistical methods have been developed for producing high quality compression candidates (McDonald, 2006), that maintain linguistic quality, some recent work even uses ILPs for exact inference (Clarke and Lapata, 2008). This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al, 2005). McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words.  Note that their model is a strong baseline: it performed significantly better than competitive approaches (McDonald, 2006) across a variety of compression corpora. Both these systems reported results outperforming previous systems such as McDonald (2006). For example, such solvers are unable to take advantage of efficient dynamic programming routines for sentence compression (McDonald, 2006). The same framework can be readily adapted to other compression models that are efficiently decodable, such as the semi-Markov model of McDonald (2006), which would allow incorporating a language model for the compression. McDonald (2006) also presents a sentence compression model that uses a discriminative large margin algorithm.
The Language Of Bioscience: Facts Speculations And Statements In Between We explore the use of speculative language in MEDLINE abstracts. Results from a manual annotation experiment suggest that the notion of speculative sentence can be reliably annotated by humans. In addition, an experiment with automated methods also suggest that reliable automated methods might also be developed. Distributional observations are also presented as well as a discussion of possible uses for a system that can recognize speculative language. The most clearly relevant study is Light et al (2004) where the focus is on introducing the problem, exploring annotation issues and outlining potential applications rather than on the specificities of the ML approach, though they do present some results using a manually crafted substring matching classifier and a supervised SVM on a collection of Medline abstracts. To further elucidate the nature of the task and improve annotation consistency, we have developed a new set of guidelines, building on the work of Light et al (2004).  As a baseline classifier we use the substring matching technique of (Light et al, 2004), which labels a sentence as spec if it contains one or more of the following: suggest, potential, likely, may, at least, in part ,possibl, further investigation, unlikely ,putative, insights, point toward, promise and propose. Previous studies (Light et al, 2004) showed that the detection of hedging can be solved effectively by looking for specific keywords which imply that the content of a sentence is speculative and constructing simple expert rules that describe the circumstances of where and how a key word should appear. Results obtained adding external dictionaries In our final model we added the keywords used in (Light et al, 2004) and those gathered for our ICD 9-CM hedge detection module. Baseline 1 denotes the substring matching system of Light et al (Light et al, 2004) and Baseline 2de notes the system of Medlock and Briscoe (Medlock and Briscoe, 2007). For clinical free texts, Baseline 1 is an out-domain model since the keywords were collected for scientific texts by (Light et al, 2004) . Our finding that token unigram features are capable of solving the task accurately agrees with the the results of previous work son hedge classification ((Light et al, 2004), (Med 287 lock and Briscoe, 2007)), and we argue that 2-3 word-long phrases also play an important role as hedge cues and as non-speculative uses of an otherwise speculative keyword as well (i.e. to resolve an ambiguity). In recent years, there has been increasing interest in the speculative aspect of biomedical language (Light et al, 2004, Wilbur et al, 2006, Medlock and Briscoe, 2007). In general, these studies focus on issues regarding annotating speculation and approach the problem of recognizing speculation as a text classification problem, using the well-known bag of words method (Light et al 2004, Medlock and Briscoe, 2007) or simple substring matching (Light et al, 2004). Light et al (2004) explore issues with annotating speculative language in biomedicine and out line potential applications. Light et al (2004) report low inter-annotator agreement in distinguishing low speculative sentences from highly speculative ones. First, we used the substring matching method reported in Light et al (2004), which labels sentences containing one of more of the following as speculative: suggest, potential, likely, may, at least ,inpart ,possibl, further investigation, unlikely ,putative, insights, point toward, promise and propose (Baseline1). For example, on line product and movie reviews have provided a rich context for analyzing sentiments and opinions in text (see Pang and Lee (2008) for a recent survey), while tentative, speculative nature of scientific writing, particularly in biomedical literature, has provided impetus for recent research in speculation detection (Light et al, 2004). We collected cue phrases for such a content shifted sentence detection from the dataset adapters can be found as the supplementary material 765 works of Chapman et al (2007), Light et al (2004) and Vincze et al (2008) and from the experiments of Farkas and Szarvas (2008) and Farkas et al (2009). Light et al (2004) present a study on an notating hedges in biomedical documents. The overall inter-annotator agreement was = 0.65, which is similar to what Light et al (2004) report but worse than Medlock and Briscoe' s (2007) results. And as we expected, the feature that represents whether the word is in the hedge list or not is very useful especially in hedge cue finding, indicating that methods based on a hedge cue lists (Light et al, 2004) or keyword selection (Szarvas, 2008) are quite significant way to accomplish such tasks. Light et al (2004) started to do annotations on biomedicine article abstracts, and conducted the preliminary work of automatic classification for uncertainty.
Towards Answering Opinion Questions: Separating Facts From Opinions And Identifying The Polarity Of Opinion Sentences Opinion question answering is a challenging task for natural language processing. In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level. We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level. We also present a first model for classifying opinion sentences as positive or negative in terms of the main perspective being expressed in the opinion. Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classification (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy).  (Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level. We extract several types of features, including a set of pattern features, and then design a classifier to identify sentiment polarity for each question (similar as (Yuand Hatzivassiloglou, 2003)). Our work is similar to Yu and Hatzivassiloglou (2003) and Wiebe et al (1999) in that we use lexical and POS features. Turney proposed the unsupervised method for sentiment classification (Turney, 2002), and similar method is utilized by many other researchers (Yu and Hatzivassiloglou, 2003). Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. Yu and Hatzivassiloglou (2003) addressed three challenges in the news article domain: discriminating between objective documents and subjective documents such as editorials, detecting subjectivity at the sentence level, and determining polarity at the sentence level. Pang and Riloff (2005) and Yu and Hatzivassiloglou (2003) trained sentence-level subjectivity classifiers and proved that performing sentiment analysis targeting selected subjective sentences only gets higher results. For example, Yu and Hatzivassiloglou (2003) separated facts from opinions and assigned polarities only to opinions. There have been attempt son tackling this so-called document-level subjectivity classification task, with very encouraging results (see Yu and Hatzivassiloglou (2003) and Wiebe et al (2004) for details).  Indeed, recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g, Yu and Hatzivassiloglou (2003)) and classifying the polarity based solely on the subjective portions of the document (e.g., Pang and Lee (2004)). Yu and Hatzivassiloglou (2003) use semantically oriented words for identification of polarity at the sentence level. At sentence level, Yu and Hatzivassiloglou (2003) propose to classify opinion sentences as positive or negative in terms of the main perspective being expressed in opinionated sentences.    The annotations in Yu and Hatzivassiloglou (2003) are sentence-level subjective vs. objective and polarity judgments. These approaches rely on presence and scores of sentiment-bearing words that have been acquired from dictionaries (Kim and Hovy, 2005) or corpora (Yu and Hatzivassiloglou, 2003). (Wiebe et al2001, Yu and Hatzivassiloglou 2003), a task that is not relevant for the processing of very brief pieces of direct customer feedback.
The Interaction Of Knowledge Sources In Word Sense Disambiguation Word sense disambiguation (WSD) is a computational linguistics task likely to benefit from the tradition of combining different knowledge sources in artificial intelligence research. An important step in the exploration of this hypothesis is to determine which linguistic knowledge sources are most useful and whether their combination leads to improved results. We present a sense tagger which uses several knowledge sources. Tested accuracy exceeds 94% on our evaluation corpus. Our system attempts to disambiguate all content words in running text rather than limiting itself to treating a restricted vocabulary of words. It is argued that this approach is more likely to assist the creation of practical systems. Stevenson and Wilks (2001) presented a classifier combination framework where disambiguation methods (simulated annealing, subject codes and selectional restrictions) were combined using the TiMBL memory-based approach (Daelemans et al, 1999). Stevenson and Wilks (2001) propose a somewhat related technique to handle WSD, based on integrating LDOCE classes with simulated annealing. Stevenson and Wilks (2001) investigated the interaction of knowledge sources, such as part-of-speech, dictionary definition, subject codes, etc. on WSD. However, they are quite rare, even in monolingual contexts (Stevenson and Wilks, 2001, e.g.), and they are not able to integrate and use knowledge coming from corpus and other resources during the learning process.  This is also shown by Stevenson and Wilks (2001), who used the Longman Dictionary of Contemporary English (LDOCE) as sense inventory.  However, there has been no direct comparison of which knowledge sources are the most useful or whether combining a variety of knowledge sources, a strategy which has been shown to be successful for WSD in the general domain (Stevenson and Wilks, 2001), improves results.  In the hybrid approaches that have been explored so far, deep knowledge, like selectional preferences, is either pre-processed into a vector representation to accommodate machine learning algorithms, or used in previous steps to filter out possible senses e.g. (Stevenson and Wilks, 2001). We refer to this method as stacking, and it has been previously used to integrate heterogeneous knowledge sources for WSD (Stevenson and Wilks, 2001). POS tags of the focus word itself are also included, to aid sense disambiguations related to syntactic differences (Stevenson and Wilks, 2001). Prior research (e.g., (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)) suggests that use of both syntactic and lexical features will improve disambiguation accuracies.
Parsing As Deduction By exploring the relationship between parsing and deduction, a new and more general view of chart parsing is obtained, which encompasses parsing for grammar formalisms based on unification, and is the basis of the Earley Deduction proof procedure for definite clauses. efficiency of this approach for an interesting class grammars is discussed. such as Earley deduction, to coestruct a parser, a.s shown in Pereira and Warren (1983). For reasons of time, I won't go into details of Earley Deduction (see Pereira and Warren (1983) for details}. Deductive logic (Pereira and Warren, 1983), extended with semi rings (Goodman, 1999), is an established formal ism used in parsing. While chart parsing can famously be cast as deduction (Pereira and Warren, 1983), what chart parsing really is is an algebraic closure over the rules of a phrase structure grammar, which is most naturally expressed inside a constraint solver such as CHR (Morawietz, 2000). Basically, it is similar to Earley's algorithm (Earley, 1970), augmented with unification (Pereira and Warren, 1983) and probability (Paeseler, 1987), but with a delayed commitment approach to scoring (Aho and Peterson, 1972). The "parsing as deduction" framework (Pereira and Warren, 1983) is now over 20 years old. But if we use existing techniques for parsing DCGs, then we are also confronted with an undecidability problem: the recognition problem for DCGs is undecidable (Pereira and Warren, 1983). In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983) but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. It is exactly this mismatch between structure of the traversal and Pereira and Warren (1983) point out that Earley deduction is not restricted to a left-to-right expansion of goals, but this suggestion was not followed up with a specific a lgorithm addressing the problems discussed here. The parsing-as-deduction approach proposed in Pereira and Warren (1983) and exlended in Shieber et al (1995) and the parsing schemaladetincd in Sikkel (1997) are well established parsing paradigms in computalional linguistics. One of the first definitions was suggested by Pereira and Warren (1983).  Pereira and Warren (1983) and Shieber (1985) present versions of Earley's algorithm for unification grammars, in which unification is the sole operation responsible for attribute valuation.
Tagging English Text With A Probabilistic Model In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence. The main novelty of these experiments is the use of untagged text in the training of the model. We have used a simple triclass Markov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided. Two approaches in particular are compared and combined: • using text that has been tagged by hand and computing relative frequency counts, • using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle. Experiments show that the best training is obtained by using as much tagged text as possible. They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available. Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions. Similar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions. We adopt the problem formulation of Merialdo (1994), in which we are given a dictionary of possible tags for each word type. Research into unsupervised part-of-speech tagging with a tag dictionary (sometimes called weakly supervised POS tagging) has been going on for many years Merialdo (1994). We also did not consider morphological analyzers as a form of type supervision, as suggested by Merialdo (1994). Advanced students might also want to read about a modern supervised trigram tagger (Brants, 2000), or the mixed results when one actually trains trigram taggers by EM (Merialdo, 1994). For instance, Merialdo (1994) uses maximum likelihood estimation to train a trigram HMM. Merialdo (1994), in a now famous negative result, attempted to improve HMM POS tagging by expectation maximization with unlabeled data. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). EM was first used in POS tagging in (Merialdo, 1994) which showed that except in conditions where there are no labeled training data at all, EM performs very poorly. We adopt the problem formulation of Merialdo (1994), in which we are given a raw word sequence and a dictionary of legal tags for each word type. Previous results on unsupervised POS tagging using a dictionary (Merialdo, 1994) on the full 45-tag set. The basic engine used to perform the tagging in these experiments is a direct descendent of the maximum entropy (ME) tagger of (Ratnaparkhi, 1996) which in turn is related to the taggers of (Kupiec, 1992) and (Merialdo, 1994). We can surmise from the log-likelihood plot that the drop in accuracy is not due to the optimization being led astray, but probably rather due to the complex relationship between likelihood and task specific evaluation metrics in unsupervised learning (Merialdo, 1994). Work on type-supervision goes back to (Merialdo, 1994), who introduced the still standard procedure of using a bigram Hidden Markov Model (HMM) trained via Expectation Maximization. Such work has for instance been based on hidden Markov models (Merialdo, 1994). (Merialdo, 1994): In the context of POS tagging, the author introduces a method that he calls maximum likelihood tagging. Although useful under some circumstances, when a relatively large amount of labeled data is available, the procedure often degrades performance (e.g. Merialdo (1994)). Merialdo (1994) reports an accuracy of 86.6% for an unsupervised word-based HMM,. We adopt the common problem formulation for this task described by Merialdo (1994).
Improved Automatic Keyword Extraction Given More Linguistic Knowledge In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed. The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on (such as term frequency and grams), a better result is obtained as measured by keywords previously assigned by professional indexers. In more detail, exgives a better precithan and by adding the tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied. In Hulth (2003a) an evaluation of three different methods to extract candidate terms from documents is presented. In Hulth (2003b), experiments on how the performance of the keyword extraction can be improved by combining the judgement of three classifiers are presented. For these experiments, the same machine learning system RDS is used as for the experiments presented by Hulth (2003a). It was noted in Hulth (2003b) that when extracting NP chunks, the accompanying determiners are also extracted (per definition), but that determiners are rarely found at the initial position of keywords. In the experiments presented in Hulth (2003a), only the documents present in the training, validation, and test set respectively are used for calculating the collection frequency. In the experiments discussed so far, the weights given to the positive examples are those resulting in the best performance for each individual classifier (as described in Hulth (2003a)). In the experiments presented in Hulth (2003a), the automatic keyword indexing task is treated as a binary classification task, where each candidate term is classified either as a keyword or a non-keyword. Several key phrase extraction algorithms have been discussed in the literature, including ones based on machine learning methods (Turney, 2000), (Hulth, 2003) and tf-idf ((Frank et al, 1999)). In the statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency? inverse document frequency (tf.idf) (Salton and Buckley, 1988), among others. Additional features to frequency that have been experimented are e.g. relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). More linguistic knowledge (such as syntactic features) has been explored by Hulth (2003). In statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency-inverse document frequency (tfidf) (Salton and Buckley, 1988), among others. Additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). More linguistic knowledge has been explored by Hulth (2003). Hulth (2003) contributed 2,000 abstracts of journal articles present in Inspec between the years 1998 and 2002. As shown in (Hulth, 2003), most key phrases are noun phrases. Previous work has pointed out the importance of syntactic features for supervised keyword extraction (Hulth, 2003). Finally, in recent work, (Hulth, 2003) proposes a system for keyword extraction from abstracts that uses supervised learning with lexical and syntactic features, which proved to improve significantly over previously published results. This is a relatively popular dataset for automatic key phrase extraction, as it was first used by Hulth (2003) and later by Mihalcea and Tarau (2004) and Liu et al (2009b). While Mihalcea and Tarau (2004) and our re implementations use all of these gold-standard key phrases in our evaluation, Hulth (2003 )andLiu et al address this issue by using as gold standard key phrases only those that appear in the corresponding document when computing recall.
Thumbs Up Or Thumbs Down? Semantic Orientation Applied To Unsupervised Classification Of Reviews This paper presents a simple unsupervised learning algorithm for classifying reviews up) or recdown). The classification of a review is predicted by the orientation the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., “subtle nuances”) and a negative semantic orientation when it has bad associations (e.g., “very cavalier”). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “excellent” minus the mutual information between the given phrase and the word “poor”. A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews. We also note that Turney (2002) found movie reviews to be the most difficult of several domains for sentiment classification, reporting an accuracy of 65.83% on a 120-document set (random-choice performance: 50%). Most of the authors traditionally use a classification-based approach for sentiment extraction and sentiment polarity detection (for example, Pang et al (2002), Turney (2002), Kim and Hovy (2004) and others), however, the research described in this paper uses the information retrieval (IR) paradigm which has also been used by some researchers. Following (Turney, 2002), Yuen et al (2004) investigate the association between polarity words and some strongly-polarized morphemes in Chinese, and present a method for inferring sentiment orientations of Chinese words. At phase level, Turney (2002) presents a technique for inferring the orientation and intensity of a phrase according to its PMI-IR statistical association with a set of strongly-polarized seed words. Based on (Hatzivassiloglou and Wiebe, 2000) and (Turney, 2002), we consider four types of structures (as shown in Table 5) during sentiment phrase extraction. Different from (Turney, 2002), we consider phrases with negations as their initial words. Turney (2002) described a way to automatically build such a lexicon based on looking at co-occurrences of words with other words whose sentiment is known. The problem of sentiment extraction at the document level (sentiment classification) has been tackled as a text categorization task in which the goal is to assign to a document either positive ("thumbs up") or negative ("thumbs down") polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003), Pang and Lee (2004)). Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method. In addition to the IE tasks in the biomedical domain, negation scope learning has attracted increasing attention in some natural language processing (NLP) tasks, such as sentiment classification (Turney, 2002). Thus, the method we investigate can be seen as a combination of methods for propagating sentiment across lexical graphs and methods for building sentiment lexicons based on distributional characteristics of phrases in raw data (Turney, 2002). Others, such as Turney (2002), Pang and Vaithyanathan (2002), have examined the positive or negative polarity, rather than presence or absence, of affective content in text. Much of the work in sentiment analysis in the computational linguistics domain has focused either on short segments, such as sentences (Wilson et al, 2005), or on longer documents with an explicit polarity orientation like movie or product reviews (Turney, 2002). These methods range from manual approaches of developing domain-dependent lexicons (Das and Chan, 2001) to semi-automated approaches (Hu and Liu, 2004) and fully automated approaches (Turney, 2002). Therefore, the overall sentiment of a document is not necessarily the sum of the content parts (Turney, 2002). Also, PMI-IR is useful for calculating semantic orientation and rating reviews (Turney, 2002). In previous work, statistical NLP computation over large corpora has been a slow, off line process, as in KNOWITALL (Etzioni et al, 2005) and also in PMI-IR applications such as sentiment classification (Turney, 2002). (Turney, 2002) worked on product reviews. Like our class-attribute associations, the common-sense knowledge that the word cool is positive while unethical is negative can be learned from associations in web-scale data (Turney, 2002). Search counts or search results have also been used for sentiment analysis (Turney, 2002), for transliteration (Grefenstette et al, 2004), candidate selection in machine translation (Lapata and Keller, 2005), text similarity measurements (Sahami and Heilman, 2006), in correct parse tree filtering (Yates et al, 2006), and paraphrase evaluation (Fujita and Sato, 2008).
A Prosodic Analysis Of Discourse Segments In Direction-Giving Monologues This paper reports on corpus-based research into the relationship between intonational variation and discourse structure. We examine the effects of speaking style (read versus spontaneous) and of discourse segmentation method (text-alone versus text-and-speech) on the nature of this relationship. We also compare the acoustic-prosodic features of initial, medial, and final utterances in a discourse segment. We use AuToBI's models, which were trained on the spontaneous speech Boston Directions Corpus (BDC) (Hirschberg and Nakatani, 1996), to identify prosodic events in our corpus. Note that, in order to transcribe the test data, we have trained a 20 Gaussian per state 39 MFCC Hidden Markov Model speech recognizer with HTK, using the training and development sets together with TIMIT (Fisher et al, 1986), the Boston Directions Corpus (BDC) (Hirschberg and Nakatani, 1996), and the Columbia Game Corpus (Hirschberg et al, 2005). Expert discourse structure analyses are used to derive CONSENSUS SEGMENTATIONS, consisting of discourse boundaries whose coding all three labelers agreed upon (Hirschberg and Nakatani, 1996). Speech was found to improve inter-annotator agreement in discourse segmentation of monologues (Hirschberg and Nakatani 1996). Most previous work that has combined multiple annotations has used linear segmentations, i.e. discourse segmentations without hierarchies (Hirschberg and Nakatani, 1996). A conservative way to combine segmentationsinto a gold standard is the Consensus (CNS) (or raw agreement) approach (Hir chberg and Nakatani, 1996). Discourse structure has been successfully used in non-interactive settings (e.g. understanding specific lexical and prosodic phenomena (Hirschberg and Nakatani, 1996), natural language generation (Hovy, 1993), essay scoring (Higgins et al, 2004) as well as in interactive settings (e.g. predictive/generative models of postural shifts (Cassell et al., 2001), generation/interpretation of anaphoric expressions (Allen et al, 2001), performance modeling (Rotaru and Litman, 2006)). We report results on the Boston University (BU) Radio Speech Corpus (Ostendorf et al, 1995) and Boston Directions Corpus (BDC) (Hirschberg and Nakatani, 1996), two publicly available speech corpora with manual ToBI annotations intended for experiments in automatic prosody labeling. In spontaneous monologue, Butterworth (1972) found that the beginning of a discourse segment exhibited slower speaking rate; Swerts (1995), and Passonneau and Litman (1997) found that pause length correlates with discourse segment boundaries; Hirschberg and Nakatani (1996) found that the beginning of a discourse segment correlates with higher pitch.
Robust Pronoun Resolution with Limited Knowledge Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledgebased system, however, is that it is a very labourintensive and time-consuming task. This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the success rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be successfully adapted for other languages with minimum modifications. The algorithm with the next-to-highest results in (Charniak and Elsner, 2009) is MARS (Mitkov, 1998) from the GuiTAR (Poesio and Kabadjov, 2004) toolkit. They provide restrictions on co-indexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).  ParalStuct marks whether a candidate and an anaphor have similar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998). Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors. We have already voiced concern (Mitkov, 1998a), (Mitkov, 2000b) that the evaluation of anaphora resolution algorithms and systems is bereft of any common ground for comparison due not only to the difference of the evaluation data, but also due to the diversity of pre-processing tools employed by each anaphora resolution system. RAP (Kennedy and Boguraev, 1996), Baldwin's pronoun resolution method (Baldwin, 1997) and Mitkov's knowledge-poor pronoun resolution approach (Mitkov, 1998b). Mitkov's approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent. As expected, the results reported in Table 1 do not match the original results published by Kennedy and Boguraev (1996), Baldwin (1997) and Mitkov (1998b) where the algorithms were tested on different data, employed different pre-processing tools, resorted to different degrees of manual intervention and thus provided no common ground for any reliable comparison. The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998) and a partial implementation of the algorithm for resolving definite descriptions proposed by Vieira and Poesio (2000). Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified. The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre specific antecedent indicators to the remaining candidates (Mitkov, 1998). Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).  Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov,1998). Mitkov's knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates. How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished. Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be over come by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora. The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998). For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).
Named Entity Recognition in Tweets: An Experimental Study People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner. The performance of standard NLP tools is severely degraded on tweets. This paper addresses this issue by re-building the NLP pipeline beginning with part-of-speech tagging, through chunking, to recognition. Our novel doubles compared with the NER system. the redundancy inherent in tweets to achieve this performance, using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision. LabeledLDA outperforms coincreasing 25% over ten common entity types. NLP tools are available at: We employ the method of Ritter et al (2011) to tokenise messages, and use token unigrams as features, including any hash tags, but ignoring twitter mentions, URLs and purely numeric tokens.  Our second component - chunker - is taken from (Ritter et al, 2011), which also comes with a model trained on Twitter data and shown to perform better on noisy data such as user comments. The chunker from (Ritter et al., 2011) relies on its own POS tagger, however, in our structural representations we favor the POS tags from the CMU Twitter tagger and take only the chunk tags from the chunker. For example, Ritter et al (2011) develop a system that exploits a CRF model to segment named entities and then uses a distantly supervised approach based on LabeledLDA to classify named entities. We have also used a named entity tagger trained specifically on the Twitter data (Ritter et al, 2011) to directly extract named entities from tweets. Firstly, a named entity recognizer (Ritter et al, 2011) is employed to identify named entities. The social text serves as a very valuable information source for many NLP applications, such as the information extraction (Ritter et al, 2011), retrieval (Subramaniam et al, 2009), summarization (Liu et al, 2011a), sentiment analysis (Celikyilmaz et al, 2010), etc. We generate part-of-speech information over the original raw text using a Twitter part-of-speech tagger (Ritter et al, 2011). Using the UW Twitter NLP tools (Ritter et al, 2011). To study the diversity of named entities (NEs) in retweets, we used UW Twitter NLP Tools (Ritter et al., 2011) to extract NEs from RT-data. We then standardized variants (i.e. "fb" as a variant of "Facebook"), and manually categorized them against the 10-class schema defined by Ritter et al. (2011). Leaving out the dedicated test set to avoid in-sample bias, we evaluate our models across three data sets: RITTER (the 10% test split of the data in Ritter et al (2011) used in Derczynski et al (2013)), the test set from Foster et al.  
Automatic Construction Of A Hypernym-Labeled Noun Hierarchy From Text Previous work has shown that automatic methods can be used in building semantic lexicons. This work goes a step further by automatically creating not just clusters of related words, but a hierarchy of nouns and their hypernyms, akin to the hand-built hierarchy in WordNet. See Caraballo (1999) for a detailed description of a method to construct such a hierarchy. In Caraballo (1999), we construct a hierarchy of nouns, including hypernym relations. Caraballo (1999) proposed the first attempt, which used conjunction and apposition features to build noun clusters. Caraballo (1999) uses a hierarchical clustering technique to build a hyponymy hierarchy of nouns. The built-in ambiguity in the hyponymy hierarchy presented in (Caraballo, 1999) is primarily an effect of the fact that all information is composed into one tree. As was also reported by Caraballo (1999), the judges sometimes found proper nouns (as hyponyms) hard to evaluate. It is difficult to compare these results with results from other studies such as that of Caraballo (1999), as the data used is not the same. Caraballo (1999) let three judges evaluate ten internal nodes in the hyponymy hierarchy that had at least twenty descendants. In Section 4, we show how correctly extracted relationships can be used as "seed-cases" to extract several more relationships, thus improving recall; this work shares some similarities with that of Caraballo (1999). Another definition is given by Caraballo (1999). First of all, it would be interesting to apply LSA to a system for building an entire hypernym-labelled ontology in roughly the way described in (Caraballo, 1999), perhaps by using an LSA-weighted voting method to determine which hypernym would be used to label each node. As stated in (Caraballo, 1999), WordNet has been an important lexical knowledge base, but it is insufficient for domain specific texts. So, many attempts have been made to automatically produce taxonomies (Grefenstette, 1994), but (Caraballo, 1999) is certainly the first work which proposes a complete overview of the problem by (1) automatically building a hierarchical structure of nouns based on bottom-up clustering methods and (2) labeling the internal nodes of the resulting tree with hypernyms from the nouns clustered underneath by using patterns such as B is a kind of A. Caraballo (1999) was the first to use clustering for labeling is-a relations using conjunction and apposition features to build noun clusters. (Caraballo, 1999B) also used contextual information to determine the specificity of nouns. Contrary, domain specific terms don't tend to be modified by other words, because they have sufficient information in themselves (Caraballo, 1999B).  Caraballo (1999) uses conjunction and appositive annotations in the vector representation. Other approaches use natural language data, sometimes just by analyzing the corpus (Sanderson and Croft 1999), (Caraballo 1999) or by learning to expand WordNet with clusters of terms from a corpus, e.g., (Girju et al 2003). For example, based on (Caraballo 1999), each parent of a leaf node could be viewed as a cluster label for its children, with the weight of a parent-child link being determined based on how strongly the child is associated with the cluster.
Unsupervised Construction Of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources. Two techniques are employed: (1) simple string edit distance, and (2) a heuristic strategy that pairs initial (presumably summary) sentences from different news stories in the same cluster. We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation. Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set. On test data extracted by the heuristic strategy, however, performance of the two training sets is similar, with AERs of 13.2% and 14.7% respectively. Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase. The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships. We are currently experimenting with data extracted from the first two sentences in each article, which by journalistic convention tend to summarize content (Dolan et al 2004). The dataset is 5,801 pairs of sentences collected from news sources (Dolan et al, 2004). The F2 dataset was constructed from the first two sentences of the corpus on the same assumptions as those used in Dolan et al (2004). For these reasons, we used the Microsoft Research Paraphrase Corpus (MSRPC) introduced by Dolan et al2004). We employ 8 different MT metrics for identifying paraphrases across two different datasets the well-known Microsoft Research paraphrase corpus (MSRP) (Dolan et al, 2004) and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task (Potthast et al, 2010). For instance, with the advent of news aggregator services such as GoogleNews, one can readily collect multiple news stories covering the same news item (Dolan et al,2004). Dolan et al (2004) used Web-aggregated news stories to learn both sentence-level and word-level alignments. Levenshtein distance has been used in natural language processing field as a component in the variety of tasks, including semantic role labeling (Tjong Kim Sang et al, 2005), construction of the paraphrase corpora (Dolan et al, 2004), evaluation of machine translation output (Leusch et al, 2003), and others.    Although the F2 heuristic proposed by Dolan et al (2004), which takes the first two sentences of each document pair, obtains higher relatedness score (we evaluated F2 sentences as 50% paraphrases, 37% related, and 13% unrelated), our n-gram overlap method extracted much more sentence pairs per document pair. the Microsoft Research Paraphrase Corpus (Dolan et al, 2004) [MSR04].  In contrast, traditional paraphrase detection (Dolan et al, 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay and McKeown, 2001) and (Dolan et al,2004). More recently, (Cordeiro et al, 2007a) proposed the sumo metric specially designed for asymmetrical entailed pair identification in corpora which obtained better performance than previously established metrics, even in corpora with exclusively symmetrical entailed paraphrases as in the Microsoft Paraphrase Research Corpus (Dolan et al., 2004). Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al, 2006) and paraphrase identification (Dolan et al, 2004) are a few prototypical examples.  We evaluated the systems performance across two datasets: (Dolan et al, 2004) dataset and the Extended dataset, see the text for details.
CDER: Efficient MT Evaluation Using Block Movements Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks. In many cases though such movements still result in correct or almost correct sentences. In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation. Our measure can be exactly calculated in quadratic time. Furthermore, we will show how some evaluation measures can be improved Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al., 2005), which makes use of stemming, Word Net synonymy, verb class synonymy, matching noun phrase heads, and proper name matching.  Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. The CDER measure (Leusch et al, 2006) is based on edit distance, such as the well-known WER, but allows reordering of blocks. For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nieen et al, 2000) because of their support on fast and inexpensive evaluation. In addition to the widely used BLEU (Papineni et al, 2002) and NIST (Doddington, 2002) scores, we also evaluate translation quality with the recently proposed Meteor (Banerjee and Lavie, 2005) and four edit-distance style metrics, Word Error Rate (WER), Position independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al, 2006), and Translation Edit Rate (TER) (Snover et al, 2006). For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006). Other lexical similarity based automatic MT evaluation metrics, like NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumption that a good translation is one that shares the same lexical choices as the reference translation is not justified semantically. Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nie?en et al, 2000). Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy.  Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information.
Structural Ambiguity And Lexical Relations We propose that many ambiguous prepositional phrase attachments can be resolved on the basis of the relative strength of association of the preposition with verbal and nominal heads, estimated on the basis of distribution in an automatically parsed corpus. This suggests that a distributional approach can provide an approximate solution to parsing problems that, in the worst case, call for complex reasoning. As the result, we conclude that if we do not resolve PP attachment problem (Hindle and Rooth, 1993), to the expected extent, we will not extract he maximal noun phrases. PP-attachment is known to depend on the object of the preposition (Hindle and Rooth, 1993). For prepositions, we consider only cases in which the parent is a noun or a verb and the child is a noun (this corresponds to the cases considered by Hindleand Rooth (1993) and others). We therefore analyze the errors made on the development set to determine whether the difficult remaining cases for parsers correspond to the Hindle and Rooth (1993) style PP attachment classification task. Hindle and Rooth (1993) were the first to show that a corpus-based approach to PP attachment ambiguity resolution can lead to good results. After the initial effort by Hindle and Rooth (1993), it has become clear that this area needs statistical methods in which an easy integration of many information sources is possible. One of the earliest corpus-based approaches to prepositional phrase attachment used lexical preference by computing co-occurrence frequencies (lexical associations) of verbs and nouns with prepositions (Hindle and Rooth, 1993).  There are multiple approaches ranging from purely statistical (Ratnaparkhi, 1998), to hybrid approaches that take into account the lexical semantics of the verb (Hindle and Rooth, 1993). Li and Abe (1998) use a minimum description length-based algorithm to find an optimal tree cut over WordNet for each classification problem, finding improvements over both lexical association (Hindle and Rooth, 1993) and conceptual association, and equaling the transformation-based results. Examples of this work include a system by Liu et al (1990), and experiments by Hindle and Rooth (1993), and Resnik and Hearst (1993). It is also the case that we thought PP attachment might be improved because of the increased coverage of preposition noun and preposition-verb combinations that work such as (Hindle and Rooth, 1993) show to be so important. In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993). Hindle and Rooth (1993) used a partial parser to extract (v, n, p) tuples from a corpus, where p is the preposition whose attachment is ambiguous between the verb v and the noun n. Training on unambiguous cases is similar in spirit to (Hindle and Rooth, 1993). The pioneering work in this area was that of Hindle and Rooth (1993). Web frequencies were reliable enough and did not need smoothing for (i), but for (ii), smoothing using the technique described in Hindle and Rooth (1993) led to better recall. Pattern (5) is motivated by the observation that if n1 is a pronoun, this suggests a verb attachment (Hindle and Rooth, 1993). The standard method of solving the PP-attachment problem is based on collocation extraction (cf., e.g., (Hindle and Rooth, 1993)). Early work, including (Hindle and Rooth, 1993), concentrated on lexical associations, later also using word net information.
A Syntax-Directed Translator With Extended Domain Of Locality SD translation schema (synchronous grammar) (string relation) A syntax-directed translator first parses the source-language input into a parsetree, and then recursively converts the tree into a string in the target-language. We model this conversion by an extended treeto-string transducer that have multi-level trees on the source-side, which gives our system more expressive power and flexibility. We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation. The model is then extended to the general log-linear framework in order to rescore with other fealike language models. We devise a simple-yet-effective algorithm to non-duplicate translations rescoring. Initial experimental results on English-to-Chinese translation are presented. In this framework, decoding without language model (LM decoding) is simply a linear-time depth-first search with memoization (Huang et al, 2006), since a tree of n words is also of size O (n) and we visit every node only once. Huang et al (2006) study a TSG-based tree-to-string alignment model. Liu et al (2006) and Huang et al (2006) then used the TTS transducer on the task of Chinese-to-English and English-to-Chinese translation, respectively, and achieved decent performance. The formal description of a TTS transducer is describe din Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined in (Huang et al, 2006). The implementation of a TTS transducer can be done either top down with memoization to the visited subtrees (Huang et al, 2006), or with a bottom-up dynamic programming (DP) algorithm (Liu et al, 2006). To speed up the decoding ,standard beam search is used. In Figure 3, BinaryCombine denotes the target size binarization (Huang et al, 2006) combination. The translation candidates of the template's variables, as well as its terminals, are combined pair wise in the order they appear in the RHS of the template. Huang et al (2006) used character-based BLEU as a way of normalizing in consistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. Huang et al (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. Note that it is also possible to integrate our rule Markov model with other decoding algorithms, for example, the more common non-incremental top-down/bottom-up approach (Huang et al, 2006), but it would involve a non-trivial change to the decoding algorithms to keep track of the vertical derivation history, which would result in significant overhead. The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006). It is straightforward to generalize the algorithm for larger n-gram models and TTS templates with any number of children in the bottom using target-side binarized combination (Huang et al, 2006). Huang et al (2006) used character based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. We push the idea behind this method further and make the following contributions in this paper: We generalize cube pruning and adapt it to two systems very different from Hiero: a phrase based system similar to Pharaoh (Koehn, 2004) and a tree-to-string system (Huang et al, 2006). We test our methods on two large-scale English-to-Chinese translation systems: a phrase-based system and our tree-to-string system (Huang et al, 2006). Our data preparation follows Huang et al (2006): the training data is a parallel corpus of 28.3M words on the English side, and a trigram language model is trained on the Chinese side. We use the same test set as (Huang et al, 2006), which is a 140-sentence sub set of the NIST 2003 test set with 9-36 words on the English side. For cube growing, we use a non-duplicate k-best method (Huang et al, 2006) to get 100-best unique translations according to LM to estimate the lower-boundheuristics. This preprocessing step takes on aver age 0.12 seconds per sentence, which is negligible in comparison to the +LM decoding time. Compared with its string-based counterparts, tree-based decoding is simpler and faster: there is no need for synchronous binarization (Huang et al, 2009b; Zhang et al, 2006) and tree parsing generally runs in linear time (Huang et al, 2006). Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al, 2006), the source side (Huang et al, 2006), or both (Liu et al, 2009). The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al (2006).
A Maximum Entropy Model For Part-Of-Speech Tagging This paper presents a statistical model which trains from a corpus annotated with Part-Of- Speech tags and assigns them to previously unseen text with state-of-the-art accuracy(96.6%). The can be classified as a Entropy model and simultaneously uses many contextual &quot;features&quot; to predict the POS tag. Furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems. Since the raw Penn Treebank data contains many inconsistencies in its annotations (cf. Ratnaparkhi, 1996), a single inconsistency in a test set tree will very likely yield a zero percent parse accuracy for the particular test set sentence. For both tree banks, we convert from constituent to dependency format using pennconverter (Johansson and Nugues, 2007), and generate POS tags using the MXPOST tagger (Ratnaparkhi, 1996). We started with a maximum entropy based tagger that uses features very similar to the ones proposed in Ratnaparkhi (1996). Ratnaparkhi (1996: 134) suggests use of an approximation summing over the training data, which does not sum over possible tags:& quot; h E f j= 2 P (~) p (ti lhi) f j (hi, ti )i=1 However, we believe this passage is in error: such an estimate is ineffective in the iterative scaling algorithm. The features that define the constraints on the model are obtained by instantiation of feature templates as in Ratnaparkhi (1996). They are a subset of the features used in Ratnaparkhi (1996). The feature templates in Ratnaparkhi (1996) that were left out were the ones that look at the previous word, the word two positions before the current, and the word two positions after the current. Model Overall Unknown Word Accuracy Accuracy Baseline, 96.72% 84.5% J Ratnaparkhi 96.63% 85.56% (1996) Table 3 Baseline model performance This table also shows the results reported in Ratnaparkhi (1996: 142) for Convenience. This may stem from the differences between the two models &apos; feature templates, thresholds, and approximations of the expected values for the features, as discussed in the beginning of the section, or may just reflect differences in the choice of training and test sets (which are not precisely specified in Ratnaparkhi (1996)). One conclusion that we can draw is that at present the additional word features used in Ratnaparkhi (1996) looking at words more than one position away from the current do not appear to be helping the overall performance of the models. Some are the result of inconsistency in labeling in the training data (Ratnaparkhi 1996), which usually reflects a lack of linguistic clarity or determination of the correct part of speech in context. Following previous work (Ratnaparkhi, 1996), we assume that the tag of a word is independent of the tags of all preceding words given the tags of the previous two words (i.e.,? =2 in the equation above). A number of different sequential learning frameworks have been tried, yielding 96-97% accuracy: Lafferty et al (2001) experimented with conditional random fields (CRFs) (95.7% accuracy), Ratnaparkhi (1996) used a maximum entropy sequence classifier (96.6% accuracy), Brants (2000) employed a hidden Markov model (96.6% accuracy), Collins (2002) adopted an averaged perception discriminative sequence model (97.1% accuracy). Feature templates as in (Ratnaparkhi, 1996),. The best result known to us is achieved by Toutanova [2002] by enriching the feature representation of the MaxEnt approach [Ratnaparkhi, 1996]. For instance, implementing an efficient version of the MXPOST POS tagger (Ratnaparkhi, 1996) will simply involve composing and configuring the appropriate text file reading component, with the sequential tagging component, the collection of feature extraction components and the maximum entropy model component. In this bakeoff, our basic model is based on the framework described in the work of Ratnaparkhi (1996) which was applied for English POS tagging. We have explained elsewhere (Clark, 2002) how suitable features can be defined in terms of the  word ,pos-tag pairs in the context, and how maximum entropy techniques can be used to estimate the probabilities, following Ratnaparkhi (1996). Given the parallel corpus, we tagged the English words with a publicly available maximum entropy tagger (Ratnaparkhi, 1996), and we used an implementation of the IBM translation model (Al Onaizan et al, 1999) to align the words. The C&C supertagger is similar to the Ratnaparkhi (1996) tagger, using features based on words and POS tags in a five-word window surrounding the target word, and defining a local probability distribution over supertags for each word in the sentence, given the previous two super tags.
D-Tree Grammars designed to share some of the advantages of TAG while overcoming some its limitations. two composition operations called subsertion and sister-adjunction. The most distinctive feaof that, unlike TAG, there is complete uniformity in the way that the relate lexical items: subsertion always corresponds to complementation and sister-adjunction to modi- Furthermore, unlike TAG, provide a uniform analysis for whmovement in English and Kashmiri, despite the fact that the wh element in Kashmiri appears in sentence-second position, and not sentence-initial position as in English. We define a new grammar formalism, called D-Tree arises from work on Tree- Adjoining Grammars (TAG) (Joshi et al., 1975). A salient feature of TAG is the extended domain of locality it provides. Each elementary structure can be associated with a lexical item (as in Lexicalized (LTAG) (Joshi & 1991)). Properties related to the lexical item (such as subcategorization, agreement, certain types of word order variation) can be expressed within the elementary struc- (Kroch, 1987; Frank, 1992). In addition, remain tractable, yet their generative capacity is sufficient to account for certain syntactic phenomena that, it has been argued, lie beyond Context-Free Grammars (CFG) (Shieber, 1985). TAG, however, has two limitations which provide the motivation for this The first problem (discussed in Section that the of substitution and adjunction do not map cleanly onto the relations of complementation and modification. A second problem (discussed in Section 1.2) has to do with the of provide analyses for certain syntactic phenomena. In developing DTG we have tried to overcome these problems while remaining faithto what we see as the key advantages of particular, its enlarged domain of locality). In Section 1.3 we introduce some of the key features of explain how they are intended to address problems that we have identified with 1.1 Derivations and Dependencies operations of substitution and adjunction relate two lexical items. It is therefore natural to interpret these operations as establishing a direct linguistic relation between the two lexical items, namely a relation of complementation (predicateargument relation) or of modification. In purely CFG-based approaches, these relations are only implicit. However, they represent important linguistic intuition, they provide a uniform interface to semantics, and they are, as Schabes & Shieber (1994) argue, important in order to support statistical parameters in stochastic frameworks and appropriate constraints in many frameworks, complementation and modification are in fact made & Kaplan, 1982) provides a separate functional (f-) structure, and dependency grammars (see e.g. Mel'euk (1988)) use these notions as the principal basis for syntactic representation. We will follow the dependency literature in referring to complementation and modification as syntactic dependency. As observed by Rambow and Joshi (1992), for TAG, the importance of the dependency structure means that not only the derived phrase-structure tree is of interest, but also the operations by which we obtained it from elementary structures. This information is encoded in the derivation tree (Vijay-Shanker, 1987). However, as Vijay-Shanker (1992) observes, the operations are not used uniformly: while substitution is used only to add a (nominal) complement, adjunction is used both for modification and (clausal) complementation. Clausal complementation could not be handled uniformly by substitution because of the existence of syntactic such as in English. Furthermore, there is an inconsistency in 151 the directionality of the operations used for complementation in TAG@: nominal complements are substituted into their governing verb's tree, while the governing verb's tree is adjoined into its own clausal complement. The fact that adjunction and substitution are used in a linguistically heterogeneous manner means that (standard) TAG derivation trees do not provide a good representation of the dependencies between the words of the sentence, i.e., of the predicate-argument and modification structure. adore SUBJ COMP Figure 1: Derivation trees for (1): original definition (left); Schabes & Shieber definition (right) For instance, English sentence (1) gets the derivation structure shown on the left in Figure 1'. spicy hotdogs he claims Mary seems to adore When comparing this derivation structure to the dependency structure in Figure 2, the following problems become apparent. First, both adjectives deon in the derivation structure is daughter of addition, deon does its nominal argument, on the derivation strucis daughter of direction does express the actual dependency), and also daughter of neither is an argument of the other). claim SUBOMP he seem I COMP adore Mary hotdog </LOD spicy small Figure 2: Dependency tree for (1) Schabes & Shieber (1994) solve the first problem 'For clarity, we depart from standard TAG notational practice and annotate nodes with lexemes and arcs with grammatical function. by distinguishing between the adjunction of modifiers and of clausal complements. This gives us the derivation structure shown on the right in Figure 1. While this might provide a satisfactory treatment of modification at the derivation level, there are now three types of operations (two adjunctions and substitution) for two types of dependencies (arguments and modifiers), and the directionality problem for embedded clauses remains unsolved. defining have attempted to resolve these problems with the use of a single operation (that we call subsertion) for handling all complementation and a second operation (called sisteradjunction) for modification. Before discussion these operations further we consider a second problem with TAG that has implications for the design of these new composition operations (in particular, subsertion). 1.2 Problematic Constructions for TAG be used to provide suitable analyses for certain syntactic phenomena, including longdistance scrambling in German (Becker et al., 1991), Romance Clitics (Bleam, 1994), wh-extraction out of complex picture-NPs (Kroch, 1987), and Kashmiri wh-extraction (presented here). The problem in describing these phenomena with TAG arises from the fact (observed by Vijay-Shanker (1992)) that adjoining is an overly restricted way of combining structu- We illustrate the problem by considering Kashon Bhatt (1994). extraction in Kashmiri proceeds as in English, except that the wh-word ends up in sentence-second position, with a topic from the matrix clause in sentence-initial position. This is illustrated in (2a) for a simple clause and in (2b) for a complex clause. (2) rameshan kyaa dyutnay tse RameshERG whatNom gave youoAr What did you give Ramesh? b. rameshan kyaa, chu baasaan what is believeNPertthat kor 'ERG do What does Ramesh believe that I did? Since the moved element does not appear in sentence-initial position, the TAG analysis of English wit-extraction of Kroch (1987; 1989) (in which the matrix clause is adjoined into the embedded clause) cannot be transferred, and in fact no linguistically plausible TAG analysis appears to be available. In the past, variants of TAG have been developed to extend the range of possible analyses. In Multi-Component TAG (MCTAG) (Joshi, 1987), trees are grouped into sets which must be adjoined to- (multicomponent adjunction). However, MCexpressive power since, while syntactic relations are invariably subject to c-command or dominance constraints, there is no way to state that Mary OBJ hotdog claim SUBJ spicyhe I MOD small adore COMP COMP Mary OBJ seem hotdog claim MOD SUBJ spicy small he COMP seamSUBJ 152 two trees from a set must be in a dominance relain the derived tree. Domination et al., 1991) are multicomponent systems that allow for the expression of dominance constraints. However, MCTAG-DL share a further problem with MCTAG: the derivation structures cannot be given a linguistically meaningful interpretation. Thus, they fail to address the first prowe discussed (in Section 1.3 The DTG Approach Vijay-Shanker (1992) points out that use of adjunction for clausal complementation in TAG corresponds, at the level of dependency structure, to substitution at the foot node' of the adjoined tree. However, adjunction (rather than substitution) is used since, in general, the structure that is substituted may only form part of the clausal complement: the remaining substructure of the clausal complement appears above the root of the adjoined tree. Unfortunately, as seen in the examples given in Section 1.2, there are cases where satisfactory analyses cannot be obtained with adjunction. In particular, using adjunction in this way cannot handle cases in which parts of the clausal complement are required to be placed within the structure of the adjoined tree. of subsertion is designed to overcome this limitation. Subsertion can be viewed as a generalization of adjunction in which components of the clausal complement (the subserted structure) which are not substituted can be interspersed within the structure that is the site of the subsertion. Following earlier work (Becker et al., Vijay-Shanker, 1992), a mechanism involving the use of domination links (d-edges) that ensure that parts of the subserted structure that are not substituted dominate those parts that are. Furthermore, there is a need to constrain the way in which the non-substituted components can be interspersed'. This is done by either using appropriate feature constraints at nodes or by means of subsertion-insertion constraints (see Section 2). We end this section by briefly commenting on the of sister-adjunction. In TAG, modification is performed with adjunction of modifier trees that have a highly constrained form. In particular, the foot nodes of these trees are always daughters of the root and either the leftmost or rightmost frontier nodes. The effect of adjoining a these cases the foot node is an argument node of the lexical anchor. 'This was also observed by Rambow (1994a), where integrity constraint (first defined for an of TAG (Becker etal., 1991)) is defined for a MCTAG-DL version called V-TAG. However, this was found to be insufficient for treating both long-distance scrambling and long-distance topicalization in German. V-TAG retains adjoining (to handle topicalization) for this reason. tree of this form corresponds (almost) exactly to the addition of a new (leftmost or rightmost) subtree below the nede that was the site of the adjunction. this reason, we have equipped an operation (sister-adjunction) that does exactly this and more. From the definition of Section 2 it can be seen that the essential aspects of Schabes & Shieber (1994) treatment for modification, including multiple modifications of a phrase, be captured by using this defining Section 2, we discuss, in Section 3, DTG analyses for the English and Kashmiri data presented in this section. Section 4 briefly algorithms. 2 Definition of D-Tree Grammars d-tree is a tree with two types of domination edges (d-edges) and immediate domination edges (i-edges). D-edges and i-edges express domination and immediate domination relations between nodes. These relations are never rescinded when dtrees are composed. Thus, nodes separated by an i-edge will remain in a mother-daughter relationship throughout the derivation, whereas nodes separated by an d-edge can be equated or have a path of any length inserted between them during a derivation. D-edges and i-edges are not distributed arbitrarily in d-trees. For each internal node, either all of its daughters are linked by i-edges or it has a single daughter that is linked to it by a d-edge. Each node is labelled with a terminal symbol, a nonterminal or the empty string. containing n can be decomposed into n + 1 containing only i-edges. D-trees can be composed using two operations: subsertion and sister-adjunction. When a d-tree is subserted into another d-tree component of a is substituted at a frontier nonterminal node (a node) and all components of a that are above the substituted component are inserted into d-edges above the substituted node or placed above the root node. For example, consider the d-trees a and shown in Figure 3. Note that components are shown as triangles. In the composed d-tree 7 the component a(5) is substituted at a substitution node in /3. The components, a(1), a(2), and a(4) of a above a(5) drift up the path in which runs from the substitution node. These are then into in # or above the root of #. In general, when a component some d-tree a is inserted into a d-edge betnodes and two new d-edges are created, the first of which relates and the root node of the second of which relates the frontier 'Santorini and Mahootian (1995) provide additional evidence against the standard TAG approach to modification from code switching data, which can be accounted for by using sister-adjunction. 153 Figure 3: Subsertion node of a(i) that dominates the substituted comto is possible for components above the substituted node to drift arbitrarily far up the d-tree and distribute themselves within domination edges, or above the root, in any way that is compatible with the domination relationships present in the d-tree. a mechanism called constraints control what can appear within d-edges (see below). The second composition operation involving dtrees is called sister-adjunction. When a d-tree a is at a node ri in a d-tree the comd-tree from the addition to fl of a as a new leftmost or rightmost sub-d-tree below that sister-adjunction involves the addition of exactly one new immediate domination edge and that several sister-adjunctions can occur at the same constraints where d-trees can be sister-adjoined and whether they will be rightor left-sister-adjoined (see below). DTG a four tuple G = , VT S, D) the usual nonterminal and termialphabets, a distinguished nonterand a finite set of DTG said to be each d-tree in the grammar has at least one terminal node. The d-trees of a grammar two additional annotations: subsertion-insertion constraints and sister-adjoining constraints. These will be described below, but first we define simultaneously and subsertion-adjoining trees (SAtrees), which are partial derivation structures that can be interpreted as representing dependency in In this section we consider two existing grammars: the XTAG grammar, a wide coverage LTAG, and the LEXSYS grammar, a wide coverage D-Tree Substitution Grammar (Rambow et al, 1995). Sister-adjunctionis not an operation found in standard de ni tions of TAG, but is borrowed from D-TreeGrammar (Rambow et al, 1995). It also has much in common with d tree substitution grammar (Rambow et al, 1995).  These spines can be combined using a sister-ad junction operation (Rambow et al, 1995), to form larger pieces of structure. Chiang's model adds a third composition operation called sister-ad junction (see Figure 3), borrowed from D-tree substitution grammar (Rambow et al, 1995). Variants of TAG like TIG (Schabes and Waters, 1995) or D-Tree grammars (Rambow et al, 1995) are motivated by linguistic or formal considerations rather than pedagogical or computational ones. Other formalisms aiming to model dependency correctly similarly expand weak generative capacity, notably D-tree Substitution Grammar (Rambow et al, 1995), and consequently end up with much greater parsing complexity. Consider the case of sentences which contain both bridge and raising verbs, noted by Rambow et al (1995). A more recent instance is D-Tree Substitution Grammars (DSG) (Rambow et al, 1995), where the derivations are also interpreted as dependency relations. D-Tree Grammar (DTG) is proposed in (Rambow et al, 1995) to remedy some empirical and theoretical shortcomings of TAG; Tree Description Grammar (TDG) is introduce din (Kallmeyer, 1999) to support syntactic and semantic underspecification and Interaction Grammar is presented in (Perrier, 2000) as an alternative way of formulating linear logic grammars.
Non-Projective Dependency Parsing in Expected Linear Time We present a novel transition system for dependency parsing, which constructs arcs only between adjacent words but can parse arbitrary non-projective trees by swapping the order of words in the input. Adding the swapping operation changes the time complexity for deterministic parsing from linear to quadratic in the worst case, but empirical estimates based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora. Evaluation on data from five languages shows state-of-the-art accuracy, with especially good results for the labeled exact match score. For MaltParser we used the projective Stack algorithm (Nivre, 2009) with default settings and a slightly enriched feature model. To deal with crossing arcs, Titov et al (2009) and Nivre (2009) designed a SWAP transition that switches the position of the two topmost nodes on the stack. Recent works on dependency parsing speedup mainly focus on inference, such as expected linear time non-projective dependency parsing (Nivre, 2009), integer linear programming (ILP) for higher order non-projective parsing (Martinset al, 2009).        Despite this fact, it is possible to perform non-projective parsing in linear time in practice (Nivre, 2009). Note that Nivre (2009) has a similar idea of performing projective and non-projective parsing selectively. 'Nivre' is Nivre's swap algorithm (Nivre, 2009), of which we use the implementation from MaltParser (maltparser.org). In this section, we start by defining a transition system for joint tagging and parsing based on the non-projective transition system proposed in Nivre (2009). Figure 1: Transitions for joint tagging and dependency parsing extending the system of Nivre (2009). Except for the addition of a tag parameter p to the SHIFT transition, this is equivalent to the system described in Nivre (2009), which thanks to the SWAP transition can handle arbitrary non-projective trees. Non-projective parsing algorithms for supervised dependency parsing have, for example, been presented in McDonald et al. (2005) and Nivre (2009).  However, the goal of that transition is different from ours (selecting between projective and non-projective parsing, rather than building some arcs in advance) and the approach is specific to one algorithm while ours is generic for example, the LEFT ARC transition cannot be added to the arc-standard and arc-eager parsers, or to extensions of those like the ones by Attardi (2006) or Nivre (2009), because these already have it. Nivre (2009) introduced a transition based non projective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time.
Distortion Models For Statistical Machine Translation In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation. We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations. We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used. We also propose a novel metric to measure word order similarity (or difference) between any pair of languages based on word alignments.  Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al, 2010). Demonstrating the inadequacy of such approaches, Al-Onaizan and Papineni (2006) showed that even given the words in the reference translation, and their alignment to the source words, a decoder of this sort charged with merely rearranging them into the correct target-language order could achieve a BLEU score (Papineni et al., 2002) of at best 69% and that only when restricted to keep most words very close to their source positions. This is similar to the oracle ordering used by Al-Onaizan and Papineni (2006), but differs in the handling of unaligned words.  Since we are using the distortion model in (Al-Onaizan and Papineni, 2006) the entire last source phrase interval needs to be stored. As mentioned by (Al-Onaizan and Papineni, 2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. Our baseline MT decoder is a phrase-based decoder as described in (Al-Onaizan and Papineni 2006). As pointed out in (Al-Onaizan and Papineni, 2006), these strategies make hard decisions in reordering which cannot be undone during decoding. As mentioned by (Al-Onaizan and Papineni,2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. We then trained the lexicalized reordering model that produced distortion costs based on the number of words that are skipped on the target side, in a manner similar to (Al-Onaizan and Papineni, 2006). Other further generalizations of orientation include the global prediction model (Nagata et al, 2006) and distortion model (Al-Onaizan and Papineni, 2006). The MT system is a phrase based SMT system as described in (Al-Onaizanand Papineni, 2006). This assumption is realistic: while truly parallel data (humanly created) might be in short supply or harder to acquire, adapting statistical machine translation (SMT) systems from one language-pair to another is not as challenging as it used to be (Al-Onaizan and Papineni, 2006). The Chinese to English SMT system has similar architecture to the one described in (Al-Onaizan and Papineni, 2006). To remedy these deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model.  The lexicalized distortion model was used as described in (Al-Onaizan and Papineni, 2006) with a window width of up to 5 and a maximum number of skipped (not covered) words during decoding of 2.  In future work, we plan to extend the parameterization of our models to not only predict phrase orientation, but also the length of each displacement as in (Al-Onaizan and Papineni, 2006).
Characterizing Structural Descriptions Produced By Various Grammatical Formalisms We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate. In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages. The approach that Vijay-Shanker et al (1987) and Weir (1988) take, elaborated on by Becker et al (1992), is to identify a very general class of formalisms, which they call linear context free rewriting systems (CFRSs), and define for this class a large space of structural descriptions which serves as a common ground in which the strong generative capacities of these formalisms can be compared. Here we use the standard definition of LCFRS (Vijay-Shanker et al, 1987) and only fix our notation; for a more thorough discussion of this formalism, we refer to the literature. There are many (structural) mildly context sensitive grammar formalisms, e.g. mcfg, lcfrs, mg, and they have been shown to be equivalent (Vijay-Shanker et al., 1987). They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al, 1987). Following this line, (Vijay-Shanker et al, 1987) have introduced a formalism called linear context-free rewriting systems (LCFRSs) that has received much attention in later years by the community. We briefly summarize here the terminology and notation that we adopt for LCFRS; for detailed definitions, see (Vijay-Shanker et al, 1987). Linear Context-Free Rewriting Systems Gap-restricted dependency languages are closely related to Linear Context-Free Rewriting Systems (lcfrs) (Vijay-Shanker et al, 1987), a class of formal systems that generalizes several mildly context-sensitive grammar formalisms. In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al, 1987). We briefly summarize the terminology and notation that we adopt for LCFRS; for detailed definitions, see Vijay-Shanker et al (1987). LCFRS (Vijay-Shanker et al, 1987) are a natural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals. A LCFRS ( Vijay-Shanker et al, 1987) is a tuple G= (N, T, V, P, S) where N is a finite set of non-terminals with a function dim: N? N that determines the fan-out of each A? N. In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al (1987)). By this result, CCG falls in line with context-free grammars, TAG, and LCFRS, whose sets of derivational structures are all regular (Vijay-Shanker et al., 1987). It is important to note that while CCG derivations themselves can be seen as trees as well, they do not always form regular tree languages (Vijay-Shanker et al, 1987). On this line of investigation, mildly context-sensitive grammar formalisms have been introduced (Joshi,1985), including, among several others, the tree ad joining grammars (TAGs) of Joshi et al (1975). Linear context-free rewriting system (LCFRS), introduced by Vijay-Shanker et al (1987), is a mildly context-sensitive formalism that allows the derivation of tuples of strings, i.e., discontinuous phrases. CFTG are weakly equivalent to the simple macro grammars of Fischer (1968), which are a notational variant of the well-nested linear context-free rewriting systems (LCFRS) of Vijay-Shanker et al (1987) and the well-nested multiple context-free grammars (MCFG) of Seki et al (1991).
A Program For Aligning Sentences In Bilingual Corpora Researchers in both machine translation (e.g., al., and bilingual lexicography (e.g., Klavans and Tzoulcermann, 1990) have recently become interested in studying parallel texts, texts such as the Canadian Hansards (parliamentary proceedings) which are available in multiple languages (French and English). This paper describes a method for aligning sentences in these parallel texts, based on a simple statistical model of character lengths. The method was developed and tested on a small trilingual sample of Swiss economic reports. A much larger sample of 90 million words of Canadian Hansards has been aligned and donated to the ACL/DCI. Gale and Church (1991) and Brown (1991) report the early works using length statistics of bilingual sentences. All sentences are aligned using a tool based on the Gale and Church (1991) algorithm. Brown, Lai and Mercer (Brown et al, 1991) used word count as the sentence length, whereas Gale and Church (Gale and Church, 1991) used character count. Using this search method meant that no prior segmentation of the corpora was needed (Moore, 2002), either in terms of aligned paragraphs (Gale and Church, 1991), or some aligned sentences as anchors (Brown et al, 1991). One reason for this is that there are more of characters than words (Gale and Church, 1991). Alignment is performed using dynamic programming (Gale and Church, 1991) with a weight function based on the number of common words in a sentence pair. In (Gale and Church, 1991) and (Brown et al, 1991), the authors start from the fact that the length of a source text sentence is highly correlated with the length of its target text translation: short sentences tend to have short translations, and long sentences tend to have long translations. Gale and Church (1991) extract pairs of anchor words, such as numbers, proper noduns (organization, person, title), dates, and monetary information. We were motivated by statistical alignment models such as (Gale and Church, 1991) to investigate whether byte-length probabilities could improve or replace the lexical matching based method. Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information. Automatic sentential alignment (Gale and Church, 1991) will be necessary if we have already had on line bilingual texts. Gale and Church (1991) noted that the byte length ratio of target sentence to source sentence is normally distributed. For bilingual texts, Gale and Church (1991) demonstrated the extraordinary effectiveness of a global alignment dynamic programming algorithm, where the basic similarity score was based on the difference in sentence lengths, measured in characters.  We used the precomputed alignments that are provided with the corpus, and which are based on the algorithm by Gale and Church (1991). A bilingual sentence alignment program (Gale and Church, 1991, and Brown et al, 1991) is the crucial part in this adaptation procedure, in that it collects bilingual document pairs from the Inter net, and identifies sentence pairs, which should have a high likelihood of being correct translations of each other. Many parallel corpora have been built, such as the Canadian Hansards (Gale and Church, 1991), the Europarl corpus (Koehn, 2005), the Arabic-English and English-Chinese parallel corpora used in the NIST Open MT Evaluation. For instance, to produce sentence alignments, Brown et al (1991) and Gale and Church (1991) both proposed methods that completely ignored the lexical content of the texts and both reported accuracy levels exceeding 98%. Once the search space is reduced, the system aligns the sentences using the well-known sentence-length model described in (Gale and Church, 1991). The Bayesian prior can be estimated as per Gale and Church (1991) by assuming that it is equal to the frequency of distinct n-m matches in the training set.
Efficient Algorithms For Parsing The DOP Model Excellent results have been reported for Data- Oriented Parsing (DOP) of natural language texts (Bod, 1993c). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model to,a small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree. Using the optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data. We show that Bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers. In Goodman (1996), an efficient parsing strategy is given that maximizes the expected number of correct constituents.  Although Sima'an (1996) and Goodman (1996) also report experiments on unedited ATIS trees, their results do not refer to the most probable parse but to the most probable derivation and the maximum constituents parse respectively. This does not scale well to large treebanks, forcing the use of implicit representations (Goodman, 1996) or heuristic subsets (Bod, 2001). Indeed, our methods were inspired by past work on variational decoding for DOP (Goodman, 1996) and for latent-variable parsing (Matsuzaki et al, 2005). Although see (Goodman 1996) for an efficient algorithm for the DOP model, which we discuss in section 7 of this paper. (Goodman 1996) gives a polynomial time conversion of a DOP model into an equivalent PCFG whose size is linear in the size of the training set.  The relation between DOP and enrichment/conditioning models was clarified by Joshua Goodman, who devised an efficient PCFG transform of the DOP1 model (Goodman, 1996). Although we omit the details, we can prove the NP-hardness by observing that a stochastic tree substitution grammar (STSG) can be represented by a PCFG-LA model in a similar way to one described by Goodman (1996a), and then using the NP-hardness of STSG parsing (Sima'an, 2002). Implicit grammars Goodman (1996, 2003 ) defined a transformation for some versions of DOP to an equivalent PCFG-based model, with the number of rules extracted from each parse tree linear in the size of the trees. Probabilities for the PCFG rules are computed monolingually as in the standard Goodman reduction for DOP (Goodman, 1996). The difference is that Goodman (1996a) collapses our BEGIN and END rules into the binary productions, giving a larger grammar which is less convenient for weighting.   The most probable parse can be estimated by iterative Monte Carlo sampling (Bod 1995), but efficient algorithms exist only for sub-optimal solutions such as the most likely derivation of a sentence (Bod 1995, Simaa'as; an 1995) or the labelled recall parse of a sentence (Goodman 1996).
Unsupervised Word Sense Disambiguation Rivaling Supervised Methods This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints — that words tend to have one sense per discourse and one sense per collocation — exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%. In comparison, (Yarowsky, 1995) achieved 91.4% correct performance, using 1380 contexts and the dictionary definitions in training. Recently, Yarowsky (1995) combined a MIlD and a corpus in a bootstrapping process. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). Yarowsky (1995) successfully used this observation as an approximate annotation technique in an unsupervised WSD model. This heuristic naturally reflects the broadly known assumption about lexical ambiguity presented in (Yarowsky, 1995), namely the one-sense-per-discourse heuristic. This heuristic mimes the one-sense-per collocation heuristic presented in (Yarowsky, 1995). These include the bootstrapping approach [Yarowsky 1995] and the context clustering approach [Schutze 1998]. Yarowsky (1995) presented an approach that significantly reduces the amount of labeled data needed forword sense disambiguation. Many of these tasks have been addressed in other fields, for example, hypothesis verification in the field of machine translation (Tran et al, 1996), sense disambiguation in speech synthesis (Yarowsky, 1995), and relation tagging in information retrieval (Marsh and Perzanowski, 1999). Yarowsky (1995) used both supervised and unsupervised WSD for correct phonetizitation of words in speech synthesis. The idea of sense consistency was first introduced and extended to operate across related documents by (Yarowsky, 1995). This method, initially proposed by (Yarowsky, 1995), was successfully evaluated in the context of the SENSEVAL framework (Mihalcea, 2002). See Yarowsky (1995) for details. The well-known observation that words rarely exhibit more than one sense per discourse (Yarowsky, 1995) implies that features closely associated with a particular sense have a low probability of appearing in the same document as features associated with another sense. Yarowsky (1995) first recognized that it is possible to use a small number of features for different senses to bootstrap an unsupervised word sense disambiguation system. Self-training (Yarowsky, 1995) is a semi supervised algorithm which has been well studied in the NLP area and gained promising result. The algorithm proposed by Yarowsky (1995) for the problem of word sense disambiguation has been cited as the origination of self-training. For the fine-grained track, it achieves 2nd place after that of Tugwell and Kilgarriff (2001), which used a decision list (Yarowsky, 1995) on manually selected corpora evidence for each inventory sense, and thus is not subject to loss of distinguishability in the glosses as Lesk variants are. Two more recent investigations are by Yarowsky, (Yarowsky, 1995), and later, Mihalcea, (Mihalcea,2002). Self-training (Yarowsky, 1995) is a form of semi-supervised learning.
Ranking Algorithms For Named Entity Extraction: Boosting And The Voted Perceptron This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the application being the recovery of named-entity boundaries in a corpus of web data. The first approach uses a boosting algorithm for ranking problems. The second approach uses the voted perceptron algorithm. Both algorithms give comparable, significant improvements over the maximum-entropy baseline. The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples. We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged (the full feature set is described in (Collins 2002a)). For related work on the voted perceptron algorithm applied to NLP problems, see (Collins 2002a) and (Collins 2002b). (Collins 2002a) describes experiments on the same named-entity dataset as in this paper, but using explicit features rather than kernels.   The true segmentation can now be compared with the N-best list in order to train an averaged perceptron algorithm (Collins, 2002a). However, due to the computational issues with the voted perceptron, the averaged perceptron algorithm (Collins, 2002a) is used instead. To reduce the time complexity, we adapted the lazy update proposed in (Collins, 2002b), which was also used in (Zhang and Clark, 2007). For all objectives, we use the same standard set of feature templates, following Kazama and Torisawa (2007) with additional token shape like those in Collins (2002b) and simple gazetteer features.  This approach has been used earlier by (Collins, 2002). This result is used to explain the convergence of weighted or voted perceptron algorithms (Collins, 2002a). The detailed algorithm can be found in (Collins, 2002). Collins (2002) augmented a baseline NE tagger with a re-ranker that used only local, NE-oriented features.   Collins (2002) includes a number of interesting contextual predicates for NER. Collins (2002) also describes a mapping from words to word types which groups words with similar orthographic forms into classes. Using a wider context window than 2 words may improve performance; a reranking phase using global features may also improve performance (Collins, 2002). Each shape replaces characters by their types (case sensitive letters, digits, and punctuation), and deletes repeated types - e.g., Confidence and 2,664,098 are respectively mapped to Aa and 0,0+,0+ (Collins, 2002b).
Generalized Probabilistic LR Parsing Of Natural Language (Corpora) With Unification-Based Grammars The first issue to consider is what the analysis will be used for and what constraints this places on its form. The corpus analysis literature contains a variety of proposals, ranging from part-of-speech tagging to assignment of a unique, sophisticated syntactic analysis. Our eventual goal is to recover a semantically and pragmatically appropriate syntactic analysis capable of supporting semantic interpretation. Two stringent requirements follow immediately: firstly, the analyses assigned must determinately represent the syntactic relations that hold between all constituents in the input; secondly, they be drawn from an priori well-formed set of possible syntactic analyses (such as the set defined by a generative grammar). Otherwise, semantic interpretation of the resultant analyses cannot be guaranteed to be (structurally) unambiguous, and the semantic operations defined (over syntactic configurations) cannot be guaranteed to match and yield an interpretation. These requirements immediately suggest that approaches that recover only lexical tags (e.g. de Rose 1988) or a syntactic analysis that is the 'closest fit' to some previously defined set of possible analyses (e.g. Sampson, Haigh, and Atwell 1989), are inadequate (taken alone). Pioneering approaches to corpus analysis proceeded on the assumption that computationally tractable generative grammars of sufficiently general coverage could not be developed (see, for example, papers in Garside, Leech, and Sampson 1987). However, the development of wide-coverage declarative and computationally tractable grammars makes this assumption questionable. For example, the ANLT word and sentence grammar (Grover et al. 1989; Carroll and Grover 1989) consists of an English lexicon of approximately 40,000 lexemes and a 'compiled' fixed-arity term unification grammar containing around 700 phrase structure rules. Taylor, Grover, and Briscoe (1989) demonstrate that an earlier version of this grammar was capable of assigning the correct analysis to 96.8% of a corpus of 10,000 noun phrases extracted (without regard for their internal form) from a variety of corpora. However, although Taylor, and show that the ANLT grammar very wide coverage, they abstract away from issues of lexical idiosyncrasy by formimg equivalence classes of noun phrases and parsing a single token of each class, and they do not address the issues of 1) tuning a grammar to a particular corpus or sublanguage 2) selecting the correct analysis from the set licensed by the grammar and 3) providing reliable analyses of input outside the coverage of the grammar. Firstly, it is clear that vocabulary, idiom, and conventionalized constructions used in, say, legal language and dictionary definitions, will differ both in terms of the range and frequency of words and constructions deployed. Secondly, Church and Patil (1982) demonstrate that for a realistic grammar parsing realistic input, the set of possible analyses licensed by the grammar can be in the thousands. Finally, it is extremely unlikely that any generative grammar will ever be capable of correctly analyzing all naturally occurring input, even when tuned for a particular corpus or sublanguage (if only because of the synchronic idealization implicit in the assumption that the set of grammatical sentences of a language is well formed.) this paper, we describe our to the first and second problems and make some preliminary remarks concerning the third (far harder) problem. Our apto grammar tuning is based on a semi-automatic parsing phase which additions to the grammar are made manually and statistical information concerning the frequency of use of grammar rules is acquired. Using this statistical information and modified grammar, a breadth-first probabilistic parser is constructed. The latter is capable of ranking the possible parses identified by the grammar in a useful (and efficient) manner. However, (unseen) sentences whose correct analysis is outside the coverage of the grammar reri:ain a problem. The feasibility and usefulness of our approach has been investigated in a preliminary way by analyzing a small corpus of 26 Ted Briscoe and John Carroll Generalized Probabilistic LR Parsing definitions drawn from the Dictionary of Contemporary English (Procter 1978). This corpus was chosen because the vocabulary employed is restricted (to approximately 2,000 morphemes), average definition length is about 10 words (with a maximum of around 30), and each definition is independent, allowing us to ignore phenomena such as ellipsis. In addition, the language of definitions represents a recognizable sublanguage, allowing us to explore the task of tuning a general purpose grammar. The results reported below suggest that probabilistic information concerning the frequency of occurrence of syntactic rules correlates in a useful (though not absolute) way with the semantically and pragmatically most plausible analysis. In Section 2, we briefly review extant work on probabilistic approaches to corpus analysis and parsing and argue the need for a more refined probabilistic model to distinguish distinct derivations. Section 3 discusses work on LR parsing of natural language and presents our technique for automatic construction of LR parsers for unification-based grammars. Section 4 presents the method and results for constructing a LALR(1) parse table for the ANLT grammar and discusses these in the light of both computational complexity and other empirical results concerning parse table size and construction time. Section 5 motivates our interactive and incremental approach to semi-automatic production of a disambiguated training corpus and describes the variant of the LR parser used for this task. Section 6 describes our implementation of a breadth-first LR parser and compares its performance empirically to a highly optimized chart parser for the same grammar, suggesting that (optimized) LR parsing is more efficient in practice for the ANLT grammar despite exponential worst case complexity results. Section 7 explains the technique we employ for deriving a probabilistic version of the LR parse table from the training corpus, and demonstrates that this leads to a more refined and parse-context—dependent probabilistic model capable of distinguishing derivations that in a probabilistic context-free model would be equally probable. Section 8 describes and presents the results of our first experiment parsing LDOCE noun definitions, and Section 9 draws some preliminary conclusions and outlines ways in which the work described should be modified and extended. 2. Probabilistic Approaches to Parsing In the field of speech recognition, statistical techniques based on hidden Markov mod Briscoe and Carroll (1993) observed that half of the parse failures were caused by inaccurate sub categorization information in the lexicon. If, instead, this procedure returns a list of several possible actions with corresponding probabilities, we can then parse with a model similar to the probabilistic LR models described by Briscoe and Carroll (1993), where the probability of a parse tree is the product of the probabilities of each of the actions taken in its derivation.  work by Briscoe and Carroll (1993) on statistical parsing uses an adapted version of the system which is able to process tagged input, ignoring the words in order to parse sequences of tags. Finally, we observe that there are also trainable stochastic shift-reduce parser models (Briscoe and Carroll, 1993), which are theoretically related to shift-reduce parsing, but operate in a highly non deterministic fashion during parsing. Following (Briscoe and Carroll, 1993), conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration. The probability function can be obtained on the basis of a treebank, as proposed by (Briscoe and Carroll, 1993). The model by (Briscoe and Carroll, 1993) however incorporated a mistake involving lookahead, which was corrected by (Inui et al, 2000). One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al, 2000) is that trained probabilistic LR parsers should be proper. There have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., Chodorow et al 1985), specially constructed definition parsers (e.g., Wilks et al 1996, Vossen 1995), and even general coverage syntactic parsers (e.g., Briscoe and Carroll 1993). Our second method of acquiring verb grammatical relations uses the statistical parser developed by Briscoe and Carroll (1993, 1997) which is an extension of the ANLT grammar development system which we used for our deep grammatical analysis as reported in Section 3 above.
Randomized Algorithms And NLP: Using Locality Sensitive Hash Functions For High Speed Noun Clustering In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data. We apply these algorithms to generate noun similarity lists from 70 million pages. We reduce the running time from quadratic to practically linear in the number of elements to be computed. NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) downstream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al, 2005), lexical variants mining (Gouws et al, 2011), and large-scale first story detection (Petrovic et al, 2010).  Ravichandran et al (2005) have shown that by using the LSH nearest neighbors calculation can be done in O (nd) time. III and Marcu (2005), who use word class features derived from a Web-scale corpus via a process described in Ravichandran et al (2005). Our classifier for (m, n)-cousins is derived from the algorithm and corpus given in (Ravichandran et al, 2005).  Ravichandran et al (2005) used this cosine variant and showed it to produce over 70% accuracy in extracting synonyms when compared against Pantel and Lin (2002). It was used by Ravichandran et al (2005) to improve the efficiency of distributional similarity calculations. The frequency statistics were weighted using mutual information, as in Ravichandran et al (2005). When the cut-off was increased to 100, as used by Ravichandran et al (2005), the results improved significantly. We used randomized algorithms (Ravichandran et al, 2005) to build the semantic space efficiently. This scheme was used, e.g., for creating similarity lists of nouns collected from a web corpus in Ravichandran et al (2005). This baseline system follows the design of previous work (Ravichandran et al, 2005). We followed the notation of the original paper (Ravichandran et al, 2005) here. Ravichandran et al (2005) applied LSH to the task of noun clustering. However, Ravichandran et al (2005) approach stored an enormous matrix of all unique words and their contexts in main memory, which is infeasible for very large data sets. In practice p is generally large, Ravichandran et al (2005) used p= 1000 in their work. Data sets: We use two data sets: Gigaword (Graff, 2003) and a copy of news web (Ravichandran et al., 2005). We set the number of projections k= 3000 for all three methods and for PLEB and FAST-PLEB, we set number of permutations p= 1000 as used in large-scale noun clustering work (Ravichandran et al 2005). Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies.
An Algebra For Semantic Construction In Constraint-Based Grammars We develop a framework for formalizing semantic construction within grammars expressed in typed feature struclogics, including The approach provides an alternative to the lambda calculus; it maintains much of the desirable flexibility of unificationbased approaches to composition, while constraining the allowable operations in order to capture basic generalizations and improve maintainability. Given the fine-grained syntactic and semantic analysis of the HPSG grammar and its robustness (through SNLP integration), we decided to use the semantic representation (MRS, see (Copestake et al, 2001)) as additional input for IE. The semantic representations used are flat semantic representations in the sense of [Copestake et al, 2001] and the semantic parameters (that is, the semantic indices representing the missing arguments of the semantic functors) are represented by unification variables. Ge and Mooney (2009) extracts semantic representations using syntactic structures while Copestake et al (2001) develops algebras for semantic construction within grammars. Recent work in semantic construction for HPSG (Copestake et al., 2001) supports our conjecture: the examples discussed there are compatible with our simplification. The original Grammar Matrix consisted of types defining the basic feature geometry, types associated with Minimal Recursion Semantics (e.g., (Copestake et al, 2001)), types for lexical and syntactic rules, and configuration files for the LKB grammar development environment (Copestake, 2002) and the PET system (Callmeier,2000). The Redwoods tree bank provides deeper semantics expressed in the Minimum Recursion Semantics formalism (Copestake et al, 2001), but in the present experiments we have not explored this fully. In future work we will compare our semantics construction principles to the general model of Copestake et al (2001). The semantic interpretations are expressed using Minimal Recursion Semantics (MRS) (Copestake et al, 2001), which provides the means to represent interpretations with a flat, underspecified semantics using terms of the predicate calculus and generalized quantifiers. The grammar is couched in the theoretical framework of Head-Driven Phrase Structure Grammar (HPSG) (Pollard; Sag 1994), with semantic representations in Minimal Recursion Semantics (MRS) (Copestake et al 2001). Our method effectively performs automatic RMRS semantics construction from functional dependencies, following the semantic algebra of Copestake et al (2001). We show how to adapt the construction principles of the semantic algebra of Copestake et al (2001) to RMRS construction from dependencies in a rewrite scenario, and discuss the treatment of some special phenomena, such as verbal complementation, coordination and modification. For (R) MRS construction from dependencies we follow the algebra for semantics composition in Copestake et al (2001). Copestake et al (2001) mention a third feature to be included in the hook as an externally visible variable, which they instantiate with the index of the controlled subject in equi constructions and which is also used to implement the semantics of predicative modification. Otherwise, composition strictly follows the semantic operations of the algebra of Copestake et al (2001): the composition rules only refer to the hook and slots of functors and arguments, to achieve the binding of argument variables and the encoding of scope constraints. The algebra of Copestake et al (2001) defines modifiers to externalise the variable of the ARG1. In future research, we will investigate how the semantic algebra of Copestake et al (2001) compares to Glue Semantics (Dalrymple, 1999). The coincidence with Copestake et al.'s terminology (Copestake et al, 2001) is not casual; in fact, our formulation can be regarded as a decoupled fragment of theirs, since neither our holes involves syntactic labels nor are scopal issues ever touched. We use this parser to parse the defining sentences into a full meaning representation using minimal recursion semantics (MRS: Copestake et al (2001)). The output is written in Minimal Recursion Semantics (Copestake et al, 2001). An MRS consists of a bag of labeled elementary predicates and their arguments, a list of scoping constraints, and a pair of relations that provide a hook into the representation - a label, which must outscope all the handles, and an index (Copestake et al, 2001).
Explo i t ing a Probabi l ist ic  Hierarchical Mode l  for Generat ion Srinivas Bangalore and Owen Rambow AT&T Labs Research 180 Park Avenue F lorham Park, NJ 07932 {sr in?,  rambow}@research,  a r t .  FERGUS (Bangalore and Rambow,2000) took dependency structures as inputs, and produced XTAG derivations by a stochastic tree model automatically acquired from an annotated corpus. However, we can automatically estimate English word order by using a language model or an English surface sentence generator such as FERGUS (Bangalore and Rambow, 2000). More generally, the NLG problem of non-deterministic decision making has been addressed from many different angles, including PENMAN-style choosers (Mann and Matthiessen,1983), corpus-based statistical knowledge (Langkilde and Knight, 1998), tree-based stochastic models (Bangalore and Rambow, 2000), maximum entropy based ranking (Ratnaparkhi, 2000), combinatorial pattern discovery (Duboue and McKeown, 2001), instance-based ranking (Varges, 2003), chart generation (White, 2004), planning (Koller and Stone, 2007), or probabilistic generation spaces (Belz, 2008) to name just a few. The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees. Fergus (Bangalore and Rambow, 2000) used the Penn TreeBank as a corpus, requiring a more substantial transformation algorithm since it requires a lexical predicate-argument structure instead of the TreeBank's representation. Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimotoetal., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. We extend the work of (Walker et al., 2001) and (Bangalore and Rambow, 2000) in various ways. WordNet has been used by many researchers for different purposes ranging from the construction or extension of knowledge bases such as SENSUS (Knight and Luk, 1994) or the Lexical Conceptual Structure Verb Database (LVD) (Green et al, 2001) to the faking of meaning ambiguity as part of system evaluation (Bangalore and Rambow, 2000). These concepts are then realized into words resulting in a bag of words with syntactic relations (Bangalore and Rambow, 2000). We have introduced a novel type of supertagger, which we have dubbed a hypertagger, that assigns CCG category labels to elementary predications in a structured semantic representation with high accuracy at several levels of tagging ambiguity in a fashion reminiscent of (Bangalore and Rambow, 2000). Bangalore and Rambow proposed a method to generate candidate-text sentences in the form of trees (Bangalore and Rambow, 2000).  In recent years, there has been a steady stream of research in statistical text generation (see Langkilde and Knight (1998), and Bangalore and Rambow (2000)).  In other words, in generating a form f to express an input, one wants to maximize the probability of the form, P (f), with respect to some gold-standard corpus, and thus express the in put in a way that resembles the realizations in the corpus most closely (Bangalore and Rambow, 2000). FERGUS (Bangalore and Rambow, 2000), on the other hand, employs a model of syntactic structure during sentence realization. Both the model of Amalgam and that presented here differ considerably from the n-gram models of Langkilde and Knight (1998), the TAG models of Bangalore and Rambow (2000), and the stochastic generation from semantic representation approach of Soricutand Marcu (2006). Bangalore and Rambow (2000) use n-gram word sequence statistics in a TAG-based generation model to rank output strings and additional statistical and symbolic resources at intermediate generation stages. 
Knowledge-Free Induction Of Morphology Using Latent Semantic Analysis Morphology induction is a subproblem of important tasks like automatic learning of machine-readable dictionaries and grammar induction. Previous morphology induction approaches have relied solely on statistics of hypothesized stems and affixes to choose which affixes to consider legitimate. Relying on stemand-affix statistics rather than semantic knowledge leads to a number of problems, such as the inappropriate use of valid affixes (&quot;ally&quot; stemming to &quot;all&quot;). We introduce a semantic-based algorithm for learning morphology which only proposes affixes when the stem and stem-plusaffix are sufficiently similar semantically. We implement our approach using Latent Semantic Analysis and show that our semantics-only approach provides morphology induction results that rival a current state-of-the-art system. Our algorithm extends earlier approaches to morphology induction by combining various induced information sources: the semantic relatedness of the affixed forms usinga Latent Semantic Analysis approach to corpus based semantics (Schone and Jurafsky, 2000), affix frequency, syntactic context, and transitive closure. Most of the existing algorithms described focus on approach falls into this category (expanding upon suffixing in inflectional languages (though our earlier approach (Schone and Jurafsky, 2000)), Jacquemin and Jean describe work on prefixes). In our earlier work, we (Schone and Jurafsky (2000)) generated a list of N candidate suffixes and used this list to identify word pairs which share the same stem but conclude with distinct candidate suffixes. We had noted previously (Schone and Jurafsky, 2000), however, that errors can arise from strictly orthographic systems. As in our earlier approach (Schone and Jurafsky, 2000), we begin by generating, from an untagged corpus, a list of word pairs that might be morphological variants. Using this final lexicon, we can now seek for suffixes in a manner equivalent to what we had done before (Schone and Jurafsky, 2000). In order to obtain semantic representations of each word, we apply our previous strategy (Schone and Jurafsky (2000)). To correlate these semantic vectors, we use normalized cosine scores (NCSs) as we had illustrated before (Schone and Jurafsky (2000)). We compare this improved algorithm to our former algorithm (Schone and Jurafsky (2000)) as well as to Goldsmith's Linguistica (2000). They are either based solely on corpus statistics (Djean, 1998), measure semantic similarity between input and output lemma (Schone and Jurafsky, 2000), or bootstrap derivation rules starting from seed examples (Piasecki et al, 2012). Using latent semantic analysis, Schone and Jurafsky (2000) have previously demonstrated the success of using semantic information in morphological analysis. Many researchers, including Schone and Jurafsky (2000), Harris (1958), and Djean (1998), suggest looking for nodes with high branching (out-degree) or a large number of continuations. In a different approach, Schone and Jurafsky (2000) utilize the context of each term to obtain a semantic representation for it using LSA. The idea thata stem and stem+affix should be semantically similar has been exploited previously for morphological analysis (Schone and Jurafsky, 2000). Next along the spectrum of orthographic similarity bias is the work of Schone and Jurafsky (2000), who first acquire a list of pairs of potential morphological variants (PPMVs) using an or tho graphic similarity technique due to Gaussier (1999), in which pairs of words from a corpus vocabulary with the same initial string are identified. To reduce the running time of the model we limit the space of considered morpheme boundaries as follows: Given the target side of the corpus, we derive a list of K most frequent prefixes and suffixes using a simple trie-based method proposed by (Schone and Jurafsky, 2000). Following Schone and Jurafsky (2000), clusters are evaluated for whether they capture inflectional paradigms using CELEX (Baayen et al, 1993). Schone and Jurafsky (2000) attempts to cluster morphologically related words starting with an unrefined trie search (but with a parameter of minimum possible stem length and an upper bound on potential affix candidates) that is constrained by semantic similarity in a word context vector space. Schone and Jurafsky (2000) give definitions for correct (C), inserted (I), and deleted (D) words in model-derived conflation sets in relation to a gold standard. 
A Global Joint Model for Semantic Role Labeling We present a model for semantic role labeling that effectively captures the linguistic intuition that a semantic argument frame is a joint structure, with strong dependencies among the arguments. We show how to incorporate these strong dependencies in a statistical joint model with a rich set of features over multiple argument phrases. The proposed model substantially outperforms a similar state-of-the-art local model that does not include dependencies among different arguments. We evaluate the gains from incorporating this joint information on the Propbank corpus, when using correct syntactic parse trees as input, and when using automatically derived parse The gains amount to reduction on all arguments and core arguments for gold-standard parse trees on Propbank. For automatic parse trees, the error reductions are all and core arguments, respectively. We also present results on the CoNLL 2005 shared task data set. Additionally, we explore considering multiple syntactic analyses to cope with parser noise and uncertainty. Toutanova et al (2008) presented a re-ranking model to jointly learn the semantic roles of multiple constituents in the SRL task. We also compare with the multi parse system of (Toutanova et al, 2008) which uses a global joint model using multiple parse trees. Consider the following examples, due to Toutanova et al (2008): (3) [Temporal The day] that [arg0 the ogre] [Predicate cooked] [arg1 the children] is still remembered. The complex SRL architectures proposed (usually combining local and global, i.e. joint, models of argument classification, e.g. (Toutanova et al., 2008)) require a large number of annotated examples. Most of the CoNLL 2005 systems show a significant performance drop when the tested corpus, i.e. Brown, differs from the training one (i.e. Wall Street Journal), e.g. (Toutanova et al, 2008). Learning from richer linguistic descriptions of more complex structures is proposed in (Toutanova et al, 2008). In (Toutanova et al, 2008) a SRL model over Propbank that effectively exploits the semantic argument frame as a joint structure, is presented. This approach effectively introduces a new step in SRL, also called Joint Re-ranking, (RR), e.g. (Toutanova et al., 2008) or (Moschitti et al., 2008). Toutanova et al (2008), Johansson and Nugues (2008), and Bjorkelund et al (2009) presented importance of capturing non-local dependencies of core arguments in predicate-argument structure analysis. While there are a number of existing tools for performing these tasks based on the linguistic context (e.g., Toutanova et al, 2008, Erk and Pado, 2006), their performance is only moderate (e.g., Agirre et al 2007). Supervised SRL systems have mostly used local classifiers that assign a role to each constituent independently of others, and only modeled limited correlations among roles in a sequence (Toutanova et al., 2008). Similar to Toutanova et al (2008), we propose to use global role ordering preferences but in a generative model in contrast to their discriminative one. Toutanova et al (2008) currently have the best performing SRL system on the Brown corpus test set with an F1 score of 68.81 (80.8 for the WSJtest). Indeed, when SRL systems use gold standard parses, they tend to perform extremely well (Toutanova et al, 2008).   These approaches have been shown to be successful for tasks such as parsing and named entity recognition in newswire data (Finkel and Manning, 2009) or semantic role labeling in the Penn Treebank and Brown corpus (Toutanova et al,2008). Toutanova et al (2008) and Punyakanok et al (2008) presented a re-ranking model and an integer linear programming model respectively to jointly learn a global optimal semantic roles assignment. Reranking has become a popular technique for solving various structured prediction tasks, such as phrase-structure (Collins, 2000) and dependency parsing (Hall, 2007), semantic role labeling (Toutanova et al 2008) and machine translation (Shen et al 2004). For instance, the confusion matrix in (Toutanova et al, 2008) indicates that their model scores 99.5% accuracy on this task.
Soft Syntactic Constraints For Word Alignment Through Discriminative Training Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree. However, this hard constraint can also rule out correct alignments, and its utility decreases as alignment models become more complex. We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint. The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser. (Cherry and Lin, 2006) used dependency structures as soft constraints to improve word alignment in an ITG framework. Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block models (Haghighi et al, 2009). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. The first two rows repeat the experiments of Taskar et al (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. (Cherry and Lin, 2006) modify an ITG aligner by introducing a penalty for induced parses that violate syntactic bracketing constraints. The first is to relax or update the independence assumptions based on more information, usually syntactic, from the language pairs (Cherry and Lin, 2006). Fox (2002) showed that cohesion is held in the vast majority of cases for English-French, while Cherry and Lin (2006) have shown it to be a strong feature for word alignment. Fox (2002) demonstrated and counted cases where cohesion was not maintained in hand aligned sentence-pairs, while Cherry and Lin (2006) showed that a soft cohesion constraint is superior to a hard constraint for word alignment. One can further refine existing word alignment models with syntactic constraints (e.g. (Cherry and Lin, 2006)). The syntactic constraints are specifically imposed on the n words involved in 1-to-n alignments, which is different from the cohesion constraints (Fox, 2002) as explored by Cherry and Lin (2006), where knowledge of cross-lingual syntactic projection is used.
Improved Alignment Models For Statistical Machine Translation PP — 31.5 In all experiments, we use the following three error criteria: • WER (word error rate): The WER is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated string into the target string. This performance criterion is widely used in speech recognition. • PER (position-independent word error rate): A shortcoming of the WER is the fact that it requires a perfect word order. This is 26 2: for Text and Speech Input: error rate (WER), positionindependent word error rate (PER) and subjective sentence error rate (SSER) with/without preprocessing (147 sentences = 1 968 words of the Verbmobil task). Input Preproc. WER[%] PER[Vo] SSER[%] Single-Word Based Approach Text No 53.4 38.3 35.7 Yes 56.0 41.2 35.3 Speech No 67.8 50.1 54.8 Yes 67.8 51.4 52.7 Alignnient Templates Text No 49.5 35.3 31.5 Yes 48.3 35.1 27.2 Speech No 63.5 45.6 52.4 Yes 62.8 45.6 50.3 particularly a problem for the Verbmobil task, where the word order of the German- English sentence pair can be quite different. As a result, the word order of the automatically generated target sentence can be different from that of the target sentence, but nevertheless acceptable so that the WER measure alone could be misleading. In order to overcome this problem, we introduce as additional measure the positionindependent word error rate (PER). This measure compares the words in the two senthe word order into account. Words that have no matching counterparts are counted as substitution errors. Depending on whether the translated sentence is longer or shorter than the target translation, the remaining words result in either insertion or deletion errors in addition to substitution errors. The PER is guaranteed to be less than or equal to the WER. • SSER (subjective sentence error rate): For a more detailed analysis, subjective judgments by test persons are necessary. Each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0. A score of 0.0 means that the translation is semantically and syntactically correct, a score of 0.5 means that a sentence is semantically correct but syntactically wrong and a score of 1.0 means that the sentence is semantically wrong. The human examiner was offered the translated sentences of the two approaches at the same As a result we expect a better possibility of reproduction. The results of the translation experiments using the single-word based approach and the alignment template approach on text input and on speech input are summarized in Table 2. The results are shown with and without the use of domain-specific preprocessing. The alignment template approach produces better translation results than the single-word based approach. From this we draw the conclusion that it is important to model word groups in source and target language. Considering the recognition word error rate of 31% the degradation of about 20% by speech input can be expected. The average translation time on an Alpha workstation for a single sentence is about one second for the alignment template appreach and 30 seconds for the single-word based search procedure. Within the Verbmobil project other translation modules based on rule-based, examplebased and dialogue-act-based translation are used. We are not able to present results with these methods using our test corpus. But in the current Verbmobil prototype the preliminary evaluations show that the statistical methods produce comparable or better results than other systems. An advantage of the systhat it is robust and always produces a translation result even if the input of the speech recognizer is quite incorrect. 5 Summary We have described two approaches to perform statistical machine translation which extend the baseline alignment models. The single-word 27 based approach allows for the the possibility of one-to-many alignments. The alignment template approach uses two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words. As a result the context of words has a greater influence and the changes in word order from source to target language can be learned explicitly. An advantage of both methods is that they learn fully automatically by using a bilingual training corpus and are capable of achieving better translation results on a limited-domain task than other example-based or rule-based translation systems. Acknowledgment This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project by the by the European Community (ESPRIT project number 30268). To obtain the best single alignment, it is common practice to use a post-hoc algorithm to merge these directional alignments (Och et al, 1999). The most common techniques for bidirectional alignment are post-hoc combinations, such as union or intersection, of directional models, (Och et al, 1999), or more complex heuristic combiners such as grow-diag-final (Koehn et al, 2003). We view our use of part-of-speech patterns as a natural extension to the introduction of structural elements to statistical machine translation by Wang [1998] and Och et al [1999]. The translation models and lexical scores were estimated on the training corpus which was automatically aligned using Giza++ (Och et al, 1999) in both directions between source and target and symmetrised using the growing heuristic (Koehn et al, 2003). Zens et al (2004) introduce a left-to-right decoding algorithm with ITG constraints on the alignment template system (Och et al, 1999). A significant source of errors in statistical machine translation is the word reordering problem (Och et al, 1999). The blocks are simpler than the alignment templates in (Och et al, 1999) in that they do not have any internal structure. We take the intersection of the two alignments as described in (Och et al, 1999). A similar block selection scheme has been presented in (Och et al, 1999). We use another technique to speed up direct search by storing and re-using search graphs, which consist of lattices in the case of phrase-based decoding (Och et al, 1999) and hypergraphs in the case of hierarchical decoding (Chiang, 2005). The phrase extraction heuristic then extracts all the bi phrases that are compatible with the word alignment (Och et al, 1999). Second, using a heuristic proposed in (Och et al, 1999), all the aligned phrase pairs (x?, a?, y?) satisfying the following criteria are extracted: (1) x? and y? consist of consecutive words of x and y, and both have length at most k, (2) a? is the alignment between words of x? and y? induced by a, (3) a? contains at least one link, and (4) there are no links in a that have just one end in x? or y?.  Only phrases that conform to the so-called consistent alignment restrictions (Och et al, 1999) are extracted. Och et al (1999) proposed a translation template approach that computes phrasal mappings from the viterbi alignments of a training corpus. Re-ordering effects across languages have been modeled in several ways, including word-based (Brown et al, 1993), template-based (Och et al, 1999) and syntax-based (Yamada, Knight, 2001). To implement our phrase extraction technique, the maximum approximation alignments were combined with the union operation as described in (Och et al, 1999), resulting in a dense but inaccurate alignment map as measured against a human aligned gold standard. This method of phrase pair extraction was originally described by Och et al (1999). Once an alignment is obtained, phrases which satisfy the inverse projection constraint are extracted (although earlier this constraint was called consistent alignments (Och et al, 1999)). The most common method for obtaining the phrase table is heuristic extraction from automatically word-aligned bilingual training data (Och et al, 1999).
The Necessity Of Parsing For Predicate Argument Recognition Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time. Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text. In this paper, we quantify the effect of parser accuracy on these systems' performance, and examine the question of whether a flatter &quot;chunked&quot; representation of the input can be as effective for the purposes of semantic role identification. In this paper, we find that the use of deep linguistic representations to predict these semantic labels are more effective than the generally more surface-syntax representations previously employed (Gildea and Palmer (2002)). Gildea and Palmer (2002) show that semantic role labels can be predicted given syntactic features derived from the PTB with fairly high accuracy. For example, in their inclusion of voice, Gildea and Palmer (2002) note that this deep syntax feature plays an important role in connecting semantic role with surface grammatical function. We first experiment with the set of features described in Gildea and Palmer (2002): Pred HW, Arg HW, Phrase Type, Position, Path, Voice. The error rate, 10.0%, is lower than that reported by Gildea and Palmer (2002), 17.2%. Note also that the transformations which are taken into account are a superset of the transformations taken into account by Gildea and Palmer (2002). These results are comparable to the results from Gildea and Palmer (2002), but only roughly because of differences in corpora. Gildea and Palmer (2002) achieve a recall of 0.50, a precision of 0.58, and an F-measure of 0.54 when using the full parser of Collins (1999). For example, much work has shown the usefulness of syntactic representations for subsequent tasks such as relation extraction, semantic role labeling (Gildea and Palmer, 2002) and paraphrase detection (CallisonBurch, 2008). In the last few years, many researchers (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002), (Pradhan et al. 2003) have focused on the automatic prediction of semantic roles using statistical techniques.  The first experiment compares the random forest classifier to three other classifiers, a statistical Bayesian approach with back off (Gildea and Palmer, 2002), a decision tree classifier (Surdeanu et al, 2003), and a Support Vector Machine (SVM) (Pradhan et al, 2003).  Gildea and Palmer (2002) report F-score results in the 55% range for argument and boundary recognition based on automatic parses. For example, the tree-path feature has been shown to be valuable in semantic role labeling (Gildea and Palmer, 2002). The Gildea and Palmer (2002) system uses the same features and the same classification mechanism used by G&J.  the accuracy on FrameNet (85.2%) is higher than the best result obtained in literature, i.e. 82.0% in (Gildea and Palmer, 2002). More recent representative efforts includes that of Gildea and Jurafsky (2002), Gildea and Palmer (2002), and Punyakanok et al (2008). This is in line with results in (Gildea and Palmer, 2002), who compare the effect of manual and automatic parsing on semantic predicate argument recognition.
Evaluation Metrics For Generation Certain generation applications may profit from the use of stochastic methods. In developing stochastic methods, it is crucial to be able to quickly assess the relative merits of different approaches or models. In this paper, we present several types of intrinsic (system internal) metrics which we have used for baseline quantitative assessment. This quantitative assessment should then be augmented to a fuller evaluation that examines qualitative aspects. To this end, we describe an experiment that tests correlation between the quantitative metrics and human qualitative judgment. The experiment confirms that intrinsic metrics cannot replace human evaluation, but some correlate significantly with human judgments of quality and understandability and can be used for evaluation during development. Su et al (1992), Alshawi et al (1998) and Bangalore et al (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. More on these and other metrics can be found in [Bangalore et al, 2000]. Common evaluation techniques for NLG systems [Mellish and Dale, 1998] include: Showing generated texts to users, and measuring how effective they are at achieving their goal, compared to some control text (for example, [Young, 1999]); Asking experts to rate computer-generated texts in various ways, and comparing this to their rating of manually authored texts (for example, [Lester and Porter, 1997]); Automatically comparing generated texts to a corpus of human authored texts (for example, [Bangalore et al 2000]). The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al (2000). Bangalore et al describe some of the quantitative measures that have been used in (Bangalore et al, 2000). In contrast, quantitative metrics for automatic evaluation of surface realisers have been developed (Bangalore et al, 2000) and they have been shown to correlate well with human judgement for quality and understandability. Simple String Accuracy (SSA, Bangalore et al 2000) has been proposed as a baseline evaluation metric for natural language generation. Kay (1996) identified parsing charts as such an architecture, which led to the development of various chart generation algorithms: Carroll et al (1999) for HPSG, Bangalore et al (2000) for LTAG, Moore (2002) for unification grammars, White and Baldridge (2003) for CCG. Similarly, the metrics proposed for text generation by (Bangalore et al, 2000) (simple accuracy, generation accuracy) are based on string-edit distance from an ideal output.  We also used the version of string-edit distance described by Bangalore et al (2000) which normalises for length.  (Bangalore et al, 2000) finds this metric to correlate well with human judgments of understandability and quality. It is not appropriate to reward the mere presence (regardless of place in the string) of, say, by midnight (which is what some evaluation metrics are specifically designed to do, e.g. [Bangalore et al, 2000]). This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al, 2000) for judging compression quality (Clarke and Lapata, 2006).
Deep Read: A Reading Comprehension System paper describes initial work on Read, an automated reading comprehension system that accepts arbitrary text input (a story) and answers questions about it. We have acquired a corpus of 60 and 60 test stories of to grade material; each story is followed by short-answer questions (an answer key was also provided). We used these to construct and evaluate a baseline system that uses pattern matching (bag-of-words) techniques augmented with additional automated linguistic processing (stemming, name identification, semantic class identification, and pronoun resolution). This simple system retrieves the sentence containing the answer 30-40% of the time. The Deep Read reading comprehension prototype system (Hirschman et al, 1999) achieves a level of 36% of the answers correct using a bag-of-words approach together with limited linguistic processing. The RC task was first proposed by the MITRE Corporation which developed the Deep Read reading comprehension system (Hirschman et al, 1999). (Hirschman et al 1999) reported a HumSent accuracy of 36.6% on the Remedia test set. The Deep Read reading comprehension system (Hirschman et al, 1999) uses a statistical bag-of-words approach, matching the question with the lexically most similar sentence in the story. Moreover, the domain is another example of "found test material" in the sense of (Hirschman et al, 1999): puzzle texts were developed with a goal independent of the evaluation of natural language processing systems, and so provide a more realistic evaluation framework than specially-designed tests such as TREC QA. We call this set the MITRE corpus (Hirschman et al, 1999). We used the Remedia corpus (Hirschman et al, 1999) and ChungHwa corpus (Xu and Meng, 2005) in our experiments.
The Importance of Syntactic Parsing and Inference in Semantic Role Labeling Vasin Punyakanok??  We follow the approach in (Punyakanok et al, 2008) in framing the SRL problem as a two-stage pipeline: identification followed by labeling. In contrast to the approach in (Punyakanok et al, 2008), which tags constituents directly, we tag headwords and then associate them with a constituent, as in a previous CCG-based approach (Gildea and Hockenmaier, 2003). Finally, the system described in (Punyakanok et al., 2008) uses a joint inference model to resolve discrepancies between multiple automatic parses. First, others (Punyakanok et al, 2008, e.g.), have found that different parsers have different error patterns, and so using multiple parsers can yield complementary sources of correct information. The results for gold standard parses are comparable to the winning system of the CoNLL 2005 shared task on semantic role labeling (Punyakanok et al, 2008). These annotations were obtained by automatically semantic role labelling portions of CHILDES with the system of Punyakanok et al (2008) before roughly hand-correcting them (Connor et al, 2008). Punyakanok et al. (2008) and Toutanova et al. (2008) used global inference to ensure that the predictions across all arguments of the same predicate are coherent. We re-implemented the system of Punyakanok et al (2008), which we briefly describe here, to serve as our baseline verb semantic role labeler. The verb SRL system of Punyakanok et al (2008) consists of four stages: candidate generation, argument identification, argument classification and inference. ILPs have since been used successfully in many NLP applications involving complex structures - Punyakanok et al. (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al. (2009) for dependency parsing and several others. Compared to the 76.29% F1 score reported by Punyakanok et al (2008) using single parse tree predictions from the parser, our system obtained 76.22% F1 score on section 23 of the Penn Treebank. As a technical point, this defines one inference problem per sentence, rather than per predicate as in the verb SRL system of Punyakanok et al (2008). Following Das et al (2014) and Punyakanok et al (2008) we use the log-probability of the local classifiers as a score in an integer linear program (ILP) to assign roles subject to hard constraints described in 5.4 and 5.5. Here, too, the embedding model barely misses the performance of the best baseline, but we are at par and sometimes better than the single parser setting of a state-of-the-art SRL system (Punyakanok et al, 2008). Directed edges link verbs to the head words of semantic role labeling arguments produced by (Punyakanok et al, 2008). Punyakanok et al (2008) formulate a variety of constraints on argument configurations. It also has been shown to produce state-of-the-art results on many natural language applications (Punyakanok et al, 2008). Following the joint inference procedure in (Punyakanok et al, 2008), we want to select a label for each argument such that the total score is maximized subject to some constraints. Our baseline SRL model is an implementation of (Punyakanok et al, 2008) which was the top performing system in CoNLL 2005 shared task. Due to space constraints, we omit the details of the system and refer readers to (Punyakanok et al, 2008).
A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees. Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages.  Li et al (2012) and Bohnet and Nivre (2012) use joint models for POS tagging and dependency parsing, significantly outperforming their pipeline counterparts. In parsing, Bohnet and Nivre (2012) and Bohnet et al (2013) propose a model for joint morphological analysis, part-of speech tagging and dependency parsing using a Usingeval.pl from Buchholz and Marsi (2006).  Bohnet and Nivre (2012)'s transition-based system jointly performs POS tagging and dependency parsing, which shows higher accuracy than ours. Bohnet and Nivre (2012) introduced a transition-based system that jointly performed POS tagging and dependency parsing. 
Syntax-Based Alignment Of Multiple Translations: Extracting Paraphrases And Generating New Sentences We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets. These FSAs are good representations of paraphrases. They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets. Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations. We point out that one such approach, recently proposed by Pang et al (2003), also represents paraphrases by lattices, similarly to our method, although their lattices are derived using parse information. Much prior work has used lattices to compactly represent a range of lexical choices (Pang et al,2003). Similar work is described in [Pang et al, 2003], who describe a syntax-based algorithm that builds word lattices from parallel translations which can be used to generate new para phrases.  In a similar vein, Pang et al (2003) used a corpus of alternative English translations of Chinesenews stories in combination with a syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified.  Evaluation is sped up by using a compact word lattice view for eliciting human judgments, built using the syntactic fusion algorithm of (Pang et al, 2003).  To avoid combinatorial problems, implementing multiple simultaneous substitutions could be done using a lattice, much like in (Pang et al, 2003). In the automatic evaluation of machine translation, paraphrases may help to alleviate problems presented by the fact that there are often alternative and equally valid ways of translating a text (Pang et al, 2003).    Earlier, Barzilay and Lee (2003) and Pang et al (2003) developed approaches to aligning multiple reference translations in order to extract para phrases and generate new sentences. Pang et al (Pang et al, 2003) used parallel monolingual corpora built from news stories that had been independently translated several times to learn lattices from a syntax-based alignment process. Pang et al provide a remedy to this problem by performing alignment on the Charniak parse trees of the clustered sentences (Pang et al, 2003). Our work is closest in spirit to the two papers that inspired us (Barzilay and Lee, 2003) and (Pang et al, 2003). Pang et al (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences. Pang et al (2003) use word lattices as paraphrase representations from semantically equivalent translations sets. On the basis of this hypothesis, Barzi lay and McKeown (2001) and Pang et al2003) created monolingual parallel corpora from multiple human translations of the same source.
Assigning Time-Stamps To Event-Clauses We describe a procedure for arranging into a time-line the contents of news stories describing the development of some situation. We describe the parts of the system that deal with 1. breaking sentences into event-clauses and 2. resolving both explicit and implicit temporal references. Evaluations show a performance of 52%, compared to humans. These numbers compare favorably with the previous literature: (Filatova and Hovy 2001) obtained 82% accuracy on anchoring for a single type of event/topic on 172 clauses, while (Mani and Wilson 2000) obtained accuracy of 59.4% on anchoring over 663 verb contexts. Filatova and Hovy (2001) also explore temporal linking with events, but do not assume that events and time stamps have been provided by an external process. Filatova and Hovy (2001) assign time stamps to clauses in which an event is mentioned. Filatova and Hovy (2001) also process explicit temporal expressions within a text and apply this information throughout the whole article, assigning activity times to all clauses. Filatova and Hovy (2001) infer time values based on the most recently assigned date or the date of the article.  
Online Large-Margin Training Of Dependency Parsers We present an effective training algorithm for linearly-scored dependency parsers that implements online largemargin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996). The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements. The recent advances in parsing have achieved parsers with O(n3) time complexity without the grammar constant (McDonald et al, 2005).  A dependency-based system using MST Parser (McDonald et al, 2005). This may be partially compensated for by including features about the surrounding words (McDonald et al., 2005), but any feature templates which would be identical across the two contexts will be in tension.    Many of the features above were introduced in McDonald et al (2005a); specifically, the node type, inside, and edge features. In turn, those features were inspired by successful previous work in first order dependency parsing (McDonald et al, 2005). Although (McDonald et al, 2005) used the prefix of each word form instead of word form itself as features, character-level features here for Chinese is essentially different from that.  The latest state-of-the-art statistical dependency parsers are discriminative, meaning that they are based on classifiers trained to score trees, given a sentence, either via factored whole-structure scores (McDonald et al, 2005a) or local parsing decision scores (Hall et al, 2006). The solution to the conditionalization problem is given in Section 3, using a widely-known but newly-applied Matrix Tree Theorem due to Tutte (1984), and experimental results are presented with a comparison to the MIRA learning algorithm used by McDonald et al (2005a).   We compare conditional training of a non projective edge-factored parsing model to the online MIRA training used by McDonald et al (2005b). Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)). We used CoNLL 03 data (Tjong Kim Sang and De Meulder, 2003) for NER, and the Penn Treebank (PTB) III corpus (Marcus et al, 1994) converted to dependency trees for DEPAR (McDonald et al, 2005). L2PA is also known as a loss augmented variant of one best MIRA, well-known in DEPAR (McDonald et al, 2005). 
Class-Based N-Gram Models Of Natural Language We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics. On the other hand, the thesaurus-based method of Yarowsky (1992) may suffer from loss of information (since it is semi-class-based) as well as data sparseness since H Classes used in Resnik (1992) are based on the WordNet taxonomy while classes of Brown et al (1992) and Pereira et al (1993) are derived from statistical data collected from corpora. For all languages we use Brown clustering (Brown et al, 1992) to construct a log (C)+ C feature vector where the first log (C) elements indicate which mer gable cluster the word belongs to, and the last C elements indicate the cluster identity. We use the word clusters computed by Candito and Crabbe (2009) using Percy Liang's implementation of the Brown unsupervised clustering algorithm (Brown et al, 1992).  One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al, 1992). Agglomerative clustering algorithm by Brown et al (1992) is used for this purpose. Formally, as mentioned in Brown et al (1992), let C be a hard clustering function which maps vocabulary V to one of the K clusters. Note this is different from the likelihood estimation of Brown et al (1992). The features were all derived from the publicly available clusters produced by running the Brown clustering algorithm (Brown et al, 1992) over the BLLIP corpus with the Penn Treebank sentences excluded. Popular clustering algorithms used prevalently in many NER systems are, for example, the combination of distributional and morphological similarity work of (Clark 2003) or the classic N-gram language model based clustering algorithm of (Brown et al 1992). As a state of-the-art clustering method, we consider Brown clustering (Brown et al 1992) as implemented in the SRILM-toolkit (Stolcke, 2002). Methods that use bigrams (Brown et al, 1992) or trigrams (Martin et al, 1998) cluster words considering as a word's context he one or two immediately adjacent words and employ as clustering criteria the minimal loss of average information and the perplexity improvement respectively. Brown et al (1992) also proposed a window method introducing the concept of semantic stickiness of two words as the relatively frequent close occurrence between them (less than 500 words distance). We find that the oldest system tested (Brown et al, 1992) produces the best prototypes, and that using these prototypes as input to Haghighi and Klein's system yields state of-the-art performance on WSJ and improvements on seven of the eight non-English corpora. The systems are as follows:1 [brown]: Class-based n-grams (Brown et al,1992). We found that the oldest system (Brown et al, 1992) yielded the best prototypes, and that using these prototypes gave state-of-the-art performance on WSJ, as well as improvements on nearly all of the non-English corpora.  The idea was introduced by Brown et al (1992) and used in different applications, including speech recognition, named entity tagging, machine translation, query expansion, text categorization, and word sense disambiguation. To this end, the Brown algorithm (Brown et al, 1992) is applied to pairwise word co-occurrence statistics based on different definitions of word co-occurrence. In order to cluster lexical items, we use the algorithm proposed by Brown et al (1992), as implemented in the SRILM toolkit (Stolcke, 2002).
Hedge Trimmer: A Parse-And-Trim Approach To Headline Generation and Abstracts for Nice Summaries, In Workon Automatic Philadelphia, PA, pp. 9-14. Edmundson, H. (1969). “New methods in automatic of the 16(2). Grefenstett, G. (1998). Producing intelligent telegraphic text reduction to provide an audio scanning serfor the blind. In Notes of the AIII Spring on Intelligent Text Summarization, A more common, extractive approach operates top-down, by starting from an extracted sentence that is compressed (Dorr et al, 2003) and annotated with additional information (Zajic et al, 2004). HedgeTrimmer is our implementation of the Hedge Trimer system (Dorr et al, 2003), and Topiary is our implementation of the Topiary system (Zajicet al, 2004). For this reason, most early headline generation work focused on either extracting and reordering n-grams from the document to be summarized (Banko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003). Hedge Trimmer (Dorr et al, 2003) is a system that creates a headline for an English newspaper story using linguistically-motivated heuristics to choose a potential headline. The algorithm used by Dorr et al (2003) removes subordinate clauses, to name one example.  
Discriminative Training And Maximum Entropy Models For Statistical Machine Translation We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is significantly improved using this approach. The approach of optimizing a small number of meta parameters has been applied to machine translation by Och and Ney (2002). Each translation rule in the phrase-based translation model has a set number of features that are combined in the log-linear model (Och and Ney, 2002), and our semi-supervised DAE features can also be combined in this model. An alternate way to optimize weights over translation features is described in Och and Ney (2002). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative method (Och and Ney, 2002). Smoothing the objective function may allow differentiation and standard ML learning techniques (Och and Ney, 2002). For example, the system described in (Koehnet al, 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002). For all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al, 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002).  Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). We optimized feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set. This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training. The training of the model scaling factors as described in (Och and Ney, 2002) was done on N-best lists. The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleu based minimum error rate training (Och, 2003) to optimize the weights of their log linear models' feature functions (Och and Ney, 2002). The feature functions hi are the system models and the weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). Word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (Koehn et al, 2003), among all the features utilized in a maximum-entropy (log linear) model (Och and Ney, 2002).  To translate the input documents into English we use phrase-based statistical machine translation systems based on the log-linear formulation of the problem (Och and Ney, 2002). In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002).
Immediate-Head Parsing For Language Models We present two language models based upon an “immediate-head” parser — our name for a parser that conditions events below a constituent head of While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology. The perplexity for both of these models significantly improve upon the trigram model base-line as well as the best previous grammarbased language model. For the better of our two models these improvements are 24% and 14% respectively. We also suggest that improvement of the underlying parser should significantly improve the model’s perplexity and that even in the near term there is a lot of potential for improvement in immediatehead language models. are what we will call parsers in that all of the properties of the immedescendants of a constituent assigned probabilities that are conditioned on the lexical of For example, in Figure 1 the probability the into np pp conditioned on head of the “put”, as are the choices of the under the i.e., “ball” (the head of and “in” (the head of the It is the experience of the statistical parsing community that immediate-head parsers are the most accurate we can design. It is also worthy of note that many of these [1,3,6,7] are that is, for a try to find the parse by Equation 1: = arg (1) This is interesting because insofar as they comthese parsers define a language-model in that they can (in principle) assign a probability to all possible sentences in the language by com    That is, we parse all sentences using the Charniak's parser (Charniak, 2001), relation instances are generated by iterating over all pairs of entity mentions occurring in the same sentence. The model presented by Charniak (Charniak, 2001) identifies both syntactic structural and lexical dependencies that aid in language modeling. The first stage is a PCFG word-lattice parser that generates a set of candidate parses over strings in a word-lattice, while the second stage rescores these candidate edges using a lexicalized syntactic language model (Charniak, 2001). These parses are then rescored using a lexicalized syntactic model (Charniak, 2001). These contexts include syntactic structure such as parent and grandparent category labels as well as lexical items such as the head of the parent or the head of a sibling constituent (Charniak, 2001). The second stage parser used is a modified version of the Charniak language modeling parser described in (Charniak, 2001). Charniak (Charniak, 2000) developed a state-of-the-art statistical CFG parser and then built an effective language model based on it (Charniak, 2001). Our system embeds Phramer2 (used for minimum error rate training, decoding, decoding tools), Pharaoh (Koehn, 2004) (decoding), Carmel 3 (helper for Pharaoh in n-best generation), Charniak's parser (Charniak, 2001) (language model) and SRILM4 (n-gram LM construction). We used Charniak's parser as an additional LM (Charniak, 2001) in reranking. Hall and Johnson (2003) use a best-first probabilistic context free grammar (PCFG) to parse the input lattice, pruning to a set of local trees (candidate partial parse trees), which are then passed to a version of the parser of Charniak (2001) for more refined parsing. The parses were automatically produced by the parser of Charniak (2001). The current best-performing models, in terms of WER, for the HUB-1 corpus, are the models of Roark (2001), Charniak (2001) (applied to n-best lists by Hall and Johnson (2003)), and the SLM of Chelba and Jelinek (2000) (applied to n-best lists by Xu et al (2002)). Hall (2003) is a lattice-parser related to Charniak (2001). The difference in WER between our parser and those of Charniak (2001) and Roark (2001) applied to word lists may be due in part to the lower PARSEVAL scores of our system. Another contributing factor to the accuracy of Charniak (2001) is the size of the training set: 20M words larger than that used in this work. We use a syntax-based language model which was originally developed for use in speech recognition (Charniak, 2001) and later adapted to work with a syntax-based machine translation system (Charniaket al, 2001). Noun noun (adjective noun, respectively) sequences of words were extracted using the Lauer heuristic (Lauer 1995) which looks for consecutive pairs of nouns that are neither preceded nor succeeded by a noun after each sentence was syntactically parsed with Charniak parser (Charniak, 2001) (for XWN we used the gold parse trees).
Pseudo-Projectivity A Polynomially Parsable Non-Projective Dependency Grammar  It is possible to obtain a polynomial parser provided that we limit the number of nodes simultaneously involved in non-projective configurations (see Kahane et al 1998 for similar techniques). This formalism is based on previous work presented in (Kahane et al, 1998), which has been substantially reformulated in order to simplify it. We will extend our basic approach in the spirit of (Kahane et al, 1998) in future work. Kahane et al (1998) present three different types of rules, for sub categorization, modification, and linear precedence. It is also related to the lifting rules of (Kahane et al, 1998), but where they choose to stipulate rules that license liftings, we opt instead for placing constraints on otherwise unrestricted climbing. The pseudo-projective grammar proposed by Kahane et al (1998) can be parsed in polynomial time and captures non-local dependencies through a form of gap-threading, but the structures generated by the grammar are strictly projective. With this conversion technique, output dependency trees are necessarily projective, and extracted dependencies are necessarily local to a phrase, which means that the automatically converted trees can be regarded as pseudo-projective approximations to the correct dependency trees (Kahane et al, 1998). This concept was introduced as lifting in (Kahane et al, 1998). based: for example, those described by Lombardoand Lesmo (1996), Barbero et al (1998) and Kahane et al (1998) are tied to the formalizations of dependency grammar using context-free like rules described by Hays (1964) and Gaifman (1965). However, predictive grammar-based algorithms such as those of Lombardo and Lesmo (1996) and Kahane et al (1998) have operations which postulate rules and can not be defined in terms of dependency graphs, since they do not do any modifications to the graph. In addition, the work of Kahane et al (1998) provides a polynomial parsing algorithm for a constrained class of non projective structures. The definition of non-projectivity can be found in Kahane et al (1998).  First, the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al, 1998) and encoding information about these lifts in arc labels. We call this pseudo projective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al, 1998). The dependency graph in Figure 1 satisfies all the defining conditions above, but it fails to satisfy the condition of projectivity (Kahane et al, 1998). Using the terminology of Kahane et al (1998), we say that jedna is the syntactic head of Z, while je is its linear head in the projectivized representation. Unlike Kahane et al (1998), we do not regard a projectivized representation as the final target of the parsing process.
Seeing Stars: Exploiting Class Relationships For Sentiment Categorization With Respect To Rating Scales address the wherein rather than simply decide whether a review is “thumbs up” or “thumbs down”, as in previous sentiment analysis work, one must determine an author’s evaluation with respect to a multi-point scale (e.g., one to five “stars”). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, “three stars” is intuitively closer to “four stars” than to “one star”. We first evaluate human performance at task. Then, we apply a based on a labeling formulation of the problem, that alters a given-ary classifier’s output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem. This can help explain why, as Pang and Lee (2005) note, one person's four-star review is another's two-star.  We use a sentiment-annotated data set consisting of movie reviews by (Pang and Lee, 2005) and tweets from http: //help.sentiment140.com/for-students. There is a huge body of work on OM in movie reviews which was sparked by the dataset from Pang and Lee (2005). Pang and Lee (2005) use metric labeling to perform multi-class collective classification of movie reviews.  In order to compare our approach to other methods we also show results on commonly used sentiment datasets: movie reviews (MR) (Pang and Lee, 2005) and opinions5 (MPQA) (Wiebe et al, 2005). The optimal POS bi-tags have been derived experimentally by using top 10% features on information gain based-pruning classifier on polarity dataset by (Pang and Lee, 2005). Cascaded models for sentiment classification were studied by (Pang and Lee, 2005). This indicates there is some difficulty distinguishing between the fine-grained categories we specified, but high agreement at a coarser level, which advocates using a ranking approach for evaluation (see also Pang and Lee 2005).  Pang and Lee (2005) treat sentiment analysis as an ordinal ranking problem. Machine learning techniques have been proposed for sentiment classification (Pang et al., 2002; Mullen and Collier, 2004) based on annotated samples from experts, but they have limited performance especially when estimating ratings of a multi-point scale (Pang and Lee, 2005). We then adopt the machine learning method proposed in (Pangand Lee, 2005) and the Bayesian Network classifier (Russell and Norvig, 2002) for feature rating estimation. In 2005, Pang and Lee extended their earlier work in (Pang and Lee, 2004) to determine a reviewer's evaluation with respect to multi scales (Pang and Lee, 2005). We adopt the approach of Pangand Lee (Pang and Lee, 2005) described in Section 2 for feature rating estimation. RT-s: Short movie reviews dataset containing one sentence per review (Pang and Lee, 2005). We might attempt to exploit these dependencies in a manner similar to Pang and Lee (2005) to improve three-way classification. Data: The dataset consists of snippets from Rotten Tomatoes (Pang and Lee, 2005). This dataset was created and used by Pang and Lee (2005) to train a classifier for identifying positive sentences in a full length review.
Statistical Machine Translation With Scarce Resources Using Morpho-Syntactic Information In statistical machine translation, correspondences between the words in the source and the target language are learned from parallel corpora, and often little or no linguistic knowledge is used to structure the underlying models. In particular, existing statistical systems for machine translation often treat different inflectedforms of the same lemma as if they were independent ofone another. The bilingual training data can be better exploited by explicitly taking into account the interdependencies of related inflected forms. We propose the construction of hierarchical lexicon models on the basis of equivalence classes of words. In addition, we introduce sentence-level restructuring transformations which aim at the assimilation of word order in related sentences. We have systematically investigated the amount of bilingual training data required to maintain an acceptable quality of machine translation. The combination of the suggested methods for improving translation quality in frameworks with scarce resources has been successfully tested: We were able to reduce the amount of bilingual training data to less than 10% of the original corpus, while losing only 1.6% in translation quality. The improvement of the translation results is demonstrated on two German-English corpora taken from the Uerbmobil task and the Nespole! task. In addition, Niessen and Ney (2004) decompose German words into a hierarchical representation using lemmas and morphological tags, and use a MaxEnt model to combine the different levels of representation in the translation model. There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). Niessen and Ney (2004) used morphological decomposition to get better alignments.   Niessen and Ney (2004) describe an approach for translation from German to English that combines verbs with associated particles, and also reorders questions. Niessen and Ney (2004) have used morphological decomposition to improve alignment quality. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. (Niessen and Ney, 2004) describe a method that combines morphologically split verbs in German, and also reorders questions in English and German. Among these were cases that can be handled such as separable prefix verbs like 'aufzeigten' ('pointed out') (Niessen and Ney, 2000) or adjective compounds such as 'multidimensionale' ('multi dimensional').
Robust Bilingual Word Alignment For Machine Aided Translation We have developed a new program called aligning parallel text, text such as the Canadian Hansards that are available in two or more languages. The program takes the of 1993), a robust alternative to sentence-based alignment programs, and applies word-level constraints usa version of Brown Model 2 (Brown et al., 1993), modified and extended to deal robustness issues. tested on a subset of Canadian Hansards supplied by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of over More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology. There have been quite a number of recent papers on parallel text: Brown et al (1990, 1991, 1993), Chen (1993), Church (1993), Church et al (1993), Dagan et al (1993), Gale and Church (1991, 1993), Isabelle (1992), Kay and Rgsenschein (1993), Klavans and Tzoukermann (1990), Kupiec (1993), Matsumoto (1991), Ogden and Gonzales (1993), Shemtov (1993), Simard et al (1992), WarwickArmstrong and Russell (1990), Wu (to appear). This estimate could be used as a starting point for a more detailed alignment algorithm such as word_align (Dagan et al, 1993). This concept of alignment has been also used for tasks like automatic vocabulary derivation and corpus alignment (Dagan et al, 1993). In Ido Dagan et al (1993) noisy points were filtered out by deleting frequent words. However, (Dagan et al, 1993) have shown that knowledge of target-text length is not crucial to the model's performance. In the recent years, there have been a number of papers considering this or similar problems: (Brown et al, 1990), (Dagan et al, 1993), (Kay et al, 1993), (Fung et al, 1993). 
Bootstrapping Path-Based Pronoun Resolution We present an approach to pronoun resolution based on syntactic paths. Through a simple bootstrapping procedure, we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities. This path information enables us to handle previously challenging resolution instances, and also robustly addresses traditional syntactic coreference constraints. Highly coreferent paths also allow mining of precise probabilistic gender/number information. We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classifier. Significant gains in performance are observed on several datasets.  We follow the closed track setting where systems may only be trained on the provided training data, with the exception of the English gender and number data compiled by Bergsma and Lin (2006). Bergsma and Lin (2006) determine the likelihood of coreference along the syntactic path connecting a pronoun to a possible antecedent, by looking at the distribution of the path in text. Given an automatically parsed corpus, Bergsma and Lin (2006) extract from each parse tree a dependency path, which is represented as a sequence of nodes and dependency labels connecting a pronoun and a candidate antecedent, and collect statistical information from these paths to determine the likelihood that a pronoun and a candidate antecedent connected by a given path are coreferent.   Gender and Animacy processor: This modules collects gender information from the gender corpus (Bergsma and Lin, 2006) and checks a self-made corpus for profession (teacher, doctor, etc) and family relations (mother, father, etc), extracted from web searches. In the closed track, systems were limited to the provided data, plus the use of two pre-specified external resources: i) WordNet and ii) a pre-computed number and gender table by Bergsma and Lin (2006).  As noted above, systems were allowed to make use of gender and number predictions for NPs using the table from Bergsma and Lin (Bergsma and Lin, 2006). We assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular or plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from (Bergsma and Lin, 2006). All the knowledge required by the feature functions is obtained from the annotations of the corpora and no external resources have been used with the exception of WordNet (Miller, 1995), gender and number information (Bergsma and Lin, 2006) and sense inventories. Both Ge et al (1998) and Bergsma and Lin (2006) show that learned gender is the most important feature in their pronoun resolution systems. We use the approach of Bergsma and Lin (2006), both because it achieves state-of-the-art gender classification performance, and because a database of the obtained noun genders is available online. We can regard the Bergsma and Lin (2006) approach and our discriminative system as two orthogonal views of gender, in a co-training sense (Blum and Mitchell, 1998). For English, number and gender for common nouns are computed via a comparison of head lemma to head and using the number and gender data of Bergsma and Lin (2006).  For non pronominal mentions, we used the number and gender data (Bergsma and Lin, 2006) provided by the task organizers and queried it for the head word of the mention. Bergsma and Lin (2006) built a statistical model from paths that include the lemma of the intermediate tokens, but replace the end nodes with noun, pronoun, or pronoun-self for nouns, pronouns, and reflexive pronouns, respectively. For the gender task that we study in our experiments, we acquire class instances by filtering the dataset of nouns and their genders created by Bergsma and Lin (2006).
Recognising Textual Entailment With Logical Inference We use logical inference techniques for recognising textual entailment. As the performance of theorem proving turnsout to be highly dependent on not read ily available background knowledge, we incorporate model building, a technique borrowed from automated reasoning, and show that it is a useful robust method to approximate entailment. Finally, we use machine learning to combine these deep semantic analysis techniques with simpleshallow word overlap; the resulting hy brid model achieves high accuracy on the RTE testset, given the state of the art. Ourresults also show that the different techniques that we employ perform very dif ferently on some of the subsets of the RTE corpus and as a result, it is useful to use the nature of the dataset as a feature. However, this method does not work for realworld datasets such as PASCAL RTE (Dagan et al., 2006), because of the knowledge bottleneck: it is often the case that the lack of sufficient linguistic knowledge causes failure of inference, thus the system outputs "no entailment" for almost all pairs (Bos and Markert, 2005). On PASCAL RTE datasets, strict logical inference is known to have very low recall (Bos and Markert, 2005), so on-the-fly knowledge is crucial in this setting. Bos and Markert (2005) proposes features from a model builder; Raina et al (2005) proposes an abduction process; Tatu and Moldovan (2006) shows handcrafted rules could drastically improve the performance of a logic-based RTE system. NutCracker (Bos and Markert, 2005) is a system based on logical representation and automatic theorem proving, but utilizes only WordNet (Fellbaum, 1998) as a lexical knowledge resource.
Corpus Based PP Attachment Ambiguity Resolution With A Semantic Dictionary This paper deals with two important ambiguities of natural language: prepositional phrase attachment and word sense ambiguity. We propose a new supervised learning method for PPattachment based on a semantically tagged corpus. Because any sufficiently big sense-tagged corpus does not exist, we also propose a new unsupervised context based word sense disambiguation algorithm which amends the training corpus for the PP attachment by word sense tags. We present the results of our approach and evaluate the achieved PP attachment accuracy in comparison with other methods. The state of the art is a supervised algorithm that employs a semantically tagged corpus (Stetina and Nagao, 1997). clSN induces a decision tree with a sense-tagged corpus, using a semantic dictionary (Stetina and Nagao, 1997 ). At the other extreme, Stetina and Nagao (1997) developed a customized, explicit WSD algorithm as part of their decision tree system. Stetina and Nagao (1997) trained on a version of the Ratnaparkhi et al (1994) dataset that contained modifications similar to those by Collins and Brooks (1995) and excluded forms not present in WordNet. Finally, Greenberg (2013) implemented a decision tree that reimplemented the WSD module from Stetina and Nagao (1997), and also used WordNet morphosemantic (teleological) links, WordNet evocations, and a list of phrasal verbs as features. We explored the effect of excluding quadruples with lexically-specified prepositions (usually tagged PPCLR in WSJ), removing sentences in which there was no actual V, N 1, P, N 2 string found, manually removing encountered misclassifications, and reimplementing data sparsity modifications from Collins and Brooks (1995) and Stetina and Nagao (1997). Additional meanings derived from specific synsets have been attached to the words as described in (Stetina and Nagao, 1997).  The current state of the art is 88% reported by Stetina and Nagao (1997) using the WSJ text in conjunction with WordNet. The best published results over RRR are those of Stetina and Nagao (1997), who employ WordNet sense predictions from an unsupervised WSD method within a decision tree classifier. The fact that the improvement is larger for PP attachment than for full parsing is suggestive of PP attachment being a parsing subtask where lexical semantic information is particularly important, supporting the findings of Stetina and Nagao (1997) over a standalone PP attachment task. Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maxi mum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998). The state-of-the-art is set by (Stetina and Nagao, 1997) who generalize corpus observations to semantically similar words as they can be derived from the WordNet hierarchy. 
