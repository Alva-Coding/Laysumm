Finding Parts In Very Large Corpora We present a method for extracting parts of objects from wholes (e.g. &quot;speedometer&quot; from &quot;car&quot;). Given a very large corpus our method finds part words with 55% accuracy for the top 50 words as ranked by the system. The part list could be scanned by an end-user and added to an existing ontology (such as WordNet), or used as a part of a rough semantic lexicon. Berland and Charniak (1999) use Hearst style techniques to learn meronym relation ships (part-whole) from corpora. (Berland and Charniak, 1999) proposed similar lexico-syntactic patterns to extract part-whole relationships. The OtherY-Model performs particularly poorly on smaller data sizes, where coverage of the Hearst-style patterns maybe limited, as also observed by Berland and Charniak (1999). Berland and Charniak (1999) report what they believed to be the first work finding part-whole relations from unlabelled corpora. In 1999, Berland and Charniak (Berland and Charniak, 1999) applied statistical methods on a very large corpus to find PART-WHOLE relations. Berland and Charniak (1999) used similar pattern-based techniques and other heuristics to extract meronymy (part-whole) relations. Berland and Charniak (1999) used a similar method for extracting instances of meronymy relation. Lexical patterns have been successfully used to represent various semantic relations between words such as hypernymy (Hearst, 1992), and meronymy (Berland and Charniak, 1999). A similar approach was pursued in parallel by Berland and Charniak (1999). Berland and Charniak (1999) suggest their work may be useful for building a lexicon or ontology, like WordNet. Berland and Charniak (1999) use Hearst's manual procedure. In attribute extraction, typically one must choose between the precise results of rich patterns (involving punctuation and parts-of-speech) applied to small corpora (Berland and Charniak, 1999) and the high-coverage results of superficial patterns applied to web-scale data, e.g. via the Google API (Almuhareb and Poesio, 2004). Prior work has mostly focused on finding "relevant" attributes (Alfonseca et al., 2010) or "correct" parts (Berland and Charniak, 1999). Indeed, Berland and Charniak (1999) attempted to filter out attributes that were regarded as qualities (like drive ability) rather than parts (like steering wheels) by removing words ending with the suffixes -ness, -ing, and -ity. Also, previous relation extraction work, http: //projects.ldc.upenn.edu/ace/ such as Berland and Charniak (1999) and Girju et al. Berland and Charniak (1999) used similar pattern-based techniques and other heuristics to extract meronymy (part-whole) relations. Berland and Charniak (1999) proposed a system for part-of relation extraction, based on the (Hearst 1992) approach. (Berland and Charniak, 1999) use hand crafted patterns to discover part-of (meronymy) relation ships, and (Chklovski and Pantel, 2004) discover various interesting relations between verbs. OPINE's use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004). relation (Berland and Charniak 1999), causal relation (Girju 2003), and entailment relation (Geffet and Dagan 2005).
The Mathematics Of Statistical Machine Translation: Parameter Estimation We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus. The program takes the output of char_align (Church, 1993), a robust alternative to sentence-based alignment programs, and applies word-level constraints using a version of Brown's Model 2 (Brown et al, 1993), modified and extended to deal with robustness issues. Recent work in machine translation has evolved from the traditional word (Brown et al, 1993). The IBM Model 1 (Brown et al, 1993) and hidden Markov model (HMM) (Vo gel et al, 1996) are used to estimate the alignment. A special NULL word is typically used when learning word alignment (Brown et al, 1993). For instance, the most relaxed IBM Model-1, which assumes that any source word can be generated by any target word equally regardless of distance, can be improved by demanding a Markov process of alignments as in HMM-based models (Vogel et al, 1996), or implementing a distribution of number of target words linked to a source word as in IBM fertility-based models (Brown et al, 1993). It can be applied to complicated models such IBM Model-4 (Brown et al, 1993). We shall take HMM-based word alignment model (Vogel et al., 1996) as an example and follow the notation of (Brown et al, 1993). The word alignment was trained with six iterations of IBM model 1 (Brown et al 1993). The feature set we use is inspired by Munteanu and Marcu (2005) who define the features based on IBM Model-1 (Brown et al, 1993) alignments for source and target pairs. In (Brown et al, 1994), the authors proposed a method to integrate the IBM translation model 2 (Brown et al, 1993) with an ASR system. We rescore the ASR N -best lists with the standard HMM (Vogel et al, 1996) and IBM (Brown et al, 1993) MT models. In (Brown et al, 1993), three alignment models are described that include fertility models, these are IBM Models 3, 4, and 5. Word based models have a long history in machine translation, starting with the venerable IBM translation models (Brown et al, 1993) and the hidden Markov model (Vogel et al, 1996). Originally introduced as a byproduct of training statistical translation model since (Brown et al, 1993), word alignment has become the first step in training most statistical translation systems, and alignments are useful to a host of other tasks. The IBM models (Brown et al, 1993) benefit from a one-to-many constraint. The statistical machine translation framework (SMT) formulates the problem of translating a sentence from a source language S into a target language T as the maximization problem of the conditional probability: TM LM =argmaxT p (S|T) p (T), (1) where p (S|T) is called a translation model (TM), rep resenting the generation probability from T into S, p (T) is called a language model (LM) and represents the likelihood of the target language (Brown et al, 1993). We then train IBM models (Brown et al, 1993) using the GIZA++ package (Och and Ney, 2000). Statistical machine translation (SMT) was originally focused on word to word translation and was based on the noisy channel approach (Brown et al., 1993). lexical translation models (Brown et al, 1993) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We defined a feature set which includes: length ratio and length difference between source and target sentences, lexical probability scores similar to IBM model 1 (Brown et al, 1993), number of aligned/unaligned words and the length of the longest aligned word sequence.
Optimizing Chinese Word Segmentation for Machine Translation Performance Previous work has shown that Chinese word segmentation is useful for machine translation to English, yet the way different segmentation strategies affect MT is still poorly understood. In this paper, we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better MT performance. We find that other factors such as segmentation consistency and granularity of Chinese “words” can be more important for machine translation. Based on these findings, we implement methods inside a conditional random field segmenter that directly optimize segmentation granularity with respect to the MT task, providing an improvement of 0.73 BLEU. We also show that improving segmentation consistency using external lexicon and proper noun features yields a 0.32 BLEU increase. Chinese word segmentation is done by the Stanford Chinese segmenter (Chang et al, 2008). The most obvious example of this lies in languages that do not separate words with white space such as Chinese, Japanese, or Thai, in which the choice of a segmentation standard has a large effect on translation accuracy (Chang et al., 2008). It has been recognized that varying segmentation granularities are needed for SMT (Chang et al, 2008). All Chinese data was re-segmented with the CRF-based Stanford Chinese segmenter (Chang et al, 2008) that is trained on the segmentation of the Chinese Treebank for consistency. The Chinese text was segmented with a CRF-based Chinesesegmenter optimized for MT (Chang et al, 2008). Chinese words were automatically segmented with a conditional random field (CRF) classifier (Chang et al, 2008) that conforms to the Chinese Treebank (CTB) standard. These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al, 2008). But one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the tree bank segmentation standard designed for monolingually linguistic intuition, rather than specific to the SMT task. Chang et al (2008) enhanced a CRF s segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence. Chang et al (2008) described constraint driven learning (CODL) that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge. The optimal set of the model parameter values was found on dev MT to be k= 3, t AC= 0.0 and t COOC= 15. The comparison candidates also involve two popular off-the-shelf segmentation models: Stanford Segmenter: this model, trained by Chang et al (2008), treats CWS as a binary word boundary decision task. Zhang et al (2008b) and Chang et al (2008) show that get ting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. The third and fourth tokenizations come from the CRF-based Stanford Chinese segmenter described by Chang et al (2008). This affirms our be lief that consistency in tokenization is important for machine translation, which was also mentioned by Chang et al (2008). The use of monolingual probabilistic models does not necessarily yield a better MT performance (Chang et al, 2008).  we first segmented the sentences using the Stanford Chinese Word Segmenter (Chang et al, 2008). We tokenized the English with packages from the Stan ford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al, 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al, 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al, 2008) according to the Penn Chinese Treebank standard (Xue et al, 2005). For English corpora, the pre-processing are the same as that in (Qiu et al, 2009), and for Chinese corpora, the Stanford Word Segmenter (Changet al, 2008) is used to perform word segmentation. The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al, 2008), and the English text was parsed using the Stanford parser (Klein and Manning, 2003). A variety of segmentation granularities, or atomic units, exist, including segmentations at the morpheme (e.g., Sirts and Alumae 2012), word (e.g., Chang et al 2008), sentence (e.g., Reynar and Ratnaparkhi 1997), and paragraph (e.g., Hearst 1997) levels.
The viability of web-derived polarity lexicons We examine the viability of building large polarity lexicons semi-automatically from the web. We begin by describing a graph propagation framework inspired by previous work on constructing polarity lexicons from lexical graphs (Kim and Hovy, 2004; Hu and Liu, 2004; Esuli and Sabastiani, 2009; Blair- Goldensohn et al., 2008; Rao and Ravichandran, 2009). We then apply this technique to build an English lexicon that is significantly larger than those previously studied. Crucially, this web-derived lexicon does not require WordNet, part-of-speech taggers, or other language-dependent resources typical of sentiment analysis systems. As a result, the lexicon is not limited to specific word classes – e.g., adjectives that occur in WordNet – and in fact contains slang, misspellings, multiword expressions, etc. We evaluate a lexicon derived from English documents, both qualitatively and quantitatively, and show that it provides superior performance to previously studied lexicons, including one derived from Recent work in this area includes Velikovich et al (2010), who developed a method for automatically deriving an extensive sentiment lexicon from the web as a whole. We examine two methods for sentiment detection that of Brody and Elhadad (2010) for detecting sentiment in reviews, and that of Velikovich et al (2010) for finding sentiment terms in a giga-scale web corpus. Velikovich et al (2010) constructed a graph where the nodes were 20 million candidate words or phrases, selected using a set of heuristics including frequency and mutual information of word boundaries. On the other hand, the method of Velikovich et al (2010) is based on huge amounts of data, and takes advantage of the abundance of contextual information available in full documents, whereas our domain is closer to that of Brody and Elhadad (2010), who dealt with a small number of candidates and short documents typical to online reviews. Once the graph is constructed, we can use either of the propagation algorithms of Brody and Elhadad (2010) and Velikovich et al (2010), which we will denote Reviews and Web, respectively. Velikovich et al (2010) employed a different label propagation method, as described in Figure 3.  In Velikovich et al (2010), the parameters were tuned on a held out dataset. Top fifteen negative and positive words for the algorithms of Brody and Elhadad (2010) (Reviews) and Velikovich et al (2010) (Web). Although such an assumption played a key role in previous work for the analogous task of learning sentiment lexicon (Velikovich et al, 2010), we expect that the same assumption would be less reliable in drawing subtle connotative sentiments of words.  Velikovich et al (2010) use graph propagation algorithms for constructing a web-scale polarity lexicon for sentiment analysis. A technique named label propagation (Zhu and Ghahramani, 2002) has been used by Rao and Ravichandran (2009) and Velikovich et al (2010), while random walk based approaches, PageRank in particular, have been used by Esuli and Sebastiani (2007).  However in recent years, sentiment lexicons started expanding to include some of those words that simply associate with sentiment, even if those words are purely objective (e.g., Velikovich et al (2010), Baccianellaet al (2010)). In order to collectively induce the visually descriptive words from this graph, we apply the graph propagation algorithm of Velikovich et al (2010), a variant of label propagation algorithms (Zhu and Ghahramani, 2002) that has been shown to be effective for inducing a web-scale polarity lexicon based on word co-occurrence statistics. Examples include constructing polarity lexicons based on lexical graphs from WordNet (Rao and Ravichandran, 2009), constructing polarity lexicons from web data (Velikovich et al 2010) and unsupervised part-of-speech tagging using label propagation (Das and Petrov, 2011). For example, constructing web-derived polarity lexicons (Velikovich et al 2010), top 25 edges were used, and for unsupervised part-of-speech tagging using label propagation (Das and Petrov, 2011), top 5 edges were used. A web-derived lexicon (Velikovich et al, 2010) was constructed for all words and phrases using graph propagation algorithm which propagates polarity from seed words to all other words. Recently, (Velikovich et al, 2010) showed how to use a seed lexicon and a graph propagation framework to learn a larger sentiment lexicon that also includes polar multi-word phrases such as 'once in a life time'.
Combining Lexical Syntactic And Semantic Features With Maximum Entropy Models For Information Extraction Extracting semantic relationships between entities is challenging because of a paucity of annotated data and the errors induced by entity detection modules. We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text. Our system obtained competitive results in the Automatic Content Extraction (ACE) evaluation. Here we present our general approach and describe our ACE results. They use two kinds of features: syntactic ones and word based ones, for example, the path of the given pair of NEs in the parse tree and the word n-gram between NEs (Kambhatla, 2004). Supervised learning method using syntactic and word-based features, the path of the pairs of NEs in the parse tree and the word n gram between pairs of NEs (Kambhatla, 2004). The approaches proposed to the ACE RDC task such as kernel methods (Zelenko et al, 2002) and Maximum Entropy methods (Kambhatla, 2004) required the availability of large set of human annotated corpora which are tagged with relation instances. We compare our results to a state-of-the-art supervised system similar to the system described in (Kambhatla, 2004). Kambhatla (2004) took a similar approach but used multivariate logistic regression (Kambhatla, 2004).  Kambhatla (2004) developed a method for extracting relations by applying Maximum Entropy models to combine lexical, syntactic and semantic features and report that they obtain improvement in results when they combine variety of features.  Similar to our earlier work (Kambhatla, 2004), we used a combination of lexical, syntactic, and semantic features including all the words in between the two mentions, the entity types and subtypes of the two mentions, the number of words in between the two mentions, features derived from the smallest parse fragment connecting the two mentions, etc. For the feature-based methods, Kambhatla (2004) employed Maximum Entropy models to combine diverse lexical, syntactic and semantic features in relation extraction, and achieved the F-measure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus. Another problem is that, although they can explore some structured information in the parse tree (e.g. Kambhatla (2004) used the non-terminal path connecting the given two entities in a parse tree while Zhou et al (2005) introduced additional chunking features to enhance the performance), it is found difficult to well preserve structured information in the parse trees using the feature-based methods.  Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes. (Culotta and Sorensen, 2004) extended this work to estimate kernel functions between augmented dependency trees, while (Kambhatla, 2004) combined lexical features, syntactic features, and semantic features in a maximum entropy model. However, the semantic features discussed in (Kambhatla, 2004) still focus on the word level instead of the conceptual level. Kambhatla (2004) employs Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text for relation extraction.  Kambhatla (2004) use the path of non-terminals connecting two mentions in a parse tree as the parse tree features.
Similarity of Semantic Relations are at least two kinds of similarity. similarity correspondence between rein contrast with which is correspondence between attributes. two words have a high degree of attributional similarity, we call them When two pairs of words have a high degree of relational similarity, we say that their relations are For example, the word pair mason:stone is analogous to the pair carpenter:wood. This article introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector offrequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) The patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM. Veale (2004) used WordNet to answer 374 multiple-choice SAT analogy questions, achieving an accuracy of 43%, but the best corpus-based approach attains an accuracy of 56% (Turney, 2006). The template we use here is similar to Turney (2006), but we have added extra context words before the X and after the Y. Turney (2006) also selects patterns based on the number of pairs that generate them, but the number of selected patterns is a constant (8000), independent of the number of input word pairs. Turney (2006) used a corpus-based algorithm. The best previous result is an accuracy of 56.1% (Turney, 2006). The average senior high school student achieves 57% correct (Turney, 2006). PairClass generates probability estimates, whereas Turney (2006) uses a cosine measure of similarity. The automatically generated patterns in PairClass are slightly more general than the patterns of Turney (2006). The morphological processing in PairClass (Minnen et al, 2001) is more sophisticated than in Turney (2006). Whereas the above mentioned approaches rely on additional knowledge sources, Turney (2006) developed a corpus based approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms. This aspect is most striking for ukWaC where the coverage is low and by only utilizing the single-occurrence sub-vectors we obtain a performance of 38.2% correct answers (the comparable "attributional" models reported in Turney, 2006, have an average performance of 31%). Vector-based distributional similarity methods have proven to be a valuable tool for a number of tasks on automatic discovery of semantic relatedness between words, like synonymy tests (Rapp, 2003) or detection of analogical similarity (Turney, 2006). On a structural level, the prediction of meta alternations shows a clear correspondence to analogy prediction as approached in Turney (2006) (carpenter:wood is analogous to mason:stone, but not to photograph:camera). We solve SAT analogies with a simplified version of the method of Turney (2006). We use this space to measure "relational" similarity (Turney, 2006) of concept pairs, e.g., finding that the relation between teachers and handbooks is more similar to the one between soldiers and guns, than to the one between teachers and schools. The Attr cells summarize the performance of the 6 models on the wiki table that are based on "attributional similarity" only (Turney, 2006). In particular, we need to develop a backoff strategy for unseen pairs in the relational similarity tasks, that, following Turney (2006), could be based on constructing surrogate pairs of taxonomically similar words found in the CxLC space. Some of the recent work on this problem includes that of Butnariu et al (2009), Girju (2007), Girju et al (2005), Kim and Baldwin (2005), Nakov (2008), Nastase et al (2006), Turney (2006), and Saghdha and Copestake (2009). The distinction between lexical and relational similarity for word pair comparison is recognised by Turney (2006) (he calls the former attributional similarity), though the methods he presents focus on relational similarity. Turney (2006) describes a method (Latent Relational Analysis) that extracts subsequence patterns for noun pairs from a large corpus, using query expansion to increase the recall of the search and feature selection and dimensionality reduction to reduce the complexity of the feature space.
Further Meta-Evaluation of Machine Translation j schroeder ed ac uk Abstract This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish. We report the translation quality of over 30 diverse translation systems based on a large-scale manual evaluation involving hundreds of hours of effort. We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the correlation with human judgments at both the system-level and at the sentence-level. We validate our manual evaluation methodology by measuring intraand inter-annotator agreement, and collecting timing information. Tests were run on the ACL WSMT 2008 test set (Callison-Burch et al, 2008). Spearman's rank correlation coefficients on the document (system) level between all the metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the frame work of the third (Callison-Burch et al, 2008), fourth (Callison-Burch et al, 2009), fifth (Callison Burch et al, 2010) and sixth (Callison-Burch et al, 2011) shared translation tasks. In the framework of the EuroMatrix project, a test set of general news data was provided for the shared translation task of the third workshop on SMT (Callison-Burch et al, 2008), called newstest 2008 in the following. This increased the BLEU score by about 1 BLEU point in comparison to the results reported in the official evaluation (Callison-Burch et al, 2008). PC Translator this year and also in Callison-Burch et al (2008). The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008). RWTH participated in this shared task with the two most promising metrics according to the previous experiments ,i.e. POSBLEU and POSF, and the detailed results can be found in (Callison-Burch et al, 2008). The Spearman's rank correlation coefficients on the document (system) level between the IBM1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al, 2008), fourth (Callison Burch et al, 2009) and fifth (Callison-Burch et al, 2010) shared translation tasks. The system level evaluation procedure follows WMT08 (Callison-Burch et al., 2008), which ranked each system submitted on WMT08 in three types of tasks: Rank: Human judges candidate sentence ranking order of quality. test set in WMT08 (Callison-Burch et al, 2008). For German, Spanish and Czech we use the news test sets proposed in (Callison-Burch et al 2010), for French and Italian the news test sets presented in (Callison-Burch et al 2008), for Arabic, Farsi and Turkish, sets of 2,000 news sentences extracted from the Arabic-English and English-Persian datasets and the SE-Times corpus. Thus, the human an notation for the WMT 2008 dataset was collected in the form of binary pairwise preferences that are considerably easier to make (Callison-Burch et al, 2008). Following Callison-Burch et al (2008), we assigned a score to each of the 11 MT systems based on how of ten its translations were judged to be better than or equal to any other system. Detailed token and type statistics can be found in Callison-Burch et al (2008). the model is tuned with mert (Bertoldi, et al) 5) the official test set from ACL WMT 2008 (Callison-Burch et al, 2008), consisting of 2000 sentences, is used as test set. We followed the benchmark assessment procedure in WMT and NIST MetricsMaTr (Callison-Burch et al, 2008, 2010), assessing the performance of the propose devaluation metric at the sentence level using ranking preference consistency, which also known as Kendall's rank correlation coefficient, to evaluate the correlation of the proposed metric with human judgments on translation adequacy ranking. Traditionally, human ratings for MT quality have been collected in the form of absolute scores on a five or seven-point Likert scale, but low reliability numbers for this type of annotation have raised concerns (Callison-Burch et al, 2008). For details, see Callison-Burch et al (2008). only ,i.e., how often the translations of the system were rated as better than the translations from other systems (Callison-Burch et al, 2008). Both resources are taken from the shared translation task in WMT-2008 (Callison-Burch et al, 2008).
Soft Syntactic Constraints for Hierarchical Phrased-Based Translation In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic analysis, versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data. A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then finding appropriate ways to soften that commitment. We present an approach that explores the tradeoff from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language. We obtain substantial improvements in performance for translation from Chinese and Arabic to English. In Marton and Resnik (2008), hiero variables were disambiguated with additional binary feature functions, with their weights optimized in standard MER training. Our stronger baseline employs, in addition, the fine-grained syntactic soft constraint features of Marton and Resnik (2008), here after MR08. Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. Marton and Resnik (2008) utilize the language linguistic analysis that is derived from parse tree to constrain the translation in a soft way. Chiang (2005), Marton and Resnik (2008) explored the constituent match/violation in hiero; Xiong (2009 a) added constituent parse tree based linguistic analysis into BTG model; Xiong (2009 b) added source dependency structure to BTG; Zhang (2009) added tree-kernel to BTG model. We believe a greater improvement can be expected if we apply our idea to finer-grained approaches that use constraints softly (Marton and Resnik (2008) and Cherry (2008)). By relaxing the distortion limit, we have left room for more sophisticated re-ordering models in conventional phrase-based decoders while maintaining a significant performance advantage over hierarchical systems (Marton and Resnik, 2008). Marton and Resnik (2008) revised this method by distinguishing different constituent syntactic types, and defined features for each type to count whether a phrase matches or crosses the syntactic boundary. Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008). Marton and Resnik (2008) add feature functions to penalize or reward non-terminals which cross constituent boundaries of the source sentence. Marton and Resnik (2008) and Cherry (2008) imposed syntactic constraints on the PBMT system by making use of prior linguistic knowledge in the form of syntax analysis. Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marton and Resnik, 2008) and TOFW (topological ordering of function words) stands for the method in (Setiawan et al., 2009). Early works reward/penalize spans that respect the syntactic parse constituents of an input sentence (Chiang, 2005), and (Marton and Resnik, 2008). On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries. Marton and Resnik (2008) find that some constituency types favor matching the source parse while others encourage violations. To compare with the CMVC, we also conduct experiments using (Marton and Resnik, 2008)'s XP+. Like (Marton and Resnik, 2008), we find that the XP+feature obtains a significant improvement of 1.08 BLEU over the baseline. Experiments show that our model achieves substantial improvements over baseline and significantly outperforms (Marton and Resnik, 2008)'s XP+. Marton and Resnik (2008) find that their constituent constraints are sensitive to language pairs.
A Unification Method For Disjunctive Feature Descriptions Although disjunction has been used in several unificationbased grammar formalisms, existing methods of unification have been unsatisfactory for descriptions containing large quantities of disjunction, because they require exponential time. This paper describes a method of unification by successive approximation, resulting in better average performance. It is well-known that disjunctive unification is NP complete (Kasper 1987). The unification of disjunctive feature structures is implemented according to Kasper's algorithm (Kasper, 1987). The general problem of unifying two disjunctive feature structures is non-polynomial in the number of disjunctions (Kasper, 1987). There is reason to hope that this will often be the case; while disjunction may be widespread in grammar ules and texical entries, Kasper (1987) observes that in his implementation "in the analysis of a particular sentence most features have a unique value, and some features are not present at, all". Kasper (1987) describes a technique which, for every set of n conjoined disjunctions, checks the consistency first of single disjuncts against the definite part of the description, then that of pairs and so on up ton-tuples for full consistency. This variation of the algorithm given in Kasper (1987) is closer to Propane's strategy, but the expansion to full DNF is itself in general an exponential process and will, when many disjunctions remain, be far more expensive than looking for a single realization. Unification time here is order n log n in the sizes n of the input structures (Kasper, 1987). Kasper (1987) and Eisele and DSrre (1988) have tackled this problem and proposed unification methods for disjunctive feature descriptions.
Using Corpus Statistics And WordNet Relations For Sense Identification Corpus-based approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneck. We show how knowledge-based techniques can be used to open the bottleneck by automatically locating training corpora. We describe a statistical classifier that combines topical context with local cues to identify a word sense. The classifier is used to disambiguate a noun, a verb, and an adjective. A knowledge base in the form of WordNet's lexical relations is used to automatically locate training examples in a general text corpus. Test results are compared with those from manually tagged training examples. Leacock et al (1998), Agirre and Lopezde Lacalle (2004), and Mihalcea and Moldovan (1999) propose a set of methods for automatic harvesting of web data for the purposes of creating sense annotated corpora. The following similarity measures were considered: two measures based on path lenghts between concepts (path and lch (Leacocket al, 1998)), three measures based on information content. Many corpus based methods have been proposed to deal with the sense disambiguation problem when given definition for each possible sense of a target word or a tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (Leacocketal., 1998), and semi-supervised sense disambiguation (Yarowsky, 1995). Many methods have been proposed to deal with this problem, including supervised learning algorithms (Leacock et al, 1998). Inspired by the work of (Leacock et al, 1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF. It was first suggested by Leacock et al (1998). Leacock et al (1998) attempted to exclude irrelevant or spurious examples by using only monosemous relatives in WordNet. Our approach is somewhat similar to the WordNet based approach of Leacock et al (1998) in that it acquires relatives of a target word from WordNetand extracts co-occurrence frequencies of the relatives from a raw corpus, but our system uses poly semous as well as monosemous relatives. Other notable unsupervised and semi-supervised approaches are those of McCarthy et al (2004), who combine ontological relations and untagged corpora to automatically rank word senses in relation to a corpus, and Leacock et al (1998) who use untagged data to build sense-tagged data automatically based on monosemous words. Inspired by the work of Leacock et al (1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF. Others, such as Leacock et al (1998) and Agirre and Martnez (2004b), used information from WordNet to construct queries which were used to retrieve training examples. In (Leacock et al, 1998), they used Bayesian approach for sense disambiguation of three ambiguous words. Another method, by (Leacock et al, 1998), normalizes path distance based on the depth of hierarchy. Various word-to-word similarity measures where applied, including distributional similarity (such as (Lin, 1998)), web-based co-occurrence statistics and WordNet based similarity measures (such as (Leacock et al, 1998)). Many supervised learning algorithms have been applied for WSD, ex. Bayesian learning (Leacock et al., 1998), exemplar based learning (Ng and Lee, 1996), decision list (Yarowsky, 2000), neural network (Towel and Voorheest, 1998), maximum entropy method (Dang et al., 2002), etc. The simple path measure computes the similarity between a pair of nodes in WordNet as the reciprocal of the number of edges in the shortest path between them, the LChmea sure (Leacock et al, 1998) also uses information about the length of the shortest path between a pair of nodes. To our knowledge, the methods of auto-acquiring sense-labeled instances include using parallel corpora like Gale et al. (1992) and Ng et al. (2003), extracting by monosemous relative of WordNet like Leacock et al. (1998), Mihalcea and Moldovan (1999), Agirre and Martínez (2004), Martínez et al. (2006) and PengYuan et al. (2008). The method we applied is based on the monosemous relatives of the target words (Leacock et al, 1998), and we studied some parameters that affect the quality of the acquired corpus, such as the distribution of the number of training instances per each word sense (bias), and the type of features used for disambiguation (local vs. topical). In (Leacock et al, 1998), the method to obtain sense-tagged examples using monosemous relatives is presented. This method is inspired in (Leacock et al, 1998).
Mildly Non-Projective Dependency Structures Syntactic parsing requires a fine balance between expressivity and complexity, so that naturally occurring structures can be accurately parsed without compromising efficiency. In dependency-based parsing, several constraints have been proposed that restrict the class of permissible structures, such as projectivity, planarity, multi-planarity, well-nestedness, gap degree, and edge degree. While projectivity is generally taken to be too restrictive for natural language syntax, it is not clear which of the other proposals strikes the best balance between expressivity and complexity. In this paper, we review and compare the different constraints theoretically, and provide an experimental evaluation using data from two treebanks, investigating how large a proportion of the structures found in the treebanks are permitted under different constraints. The results indicate that a combination of the well-nestedness constraint and a parametric constraint on discontinuity gives a very good fit with the linguistic data. First, well-nestedness is interesting as a generalization of projectivity (Marcus, 1967) while more than 23% of the 73088 dependency structures in the Prague Dependency Treebank of Czech (Hajic et al., 2001) are non-projective, only 0.11% are not well-nested (Kuhlmann and Nivre, 2006). Mildly non-projective trees are of both theoretical and practical interest, as they correspond to derivations in Lexicalized Tree Adjoining Grammar (Bodirsky et al, 2005) and cover the overwhelming majority of sentences found in tree banks for Czech and Danish (Kuhlmann and Nivre, 2006). In this paper, we consider all constraints and measures evaluated by Kuhlmann and Nivre (2006) with some minor variations. None of the constraints and measures in Kuhlmann and Nivre (2006) take into account levels of nodes explicitly. They confirm the findings of Kuhlmann and Nivre (2006): planarity seems to be almost as restrictive as projectivity; well-nestedness, on the other hand, covers large proportions of trees in all languages. This supports our theoretical results and confirms that properties of non-projective edges provide a more accurate as well as expressive means for describing non projective structures in natural language than the constraints and measures considered by Kuhlmann and Nivre (2006). Kuhlmann and Nivre (2006) compare several constraints on dependency structures and among the considered ones find well-nestedness to be in good accord with empirical data. Kuhlmann and Nivre (2006) claim that the constraint of well-nestedness seems to approximate well dependency structures occurring in natural language. Unlike acyclicity and the single head constraints, which impose restrictions on the dependency relation as such, projectivity constrains the interaction between the dependency relations and the order of the nodes in the sentence (Kuhlmann and Nivre, 2006). Following (Kuhlmann and Nivre, 2006), we call this edge degree to avoid confusion.  This work further corroborates Kuhlmann's work on Czech (PDT) for Hindi (Kuhlmann and Nivre, 2006). Recent work identifies two properties that appear particularly relevant to the characterization of graph-based dependency models of syntactic structure: the absence of interleaving substructures (well-nestedness) and a bound on a type of discontinuity (gap-degree <= 1) successfully describe more than 99% of the structures in two dependency tree banks (Kuhlmann and Nivre 2006) . Relevant results from Kuhlmann and Nivre (2006). See Kuhlmann and Nivre (2006) for the definition of edge degree. If we suppose that the characterization of dependency structures as reported by Kuhlmann and Nivre (2006) for Czech and Danish extends cross-linguistically, i.e. the dependency structures for natural language falls within the class of well-nested and gap degree <= 1 dependency structures, then MC-TAG appears to correspond to the wrong class of model-theoretic dependency structures. This keeps the corresponding graph drawings within the class of structures identified by Bodirsky et al (2005) as a model of TAG derivations, and by Kuhlmann and Nivre (2006) as empirically relevant. While the number of highly non-projective dependency structures is negligible for practical applications (Kuhlmann and Nivre, 2006), the rank can not easily be bounded. It is interesting to compare our approach with techniques for well-nested dependency trees (Kuhlmann and Nivre, 2006). Alternative notions of mildly non-projective dependency structures are explored in Kuhlmann and Nivre (2006).
Automatic Labeling Of Semantic Roles present a system for identifying the semantic relationships, or filled by constituents of a sentence within a semantic frame. Given an input sentence and a target word frame, the system labels constituents with either abstract semantic roles, such as or more domain-specific semantic roles, such as and The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and its position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible fillers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classifiers. Our system achieves 82% accuracy in identifying the semantic role of presegmented constituents. At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall. Our study also allowed us to compare the usefulness of different features and feature combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data. Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997). Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). The first SRL model on FrameNet was proposed by Gildea and Jurafsky (2002). Many of the features used are inspired by those used in semantic role labeling systems (Gildea and Jurafsky, 2002). Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem. Generally speaking, these SRL approaches use a two-stage architecture: i) argument identification; ii) argument classification, to solve the task as a derivation of Gildea and Jurafsky's pioneer work (Gildea and Jurafsky, 2002). The features are listed as follows: Path The path features are similar to the path feature which is designed by (Gildea and Jurafsky, 2002). Automatic Semantic Role Labeling (SRL) is a natural language processing (NLP) technique that maps sentences to semantic representations and identifies the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). Our domain-independent layer bears some similarity to other semantic tasks, most notably Semantic-Role Labeling (SRL) introduced in (Gildea and Jurafsky, 2002), in which identifying the predicate-argument structure is considered a preprocessing step, prior to assigning argument labels. While previous programs with similar goals (Gildea and Jurafsky, 2002) were statistics-based, this tool will be based completely on hand-coded rules and lexical resources. For example, simply adding whether each word is part of a noun or verb phrase using the hand annotated parse tree (the so-called GOV feature from (Gildea and Jurafsky, 2002)) improves the performance of our system from 83.95% to 85.8%. Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). Their features are usually extended from Gildea and Jurafsky (2002)'s work, which uses flat information derived from a parse tree. Gildea and Jurafsky (2002) showed that classification accuracy was improved by manually replacing FrameNet roles into 18 thematic roles. Some previous studies have employed this idea to remedy the data sparseness problem in the training data (Gildea and Jurafsky, 2002). The characteristics of x are: frame, frame evoking word, head word, content word (Surdeanu et al, 2003), first/last word, head word of left/right sister, phrase type, position, voice, syntactic path (directed/undirected/partial), governing category (Gildea and Jurafsky, 2002), WordNet supersense in the phrase, combination features of frame evoking word; head word, combination features of frame evoking word; phrase type, and combination features of voice; phrase type.  Gildea and Jurafsky (2002) is the only one applying selectional preferences in a real SRL task. This includes the majority of work on shallow semantic analysis (Gildea and Jurafsky, 2002, inter alia).
Generative Models For Statistical Parsing With Combinatory Categorial Grammar This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Gram mar (CCG) parser. These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations. According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to thefigures given by Collins (1999) for a lin guistically less expressive grammar. Incontrast to Gildea (2001), we find a significant improvement from modeling word word dependencies. These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). Hockenmaier and Steedman (2002) describe a generative model for CCG, which only requires a non-iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. The CCG grammar used by our system is read off the derivations in CCGbank, following Hockenmaier and Steedman (2002), meaning that the CCG combinatory rules are encoded as rule instances, together with a number of additional rules which deal with punctuation and type-changing. Few hand crafted, deep linguistic grammars achieve the coverage and robustness needed to parse large corpora (see (Riezler et al, 2002), (Burke et al, 2004) and (Hockenmaier and Steedman, 2002) for exceptions), and speed remains a serious challenge. We used Clark & Curran's wide coverage statistical parser (Clark and Curran, 2004) trained on CCG-bank, which in turn is derived from the Penn-Treebank (Hockenmaier and Steedman, 2002).    In order to obtain CCG derivations for all sentences in the ACE corpus, we used the CCG parser introduced in (Hockenmaier and Steedman,2002). Hockenmaier and Steedman (2002) describe a generative model of normal-form derivations. The CCG parser has been trained and tested on CCGbank (Hockenmaier and Steedman, 2002a), a tree bank of CCG derivations obtained from the Penn Treebank, from which we also obtain our training data. The feature set for the normal-form model is the same except that, following Hockenmaier and Steedman (2002), the dependency features are defined in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features. The best-performing model encodes word-word dependencies in terms of the local rule instantiations, as in Hockenmaier and Steedman (2002). For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). We focus on two parsing models: PCFG, the baseline of Hockenmaier and Steedman (2002) which treats the grammar as a PCFG (Table 1); and HWDep, a headword dependency model which is the best performing model of the parser. Hockenmaier and Steedman (2002) saw a similar effect. The model used by the CCG parser of Hockenmaier and Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. First, we review the dependency model proposed by Hockenmaier and Steedman (2002b).
Corpus Statistics Meet The Noun Compound: Some Empirical Results Tagged Dependency -.— Tagged Adjacency -e— L. Pattern 3 5 Training scheme (integers denote window widths) Figure 5: Accuracy using a tagged corpus for various training schemes data would make a difference, the freely available Brill tagger (Brill, 1993) was applied to the corpus. Since no manually tagged training data is available for our corpus, the tagger's default rules were used (these rules were produced by Brill by training on the Brown corpus). This results in rather poor tagging accuracy, so it is quite possible that a manually tagged corpus would produce better results. Three training schemes have been used and the tuned analysis procedures applied to the test set. Figure 5 shows the resulting accuracy, with accuracy values from figure 3 displayed with dotted lines. If anything, admitting additional training data based on the tagger introduces more noise, reducing the accuracy. However, for the pattern training scheme an improvement was made to the dependency model, producing the highest overall accuracy of 81%. 4 Conclusion The experiments above demonstrate a number of important points. The most general of these is that even quite crude corpus statistics can provide information about the syntax of compound nouns. At the very least, this information can be applied in broad coverage parsing to assist in the control of search. I have also shown that with a corpus of moderate size it is possible to get reasonable results without using a tagger or parser by employing a customised training pattern. While using windowed co-occurrence did not help here, it is possible that under more data sparse conditions better performance could be achieved by this method. The significance of the use of conceptual association deserves some mention. I have argued that without it a broad coverage system would be impossible. This is in contrast to previous work on conceptual association where it resulted in little improvement on a task which could already be performed. In this study, not only has the technique proved its worth by supporting generality, but through generalisation of training information it outperforms the equivalent lexical association approach given the same information. Amongst all the comparisons performed in these experiments one stands out as exhibiting the greatest contrast. In all experiments the dependency model provides a substantial advantage over the adjacency model, even though the latter is more prevalent in proposals within the literature. This result is in accordance with the informal reasoning given in section 1.3. The model also has the further commendation that it predicts correctly the observed proportion of left-branching compounds found in two independently extracted test sets. In all, the most accurate technique achieved an accuracy of 81% as compared to the 67% achieved by guessing left-branching. Given the high frequency of occurrence of noun compounds in many texts, this suggests that the use of these techniques in probabilistic parsers will result in higher performance in broad coverage natural language processing. 5 Acknowledgements This work has received valuable input from people too numerous to mention. The most significant contributions have been made by Richard Buckland, Robert Dale and Mark Dras. I am also indebted to Vance Gledhill, Mike Johnson, Philip Resnik, Richard Sproat, Wilco ter Stal, Lucy Vanderwende and Wobcke. Financial support is gratefully ack- 53 nowledged from the Microsoft Institute and the Australian Government. A concise review of this research area can be found in, for instance, Lauer (1995), which dates back to Finin (1980). As Lauer (1995) pointed out, using (partial) parsing of the text is too costly. Lauer (1995) compared a dependency model with adjacency models, and found that the dependency model is better. This is an extension of left branch preference in Lauer (1995).  Lauer (1995): adjacency 68.90 Lauer (1995): dependency 77.50 Best Altavista 78.68 Lauer (1995): tuned 80.70 Upper bound 81.50 Table 9: Performance comparison with the literature for compound bracketing 1993). Lauer (1995) proposes an unsupervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus. Lauer (1995) tested both the adjacency and dependency models on 244 compounds extracted from Grolier's encyclopedia, a corpus of 8 million words. Lauer (1995) is the first to propose and evaluate an unsupervised probabilistic model of compound noun interpretation for domain independent text. Lauer (1995) tested the model in (7) on 282 compounds that he selected randomly from Grolier's encyclopedia and annotated with their paraphrasing prepositions. The computational problem is thus deciding whether the three-word NC has a left or right-bracketing structure (Lauer, 1995). Mark Lauer (1995) only considered English noun compounds and applied a different disambiguation strategy based on word association scores. Several researchers have tackled the syntactic analysis (Lauer, 1995), (Pustejovsky et al, 1993), (Liber man and Church, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. We also present empirical observations on the distribution of the syntax and meaning of noun phrases on two different corpora based on two state-of-the-art classification tag sets: Lauers set of 8 prepositions (Lauer, 1995) and our list of 22 semantic relations. On the other hand, the majority of corpus statistics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995). They can vary from a few prepositions (Lauer, 1995) to hundreds or thousands specific semantic relations (Finin, 1980). (Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and dependency. (Lauer and Dras, 1994) and (Lauer, 1995) address the issue of structural ambiguity by developing a dependency model where instead of computing the acceptability of A [YZ] one would compute the acceptability of A [XZ]. (Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. This method is tested using a set of general English nominal compounds developed by (Lauer, 1995) as well as a set of nominal compounds extracted from MEDLINE abstracts.
Building a Large Annotated Corpus of English: The Penn Treebank Mitchell P. Marcus* University of Pennsylvania Mary Ann Marcinkiewicz~ University of Pennsylvania Beatrice Santorini t Northwestern University 1.  Of the 1600 IBM sentences that have been parsed (those available from the Penn Treebank [Marcus et al, 19931), only 67 overlapped with the IBM-manual treebank that was bracketed by University of Lancaster. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) provides annotations for the arguments and relation senses of one hundred pre-selected discourse connectives over the news portion of the Penn Treebank corpus (Marcus et al, 1993). Marcus et al (1993) found that direct annotation takes twice as long as automatic tagging plus correction, for part-of-speech annotation. Agirre et al (2008) applied two state-of-the-art tree bank parsers to the sense tagged subset of the Brown corpus version of the Penn Treebank (Marcus et al, 1993), and added sense annotation to the training data to evaluate their impact on parse selection and specifically on PP attachment. These data sets were based on the Wall Street Journal corpus in the Penn Treebank (Marcus et al, 1993). We have used Sections 02-21 of CCG bank (Hockenmaier and Steedman, 2007), the CCG version of the Penn Treebank (Marcus et al, 1993), as training data for the newspaper domain. The English sentences were parsed using a state-of-the-art statistical parser (Charniak, 2000) trained on the University of Pennsylvania Treebank (Marcus et al, 1993). The overall goal of the Penn Discourse Treebank (PDTB) is to annotate the million word WSJ corpus in the Penn TreeBank (Marcus et al, 1993 ) with a layer of discourse annotations. These remain fixed at all levels to the standard Penn-tree-bank set Marcus et al (1993). This paper describes an algorithm for detecting empty nodes in the Penn Treebank (Marcus et al, 1993), finding their antecedents, and assigning them function tags, without access to lexical information such as valency. In the Penn Treebank (Marcus et al, 1993), null elements, or empty categories, are used to indicate non-local dependencies, discontinuous constituents, and certain missing elements. This paper reports on our experience hand tagging the senses of 25 of the most frequent verbs in 12,925 sentences of the Wall Street Journal Treebank corpus (Marcus et al 1993). The sentences included in the gold standard were chosen at random from the BNC, subject to the condition that they contain a verb which does not occur in the training sections of the WSJ section of the PTB (Marcus et al, 1993). We use the Penn WSJ treebank (Marcus et al, 1993) for our experiments. The data used for our experiment consist of English sentences from the Penn Treebank project (Marcus et al, 1993) consisting of 10948 sentences and 259104 words. For instance, about 38% of verbs in the training sections of the Penn Treebank (PTB) (Marcus et al, 1993) occur only once the lexical properties of these verbs. The third corpus was Section 23 of the Wall Street Journal data in the Penn Treebank (Marcus et al, 1993). We have observed in several experiments that the number of SuperARVs does not grow significantly as training set size in creases; the moderate-sized Resource Management corpus (Price et al, 1988) with 25,168 words produces 328 SuperARVs, compared to 538 SuperARVs for the 1 million word Wall Street Journal (WSJ) Penn Treebank set (Marcus et al, 1993). In one experiment, it has to be performed on the basis of the gold-standard, assumed-perfect POS taken directly from the training data, the Penn Treebank (Marcus et al, 1993), so as to abstract from a particular POS tagger and to provide an upper bound. Our chunks and functions are based on the annotations in the third release of the Penn Treebank (Marcus et al, 1993).
The Automated Acquisit ion of Topic Signatures for Text Summarizat ion Chin -Yew L in  and  Eduard  Hovy In fo rmat ion  S(:i(umes I l l s t i tu te Un ivers i ty  of Southern  Ca l i fo rn ia Mar ina  del Rey, CA  90292, USA { cyl,hovy }C~isi.edu Abst rac t In order to produce, a good summary, one has to identify the most relevant portions of a given text.  This is similar to the idea of topic signature introduced in (Lin and Hovy 2000). As a byproduct of our approach, we also propose an extractive summarization model based on phrasal queries to select the summary-worthy sentences in the conversation based on query terms and signature terms (Lin and Hovy, 2000). We use a method described in (Lin and Hovy, 2000) in order to identify such term sand their associated weight. Lin and Hovy (2000) first introduced topic signatures which are topic relevant terms for summarization. In this regard, we use a method similar to Lin and Hovy (2000) to identify signature terms and subsequently use them to discard sentences that contain none or few such terms. There are signature terms for different topic texts (Lin and Hovy, 2000). Topic Signatures (TS) are word vectors related to a particular topic (Lin and Hovy, 2000). Such words are called signature terms in Lin and Hovy (2000) who were the first to introduce the log-likelihood weighting scheme for summarization. Our next steps will be to take a closer look at the following work: clustering of similar words (Lin, 1998), topic signatures (Lin and Hovy, 2000) and Kilgariff's sketch engine (Kilgarriff et al, 2004). We plan also to add other lexical functions to enrich our database with a ws. We plan to experiment. These are essentially scripts that provide information that is very useful in general reasoning as well as reasoning for NLP (e.g., Schank and Abelson 1977, Lin and Hovy 2000, Clark and Porter 2000). Topic Signatures (TS) are word vectors related to a particular topic (Lin and Hovy, 2000). To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al 2005), and word similarity lists (Hindle 1990). the input and all words of the summary Topic signatures are words highly descriptive of the input, as determined by the application of log likelihood test (Lin and Hovy, 2000). Under this approach, topic representations like those introduced in (Lin and Hovy, 2000) and (Harabagiu, 2004) are used to identify aset of text passages that are relevant to a user's domain of interest. A similar approach is explored in Biryukov et al (2005), which uses Topic Signatures (Lin and Hovy, 2000) constructed around the target individual's name to identify sentences to be included in the biography. In summarization, topic signatures are a set of terms indicative of a topic (Lin and Hovy, 2000). Topic Signatures (TS) are word vectors related to a particular topic (Lin and Hovy, 2000). Two methods are used: topic signature (Lin and Hovy, 2000): a topic signature is a family of related terms (topic, signature), where topic is the target concept and signature is a vector related ms. The topic in e formula is assigned with the domain ame. Our domain topic set contains 288 words extracted from the collection of student papers using topic-lexicon ex traction software; our feature (domain Word). The software extracts topic words based on topic signatures (Lin and Hovy, 2000), and was kindly provided by Annie Louis. Among them, query relevance, centroid (Radev et al, 2004) and signature term (Lin and Hovy, 2000) are most remarkable.
Improving Machine Translation Performance By Exploiting Non-Parallel Corpora We present a novel method for discovering parallel sentences in comparable, non-parallel corpora. We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other. Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus. Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available. Nevertheless, very little of that text seems to be genuinely parallel, although recent work (Munteanu and Marcu, 2005) indicates that true parallelism may not be required for some tasks, eg machine translation, in order to gain acceptable results. Sentence-level filter: The word-overlap filtering (Munteanu and Marcu, 2005) has been implemented: for a sentence pair (S, T) to be considered parallel the ratio of the lengths of the two sentences has to be smaller than two. The baseline uses only the length-based filtering and the coverage filtering without caching the coverage decisions (Munteanu and Marcu, 2005). Currently, we are working on a feature-rich approach (Munteanu and Marcu, 2005) to improve the sentence-pair selection accuracy. In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). Munteanu and Marcu (2005) use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles. Munteanu and Marcu (2005) filter out negative examples with high length difference or low word overlap (based on a bilingual dictionary). In both the binary classifier approach and the ranking approach, we use a Maximum Entropy classifier, following Munteanu and Marcu (2005). We use a feature set inspired by (Munteanu and Marcu, 2005), who defined features primarily based on IBM Model 1 alignments (Brown et al, 1993). One approach that begins to address this problem is the use of self-training, as in (Munteanu and Marcu, 2005). In addition, machine translation (MT) systems can be improved by training on sentences extracted from parallel or comparable documents mined from the Web (Munteanu and Marcu, 2005). Others extract parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006). Cross-lingual information retrieval methods (Munteanu and Marcu, 2005) and other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. Such classifiers have been used in the past to detect parallel sentence pairs in large collections of comparable documents (Munteanu and Marcu, 2005). The feature set we use is inspired by Munteanu and Marcu (2005) who define the features based on IBM Model-1 (Brown et al, 1993) alignments for source and target pairs. In selecting negative examples, we followed the same approach as in (Munteanu and Marcu, 2005): pairing all source phrases with all target phrases, but filter out the parallel pairs and those that have high length difference or a low lexical overlap, and then randomly select a subset of phrase pairs as the negative training set. Based on this observation, dynamic programming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. This paper extends previous work on extracting parallel sentence pairs from comparable data (Munteanu and Marcu, 2005). We select source-target sentence pairs (S, T) based on a ME classifier (Munteanu and Marcu, 2005). In addition, the sentence length filter in (Munteanu and Marcu, 2005) is used: the length ratio of source and target sentence has to be smaller than 2.
Introduction To The Special Issue On Word Sense Disambiguation: The State Of The Art  Two of the fundamental components of a natural language communication are word sense discovery (Jones, 1986) and word sense disambiguation (Ide and Veronis, 1998). Ide and Veronis (1998) present a very concise survey of the history of ideas used in word sense disambiguation; for a recent survey of the state-of-the-art one can refer to (Navigli, 2009). Several types of information are useful for WSD (Ide and Veronis 1998). The NLP field has gone through a very long tradition of algorithms designed for solving this problem (Ide and Veronis, 1998). Several efforts have been made to develop automatic WSD systems that can provide accurate sense tagging (Ide and Veronis, 1998), with a current emphasis on creating manually sense-tagged data for supervised training of statistical WSD systems, as evidenced by SENSEVAL-1 (Kilgarriff and Palmer, 2000) and SENSEVAL-2 (Edmonds and Cotton, 2001). Following Ide and Veronis (1998) we distinguish between data and knowledge-driven word sense disambiguation. Historically, after work on WSD had overcome so called early doubts (Ide and Veronis, 1998) in the 1960s, it was applied to various NLP tasks, such as machine translation, information retrieval, content and grammatical analysis and text processing. In general, following Ide and Veronis (1998) the various WSD approaches of the past can be divided into two types, i.e., data and knowledge-based approaches. A great deal of research on WSD has been done over the past decade (Ide and Veronis, 1998). Regardless of the technique that is used for WSD, the most important part of the process is the context in which the word appears (Ide and Veronis 1998). WSD is fundamental to natural language understanding and is a useful intermediate step for many other language processing tasks (Ide and Veronis, 1998). Following Ide and Veronis (1998) we can distinguish between data and knowledge-driven word sense disambiguation (WSD). After work on WSD had overcome so-called early doubts (Ide and Veronis, 1998) in the 1960s, it was applied to various NLP tasks, such as machine translation ,information retrieval, content and grammatical analysis and text processing. In general, following Ide and Veronis (1998) the various WSD approaches of the past can be divided into two types, i.e., data and knowledge-based approaches. Ide and Veronis (1998) argue that word sense ambiguity is a central problem for many established HLT applications (for example Machine Translation, Information Extraction and Information Retrieval). A review of methods for word sense disambiguation is presented by Ide and colleagues (Ideand Veronis, 1998). Unsupervised methods using dictionaries and corpora were proposed for monolingual WSD (Ide and Veronis, 1998).
Providing A Unified Account Of Definite Noun Phrases In Discourse discourse utterances that combine into of the discourse, namely, units discourse that are typically larger than a single sentence, but smaller than the complete discourse. However, the constituent structure is not determined solely by the linear sequence of utterances. It is common for two contiguous utterances to be members of different subconstituents of the discourse (as with breaks between phrases in the syntactic analysis of a sentence); likewise, it is common for two utterances that are not contiguous to be members of the same subconstituent. An individual subconstituent of a discourse exhibits both internal coherence and coherence with the other subconstituents. That is, discourses have been shown to of coherence. coherence to the ways in which the larger segments of discourse relate to one another. It depends on such things as the function of a discourse, its subject matter, and rhetorical schema 1977, 1981; Reichman, 19811. coherence refers to the ways in which individual sentences bind together to form larger discourse segments. It depends on such things as the syntactic structure of an utterance, and the use of referring expressions 1Sidner, 19811. The two levels of discourse coherence correspond to two of focusing—global centering. Participants are said to be globally focused on a set of entities relevant to the overall discourse. These entities may either have been explicitly introduced into the discourse or be sufficiently closely related to such entities to be considered implicitly in focus [Grosz, 19811. In contrast. centering refers to a more local focusing process, one relates to identifying the single entity that an individual utterance most centrally concerns (Sidner, 1979; Joshi and Weinstein, 19811. 44 The two levels of focusing/coherence have different effects on the processing of pronominal and nonpronominal definite noun phrases. Global coherence and focusing are major factors in the generation and interpretation of nonpronominal definite referring expressions.- Local coherence and centering have greater effect on the processing of pronominal expressions. In Section 5 we shall describe the rules governing the use of these kinds of expressions and shall explain why additional processing by the hearer (needed for drawing additional inferences) is involved when pronominal expressions are used to refer to globally focused entities or nonpronominal expressions are used to refer to centered entities. Many approaches to language interpretation have ignored these differences, depending instead on powerful inference mechanisms to identify the referents of referring expressions. Although such approaches may suffice, especially for well-formed texts, they are insufficient in general. In particular, such approaches will not work for generation. Here the relationships among focusing, coherence, and referring expressions are essential and must be explicitly provided for. Theories—and systems based on them--will generate unacceptable uses of referring expressions if they do not take these into 3. Centering and Anaphora theory, the centers of a sentence in a discourse serve to integrate that sentence into the discourse. Each S, has a single center, a set of centers, Cb(S) serves to link S to the preceding discourse, while Cf(S) provides a set of entities to which the succeeding discourse may be linked. To avoid confusion, the phrase 'the center' will he used to refer only to Cb(S). To clarify the notion of center, we will consider a number of discourses illustrating the various factors that combined in (abstractly) and in its identification in a discourse. In Section 5 we define center more precisely, show how it relates to Sidner's [19811 immediate focus and potential foci, and discuss how the linkages established by the centers of a sentence help to determine the degree of intelligibility of a discourse. We begin by showing that the center cannot be defined in syntactic terms alone. The interaction of semantics and centering is more complex and is discussed in Section 4. The following examples, drawn from Reinhart [19821, illustrate the point that the notion of center is not i.e., the syntax of a sentence S not determine which of its NPs (The differ in other respects also. Reichman [19811 and Grosz 110811 discuss some of these. attempts to incorporate focusing mechanisms in generation systems are described in [Appelt, 1981 and McKeown, 19821. can obviously affect the interpretation; for of this paper, it may be regarded as part of a for the use of this terminology discussed (la) Who did Max see yesterday? (lb) Max saw Rosa. (2a) Did anyone see Rosa yesterday? (2b) Max saw Rosa. (lb) and (2b) are identical, Cb(lb) Max and Cb(2b) is Rosa. This can be seen in part by noticing that saw Rosa' seems more natural than (Ib) *Max saw her' than (2b) (a fact consistent with the centering rule introduced in Section 5.) The subject NP is the center in one context, the object NP in the other. when the NP used to realize Cb(S) syntactically determined, the Cb(S) itself is not yet fully determined, for Cb(S) is typically not a linguistic entity (i.e., it is not a particular linguistic expression). Rosa, not 'Rosa' is the Cb(2b). Consider. the discourse: (3a) How is Rosa? (3b) Did anyone see her yesterday? saw her. Here, Cb(3c) is Rosa, but clearly would not be in other contexts where the expression 'her' still realized the backward-looking center of 'Max saw her.' This is seen most simply by considering the discourse that would result if &quot;How is Joan?' replaced (3a). In the discourse that resulted, Joan, not Rosa, would be the center of (3c). 4. Centering and Realization The interactions of semantic and pragmatic factors with centering and their effects on referring expressions are more complex than the preceding discussion suggests. In the examples given above, the NPs that realize Cb(S) also denote it, but this is not always the case: we used the term 'realize' in the above discussion advisedly. In this section, we consider two kinds of examples in which the center of a sentence is not simply the denotation of some noun phrase occurring in the sentence. First, we will examine several examples in which the choice of and interaction among different kinds of interpretations of definite noun phrases are affected by the local discourse context (i.e., centering). Second, the role of pragmatic factors in some problematic cases of referential uses of definite descriptions [Donnellan 19661 is discussed. 4.1. Realization and Value-Free and Value-Loaded Interpretations distinction between semantic denotation is necessary to treat the interaction between value-free and value-loaded interpretations [Barwise and Perry, 19821 of definite descriptions, as they occur in extended discourse. Consider, for example, the following sequence: (4a) The vice president of the United States is also president of the Senate. (4b) Historically, he is the president's key man in negotiations with Congress. to China, he handled tricky negotiations, so he prepared for this Cb(4b) and Cb(4b') are each realized by the anaphoric element 'he.' But (4b) expresses the same thing as 'Historically, the vice president of the United States is the president's key man in negotiations with Congress' (in which it is clear that no single individual vice president is being referred to) whereas (4b1 expresses the same thing as, 'As ambassador to China, the [person who is now] vice president of the United States handled many tricky negotiations,...' This can be accounted for by observing that 'the vice president of the United States' contributes both its value-free interpretation and its value-loading at the world type to Cf(4a). Cb(4b) is then the value-free interpretation and Cb(4b') is the valueloading, i.e., George Bush. In this example, both value-free and value-loaded interpretations are shown to stern from the same full definite noun phrase. It is also possible for the movement of the center from a value-free interpretation (for Cb(S)) to a value-loaded interpretation (for Cb of the next sentence)--or vice versa—to be accomplished solely with pronouns. That is, although (4b)-(4b1 is (at least for some readers) not a natural dialogue, similar sequences are possible. There appear to be strong constraints on the kinds of transitions that are allowed. In particular, if a given sentence forces either the value-free or value-loaded interpretation, then only that interpretation becomes possible in a subsequent sentence. However, if some sentence in a given context merely prefers one interpretation while allowing the other, then either one is possible in a subsequent sentence. For example, the sequence (6a) The vice president of the United States also president Senate. the president's key aan in negotiations vith Congress. in which 'he' may be interpreted as either value-free or (VL), may be followed by either of following As to China, he zany tricky negotiations. is required to be at least old. However, if we change (5b) to force the value-loaded interpretation, as in (5b&quot;), then only (5c) is possible. (Sb') Right now he is the president's key man in negotiations with Congress. Similarly, if (5b) is changed to force the value-free interpretation, as in (4b), then only (5c') is possible. If an intermediate sentence allows both interpretations but prefers one in a given context, then either is possible in the third sentence. A use with preference for a valueloaded interpretation followed by a use indicating the value-free interpretation is illustrated in the sequence: John thinks that the telephone is a toy. it every day. (11 preferred; ok) He doesn't realize that it is an invention changed the world. preference a value-free interpretation that is by value-loaded one is easiest to see in a dialogue situation; vice president of the United States is also president of the Senate. I he played some role in the House. (VF preferred; VL did, but that was before he was Realization and Use these examples, might appear that the concepts of value-free and value-loaded interpretation are identical to DonneIlan's 119661 attributive and referential uses of noun phrases. However, there is an important difference between these two distinctions. The importance to our theory is that the referential use of definite noun phrases introduces the need to take pragmatic factors (in particular speaker intention) into account, not just semantic factors. Donnellan [1966] describes the referential and uses of descriptions in the following way: 'A speaker who uses a definite description attributively in an assertion states something whoever or whatever is the so-and-so. speaker who uses a definite description referentially in an assertion, on the other hand, uses the description to enable his audience to pick out whom or what he is talking about and states something about that person or thing. In the first case the definite description might be said to occur essentially, for the speaker wishes to assert something about whatever or whoever fits that description; but in the referential use the definite description is merely one tool for doing a certain job--calling attention to a person or thing--and in general any other device for doing the same job, another description or a name, would do as well. In the attributive use, the attribute of being the so-and-so is all important, while it is not in the referential use.' The distinction Donnellan suggests can be formulated in terms of the different propositions a sentence S containing a definite description D may be used to express on different occasions of use. When D is used referentially, it contributes its denotation to the proposition expressed by 46 S; when it is used attributively, it contributes to the proposition expressed by S a semantic interpretation related to the descriptive content of D. The identity of this semantic interpretation is not something about which Donnellan is explicit. Distinct formal treatments of the semantics of definite descriptions in natural language would construe the appropriate interpretation differently. In semantic treatments based on possible worlds, the appropriate interpretation would be a (partial) function from possible worlds to objects; in the situation semantics expounded by Barwise and Perry, the appropriate interpretation is a (partial) function from resource to objects. As just described, the referential-attributive distinction appears to be exactly the distinction that Barwise and Perry formulate in terms of the value-loaded and valuefree interpretations of definite noun phrases. But this gloss omits an essential aspect of the referentialattributive distinction as elaborated by Donnellan. In view, a speaker may use referentially to refer to an object distinct from the semantic denotation of the description, and, moreover, to refer to an object even when the description has no semantic denotation. In one sense, this phenomenon arises within the framework of Barwise and Perry's treatment of descriptions. If we understand the semantic denotation of a description to be the unique object that satisfies the content of the description, if there is one, then Barwise and Perry would allow that there are referential uses of a description D that contribute objects other than the semantic denotation of D to the propositions expressed by uses of sentences in which D occurs. But this is only because Barwise and Perry allow that a description may be evaluated at a resource situation other than the complete situation in order to arrive at its denotation on a given occasion of use. Still, the denotation of the description relative to a given resource situation is the unique object in the situation that satisfies the description relative to that situation. The referential uses of descriptions that Donnellan gives of do seem to arise by evaluation of descriptions at alternative resource situations, but rather through the *referential intentions' of the speaker in his of the description. aspect of referential use is a rather a semantic phenomenon and is best analyzed in terms of the distinction between semantic reference and speaker's reference elaborated in Kripke [10771. the following discourses from Kripke [10771: 'any situation on which the speaker can focus attention is a potential candidate for a resource situation with to which the speaker may value load uses of descriptions. Such resource situations must contain a unique object satisfies description. husband is kind to her. NO. isn't. The can you're referring to isn't her husband. Her husband to He her isn't her husband. With (6a) and (7a), Kripke has in mind a case like the one discussed in Donnellan [19661, in which a speaker uses a description to refer to something other than the referent of that description, the unique thing that satisfies the description (if there is one). Kripke analyzes this case as an instance of the general phenomenon of a clash of intentions in language use. In the case at hand, the speaker has a general intention to use the description to refer to its semantic referent; his intention, distinct from general semantic intention, is to use it to refer to a particular individual. He incorrectly believes that these two intentions coincide this gives rise to a use of referring expression in which the speaker's reference reference are (The speaker's referent is presumably the woman's lover). From our point of view, the importance of the case resides in its showing that Cf(S) may include more than one entity, that is realized by a single NP in S. In this case, 'her husband' contributes both the husband and the lover to Cf(6a) and Cf(7a). This can be seen by observing that both discourses seem equally appropriate and that the backward-looking centers of (6b) and (7b) are the husband and the lover, respectively, realized by their anaphoric elements. Hence, the forward-looking centers of a sentence may be related not semantically but to the that realize Hence, the importance of the referential/attributive distinction from our point of view is that it leads to cases in which the centers of a sentence may be pragmatically rather than semantically related to the noun phrases that realize them. 5. Center Movement and Center Realization-- Constraints En the foregoing sections we have discussed a number of examples to illustrate two essential points. First, the noun phrase that realizes the backward-looking center of an utterance in a discourse cannot be determined from the of the utterance alone. Second, the relation c noun phrases centers solely a semantic a pragmatic relation. This discussion has proceeded at a rather intuitive level, without explicit elaboration of the framework we regard as appropriate for dealing with centering and its role in explaining discourse phenomena. Before going on to describe constraints on the realization relation that are, of course, several alternative explanations; e.g., the may believe that the description is more likely than to be interpreted correctly by the hearer. Ferreting out the in a given situation requires of belief and the like. A discussion of issues is beyond the this paper. 67 explain certain phenomena in discourse, we should be somewhat more explicit about the notions of center and realization. We have said that each utterance S in a discourse has associated with it a backward-looking center, Cb(S), and a set of forward-looking centers, Cf(S). What manner of objects are these centers? They are the sort of objects that can serve as the semantic interpretations of singular That is, either they are objects in the world (e.g., planets, people, numbers) or they are functions from possible worlds (situations, etc.) to objects in the world that can be used to interpret definite descriptions. That is, whatever serves to interpret a definite noun phrase can be a center. For the sake of concreteness in many of the examples in the preceding discussion, we have relied on the situation semantics of Barwise and Perry. The theory we are developing does not depend on this particular semantical treatment of definite noun phrases, but it does require several of the distinctions that treatment provides. In particular, our theory requires a semantical treatment that accommodates the distinction between interpretations of definite noun phrases that contribute their content to the propositions expressed by sentences in which they occur and interpretations that contribute only their denotation—in other words, the distinction between value-free and value-loaded interpretations. As noted, a distinction of this sort can be effected within the framework of 'possible-worlds' approaches to the semantics of natural language. In addition, we see the need for interpretations of definite noun phrases to be dependent on their discourse context. Once again, this is a feature of interpretations that is accommodated in the relational approach to semantics advocated by Barwise and Perry, but it might be accommodated within other as Given that Cb(S), the center of sentence S in a discourse, is the interpretation of a definite noun phrase, how does it become related to S? In a typical example, S will contain a full definite noun phrase or pronoun that realizes the center. The realization relation is neither nor pragmatic. For example, realizes c in cases where a definite description and is interpretation, or an object related to it by a 'speaker's reference.' More when is pronoun, the principles that which c are such that realizes c from neither semantics nor pragmatics exclusively. They are principles that must be elicited from the study of itself. A tentative formulation of some principles is given below. it is typical that, when a center of S, S an that realizes c, is by no means necessary. In particular, for sentences containing noun treatment of our theory we will consider centers that are realized by constituents in other syntactic categories. 119831 discusses some of these issues and compares several of with Montague semantics. phrases that express functional relations (e.g., 'the door,' 'the owner') whose arguments are not exhibited explicitly (e.g., a house is the current center, but so far its door nor its owner has been it is the case that such argument can be backward-looking center of the sentence. We are studying such and expect to integrate that into our theory of discourse The basic rule that constrains the realization of the backward-looking center of an utterance is a constraint on the speaker, namely: the Cb the current utterance is the same as the of the previous utterance, a pronoun should be are two things to about this rule. First, it not preclude using for other entities as long as one is used for the center. Second, it is not a hard but rather principle, like a Gricean maxim, that violated. However, such violations lead at best to in which the is forced to draw additional inferences. simple example, consider the following sequence, assuming at the outset that John is the center of the discourse: (8a) He called up Mike yesterday. (he=John) (8b) He vas annoyed by John's call.   Grosz et al (1983) proposed the centering model which is concerned with the interactions between the local coherence of discourse. The proposed method employs contextual features based on centering theory (Grosz et al, 1983) as well as conventional syntactic and word-based features. To resolve referring expression, one of the well known methods is centering theory developed by Grosz, Joshi, and Weinstein (Grosz et al (1983)).
Unsupervised Learning of Narrative Schemas and their Participants We describe an unsupervised system for learncoherent sequences or sets events whose arguments are filled with participant semantic roles defined over words jury, Unlike most previous work in event structure or semantic role learning, our system does not use supervised techniques, hand-built knowledge, or predefined classes of events or roles. Our unsupervised learning algorithm uses coreferring arguments in chains of verbs to learn both rich narrative event structure and argument roles. By jointly addressing both tasks, we improve on previous results in narrative/frame learning and induce rich frame-specific semantic roles. Second, recent work by Chambers and Jurafsky (2009) has induced narrative chains, i.e., likely sequences of events, by their use of similar head words. Chambers and Jurafsky (2009) describe a process to induce a partially ordered set of events related by a common protagonist by using an unsupervised distributional method to learn relations between events sharing co referring arguments, followed by temporal classification to induce partial order. Chambers and Jurafsky (2009, 2008) propose an unsupervised method for learning narrative schemas, chains of events whose arguments are filled with participant semantic roles defined over words. Inspired by Chambers and Jurafsky (2009) we acquire story plots automatically by recording events, their participants, and their precedence relationships as at tested in a training corpus. Our narrative schemas differ slightly from Chambers and Jurafsky (2009). The unsupervised setting has also been considering for the related problem of learning narrative schemas (Chambers and Jurafsky, 2009). Our previous work (Chambers and Jurafsky, 2009) learned situation-specific roles over narrative schemas, similar to frame roles in FrameNet (Baker et al, 1998). Inducing narrative schemas (Chambers and Jurafsky, 2009) may be viewed as a possible next step in a narrative induction pipeline, subsequent to disentangling the text comprising individual narrative threads. Similar observations can be made with respect to Chambers and Jurafsky (2009) and Kasch and Oates (2010), who also study a single discourse relation (narration), and are thus more limited in scope than the approach described here. It thus represents the basis for further experiments, e.g., with respect to the enrichment the BKB with information provided by Riaz and Girju (2010), Chambers and Jurafsky (2009) and Kasch and Oates (2010). Chambers and Jurafsky (2009) described a system that can learn (without supervision) the sequence of events described in a narrative, and Elson and McKeown (2009) created a platform that can symbolically represent and reason over narratives. Narrative structure has also been studied by representing character interactions as networks. Our system is based on a cluster-ranking model proposed by Rahman and Ng (2009), with novel semantic features based on recent research on narrative event schema (Chambers and Jurafsky, 2009). Chambers and Jurafsky (2009) present an unsupervised method for learning narrative schemas from news, i.e., coherent sets of events that involve specific entity types (semantic roles). McIntyre and Lapata (2010) create a story generation system that draws on earlier work on narrative schemas (Chambers and Jurafsky, 2009). Event structures in open domain texts are frequently highly complex and nested: a "crime" event can cause an "investigation" event, which can lead to an "arrest"event (Chambers and Jurafsky, 2009).
Name Tagging With Word Clusters And Discriminative Training We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus. membership is encoded in features that are incorporated in a discriminatively trained tagging model. Active learning is used to select training examples. We evaluate the technique for named-entity tagging. Compared with a state-of-the-art HMM-based name finder, the presented technique requires only 13% as much annotated data to achieve the same level of performance. Given a large annotated training set of 1,000,000 words, the technique achieves a 25% reduction in error over the state-of-the-art HMM trained on the same material. Miller et al (2004) describe a relevant technique for the latter. We chose to compare with ASO because it consistently outperforms cotraining (Blum and Mitchell, 1998) and clustering methods (Miller et al, 2004). By using prefixes of various lengths, we can produce clusterings of different granularities (Miller et al, 2004). Following Miller et al (2004), we use prefixes of the Brown cluster hierarchy to produce clusterings of varying granularity. We found that it was nontrivial to select the proper prefix lengths for the dependency parsing task; in particular, the prefix lengths used in the Miller et al (2004) work (between 12 and 20 bits) performed poorly in dependency parsing. As mentioned earlier, our approach was inspired by the success of Miller et al (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach. Miller et al (2004) use word clusters (Brown et al, 1992) learned from unlabeled text, resulting in a performance improvement of NER. Given the amount of training, our results are lower than in Miller et al (2004) (an F1 of 90 with less than 25K words of training).  Though all of them used the same hierarchical word clustering algorithm for the task of name tagging and reported improvements, we noticed that the clusters used by Miller et al (2004) were quite different from that of Ratinov and Roth (2009) and Turian et al (2010). It has shown promise in improving the performance of many tasks such as name tagging (Miller et al, 2004), semantic class extraction (Lin et al, 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998). This method has been shown to be quite successful in named entity recognition (Miller et al. 2004) and dependency parsing (Koo et al, 2008). Previous approaches ,e.g., (Miller et al 2004) and (Koo et al 2008), have all used the Brown algorithm for clustering (Brown et al 1992). Miller et al, (2004) augmented name tagging training data with hierarchical word clusters and encoded cluster membership in features for improving name tagging. This simple solution has been shown effective for named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008). Semi-supervised approach has been successfully applied to named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008).  (Miller et al, 2004) cut the BCluster tree at a certain depth k to simplify the tree, meaning every leaf descending from a particular internal node at level k is made an immediate child of that node. In addition, Miller et al (2004) and Freitag (2004) employ distributional and hierarchical clustering methods to improve the performance of NER within a single domain. One approach has been that proposed in both Miller et al (2004) and Freitag (2004), uses distributional clustering to induce features from a large corpus, and then uses these features to augment the feature space of the labeled data.
Effects of Adjective Orientation and Gradability on Sentence Subjectivity Vas i le ios  Hatz ivass i log lou Depar tment  o1 Computer  Sc ience Co lumbia  Un ivers i l y New York,  NY  10027 vh@cs ,  co lumbia ,  edu Janyce  M.  Wiebe Depar tment  o f  Computer  Sc ience New Mex ico  State Un ivers i ty Las  Cruces ,  NM 88003 w iebe@cs ,  nmsu.  Unlike nouns, many adjectives are inherently subjective, and the number of adjectives in texts correlates with human judgements of their subjectivity (Hatzivassiloglou and Wiebe, 2000). Words that encode a desirable state (e.g., beautiful) have a positive SO, while words that represent undesirable states (e.g. absurd) have a negative SO (Hatzivassiloglou and Wiebe, 2000). Based on (Hatzivassiloglou and Wiebe, 2000) and (Turney, 2002), we consider four types of structures (as shown in Table 5) during sentiment phrase extraction. Separately, Hatzivassiloglou and Wiebe (2000) report a statistical correlation between the number of adjectives in a text and human judgments of subjectivity. Subsequently, Hatzivassiloglou and Wiebe (2000) showed that automatically detected gradable adjectives are a useful feature for subjectivity classification, while Wiebe (2000) introduced lexical features in addition to the presence/absence of syntactic categories. In addition, the presence of semantically oriented (positive and negative) words in a sentence is an indicator that the sentence is subjective (Hatzivassiloglou and Wiebe, 2000).
Minimized Models for Unsupervised Part-of-Speech Tagging We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values. We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems in both settings. Bayesian methods (85.2% from Goldwater and Griffiths (2007), who use a trigram model) and close to the best accuracy reported on this task (91.8% from Ravi and Knight (2009b), who use an integer linear program to minimize the model directly). Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data. The strategies employed in Ravi and Knight (2009) and Baldridge (2008) are complementary. Applying the approach of Ravi and Knight (2009) naively to CCG supertagging is intractable due to the high level of ambiguity. However, the original IP method of Ravi and Knight (2009) is intractable for supertagging, so we propose a new two-stage method that scales to the larger tag sets and data involved. This stage uses the original minimization formulation for the supertagging problem I Poriginal, again using an integer programming method similar to that proposed by Ravi and Knight (2009). Ravi and Knight (2009) exploited this to iteratively improve their POS tag model: since the first minimization procedure is seeded with a noisy gram mar and tag dictionary, iterating the IP procedure with progressively better grammars further improves the model. Minimized models for EM-HMM with 100 random restarts (Ravi and Knight, 2009). (Ravi and Knight, 2009) focus on the POS tag collection to find the smallest POS model that explain the data. The abuse of the rare tags is presented in Table 5 in a similar fashion with (Ravi and Knight, 2009). The IP+EM system constructs a model that describes the data by using minimum number of bi gram POS tags then uses this model to reduce the dictionary size (Ravi and Knight, 2009). Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side. A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types. To avoid the need for manually pruning the tag dictionary, Ravi and Knight (2009) proposed that low-probability tags might be automatically filtered from the tag dictionary through a model minimization procedure applied to the raw text and constrained by the full tag dictionary. Ravi and Knight (2009) use a dictionary and an MDL inspired modification to the EM algorithm. This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008).  Columns labeled 973k train describe models trained on the subset of 973k tokens used by Ravi and Knight (2009). Ravi and Knight (2009) employs integer linear programming to select a minimal set of parameters that can generate the test sentences, followed by EM to set parameter values. For example, the work of Ravi and Knight (2009) minimizes the number of possible tag-tag transitions in the HMM via an integer program, hence discarding unlikely transitions that would confuse the model.
A Polynomial-Time Algorithm For Statistical Machine Translation Hong Kong's stabilize boom is us life styles's pillar. Our prosperity and stability underpin our way of life. 44NMINVitta (Ben gang de jing ji qian jing yu zhang gu6, te bie shi guang dong sheng de jing ji qian jing xi xi xiang guan.) Hong Kong's economic foreground with China, particular Guangdong province's economic foreground vitally interrelated. Our economic future is inextricably bound up with China, and with Guangdong Province in particular. firdtittifirg.g. (WO win quin zhi chi ta de yi jian.) I absolutely uphold his views. I fully support his views. Mt (Zhe xie an pai ke jia qiing wo men ri hOu wei chi jin r6ng wen ding de neng 11.) These arrangements can enforce us future kept financial stabilization's competency. These arrangements will enhance our ability to maintain monetary stability in the years to come. tWt, ftRAMT.A1t: ZOM, ftrig*IliTtAtIttM3R/OIAMPfiEfissi R. wa zai ke yl ken ding de shuO, wO men jiang hul ti gong wei di dao ge xiang zhii yao mu biao suO xil de jing fei.) However, I now can certainty's say, will provide for us attain various dominant goal necessary's current expenditure. The consultation process is continuing but I can confirm now that the necessary funds will be made available to meet the key targets. Figure 4: Example translation outputs. translation accuracy was performed on a random sample drawn from Chinese sentences of fewer than 20 words from the parallel corpus, the results of which are shown in Figure 3. We have judged only whether the correct meaning (as determined by the corresponding English sentence in the parallel corpus) is conveyed by the translation, paying particular attention to word order, but otherwise ignoring morphological and function word choices. For comparison, the accuracies from the A*-based systems are also shown. There is no significant difference in the accuracy. Some examples of the output are shown in Figure 4. On the other hand, the new algorithm has indeed proven to be much faster. At present we are unable to use direct measurement to compare the speed of the systems meaningfully, because of vast implementational differences between the systems. However, the order-of-magnitude improvements are immediately apparent. In the earlier system, translation of single sentences required on the order of hours (Sun Sparc 10 workstations). In contrast the new algorithm generally takes less than one minute—usually substantially less—with no special optimization of the code. 6 Conclusion We have introduced a new algorithm for the runtime optimization step in statistical machine translation systems, whose polynomial-time complexity addresses one of the primary obstacles to practicality facing statistical MT. The underlying model for the algorithm is a combination of the stochastic BTG and bigram models. The improvement in speed does not appear to impair accuracy significantly. We have implemented a version that accepts ITGs rather than BTGs, and plan to experiment with more heavily structured models. However, it is important to note that the search complexity rises exponentially rather than polynomially with the size of the grammar, just as for context-free parsing (Barton, Berwick, and Ristad, 1987). This is not relevant to the BTG-based model we have described since its grammar size is fixed; in fact the BTG's minimal grammar size has been an important advantage over more linguistically-motivated ITG-based models. 157 We have also implemented a generalized version that accepts arbitrary grammars not restricted to normal form, with two motivations. The pragmatic benefit is that structured grammars become easier to write, and more concise. The expressiveness benefit is that a wider family of probability distributions can be written. As stated earlier, the normal form theorem guarantees that the same set of shapes will be explored by our search algorithm, regardless of whether a binary-branching BTG or an arbitrary BTG is used. But it may sometimes be useful to place probabilities on n-ary productions that vary with n in a way that cannot be expressed by composing binary productions; for example one might wish to encourage longer straight productions. The generalized version permits such strategies. Currently we are evaluating robustness extensions of the algorithm that permit words suggested by the language model to be inserted in the output sentence, which the original A* algorithms permitted. Acknowledgements Thanks to an anonymous referee for valuable comments, and to the SILC group members: Xuanyin Xia, Eva Wai-Man Fong, Cindy Ng, Hong-sing Wong, and Daniel Ka-Leung Chan. Many thanks also to Kathleen McKeown and her group for discussion, support, and assistance. As with the pure statistical translation model described by Wu (1996) (in which a bracketing transduction grammar models the channel), alternative hypotheses compete probabilistically, exhaustive search of the translation hypothesis pace can be performed in polynomial time, and robustness heuristics arise naturally from a language-independent inversion transduction model. A step was taken by Wu (Wu, 1996) who introduced a polynomial-time algorithm for the runtime search for an optimal translation. Subsequently, a method was developed to use a special case of the ITGR the aforementioned BTGR for the translation task itself (Wu, 1996). Wu (Wu, 1996) experimented with Chinese-English translation, while this paper experiments with English-Chinese translation. To tackle the problem of glue rules, He (2010) extended the HPB model by using bracketing transduction grammar (Wu, 1996) instead of the monotone glue rules, and trained an extra classifier for glue rules to predict reorderings of neighboring phrases. Wu (1996) presented a polynomial-time algorithm for decoding ITG combined with an m-gram language model. The of ITG decoding algorithm of Wu (1996) can be viewed as a variant of the Viterbi parsing algorithm for alignment selection. To do bigram-integrated decoding, we need to augment each chart item (X,i, j) with two target-language boundary words u and v to produce a bigram-item like u ··· v Xi j, following the dynamic programming algorithm of Wu (1996). NP (1) VPP-VP (2), NP (1) VPP-VP (2) VPP-VP? VP (1) PP (2), PP (2) VP (1) In this case m-gram integrated decoding can bedone in O (|w|3+4 (m? 1)) time which is much lower order polynomial and no longer depends on rule size (Wu, 1996), allowing the search to be much faster and more accurate facing pruning, as is evidenced in the Hiero system of Chiang (2005) where he restricts the hierarchical phrases to be a binary SCFG. In (Wu, 1996) the baseline ITG constraints were used for statistical machine translation. 3.1, but here, we use monotone translation hypotheses of the full IBM Model 4 as initialization, whereas in (Wu, 1996) a single-word based lexicon model is used. BTG is widely adopted in SMT systems, because of its good trade-off between efficiency and expressiveness (Wu, 1996). Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically. See Wu (1996) or Melamed (2004) for a detailed exposition. Reordering restrictions for word-based SMT decoders were introduced by (Berger et al, 1996) and (Wu, 1996). (Wu,1996) propose using contiguity restrictions on the reordering. It is also relevant since it can form the basis of a decoder for inversion transduction grammar (Wu, 1996). It follows the decoding-as-parsing idea exemplified by Wu (1996) and Yamada and Knight (2002). It also ensures the compatibility of projective parsing algorithms with many important natural language processing methods that work within a bottom-up chart parsing framework ,including information extraction (Miller et al, 2000) and syntax-based machine translation (Wu, 1996). To integrate with a bigram language model, we can use the dynamic-programming algorithms of Och and Ney (2004) and Wu (1996) for phrase-based and SCFG-based systems, respectively, which we may think of as doing a finer-grained version of the deductions above.
An Unsupervised Morpheme-Based HMM For Hebrew Morphological Disambiguation Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text. When the word is ambiguous (there are several possible analyses for the word), a disambiguation procedure based on the word context must be applied. This paper deals with morphological disambiguation of the Hebrew language, which combines morphemes into a word in both agglutinative and fusional ways. We present an unsupervised stochastic model – the only resource we use is a morphological analyzer – which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language. We present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules (which are quite restricted in Hebrew) helps in the disambiguation. We adapt HMM algorithms for learning and searching this text representation, in such a way that segmentation and tagging can be learned in parallel in one step. Results on a large scale evaluation indicate that this learning improves disambiguation for complex tag sets. Our method is applicable to other languages with affix morphology. Our analyzer provides segmentation and PoS tags with 92.5% accuracy and full morphology with 88.5% accuracy (Adler and Elhadad, 2006). Recent PoS taggers and morphological analyzers for Hebrew (Adler and Elhadad, 2006) address this issue and provide for each word not only the PoS, but also full morphological features, such as Gender, Number, Person, Construct, Tense, and the affixes' properties. Since it cannot predict its own segmentation, automatic segments and tags are predicted using the system of Adler and Elhadad (2006). Recently, Adler and Elhadad (2006) presented an unsupervised, HMM-based model for Hebrew morphological disambiguation, using a morphological analyzer as the only resource. Our best result, 91.44% accuracy, reflects a reduction of 25% in error rate compared to the previous state of the art (Adler and Elhadad, 2006), and almost 40% compared to the baseline. Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al (2005), Adler and Elhadad (2006), Shacham and Wintner (2007), and achieved good results (the best segmentation result so far is around 98%). A possible probabilistic model for assigning probabilities to complex analyses of a surface form may be P (REL, VB|fmnh, context)= P (REL|f) P (VB|mnh, REL) P (REL, VB| context) and indeed recent sequential disambiguation models for Hebrew (Adler and Elhadad, 2006) and Arabic (Smith et al, 2005) present similar models. Adler and Elhadad (2006) presented an HMM-based approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of He brew, based on lattice parsing. Since these parsers cannot choose their own tags, automatically predicted segments and tags are provided by Adler and Elhadad (2006). (Adler and Elhadad,2006) perform Hebrew morphological disambiguation using an unsupervised morpheme-based HMM, but they report lower scores than those achieved by our model. In a preliminary experiment, the POS tagger (Adler and Elhadad, 2006) accuracy on the Responsa Corpus was less than 60%, while the accuracy of the same tagger on modern Hebrew corpora is ~90% (Bar Haim et al, 2007). Adler and Elhadad (2006) present a lattice-based modification of the BaumWelch algorithm to handle this segmentation ambiguity. Some are language independent (see e.g. Attia et al (2010), Adler et al (2008)) while others focus on specific languages (see e.g. Habash and Rambow (2005, 2007) and Marsi et al (2005) on Arabic and Adler and Elhadad (2006) on Hebrew, another Semitic language with similar morphological structure).
The Importance Of Supertagging For Wide-Coverage CCG Parsing This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis. The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training. It also dramatically increases the speed of the parser. We show that large increases in speedcan be obtained by tightly integrating the su pertagger with the CCG grammar and parser.This is the first work we are aware of to success fully integrate a supertagger with a full parser which uses an automatically extracted grammar.We also further reduce the derivation space us ing constraints on category combination. The result is an accurate wide-coverage CCG parserwhich is an order of magnitude faster than comparable systems for other linguistically moti vated formalisms. The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. The parser used in this paper is described in Clark and Curran (2004b). A Maximum Entropy CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. The parsing results in Clark and Curran (2004b) rely on a supertagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). However, the scores in Clark and Curran (2004b) give an indication of how supertagging accuracy corresponds to overall dependency recovery. This is particularly true of the C&C parser, which exploits CCG? s lexicalisation to divide the parsing task between two integrated models (Clark and Curran, 2004). The dependency parsing model of Clark and Curran (2004b) is extended to exploit this partial training data. The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages: first, CCG lexical categories are assigned to the words in the sentence, and then the categories are combined by the parser (Clark and Curran, 2004a). The partial training method uses the log-linear dependency model described in Clark and Curran (2004b), which uses sets of predicate-argument dependencies, rather than derivations, for training. Clark and Curran (2004b) describes two log-linear parsing models for CCG: a normal-form derivation model and a dependency model. We define the probability of a dependency structure as the sum of the probabilities of all those derivations leading to that structure (Clark and Curran, 2004b). Clark and Curran (2004b) describes the training procedure for the dependency model, which uses a discriminative estimation method by maximising the conditional likelihood of the model given the data (Riezler et al, 2002). Clark and Curran (2003) shows how the sum over the complete derivation space can be performed efficiently using a packed chart and the inside-outside algorithm, and Clark and Curran (2004b) extends this method to sum over all derivations leading to a gold-standard dependency structure. The definitions of the objective function (4) and the gradient (5) for training remain the same in the partial-data case; the only differences are that (pi) is now defined to be those derivations which are con sis tent with the partial dependency structure pi, and the gold-standard dependency structures pij are the partial structures extracted from the gold-standard lexical category sequences. Clark and Curran (2004b) gives an algorithm for finding all derivations in a packed chart which produce a particular set of dependencies. The lexical category sequences for the sentences in 2-21 can easily be read off the CCGbank derivations. The derivations licenced by a lexical category sequence were created using the CCG parser described in Clark and Curran (2004b). The training data for the dependency model was created by first supertagging the sentences in sections 2-21, using the supertagger described in Clark and Curran (2004b). The average number of categories Since our training method is intended to be applicable in the absence of derivation data, the use of such rules may appear suspect. Approximate memory usage in each case was 17.6 GB of RAM.The dependency model uses the same set of features described in Clark and Curran (2004b): dependency features representing predicate-argument dependencies (with and without distance measures); rule instantiation features encoding the combining categories together with the result category (wit hand without a lexical head); lexical category features, consisting of word category pairs at the leaf nodes; and root category features, consisting of head word category pairs at the root nodes. The CCG parsing consists of two phases: first the supertagger assigns the most probable categories toeach word, and then the small number of combinatory rules, plus the type-changing and punctuation rules, are used with the CKY algorithm to build a packed chart. We use the method described in Clark and Curran (2004b) for integrating the supertagger with the parser: initially a small number of categories is assigned to each word, and more categories are requested if the parser can not find a spanning analysis.
Overview of BioNLP Shared Task 2011 The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams. Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects. In the current BioNLP 11 Shared Task1 (Kim et al, 2011), we demonstrate its generalizability to different event extraction tasks by applying what is, to a large extent, the same system to every single task and subtask. The BioNLP Shared Task 2011 (BioNLP ST 11) (Kim et al, 2011a), the follow-up event to the BioNLP 09 Shared Task (Kim et al, 2009), was organized from August 2010 (sample data release) to March 2011. For compatibility with the BioNLP ST 09 and its repeat as the GE task in 2011 (Kim et al, 2011b), the REL task training/development/test set division of the GENIA corpus abstracts matches that of the BioNLP ST? 09 data. To date, most approaches to the BioNLP event extraction task (Kim et al, 2011a) use a single model to produce their output. We describe the Stanford entry to the BioNLP2011 shared task on biomolecular event extraction (Kim et al, 2011a). The Infectious Diseases (ID) task of the BioNLPShared Task 2011 (Kim et al, 2011a) is an information extraction task focusing on the biomolecular mechanisms of infectious diseases. These are a superset of those targeted in the BioNLP ST 09 and its repeat, the 2011 GE task (Kim et al, 2011b). Nevertheless, extraction performance for the top systems is comparable to the state-of-the-art results for the established BioNLP ST 09 task (Miwa et al, 2010) as well as its repetition as the 2011 GE task (Kim et al, 2011b), where the highest overall result for the primary evaluation criteria was also 56% F score for the FAUST system (Riedel et al, 2011). The five top-ranking systems participated also in the GE task (Kim et al, 2011b), which involves asubset of the ID extraction targets. We participated in the BioNLP-ST 2011 (Kim et al, 2011a), and applied a graph matching-based approach (Liu et al, 2010) to tackling the Task 1 of the GE NIA event extraction (GE) task (Kim et al, 2011b), and the core task of the Epigenetics and Post-translational Modifications (EPI) task (Ohta et al, 2011), two main tasks of the BioNLP-ST 2011. This section presents our results on the GE and the EPI tasks (Kim et al, 2011b; Ohta et al, 2011) respectively. Different experimental methods in processing the obtained event rules are described for the purpose of improving the precision of both tasks and increasing the recall of the EPI task. The Epigenetics and Post-translational Modifications (EPI) task is a shared task on event extraction from biomedical domain scientific publications, first introduced as a main task in the BioNLP Shared Task 2011 (Kim et al, 2011a). The EPI task focuses on events relating to epigenetic change, including DNA methylation and hi stone methylation and acetylation (see e.g. Basic modification events are defined similarly to the PHOSPHORYLATION event type targeted in the 09 and the 2011 GE and ID tasks (Kim et al, 2011b; Pyysalo et al, 2011b), with the full task extending previously defined arguments with two additional ones, Sidechain and Contextgene. These choices may be motivated in part by the success of systems using the tools in the previous shared task and the availability of the analyses as supporting resources (Stenetorp et al., 2011). Despite the availability of PTM and DNAmethylation resources other than those specifically introduced for the task and the PHOSPHORYLATION annotations in the GE task (Kim et al, 2011b), no participant chose to apply other corpora for training. The BioNLP 2011 Shared Task ((Kim et al, 2011)) series generalized this defining a series of tasks involving more text types, domains and target event types. For example, Miwa et al (Miwa et al, 2010b) reported a significant improvement with binding events, achieving 50% of performance level. The task introduced in BioNLP-ST 2009 was re named to Genia event (GE) task, and was hosted again in BioNLP-ST 2011, which also hosted four other IE tasks and three supporting tasks (Kim et al, 2011).  There are 4 event extraction tracks: in addition to the GENIA track that again focuses on transcription factors (Kim et al, 2011b), the epigenetics and post translational modification track (EPI) focuses on events relating to epigenetic change, such as DNAmethylation and hi stone modification, as well asother common post-translational protein modifications (Ohta et al, 2011), whereas the infectious diseases track (ID) focuses on bio-molecular mechanisms of infectious diseases (Pyysalo et al, 2011a). This paper presents the UMass entry to the BioNLP 2011 shared task (Kim et al, 2011a). For Genia (GE) Task 1 (Kim et al, 2011b) we achieve the second best results.
Word-Sense Disambiguation Using Statistical Methods We describe a statistical technique for assigning senses to words. An instance of a word is assigned a sense by asking a question about the context in which the word appears. The question is constructed to have high mutual information with the translation of that instance in another language. When we incorporated this method of assigning senses into our statistical machine translation system, the error rate of the system decreased by thirteen percent. As a result, we can find complementary cues in the two languages that help to disambiguate named entity mentions (Brown et al, 1991). It has been equally shown that collocations are useful in a range of other applications, such as word sense disambiguation (Brown et al, 1991) and parsing (Alshawi and Carter, 1994). Brown et al (1991) and Gale et al (1992) used the translations of the ambiguous word in a bilingual corpus as sense tags. Brown et al (1991) proposed a WSD algorithm to disambiguate English translations of French target words based on the single most informative context feature. The idea of obtaining linguistic information about a text in one language by exploiting parallel or comparable texts in another language has been explored in the field of Word Sense Disambiguation (WSD) since the early 90's, the most representative works being (Brown et al, 1991), (Gale et al, 1992), and (Dagan and Itai, 1994). TMs have been used for statistical machine translation (Bergeretal., 1996), word alignment of a translation corpus (Melamed, 2000), multilingual document retrieval (Franz et al, 1999), automatic dictionary construction (Resnik and Melamed, 1997), and data preparation for word sense disambiguation programs (Brown et al, 1991). Mutual information has been positively used in many NLP tasks such as collocation analysis (Church and Hanks, 1989), terminology extraction (Damerau, 1993), and word sense disambiguation (Brown et al, 1991). As a somewhat radical alternative to taxonomical relationships, other ways of measuring semantic similarity based on distributional evidence have been put forward in the literature (see, among others, Brown et al 1991, Gale et al 1992, Pereira and Tishby 1992), which emphasise the role played by context in this game.  Related Work Brown et al (1991) is the first to have explored statistical methods in word sense disambiguation in the context of machine translation. In contrast with Brown et al (1991), our approach incorporates the predictions of state-of-the art WSD models that use rich contextual features for any phrase in the input vocabulary. The reduction of input data requirements offers a significant advantage compared with methods such as those presented in Brown et al (1991), Gale et al (1992), Yarowsky (1995), and Karol and Edelman (1996) where strong reliance on statistical techniques for the calculation of word and context similarity commands large source corpora. Brown et al (1991) pioneered the use of statistical WSD for translation, building a translation model from one million sentences in English and French. The first statistical model of WSD was built by Brown et al (1991). A historical approach (Brown et al 1991) uses bilingual corpora to perform unsupervised word alignment and determine the most appropriate translation for a target word from a set of contextual features.
Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data. In this paper, we present a syntax-based statistical matranslation system based on a probabilistic synchronous dependency insertion grammar. Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees. We first introduce our approach to inducing such a grammar from parallel corpora. Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer. We introduce a polynomial time decoding algorithm for the model. We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software. The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality. Dependency trees capture important aspects of functional relationships between words and have been shown to be useful in many applications including relation extraction (Culotta and Sorensen, 2004), paraphrase acquisition (Shinyama et al, 2002) and machine translation (Ding and Palmer, 2005). A syntax-based system might be able to check this sort of agreement if it produced a target-side dependency tree as in Ding and Palmer (2005). Ding and Palmer (2005) improve over word-based MT baseline with a formalism very similar to STSG. Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). Ding and Palmer (2005) introduced a version of probabilistic extension of Synchronous Dependency Insertion Grammars (SDIG) to deal with the pervasive structure divergence. Dependency trees have been used in a variety of NLP applications, such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al, 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al, 2004).  Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insertion grammar. It is not rare to see dependency relations used as features, in tasks such as relation extraction (Bunescu and Mooney, 2005) and machine translation (Ding and Palmer, 2005). Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). As mentioned in (Ding and Palmer, 2005), most of these approaches require some assumptions on the level of isomorphism (lexical and/or structural) between two languages.  It is no longer rare to see dependency relations used as features, in tasks such as machine translation (Ding and Palmer, 2005) and relation extraction (Bunescu and Mooney, 2005). (Ding and Palmer, 2005) presents a translation model based on Synchronous Dependency Insertion Grammar (SDIG), which handles some of the non-isomorphism but requires both source and target dependency structures. This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al, 2005). Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees.  
Joshua: An Open Source Toolkit for Parsing-Based Machine Translation describe an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context grammars (SCFGs): chart-parsing, gram language model integration, beamcube-pruning, and extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French-English translation task. We used Joshua (Li et al., 2009), a syntax-based decoder with a suffix array implementation, and rule induction via the standard Hiero grammar extraction heuristics (Chiang, 2007) for the TMs. Although the comparison against the Zhu system, which uses syntax-driven machine translation, shows no clear benefit for syntax-based machine translation, it may still be the case that approaches such as Hiero (Chiang et al, 2005) and Joshua (Li et al, 2009), enhanced by dissimilarity based re-ranking, would improve over our current system. We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al, 2009). We use the Joshua implementation of the method for decoding (Li et al, 2009). Our hybrid machine translation system combines translation output from: a) the Lucy RBMT system, described in more detail in (Alonso and Thurmair, 2003), and b) one or several other MT systems, e.g. Moses (Koehn et al, 2007), or Joshua (Li et al., 2009). Decoding is carried out with Joshua (Li et al, 2009), an open-source platform for SCFG-based MT. Due to engineering limitations in decoding with a large grammar, we apply three additional error correction and filtering steps to every system. A Hiero-style decoder Joshua (Li et al, 2009) is also used in our experiments. However, it would be artificial to ignore dictionary resources when they exist. We experiment with two translation models: hierarchical phrase-based translation (Chiang, 2007) and syntax augmented translation (Zollmann and Venugopal, 2006), both of which are implemented in the Joshua decoder (Li et al, 2009). Yet, our best system exhibits Hiero-level performance on French-English Europarl data using an SCFG-based decoder (Li et al, 2009). We use our learned stochastic SCFG grammar with the decoding component of the Joshua SCFG toolkit (Liet al, 2009). Joshua (Li et al, 2009): A decoder written in Java by the John Hopkins University. The 2011 WMT Tunable Metrics task consists of using Z-MERT (Zaidan, 2009) to tune a pre-built Urdu-English Joshua (Li et al, 2009) system to a new evaluation metric on a tuning set with 4 reference translations and decoding a test set using the resulting parameter set. Decoding was carried out in Joshua (Li et al, 2009), an open-source framework for parsing-based MT. We managed our experiments with LoonyBin (ClarkandLavie, 2010), an open-source tool for defining, modifying, and running complex experimental pipelines. The unlabeled data was subsampled (Li et al, 2009) from a larger corpus by selecting sentences which have good tune and test set coverage, and limited to sentences of length at most 40. We perform experiments using the open-source MT toolkit Joshua (Li et al, 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. We perform experiments with the syntax-based MT system Joshua (Li et al, 2009a), which implements dynamic programming algorithms for second-order expectation semi rings (Li and Eisner, 2009) to efficiently compute the gradients needed for optimizing (8). While (a) is not true in our setting because Xi is a hyper graph (which is ambiguous), Li et al (2009b) show how to approximate a hyper graph representation of p (x |yi) by an unambiguous WFSA. We report results on Chinese-to-English translation tasks using Joshua (Li et al, 2009a), an open-source implementation of Hiero (Chiang, 2007). We use Joshua, a Java-based open source implementation of the hierarchical decoder (Li et al, 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrase model.perl script packed with Moses2 (Koehn et al., 2007). In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG).
Incrementality In Deterministic Dependency Parsing Deterministic dependency parsing is a robust and efficient approach to syntactic parsing of unrestricted natural language text. In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable within this framework. However, we also show that it is possible to minimize the number of structures that require nonincremental processing by choosing an optimal parsing algorithm. This claim is substantiated with experimental evidence showing that the algorithm achieves incremental parsing for 68.9% of the input when tested on a random sample of Swedish text. When restricted to sentences that are accepted by the parser, the degree of incrementality increases to 87.9%. Our bottom-up deterministic parser adopts Nivre's algorithm (Nivre, 2004) with a preprocessor. The parser is a bottom-up deterministic dependency parser based on the algorithm proposed by (Nivre, 2004). The main part of our dependency parser is based on Nivre' s algorithm (Nivre, 2004), in which the dependency relations are constructed by a bottom up deterministic schema. If we consider transition-based dependency parsing (Nivre, 2008), the purely bottom-up strategy is implemented by the arc-standard model of Nivre (2004). Nivre (2004) investigated the issue of (strict ) incrementality for this type of parsers ;i.e., if at any point of the analysis the processed input forms one connected structure. Incrementality is not strict here in the sense of (Nivre, 2004), because sometimes more than one word is needed before parts of the frame are constructed and out put: into the right, for instance, needs to wait for a word like leg that completes the chunk. The semantics of this transition system is described in (Nivre,2004). When restricted to these three transitions, the system is equivalent to the so-called stack-based arc-standard model of Nivre (2004). This idea has been applied to constituency parsing, for example in Sagae and Lavie (2006), and we describe below a simple variant for dependency parsing similar to Yamada and Matsumoto (2003) and the arc-standard version of Nivre (2004). The MaltParser is a dependency parser generator, with three parsing algorithms: Nivre's arc standard, Nivre's arc eager (see Nivre (2004) for a comparison between the two Nivre algorithms), and Covington's (Covington, 2001). The parsing algorithm is the arc-standard method (Nivre, 2004), which is briefly described in Section 2. These features are found to have high overall accuracy in the Nivre parser (Nivre, 2004) and in human sentence processing modeling (Boston et al, 2008) . In this paper, we propose a model based on Arc-Standard Transition System of Nivre (2004), which is known as an incremental greedy projective parsing model that parses sentences in linear time. Actions in Arc-Standard Transition System (Nivre, 2004) clues to unsupervised parsing. We implement three transition-based dependency parsers with three different parsing algorithms: Nivre's arc standard, Nivre's arc eager (see Nivre (2004) for a comparison between the two Nivre algorithms), and Liang's dynamic algorithm (Huang and Sagae, 2010). In this respect such a model is very restrictive and suffers from the pitfalls of the incremental processing (Nivre, 2004). This is in line with the Arc-Standard parsing strategy of shift-reduce dependency parsers (Nivre, 2004). There are other transition-based dependency parsing algorithms that take a similar approach; Nivre (2009) integrated a SWAP transition into Nivre's arc-standard algorithm (Nivre, 2004) and Fernandez Gonzalez and Gomez-Rodriguez (2012) integrated a buffer transition into Nivre's arc-eager algorithm to handle non-projectivity.
SemEval-2007 Task 15: TempEval Temporal Relation Identification The TempEval task proposes a simple way to evaluate automatic extraction of temporalrelations. It avoids the pitfalls of evaluating a graph of inter-related labels by defining three sub tasks that allow pairwise eval uation of temporal relations. The task not only allows straightforward evaluation, italso avoids the complexities of full tempo ral parsing. The 2007 TempEval competition tries to address this question by establishing a common corpus on which research systems can compete to find temporal relations (Verhagen et al, 2007). For instance, TempEval (Verhagen et al, 2007) only labeled relations between events that syntactically dominated each other. Our task is similar to task A and C of TempEval-1 (Verhagen et al 2007) in the sense that we attempt to identify temporal relation between events and time expressions or document dates. challenge held at the SemEval 2007 Workshop (Verhagen et al, 2007). In order to drive forward research on temporal relation identification, the SemEval 2007 shared task (Verhagen et al, 2007) (TempEval) included the following three tasks. Much recent work on temporal relations revolved around the TimeBank and TempEval (Verhagen et al., 2007). After several evaluation campaigns targeted at temporal processing of text, such as MUC, ACE TERN and TempEval-1 (Verhagen et al, 2007), the recognition and normalization task has been again newly reintroduced in TempEval-2 (Pustejovsky& amp; Verhagen, 2009). Temporal information processing is a topic of natural language processing boosted by recent evaluation campaigns like TERN2004,1 TempEval-1 (Verhagen et al, 2007) and the forthcoming TempEval-22 (Pustejovsky and Verhagen, 2009). TempEval (Verhagen et al 2007), in 2007, and more recently TempEval-2 (Verhagen et al 2010), in 2010, were concerned with this problem. A first attempt to standardize this task was the 2007 TempEval competition (Verhagen et al, 2007). TempEval07 (Verhagen et al, 2007) integrated 14 TLINK relations into three: before, after, and. Previous research such as Verhagen et al (2007) using three reltions as target relations showed from 60% to 80% performance according to TLINKtypes. A previous module for temporal analysis was developed and integrated into the English grammar (Hagege and Tannier, 2008), and evaluated during TempEval campaign (Verhagen et al, 2007). Although we have not yet evaluated our tagging of relative dates, the system on which our current date normalization is based achieved good results in the TempEval (Verhagen et al., 2007) campaign. The TempEval track consists of three different tasks described in (Verhagen et al 2007). The relevance of temporal information has been reflected in specialized conferences (Schilder et al, 2007) and evaluation forums (Verhagen et al, 2007). See (Verhagen et al, 2007) for details. Accordingly, the performance results given in (Verhagen et al, 2007) are reported using metrics of precision, recall and F-measure. Mani et al (2006), Chambers et al (2007) and some of the TempEval 2007 participants (Verhagen et al, 2007). The main challenges involved in this task were first addressed during TempEval-1 in 2007 (Verhagen et al, 2007).
A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. I(lavans, M. Liberman, M. Jl4arcus, S. Roukos, B. Santorini, T. Strzalkowski IBM Research Division, Thomas J. Watson Research Center Yorktown Heights, NY 10598 The problem of quantitatively comparing tile perfor- mance of different broad-coverage rammars of En- glish has to date resisted solution.  To evaluate the parsing performance, we use the three standard ways to measure the performance: unlabeled (i.e., hierarchical spans) and labeled (i.e., nuclearity and relation) F-score, as defined by Black et al (1991).  We evaluated our parser using the standard PARSEVAL measures (Black et al, 1991): labelled precision, labelled recall, and labelled F-measure (Prec., Rec., and F1, respectively), which are based on the number of non-terminal items in the parser's output that match those in the gold-standard parse. The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to compute a PARSEVAL (Black et al, 1991) score that is indicative of the intrinsic performance of the parser. Parse trees are commonly scored with the PARSEVAL set of metrics (Black et al, 1991). Final testing was carried out on section 00, and the PARSEVAL measures (Black et al, 1991) were used to evaluate the performance. The performance is assessed using labeled recall and labeled precision as defined by the standard Parseval metric (Black et al, 1991). Consequently, they relaxed standard PARSEVAL (Black et al, 1991) to treat EDITED constituents like punctuation: adjacent EDITED constituents are merged, and the internal structure and attachment of EDITED constituents is not evaluated. Nevertheless, we agree with the widespread sentiment that dependency-based evaluation of parsers avoids many of the problems of the traditional Parseval measures (Black et al, 1991), and to the extent that the Stanford dependency representation is an effective representation for the tasks envisioned, it is perhaps closer to an appropriate task based evaluation than some of the alternative dependency representations available. In particular, metrics like attachment score for dependency parsers (Buchholz and Marsi, 2006) and Parseval for constituency parsers (Black et al, 1991) suffer from being an average over a highly skewed distribution of different grammatical constructions. Table 3 shows the results of 1st-level partial parsing and full parsing, using the PARSEVAL evaluation methodology (Black et al 1991) on the UPENN Chinese Tree Bank of 100k words developed by Univ. of Penn. The standard PARSEVAL metric (Black et al., 1991) counts labeled nonempty brackets: items are (X, i, j) for each nonempty nonterminal node, where X is its label and i, j are the start and end positions of its span. At their time, each of these models improved the state-of-the-art, bringing parsing performance on the standard test set of the Wall-Street-Journal to a performance ceiling of 92% F1-score using the PARSEVAL evaluation metrics (Black et al, 1991). PARSEVAL measures (Black et al, 1991) are used to evaluate a parser's phrase-structure trees against a gold standard. After the release of the Penn Treebank (PTB) (Marcus et al, 1993) and the PARSEVAL metrics (Black et al, 1991), some new corpus based syntactic parsing techniques were explored in the English language. Empty categories were and still are routinely pruned out in parser evaluations (Black et al, 1991). Measured by the ParsEval metric (Blacketal., 1991), the parser accuracy stands at 80.3% (F score), with a precision of 81.8% and a recall of 78.8% (recall). The parameters lambda i and rho are tuned by the Powell's method (Powell, 1964) on a development set, using the F1 score of PARSEVAL (Black et al, 1991) as objective. As accuracy metric we used the standard PAP, SEVAI, scores (Black et al 1991) to compare a proposed parse P with tile corresponding correct tree bank parse T as follows. A comparison of unlexicalised PCFG parsing (Ku?bler, 2005) trained and evaluated on the German NEGRA (Skut et al, 1997) and the TuBa D/Z (Telljohann et al, 2004) tree banks using LoPar (Schmid, 2000) shows a difference in parsing results of about 16%, using the PARSEVAL metric (Black et al, 1991).
Generating Phrasal and Sentential Paraphrases: A Survey of Data-Driven Methods The task of paraphrasing is inherently familiar to speakers of all languages. Moreover, the task of automatically generating or extracting semantic equivalences for the various units of language— words, phrases, and sentences—is an important part of natural language processing (NLP) and is being increasingly employed to improve the performance of several NLP applications. In this article, we attempt to conduct a comprehensive and application-independent survey of data-driven phrasal and sentential paraphrase generation methods, while also conveying an appreciation for the importance and potential use of paraphrases in the field of NLP research. Recent work done in manual and automatic construction of paraphrase corpora is also examined. We also discuss the strategies used for evaluating paraphrase generation techniques and briefly explore some future trends in paraphrase generation. For a more extensive survey on paraphrasing methods, see Androutsopoulos and Malakasiotis (2010) and Madnani and Dorr (2010). There are various data-driven approaches to this NLP-task (Madnani and Dorr, 2010), but they usually focus on lexical paraphrases and do not address the problem of sentence splitting, either. Madnani and Dorr (2010) survey a variety of data driven paraphrasing techniques, categorizing them based on the type of data that they use. It could be applied to other data driven paraphrasing techniques (see Madnani and Dorr (2010) for a survey). Paraphrase generation, on the other hand, has been an area of active research and the related work has been thoroughly surveyed in Androutsopoulos and Malakasiotis (2010) as well as in Madnani and Dorr (2010). A much more detailed discussion on the use of paraphrases and ways to extract them is given in (Madnani and Dorr, 2010). See Madnani and Dorr (2010) for a good paraphrasing survey. More comprehensive surveys of data-driven paraphrasing techniques can be found in Androutsopoulos and Malakasiotis (2010) and Madnani and Dorr (2010). As Madnani and Dorr (Madnani and Dorr, 2010) suggested, it would be beneficial to the research community to develop a standard, shared evaluation that would act to catalyze further advances and encourage more meaningful comparative evaluations of such approaches moving forward. they assume that the same paraphrase generation resources (Madnani and Dorr, 2010), for example paraphrasing rules, that some abstractive sentence compressors (including ours) use always produce acceptable paraphrases, which is not the case as discussed below. Paraphrase generation methods that operate both on a single monolingual corpus or on parallel corpus are discussed by Madnani and Dorr 2010. Synonym extraction (Wu and Zhou, 2003), lexical substitution (McCarthy and Navigli, 2007) and paraphrasing (Madnani and Dorr, 2010) are related to collocation correction in the sense that they try to find semantically equivalent words or phrases. Because equivalence is the most fundamental semantic relationship, techniques for generating and recognizing paraphrases play an important role in a wide range of natural language processing tasks (Madnani and Dorr, 2010). Data-driven paraphrase acquisition techniques can be categorized by the type of data that they use (Madnani and Dorr,2010). A detailed survey of paraphrase generation techniques can be found in (Androutsopoulos and Malakasiotis, 2010) and (Madnani and Dorr, 2010). Sub-sentential paraphrases can be acquired from text pairs expressing the same meaning (Madnani and Dorr, 2010). Over the years, paraphrase acquisition and generation have attracted a wealth of research works that are too many to adequatly summarize here: (Madnani and Dorr, 2010) presents a complete and up to-date review of the main approaches. The wide variety of techniques for acquiring phrasal paraphrases, which can subsequently be used by text paraphrasing techniques (Madnani and Dorr, 2010), the inherent polysemy of such linguistic units and the pragmatic constraints on their uses make it impossible to ensure that potential paraphrase pairs will be substitutable in any context, an observation which was already made at a lexical level (Zhao et al., 2007). The acquisition of sub-sentential paraphrases has attracted a lot of attention recently (Madnani and Dorr,2010).
Tree-To-String Alignment Template For Statistical Machine Translation We present a novel translation model on alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models. A very similar system for the reverse direction is described in (Liu et al, 2006). We perform derivation-level combination as described in (Liu et al, 2009b) for mixing different types of translation rules within one derivation. Liu et al (2006) propose a tree-to-string model. Liu et al (2006) treat all bilingual phrases as lexicalized tree-to-string rules, including those non-syntactic phrases in training corpus. Liu et al (2006) propose a tree-to-string translation model.   In this work we also take advantages of the augmented Chinese parse trees (with ECs projected to the surface) and extract tree-to-string grammar (Liu et al, 2006) for a tree-to-string MT system. Liu et al (2006) also add non-syntactic PBSMT phrases into their tree-to-string translation system. Liu et al (2006) changed the translation unit from phrases to tree-to-string alignment templates (TATs) while we do not.   Liu et al (2006) experimented with tree-to-string translation models that utilize source side parse trees. When using the projected parser in a tree based translation model (Liu et al, 2006), we achieve translation performance comparable with using a state-of-the-art supervised parser trained on thousands of CTB trees. We first extract the tree-to-string translation rules from the training corpus by the algorithm of (Liu et al, 2006), and train a 4-gram language model on the Xinhua portion of GIGAWORD corpus with Kneser-Ney smoothing using the SRI Language Modeling Toolkit (Stolcke and Andreas, 2002). However, if one were to use rule Markov models with a conventional CKY-style bottom-up decoder (Liu et al, 2006), the complexity would increase to O (n Cm 1|V |4 (g 1)), where C is the maximum number of outgoing hyper edges for each node in the translation forest, and m is the order of the rule Markov model. Originally, the output of the parser stage was a single parse tree, and this type of system has been shown to outperform phrase-based translation on, for instance, Chinese to-English translation (Liu et al, 2006). For example, (Chiang, 2007) adopts a CKY style span-based decoding while (Liu et al, 2006) applies a linguistically syntax node based bottom-up decoding, which are difficult to integrate. In this paper, we incorporate the MERS model into a state of-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al, 2006). Our baseline system is Lynx (Liu et al, 2006), which is a linguistically syntax-based SMT system.
  In contrast, Blatz et al (2004) introduced a sentence level QE system where an arbitrary threshold is used to classify the MT output as good or bad. The idea is explored more comprehensively in (Blatz et al, 2004). In this respect, the goal is more related to the area of confidence estimation for MT (Blatz et al, 2004). We compute sentence CMs by combining the scores given by a word CM based on the IBM model 1 (Brown et al, 1993), similar to the one described in (Blatz et al, 2004). The work of (Blatz et al, 2004) is among the best known study of sentence and word level features for translation error prediction. This resembles approaches that merge different classifiers (Riedel et al 2011) or attempt to estimate confidence of models (Blatz et al 2004). We used two different methods to combine subsequence features: Average value of subsequence-level scores, as done in (Blatz et al, 2004). To this end, we estimate a confidence score for each SMT hypothesis, using a discriminative classification framework reminiscent of Blatz et al (2004). A considerable amount of work has been done in the related area of confidence estimation for MT, for which Blatz et al (2004) provide a good overview. In contrast to most of the work on confidence estimation (Blatz et al, 2004), the features we use are not internal features of the MT system. 33 from automatic MT evaluation metrics (Blatz et al., 2004) such as BLEU (Papineni et al, 2002) at training time, it soon became clear that human labels result in significantly better models (Quirk, 2004). (Blatz et al, 2004) conducted extensive study incorporating various sentence-level and word-level features thru multi layer perceptron and naive Bayes algorithms for sentence and word confidence estimation. Blatz et al (2004) attempted sentence-level assessment using a set of 91 features (from the SMT system input and translation texts) and automatic annotations such as NIST and WER. Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al, 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al, 2001), and machine translation (Blatz et al, 2004) to name a few.  The organizers have made available a baseline QE system that consists of a number of well established features (Blatz et al, 2004) and serves as a starting point for development. Blatz et al (2004) only investigated source n gram frequency statistics and source language model features, while other work mainly focused on target side features.
Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchronization This paper presents the first round of the on Textual Entailment for organized within SemEval-2012. The task was designed to promote research on semantic inference over texts written in different languages, targeting at the same time a real application scenario. Participants were presented with datasets for different language pairs, where multi-directional entailment relations (“forward”, “backward”, “bidirectional”, “no entailment”) had to be identified. We report on the training and test data used for evaluation, the process of their creation, the participating systems (10 teams, 92 runs), the approaches adopted and the results achieved. Recently, a new dataset including "Unknown" pairs has been used in the "Cross-Lingual Textual Entailment for Content Synchronization" task at SemEval-2012 (Negri et al, 2012).  For a comprehensive description of the task see (Negri et al, 2012). Readers can refer to M. Negri et al 2012.s., for more detailed introduction. Spanish data sets provided in the task 8 of SemEval 2012 (Negri et al, 2012). The SemEval-2012 CLTE task (Negri et al, 2012) asks participants to judge entailment pairs in four language combinations, defining four target entailment relations, for ward, backward, bidirectional and no entailment. Cross-Lingual Text Entailment (CLTE), besides introducing the extra dimension of cross-linguality, also requires to determine the exact direction of the entailment relation, to provide content synchronization (Negri et al, 2012). In this paper we have presented the DirRelCond3 systems that participated at the CLTE task (Negri et al., 2012) from SemEval-2012.
Pronunciation Modeling For Improved Spelling Correction This paper presents a method for incorporating word pronunciation information in a noisy channel model for spelling correction. The proposed method builds an explicit error model for word pronunciations. By modeling pronunciation similarities between words we achieve a substantial performance improvement over the previous best performing models for spelling correction. Pronunciation modeling in (Toutanova and Moore, 2002) further improves spelling correction performance. (Toutanova and Moore, 2002) improved the string to string edits model by modeling pronunciation similarities between words. Choudhury et al. (2007) implemented the noisy channel through a Hidden-Markov Model (HMM) able to handle both graphemic variants and phonetic plays as proposed by (Toutanova and Moore, 2002), while Cook and Stevenson (2009) enhanced the model by adapting the channel's noise P (O|W, wf) according to a list of predefined observed word formations {wf}: stylistic variation, word clipping, phonetic abbreviations, etc. The key component here is the error model, which should not only capture orthographic similarities (Brill and Moore, 2000), but also phonetic similarities (Toutanova and Moore, 2002). (Brill and Moore, 2000) presented an improved error model over the one proposed by (Kernighan et al, 1990) by allowing generic string-to-string edit operations, which helps with modeling major cognitive errors such as the confusion between le and al. Via explicit modeling of phonetic information of English words, (Toutanova and Moore, 2002) further investigated this issue. This approach has also been extended to incorporate a pronunciation model (Toutanova and Moore, 2002). Neither do we require pairs of misspelled names and their correct spellings for learning the error model unlike (Brill and Moore, 2000) or large-coverage general purpose lexicon for unlike (Cucerzan and Brill, 2004) or pronunciation dictionaries unlike (Toutanova and Moore, 2002). For example, Brill and Moore (2000) developed a generative model including contextual substitution rules; and Toutanova and Moore (2002) further improved the model by adding pronunciation factors into the model. Toutanova and Moore (2002) improve the model by incorporating pronunciation information. Toutanova and Moore (2002) further explored this via explicit modeling of phonetic information of English words. Our model of pronunciation variation is used to extend a pronouncing dictionary for use in the spelling correction algorithm developed by Toutanova and Moore (2002), which includes models for both orthography and pronunciation. The pronunciation variation model is used to generate multiple pronunciations for each canonical pronunciation in a pronouncing dictionary and these variations are used in the spelling correction approach developed by Toutanova and Moore (2002), which uses statistical models of spelling errors that consider both orthography and pronunciation. Toutanova and Moore (2002) extend Brill and Moore (2000) to consider edits over both letter sequences and sequences of phones in the pronunciations of the word and misspelling. The spelling correction models from Brill and Moore (2000) and Toutanova and Moore (2002) use the noisy channel model approach to determine the types and weights of edit operations. Toutanova and Moore (2002) describe an extension to Brill and Moore (2000) where the same noisy channel error model is used to model phone sequences instead of letter sequences. Since a spelling correction model needs to rank candidate words rather than candidate pronunciations, Toutanova and Moore (2002) derive an error model that determines the probability that a word w was spelled as the non-word r based on their pronunciations. Like Toutanova and Moore (2002), we use the n-gram LTP model from Fisher (1999) to predict these pronunciations. The pronunciation based spelling correction approach developed in Toutanova and Moore (2002) requires a list of possible pronunciations in order to compare the pronunciation of the misspelling to the pronunciation of correct words. In order to evaluate the effect of pronunciation variation in Toutanova and Moore (2002)'s spelling correction approach, we compare the performance of the pronunciation model and the combined model. The noisy channel spelling correction approach developed by Brill and Moore (2000) and Toutanova and Moore (2002) appears well-suited for writers of English as a foreign language.
Unsupervised Models For Named Entity Classification This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classi- However, we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type. We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98). Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999). (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co Boosting).  Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on. DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus. (Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify. In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification. Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition.  Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999). While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collins and Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention). This can be either a fixed number of added unlabelled examples (Blum and Mitchell, 1998), the performance drop on a control set of labelled instances, or a filter on the disagreement of h1 and h2 in classifying U (Collins and Singer, 1999). In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999).  Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998). This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999). This criterion was used in a lightly-supervised NE recognizer (Collins and Singer, 1999). (6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here. We use Collins and Singer (1999) for our exact specification of Yarowsky. This is not clearly specified in Collins and Singer (1999), but is used for DL-CoTrain in the same paper.
Distinguishing Word Senses In Untagged Text This paper describes an experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text. The methods described in this paper, McQuitty's similarity analysis, Ward's minimum—variance method, and the EM algorithm, assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text. These methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs. Overall, the most accurate of these procedures is McQuitty's similarity analysis in combination with a high dimensional feature set. Once the corpus has been processed, clusters are repeatedly merged using HAC with the aver age link criteria, following (Pedersen and Bruce,1997). Here we are following (Pedersen and Bruce, 1997), who likewise took this approach to feature representation. Context Representations SenseClusters supports two different representations of context, first order context vectors as used by (Pedersen and Bruce, 1997) and second order context vectors as suggested by (Schutze,1998). But our model does have a natural preference for the most frequent sense in the thesaurus training corpus, which is a useful heuristic for word sense disambiguation (Pedersen and Bruce, 1997). Previous work in word sense discrimination has shown that contexts of an ambiguous word can be effectively represented using first order (Pedersen and Bruce, 1997) or second order (Schutze, 1998) representations.   (Pedersen and Bruce, 1997) and (Pedersen and Bruce,1998) propose a (dis) similarity based discrimination approach that computes (dis) similarity among each pair of instances of the target word. (Schutze, 1998) points out that single link clustering tends to place all instances into a single elongated cluster, whereas (Pedersen and Bruce, 1997) and (Purandare, 2003) show that hierarchical agglomerative clustering using average link (via McQuitty's method) fares well. The objective of this research is to extend previous work in discrimination by (Pedersen and Bruce, 1997), who developed an approach using agglomerative clustering. We believe that this is an aggressive number of senses for a discrimination system to attempt, considering that (Pedersen and Bruce, 1997) experimented with 2 and 3 senses, and (Schutze, 1998) made binary distinctions. Our method of name discrimination is described in more detail in (Pedersen et al, 2005), but in general is based on an unsupervised approach to word sense discrimination introduced by (Purandare and 25 Pedersen, 2004), which builds upon earlier work in word sense discrimination, including (Schutze, 1998) and (Pedersen and Bruce, 1997). For example, Pedersen and Bruce (1997) cluster the occurrences of an ambiguous word by constructing a vector of terms occurring in the context of the target. An evaluation was carried out on the full 27,132 instance train+test data set using the SenseClusters evaluation methodology, which was first defined in (Pedersen and Bruce, 1997).   In (Pedersen and Bruce, 1997), they described an experimental comparison of three clustering algorithms for word sense discrimination.
A Hierarchical Phrase-Based Model For Statistical Machine Translation We present a statistical phrase-based translamodel that uses phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as shift to the of syntaxtranslation systems without any lin- In our experiments using BLEU as a metric, the hierarchical phrasebased model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system. Similarly, Chiang (2005) uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU.  We also implemented Algorithms 2 and 3 in a parsing-based MT decoder (Chiang, 2005) and report results on decoding speed. Our second experiment was on a CKY-based decoder for a machine translation system (Chiang, 2005), implemented in Python 2.4 accelerated with Psyco 1.3 (Rigo, 2004). Following this WSD reformulation for SMT, Chan et al (2007) integrate a state-of-the-art WSD system into a hierarchical phrase-based system (Chiang, 2005). Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005). For instance, Zollmann et al (2006) follow Chiang (2005) in disallowing adjacent non terminals. (Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al, 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. More previously, (Chiang, 2005) rewards hypotheses whenever they exactly match constituent boundaries of parse trees on the source side.  To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. Chiang (2005)'s hierarchal phrase-based model achieves significant performance improvement. However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005). However, formal SCFG show much better performance in the formally syntax-based translation framework (Chiang, 2005). This is because the formal syntax is learned from phrases directly without relying on any linguistic theory (Chiang, 2005). Standard ngram language models assign probabilities to translation hypotheses in the target language, typically as smoothed trigram models (Chiang, 2005). Hiero (Chiang, 2005) is a hierarchical, string-to-string translation system. Recent work in machine translation has evolved from the traditional word (Brown et al, 1993) and phrase based (Koehn et al, 2003a) models to include hierarchical phrase models (Chiang, 2005) and bilingual synchronous grammars (Melamed, 2004). (Chiang, 2005) generates synchronous context free grammar (SynCFG) rules from an existing phrase translation table. While (Chiang, 2005) uses only two nonterminal symbols in his grammar, we introduce multiple syntactic categories, taking advantage of a target language parser for this information.
Mixture-Model Adaptation for SMT We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components. We investigate a number of variants on this approach, including cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to. The best methods achieve gains of approximately one BLEU percentage point over a state-of-the art non-adapted baseline system. In this respect our approach is similar to that of Foster and Kuhn (2007), however we used a probabilistic classifier to determine a vector of probabilities representing class-membership, rather than distance based weights. Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007), extended this to include the translation model. Early efforts focus on building separate models (Foster and Kuhn, 2007) and adding features (Matsoukas et al, 2009) to model domain information. Besides many works addressing holistic LM domain adaptation for SMT, e.g. Foster and Kuhn (2007), recently methods were also proposed to explicitly adapt the LM to the discourse topic of a talk (Ruiz and Federico, 2011). Both we restudied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log linearly. Thus, the variant of VSM adaptation tested here bears a superficial resemblance to domain adaptation based on mixture models for TMs, as in (Foster and Kuhn, 2007), in that both approaches rely on information about the subcorpora from which the data originate. For details, refer to (Foster and Kuhn, 2007). In (Foster and Kuhn, 2007), two kinds of linear mixture were described: linear mixture of language models (LMs), and linear mixture of translation models (TMs). Some of the results reported above involved linear TM mixtures, but none of them involved linear LM mixtures. In (Foster and Kuhn, 2007) two basic settings are investigated: cross-domain adaptation, in which a small sample of parallel in-domain text is assumed, and dynamic adaptation, in which only the current input source text is considered. Although dynamic adaptation is closely related to static domain adaptation (Foster and Kuhn, 2007), in this scenario we are not interested in the quality of the final model. Foster and Kuhn (2007) interpolated the in and general-domain phrase tables together, assigning either linear or log-linear weights to the entries in the tables before combining overlapping entries; this is now standard practice. In this work, we directly compare the approaches of (Foster and Kuhn, 2007) and (Koehn and Schroeder, 2007) on the systems generated from the methods mentioned in Section 2.1. (Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain by using weights that depend on text distances to mixture components. Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a maximum a posteriori combination (Bacchiani et al 2006). Foster and Kuhn (2007) presented an approach that resembles more to our work, in which they divided the training corpus into different components and integrated models trained on each component using the mixture modeling. Among other applications, language model perplexity has been used for domain adaptation (Foster and Kuhn, 2007). Mixture-modelling for language models is well established (Foster and Kuhn, 2007). Foster and Kuhn (2007) find that both TM and LM adaptation are effective, but that combined LM and TM adaptation is not better than LM adaptation on its own. A second strand of research in domain adaptation is data selection, i.e. choosing a subset of the training data that is considered more relevant for the task at hand. Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before. They used sampling without replacement to create a number of base models whose phrase-tables are combined with that of the baseline (trained on the full training-set) using linear mixture models (Foster and Kuhn, 2007).
CorMet: A Computational Corpus-Based Conventional Metaphor Extraction System CorMet is a corpus-based system for discovering metaphorical mappings between concepts. It does this by finding systematic variations in domain-specific selectional preferences, which are inferred from large, dynamically mined Internet corpora. Metaphors transfer structure from a source domain to a target domain, making some concepts in the target domain metaphorically equivalent to concepts in the source domain. The verbs that select for a concept in the source domain tend to select for its metaphorical equivalent in the target domain. This regularity, detectable with a shallow linguistic analysis, is used to find the metaphorical interconcept mappings, which can then be used to infer the existence of higher-level conventional metaphors. Most other computational metaphor systems use small, hand-coded semantic knowledge bases and work on a few examples. Although CorMet’s only knowledge base is WordNet (Fellbaum 1998) it can find the mappings constituting many conventional metaphors and in some cases recognize sentences instantiating those mappings. CorMet is tested on its ability to find a subset of the To determine whether semantic restrictions are being violated, domain information from ontologies/thesauri such as WordNet could be used and/or statistical techniques as used by Mason (2004). Previous work on automated metaphor detection includes Fass (1991), Martin (1990), and Mason (2004). The CorMet system (Mason, 2004) dynamically mines domain specific corpora to find less frequent usages and identifies conceptual metaphors. Mason (2004) shows how statistical analysis can automatically detect and extract conventional metaphors from corpora, though creative metaphors still remain a tantalizing challenge.  By finding semantic differences between the selectional preferences, it can articulate the higher-order structure of conceptual metaphors ((Mason, 2004), p. 24), finding mappings like LIQUID -> MONEY. CorMet (Mason, 2004) is designed to extract known conventional metaphors from domain-specific textual corpora, which are derived from Google queries. The CMI system described in this paper is informed largely by CorMet (Mason, 2004).   Our method is different from automated work on metaphor recognition such as (Mason, 2004) and (Gedigian et al, 2006) in that it includes nouns as parts of speech.
Polylingual Topic Models Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts. Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages. We introduce a polylingual topic model that discovers topics aligned across multiple languages. We explore the model’s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages. This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs. Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009). (Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations. Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingual topic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction. For Europarl data sets, we artificially make them comparable by considering the first half of English document and the second half of its aligned foreign language document (Mimno et al,2009). Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details. Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly. Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages. Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009). We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009). The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009). Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference. For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009). Documents are defined as speeches by a single speaker, as in (Mimno et al, 2009). In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours. Multilingual LDA has been used before in natural language processing, e.g. polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009). Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles). Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora. A good candidate for multilingual topic analyses are polylingual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic. To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics.
Multiple Aspect Ranking Using the Good Grief Algorithm We address the problem of analyzing multiple related opinions in a text. For instance, in a restaurant review such opinions may include food, ambience and service. We formulate this task as a multiple aspect ranking problem, where the goal is to produce a set of numerical scores, one for each aspect. We present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks. This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast. We prove that our agreementbased joint model is more expressive than individual ranking models. Our empirical results further confirm the strength of the model: the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model. Similarly, Snyder and Barzilay (2007) decompose an opinion across several dimensions and capture the sentiment across each dimension. However, it has been observed that ratings for different aspects can be correlated (Snyder and Barzilay, 2007), e.g., very negative opinion about room cleanliness is likely to result not only in a low rating for the aspect rooms, but also is very predictive of low ratings for the aspects service and dining. Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect (or target) model to make a more informed overall decision for sentiment classification. Also, it would be interesting to take a closer look at the interactions between aspect and sentiment, especially at a multiple-sentence level (see Snyder and Barzilay 2007). In a multi-aspect setting, however, information about the sentence topic is required to determine the aspect to which a sentiment-bearing word relates (Snyder and Barzilay, 2007). Our first task is a multi-aspect sentiment analysis task, where a system predicts the aspect-specific sentiment ratings (Snyder and Barzilay, 2007). The goal of multi-aspect sentiment classification is to predict a set of numeric ranks that reflects the user satisfaction for each aspect (Snyder and Barzilay, 2007). Examples are movie reviews (Pang and Lee, 2005), opinions (Wiebe et al., 2005), customer reviews (Ding et al, 2008) or multiple aspects of restaurants (Snyder and Barzilay, 2007). In multi-aspect rating (Snyder and Barzilay, 2007) one finds several distinct aspects such as food or service in a restaurant and then rates them on a fixed linear scale such as 1-5 stars, where all aspects could obtain just 1 star or all aspects could obtain 5 stars independently. We also propose to consider aspects of reviews (Snyder and Barzilay, 2007), and investigate other methods that measure class similarity, such as selecting typical in stances (Zhang, 1992).  
A Multi-Pass Sieve for Coreference Resolution Most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features. This approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones. To overcome this problem, we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision. Each tier builds on the previous tier’s entity cluster output. Further, our model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster. This cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time. The framework is highly modular: new coreference modules can be plugged in without any change to the other modules. In spite of its simplicity, our approach outperforms many state-of-the-art supervised and unsupervised models on several standard corpora. This suggests that sievebased approaches could be applied to other NLP tasks.  Instead, we use a sieve-based greedy search approach to inference (shown in Figure 3) inspired by recent work on coreference resolution (Raghunathan et al, 2010). Our system is an extension of Stanford's multi-pass sieve system, (Raghunathan et al, 2010) and (Lee et al, 2011), by adding novel constraints and sieves. In contrast, (Raghunathan et al, 2010) proposed a rule based model which obtained competitive result with less time. We made three considerable extensions to the Raghunathan et al (2010) model. Please see (Raghunathan et al, 2010) for more details. The core of our coreference resolution system is an incremental extension of the system described in Raghunathan et al (2010). Proper Head Word Match: This sieve marks two mentions headed by proper nouns as coreferent if they have the same head word and satisfy the following constraints: Not i-within-i same as Raghunathan et al (2010). The candidate antecedents for the pronoun are ordered based on a notion of discourse salience that favors syntactic salience and document proximity (Raghunathan et al, 2010). By matching the performance of the DT system in the first two rows of the table, the AC system proves that it can successfully learn the relative importance of the deterministic sieves, which in (Raghunathanet al, 2010) and (Lee et al, 2011) have been manually ordered using a separate development dataset. Chen built upon the sieve architecture proposed in Raghunathan et al (2010) and added one more sieve - head match - for Chinese and modified two sieves. We incorporate lexicalized feature sets into two different coreference architectures: Reconcile (Stoyanov et al, 2010), a pairwise coreference classifier, and Sieve (Raghunathan et al, 2010), a rule-based system. There is no polynomial-time dynamic program for inference in a model with arbitrary entity-level features, so systems that use such features typically rely on making decisions in a pipelined manner and sticking with them, operating greedily in a left-to-right fashion (Rahmanand Ng, 2009) or in a multi-pass, sieve-like manner (Raghunathan et al, 2010). Compared with machine learning methods, (Raghunathan et al, 2010) proposed rule-base models which have been witnessed good performance. This fact explains a new trend to develop accurate unsupervised systems that exploit simple but robust linguistic principles (Raghunathan et al, 2010). This is an interesting observation because pronominal anaphora problem has been reported with much higher results on other domains (Raghunathan et al, 2010), and also on other bio data (hsiang Lin and Liang, 2004). Our multi-sieve approach is different from (Raghunathan et al 2010) in several respects: (a) our sieves are machine-learning classifiers, (b) the same pair of mentions can fall into multiple sieves, (c) later sieves can override the decisions made by earlier sieves, allowing to recover from errors as additional evidence becomes available. (Raghunathan et al 2010) recorded the best result on CoNLL 2011 shared task. The ordering should be such that (a) maximum amount of information is injected at early stages (b) the precision at the early stages is as high as possible (Raghunathan et al 2010). Third, while our division to sieves may resemble witchcraft, it is motivated by the intuition that mentions appearing close to one another are easier instances of co-ref as well as linguistic insights of (Raghunathan et al 2010).
The Kappa Statistic: A Second Look dialogue structure coding scheme. 23(1):13–31. Cicchetti, Domenic V. and Alvan R. Feinstein. 1990. High agreement but low kappa: II. Resolving the paradoxes. of Clinical 43(6):551–558. Cohen, Jacob. 1960. A coefficient of for nominal scales. We follow the notation of Di Eugenio and Glass (2004). To assess this classification task we also used the kappa statistics which yielded KCo=0.922 (following (Eugenio and Glass, 2004) we report Kas KCo, indicating that we calculate K a la Cohen (Cohen, 1960). Feinstein and Cicchetti (1990), followed by Di Eugenio and Glass (2004) proved that Kappa is subject to the effect of prevalence and that different marginal distributions can lead to very different Kappa values for the same observed agreement. But Di Eugenio and Glass (2004) have found that this interpretation does not hold true for all tasks. Ever since its introduction in general (Cohen, 1960) and in computational linguistics (Carletta, 1996), many researchers have pointed out that there are quite some problems in using κ (e.g. (Di Eugenio and Glass, 2004)), one of which is the discrepancy between p0 and κ for skewed class distribution.
Dependency Treelet Translation: Syntactically Informed Phrasal SMT We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation. This method requires a source-language dependency parser, target language word segmentation and an unsupervised word alignment component. We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model. We describe an efficient decoder and show that using these treebased models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser.  This treelet-based SMT system (Quirk et al., 2005) is trained on about 4.6M parallel sentence pairs from diverse sources including bilingual books, dictionaries and web publications. Yamada and Knight (2001) and Galley et al (2004) describe methods that make use of syntactic information in the target language alone; Quirk et al (2005) describe similar methods that make use of dependency representations. We borrow the term tree let from Quirk et al (2005), who use it to refer to an arbitrary connected subgraph of a tree.  Thus we avoid the sparseness problem that other methods based on treelets suffer (Quirk et al, 2005). This is a syntactically-informed MT system, designed following (Quirk et al, 2005). For the phrase-based system, we generated the annotations needed by first parsing the source sentence e, aligning the source and candidate translations with the word-alignment model used in training, and projected the dependency tree to the target using the algorithm of (Quirk et al, 2005). We believe that the advantage of dep2str comes from the characteristics of dependency structures tending to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and are better suited to lexicalized models (Quirk et al, 2005). (Quirk et al, 2005) extends paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on tree let pairs.  Quirk et al (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. The obvious next step for our framework is to include bilingual rules that include source syntax (Quirk et al, 2005), target syntax (Shen et al,2008), and syntax on both sides. Based on the assumption that constituents generally move as a whole (Quirk et al, 2005), we decompose the sentence reordering probability into the reordering probability for each aligned source word with respect to its head, excluding the root word at the top of the dependency hierarchy which does not have a head word. Our dependency orientation feature is similar to the order model within dependency tree let translation (Quirk et al, 2005).  Quirk et al, 2005 demonstrates the success of using fragments of a target language's grammar, what they call treelets, to improve performance in phrasal translation. We previously described (Quirk et al 2005) a linguistically syntax-based system that parses the source language, uses word-based alignments to project a target dependency tree, and extracts paired dependency tree fragments (treelets) instead of surface phrases. For each pair of parallel training sentences, we parse the source sentence, obtain a source dependency tree, and use GIZA++ word alignments to project a target dependency tree as described in Quirk et al (2005). We use all of the Treelet models we described in Quirk et al (2005) namely: Treelet table with translation probabilities estimated using maximum likelihood, with absolute discounting.
A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English In this paper, we present an approach to the automatic identification and correction ofpreposition and determiner errors in non native (L2) English writing. We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing. Since the features of machine learned error detectors are often part-of-speech n grams or word word dependencies extracted from parser output (De Felice and Pulman, 2008, for example), it is important to understand how part-of speech taggers and parsers react to particular grammatical errors. As De Felice and Pulman (2008) did not perform word sense disambiguation, neither did we. Han et al (2006) and De Felice and Pulman (2008) train a maximum entropy classifier. The best results of 92.15% are reported by De Felice and Pulman (2008). In the context of automated preposition and determiner error correction in L2 English, De Felice and Pulman (2008) noted that the process is often disrupted by misspellings.  T& amp; C08, De Felice and Pulman (2008) and Gamon et al (2008) describe very similar preposition error detection systems in which a model of correct prepositional usage is trained from well formed text and a writer's preposition is compared with the predictions of this model. On the other hand, supervised models, typically treating error detection/correction as a classification problem, utilize the training of well-formed texts ((De Felice and Pulman, 2008) and (Tetreault et al, 2010)), learner texts, or both pair wisely (Brockett et al, 2006). On the other hand, supervised models, typically treating error detection/correction as a classification problem, may train on well-formed texts as in the methods by De Felice and Pulman (2008) and Tetreault et al. Chodorow et. al (2007), Tetreault and Chodorow (2008), and De Felice and Pulman (2008) train a maximum entropy model and De Felice and Pulman (2007) train a voted perceptron algorithm to correct preposition errors. 
Concept Discovery From Text Broad-coverage lexical resources such as WordNet are extremely useful. However, they often include many rare senses while missing domain-specific senses. We present a clustering algorithm called CBC (Cluster ing By Committee) that automatically discovers concepts from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the cluster. We proceed by assigning elements to their most similar cluster. Evaluating cluster quality has always been a difficult task. We present a new evaluation methodology that is based on the editing distance between output clusters and classes extracted from WordNet (the answer key). Our experiments show that CBC outperforms several well-known clustering algorithms in cluster quality. Different measures have been proposed, which are not easy to evaluate (see (Lin and Pantel, 2002) for proposals). NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). The labeled classes are acquired in three stages: 1) extraction of a noisy pool of pairs of a class label and a potential class instance, by applying a few Is-A extraction patterns, selected from (Hearst, 1992), to Web documents: (fruits, apple), (fruits, corn), (fruits, mango), (fruits, orange), (foods, broccoli), (crops, lettuce), (flowers, rose); 2) extraction of unlabeled clusters of distributionally similar phrases, by clustering vectors of contextual features collected around the occurrences of the phrases within Web documents (Lin and Pantel, 2002).  For CBC we simply used the same parameter values as reported in (Lin and Pantel, 2002). (Schutze, 1998) and (Lin and Pantel, 2002a, b) show that clustering methods are helpful in this area. Options for identifying interesting classes include manually created methods (WordNet (Miller et al, 1990)), textual patterns (Hearst, 1992), automated clustering (Lin and Pantel, 2002), and combinations (Snow et al, 2006). Mutual information (MI) is an information theoric measure and has been used in many NLP tasks, including clustering words (e.g. Lin and Pantel, 2002).   To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al 2005), and word similarity lists (Hindle 1990). To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web documents. Clustering by committee has also been used to discover concepts from a text by grouping terms into conceptually related clusters (Lin and Pantel, 2002).
The Generative Lexicon In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories outline a theory of lexical semantics embodying a notion of well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is perthan assuming a fixed set of primitives, I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop theory of Structure, representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the lexical knowledge base through a theory of inheritance. provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.  The work of Pustejovsky [Pustejovsky, 1991] is related in its attempt o reduce the size and complexity of individual lexical entries. Our view of relations in a concept network being the interpretations of metonymies is strongly reminiscent of older work in metonymy resolution such as Hobbs et al1993), Fass (1991), Markert and Hahn (2002) or the use of a generative lexicon and its relations in Pustejovsky (1991), which also are unsupervised. The term quale (plural :qualia) is borrowed from Pustejovsky (1991). To provide semantics for nouns, we use CoreLex (Buitelaar, 1998), in turn based on the generative lexicon (Pustejovsky, 1991). CoreLex is based on a different theory than Levin's (that of the generative lexicon (Pustejovsky, 1991)), but does provide a compatible decompositional meaning representation for nouns. Generative Lexicon (Pustejovsky, 1991), for example have been proposed to facilitate computationally precise description of natural language syntax and semantics. The idea that noun meaning involves event-based description has been particularly emphasized by J. Pustejovsky (1991). This semantic framework demonstrates that the association between nominal constituents and underlying predicative relation in root compounds is not arbitrary: it involves conceptual mechanisms that are triggered in other linguistic phenomena such as type coercion (Pustejovsky, 1991), anaphora (Fradin, 1984) or adjectival constructions (Bouillon and Viegas, 1993). Such compounds illustrate the notion of co-compositionality (Pustejovsky 1991). Pustejovsky (1991) refers to this kind of relation as co specification i.e. like verb can select for its argument type, an argument also can select its associated predicates. The associated predicate information is included in the qualia structure of a lexical item (Pustejovsky, 1991). I declare that Korean common nouns have both the RESTR (ICTION) for normal semantics and the QUALIA-ST (RUCTURE), which in turn has the AGENTIVE and TELIC attributes, adopting the basic idea from Pustejovsky (1991). The VPs with the verbs start or finish (see Pustejovsky, 1991) can also be accounted for using the qualia structure. The vast space between these two extremes can still be explained in terms of compositional principles with mechanisms from GLT such as type coercion and sub selection (Pustejovsky, 1991, 1993). The Generative Lexicon Theory (GLT) (Pustejovsky, 1991, 1994c) can be said to take advantage of both linguistic and conceptual approaches, providing a framework which arose from the integration of linguistic studies and of techniques found in AI. The basic sense originates derived usages, which are more or less constrained and limited, via metonymy, metaphor, slight sense-shiftings or co-composition (Pustejovsky, 1991, 1995). Finally, SEMANTIC COLLOCATIONS like long book derive their particular meaning from the recovery in context of parameters for events and other entities (Pustejovsky, 1991). We are investigating the use of principles of the Generative Lexicon (Pustejovsky 1991) for that purpose. Qualia structures have been originally introduced by (Pustejovsky, 1991) and are used for a variety of purposes in natural language processing (NLP).
Bayesian Learning of a Tree Substitution Grammar Tree substitution grammars (TSGs) offer many advantages over context-free grammars (CFGs), but are hard to learn. Past approaches have resorted to heuristics. In this paper, we learn a TSG using Gibbs sampling with a nonparametric prior to control subtree size. The learned grammars perform significantly better than heuristically extracted ones on parsing accuracy.   Similar models were developed independently by O'Donnell et al. (2009) and Post and Gildea (2009). A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009).   Of these approaches, work in Bayesian learning of TSGs produces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010). A Bayesian-learned tree substitution grammar (Post and Gildea, 2009a).
A Generative Constituent-Context Model For Improved Grammar Induction We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts. Parameter search with EM produces higher quality analyses than previously exhibited by unsupervised systems, giving the best published unparsing results on the Experiments on Penn treebank sentences of comparalength show an even higher 71% on nontrivial brackets. We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model. We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task. A number of studies are related to the work we presented, most specifically work on parallel-text based "information projection" for parsing (Hwa et al., 2002), but also grammar induction work based on constituent/distituent information (Klein and Manning, 2002) and (language-internal) alignment based learning (van Zaanen, 2000). Empirically, our algorithm performs favorably compared to the constituent context model of Klein and Manning (2002) without the need for careful initialization. We primarily compare our method to the constituent-context model (CCM) of Klein and Manning (2002). CCM is used with the initializer proposed in Klein and Manning (2002). The EM algorithm with the CCM requires very careful initialization, which is described in Klein and Manning (2002). Empirically, our algorithm performs favorably to the CCM of Klein and Manning (2002) without the need for careful initialization. Finally, there are "unsupervised" strategies where no data is labeled and all annotations (including the grammar itself) must be discovered (Klein and Manning, 2002). When Klein and Manning induce the parts-of-speech, they do so from a much larger corpus containing the full WSJ tree bank together with additional WSJ newswire (Klein and Manning,2002). An excellent recent result is by Klein and Manning (2002). We refer readers to Klein and Manning (2002) or Cover and Thomas (1991, p. 72) for details; computing expected counts for a sentence is a closed form operation. The third line corresponds to the setup reported by Klein and Manning (2002). We implement the baseline system, which Klein and Manning (2002) use for their grammar induction experiments with induced part-of-speech tags. We follow Klein and Manning (2002) in using K means to cluster the d dimensional word vectors into parts-of-speech. We chose the baseline system primarily to match previous evaluations of grammar induction using induced tags (Klein and Manning, 2002). Klein and Manning (2002) present a generative model for inducing constituent boundaries from part-of-speech tagged text. We evaluate induced constituency trees against those of the Penn Treebank using the versions of unlabeled precision, recall, and F-score used by Klein and Manning (2002). Evaluation of the algorithm is done according to PARSEVAL, except for a few changes that are also proposed by Klein and Manning (2002). Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models. To improve the quality of the induced trees, we combine our PCFG induction with the CCM model of Klein and Manning (2002), which has complementary stengths: it identifies brackets but does not label them. Finally, we intersect the feature-augmented PCFG with the CCM model of Klein and Manning (2002), a high quality bracketing model.
A Smorgasbord Of Features For Statistical Machine Translation We describe a methodology for rapid experimentation in statistical machine translation which we use to add a large number of features to a baseline system exploiting features from a wide range of levels of syntactic representation. Feature values were combined in a log-linear model to select the highest scoring candidate from an list. Feature weights were optimized directly against the BLEU evaluation metric on held-out data. We present results for a small selection of features at each level of syntactic representation. Furthermore, at the 2003 Johns Hopkins summer workshop on statistical machine translation, a large number of features were tested to discover which ones could improve a state-of-the-art translation system, and the only feature that produced a 'truly significant improvement' was the Model 1 score (Och et al., 2004). In (Och et al, 2004), the effects of integrating syntactic structure into a state-of-the-art statistical machine translation system are investigated. Although the improvement on the IWSLT 04 set is only moderate, the results are nevertheless comparable or better to the ones from (Och et al, 2004). As a workaround, parsers can rerank the translated output of translation systems (Och et al, 2004). Och et al (2004) and Cherry and Quirk (2008) both use the 1-best output of a machine translation system. Och et al (2004) also report using a parser probability normalized by the unigram probability (but not length), and did not find it effective. We follow Och et al (2004) and Cherry and Quirk (2008) in evaluating our language models on their ability to distinguish the 1-best output of a machine translation system from a reference translation in a pairwise fashion. A typical reranking approach to SMT (Och et al, 2004) uses a 1000 best list. (Och et al, 2004) describe the use of syntactic features in the rescoring step. Many solutions to the reordering problem have been proposed ,e.g. syntax-based models (Chiang, 2005), lexicalized reordering (Och et al, 2004), and tree-to-string methods (Zhang et al,2006). (Och et al, 2004) and (Shen et al, 2004) describe the use of syntactic features in reranking the output of a full translation system, but the syntactic features give very small gains. Oracle BLEU scores computed over k-best lists (Och et al, 2004) show that many high quality hypotheses are produced by first-pass SMT decoding. Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in cross lingual IR (Schonhofen et al, 2008) or re-score candidate translation outputs (Och et al, 2004). Despite the notational similarities, our approach should not be confused with projected POS models, which use source side POS tags to model reordering (Och et al, 2004). We compared these results against an inverse IBM model 1 but the results were inconclusive which is consistent with the results presented in (Och et al, 2004) where no improvements were achieved using p (e|f). A common approach of integrating new models with a statistical MT system is to add them as new feature functions which are used in decoding or in models which re-rank n-best lists from the MT system (Och et al, 2004). There are ten feature functions in the treelet system, including log-probabilities according to inverted and direct channel models estimated by relative frequency, lexical weighting channel models following Vogel et al. (2003), a trigram target language model, an order model, word count, phrase count, average phrase size functions, and whole-sentence IBM Model 1 logprobabilities in both directions (Och et al. 2004). Such an approach has been taken by Och et al (2004) for integrating sophisticated syntax-informed models in a phrase based SMT system. This method is a straightforward application of the n-best re-ranking approach described in Och et al (2004). Many different feature functions were explored in (Och et al, 2004).
Fluency Adequacy or HTER? Exploring Different Human Judgments with a Tunable MT Metric Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance. Different types of human judgments, such as Fluency, Adequacy, and HTER, measure varying aspects of MT performance that can be captured by automatic MT metrics. We explore these differences through the use of a new tunable MT metric: TER-Plus, which extends the Translation Edit Rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases. TER-Plus was shown to be one of the top metrics in NIST’s Metrics MATR 2008 Challenge, having the highest average rank in terms of Pearson and Spearman correlation. Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments. Recently, Snover et al (2009) extended the TER algorithm in a similar fashion to produce a new evaluation metric, TER plus (TERp), which allows tuning of the edit costs in order to maximize correlation with human judgment. In addition to human evaluation, we also ran system-level automatic evaluations using BLEU (Papineni et al, 2001), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al, 2009), and GTM (Turianetal., 2003). The extended TER plus (Snover et al 2009) metric addresses the first problem but not the other two. Lemma is added later in the TERplus extension (Snover et al 2009).  Effort in this direction brings up some advanced metrics such as METEOR (Banerjee and Lavie, 2005) and TERp (Snover et al, 2009) that seem to have already achieved considerably strong correlations with human judgments. The extended TER plus (Snover et al, 2009) metric addresses the first problem but not the other two. Synonym relations are defined according to WordNet (Miller et al, 1990), and paraphrase matches are given by a lookup table use din TERplus (Snover et al, 2009). Lemma is added later in the TERplus extension (Snover et al, 2009). Additionally, the average scores for adequacy and fluency were themselves averaged into a single score, following (Snover et al, 2009), and the Spearman's correlation of each of the automatic metrics with these scores are given in Table 4. The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009). TER= min edits avg ref length (4) TER-Plus (TERp) (Snover et al, 2009) extends TER by allowing the cost of edit operations to be tuned in order to maximize the metric's agreement with human judgments. However, instead of ITG alignments that were used in (Karakos et al, 2008), alignments based on TER-plus (Snover et al, 2009) were used now as the core system alignment algorithm. These operations relax the shifting constraints of TER; shifts are now allowed if the words of one string are synonyms or share the same stem as the words of the string they are compared to (Snover et al, 2009). TER-plus identifies words with the same stem using the Porter stemming algorithm (Porter et al, 1980), and identifies synonyms using the WordNet database (Miller et al, 1995). After the extraction, pruning techniques (Snover et al, 2009) can be applied to increase the precision of the extracted paraphrases. TERp (Snover et al, 2009): Translation Edit. The scores were case-insensitive and edit costs from Snover et al (2009) were used to produce scores tuned for fluency and adequacy. This algorithm is not equivalent to an incremental TER Plus (Snover et al, 2009) due to different shift constraints and the lack of paraphrase matching 30 1cat (1) 2sat (1) mat (1) (a) Skeleton hypothesis. Its automatic versions TER and TERp (Snover et al 2009), however, remain sentence based metrics. In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al, 2009) ,mWER (Nieen et al, 2000), and PER (Tillmann et al, 1997).
Robust Applied Morphological Generation natural language generation sysit often advantageous to have a separate component that deals purely with morphological processing. We present such a component: a fast and robust morphological generator for English based on finite-state techniques that generates a word form given a specification of the lemma, part-of-speech, and the type of inflection required. We describe how this morphological generator is used in a prototype system for automatic simplification of English newspaper text, and discuss practical morphological and orthographic issues we have encountered in generation of unrestricted text within this application. The relation tuple is then converted to root form using the Sussex morphological analyser (Minnen et al, 2000) and the POS tags are removed. Following Chambers and Jurafsky (2008), we extracted and lemmatized the verbs from the New York Times section of the Gigaword Corpus using the Stanford POS tagger (Toutanova et al, 2004) and the Morphalemmatizer (Minnen et al, 2000). Each word was reduced to its morphological root using the morphological analyser described in (Minnen et al, 2000). The relation tuple is then converted to root form using the Sussex morphological analyser (Minnen et al., 2000) and the POS tags are stripped. Information on lemmatisation, as well as abbreviations and their long forms, is added using the morpha lemmatiser (Minnen et al, 2000) and the ExtractAbbrev script of Schwartz and Hearst (2003), respectively. Heads are extracted from the chunks and lemmatized (Minnen et al, 2000). Further linguistic markup is added using the morpha lemmatiser (Minnen et al, 2000) and the C&C named entity tagger (Curran and Clark, 2003) trained on the data from MUC-7. We next lemmatised the data using morpha (Minnen et al, 2000), and chunk parsed the WSJ with TiMBL 4.1 (Daelemans et al, 2001) using the Brown corpus as training data. Tokenisation, species word identification and chunking were implemented in-house using the LTXML2 tools (Grover and Tobin, 2006), whilst abbreviation extraction used the Schwartz and Hearst abbreviation extractor (Schwartz and Hearst, 2003) and lemmatisation used morpha (Minnen et al, 2000). We used the morpha lemmatizer (Minnen et al, 2000), which is built into the C&C tools, to match tokens across T and H; and we converted all tokens to lowercase. Part-of-speech (POS) tagging is done using the C&C tagger (Curran and Clark, 2003a) and lemmatisation is done using morpha (Minnen et al, 2000). The Grefenstette (1994) relation extractor produces context relations that are then lemmatised using the Minnen et al (2000) morphological analyser. B5: Lemmatize the tokens using morpha, (Minnen et al, 2000). The tokenisation, sentence boundary detection, head word identification and chunking components were implemented with the lt-xml2tools (Grover and Tobin, 2006), and the lemmatisation used morpha (Minnen et al, 2000). Since MINIPAR performs morphological analysis on the context relations we have added an existing morphological analyser (Minnen et al, 2000) to the other extractors. For our purposes, we use a Penn tree bank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001), and further lemmatise the output of the tagger using morph (Minnen et al, 2000). We use a morphological tool (Minnen et al, 2000) to obtain the base form from the original verb or noun, so that YAG can generate grammatical sentences.
A Corpus-Based Approach For Building Semantic Lexicons Semantic knowledge can be a great asset to natural language processing systems, but it is usually hand-coded for each application. Although some semantic information is available in general-purpose knowledge bases such as WordNet and Cyc, many applications require domain-specific lexicons that represent words and categories for a particular topic. In this paper, we present a corpus-based method that can be used to build semantic lexicons for specific categories. The input to the system is a small set of seed words for a category and a representative text corpus. The output is a ranked list of words that are associated with the category. A user then reviews the top-ranked words and decides which ones should be entered in the semantic lexicon. In experiments with five categories, users typically found about 60 words per category in 10-15 minutes to build a core semantic lexicon. Our approach builds upon earlier work on corpus-based methods for generating extraction patterns (Riloff, 1996b) and semantic lexicons (Riloff and Shepherd, 1997). The corpus-based algorithm that we used to build the semantic lexicon (Riloff and Shepherd, 1997) requires five seed words as input for each semantic category, and produces a ranked list of words that are statistically associated with each category. For more details of this algorithm, see (Riloff and Shepherd, 1997). In addition, we exploit syntactic constructions shown to be useful by other studies - lists and conjunctions (Roark and Charniak, 1998), and adjacent words (Riloff and Shepherd, 1997). (Hearst, 1992), Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff and Shepherd, 1997), and Junto (Talukdar, 2010) among others, also have similarities to our approach. Riloff and Shepherd (1997) used a semi automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision. For example, Riloff and Shepherd (Riloff and Shepherd, 1997) developed a statistical co-occurrence model for semantic lexicon induction that was designed with these structures in mind. Many of the successful methods follow the unsupervised iterative bootstrapping framework (Riloff and Shepherd, 1997). NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). The idea here is that nouns in conjunctions or appositives tend to be semantically related, as discussed in Riloff and Shepherd (1997) and Roark and Charniak (1998). Both Hearst (1992) and Riloff and Shepherd (1997) use unparsed text. Riloff and Shepherd (1997) suggested using conjunction and appositive data to cluster nouns; however, they approximated this data by just looking at the nearest NP on each side of a particular NP. The main results to date in the field of automatic lexical acquisition are concerned with extracting lists of words reckoned to belong together in a particular category, such as vehicles or weapons (Riloff and Shepherd, 1997) (Roark and Charniak, 1998). Algorithms of this type were used by Riloff and Shepherd (1997) and Roark and Charniak (1998), reporting accuracies of 17% and 35% respectively. Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998). These conditions are at least as stringent as those of previous experiments, particularly those of Riloff and Shepherd (1997) who also give credit for words associated with but not belonging to a particular category. Our results are an order of magnitude better than those reported by Riloff and Shepherd (1997) and Roark and Charniak (1998), who report average accuracies of 17% and 35% respectively. The experiments in (Riloff and Shepherd, 1997) were performed on the 500,000 word MUC-4 corpus, and those of (Roark and Charniak, 1998) were performed using MUC-4 and the Wall Street Journal corpus (some 30 million words). Riloff and Shepherd (1997) presented a corpus based method that can be used to build semantic lexicons for specific categories. Following the work of (Riloff and Shepherd, 1997), we adopted the following evaluation setting.
Automatically Constructing A Lexicon Of Verb Phrase Idiomatic Combinations We investigate the lexical and syntactic flexibility of a class of idiomatic expressions. We develop measures that draw on such linguistic properties, and demonstrate that these statistical, corpus-based measures can be successfully used for distinguishing idiomatic combinations from non-idiomatic ones. We also propose a means for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation. Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al, 1998), Distributed frequency of object using verb information (Venkatapathy and Joshi, 2005), Similarity of object in verb object pair using the LSA model (Baldwin et al, 2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006). Another work uses both the syntactic and the lexical fixedness of VNICs in order to distinguish them from non-idiomatic ones, and eventually to extract them from corpora (Fazly and Stevenson, 2006). To measure fixedness, we use statistical measures of lexical, syntactic, and overall fixedness that we have developed in a previous study (Fazly and Stevenson, 2006), as well as some new measures we introduce here. The small amount of previous work on the identification of syntactic fixedness (Wermter and Hahn (2004), Fazly and Stevenson (2006)) has either focused on a single variation variety, or has only been evaluated for combinations of a small preselected list of words, presumably due to noise. Fazly and Stevenson (2006) propose a measure for detecting the syntactic fixedness of English verb phrases of the same variety as us. Fazly and Stevenson (2006) combine information about syntactic and lexical fixedness (i.e., estimated degree of compositionality) into one measure. In particular, Fazly and Stevenson (2006) look at the correlation between syntactic fixedness (in terms of e.g. passivisation, choice of determiner type and pluralisation) and non-compositionality of verb-noun compounds such as shoot the breeze. In their work on automatically identifying idiom types, Fazly and Stevenson (2006) - henceforth FS06 - show that an idiomatic VNC tends to have one (or at most a small number of) canonical form (s), which are its most preferred syntactic patterns. Fazly and Stevenson (2006) combine information about syntactic and lexical fixedness (i.e., estimated degree of compositionality) into one measure. In a previous study (Fazly and Stevenson, 2006), the authors came up with a dozen possible syntactic forms for verb-object pairs (based on passivization, determiner, and object pluralization) and used a corpus based statistical measure to determine the canonical form (s). Fazly and Stevenson (2006) use statistical measures of syntactic behaviour to gauge whether a verb and noun combination is likely to be a idiom. Certainly it would be worth combining the preferences with other measures, such as syntactic fixedness (Fazly and Stevenson, 2006). Nonetheless, the mentioned characteristics are useful indicators to distinguish literal and idiomatic expressions (Fazly and Stevenson, 2006). Fazly and Stevenson (2006) use lexical and syntactic fixedness as partial indicators of noncompositionality. Similar to Lin (1999), McCarthy et al (2003) and Fazly and Stevenson (2006), our method makes use of automatically generated thesauri; the technique used to compile the thesauri differs from previous work. Combining our semantics-based approach with other extraction techniques such as the syntactic fixedness measure proposed by Fazly and Stevenson (2006) might improve the results significantly.
Finding Terminology Translations From Non-Parallel Corpora We present a statistical word feature, the Word Relation Matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across language groups. Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures. Word Relation Matrices are then mapped across the corpora to find translation pairs. Translation accuracies are around 30% when only the top candidate is counted. Nevertheless, top 20 candidate output give a 50.9% average increase in accuracy on human translator performance. For the computation of related terms and synonyms, Ruge (1995), Landauer and Dumais (1997), and Fung and McKeown (1997) used the cosine measure, whereas Grefenstette (1994, p. 48) used a weighted Jaccard measure. (Fung and McKeown 1997, Kikui 1999, Zhao and Vogel 2002) extracted bilingual word senses, lexicon and parallel sentence pairs from such corpora. In Fung and McKeown (1997), a translation model applied to a pair of unrelated languages (English/Japanese) with a random selection of test words, many of them multi-word terms, gives a precision around 30% when only the top candidate is proposed. For example, in Rapp (1995), Fung and McKeown (1997), Morin et. al.
Espresso: Leveraging Generic Patterns For Automatically Harvesting Semantic Relations this paper, we present a weakly-supervised, general-purpose, and accurate algorithm for harvesting semantic relations. The main contributions are: i) a method for exploiting generic patterns by filtering incorrect instances using the Web; and ii) a principled measure of pattern and instance reliability enabling the filtering algorithm. We present an empirical comof various state of the art systems, on different size and genre corpora, on extracting various general and specific relations. Experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision.   Here we used the Tchai algorithm (Komachi and Suzuki, 2008), a modified version of Espresso (Pantel and Pennacchiotti, 2006) to collect such candidates. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. Short paths are more likely to be generic patterns such as 'of' and can be handled separately as in (Pantel and Pennacchiotti, 2006). We compare our results to two pattern based methods: CDP (the Stage 1 extractor) and Espresso (Pantel and Pennacchiotti, 2006a). In the pattern induction step (section 3.2 in (Pantel and Pennacchiotti, 2006a)), Espresso computes a reliability score for each candidate pattern based on the weighted PMI of the pattern with all instances extracted so far. Others have used classifications based on the requirements for a specific task, such as Information Extraction (Pantel and Pennacchiotti, 2006) or biomedical applications (Stephens et al, 2001). In our study, we use point wise mutual information (Cover and Thomas, 1991) to measure association strength, which has been proved effective in the task of semantic relation identification (Pantel and Pennacchiotti, 2006). Second, we extend Pantel and Pennacchiotti (2006)'s Espresso algorithm, which induces specific reliable LSPs in a bootstrapping manner for entity relation extraction, so that the extended algorithm can apply to event relations (Sections 4.2 to 4.4). This section overviews Pantel and Pennacchiotti (2006)'s Espresso algorithm. Espresso (Pantel and Pennacchiotti, 2006) is also concerned in finding patterns to represent relations. However, our initial experiments suggest that good pattern generalization would have a significant impact on recall, without negative impact on precision, which agrees with findings in the literature (Pantel and Pennacchiotti, 2006).  In this paper, we propose a graph-based approach to seed selection and stop list creation for the state-of-the-art bootstrapping algorithm Espresso (Panteland Pennacchiotti, 2006). To answer these questions, we bootstrapped a minimally-supervised relation extraction algorithm, based on Espresso (Pantel and Pennacchiotti, 2006), with different seed-sets for the various types of part-whole relations, and analyzed the harvested tuples and patterns. The Espresso algorithm (Pantel and Pennacchiotti, 2006) achieves a precision of 80% in learning part whole relations from the Acquaint (TREC-9) corpus of nearly 6M words. Similarly, the minimally-supervised Espresso algorithm (Pantel and Pennacchiotti, 2006) is initialized with a single set that mixes seeds of heterogeneous types, such as leader-panel and oxygen-water, which respectively correspond to the member-of and sub-quantity-of relations in the taxonomy of Keet and Artale (2008). Compared to traditional surface-pattern representations, used by Pantel and Pennacchiotti (2006), dependency paths abstract from surface texts to capture long range dependencies between terms. As IE algorithm for extracting part-whole relations from our texts, we relied on Espresso, a minimally-supervised algorithm, as described by Pantel and Pennacchiotti (2006).
Hierarchical Phrase-Based Translation   present a statistical machine translation model that uses that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations. Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation. We describe our system’s training and decoding methods in detail, and evaluate it for translation speed and translation accuracy. Using BLEU as a metric of translation accuracy, we find that our system performs significantly better than the Alignment Template System, a state-of-the-art phrasebased system. See (Chiang, 2007) for more details. In this section, we express the hierarchical phrase based extraction technique of (Chiang, 2007) as an extraction program. For instance, (Chiang, 2007) lists six criteria that he uses in practice to restrict the generation of Hiero rules. Hierarchical Phrase-based Machine Translation, proposed by Chiang (Chiang, 2007), uses a general non-terminal label X but does not use linguistic information from the source or the target language. The hierarchical phrase-based model (Chiang, 2007) makes an advance of statistical machine translation by employing hierarchical phrases, which not only uses phrases to learn local translations but also uses hierarchical phrases to capture reorderings of words and subphrases which can cover a large scope. We evaluate the distribution of these rules in the same way as Chiang (2007). So we follow the method described in Chiang (2007) to filter the rule set except that we allow two nonterminals to be adjacent. We compared our loose decoder and tight decoder with our in-house hierarchical phrase-based decoder (Chiang, 2007) and the tree-to-string decoder (Liu et al., 2006). Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a subtranslation (Huang and Chiang, 2007), whereas a phrase-based decoder only needs to do this at one end since the translation is always growing left-to-right. The complexity of this dynamic programming algorithm for g-gram decoding is O(2nn 2 |V | g−1 ) where n is the sentence length and |V | is the English vocabulary size (Huang and Chiang, 2007). This is also the case with other syntax-based models like Hiero or GHKM: language model integration overhead is the most significant factor that causes syntax-based decoding to be slow (Chiang, 2007). Following previous work (Chiang, 2007), we assume a constant number of English translations for each foreign word in the input sentence, so |V|= O (n). The work of Watanabe et al (2006) is closest in spirit to ours: they also design an incremental decoding algorithm, but for the hierarchical phrase-based system (Chiang, 2007) instead. A Hiero grammar (Chiang, 2007) is an SCFG with only one type of nonterminal symbol, traditionally labeled X. Chiang (2007) gives reasonable heuristic choices for these parameters when extracting a Hiero grammar, and Lopez (2008) confirms some of them (maximum rule span of 10, maximum number of sourceside symbols at 5, and maximum number of nonterminals at 2 per rule). Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). The first decoder, Hiero Cube Pruning (HCP), is a k-best decoder using cube pruning implemented as described by Chiang (2007). For the HCP system, MET is done following Chiang (2007). A language model was incorporated using cube pruning (Huang and Chiang, 2007), using a 200 best limit at each node during LM integration.
Evaluating Content Selection In Summarization: The Pyramid Method We present an empirically grounded method for evaluating content selection in summarization. It incorporates the idea that no single best model summary for a collection of documents exists. Our method quantifies the relative importance of facts to be conveyed. We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference. It also corresponds to the pyramid measure proposed by Nenkova and Passonneau (2004), which also considers an estimation of the maximum value reachable. In evaluations of summarization algorithms, it is common practice to derive the gold standard con tent importance scores from human summaries, as done, for example, in the pyramid method, where the importance of a content element corresponds to the number of reference human summaries that make use of it (Nenkova and Passonneau, 2004). To evaluate our system, we use the pyramid evaluation method (Nenkova and Passonneau, 2004) at sentence level. Each fact in the citation summary of a paper is a summarization content unit (SCU) (Nenkova and Passonneau, 2004), and the fact distribution matrix, created by annotation, provides the information about the importance of each fact in the citation summary. These annotations give the list of nuggets covered by each sentence in each citation summary, which are equivalent to the summarization content unit (SCU) as described in (Nenkova and Passonneau, 2004). The second metric we use is similar to Pyramid (Nenkova and Passonneau, 2004), which has been used for summarization evaluation. In the PYRAMID scheme for manual evaluation of summaries (Nenkova and Passonneau, 2004), machine-generated summaries were compared with human-written ones at the nugget level. the PYRAMID framework (Nenkova and Passonneau, 2004) was used for manual summary evaluations. Pyramid evaluation: The pyramid evaluation method (Nenkova and Passonneau, 2004) has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations (Nenkova et al, 2007). Paralleling work in summarization, it is hypothesized that the quality of a rewritten story can be defined by the presence or absence of 'semantic content units' that are crucial details of the text that may have a variety of syntactic forms (Nenkova and Passonneau, 2004). Three most noticeable efforts in manual evaluation are SEE (Lin and Hovy, 2001), Factoid (Van Halteren and Teufel, 2003), and the Pyramid method (Nenkova and Passonneau, 2004). This paraphrase matching process is observed in the Pyramid annotation procedure shown in (Nenkova and Passonneau, 2004) over three summary sets (10 summaries each). From an in-depth analysis on the manually created SCUs of the DUC2003 summary set D30042 (Nenkova and Passonneau, 2004), we find that 54.48% of 1746 cases where a non-stop word from one SCU did not match with its supposedly human-aligned pairing SCUs are in need of some level of paraphrase matching support. To assess the quality of system responses, we adopt the nugget-based methodology used previously for many types of complex questions (Voorhees, 2003), which shares similarities with the pyramid evaluation scheme used in summarization (Nenkova and Passonneau, 2004). They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). Pyramid (Nenkova and Passonneau, 2004) is a manually evaluated measure of recall on facts or Semantic Content Units appearing in the reference summaries. The pyramid method (Nenkova and Passonneau, 2004) addresses the problem by using multiple human summaries to create a gold-standard. As an example, the Pyramid Method (Nenkova and Passonneau, 2004), represents a good first attempt at a realistic model of human variations. Nenkova and Passonneau (2004) proposed a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents. In recent years the Pyramids evaluation method (Nenkova and Passonneau, 2004) was introduced.
Efficient Parsing Of Highly Ambiguous Context-Free Grammars With Bit Vectors An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser computes a compact parse forest representation of the complete set of possible analyses forlarge treebank grammars and long input sen tences. The parser uses bit-vector operations to parallelise the basic parsing operations. The parser is particularly useful when all analyses are needed rather than just the most probable one. For parsing, we mapped all unknown words to unknown word symbols, and applied the Viterbi algorithm as implemented in Schmid (2004), exploiting its ability to deal with highly-ambiguous grammars. The starred results are statistically significant improvements over the Baseline (at confidence p > 0.05). English side of the bilingual data was parsed using the Charniak parser of Charniak and Johnson (2005), and the German side was parsed using BitPar (Schmid, 2004) without the function and morphological annotations. The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR, a morphological analyzer/generator of German (Schmid et al 2004) and the BitPar parser, which is a state-of-the-art parser of German (Schmid, 2004). We use a frequency-based notation because we use out of-the-box software Bitpar (Schmid, 2004) which implements inside-outside estimation Bitpar reads in frequency models and converts them to relative frequency models. The reestimation was carried out using Bitpar (Schmid,2004) for inside-outside estimation. For our parsing results we use BitPar, a fast and freely available general PCFG parser (Schmid, 2004). The German V-O pairs were extracted from a syntactic analysis of the HGC carried out using the BitPar parser (Schmid, 2004). We used only V-O pairs because they cons ti tute far more sense-discriminative contexts than, for example, verb-subject pairs, but we plan to examine these and other grammatical relationships in future work. An existing SCFG parser (Schmid, 2004) was then used, with a simple unknown word heuristic, to generate the Viterbi n-best parses with n= 100, and, after removing the address labels, all equal parses and their probabilities were summed, and the one with highest probability chosen. Compact binarization (Schmid, 2004) tries to minimize the size of the binarized grammar. Compact binarization was introduced in Schmid (2004), based on the intuition that a more compact grammar will help achieve a highly efficient CKY parser, though from our experiment it is not always true. We use a general-purpose CKY parser (Schmid, 2004) to exhaustively parse the sentences, and we strip off all model-specific information prior to evaluation. This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser,10 together with various tree bank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis. We experimented with increasingly rich grammars read off of the tree bank.  BitPar (Schmid, 2006) is a probabilistic context free parser using bit-vector operations (Schmid, 2004). For Experiment II we trained BitPar (Schmid, 2004), a parser for highly ambiguous PCFG grammars, on the two tree banks.  The remaining sentences are part-of speech tagged and lemmatized using TreeTagger (Schmid, 2004). We tokenize and sentence split the data with the default DKProsegmenter, and then use TreeTagger (Schmid, 2004) to POS-tag and chunk the sentences. In our experiments, we used the BitParparser (Schmid, 2004) and a PCFG which was extracted from a version of the PENN tree bank that was automatically annotated with features in the style of (Klein and Manning, 2003). We parse all German and English articles with BitPar (Schmid, 2004) to extract verb-argument relations.
Feature-Rich Part-Of-Speech Tagging With A Cyclic Dependency Network We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result. Recently, Toutanova et al (2003) presented a supervised conditional Markov Model part-of-speech tagger (CMM) which exploited information coming from both left and right contexts. The algorithms were trained and tested using version 3 of the Penn Treebank, using the training, development, and test split described in Collins (2002) and also employed by Toutanova et al (2003) in testing their supervised tagging algorithm. In the future, we will consider making an increase the context-size, which helped Toutanova et al (2003). We use the Stanford POS Tagger (Toutanova et al, 2003) to tokenize and POS tag English and German sentences. We then obtain their POS N-grams from the Stanford POS tagger (Toutanova et al 2003), and count how many of them have the POS N-gram. We used the Stanford tagger (Toutanova et al, 2003 ) v3.1, with the MEMM model. For the discriminative distortion models, we tag the pre-processed input using the log-linear POS tagger of Toutanova et al (2003). We use the log-linear tagger of Toutanova et al (2003), which gives 96.8% accuracy on the test set. We also added part of speech (POS) tags to the data using the tagger of Toutanova et al (2003), and used the tags to decide if mentions were plural or singular. Text is tagged using the Stanford POS tagger (Toutanova et al, 2003). Decoding is performed with the Viterbi algorithm. We also evaluate state-of-the-art Maximum Entropy taggers: the Stanford Left tagger (Toutanova and Manning, 2000). We then perform POS tagging using the Stanford POS tagger (Toutanova et al, 2003). We tag the source language with the Stanford POS tagger (Toutanova et al, 2003). They surpassed their earlier work in 2003 with acyclic dependency network tagger, achieving 97.2% /89.05% (seen/unseen) (Toutanova et al, 2003). Text was tagged using the Stanford POS tagger (Toutanova et al., 2003). All data is tokenized, POS tagged (Toutanova et al, 2003) and lemmatized, resulting in 341,557 sense definitions and 3,563,649 words. Our system reports an error rate of 2.67% on the standard PTB test set, a relative 3.3% error reduction of the previous best system (Toutanova et al, 2003) by using fewer features. Toutanova et al (2003) reported a POS tagger based on cyclic dependency network.  Compared to previous best result on the same dataset, 2.76% by (Toutanova et al, 2003), our best result shows a relative error reduction of 3.3%.
Detecting Text Similarity Over Short Passages: Exploring Linguistic Feature Combinations Via Machine Learning We present a new composite similarity metric that combines information from multiple linguistic indicators to measure semantic distance between pairs of small textual units. Several potential features are investigated and an optimal combination is selected via machine learning. We discuss a more restrictive definition of similarity than traditional, document-level and information retrieval-oriented, notions of similarity, and motivate it by showing its relevance to the multi-document text summarization problem. Results from our system are evaluated against standard information retrieval techniques, establishing that the new method is more effective in identifying closely related textual units. 1 Research Goals In this paper, we focus on the problem of detecttwo small textual units (paragraphor sentence-sized) contain common information, as a necessary step towards extracting such common information and constructing thematic groups of text units across multiple documents. pieces of text has many applications (e.g., summarization, information retrieval, text clustering). Most research in this area has centered on detecting similarity between documents [Willet 1988], similarity between a query and a document [Salton 1989] or between a query and a segment of a document [Callan 1994]. While effective techniques have been developed for document clustering and classification which depend on inter-document similarity measures, these techniques mostly on shared words, or occasionally collocations of words [Smeaton 1992]. When larger units of text are compared, overlap may be sufficient to detect similarity; but when the units of text are small, simple surface matching of words and phrases is less likely to succeed since the number of potential matches is smaller. Our task differs from typical text matching applications not only in the smaller size of the text units compared, but also in its overall goal. Our notion of similarity is more restrictive than topical similarity—we provide a detailed definition in the next section. We aim to recover small textual units from a of documents so that each text unit within a set describes the same action. syswhich is fully implemented, is motivated by the need for determining similarity between small pieces of text across documents that potentially span different topics during multi-document summarization. It serves as the first component of a domain-independent multisummarization system which generates a through reformulation [Barzilay al. by combining information from these similar text passages. We address concerns of sparse data and the narrower than topical definition of similarity by exploring several linguistic features, in addition to shared words or collocations, as indicators of similarity. Our include linked noun phrases, WordNet synonyms, and similar verbs. We also define comover pairs of features. provide an effective method for aggregating the feature values into a similarity measure using machine learning, and present results 203 on a manually annotated corpus of 10,345 pairs of compared paragraphs. Our new features, and especially the composite ones, are shown to outperform traditional techniques such as TF*IDF [Buckley 1985; Salton 1989] for determining similarity over small text units. 2 Definition of Similarity Similarity is a complex concept which has been widely discussed in the linguistic, philosophical, and information theory communities. For example, Frawley [1992] discusses all semantic typing in terms of two mechanisms: the detection of similarity and difference. Jackendoff [1983] argues that standard semantic relations such as synonymy, paraphrase, redundancy, and entailment all result from judgments of likeness whereas antonymy, contradiction, and inconsistency derive from judgments of difference. Losee [1998] reviews notions of similarity and their impact on information retrieval techniques. For our task, we define two text units as similar if they share the same focus on a common concept, actor, object, or action. In addition, the common actor or object must perform or be subjected to the same action, or be the subject of the same description. For example, Figure 1 shows three input text fragments (paragraphs) taken from the TDT pilot corpus (see Section 5.1), all from the same topic on the forced landing of a U.S. helicopter in North Korea. We consider units (a) and (b) in Figure 1 to be similar, because they both focus on the same event (loss of contact) with the same primary participant (the helicopter). On the other hand, unit (c) in Figure 1 is not similar to either (a) or (b). Although all three refer to a helicopter, the primary focus in (c) is on the emergency landing rather than the loss of contact. We discuss an experimental validation of our similarity definition in Section 5.2, after we introduce the corpus we use in our experiments. 3 Related Work Although there is related empirical research on determining text similarity, primarily in the information retrieval community, there are two major differences between the goals of this earlier work and the problem we address in this (a) An OH-58 helicopter, carrying a crew of two, was on a routine training orientation when contact was lost at about 11:30 a.m. Saturday (9:30 p.m. EST Friday). (b) &quot;There were two people on board,&quot; said Bacon. &quot;We lost radar contact with the helicopter about 9:15 EST (0215 GMT).&quot; (c) An OH-58 U.S. military scout helicopter made an emergency landing in North Korea at about 9.15 p.m. EST Friday (0215 GMT Saturday), the Defense Department said. Figure 1: Input text units (from the TDT pilot corpus, topic 11). paper. First, the notion of similarity as defined in the previous section is more restrictive than the traditional definition of similarity [Anderberg 1973; Willet 1988]. Standard notions of similarity generally involve the creation of a vector or profile of characteristics of a text fragment, and then computing on the basis of frequencies the distance between vectors to determine conceptual distance [Salton and Buckley 1988; Salton 19891. Features typically include stemmed words although sometimes multi-word units and collocations have been used [Smeaton 1992], as well as typological characteristics, such as thesaural features. The distance between vectors for one text (usually a query) and another (usually a document) then determines closeness or similarity [van Rijsbergen 1979]. In some cases, the texts are represented as vectors of sparse n-grams of word occurrences and learning is applied over those vectors [Schapire and Singer 1999]. But since our definition of similarity is oriented to the small-segment goal, we make more fine-grained distinctions. Thus, a set of passages that would probably go into the same class by standard IR criteria would be further separated by our methods. Second, we have developed a method that functions over pairs of small units of text, so the size of the input text to be compared is different. This differs from document-to-document 204 or query-to-document comparison. A closely related problem is that of matching a query to the relevant segment from a longer document [Callan 1994; Kaszkiel and Zobel 1998], which primarily involves determining which segment of a longer document is relevant to a query, whereas our focus is on which segments are similar to each other. In both cases, we have less data to compare, and thus have to explore additional or more informative indicators of similarity. 4 Methodology compute a feature vector over a pair of texunits, where features are either of one characteristic, or consisting of pairs of primitive features. 4.1 Primitive Features draw on a number of linguistic approaches to text analysis, and are based on both single words and simplex noun phrases (head preceded by optional premodifiers with no embedded recursion). Each of these syntactic, and semantic several variations. We thus consider following potential matches between text units: Word co-occurrence, sharing a single word between text units. Variations of this feature restrict matching to cases where the parts of speech of the words also match, or relax it to cases where just the stems of the two words are identical. Matching noun phrases. the LINKIT tool [Wacholder 1998] to identify simplex noun phrases and match those that share the same head. WordNet synonyms. provides sense information, placing in sets of synonyms match words that appear in the same synset. Variations on this feature restrict the words considered to a specific part-of-speech class. • Common semantic classes for verbs. Levin's [1993] semantic classes for verbs have been found to be useful for determining document type and text similarity [Klavans and Kan 1998]. We match two verbs that share the same semantic class. Shared proper nouns. nouns are using the set [Abal. Variations on proper noun matching include restricting the proper noun type to a person, place, or an organization subcategories are also extracted entity finder). In order to normalize for text length and frequency effects, we experimented with two types of optional normalization of feature values. The first is for text length (measured in words), where each feature value is normalized by the of the textual units in the pair. for of textual units feature values are divided by: length(A) x length(B) (1) This operation removes potential bias in favor longer text The second type of normalization we examined was based on the relative frequency of occurrence of each primitive. This is motivated the fact that infrequently primiare likely to have higher impact on similarity than primitives which match more frequently. We perform this normalization in manner similar to the IDF part of Every primitive element is associated with a value which is the number of textual units in which the primitive appeared in the corpus. For a primitive element which compares single words, this is the number of textual units which contain that word in the corpus; for a noun phrase, this is the number of textual units that contain noun phrases that share the same head; and similarly for other primitive types. We multiply each feature's value by: number of textual units (2) Number of textual units containing this primitive Since each normalization is optional, there are four variations for each primitive feature. 4.2 Composite Features addition to the above that compare single items from each text unit, we which combine pairs of primitive features. Composite features are defined by placing different types of restrictions on the participating primitive features: 205 Figure 2: A composite feature over word primitives with a restriction on order would count the pair &quot;two&quot; and &quot;contact&quot; as a match because they occur with the same relative order in both textual units. An 011-58 helicopter, carrying a crew of orientation when c ntac as lost (9:30 p.m. EST Friday). (a) was on a routine training out 11:30 a.m. Saturday (b) &quot;There were[twolpeople on board,&quot; said Bacon. &quot;We lost radar with the helicopter about 9:15 EST (0215 GMT).&quot; Figure 3: A composite feature over word primitives with a restriction on distance would match on the pair &quot;lost&quot; and &quot;contact&quot; because they occur within two words of each other in both textual units. :0}145theligoPterjearryinga crew of two, was on a routine training (a) rientation when contact was ft t about 11:30 a.m. Saturday ('10 p.m. EST Friday). (b) &quot;T ere were two people on board,&quot; said Bacon. &quot;Wetradar contact with th.elielico ter bout 9:15 EST (0215 GMT).&quot; Figure 4: A composite feature with restrictions on the primitives' type. One primitive must be a matching simplex noun phrase (in this case, a helicopter), while the other primitive must be a matching verb (in this case, &quot;lost&quot;.) The example shows a pair of textual units where this composite feature detects a valid match. An 011-58 helicopter, carrying a crew of two, was on a routine training (a) orientation when vas Li t about 11:30 am. Saturday (9:30 p.m. EST Friday). (b) &quot;There were two people on board,&quot; said Bacon. &quot;W with the helicopter about 9:15 EST (0215 GMT).&quot; Ordering. pairs of primitive elements are required to have the same relative order in both textual units (see Figure 2). Distance. pairs of primitive elements are required to occur within a certain distance in both textual units (see Figure 3). The maximum distance between the primitive elements can vary as an additional parameter. A distance of one matches rigid collocations whereas a distance of five captures related primitives within a region of the text unit [Smeaton 1992; Smadja 1993]. Primitive. element of the pair of primitive elements can be restricted to a specific primitive, allowing more expressiveness in the composite features. For example, we can restrict one of the primitive features to be a simplex noun phrase and the other to be a verb; then, two noun phrases, one from each text unit, must match according to the rule for matching simplex noun phrases (i.e., sharing the same head), and two verbs must match according to the rule for verbs (i.e., sharthe same semantic class); see Figure This particular combination loosely approximates grammatical relations, e.g., matching subject-verb pairs. 'Verbs can also be matched by the first (and more reof Section 4.1, namely requiring that their stemmed forms be identical. 206 Since these restrictions can be combined, many different composite features can be defined, although our empirical results indicate that the most successful tend to include a distance constraint. As we put more restrictions on a composite feature, the fewer times it occurs in the corpus; however, some of the more restrictive features are most effective in determining similarity. Hence, there is a balance between the discriminatory power of these features and applicability to number of cases. features are normalized features are (i.e., for text unit length and for frequency of occurrence). This type of normalization also uses equation (2) but averages the normalization values of each primitive in the composite feature. 4.3 Learning a Classifier For each pair of text units, we compute a vector of primitive and composite feature values. To determine whether the units match overall, we employ a machine learning algorithm, RIP- PER [Cohen 1996], a widely used and effective rule induction system. RIPPER is trained over a corpus of manually marked pairs of units; we discuss the specifics of our corpus and of the annotation process in the next session. We experwith varying RIPPER's ratio, measures the cost of a false positive relative to that of a false negative (where we view &quot;similar&quot; as the positive class), and thus controls the relative weight of precision versus recall. This is an important step in dealing with the sparse data problem; most text units are not similar, given our restrictive definition, and thus positive instances are rare. 5 Results 5.1 The Evaluation Corpus For evaluation, we use a set of articles already classified into topical subsets which we obtained from the Reuters part of the 1997 pilot Topic Detection and Tracking (TDT) corpus. The TDT corpus, developed by NIST and DARPA, is a collection of 16,000 news articles from Reuters and CNN where many of the articles and transcripts have been manually grouped into 25 categories each of which corresponds a single event (see //morph. ldc edu/Catalog/LDC98T25 .html). the Reuters part of the corpus, we selected five of the larger categories and extracted all articles assigned to them from several randomly chosen days, for a total of 30 articles. Since paragraphs in news stories tend to be short—typically one or two sentences—in this study we use paragraphs as our small text units, although sentences would also be a possibility. In total, we have 264 text units and 10,345 comparisons between units. As comparisons are made between all pairs of paragraphs from the same topic, the total number of comparisons is equal to 2 the number of paragraphs in all selected articles from topical category i. Training of our machine learning component was done by three-fold cross-validation, ransplitting the pairs paragraphs into three (almost) equally-sized subsets. In each of the three runs, two of these subsets were used for training and one for testing. To create a reference standard, the entire collection of 10,345 paragraph pairs was marked for by two reviewers who were given definition and detailed instructions. Each reindependently marked each paragraphs as similar or not similar. Subsequently, the two reviewers jointly examined cases where was disagreement, discussed reasons, reconciled the differences. 5.2 Experimental Validation of the In order to independently validate our definiof similarity, we performed additional experiments. In the first, we asked three addijudges to determine a ransample 40 paragraph pairs. High agreement between judges would indicate that our definition of similarity reflects an objective reality and can be mapped unambiguously to an operational procedure for marking text units as similar or not. At the same time, it would also validate the judgments between text units that we use for our experiments (see Section 5.1). this task, judges were given opportuprovide reasons for claiming similarity or dissimilarity, and comments on the task were for future analysis. three additional 207 judges agreed with the manually marked and standardized corpus on 97.6% of the comparisons. Unfortunately, approximately 97% (depending on the specific experiment) of the comparisons in both our model and the subsequent validation experiment receive the value &quot;not similar&quot;. This large percentage is due to our finegrained notion of similarity, and is parallel to happens in randomly sampled collections, since in that case most documents will not be relevant to any given query. Nevertheless, we can account for the high probability of inter-reviewer agreement expected by chance, 0.97.0.97+ (1 —0.97)- (1-0.97) --- 0.9418, by referring to the kappa statistic [Cohen 1960; Carletta 1996]. The kappa statistic is defined as PA PO K — the probability that two reviewers agree in practice, and Po is the probability that they would agree solely by chance. In our case, 0.9418, and = indicating that the observed agreement by the is indeed If Po is estimated from the particular sample used in this experiment rather than from our entire corpus, it would be only 0.9, producing a value of 0.76 In addition to this validation experiment that used randomly sampled pairs of paragraphs (and reflected the disproportionate rate of occurrence of dissimilar pairs), we performed a balanced experiment by randomly selecting 50 of the dissimilar pairs and 50 of the similar pairs, in a manner that guaranteed generation an independent Pairs in this subset were rated for similarity by two additional independent reviewers, who agreed on their decisions 91% of the time, versus 50% expected chance; in this case, = Thus, we feel confident in the reliability of our annotation is always between 0 and 1, with 0 indicating no better agreement than expected by chance and 1 indicating perfect agreement. guarantee independence, pairs of paragraphs were randomly selected for inclusion in the sample a pair (A, immediately rejected if there were paragraphs Xi, , X.n. for n > 0 such that all (X1, X2), . . . , , 13) already been included in the sample. process, and can use the annotated corpus to assess the performance of our similarity measure and compare it to measures proposed earlier in the information retrieval literature. 5.3 Performance Comparisons We compare the performance of our system to three other methods. First, we use standard TF*IDF, a method that with various alterations, remains at the core of many information retrieval and text matching systems [Salton and Buckley 1988; Salton 1989]. We compute the total frequency (TF) of words in each text unit. We also compute the number of units each word appears in in our training set (DF, or document frequency). Then each text unit is represented as a vector of TF*IDF scores calculated as Similarity between text units is measured by the cosine of the angle between the corresponding two vectors (i.e., the normalized inner product of the two vectors). A further cutoff point is selected to convert similarities to hard decisions of &quot;similar&quot; or &quot;not similar&quot;; different cutoffs result in different tradeoffs between recall and precision. Second, we compare our method against a standard, widely available information retrieval system developed at Cornell University, [Buckley SMART utilizes a modified TF*IDF measure (ATC) plus stemming and a fairly sizable stopword list. Third, we use as a baseline method the default selection of the most frequent category, i.e., &quot;not similar&quot;. While this last method cannot be effectively used to identify similar paragraphs, it offers a baseline for the overall accuracy of any more sophisticated technique for this task. 5.4 Experimental Results Our system was able to recover 36.6% of the similar paragraphs with 60.5% precision, as shown in Table 1. In comparison, the unmodiobtained only 32.6% precision when recall is 39.1%, i.e., close to our system's recall; and only 20.8% recall at precision of 62.2%, comparable to our classifier's used version 11.0 of SMART, released in July 1992. • log number of units 208 Recall Precision Accuracy Machine learning over linguistic indicators 36.6% 60.5% 98.8% TF*IDF 30.0% 47.4% 97.2% SMART 29.1% 48.3% 97.1% Default choice (baseline) 0% undefined 97.5% Table 1: Experimental results for different similarity metrics. For comparison purposes, we list the average recall, precision, and accuracy obtained by TF*IDF and SMART at the two points in the precision-recall curve identified for each method in the text (i.e., the point where the method's precision is most similar to ours, and the point where its recall is most similar to ours). precision. SMART (in its default configuration) offered only a small improvement over the base TF*IDF implementation, and significantly underperformed our method, obtaining 34.1% precision at recall of 36.7%, and 21.5% recall at 62.4% precision. The default method of always marking a pair as dissimilar obtains of course 0% recall and undefined precision. Figure 5 illustrates the difference between our system and straight TF*IDF at different points of the precision-recall spectrum. When overall accuracy (total percentage of correct answers over both categories of similar and non-similar pairs) is considered, the numbers are much closer together: 98.8% for our approach; 96.6% and 97.8% for TF*IDF on the two P-R points mentioned for that method 96.5% and for SMART, again at the two P-R points mentioned for SMART and 97.5% for the default Nevertheless, since the challenge of identifying sparsely occurring similar small text units is our goal, the accuracy measure and the baseline technique of classifying everything as not similar are included only for reference but do tests of significance cannot be performed for cmnparing these values, since paragraphs appear in multiple comparisons and consequently the comparisons are not independent. Figure 5: Precision-recall graph comparing our using line with squares) versus TF*IDF (dotted line with triangles). not reflect our task. 6 Analysis and Discussion of Feature Performance We computed statistics on how much each feature helps in identifying similarity, summarized in Table 2. Primitive features are named acto the type of the feature (e.g., the feature that counts the number of matching verbs according to exact matches). Composite feature names indicate the restrictions applied to primitives. For example, the composite fea- < a pair of matching primitives to occur within a relative distance of four words. If the composite feature also restricts the types of the primitives in the pair, the name of the restricting primitive feature is added to the composite feature name. For exthe feature named Distance < 5 requires one member of the pair to be a verb and the relative distance between the primitives to be at most five. The second column in Table 2 shows whether the feature value has been normalized accordto its overall while the third column indicates the actual threshold used in decisions assuming that only this feature is used for clas- The fourth column shows the applicathat feature, that is, the percentage of results reported in Table 2 include our first norstep that accounts for the difference in length of text units. 209 Feature Name Normalized? Threshold Applicability Recall Precision Any word Yes 0.360 2.2% 31.4% 41.8% 0.505 16.7% 75.4% Noun Yes 0.150 8.1% 43.2% 15.9% 0.275 1.5% 20.9% 37.0% Proper noun Yes 0.200 0.2% 2.0% 30.8% Verb No 0.775 ' 10.6% 19.7% 1.6% Simplex NP Yes 0.150 5.7% 35.5% 18.6% 0.275 2.7% 10.1% 44.6% 0.350 0.7% 3.7% 69.2% Semantic class of verbs No 0.875 0.1% 2.0% 3.4% WordNet Yes 0.250 5,4% 4.1% 2.3% Distance < 2 Yes 0.075 4.7% 24.9% 15.7% Distance < 3 Yes 0.250 0.5% 10.2% 55.6% Distance < 4 Yes 0.275 1.9% 14.6% 50.0% Distance < 5 Yes 0.200 1.9% 22.4% 53.4% Order Distance < 5 Yes 0.200 1.5% 20.4% 40.7% Noun Distance < 5 Yes 0.175 1.9% 21.2% 31.9% Verb Distance < 5 Yes 0.200 0.3% 7.3% 66.7% No 0.850 0.6% 11.0% 56.3% Table 2: Statistics for a selected subset of features. Performance measures are occasionally given multiple times for the same feature and normalization option, highlighting the effect of different decision thresholds. paragraph pairs for which this feature would apply (i.e., have a value over the specified threshold). Finally, the fifth and sixth columns show the recall and precision on identifying similar paragraphs for each independent feature. Note that some features have low applicability over the entire corpus, but target the hard-to-find similar pairs, resulting in significant gains in recall and precision. Table 2 presents a selected subset of primitive and composite features in order to demonstrate our results. For example, it was not surprising to observe that the most effective primitive feain determining similarity are word, NP, other primitives as not as effective independently. This is to be expected since nouns name objects, entities, and concepts, and frequently exhibit more sense constancy. In contrast, verbs are functions and tend to shift senses in a more fluid fashion depending on context. Furthermore, our technique does not label phrasal verbs (e.g. look up, look out, look over, look for, etc. ), which are a major source of verbal ambiguity in English. Whereas primitive features viewed independently might not have a directly visible effect on identifying similarity, when used in composite features they lead to some novel results. The pronounced case of this is for the composite feature Distance < can help identify similarity effectively, as seen in Table 2. This composite feature approximates verb-argument and verb-collocation relations, which are strong indicators of similarity. At the same time, the more restrictive a feature is, the fewer occurrences of that feature appear in the training set. This suggests that we could consider adding additional features suggested by current results in order to further refine and improve our similarity identification algorithm. 7 Conclusion and Future Work We have presented a new method to detect similarity between small textual units, which combines primitive and composite features using machine learning. We validated our similarity definition using human judges, applied 210 our method to a substantial number of paragraph pairs from news articles, and compared results to baseline and standard information retrieval techniques. Our results indicate that our method outperforms the standard techniques for detecting similarity, and the system has been successfully integrated into a larger multipledocument summarization system [McKeown et We are currently working on incorporating a clustering algorithm in order to give as output a set of textual units which are mutually similar rather than just pairwise similar. Future work includes testing on textual units of different size, comparing with additional techniques proposed for document similarity in the information retrieval and computational linguistics literature, and extending the feature set to incorporate other types of linguistic information in the statistical learning method. Acknowledgments We are grateful to Regina Barzilay, Hongyan Jing, Kathy McKeown, Shimei Pan, and Yoram Singer for numerous discussions of earlier versions of this paper and for their help with setting up and running RIPPER and SMART. This research has been supported in part by an NSF STIMULATE grant, IRI-96-1879. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the National Science Foundation. This extension will require using a more selective alignment technique (similar to that of (Hatzivassiloglou et al., 1999)). Another approach (Hatzivassiloglou et al,1999) has been to use a machine learning algorithm in which features are based on combinations of simple features (e.g., a pair of nouns appear within 5 words from one another in both texts). We use SimFinder (Hatzivassiloglou et al, 1999) for sentence clustering and the f-measure for word overlap to compare noun phrases. We use SimFinder (Hatzivassiloglou et al, 1999) for sentence clustering and its similarity metric to evaluate cluster quality; SimFinder outputs similarity values (simvals) between 0 and 1 for pairs of sentences, based on word overlap, synonymy andn-gram matches. We then cluster the simplified sentences withSimFinder (Hatzivassiloglou et al, 1999). At the level of short passages or sentences, (Hatzivassiloglou et al, 1999) goes beyond N-gram, taking advantage of WordNet synonyms, as well as ordering and distance between shared words. For instance, Hatzivassiloglou et al (1999) trained a classifier for paraphrase detection, though their performance only reached roughly 37% recall and 61% precision. In the context of multi document summarization, SimFinder (Hatzivassiloglou et al, 1999) identifies sentences that convey similar information across in put documents to select the summary content. An obvious choice for a baseline in this task is the following: any two sentences are considered aligned if their cosine similarity exceeds a certain threshold. We also compare our algorithm with two state-of the-art systems, SimFinder (Hatzivassiloglou et al, 1999) and Decomposition (Jing, 2002). To this end, we intend to implement a second-pass analysis that would rerank the candidates produced by fuzzy inverted generation by computing text similarities over short passages such as those propose din (Hatzivassiloglou et al, 1999). Hatzivassiloglou et al (1999) proposed to use linguistic features as indicators of text similarity to address the problem of sparse representation of sentences.
Statistical Decision-Tree Models For Parsing Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to textprocessing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing n-grain modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATachieves 86% precision, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% precision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length. The conversion uses head propagation rules to find the head on the right-hand side of the CFG rules, first proposed for English in (Magerman, 1995). Not to mention earlier non-PCFG lexicalized statistical parsers, notably Magerman (1995) for the Penn Treebank, also modified the treebank to contain different labels for standard and for base noun phrases. Furthermore, subtrees with terminal symbols can be viewed as learning dependencies among the words in the subtree, obviating the need for the manual specification (Magerman, 1995) or automatic inference (Chiang and Bikel, 2002) of lexical dependencies. The probability models of Charniak (1997), Magerman (1995) and Ratnaparkhi (1997) dier in their details but are based on similar features. FTB-UC-DEP is a dependency tree bank derived from FTB-UC using the classic technique of head propagation rules, first proposed for English by Magerman (1995). Decision trees have been applied for feature selection for statistical parsing models by Magerman (1995) and Haruno et al. This is a similar, but more limited, strategy to the one used by Magerman (1995). In both cases, we report PARSEVAL labeled bracket scores (Magerman, 1995), with the brackets labeled by syntactic categories but not grammatical functions. We apply canonical lexical head projection rules (Magerman, 1995) in order to lexicalize syntactic trees. In addition to antecedent recovery, we also report parsing accuracy, using the bracketing F-Score, the combined measure of PARSEVAL-style labeled bracketing precision and recall (Magerman, 1995). These features are also computed for the head of the phrase, determined using a set of head finding rules in the style of Magerman (1995) adapted to TiGer. The head word is identified by using the head-percolation table (Magerman, 1995). For example, statistical parsers from Magerman (1995) on use features based on head-dependent relationships. The head word is identified by using the head percolation table (Magerman, 1995). For each possible constituent in a parse tree, rules first described in (Magerman, 1995) and (Jelinek et al, 1994) identify the head-child and propagate the head-word to its parent. Lexical heads have been calculated using the projection rules of Magerman (1995), and annotated between brackets. The conventional wisdom since Magerman (1995) has been that lexicalization substantially improves performance compared to an unlexicalized baseline model (e.g., a probabilistic context-free grammar, PCFG). The input to our sentence realiser are bag of words with dependency constraints which are automatically extracted from the Penn tree bank using head percolation rules used in (Magerman, 1995), which do not contain any order information.  
Identifying Semantic Roles Using Combinatory Categorial Grammar We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar. This system performs at least as well as a system based on a traditional Treebank parser, and outperforms it on core argument roles. The Gildea and Hockenmaier (2003) system uses features extracted from Combinatory Categorial Grammar (CCG) corresponding to the features that were used by G&J and G&P systems. This analysis allows recovery of verbal arguments of nominalised raising and control verbs, a construction which both Gildea and Hockenmaier (2003) and Boxwell and White (2008) identify as a problem case when aligning Propbank and CCGbank. Conventionally there are two kinds of methods for role assignments, one is using only statistical information (Gildea and Jurafsky, 2002) and the other is combining with grammar rules (Gildea and Hockenmaier, 2003).  Gildea and Hockenmaier (2003) report that using features extracted from a Combinatory Categorial Grammar (CCG) representation improves semantic labeling performance on core arguments. However, this mismatch is significantly less than the 23% mismatch reported in (Gildea and Hockenmaier, 2003) between the CCGBank and an earlier version of the PropBank. In order to generalize the path feature (see Table 1 in Section 3) which is probably the most salient (while being the most data sparse) feature for SRL, previous work has extracted features from other syntactic representations, such as CCG derivations (Gildea and Hockenmaier, 2003) and dependency trees (Hacioglu, 2004) or integrated features from different parsers (Pradhan et al, 2005b). For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. As with a previous approach in CCG semantic role labeling (Gildea and Hockenmaier, 2003), this feature shows the exact nature of the syntactic dependency between the predicate and the word we are considering, if any such dependency exists. We also compare with the CCG-based SRL presented in (Gildea and Hockenmaier, 2003) , which has a similar motivation as this paper, except they use the Combinatory Categorial Grammar formalism and the CCGBank syntactic Treebank which was converted from the Penn Tree bank. As a result they show that the oracle f-score improves by over 2 points over the (Gildea and Hockenmaier, 2003) oracle results for the numbered arguments only (A0,..., A5). In contrast to the approach in (Punyakanok et al, 2008), which tags constituents directly, we tag headwords and then associate them with a constituent, as in a previous CCG-based approach (Gildea and Hockenmaier, 2003). This feature has been shown (Gildea and Hockenmaier, 2003) to be an effective substitute for tree path-based features.  We follow a previous CCG based approach (Gildea and Hockenmaier, 2003) in using a feature to describe the PARG relationship between the two words, if one exists. Using a version of Brutus incorporating only the CCG-based features described above, we achieve better results than a previous CCG based system (Gildea and Hockenmaier, 2003, henceforth G&H).
Mining WordNet For A Fuzzy Sentiment: Sentiment Tag Extraction From WordNet Glosses Many of the tasks required for semantic tagging of phrases and texts rely on a list of words annotated with some semanticfeatures. We present a method for ex tracting sentiment-bearing adjectives fromWordNet using the Sentiment Tag Extrac tion Program (STEP). We did 58 STEP runs on unique non-intersecting seed lists drawn from manually annotated list ofpositive and negative adjectives and evaluated the results against other manually annotated lists. The 58 runs were then col lapsed into a single set of 7, 813 unique words. For each word we computed a Net Overlap Score by subtracting the totalnumber of runs assigning this word a neg ative sentiment from the total of the runs that consider it positive. We demonstrate that Net Overlap Score can be used as ameasure of the words degree of member ship in the fuzzy category of sentiment:the core adjectives, which had the high est Net Overlap scores, were identifiedmost accurately both by STEP and by hu man annotators, while the words on the periphery of the category had the lowest scores and were associated with low rates of inter-annotator agreement. The unigrams used for sentence/headline classification were learned from WordNet (Fellbaum, 1998) dictionary entries using the STEP system described in (Andreevskaia and Bergler, 2006b). Since sentiment-bearing words in English have different degree of centrality to the category of sentiment, we have constructed a measure of word centrality to the category of positive or negative sentiment described in our earlier work (Andreevskaiaand Bergler, 2006a).  At lexical level, Andreevskaia and Bergler (2006) exploit an algorithm for extracting sentiment-bearing adjectives from the WordNet based on fuzzy logic. Moreover, Andreevskaia and Bergler (2006) show that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets they use. However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) show that subjectivity recognition might be the harder problem with lower human agreement and automatic performance. In addition, Andreevskaia and Bergler (2006) show that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets they use. Researchers have found this at various levels of analysis, including the manual annotation of phrases (Takamura et al, 2006), sentiment classification of phrases (Wilson et al, 2005), sentiment tagging of words (Andreevskaia and Bergler, 2006b), and sentiment tagging of word senses (Esuli and Sebastiani, 2006a). Similarly, Andreevskaia and Bergler (2006) used WordNet to expand seed lists with fuzzy sentiment categories, in which words could be more central to one category than the other. Non-neutral adjectives were extracted from WordNet and assigned fuzzy sentiment category membership/centrality scores and tags in Andreevskaia and Bergler (2006).  A similar method is presented in (Andreevskaia and Bergler, 2006) where WordNet synonyms, antonyms, and glosses are used to iteratively expand a list of seeds. 
Clause Restructuring For Statistical Machine Translation We describe a method for incorporating syntactic information in statistical machine translation systems. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the parse tree, effectively reordering the surface string on the source language side of the translation system. The goal of this step is to recover an underlying word order that is closer to the target language word-order than the original string. The reordering approach is applied as a pre-processing step in both the training and decoding phases of a phrase-based statistical MT system. We describe experiments on translation from German to English, showing an improvement from 25.2% Bleu score for a baseline system to 26.8% Bleu score for the system with reordering, a statistically significant improvement.   This result was found to be significant (p= 0.021) under the paired bootstrap resampling method of Koehn (2004), and is close to significant (p= 0.058) under the sign test of Collins et al (2005). Our best performing method used unsupervised morphology with L-match (see Section 2.2) and the improvement is significant: bootstrap resampling provides a confidence margin of 0.77 and a t-test (Collins et al, 2005) showed significance with p= 0.001.  We parse the input using the Collins parser (Collins, 1997) and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order (Collins et al, 2005). We are aware of two methods that have been proposed for significance testing with BLUE: bootstrap resampling (Koehn, 2004b; Zhang et al, 2004) and the sign test (Collins et al, 2005). But Collins et al (2005) note that it is not clear whether the conditions required by bootstrap resampling are met in the case of BLUE, and recommend the sign test instead. All results are statistically significant with p= 0.05 using the sign-test described in (Collins et al, 2005). Clause restructuring performed with hand-crafted reordering rules for German-to-English and Chinese-to-English tasks are presented in (Collins et al, 2005) and (Wang et al, 2007), respectively. At the same time, many reorderings can be performed more efficiently based on fixed (hand-crafted) rules (as it is done in (Collins et al, 2005)). We followed the approximation described in (Collins et al, 2005) to get around this problem. Collins et al (2005) approach the problem of properly translating negation in their general reordering setting. To make the word order of German input sentences more English-like a version of the rules of (Collins et al, 2005) were partially implemented using tagged output from the RFTagger. Bold numbers are not significantly different from the best result according to the sign test (p= 0.05) (Collins et al, 2005). The test data for the experiments consisted of 2,000 sentences, and was the same test set as that used by Collins et al (2005).  Collins et al (2005) address this problem by reordering German sentences to more closely parallel English word order, prior to translation by a PSMT system. Collins et al (2005) (German-to-English) use six hand-crafted reordering rules targeting the placement of verbs, subjects, particles and negation. Zwarts and Dras (2007) implement six rules for Dutch-to-English translation, analogous to those of Collins et al (2005), as part of an exploration of dependency distance in syntax-augmented PSMT.
From Discourse Structures To Text Summaries We describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text We show how these concepts can be implemented and we discuss results that we obtained with a discourse-based summanzation program 1 Motivation The evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some &quot;consamples of the output In very few cases, output of a summarization program with a human-made summary or evaluated with the help of human subjects, usually, the results are modest Unfortunately, evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions The position that we take in this paper is that, in order to build high-quality summarization programs, one needs to evaluate not only a representative set of automatically generated outputs (a highly difficult problem by itself), but also the adequacy of the assumptions that these programs use That way, one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each With few exceptions, automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text (see Paice (1990) for an excellent overview) Determining the salient parts is considered to be achievable because one or more of the following assumptions hold (i) important sentences in a text contain words that are used frequently (Lahn, 1958, Edmundson, 1968), (n) important sentences contain words that are used in the tide and section headings (Edmundson, 1968), (in) important sentences are located at the beginning or end of paragraphs (Baxendale, 1958), (Iv) important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically, through training techniques (Lin and Hovy, 1997), (v) important sentences use words as &quot;greatest&quot; and &quot;significant&quot; or indiphrases as &quot;the main aim of this paper&quot; and &quot;the purpose of this article&quot;, while non-important senuse words as &quot;impossible&quot; (Edmundson, 1968, Rush, Salvador, and Zamora, (vi) important sentences and concepts highest connected entities in elaborate semantic structures (Skorochodko, 1971, Lin, 1995, Barzilay and Elhadad, 1997), and (vn) imponant and non-important sentences are derivable from a discourse representation of the text (Sparck Jones, 1993, Ono, Surmta, and Mike, 1994) In determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections, computers are accurate tools Flowever, in determining the concepts that are semantically related or the discourse structure of a text, computers are no longer so accurate, rather, they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement Although it is plausible that elaborate cohesionand coherence-based structures can be used effectively in summarization, we believe that before building summarization programs, we should determine the extent to which these assumptions hold In this paper, we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text We show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program 2 From discourse trees to summaries — an empirical view Also, if we want to select the most important parts of a text, sentences might prove again to be too large segments (Marcu, 1997a; Teufel and Moens, 1998): in some cases, only one of the clauses that make up a sentence should be selected for summarization.    This kind of approach has been very popular in summarization; however the difficulty of this task often requires more complex representations, and different kinds of models to learn relevance in text have been proposed, such as discourse-based (Marcu, 1997) or network-based (Salton et al, 1997) models and many others. A variety of approaches exist for determining the salient sentences in the text: statistical techniques based on word distribution (Kupiec et al, 1995), (Zechner, 1996), (Salton et al., 1991), (Teufell and Moens, 1997), symbolic techniques based on discourse structure (Marcu, 1997) and semantic relations between words (Barzilay and Elhadad, 1997). Some summarization systems assume that the importance of a sentence is derivable from a rhetorical representation of the source text (Marcu, 1997), while others leverage information from multiple texts to re-score the importance of conceptual units across all the sources (Hatzivassiloglou et al, 2001). We intend to investigate any potential linkages between the word groups in the texts and other theories that provide pre-determined structures of text, such as Rhetorical Structure Theory (Marcu, 1997). RST can be used in sentence selection for single document summarization [Marcu, 1997]. Ono et al (1994), T'sou et al (1992) and Marcu (1997) focus on discourse structure in summarization using the Rhetorical Structure Theory (RST). Ono et al (1994), T'sou et al (1992) and Marcu (1997) focus on discourse structure in summarization using the Rhetorical Structure Theory (RST, Mann and Thompson 1986). Most previous work on summarization focused on extractive methods, investigating issues such as cue phrases (Luhn, 1958), positional indicators (Edmundson, 1964), lexical occurrence statistics (Mathis et al, 1973), probabilistic measures for token salience (Salton et al, 1997), and the use of implicit discourse structure (Marcu,1997). Theories such as RST have been popular for sometime as a way of describing the multi-levelled rhetorical relations that exist in text, with relevant applications such as automatic summarization (Marcu, 1997) and natural language generation (Knott and Dale, 1996).
Word Association Norms Mutual Information And Lexicography 1982) for constructing language models for applications in speech recognition. 2. Smadja (in press) discusses the separation between collocates in a very similar way. This definition y) a rectangular window. It might be interesting to consider alternatives (e.g. a triangular window or a decaying exponential) that would weight words less and less as they are separated by more and more words. Other windows are also possible. For example, Hindle (Church et al. 1989) has used a syntactic parser to select words in certain constructions of interest. 4. Although the Good-Turing Method (Good 1953) is more than 35 years old, it is still heavily cited. For example, Katz (1987) uses the in order to estimate trigram probabilities in the recognizer. The Good-Turing Method is helpful for trigrams that have not been seen very often in the training corpus. The last unclassified line, . . . shoppers anywhere from $50 . . . raises interesting problems. Syntactic &quot;chunking&quot; shows that, in spite its co-occurrence of line does not belong here. An intriguing exercise, given the lookup table we are trying construct, is how to guard against false inferences such as that since tagged [PERSON], here count as either a LOCATION. Accidental coincidences of this kind do not have a significant effect on the measure, however, although they do serve as a reminder of the probabilistic nature of the findings. The word also occurs significantly in the table, but on closer it is clear that this use of to time) as something like a commodity or resource, not as part of a time adjunct. Such are the pitfalls of lexicography (obvious when they are pointed out). The results of these studies have important applications in lexicography, to detect lexico-syntactic regularities (Church and Hanks). In (Church and Hanks, 1990) the significance of an association (x, y) is measured by the mutual information I (x, y) ,i.e. the probability of observing x and y together, compared with the probability of observing x and y independently. We also measured the syntagmatic association of neighbour a and neighbour b, with a mutual information measure (Church and Hanks, 1990), computed from the cooccurrence of two tokens within the same paragraph in Wikipedia. On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islamand Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al, 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. In this equation, pmi (i, p) is the pointwise mutual information score (Church and Hanks, 1990) between a pattern, p (e.g. consist-of), and a tuple, i (e.g. engine-car), and maxpmi is the maximum PMI score between all patterns and tuples. To this end we follow the method introduced by (Church and Hanks, 1990), i.e. by sliding a window of a given size over some texts. Like (Church and Hanks, 1990), we used mutual information to measure the cohesion between two words. We approached this task by selecting target roles from the first experiment and ranking characteristic attributes for each using point wise mutual information (PMI) (Church and Hanks, 1990). Collocation has been applied successfully to many possible applications (Church et al, 1989). Collocations were extracted according to the method described in (Church and Hanks, 1990) by moving a window on texts. We use pointwise mutual information (PMI) (Church and Hanks, 1990) to measure the strength of the association between x and y, which is defined as follows PMI (x, y)= log (P (x, y) P (x) P (y)). The information content of this set is defined as mutual information I (F (w)) (Church and Hanks, 1990). PMI scores have been widely used in previous studies to measure association between words (Church and Hanks (1990)). Computational linguists have demonstrated that a word's meaning is captured to some extent by the distribution of words and phrases with which it commonly co-occurs (Church and Hanks, 1990). Following a very long tradition in computational linguistics (Church and Hanks, 1990), we use cooccurrence statistics for words in certain contexts to hypothesise a meaningful connection between the words. Early approaches to MWEs identification concentrated on their collocational behavior (Church and Hanks, 1990). Church and Hanks (1990) suggested pointwise mutual information: PMI (wi ,wj)= log Pr (wi ,wj) Pr (wi) Pr (wj), showing linguistically appealing results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora. Therefore, we propose a second baseline where pairs are rated according to their Pointwise Mutual Information (PMI) (Church and Hanks, 1990), which measures the statistical association between two words. Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon. Word collocation Various collocation metrics have been proposed, including mean and variance (Smadja, 1994), the t-test (Church et al, 1991), the chi-square test, point wise mutual information (MI) (Church and Hanks, 1990), and binomial log likelihood ratio test (BLRT) (Dunning, 1993).
Chart Generation Charts constitute a natural uniform architecture for parsing and generation provided string position is replaced by a notion more appropriate to logical forms and that measures are taken to curtail generation paths containing semantically incomplete phrases. 1 Charts Shieber (1988) showed that parsing charts can be also used in generation and raised the question, which we take up again here, of whether they constitute a natural uniform architecture for parsing and generation. In particular, we will be interested in the extent to which they bring to the generation process advantages comparable to those that make them attractive in parsing. Chart parsing is not a well defined notion. The usual conception of it involves at least four related ideas: edges. context-free grammar, all phrases of a given category that cover a given part of the string are equivalent for the purposes of constructing larger phrases. Efficiency comes from collecting equivalent of phrases into (inactive) constructing edges from edges rather than phrases from phrases. edges. phrases of whatever size can be built by considering existing edges pair-wise if provision is made for partial phrases. Partial phrases are collected edges that are said to be they can be thought of as actively seeking material to complete them. algorithm schema. created edges are placed an are moved from the agenda to the by one until none remains to be moved. When an edge is moved, all interactions between it and edges already in the chart are considered and any new edges that they give rise to are added to the agenda. positions in the string at which phrases begin and end can be used to index edges so that the algorithm schema need consider interactions only between adjacent pairs. Chart parsing is attractive for the analysis of natural languages, as opposed to programming languages, for the way in which it treats ambiguity. Regardless of the number of alternative structures for a particular string that a given phrase participates in, it will be constructed once and only once. Although the number of structures of a string can grow exponentially with the length of the string, the number of edges that needs to be constructed grows only with the square of the string length and the whole parsing process can be accomplished in cubic time. Innumerable variants of the basic chart parsing scheme are possible. For example, if there were languages with truly free word order, we might attempt to characterize them by rules like those of context-free grammar, but with a somewhat different interpretation. Instead of replacing nonterminal symbols in a derivation with strings from the righthand side of corresponding rules, we would remove the nonterminal symbol and insert the symbols from the righthand side of the rule at arbitrary places in the string. A chart parser for languages with free word order would be a minor variant of the standard one. An edge would take the form where v is a vector with a bit for every word in the string and showing which of those words the edge covers. There is no longer any notion of adjacency so that there would be no indexing by string position. Interesting interactions occur between pairs of edges whose bit vectors have empty intersections, indicating that they cover disjoint sets of words. There can now be as many edges as bit-vectors and, not surprisingly, the computational complexity of the parsing process increases accordingly. 2 Generation A parser is a transducer from strings to structures or logical forms. A generator, for our purposes, is the inverse. One way to think of it, therefore, is as a parser of structures or logical forms that delivers analyses in the form of strings. This view has the apparent disadvantage of putting insignificant differences in the syntax of a logical forms, such as the relative order of the arguments to symmetric operators, on the same footing as more significant facts about them. We know that it will not generally be possible to reduce 200 logical expressions to a canonical form but this does not mean that we should expect our generator to be compromised, or even greatly delayed, by trivial distinctions. Considerations of this kind were, in part, responsible for the recent resurgence of interest in &quot;flat&quot; representations of logform (Copestake 996) and for the representations used for transfer in Shake-and-Bake translation (Whitelock, 1992). They have made semantic formalisms like those now usually associated with Davison (Davidson, 1980, Parsons, 1990) attractive in artificial intelligence for many years (Hobbs 1985, Kay, 1970). Operationally, the attraction is that the notations can be analyzed largely as free word-order languages in the manner outlined above. Consider the expression (I) (1) r: run(r), past(r), fast(r), argl(r, j), name(j, John) which we will take as a representation of the logical form of sentences ran fast ran quickly. consists of a distinguished index (r) and a list of predicates whose relative order is immaterial. The distinguished index identifies this as a sentence that makes a claim about a running event. &quot;John&quot; is the name of the entity that stands in the `argl ' relation to the running which took place in the past and which was fast. Nothing turns on these details which will differ with differing ontologies, logics, and views of semantic structure. What concerns us here is a procedure for generating a sentence from a structure of this general kind. Assume that the lexicon contains entries like those in (2) in which the italicized arguments to the semantic predicates are variables. (2) Words Cat Semantics John np(x) John) ran vp(x, y) argl(x, y), past(x) fast adv(x) quickly adv(x) x: fast(x) facie for the utility of these particular words for expressing ( I) can be made simply by noting that, instantiation of the variables, the semantics of each of these words subsumes (1). 3 The Algorithm Schema The entries in (2), with their variables suitably instantiated, become the initial entries of an agenda and we begin to move them to the chart in accordance with the algorithm schema, say in the order given. The variables in the 'Cat' and 'Semantics' columns of (2) provide the essential link between syntax and semantics. The predicates that represent the semantics of a phrase will simply be the union of those representing the constituents. The rules that sanction a phrase (e.g. (3) below) show which variables from the two parts are to be identified. the entry for moved, no interactions are because the chart is empty. When moved, the ran considered as a possible phrase on the basis of rule (3). (3) s(x) —> np(y), vp(x, y). With appropriate replacements for variables, this maps onto the subset (4) of the original semantic specification in (1). (4) r: run(r), past(r), argl(r, j), name(j, John) Furthermore it is a complete sentence. However, it does not count as an output to the generation process as a whole because it subsumes some but not all of (1). It therefore simply becomes a new edge on the agenda. string fast a verb phrase by virtue rule (5) giving the semantics (6), and the phrase the same semantics is put on the agenda when is move to the chart. (5) vp(x) —> vp(x) adv(x) (6) r: run(r), past(r), fast(r), argl(r, y) agenda now contains the entries in Words Cat Semantics John ran s(r) r: run(r), past(r), arg I (r, j), name(j, John) ran fast vp(r, j) r: run(r), past(r), fast(r), argl(r, j) ran quickly vp(r, j) r: run(r), past(r), fast(r), arg 1 (r, j) Assuming that adverbs modify verb phrases and not senthere will be no interactions when the ran is moved to the chart. the edge for fast moved, the possibility of creating the phrase fast quickly well as fast. are rejected, however, on the grounds that they would involve using a predicate from the original semantic specification more than once. This would be similar to allowing a given word to be covered by overlapping phrases in free word-order parsing. We proposed eliminating this by means of a bit vector and the same technique applies here. The fruitful interactions that occur here are fast quickly the one hand, and 201 on the other. Both give sentences whose semantics subsumes the entire input. Several things are noteworthy about the process just outlined. 1. Nothing turns on the fact that it uses a primitive version of event semantics. A scheme in which the indices were handles referring to subexpressions in any variety of flat semantics could have been treated in the same way. Indeed, more conventional formalisms with richly recursive syntax could be converted to this form on the fly. 2. Because all our rules are binary, we make no use of active edges. 3. While it fits the conception of chart parsing given at the beginning of this paper, our generator does not involve string positions centrally in the chart representation. In this respect, it differs from the proposal of Shieber (1988) which starts with all word edges leaving and entering a single vertex. But there is essentially no information in such a representation. Neither the chart nor any other special data structure is required to capture the fact that a new phrase may be constructible out of any given pair, and in either order, if they meet certain syntactic and semantic criteria. 4. Interactions must be considered explicitly between new edges and all edges currently in the chart, because no indexing is used to identify the existing edges that could interact with a given new one. 5. The process is exponential in the worst case because, if a sentence contains a word with k modifiers, then a it will be generated with each of the subsets of those modifiers, all but one of them being rejected when it is finally discovered that their semantics does not subsume the entire input. If the relative orders of the modifiers are unconstrained, matters only get worse. Points 4 and 5 are serious flaws in our scheme for which we shall describe remedies. Point 2 will have some importance for us because it will turn out that the indexing scheme we propose will require the use of distinct active and inactive edges, even when the rules are all binary. We take up the complexity issue first, and then turn to how the efficiency of the generation chart might be enhanced through indexing. 4 Internal and External Indices The exponential factor in the computational complexity of our generation algorithm is apparent in an example like (8). (8) Newspaper reports said the tall young Polish athlete ran fast The same set of predicates that generate this sentence clearly also generate the same sentence with deletion of all of the words young. a total of 8 strings. Each is generated in its entirety, though finally rejected because it fails to account for all of the semantic The words also be deleted independently giving a grand total of 32 strings. concentrate on the phrase young Polish athlete which we assumed would be combined with the verb phrase fast the rule (3). The distinguished index of the noun call it p, is identified with the variable the rule, but this variable is not associated with the syntactic category, s, on the left-hand side of the rule. The grammar has access to indices only through the variables that annotate grammatical categories in its rules, so that rules that incorporate this sentence into larger phrases can have no further to the index p. We therefore say that p is sentence tall young Polish athlete ran fast. The index p would, of course, also be internal to the young Polish athlete ran fast, the tall Polish ran fast, However, in these cases, the semantic material remaining to be expressed contains predicates that refer to this internal index, say tall(p)' , and `young(p)'. While the lexicon may have words to express these predicates, the grammar has no way of associating their referents with the above noun phrases because the variables corresponding to those referents are internal. We conclude that, as a matter of principle, no edge should be constructed if the result of doing so would be to make internal an index occurring in part of the input semantics that the new phrase does not subsume. In other words, the semantics of a phrase must contain all predicates from the input specification that refer to any indices internal to it. This strategy does not prevent the generation of an exponential number of variants of phrases containing modifiers. It limits proliferation of the ill effects, however, by allowing only the maximal one to be incorporated in larger phrases. In other words, if the final has phrases with respectively, then of the first and of the second will be created, but only one of each set will be incorporated into larger and no factor of will be introduced into the cost of the process. 5 Indexing String positions provide a natural way to index the strings input to the parsing process for the simple reason that there are as many of them as there are words but, for there to be any possibility of interaction between a pair of edges, they must come together at just one index. These are the natural points of articulation in the domain of strings. They cannot fill this role in generation because they are not natural properties of the semantic expressions that are the input to the process. The corresponding natural points of articulation in 202 flat semantic structures are the entities that we have already referring to as In the modified version of the procedure, whenever a new inactive edge is created with label B(b ...). then for all rules of the form in (9), an active edge is also created with label A(...)/C(c ...). A(...) ---> ...) C(c ...) This represents a phrase of category A that requires a phrase of category Con the right for its completion. In these labels, (variables representing) the first, or distinassociated with B and C. By analogy with parsing charts, an inactive edge labeled B(b ...) can be of as from means simply it is efficiently accessible through the index active ...) be thought of as incident from, or through, the index key property of this scheme is that active and inactive edges interact by virtue of indices that they share and, by letting vertices correspond to indices, we collect together sets of edges that could interact. We illustrate the modified procedure with the sentence (10) whose semantics we will take to be (11), the grammar rules (12)-(14), and the lexical entries in (15). (10) The dog saw the cat. (11) dog(d), def(d), saw(s), past(s), cat(c), def(c), argl(s. d), arg2(s, c). (12) s(x) np(y) vp(x, y) (13) vp(x, --> v(x, Y, z) np(z) (14) np(x) ---> det(x) n(x) (15) Words Cat Semantics cat n(x) saw z) x: see(x), past(x), argl(x, y), arg2(x,z) dog n(x) the det(x) The procedure will be reminiscent of left-corner parsing. Arguments have been made in favor of a head-driven strategy which would, however, have been marginally more (e.g. in Kay (1989), Shieber, el. and the differences are, in any case, not germane to our current concerns. The initial agenda, including active edges, and collecting edges by the vertices that they are incident from, is given in (16). The grammar is consulted only for the purpose of creating active edges and all interactions in the chart are between active and inactive pairs of edges incident from the same vertex. (16) Vert Words Cat Semantics d the det(d) d: def(d) the np(d)/n(d) d: def(d) dog n(d) d: dog(d) s saw v(s, d, c) s: see(s). past(s), d), arg2(s, c saw vp(s, d)/np(c) r: see(s), past(s), argl(r, j) the det(c) c: def(c) the np(c)/n(c) c: def(c) cat n(c) c: dog(c) (17) Vert Words Cat Semantics d the dog np(d) d: dog(d), def(d) saw the cat vp(s, d)/np(d) s: see(s), past(s), argl(s, d), arg2(s, c), cat(c), def(c) c the cat np(c) c: cat(c), def(c) s saw the cat vp(s, d) s: see(s), past(s), argl(s, d), arg2(s, c), cat(c), def(c) Among the edges in (16), there are two interactions, one at vertices c and d. They cause the first and third edges in (17) to be added to the agenda. The first interacts with the active edge originally introduced by the verb &quot;saw&quot; producing the fourth entry in (17). The label on this edge matches the first item on the right-hand side of rule (12) and the active edge that we show in the second entry is also introduced. The final interaction is between the first and second edges in (17) which give rise to the edge in (18). This procedure confirms perfectly to the standard algorithm schema for chart parsing, especially in the version that makes predictions immediately following the recognition of the first constituent of a phrase, that is, in the version that is essentially a caching left-corner parser. 203 (18) Vert Words Cat Semantics s The dog saw the cat s(s) dog(d), def(d), see(s), past( s),argl(s , d), arg2(s, c), cat(c), def(c). 6 Acknowledgments Whatever there may be of value in this paper owes much to the interest, encouragement, and tolerance of my colleagues Marc Dymetman, Ronald Kaplan, John Maxwell, and Hadar Shem Toy. I am also indebted to the anonymous reviewers of this paper. The method is an extension of the chart based generation algorithm described in Kay (1996). Kay (1996) provides a more general view of the chart structure which is designed to provide for generation advantages comparable to those it provides for parsing. We will concentrate on a detailed description of these annotations as they are a crucial component of our method and they are the major difference between the current proposal and the one described in Kay (1996). (Wang, 1980) uses handwritten rules to generate sentences from an extended predicate logic formalism; (Shieber et al., 1990) introduces a head-driven algorithm for generating from logical forms; (Kay, 1996) defines a chart based algorithm which enhances efficiency by minimising the number of semantically incomplete phrases being built; and (Shemtov, 1996) presents an extension of the chart based generation algorithm presented in (Kay, 1996) which supports the generation of multiple paraphrases from underspecified semantic input. The PCFG-based generation algorithms are implemented in terms of a chart generator (Kay, 1996). Kay (1996) identified parsing charts as such an architecture, which led to the development of various chart generation algorithms: Carroll et al (1999) for HPSG, Bangalore et al (2000) for LTAG, Moore (2002) for unification grammars, White and Baldridge (2003) for CCG. In the chart realization tradition (Kay, 1996), the OpenCCG realizer takes logical forms as input and produces strings by combining signs for lexical items. We followed two partial solutions to this problem by Kay (1996). The baseline generation algorithm, following Kay (1996)'s work on chart generation, already contains the hard constraint that when combining two chart edges they must cover disjoint sets of words. It is a bottom up, tabular algorithm [Kay, 1996] optimised for TAGs. Following Stone and Doran (1997) and Kay (1996), we enhance this TAG grammar with a syntax-semantics interface in which nonterminal nodes of the elementary trees are equipped with index variables, which can be bound to individuals in the semantic input. We follow a bottom-up chart generation approach (Kay, 1996) for production systems similar to (Varges, 2005). For instance, (Kay, 1996) proposes to reduce the number of constituents built during realisation by only considering for combination constituents with non overlapping semantics and compatible indices. The tree combining algorithm used after filtering has taken place, is a bottom-up tabular algorithm (Kay, 1996) optimised for TAGs. The standard solution to this problem (cf. (Kay, 1996)) is to index edges with semantic indices (for instance, the edge with category N/x: dog (x) will be indexed with x) and to restrict edge combination to these edges which have compatible indices. More recently, Carroll and Oepen (2005) present a perfor 1As first noted by Brew (1992) and Kay (1996), given a set of n modifiers all modifying the same structure, all possible intermediate structures will be constructed, i.e., 2n+1. The work in Kay (1996) and the extension to ambiguous input in Shemtov (1996) and Shemtov (1997) describes a chart based generation process which takes packed representations as input and generates all paraphrases without expanding first into disjunctive normal form. The basic surface realisation algorithm used is a bot tom up, tabular realisation algorithm (Kay, 1996) optimised for TAGs. The generation algorithm is based on chart generation as first introduced by Kay (1996) with Viterbi-pruning. This results in efficiently treating the well known problem originally described in Kay (1996), where one unnecessarily retains sub-optimal strings.
Stochastic Lexicalized Inversion Transduction Grammar For Alignment We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training. Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences. Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I (ak), but also the outside probability O (ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for word based ITGs (Zhang and Gildea, 2005). The classical approaches to word alignment are based on IBM models 1-5 (Brown et al, 1994) and the HMM based alignment model (Vogel et al, 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax-based approaches (Zhang and Gildea, 2005) for word alignment are also studied.  Zhang and Gildea (2005) described a model in which the nonterminals are lexicalized by Englishand foreign language word pairs so that the inversions are dependent on lexical information on the left hand side of synchronous rules. We apply one of the pruning techniques used in Zhang and Gildea (2005). In the first comparison, we measured the performance of five word aligners, including IBM models, ITG, the lexical ITG (LITG) of Zhang and Gildea (2005), and our bi lexical ITG (BLITG), on a hand-aligned bilingual corpus. Past approaches have pruned spans using IBM Model 1 probability estimates (Zhang and Gildea, 2005) or using agreement with an existing parse tree (Cherry and Lin, 2006). Fortunately, exploiting the recursive nature of the cells, we can compute values for the inside and outside components of each cell using dynamic programming in O (n4) time (Zhang and Gildea, 2005). For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. Zhang and Gildea (2005) show that Model 1 (Brown et al, 1993) probabilities of the word pairs inside and outside a span pair are useful. Tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute inside and outside scores for a span pair in O (n4). Like Zhang and Gildea (2005), it is used to prune bi text cells rather than score phrases. Our pruning differs from Zhang and Gildea (2005) in two major ways. The tic-tac-toe pruning algorithm (ZhangandGildea, 2005) uses dynamic programming to compute the product of inside and outside scores for all cells in O (n4) time. Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea (2005).
Word Representations: A Simple and General Method for Semi-Supervised Learning If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further by combining word representations. You can download word features, for use in existing NLP systems, as well as our here:  Unless stated otherwise we use word-vectors of size 50, initialized using the embeddings provided by Turian et al (2010) based on the model of Collobert and Weston (2008). In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided by Turian et al (2010). Follow (Turian et al., 2010), we randomly initialize all parameters to [-0.1, 0.1], and use stochastic gradient descent to minimize the ranking loss with a fixed learning rate 0.01. Just as monolingual word clusters are broadly applicable as feature sin monolingual models for linguistic structure prediction (Turian et al, 2010), the resulting cross-lingual word clusters can be used as features in various cross lingual direct transfer models. 256 cross-lingual word clusters and the same feature templates as Ta?ckstro?m et al (2012), with the exception that the transition factors are not conditioned on the input. The features used are similar to those used by Turian et al (2010), but include cross-lingual rather than monolingual word clusters. Each term's feature vector is its row in U; following Turian et al. (2010), we standardize and scale the standard deviation to 0.1. Further details and evaluations of these embeddings are discussed in Turian et al (2010). As the dataset is rather small, we use lower-dimensional word vectors with d=32 that are initialised with embeddings trained in an unsupervised way to predict contexts of occurrence (Turian et al, 2010). Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al, 2010). We evaluate C&W word embeddings with 25, 50 and 100 dimensions as well as HLBL word embeddings with 50 and 100 dimensions that are introduced in Turian et al (2010) and can be downloaded here. This is because the RCV1 corpus used to induce the word embeddings (Turian et al, 2010) does not cover spoken language words in cts very well. The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al (2010) and Collobert and Weston (2008)). Experiments with the Brown clusters (Brown et al, 1992) provided by Turian et al (2010) in lieu of suffixes were not promising. Following Turian et al (2010), we use Percy Liang's implementation of this algorithm for our comparison, and we test runs with 100, 320, and 1000 clusters. We downloaded these embeddings from Turian et al (2010). Besides language modeling, word embeddings induced by neural language models have been useful in chunking, NER (Turian et al, 2010), parsing (Socher et al, 2011b), sentiment analysis (Socher et al., 2011c) and paraphrase detection (Socher et al, 2011a). For the Brown algorithm, we are contrasting cluster count choices of 320 and 1000, based on reports of other successful applications [Turian et al, 2010], with clustering models trained on monolingual data from the Europarl corpus and the News Commentary corpus. Turian et al (2010) show that adapting from CoNLL to MUC-7 (Chinchor, 1998) data (thus between different newswire sources), the best unsupervised feature (Brown clusters) improves F1 from .68 to .79. The robustness of this simple approach is well documented; e.g., Turian et al (2010) show that the baseline model (gazetteer features without unsupervised features) produces an F1 of .778 against .788 of the best unsupervised word representation feature.
The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task This paper presents the task definition, resources, participation, and comparative re sults for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name. UNN-WePS achieved an average purity of 0.6, and inverse purity of 0.73 in Semeval Task 13, achieving seventh position out of sixteen competing systems (Artiles et al 2007). We have described a system, UNN-WePS that disambiguates individuals in web pages as required for Semeval task 13 (Artiles et al 2007). We used three datasets in our experiments, WePS1 Training and Testing (Artiles et al 2007), WePS2 Testing (Javier et al 2009). These datasets collected names from three different resources including Wikipedia names, program committee of a computer science conference and US census. We adopt the same evaluation process as (Han and Zhao, 2009), and evaluating these models using Purity, Inverse Purity and the F-measure (also used in WePS Task Artiles et al 2007)). We adopted the standard data sets used in the First Web People Search Clustering Task (WePS1) (Artiles et al, 2007) and the Second Web People Search Clustering Task (WePS2) (Artiles et al, 2009). We consider the problem of disambiguating person names in a Web searching scenario as described by the Web People Search Task in SemEval 2007 (Artiles et al, 2007). Two different evaluation measures are reported as described by the task: F=0.5 is a harmonic mean of purity and inverse purity of the clustering result, and F? =0.2 is a version of F that gives more importance to inverse purity (Artiles et al, 2007). Sections 3 and 4 presents in more detail the implementation of the framework for the Semeval 2007 WEPS task (Artiles et al, 2007) and Semeval-. In this section we will explain in more detail how we implemented the general schema described in the previous section to the Web People Search task (Artiles et al, 2007). The data we have used for training our system were made available in the framework of the SemEval (task 13: Web People Search) competition (Artileset al, 2007). For both categories the number of target output clusters equals (number of RIPPER output clusters+ the number of documents*0.2). Although the clustering results with the best set tings for hierarchical and agglomerative clustering were very close with regard to F-score (combining purity and inverse purity, see (Artiles et al, 2007) for a more detailed description), manual inspection of the content of the clusters has revealed big differences between the two approaches. We evaluate our methods using the benchmark test collection from the ACL SemEval-2007 web person search task (WePS hereafter) (Artiles et al, 2007). Hence the performance reported here is comparable to the official evaluation results (Artiles et al, 2007). The goal of the Web People Search task (Artiles et al 2007) is to assign Web pages to groups, where each group contains all (and only those) pages that refer to one unique entity. In this paper, we described our participating system in the SemEval-2007 Web People Search Task (Artiles et al, 2007). The research on cross-document entity coreference resolution can be traced back to the Web People Search task (Artiles et al, 2007) and ACE2008 (e.g. Baron and Freedman, 2008). Here, we concentrate on the following SemEval 2007 Web People Search Task (Artiles et al, 2007): a search engine user types in a person name as a query. Recently, there is significant research interest in a related task called Web Person Search (WePS) (Artiles et al, 2007), which seeks to determine whether two documents refer to the same person given a person name search query. The more recent Web Person Search (WePS) task (Artiles et al, 2007) has created a benchmark dataset which is also used in this work. Similar IR features are also used by other WePS systems as they are more robust to the variety of web pages (Artiles et al, 2007).
Accurate Information Extraction From Research Papers Using Conditional Random Fields With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance. This paper employs Conditional Random Fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers. The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. This paper makes an empirical exploration of several factors, including variations on Gaussian, expoand priors for improved regularization, and several classes of features and Markov order. On a standard benchmark data set, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs. See Peng and McCallum (2004) for more details and further experiments. For this underlying model, we employ a chain structured conditional random field (CRF), since CRFs have been shown to perform better than other simple unconstrained models like hidden markov models for citation extraction (Peng and McCallum, 2004). Later, CRFs were shown to perform better on CORA, improving the results from the Hmm's token-level F1 of 86.6 to 91.5 with a CRF (Peng and McCallum, 2004). This approach is limited in its use of an HMM as an underlying model, as it has been shown that CRFs perform significantly better, achieving 95.37 token-level accuracy on CORA (Peng and McCallum, 2004). In recent years discriminative probabilistic models have been successfully applied to a number of information extraction tasks in natural language processing (NLP), such as named entity recognition (NER) (McCallum and Li, 2003), noun phrase chunking (Sha and Pereira, 2003) and information extraction from research papers (Peng and McCallum, 2004).  For example, Peng and McCallum (2004) applied Conditional Random Fields to extract information, which draws together the advantages of both HMM and SVM. This paper mainly focuses on the VP problem, since linguistic features for the HP problem is the general IE topic of much past research (e.g., Peng and McCallum, 2004). Recently an increasing number of research efforts on text mining and IE have used CRF models (e.g., Peng and McCallum, 2004). More recently, Peng and McCallum (2004) applied supervised learning of Conditional Random Field (CRF) sequence models to the problem of parsing the headers of research papers. In our experiments we use the bibliographic citation dataset described in (Peng and McCallum, 2004). In recent years, conditional random fields (CRFs) (Lafferty et al, 2001) have shown success on a number of natural language processing (NLP) tasks, including shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003) and information extraction from research papers (Peng and McCallum, 2004). CORA (Peng and McCallum, 2004) consists of two collections: a set of research paper headers annotated for entities such as title, author, and institution; and a collection of references annotated with BibTeX fields such as journal, year, and publisher. In bibliography entries (Peng and McCallum, 2004), a given field (author, title, etc.) should be filled by at most one substring of the input, and there are strong preferences on the cooccurrence and order of certain fields.  Examples of these models include maximum entropy Markov models (McCallum et al, 2000), Bayesian information extraction network (Peshkin and Pfeffer, 2003), and conditional random fields (Mc Callum, 2003) (Peng and McCallum, 2004). Although we are primarily concerned with unsupervised property discovery, it is worth mentioning (Peng and McCallum, 2004) and (Ghani et al, 2006) who approached the problem using supervised machine learning techniques and require labeled data.
Semantic-Head-Driven Generation present algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applicable. In particular, unlike a previous bottom-up generator, it allows use of semantically nonmonotonic grammars, yet unlike top-down methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion. (Shieber et al., 1990) introduces a head-driven algorithm for generating from logical forms. Whereas Shieber et al (1990) have discussed similar techniques in the context of semantic head-driven generation, we are concerned here with parsing. We view the linking relation not simply as a filter to increase efficiency within the domain of syntactic analysis; this aspect is stressed by Shieber (1985) and other investigators such as Bouma (1991), but rather as a device for the top-down predictive instantiation of information, as Shieber et al (1990) have shown for semantic-head-driven generation. The previously proposed semantic-head-driven methods run into problems if none of the daughter constituents in the syntactic semantic rule schemata of a grammar fits the definition of a semantic head given in [Shieber et al, 1990]. For the phrase structure tree rooted with, there is no leaf which would fulfill the definition of a semantic head given in [Shieber et al, 1990] or [van Noord, 1993]. Shieber et al (1990) show that a top-down evaluation strategy will fail for rules such as vP-*vp x, irrespective of the order of evaluation of the right-hand side categories in the rule. It is important to compare the generation strategy presented here with Semantic-head-driven generation [Shieber et al 1990, van Noord 1990] which is a direct generation algorithm from logical form encodings. Unlike deduction-based approaches to natural language generation in computational linguistics (e.g., Shieber et al 1990),. The algorithm in fact follows a head-driven node expansion, or search through the grammar, (as in Shieber et al, 1990), with the head of the most recently expanded node being selected for the next expansion (in step 2 of the algorithm above), until a leaf node is produced. Regular feature grammars can also be compiled into generators using a version of the Semantic Head Driven algorithm (Shieber et al, 1990). In this sense, it is not unlike (Shieber et al, 1990)'s semantic-head-driven generation. As shown in detail in (Shieber et al, 1990), top down generators can fail to terminate on certain grammars because they lack the lexical information necessary for their well-foundedness. The transformation is straightforward to define in its general form, and the transformed grammars can be readily compiled into efficient generators by standard feature grammar generator-compiler algorithms like Semantic Head-Driven Generation (Shieber et al, 1990). One standard approach to sentence generation from predicate/argument structure (like the semantic-head-driven generation in (Shieber et al., 1990)) involves a simple algorithm. Our general method is to take as inputs to the process various communicative goals of the system, expressed as logical forms, and use them to construct a single new logical form to be input to Gemini's Semantic Head-Driven Generation algorithm (Shieber et al, 1990), which produces strings for Festival speech synthesis. The end result of our selection and aggregation module (see section 6.2) is a fully specified logical form which is to be sent to the Semantic-Head Driven Generation component of Gemini (Shieber et al, 1990).  Given an off-line optimization of the order in which the right-hand side categories in the rules of a logic grammar are processed (Minnen et al., 1996) the resulting processing behavior can be considered a generalization of the head corner generation approach (Shieber et al, 1990). In generation, examples of such extended processing strategies are head corner generation with its semantic linking (Shieber et al, 1990) or bottom-up (Earley) generation with a semantic filter (Shieber, 1988). Generation with the resulting grammar can be compared best with head corner generation (Shieber et al, 1990).
Contextual Dependencies In Unsupervised Word Segmentation Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages, and may shed light on how humans learn to segment speech. We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively. The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation. We also show that previous probabilistic models rely crucially on suboptimal search procedures. Similarly, Goldwater et al (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities.  USM: We learned a USM model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) for word segmentation. We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences).  Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al, 2006), and as model of learning morphology in European languages (Goldsmith, 2001). We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level, in the style of Goldwater et al (2006).  We evaluated the f-score of the recovered word constituents (Goldwater et al, 2006b).     We then investigated adaptor grammars that incorporate one additional kind of information, and found that modeling collocations provides the greatest improvement in word segmentation accuracy, resulting in a model that seems to capture many of the same inter word dependencies as the bigram model of Goldwater et al (2006b).  Goldwater et al (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al, 2009). Goldwater et al (2006) and Goldwater et al (2009) demonstrated the importance of contextual dependencies for word segmentation, and proposed a bigram model in order to capture some of these. Goldwater et al (2006) used hierarchical Dirichlet processes (HDP) to induce contextual word models. While there is no reason why these methods cannot be used to learn the syntax and semantics of human languages, much of the work to date has focused on lower-level learning problems such as morphological structure learning (Goldwater et al, 2006b) and word segmentation, where the learner is given unsegmented broad-phonemic utterance transcriptions and has to identify the word boundaries (Goldwater et al, 2006a; Goldwater et al, 2007). It confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al, 2006a).
A Statistical Model For Domain-Independent Text Segmentation We propose a statistical method that finds the maximum-probability segmentation of a given text. This method does not require training data because it estimates probabilities from the given text. Therefore, it can be applied to any text in any domain. An experiment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system. CWM is described in (Choi, 2001), U00 in (Utiyama and Isahara, 2001), C99 in (Choi, 2000), DotPlot in (Reynar, 1998) and Segmenter in (Kan et al, 1998). Misra et al (2009) extended the DP algorithm U00 from Utiyama and Isahara (2001) using TMs.  (Utiyama and Isahara, 2001) models the problem of TS as a problem of finding the minimum cost path in a graph and therefore adopts a dynamic programming algorithm. The problem of finding thematic boundaries other than sentence boundaries automatically (e.g. Utiyama and Isahara (2001)) is thus not addressed in this work. U00 is the system described in (Utiyama and Isahara, 2001), C99 the one proposed in (Choi, 2000) and LCseg is presented in (Galley et al, 2003). In more recent work, Utiyama and Isahara (2001) combine a statistical segmentation model with a graph search algorithm to find the segmentation with the maximum probability. Due to lack of space we do not describe previous work in text segmentation here in detail; we refer the reader to Utiyama and Isahara (2001) and Pevzener and Hearst (2002) for a comprehensive overview. This algorithm (Utiyama and Isahara, 2001) (UI) computes the optimal segmentation by estimating changes in the language model predictions over different partitions. In addition, we show that the benchmark segmentation system of Utiyama and Isahara (2001) can be viewed as another special case of our Bayesian model. Most similar to our work is the approach of Utiyama and Isahara (2001), who search for segmentations with compact language models; as shown in Section 3.1.1, this can be viewed as a special case of our model. Utiyama and Isahara (2001) introduced one of the first probabilistic approaches using Dynamic Programming (DP) called U00.   Lexical cohesion was placed in a probabilistic (though not Bayesian) framework by Utiyama and Isahara (2001). Two frequently-cited systems are LCSEG (Galley et al, 2003) and TEXTSEG (Utiyama and Isahara, 2001). A set of stop-words is also removed, using the same list originally employed by several competitive systems (Utiyama and Isahara, 2001). It is not clear whether our algorithm is better than (Utiyama and Isahara, 2001) (U00). In this paper, we selected for comparison three systems based merely on the lexical reiteration feature: TextTiling (Hearst, 1997), C99 (Choi, 2000) and TextSeg (Utiyama and Isahara, 2001). The TextSeg algorithm (Utiyama and Isahara,2001) implements a probabilistic approach to determine the most likely segmentation, as briefly described below.
On Coreference Resolution Performance Metrics The paper proposes a Constrained Entity Alignment F-Measure (CEAF) for evaluatingcoreference resolution. The metric is com puted by aligning reference and system entities (or coreference chains) with the constraint that a system (reference) entity is aligned with at most one reference (system) entity. We show that the best alignment is a maximum bipartite matching problem which can be solved by theKuhn-Munkres algorithm. Comparative experiments are conducted to show that the widely known MUC F-measure has serious flaws in evaluating a coreference system. The proposed metric is also compared with the ACE-Value, the official evaluation metric in the AutomaticContent Extraction (ACE) task, and we con clude that the proposed metric possesses someproperties such as symmetry and better inter pretability missing in the ACE-Value. For this reason, we compute an unweighted entity-constrained mention F-measure (Luo, 2005) and report all contrastive experiments with this metric. But the metric has a systematic bias for systems generating fewer entities (Bagga and Baldwin, 1998) - see Luo (2005). Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al, 1995), B 3 (Bagga and Baldwin, 1998), CEAF e (Luo, 2005), CEAF m (Luo, 2005), and BLANC (Recasens and Hovy, 2011). However, it turns out that the CEAF metric (Luo, 2005) was always intended to work seamlessly on predicted mentions, and so has been the case with the B 3 metric. We report recall, precision, and F1 for MUC (Vilain et al, 1995), B3 (Bagga and Baldwin, 1998), and CEAF (Luo, 2005). Three runs have been submitted for the SemEval task 1 on Coreference Resolution (Recasens et al, 2010), optimizing Corry's performance for BLANC (Recasens and Hovy, in prep), MUC (Vilain et al, 1995) and CEAF (Luo, 2005). We have collected a number of runs on the development data to optimize the performance level for a particular score: BLANC (Recasens and Hovy, in prep), MUC (Vilain et al, 1995) or CEAF (Luo, 2005). We report results in terms of recall (R), precision (P), and F-score (F) by employing the mention-based B3 metric (Bagga and Baldwin, 1998), the entity-based CEAF metric (Luo, 2005), and the pairwise F1 (PW) metric. Category Evaluation Measures set mapping purity, inverse purity, F-measure pair counting rand index, Jaccard Coefficient, Folks and Mallows FM entropy entropy, mutual information, VI, V editing distance editing distance co reference resolution MUC (Vilain et al,1995), B-Cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) Table 3. This is similar to Luo (2005) where a Bell tree is used to score and store the searching path. CEAF (Luo, 2005): For a similarity function between predicted and true clusters, CEAF scores the best match between true and predicted clusters using this function. We use the similarity function from Luo (2005). In coreference resolution, typical performance measure functions include MUC (Vilain et al, 1995), Rand index (Rand, 1971), B-CUBED (Bagga and Baldwin, 1998) and CEAF (Luo, 2005). In all our experiments, we use two popular performance measures, B-CUBED Fmeasure (Bagga and Baldwin, 1998) and CEAF F measure (Luo, 2005), to evaluate the co reference resolution result. To evaluate our system we use CEAF (Luo, 2005) and B3 (Bagga and Baldwin, 1998). We compare the end clustering quality across a variety of thresholds and for various system flavors using three metrics: MUC (Vilain et al 1995), B3 (Bagga and Baldwin, 1998) and CEAF (Luo, 2005).   Both parameters were empirically adjusted on the development set for the evaluation measure used in this shared task: the unweighted average of MUC (Vilain et al, 1995), B3 (Bagga and Baldwin, 1998) and entity-based CEAF (Luo, 2005). Results are reported in terms of recall (R), precision (P), and F-measure (F), obtained using two coreference scoring programs: the MUC scorer (Vilain et al., 1995) and the CEAF scorer (Luo, 2005).
Experiments Using Stochastic Search For Text Planning Marcu has characterised an important and difficult problem in text planning: given a set of facts to convey and a set of rhetorical relations that can be used to link them together, how can one arrange this material so as to yield the best possible text? We describe experiments with a number of heuristic search methods for this task. Mellish et al (1998) investigate the problem of determining a discourse tree for a set of elementary speech acts which are partially constrained by rhetorical relations. Mellish et al (1998) (and subsequently Karamanis and Manurung 2002) advocate genetic algorithms as an alternative to exhaustively searching for the optimal ordering of descriptions of museum artefacts. Following previous work (Mellish et al, 1998) we used a single fitness function that scored candidates based on their coherence. The genetic algorithms of Mellish et al (1998) and Karamanis and Manarung (2002), as well as the greedy algorithm of Lapata (2003), provide no theoretical guarantees on the optimality of the solutions they propose. For example, the measure from (Mellish et al, 1998) looks at the entire discourse up to the current transition for some of their cost factors. Mellish et al (1998) advocate stochastic search as an alternative to exhaustively examining the search space. As in the case of Mellish et al (1998) we construct an acceptable ordering rather than the best possible one. Mellish et al (1998) made the point that even this restricted approach would soon become intractable with more than a small set of facts when one allows weak RST relations such as Joint and Elaboration into the model. In the late 1990s, Chris Mellish implemented the first stochastic text planner (Mellish et al 1998). The evaluation function of Mellish et al (1998) also was calculated over a sum of local features of the tree, although a wider set of features were involved. For instance, the evaluation function of Mellish et al (1998) assigned +3 for each instance of subject-repetition. Genetic algorithms are also used in [Mellish et al, 1998] where the authors state the problem of given a set of facts to convey and a set of rhetorical relations that can be used to link them together, how one can arrange this material so as to yield the best possible text. Mellish et al (1998) advocate stochastic search methods for document structuring.
Lexical Semantic Techniques For Corpus Analysis In this paper we outline a research program for computational linguistics, making extensive use of text corpora. We demonstrate how a semantic framework for lexical knowledge can suggest richer relationships among words in text beyond that of simple co-occurrence. The work suggests how linguistic phenomena such as metonymy and polysemy might be exploitable for semantic tagging of lexical items. Unlike with purely statistical collocational analyses, the framework of a semantic theory allows the automatic construction of predictions about deeper semantic among words appearing in systems. illustrate the approach for the acquisition of lexical information for several classes of nominals, and how such techniques can fine-tune the lexical structures acquired from an initial seeding of a machine-readable dictionary. In addition to conventional lexical semantic relations, we show how information concerning lexical presuppositions and preference relations can also be acquired from corpora, when analyzed with the appropriate semantic tools. Finally, we discuss the potential that corpus studies have for enriching the data set for theoretical linguistic research, as well as helping to confirm or disconfirm linguistic hypotheses. Pustejovsky confronted with the problem of automatic acquisition more extensively in [Pustejovsky et al 1993]. We inferred the bracketing by modifying an algorithm initially proposed by Pustejovsky et al (1993). The bracketing problem for noun-noun-noun compounds has been investigated by Liberrnan (1992), Pustejovsky et al (1993). (Pustejovsky et al., 1993) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and dependency.   Finally, (Pustejovsky et al, 1993) present an interesting framework for the acquisition of semantic relations from corpora not only relying on statistics, but guided by theoretical lexicon principles. The best known early work on automated unsupervised NC bracketing is that of Lauer (1995) who introduces the probabilistic dependency model for the syntactic disambiguation of NCs and argues against the adjacency model, proposed by Marcus (1980), Pustejovsky et al (1993) and Resnik (1993). Pustejovsky et al (1993) show how statistical techniques, such as mutual information measures can contribute to automatically acquire lexical information regarding the link between a noun and a predicate.  For example, Pustejovsky et al (1993) use generalized syntactic patterns for extracting qualia structures from a partially parsed corpus. Concerning relatedness measure, additional corpus-based measures such as Web-basedmeasures (Cimiano and Wenderoth, 2007) or measures based on syntactic relations (Pustejovsky et al, 1993) could appear to be useful for improving the ranking of the extracted relations.
Log-Linear Models For Word Alignment We present a framework for word alignment based on log-linear models. All knowledge sources are treated as feature functions, which depend on the source langauge sentence, the target language sentence and possible additional variables. Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information. In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features. Our experiments show that log-linear models significantly outperform IBM translation models. These approaches include an enhanced HMM alignment model that uses part-of speech tags (Toutanova et al, 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al, 2005). (Liu et al., 2005) uses a log-linear model with a greedy search. We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al, 2005). (Liu et al, 2005) presented a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment. The F-measures for Chinese-English and Arabic-English are usually around 80% (Liu et al, 2005) and 70% (Fraser and Marcu, 2007), respectively. Liu et al (2005) used a conditional log-linear model with similar features to those we have employed.  Liu et al (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. Analternative ME approach models alignment directly as a log-linear combination of feature functions (Liu et al., 2005). To make more confident conclusions, we also did tests on a larger hand-aligned data set used in Liu et al (2005).  Liu et al (2005) also develop a log-linear model, based on IBM Model 3. A straightforward approach to the alignment matrix is to build a log linear model (Liu et al, 2005) for the probability of the alignment A. For example, the sum over all alignments may be restricted to a sum over the n-best list from other aligners (Liu et al, 2005). This is a key difference between our model and (Liu et al, 2005).
Building Deep Dependency Structures Using A Wide-Coverage CCG Parser This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures. The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies. A set of dependency structures used for training and testing the parser is obtained from a treebank of CCG normal-form derivations, which have been derived (semi-) automatically from the Penn Treebank. The parser correctly recovers over 80% of labelled dependencies, and around 90% of unlabelled dependencies. Also, a wide-coverage statistical parser which produces syntactic dependency structures for English is available for CCG (Clark et al, 2002). The early dependency model of Clark et al (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head-dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). This paper assumes a basic understanding of CCG; see Steedman (2000) for an introduction, and Clark et al (2002) and Hockenmaier (2003a) for an introduction to statistical parsing with CCG. Clark et al (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures. This extends the approach of Clark et al (2002) who modelled the dependency structures directly, not using any information from the derivations. The dependency structures considered in this paper are described in detail in Clark et al (2002) and Clark and Curran (2003). Following Clark et al (2002), evaluation is by precision and recall over dependencies.  The results of Clark et al (2002) and Hockenmaier (2003a) are shown for comparison. This paper assumes a basic knowledge of CCG; see Steedman (2000) and Clark et al (2002) for an introduction. Following Clark et al (2002), we augment CCG lexical categories with head and dependency information. Clark et al (2002) give examples showing how heads can fill dependency slots during a derivation, and how long-range dependencies can be recovered through unification of co-indexed head variables. We have just begun the process of evaluating parsing performance using the same test data as Clark et al (2002). See Steedman (2000) for an introduction to CCG, and see Clark et al (2002) and Hockenmaier (2003) for an introduction to wide-coverage parsing using CCG. Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures. This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al (2002), and defines a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies. The conditional model used by the CCG parser of Clark et al (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. Like Clark et al (2002), we define predicate argument structure for CCG in terms of the dependencies that hold between words with lexical functor categories and their arguments. Like Clark et al. (2002), we do not take the lexical category of the dependent into account, and evaluate hhc; wi; i; h ; w0ii for labelled, and hh ; wi; ; h ; w0ii for unlabelled recovery.
On the Complexity of Non-Projective Data-Driven Dependency Parsing In this paper we investigate several nonprojective parsing algorithms for dependency parsing, providing novel polynomial time solutions under the assumption that each dependency decision is independent of all the others, called here the edge-factored model. We also investigate algorithms for non-projective parsing that account for nonlocal information, and present several hardness results. This suggests that it is unlikely that exact non-projective dependency parsing is tractable for any model richer than the edge-factored model. Koo et al (2007) and McDonald and Satta (2007) both describe how the Matrix Tree Theorem can be applied to computing the sum of scores of edge factored dependency trees and the edge marginals. The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald and Satta, 2007). The marginal p (yi=k|x;theta) can be computed by dividing this score by Zx (McDonald and Satta, 2007). Because these features consider multiple edges, including them in the CRF model would make exact inference intractable (McDonald and Satta, 2007). Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as "arc factorization" for nonprojective dependency parsing (McDonald and Satta, 2007). A projective dependency parse (top), and a non-projective dependency parse (bottom) for two English sentences; examples from McDonald and Satta (2007). In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the non-projective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard.McDonald and Pereira (2006) adopted an approximation based on O (n3) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. Exact parsing under such model, with arbitrary second-order features, is intractable (McDonald and Satta, 2007). A projective dependency parse (top), and a non-projective dependency parse (bottom) for two English sentences; examples from McDonald and Satta (2007). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta,1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). While, as pointed out by McDonald and Satta (2007), the inclusion of these features makes inference NP hard, by relaxing the integer constraints we obtain approximate algorithms that are very efficient and competitive with state-of-the-art methods. the problem is intractable in the absence of this assumption (McDonald and Satta, 2007). Exhaustive non projective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). Unfortunately, global inference and learning for graph-based dependency parsing is typically NP-hard (McDonald and Satta, 2007). Unfortunately, the non-projective parsing problem is known to be NP-hard for all but the simplest models (McDonald and Satta, 2007). McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for non projective parsing, showing that parsing for a variety of models is NP-hard. Second, McDonald and Satta (2007) propose an O (n5) algorithm for computing the marginals, as opposed to the O (n3) matrix-inversion approach used by Smith and Smith (2007) and ourselves. For example, both papers propose minimum-risk decoding, and McDonald and Satta (2007) discuss unsupervised learning and language modeling, while Smith and Smith (2007) define hidden variable models based on spanning trees.
Indexing With WordNet Synsets Can Improve Text Retrieval tem: Experiments in Automatic Document Pro- M. Sanderson. 1994. Word sense disambiguation information retrieval. In of 17th International Conference on Research and Development in Information Retrieval. A.F. Smeaton and A. Quigley. 1996. Experiments on using semantic distances between words in imcaption retrieval. Proceedings of the International Conference on Research and Development in IR. A. Smeaton, F. Kelledy, and R. O'Donnell. 1995. TREC-4 experiments at dublin city university: Thresolding posting lists, query expansion with and POS tagging of spanish. In Proceedings of TREC-4. M. Voorhees. 1994. Query relations. In of the 17th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval. (Gonzalo et al., 1998) pointed out some more weaknesses of WordNet for Information Retrieval purposes, in particular the lack of domain information and the fact that sense distinctions are excessively fine-grained for the task. Then, following previous studies (e.g., (Gonzaloet al, 1998)), we use the synsets relations in Word Net for query expansion. The LKB can be used, among others, for monolingual and cross-lingual information retrieval, which has been demonstrated in other projects (Gonzalo et al, 1998). Gonzalo et al (1998) cite this failure to model related senses in order to explain why their study into the effects of ambiguity showed radically different results to Sanderson (1994). Gonzalo et al (1998) showed in an experiment, where words were manually disambiguated, that a substantial increase in performance is obtained when query words are disambiguated, before they are expanded. (Gonzalo et al, 1998) demonstrates an increment in performance over an IR test collection using the sense data contained in SemCor over a purely term based model. The KB can be used, among others, for monolingual and cross-lingual information retrieval, which was demonstrated by (Gonzalo et al, 1998). In another work, Gonzalo et al (1998) used a manually sense annotated corpus, SemCor, to study the effects of incorrect disambiguation.
First- and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation Forests Many statistical translation models can be regarded as weighted logical deduction. Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs). We then introduce novel semiring, which computes second-order statistics (e.g., the variance of the hypothesis length or the gradient of entropy). This second-order semiring is essential for many interesting training paradigms such as minimum risk, deterministic annealing, active learning, and semi-supervised learning, where gradient descent optimization requires computing the gradient of entropy or risk. We use these semirings in an open-source machine translation toolkit, enabling minimum-risk training a benefit of up to 1.0 More generally, this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features (Li and Eisner, 2009). Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyper arcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). In order to learn the weights on the hyper arcs we perform the following procedure iteratively in an Em fashion (Li and Eisner, 2009). We converted the grammar into a hypergraph, and learned its probability distributions using a dynamic program similar to the inside-outside algorithm (Liand Eisner, 2009). Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009). A generic first-order expectation semiring is also provided (Li and Eisner, 2009). Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al, 2009). As usual in natural language processing applications, we can exploit appropriate semirings and compute several useful statistical parameters through Mw (T?? T?), as for instance the highest weight of a computation, the inside probability and the rule expectations; see (Li and Eisner, 2009) for further discussion. The sufficient statistics for graph expected BLEU can be computed using expectation semirings (Li and Eisner, 2009). For inside-outside algorithm, see (Li and Eisner, 2009). The risk and its gradient on a hypergraph can be computed by using a second-order expectation semiring (Li and Eisner, 2009). There lative weights of these 10 features are tuned via hypergraph-based minimum risk training (Li and Eisner, 2009) on the bilingual data Set. One is a variant of the F-B on the expectation semiring proposed in Li and Eisner (2009). For the detailed description, see Li and Eisner (2009) and its references. We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Liand Eisner, 2009). We have implemented the hypergraph based minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations. The work of Smith and Eisner was extended by Li and Eisner (2009) who were able to obtain much better estimates of feature expectations by using a packed chart instead of an n-best list. Instead of n-best approximations, we may directly employ forests for a better conditional log-likelihood estimation (Li and Eisner,2009). Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). This goal is different from the minimum risk training of Li and Eisner (2009) in a subtle but important way.
Application-driven Statistical Paraphrase Generation Paraphrase generation (PG) is important in plenty of NLP applications. However, the research of PG is far from enough. In this paper, we propose a novel method for statistical paraphrase generation (SPG), which can (1) achieve various applications based on a uniform statistical model, and (2) naturally combine multiple resources to enhance the PG performance. In our experiments, we use the proposed method to generate paraphrases for three different applications. The results show that the method can be easily transformed from one application to another and generate valuable and interesting paraphrases. Researchers have made great efforts to improve paraphrasing from different perspectives, such as paraphrase extraction (Zhao et al, 2007), paraphrase generation (Quirk et al, 2004), model optimization (Zhao et al., 2009) and etc. Zhao et al (2009) proposes an unified paraphrasing framework that can be adapted to different applications using different usability models. Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. Zhao et al (2008) apply SMT-style decoding for paraphrasing, using several log linear weighted resources while Zhao et al (2009) filter out paraphrase candidates and weight paraphrase features according to the desired NLP task. The results show that: (1) although the candidate paraphrases acquired by MT engines are noisy, they provide good raw materials for further paraphrase generation; (2) the selection-based technique is effective, which results in the best performance; (3) the decoding based technique is promising, which can generate paraphrases that are different from the candidates; (4) both the selection-based and decoding-based techniques outperform a state-of-the-art approach SPG (Zhao et al, 2009). Zhao et al (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications. We compare our method with a state-of-the art approach SPG (Zhao et al, 2009), which is a statistical approach specially designed for PG. The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009). In machine translation, n-gram-based evaluation measures like BLEU have been criticized exactly because they can not cope sufficiently well with paraphrases (Callison-Burch et al, 2006), which play a central role in abstractive sentence compression (Zhao et al, 2009a). The candidate compressions were generated by first using GA-EXTR and then applying existing paraphrasing rules (Zhao et al, 2009b) to the best extractive compressions of GA-EXTR. More recently, Zhao et al (2009a) presented a sentence paraphrasing method that can be configured for different tasks, including a form of sentence compression. To obtain candidate compressions, we first applied GA-EXTR to the 346 source sentences, and we then applied the paraphrasing rules of Zhao et al (2009b) to the resulting extractive compressions; we provide more information about GA-EXTR and the paraphrasing rules below. Zhao et al (2009b) associate each paraphrasing rule with a score, intended to indicate its quality.  Also, some studies have been done in paraphrase generation in NLG (Zhao et al, 2009), (Chevelu et al, 2009). Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions). Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). As an illustration of the need to combine grammatical paraphrasing with data-driven paraphrasing, consider the example that Zhao et al (2009) use to illustrate the application of their paraphrasing method to similarity detection, shown in Table 1. Zhao et al (2008a) enrich this approach by adding multiple resources (e.g., thesaurus) and further extend the method by generating different paraphrase in different applications (Zhao et al, 2009). More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al, 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009).
On-Demand Information Extraction At present, adapting an Information Extraction system to new topics is an expensive and slow process, requiring some knowledge engineering for each new topic. We propose a new paradigm of Information Extraction which operates 'on demand' in response to a user's query. On-demand Information Extraction (ODIE) aims to completely eliminate the customization effort. Given a user’s query, the system will automatically create patterns to extract salient relations in the text of the topic, and build tables from the extracted information using paraphrase discovery technology. It relies on recent advances in pattern discovery, paraphrase discovery, and extended named entity tagging. We report on experimental results in which the system created useful tables for many topics, demonstrating the feasibility of this approach. Our work is related to previous work on domain independent unsupervised relation extraction, in particular Sekine (2006), Shinyama and Sekine (2006) and Banko et al (2007). Sekine (2006) introduces On-demand information extraction, which aims at automatically identifying salient patterns and extracting relations based on these patterns. Shinyama and Sekine (2006) apply NER, coreference resolution and parsing to a corpus of newspaper articles to extract two-place relations between NEs. Like Sekine (2006) and Shinyama and Sekine (2006), we concentrate on relations involving NEs, the assumption being that these relations are the potentially interesting ones. Romano et al (2006) and Sekine (2006) used syntactic paraphrases to obtain patterns for extracting relations. This contrasts with Open Information Extraction (Banko and Etzioni, 2008) and On-Demand Information Extraction (Sekine, 2006), which aim to extract large databases of open-ended facts, and with supervised relation extraction, which requires additional supervised data to learn new relations. For instance, Sudo et al (2003) and Sekine (2006) proposed different methods for automatic IE pattern acquisition for a given domain based on frequent subtree discovery in dependency parse trees. To reduce the knowledge engineering burden on the user in constructing and porting an IE system, unsupervised learning has been utilized, e.g. Riloff (1996), Yangarber et al (2000), and Sekine (2006).
PART-OF-SPEECH TAGGING WITH NEURAL NETWORKS Hehnut Schmid Institute for Computational Linguistics, Azenbergstr.12, 70174 Stuttgart, Germany, schmid@ims.uni-stuttgart.de Topic area: large text corpora, part-of-speech tag- ging, neural networks 1 ABSTRACT Text corpora which are tagged with part-of-speech in- formation are useful in many areas of linguistic re- search.  For both tasks the reordering was performed as a preprocessing step using POS information from the TreeTagger (Schmid, 1994) for German and using the Amira Tagger (Diab, 2009) for Arabic.   The correct rate of tagging of these models has reached 95%, in part by using a very large amount of training data (e.g., 1,000,000 words in Schmid, 1994).   A statistical-based suffix learner is presented in (Schmid, 1994).  The TreeTagger showed an accuracy of 96.06% (Schmid, 1994a).  There have been attempts to apply neural networks to POS tagging (e.g., (Schmid, 1994)). We use the Tree Tagger (Schmid, 1994) for all POS tagging except for Arabic, where we use the tagger described in Diab et al. We use TreeTagger (Schmid, 1994) to produce POS tags and then open class words are restricted if the POS tagger assigned a tag with a probability over a certain threshold. A statistical-based suffix learner is presented in (Schmid, 1994). 
ROUGE: A Package For Automatic Evaluation Of Summaries for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different included in the summarization evaluation package and their evaluations. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST. ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004). We used ROUGE (Lin, 2004) as an evaluation criterion. Part of this research takes inspiration from the work on automatic evaluation in machine translation (Papineni et al, 2002) and automatic summarisation (Lin, 2004). with the addition of skip-bigram features derived from the ROUGE-S (Lin, 2004) measure. ROUGE (Lin, 2004) and its linguistically motivated descendent, Basic Elements (BE) (Hovy et al, 2005), evaluate a summary by computing its overlap with a set of model (human) summaries;. Version 1.5.5 of the ROUGE scoring algorithm (Lin, 2004) is also used for evaluating results. Such metrics have been introduced in other fields, including PAR ADISE (Walker et al, 1997) for spoken dialogue systems, BLEU (Papineni et al, 2002) for machine translation,1 and ROUGE (Lin, 2004) for summarisation. Automated evaluation utilizes the standard DUC evaluation metric ROUGE (Lin, 2004) which represents recall over various n-grams statistics from a system generated summary against a set of human generated peer summaries. We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task. For the evaluation measure, we used the standard ROUGE suite of automatic evaluation measures (Lin, 2004).   They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). ROUGE-2 (based on bi grams) and ROUGE-SU4 (based on both unigrams and skip-bi grams, separated by up to four words) are given by the official ROUGE toolkit with the standard options (Lin, 2004). Empirical evaluations using two standard summarization metrics the Pyra mid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004) show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%. Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004). However, original measures based on lexical matching, such as BLEU (Papineni et al, 2001a) and ROUGE (Lin, 2004) are still preferred as de facto standards in MT and AS, respectively. In the case of automatic summarization (AS), we have employed the standard variants of ROUGE (Lin, 2004). We verify that our method generates summaries that are significantly better than the baseline results in terms of ROUGEscore (Lin, 2004) and subjective readability measures. We used ROUGE (Lin, 2004) for evaluating the content of summaries.
Biomedical Named Entity Recognition Using Conditional Random Fields And Rich Feature Sets  Similarly, CRFs were employed by one system in isolation (Settles, 2004) and by another system in combination with SVMs (Song et al, 2004). However, one participant (Settles, 2004) reported that their at tempt to utilize gazetteers (together with other resources) had failed in gaining better overall performance. Settles (2004)'s CRF system deserves special note in the sense that it achieved comparable performance to top ranked systems with a rather simple feature set. While most of earlier approaches rely on handcrafted rules or dictionaries, many recent works adopt machine learning approaches ,e.g, SVM (Lee, 2003), HMM (Zhou, 2004), Maximum Entropy (Lin, 2004) and CRF (Settles,2004), especially with the availability of annotated corpora such as GENIA, achieving state-of-the-art performance. Genes/Proteins are not the Same Many of the existing BNER systems, which are mainly tuned for gene/protein identification, use features such as token shape (also known as word class and brief word class (Settles, 2004)), Greek alphabet matching, Roman number matching and so forth. (Settles, 2004) reported that a system using a subset of features out performed one using a full set of features.  The features used in our experiments mainly follow the work of (Settles, 2004) and (Collins, 2001). Our performance of the single-phase CRF with maximum likelihood training is 69.44%, which agrees with (Settles, 2004) who also uses similar settings.   The named entity tagger used throughout in this section is based on Conditional Random Fields and similar to the one presented by (Settles, 2004). Hence, the use of a named entity tagger supports the evaluation results when comparing the various biomedical entity recognition (Settles, 2004).
Use Of Deep Linguistic Features For The Recognition And Labeling Of Semantic Arguments We use deep linguistic features to predict semantic roles on syntactic arguments, and show that these perform considerably better than surface-oriented features. We also show that predicting labels from a “lightweight” parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features. Similarly, Chen and Rambow (2003) argue that the kind of deep linguistic features we harvest from FrameNet is beneficial for the successful assignment of PropBank roles to constituents, in this case using TAGs generated from PropBank to generate the relevant features. However, in most cases they can only provide a local dependency between predicate and argument for 87% of the argument constituents (Chen and Rambow, 2003), which is too low to provide high SRL accuracy. (Chenand Rambow, 2003) use LTAG-based decomposition of parse trees (as is typically done for statistical LTAG parsing) for SRL. Path feature from the derived tree, (Chen and Rambow, 2003) uses the path within the elementary tree from the predicate to the constituent argument. Prop was extracted using the PropBank annotations for ar gument/modifier distinction by a method similar to Chen and Rambow (2003). Although the results can not directly be compared with another work using LTAG (Chen and Rambow, 2003) because their target annotations were limited to those localized in an elementary tree, considering that their target annotations were 87% of core arguments, our results are competitive with their results (82.57/71.41). Another possibility is to directly extract PropBank-style semantic representations by reforming the grammar extraction algorithm (Chen and Rambow, 2003), and to estimate a disambiguation model using the PropBank. As a result some of the features that undo long distance movement via trace information in the TreeBank as used in (Chen and Rambow, 2003) cannot be exploited in our model. (Chen and Rambow, 2003) discuss amodel for SRL that uses LTAG-based decomposition of parse trees (as is typically done for statistical LTAG parsing). Instead of using the typical parse tree features used in typical SRL models, (Chen and Rambow, 2003) uses the path within the elementary tree from the predicate to the constituent argument. As a result, if we do not compare the machine learning methods involved in the two approaches, but rather the features used in learning, our features are a natural generalization of (Chen and Rambow, 2003). The LTAG-spinal Treebank can be used to overcome some of the limitations of the previous work on SRL using LTAG: (Liu and Sarkar, 2007) uses LTAG-based features extracted from phrase-structure trees as an additional source of features and combined them with features from a phrase-structure based SRL framework; (Chenand Rambow, 2003) only considers those complement/adjunct semantic roles that can be localized in LTAG elementary trees, which leads to a loss of over 17% instances of semantic roles even from gold-standard trees.
Interpretation As Abduction To interpret a sentence: An approach to abductive inference developed in the TAC- ITUS project has resulted in a dramatic simplification of how the problem of interpreting texts is conceptualized. Its use in solving the local pragmatics problems of reference, compound nominals, syntactic ambiguity, and metonymy is described and illustrated. It also suggests an elegant and thorough integration of syntax, semantics, and pragmatics. Two main extensions from that work that we are making use of are: 1) proofs falling below a user defined cost threshold halt the search 2) a simple variable typing system reduces the number of axioms written and the size of the search space (Hobbs et al, 1988, pg 102). The domain axioms will bind the body variables to their most likely referents during unification with facts, and previously assumed and proven propositions similarly to (Hobbs et al, 1988). A third approach, exemplified by Moldovan et al (2003) and Raina et al (2005), is to translate dependency parses into neo-Davidsonian-style quasi-logical forms, and to perform weighted abductive theorem proving in the tradition of (Hobbs et al, 1988). As shown in "Interpretation as Abduction" (Hobbs et al 1988), abductive inference is inference to the best explanation. While pursuing the path tracing enabling minimal explanation, now we are going to propose a connectability measure similar such as "weighted abduction" (Hobbs et al 1988).
An Error-Driven Word-Character Hybrid Model for Joint Chinese Word Segmentation and POS Tagging In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging. Our word-character hybrid model offers high performance since it can handle both known and unknown words. We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an errordriven policy that delivers such balance by acquiring examples of unknown words from particular errors in a training corpus. We describe an efficient framework for training our model based on the Margin Infused Relaxed Algorithm (MIRA), evaluate our approach on the Penn Chinese Treebank, and show that it achieves superior performance compared to the state-ofthe-art approaches reported in the literature. We follow Kruengkrai et al (2009) and split the CTB 5 into training, development testing and testing sets, as shown in Table 3. Kruengkrai et al (2009) made use of character type knowledge for spaces, numerals, symbols, alphabets, Chinese and other characters. Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. The work of Kruengkrai et al (2009) is based on Nakagawa and Uchimoto (2007), which separates the processing of known words and unknown words, and uses a set of segmentation tags to represent the segmentation of characters. Our learning and decoding algorithms are also different from Kruengkrai et al (2009).  K2009 is the result of Kruengkrai et al (2009).  Therefore, it's natural that we evaluate the accuracy of joint Chinese word segmentation and part of speech tagging, as reported in previous literature (Kruengkrai et al 2009). For ordinary word segmentation, the best result is reported to be around 97% F1 on CTB 5.0 (Kruengkrai et al, 2009), while our parser performs at 97.3%, though we should remember that the result concerns flat words only. Because the tasks of word segmentation and POS tagging have strong interactions, many studies have been devoted to the task of joint word segmentation and POS tagging for languages such as Chinese (e.g. Kruengkrai et al (2009)). "Kruengkrai+ '09" is a lattice-based model by Kruengkrai et al (2009). We applied morphological analysis and part-of-speech tagging by using TreeTagger (Schmid, 1994) for English, JUMAN for Japanese, and mma (Kruengkrai et al, 2009) for Chinese, respectively. We used the MMA system (Kruengkrai et al, 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline parser to parse all the sentences in the data. Firstly, the Chinese sentences are segmented, POS tagged and parsed by the tools described in Kruengkrai et al (2009) and Cao et al (2007), both of which are trained on the Penn Chinese Treebank 6.0. On the Chinese side, we used the morphological analyzer described in (Kruengkrai et al, 2009) trained on the training data of CTBtp to perform word segmentation and POS tagging and used the first-order Parsers to parse all the sentences in the data.    We used the MMA system (Kruengkrai et al, 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline Parser to parse all the sentences in the data.
Online Learning Of Approximate Dependency Parsing Algorithms In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word. We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms. We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish. To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MST Parser 1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transition based parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011), and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al, 2013). In this paper, we integrate the cost from a graph-based model (McDonald and Pereira, 2006) which directly models dependency links. The 1st- and sibling 2nd-order models are the same as McDonald and Pereira (2006)'s definitions, except the cost factors of the sibling 2nd-order model. This is different from McDonald and Pereira (2006) in that the cost factors for left children are calculated from left to right, while those in McDonald and Pereira (2006)'s definition are calculated from right to left. Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. A set of feature templates in (Huang and Sagae, 2010) were used for the stack-based model, and a set of feature templates in (McDonald and Pereira, 2006) were used for the 2nd-order prediction model. Although the accuracy of our method did not reach that of (McDonald and Pereira, 2006), the scores were competitive even though our method is deterministic. These include beam search (Lowerre, 1976), cube pruning, which we discuss in ?3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like McDonald and Pereira (2006), in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest. We adopt the second-order graph-based dependency parsing model of McDonald and Pereira (2006) as our core parser, which incorporates features from the two kinds of subtrees in Fig. This can be done with the Viterbi decoding algorithm described in McDonald and Pereira (2006) in O (n3) parsing time.  McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work. This framework is efficient for both projective and non-projective parsing and provides an online learning algorithm which combined with a rich feature set creates state-of-the-art performance across multiple languages (McDonald and Pereira, 2006). However, McDonald and Pereira (2006) mention the restrictive nature of this parsing algorithm. For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. In McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard. McDonald and Pereira (2006) define this as a second-order Markov assumption. McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. We also investigated an extension, McDonald and Pereira (2006)'s second-order model, where more of the parsing history is taken into account, viz. Indeed, the highest scoring parsers trained using the MSTPARSER (McDonald and Pereira, 2006) and MALTPARSER (Nivre et al, 2006) parsing suites achieved only 78.8 and 81.1 labeled attachment F1, respectively.
From Grammar To Lexicon: Unsupervised Learning Of Lexical Syntax Imagine a language that is completely unfamiliar; the only means of studying it are an ordinary grammar book and a very large corpus of text. No dictionary is available. How can easily recognized, surface grammatical facts be used to extract from a corpus as much syntactic information as possible about individual words? This paper describes an approach based on two principles. First, rely on local morpho-syntactic cues to structure rather than trying to parse entire sentences. Second, treat these cues as probabilistic rather than absolute indicators of syntactic structure. Apply inferential statistics to the data collected using the cues, rather than drawing a categorical conclusion from a single occurrence of a cue. The effectiveness of this approach for inferring the syntactic frames of verbs is supported by experiments on an English corpus using a program called Lerner. Lerner starts out with no knowledge of content words—it bootstraps from determiners, auxiliaries, modals, prepositions, pronouns, complementizers, coordinating conjunctions, and punctuation. Since Brent (1993) a considerable amount of research focusing on large-scaled automatic acquisition of subcategorization frames (SCF) has met with some success not only in English but also in many other languages. Apply some statistical tests such as the Binomial Hypothesis Test (Brent, 1993) and log likelihood ratio score (Dunning, 1993) to SCCs to filter out false SCCs on the basis of their reliability and likelihood. We also experimented with a method suggested by Brent (1993) which applies the binomial test on frame frequency data.  Since (Brent 1993) began to use the method, most researchers have agreed that the BHT results in better precision and recall with SCF hypotheses of high, medium and low frequencies. (Brent 1993) estimated pe according to the acquisition system's performance.   Following studies on automatic SCF extraction (Brent, 1993), we apply a statistical test (Binomial Hypothesis Test) to the unfiltered-Levin-SCF to filter out noisy SCFs, and denote the resulting SCF set as filtered-Levin SCF. Other works describe systems that induce structures from corpora, but they use tagged corpora (Brill, 1993), or grammatical informations (Brent, 1993). Our evaluator has two parts: the Binomial Hypothesis Test (Brent, 1993) and a back-off algorithm (Sarkar and Zeman, 2000). The final component assesses the frames encountered by the parser by using the same model as (Brent, 1993), with the error rate set empirically. (Brent, 1993) relies on local morphosyntactic cues (such as the -ing suffix, except where such a word follows a determiner or a preposition other than to) in the untagged Brown Corpus as probabilistic indicators of six different predefined subcategorisation frames. (Briscoe and Carroll, 1997) observe that in the work of (Brent, 1993), (Manning, 1993) and (Ushioda et al, 1993), the maximum number of distinct sub categorization classes recognized is sixteen, and only Ushioda et al attempt to derive relative subcategorization frequency for individual predicates. In his seminal work, Brent (1993) already pointed out that the cues occur in contexts that were not aimed at. (Brent, 1993) uses regular patterns. Thus, Brent (1993) only creates hypotheses on the basis of instances of verb frames that are reliably and unambiguously cued by closed class items (such as pronouns) so there can be no other attachment possibilities. The foundational work of (Brent, 1993) was based on plain text (2.6 million words of the Wall StreetJournal (WSJ, 1994)).
Unsupervised Discovery Of Morphemes We present two methods for unsupervised segmentation of words into morphemelike units. The model utilized is especially suited for languages with a rich morphology, such as Finnish. The first method is based on the Minimum Description Length (MDL) principle and works online. In the second method, Maximum Likelihood (ML) optimization is used. The quality of the segmentations is measured using an evaluation method that compares the segmentations produced to an existing morphological analysis. Experiments on both Finnish and English corpora show that the presented methods perform well compared to a current stateof-the-art system. We previously presented two segmentation algorithms suitable for agglutinative languages (Creutz and Lagus, 2002). A Poisson distribution can be justified and has been used in order to model the length distribution of word and morph tokens [e.g., (Creutz and Lagus, 2002)], but for morph types we have chosen the gamma distribution, which has a thicker tail. The search for the optimal model given our input data corresponds closely to the recursive segmentation algorithm presented in (Creutz and Lagus, 2002). We utilize an evaluation method for segmentation of words presented in (Creutz and Lagus, 2002). For highly inflecting languages more generally, morphological analysis is often treated as a segment and-normalise problem, amenable to analysis by weighted finite state transducer (wFST), for example, Creutz and Lagus (2002) for Finnish. Creutz and Lagus (2002) proposed two unsupervised methods for word segmentation, one based on maximum description length, and one based on maximum likelihood. Finnish is also the language for which the algorithm for the unsupervised morpheme discovery (Creutz and Lagus, 2002) was originally developed. Morfessor Baseline (Creutz and Lagus, 2002): This is a public baseline algorithm based on jointly minimizing the size of the morph codebook and the encoded size of all the word forms using the minimum description length MDL cost function. Similarly, Creutz and Lagus (2002) use an MDL formulation for word segmentation.   
Efficient Normal-Form Parsing For Combinatory Categorial Grammar Under categorial grammars that have powerful rules like composition, a simple n-word sentence can have exponentially many parses. Generating all parses is inefficient and obscures whatever true semantic ambiguities are in the input. This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar, by means of an efficient, correct, and easy to implement normal-form parsing tech- The parser is proved to find exone in each semantic equivalence class of allowable parses; that is, spurious ambiguity (as carefully defined) is shown to be both safely and completely eliminated.  Eisner (1996, p.81) in fact suggested that the labeling system can be implemented in the grammar by templates, or in the processor by labeling the chart entries. But, as pointed out by Eisner (1996, p.85), this is not spurious ambiguity in the technical sense, just multiple derivations due to alternative lexical category assignments. The C&C parser employs the normal-form constraints of Eisner (1996) to address spurious ambiguity in 1-best parsing. The second strategy is to use Eisner's normal form constraints (Eisner, 1996).  The parser only used a subset of CCG, pureCCG (Eisner, 1996), consisting of the Application and Composition rules. We propose and implement a modification of the Eisner (1996) normal form to account for generalized composition of bounded degree, and an extension to deal with grammatical type-raising. Eisner (1996, section 5) also provides a safe and complete parsing algorithm which can return non-NF derivations when necessary to preseve an interpretation if composition is bounded or the grammar is restricted in other (arbitrary) ways. We have proposed a modification and extension of Eisner (1996)'s normal form that is more appropriate for commonly used variants of CCG with grammatical type-raising and generalized composition of bounded degree, as well as some non-combinatory extensions to CCG. Hockenmaier (2003a) uses a model which favours only one of the derivations leading to aderived structure, namely the normal-form derivation (Eisner, 1996). For the normal-form model we were able to reduce the size of the charts considerably by applying two types of restriction to the parser: first, categories can only combine if they appear together in a rule instantiation in sections 2 21 of CCGbank; and second, we apply the normal-form restrictions described in Eisner (1996). Our transformation is also technically related to the normal form construction for CCG parsing presented by Eisner (1996). Consider the derivations in Figures 1 and 2, which show a normal form derivation (Eisner, 1996) and fully incremental derivation, respectively. CCG parsers often limit the use of the combinatory rules (in particular: type-raising) to obtain a single right-branching normal form derivation (Eisner, 1996) for each possible semantic interpretation. Both perspectives on D ensure that the new rules are compatible with normal form constraints (Eisner, 1996) for controlling spurious ambiguity. In this section, we show that the D rules fit naturally within standard normal form constraints for CCG parsing (Eisner, 1996), by providing both combinatory and logical bases for D. Furthermore, CCG augmented with D is compatible with Eisner NF (Eisner, 1996), a standard technique for controlling derivational ambiguity in CCG-parsers, and also with the modalized version of CCG (Baldridge and Kruijff, 2003). One set are the normal form constraints, as described by Eisner (1996). We applied the same normal-form restrictions used in Clark and Curran (2004b): categories can 12 only combine if they have been seen to combine in Sections 2-21 of CCGbank, and only if they do not violate the Eisner (1996a) normal-form constraints.
Cues And Control In Expert-Client Dialogues We conducted an empirical analysis into the relation between control and discourse structure. We applied control criteria to four dialogues and identified 3 levels of discourse structure. We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control. Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not. Whittaker and Stenton (1988) devised rules for allocating dialogue control based on utterance types, and Walker and Whittaker (1990) utilized these rules for an analytical study on discourse segmentation. Suppose we adopt a model that maintains a single thread of control, such as that of (Whittaker and Stenton, 1988). Repetitions and prompts also suggest that the speaker has nothing more to say and indicate that the hearer should take over the initiative (Whittaker and Stenton, 1988). Whittaker and Stenton (1988) proposed rules for tracking initiative based on utterance types; for example, statements, proposals, and questions show initiative, while answers and acknowledgements do not. Initiative is held by the speaker who is driving the conversation at any given point in the conversation (Whittaker and Stenton, 1988). Whittaker and Stenton (1988) looked at the correlation of control boundaries to discourse markers, and Walker and Whittaker (1990) looked at anaphoric reference. In our study, the first author coded initiative using the annotation scheme of Whittaker and Stenton (1988). Whittaker and Stenton (1988) assigned initiative to the speaker of statements, except when it was the answer to a question, in which case it belonged to the speaker asking the question. Whittaker and Stenton (1988) proposed rules for tracking initiative based on utterance types; for example, statements, proposals, and questions show initiative, while answers and acknowledgements do not. Following Whittaker and Stenton (1988), we use utterance tags to determine whether an utterance shows initiative: forward functions show initiative while others do not. After experimenting with several tagging methods, we concluded that the approach presented in Walker and Whittaker (1990) adopted from (Whittaker and Stenton, 1988) best captured the aspects of the dialogue we were interested in and, as with the DAs, could be tagged reliably on our data. Constructs such as Initiative and Control (Whittaker and Stenton, 1988), which attempt to operationalize the authority over a discourse's structure, fall under the umbrella of positioning. A speaker who begins a discourse segment is said to have initiative, while control accounts for which speaker is being addressed in a dialogue (Whittaker and Stenton, 1988).
Generalizing Word Lattice Translation Word lattice decoding has proven useful in spoken language translation; we argue that it provides a compelling model for translation of text genres, as well. We show that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous context-free grammarbased models. Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models. Our experiments evaluating the approach demonstrate substantial gains for Chinese- English and Arabic-English translation. The concept of compatible coverage vectors for the locations of translated words becomes the notion of reachability between frontier nodes in the lattice (Dyer et al, 2008). We also plan to jointly optimize MT and name tagging by propagating multiple word segmentation and name annotation hypotheses in lattice structure to statistical MT and conduct lattice based decoding (Dyer et al, 2008). In applications like the one described by Dyer et al (2008), where several different segmenters for Chinese are combined to create the lattice, this is not possible. Dyer et al (2008) use it to encode different Chinese word segmentations or Arabic morphological analyses. Our work differs from (Dyer et al, 2008) in that we explicitly distinguish the various preprocessing types in the lattice so that we can define specific path features and lexicalize the lattice path probabilities within the phrase model. It was noted by Dyer et al (2008) that the standard distance-based reordering model needs to be redefined for lattice input. Using the shortest path within the lattice is reported to have better performance in (Dyer et al, 2008), however we did not implement it due to time constraints. Our word lattices are similar to those used by Dyer et al (2008) for handling word segmentation in Chinese and Arabic. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang,2008), word lattices instead of 1-best segmentations (Dyer et al, 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al, 2009). Lattice represent the system implemented as Dyer et al, (2008). Same as Dyer et al, (2008), we also extracted rules from a combined bilingual corpus which contains three copies from different segmenters. Du et al (2010), in this proceedings, explore the use of source paraphrases without targeting apparent mistranslations, using lattice translation (Dyer et al, 2008) to efficiently represent and decode the resulting very large space of paraphrase alternatives. Finally, some researchers have advocated recently the use of shared structures such as parse forests (Mi and Huang, 2008) or word lattices (Dyer et al, 2008) in order to allow a compact representation of alternative inputs to an SMT system. All of the systems we present use the lattice input format to Moses (Dyer et al, 2008), including the baselines which do not need them. Recently, several studies have shown that offering more alternatives of annotations to SMT systems will result in significant improvements, such as replacing 1-best trees with packed forests (Miet al, 2008) and replacing 1-best word segmentations with word lattices (Dyer et al, 2008). Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al, 2008), word lattices instead of 1-best segmentations (Dyer et al, 2008), and n-best alignments instead of 1-best alignments (Venugopal et al, 2008). As lattice is a more general form of confusion network (Dyer et al, 2008), we expect that replacing confusion networks with lattices will further improve system combination. Our implementation's runtime and memory overhead is proportional to the size of the lattice, rather than the number of paths in the lattice (Dyer et al, 2008). Lattice parsing is not new to translation (Dyer et al, 2008), but to our knowledge it has not been used in this way. Dyer et al (2008) report improvements from multiple Arabic segmentations in translation to English translation, but their goal was to demonstrate the value of lattice-based translation.
Feature Forest Models for Probabilistic HPSG Parsing Probabilistic modeling of lexicalized grammars is difficult because these grammars exploit complicated data structures, such as typed feature structures. This prevents us from applying common methods of probabilistic modeling in which a complete structure is divided into substructures under the assumption of statistical independence among sub-structures. For example, part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsing is split into applications of CFG rules. These methods have relied on the structure of the target problem, namely lattices or trees, and cannot be applied to graph structures including typed feature structures. This article proposes the feature forest model as a solution to the problem of probabilistic modeling of complex data structures including typed feature structures. The feature forest model provides a method for probabilistic modeling without the independence assumption when probabilistic events are represented with feature forests. Feature forests are generic data structures that represent ambiguous trees in a packed forest structure. Feature forest models are maximum entropy models defined over feature forests. A dynamic programming algorithm is proposed for maximum entropy estimation without unpacking feature forests. Thus probabilistic modeling of any data structures is possible when they are represented by feature forests. This article also describes methods for representing HPSG syntactic structures and predicate–argument structures with feature forests. Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing. The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed. These corpora are automatically parsed by Enju 2.3.1 (Miyao and Tsujii, 2008), and the features are extracted from the parsing results.  Following our precious work (Wu et al, 2010), we use head-drive phrase structure grammar (HPSG) forests generated by Enju (Miyao and Tsujii, 2008), which is a state-of-the-art HPSG parser for English. As the syntactic parser, we used the Enju (Miyao and Tsujii, 2008) English HPSG parser. Models based on deep grammars such as CCG (Hockenmaier and Steed man, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. We also experimented with the ENJU parses (Miyao and Tsujii, 2008) provided by the shared task organizers. We used the C & C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al, 2007) adapted for the biomedical domain (i.e., ENJU-Genia); There were a number of practical issues to consider when using parsers for this task.  This metric is broadly comparable to the predicate-argument dependencies of CCGBank (Hockenmaier and Steed man, 2007) or of the ENJU grammar (Miyao and Tsujii, 2008), and also somewhat similar to the grammatical relations (GR) of the Briscoe and Carroll (2006) version of DepBank. Isozaki et al (2010b) proposed a simple method of Head Finalization, by using an HPSG-based deep parser for English (Miyao and Tsujii, 2008) to obtain phrase structures and head information. Second, input English sentences are parsed by a high-quality parser, Enju (Miyao and Tsujii, 2008), which outputs syntactic heads. We used Enju (Miyao and Tsujii, 2008) v2.4.2 for parsing the English side of the training data. For this experiment, we choose the C & C parser (Clark and Curran, 2003) for CCG, Enju parser (Miyao and Tsujii, 2008) for HPSG and pipeline automatic annotator (Cahill et al, 2004) with Charniak parser for LFG. Figure 4 shows an example of text conversion and annotation alignment that are required when the Enju parser (Miyao and Tsujii, 2008) needs to be used for the annotation of protein names. For training, we use the feature forest model (Miyao and Tsujii, 2008), which was originally designed as an efficient algorithm for solving maximum entropy models for data with complex structures. The other deep parser used was the HPSG parser Enju by Miyao and Tsujii (2008), also trained on GTB. We employed the parser and the supertagger of (Miyao and Tsujii, 2008), specifically, its generalized modules for lexicalized grammars.
Improving Translation Quality by Discarding Most of the Phrasetable It is possible to reduce the bulk of phrasetables for Statistical Machine Translation us ing a technique based on the significance testing of phrase pair co-occurrence in the parallel corpus. The savings can be quitesubstantial (up to 90%) and cause no reduction in BLEU score. In some cases, an im provement in BLEU is obtained at the same time although the effect is less pronounced if state-of-the-art phrasetable smoothing is employed. We pruned the generated phrase tables following the method introduced in (Johnson et al., 2007). See (Johnson et al, 2007) for details. Significance-based filtering (Johnson et al 2007) was applied to the resulting phrase table. Johnson et al (2007) reduced the phrase table based on the significance testing of phrase pair co-occurrence in bilingual corpus. Various filtering techniques, such as (Johnson et al., 2007) and (Chen et al, 2008), have been applied to eliminate a large portion of the translation rules that were judged unlikely to be of value for the current translation. We filtered the obtained phrase table using the method described in (Johnson et al, 2007). Significance filtering of the phrase tables (Johnson et al,2007) implemented for Moses by Chris Dyer.   Johnson et al, (2007) presented a technique for pruning the phrase table in a PBMT system using Fisher's exact test. Yang and Zheng (2008) extended the work in Johnson et al, (2007) to a hierarchical PBMT model, which is built on synchronous context free grammars (SCFG). We consider all possible phrase-pairs in the training data, then use Fisher's Exact Test to filter out pairs with low correlation (Johnson et al, 2007). To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al (2007a). (Johnson et al, 2007) has presented a technique for pruning the phrase table in a phrase based SMT system using Fisher's exact test. In this paper, we extend the work in (Johnson et al, 2007) to a hierarchical phrase-based translation model, which is built on synchronous context free grammars (SCFG). Therefore, several approaches were proposed to filter these phrase-tables, reducing considerably their size without any loss of the quality, or even achieving improved performance (Johnson et al, 2007). First, statistically unreliable translation pairs (Johnson et al2007) are filtered out. The resulting translation pairs were then filtered with the significance pruning technique of (Johnson et al2007), using a+e as threshold. Johnson et al (2007) has shown that large portions of the phrase table can be removed without loss in translation quality. The work of Johnson et al (2007) is promising as it shows that large parts of the phrase table can be removed without affecting translation quality.
Meaningful Clustering Of Senses Helps Boost Word Sense Disambiguation Performance Fine-grained sense distinctions are one of the major obstacles to successful Word Sense Disambiguation. In this paper, we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies, namely the Oxford Dictionary of English. We assess the quality of the mapping and the induced clustering, and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task. However, in the coarse-grained task, the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses (Navigli, 2006). Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarse grained sense distinctions of the Oxford Dictionary of English (ODE). The classifier also made use of resources such as topic signatures data (Agirre and de Lacalle, 2004), the WordNet domain dataset (Magnini and Cavaglia`, 2000), and the mappings of WordNet senses to ODE senses produced by Navigli (2006). In addition, we show in Table 7 the F-score results provided by Snow et al (2007) for their SVM-based system and for the mapping-based approach of Navigli (2006), denoted by ODE. Navigli (2006) has induced clusters by mapping WordNet senses to a more coarse-grained lexical resource. (Navigli, 2006) presents an automatic approach for mapping between sense inventories; here similarities in gloss definition and structured relations between the two sense inventories are exploited in order to map between WordNet senses and distinctions made within the coarser-grained Oxford English Dictionary. Finally, we use as a feature the mappings produced in (Navigli, 2006) of WordNet senses to Oxford English Dictionary senses. In order to evaluate the entire sense-clustered taxonomy, we have employed an evaluation method inspired by Word Sense Disambiguation (this is similar to an evaluation used in Navigli, 2006, however we do not remove monosemous clusters). To tackle the granularity issue, we produced a coarser-grained version of the WordNet sense inventory based on the procedure described by Navigli (2006). The data were annotated with coarse-grained senses which were obtained by clustering senses from the Word Net 2.1 sense inventory based on the procedure proposed by Navigli (2006).  This clustering was created automatically with the aid of a methodology described in (Navigli, 2006).  Automatically creating new alignments is difficult because of word ambiguities, different granularities of senses, or language specific conceptualizations (Navigli, 2006).
A Metalearning Approach to Processing the Scope of Negation Finding negation signals and their scope in text is an important subtask in information extraction. In this paper we present a machine learning system that finds the scope of negation in biomedical texts. The system combines several classifiers and works in two phases. To investigate the robustness of the approach, the system is tested on the three subcorpora of the BioScope corpus representing different text types. It achieves the best results to date for this task, with an error reduction of 32.07% compared to current state of the art results. Our goal was to investigate the performance of a memory-based approach to the event extraction task, using only the information available in the training corpus and modelling the task applying an approach similar to the one that has been applied to tasks like semantic role labeling (Morante et al, 2008) or negation scope detection (Morante and Daelemans, 2009). In keeping with the evaluation presented by Morante and Daelemans (2009), the number of perfectly identified negation scopes is measured separately as the percentage of correct scopes (PCS). The best reported performance to date on the BioScope full papers corpus was presented by Morante and Daelemans (2009), who achieved an F1 score of 70.9 with predicted negation signals, and an F1 score of 84.7 by feeding the manually annotated negation cues to their scope finding system. Morante and Daelemans describe a method for improving resolution of the scope of negation by combining IGTREE, CRF, and Support Vector Machines (SVM) (Morante and Daelemans, 2009). The effect of negation has been broadly studied in NLP (Morante and Daelemans, 2009) and sentiment analysis (Jia et al, 2009). Still, Morante and Daelemans (2009), for example, used various classifiers (Memory-based Learners, Support Vector Machines, and Conditional Random Fields) to detect negation cues and their scope. Similarly, Morante and Daelemans (2009b) developed a machine learning system for identifying hedging cues and their scopes. Compared with negation scope finding, negation signal finding is much simpler and has been well resolved in the literature, e.g. with the accuracy of 95.8% -98.7% on the three sub corpora of the Bioscope corpus (Morante and Daelemans, 2009). Morante et al (2008) and Morante and Daelemans (2009) pioneered the research on negation scope finding by formulating it as a chunking problem, which classifies the words of a sentence as being inside or outside the scope of a negation signal. For example, given golden negation signals on the Bioscope corpus, Morante and Daelemans (2009) only got the performance of 50.26% in PCS (percentage of correct scope) measure on the full papers sub corpus (22.8 words per sentence on average), compared to 87.27% in PCS measure on the clinical reports subcorpus (6.6 words per sentence on average). Morante and Daelemans (2009) further improved the performance by combing several classifiers. For detailed statistics about the three subcorpora, please see Morante and Daelemans (2009). Following the experimental setting in Morante and Daelemans (2009), the abstracts sub corpus is randomly divided into 10 folds so as to perform 10-fold cross validation, while the performance on both the papers and clinical reports subcorpora is evaluated using the system trained on the whole abstracts subcorpus. Although Morante and Daelemans (2009) reported the performance of 95.8%-98.7% on negation signal finding, it lowers the performance of negation scope finding by about 7.29%-16.52% in PCS measure.  For example, given golden negation cues on the Bioscope corpus, Morante and Daelemans (2009a) only got the performance of 50.26% in PCS on the full papers sub corpus (22.8 words per sentence on average), compared to 87.27% in PCS on the clinical reports sub corpus (6.6 words per sentence on average). Speculation Scope Learning Similar to Morante and Daelemans (2009a), Morante and Daelemans (2009b) formulated speculation scope identification as a chunking problem which predicts whether a word in the sentence is inside or outside of the speculation scope, with proper post-processing to ensure consecutiveness of the speculation scope.   
Hierarchical Phrase-Based Translation with Suffix Arrays A major engineering challenge in statistical machine translation systems is the efficient representation of extremely large translationrulesets. In phrase-based models, this prob lem can be addressed by storing the training data in memory and using a suffix array asan efficient index to quickly lookup and extract rules on the fly. Hierarchical phrasebased translation introduces the added wrin kle of source phrases with gaps. Lookup algorithms used for contiguous phrases nolonger apply and the best approximate pat tern matching algorithms are much too slow, taking several minutes per sentence. Wedescribe new lookup algorithms for hierar chical phrase-based translation that reduce the empirical computation time by nearly two orders of magnitude, making on-the-fly lookup feasible for source phrases with gaps. Due to space constraints, details and proof of correctness are available in Lopez (2007a). However, in machine translation most features can still be traced back to the IBM Models of 15 years ago (Lopez, 2007b). We built grammars using its implementation of the suffix array extraction method described in Lopez (2007). We use the GIZA toolkit (Och and Ney, 2000), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och et al, 2003) to obtain word alignments, a translation model, language models, and the optimal weights for combining these mod els, respectively. Lopez (2007) extracts rules on-the-fly from the training bi text during decoding, searching efficiently for rule patterns using suffix arrays. Joshua (Li et al, 2009) is an implementation of Hiero (Chiang, 2007) using a suffix-array-based grammar extraction approach (Lopez, 2007). The toolkit also implements suffix-array grammar extraction (Lopez, 2007) and minimum error rate training (Och, 2003). In this system, we use the GIZA++toolkit (Och and Ney, 2003), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och, 2003) to obtain word-alignments, a translation model, language models, and the optimal weights for combining these models, respectively. We use GIZA++ (Och and Ney,2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006) to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. The hierarchical phrase-base translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). Besides storing the whole grammar locally in memory, other approaches have been developed, such as suffix arrays, which lookup and extract rules on the fly from the phrase table (Lopez, 2007). The pipeline extracts a Hiero-style synchronous context-free grammar (Chiang, 2007), employs suffix-array based rule extraction (Lopez, 2007), and tunes model parameters with minimum error rate training (Och,2003). This data structure has been used similarly to index whole training sentences for efficient retrieval (Lopez, 2007). (Lopez, 2007) proposed an extension of this method for retrieving discontinuous substrings, making it suitable for systems such as (Chiang, 2007).  The basis of the method in (Lopez, 2007) is to look for the occurrences of continuous substrings using a Suffix Array, and then intersect them to find the occurrences of discontinuous substrings. There is also an exponential number of discontinuous substrings, but (Lopez, 2007) only consider substrings of bounded size, limiting this problem. This hypergraph will not only fit the same role as the Prefix Tree of (Lopez, 2007), but also will allow us to easily implement different search strategies for flexible search (section 6). This allows in turn to compute by intersection the occurrences of discontinuous treelets, much like what is done in (Lopez, 2007) for discontinuous strings. In practice, the intersection operation will be implemented using merge and binary merge algorithms (Baeza-Yates and Salinger, 2005), following (Lopez, 2007).
A Statistical Parser For Czech This paper considers statistical parsing of Czech, which differs radically from English in at least two (1) it is a inflected and it has relatively word order. differences are likely to pose new problems for techniques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results — 80% dependency accuracy — represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text.   Collins et al (1999) used transformed constituency tree bank from Prague Dependency Treebank for constituent parsing on Czech. This is consistent with the findings of Collins et al (1999) for Czech, where the bigram model upped dependency accuracy by about 0.9%, as well as for English where Charniak (2000) reports an increase in F-score of approximately 0.3%.   To create dependency structures from the Penn Treebank, we used the extraction rules of Yamada and Matsumoto (2003), which are an approximation to the lexicalization rules of Collins (1999). In particular, we used the method of Collins et al (1999) to simplify part-of-speech tags since the rich tags used by Czech would have led to a large but rarely seen set of POS features. The Czech parser of Collins et al (1999) was run on a different data set and most other dependency parsers are evaluated using English. Lattice parsing (Chappelier et al, 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. For the Czech data, we used the predefined training, development and testing split of the Prague Dependency Treebank (Hajiˇc et al., 2001), and the automatically generated POS tags supplied with the data, which we reduce to the POS tag set from Collins et al (1999).   We are grateful to Yamada and Matsumoto for letting us use their rule set, which is a slight modification of the rules used by Collins (1999). This F-measure is based on the recall and precision figures reported in Figure 7.15 in Collins (1999). As the interest of the NLP community grows to encompass more languages, we observe efforts towards adapting an English parser for parsing other languages (e.g., (Collins et al, 1999)), or towards designing a language-independent framework based on principles underlying the models for parsing English (Bikel, 2002). The mechanism we employ for incorporating morphology into the PCFG model (the Model 1 parser in (Collins, 1999)) is the modification of its part-of speech (POS) tag set; in this paper, we explain how this mechanism allows the parser to better capture morphological constraints. The authors in (Collins et al, 1999) describe an approach that gives 80% accuracy in recovering unlabeled dependencies in Czech. Ourbaseline model, which we used to evaluate the effects of using morphology, was Model 1 (Collins, 1999) with a simple POS tag set containing almost no morphological information. It is also true of the adaptation of the Collins parser for Czech (Collins et al, 1999) and the finite-state dependency parser for Turkish by Oflazer (2003).
Joint Determination of Anaphoricity and Coreference Resolution using Integer Programming Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution. In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. joint ILP formulation provides score improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets. Denis and Baldridge (2007) report 67.1 F1 and 69.2 F1 on the English NWIRE and BNEWS respectively using true mention boundaries. It is also similar to the MLN-JOINT (BF) model of Song et al (2012), but we enforce the single parent constraint at a deeper structural level, allowing us to treat non-anaphoricity symmetrically with coreference as in Denis and Baldridge (2007) and Stoyanov and Eisner (2012). Denis and Baldridge (2007) adopted an Integer Linear Programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task.  Extending Denis and Baldridge (2007) and Finkel and Manning (2008)'s work, we exploit loose transitivity constraints on coreference pairs. Denis and Baldridge (2007) proposed an ILP formulation to find the optimal solution for the problem. It is worth noting, in particular, that Luo (2007), Denis and Baldridge (2007), and Finkel and Manning (2008) evaluate their approaches on true mentions extracted from the answer keys.    Ng and Cardie (2002a) and Denis and Baldridge (2007) show that when used effectively, explicitly predicting anaphoricity can be helpful. We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to bevery effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)). Examples of such approaches have exploited techniques including integer linear programming (ILP) (Denis and Baldridge, 2007a), label propagation (Zhou and Kong, 2009), and minimum cuts (Ng, 2009). We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection.  Section 2 briefly summarizes the approach proposed by Denis and Baldridge (2007). Denis and Baldridge (2007) defined the following object function for the joint anaphoricity and coreference determination problem. Non-referential it detection should not be a pre-processing step, but rather part of a globally optimal configuration, as was done for general noun phrase anaphoricity by Denis and Baldridge (2007). Ranking models provide a theoretically more adequate and empirically better alternative approach to pronoun resolution than standard classification based approaches (Denis and Baldridge, 2007b). By using joint inference for anaphoricity and coreference, Denis and Baldridge (2007a) avoid cascade-induced errors without the need to separately optimize the threshold.
Building A Large-Scale Annotated Chinese Corpus In this paper we address issues related to building a large-scale Chinese corpus. We try to answer four questions: (i) how to speed up annotation, (ii) how to maintain high annotation quality, (iii) for what purposes is the corpus applicable, and finally (iv) what future work we anticipate.   It is also binary feature and all the punctuations in the punctuation character list come from Penn Chinese Tree bank 5.1 (N.Xue et al,2002). At last, all the punctuations in Penn Chinese Treebank 5.1 (N.Xue et al,2002). reported accuracies of 93% and 93.74% on CTB-I (Xue et al, 2002) (100K words) and CTB 5.0 (500K words), respectively, each using a Maximum Entropy approach. Furthermore, to evaluate our reranking method's impact on the POS tagging task, we also performed 10-fold cross-validation tests on the 250k Penn 205Chinese Treebank (CTB) (Xue et al, 2002).  We use the Penn Chinese Treebank (Xue et al, 2002) as our corpus and the ontology/lexicon HowNet (Dong and Dong, 2000) to get ontological features for nouns. We follow the experimental setup in (Reichartand Rappoport, 2008), running the algorithm on English, German and Chinese corpora: the WSJ Penn Treebank (English), the Negra corpus (Brants, 1997) (German), and version 5.0 of the Chinese Penn Tree bank (Xue et al, 2002). In the Penn Chinese Treebank (CTB) (Xue et al, 2002) non-local dependencies are represented in terms of empty categories (ECs) and (for some of them) co indexation with antecedents, as exemplified in Figure 1. After analyzing the dependency structure of sentences in Penn Chinese Treebank 5.1 (Xue et al, 2002), we found an interesting phenomenon: if we define a main-root as the head of a sentence, and define a subsentence as a sequence of words separated by punctuations, and the head1 of these words is the child of main root or main-root itself, then the punctuations that depend on main-root can be a separator of sub-sentences. The bilingual data we use is the translated portion of the Penn Chinese Treebank (CTB) (Xue et al, 2002), corresponding to articles 1-325 of PTB, which have English translations with gold standard parse trees (Bies et al, 2007). Based on the Penn Chinese Treebank (CTB) (Xue et al, 2002) developed on the similar annotation scheme of PTB, these parsing techniques were also transferred to the Chinese language. While we did not have human-annotated gold standard parses for our training data, we did have human annotated parses for the Chinese side of our test data, which was taken from the Penn Chinese Treebank (Xue et al, 2002). We next tested U DOP on two additional domains from Chinese and German which were also used in Klein and Manning (2002, 2004): the Chinese tree bank (Xue et al 2002) and the NEGRA corpus (Skut et al 1997). We next tested UML-DOP on two additional domains which were also used in Klein and Manning (2004) and Bod (2006): the German NEGRA10 (Skut et al 1997) and the Chinese CTB10 (Xue et al 2002) both containing 2200+ sentences 10 words after removing punctuation. The closest result in the literature is Xue et al (2002), who retrain the Ratnaparkhi (1996) tagger and reach accuracies of 93% using CTB-I. However, our performance on tagging when trained on Training I and tested on just the XH part of the test set is 94.44%, which might be a more relevant comparison to Xue et al (2002). For English and Chinese we use sections 2-21 of the Penn Treebank (PTB) (Marcus et al,1993) and sections 1-270 of the Chinese Treebank (CTB) (Xue et al, 2002) respectively. We train on sections 1-270 of the Penn Chinese Treebank (Xue et al, 2002), similarly reduced (CTB10).
(Meta-) Evaluation of Machine Translation j schroeder ed ac uk Abstract This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back. We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process. We measured timing and intraand inter-annotator agreement for three types of subjective evaluation. We measured the correlation of automatic evaluation metrics with human judgments. This meta-evaluation reveals surprising facts about the most commonly used methodologies. (Callison-Burch et al, 2007) reported that the inter coder agreement on the task of assigning ranks to a given set of candidate hypotheses is much better than the intercoder agreement on the task of assigning a score to a hypothesis in isolation. While this single-scale relative ranking is perhaps faster to annotate and reaches a higher inter and intra-annotator agreement than the (absolute) fluency and adequacy (Callison-Burch et al, 2007), the technique and its evaluation are still far from satisfactory. The Moses system with a 4-gram language model and a distance-6 lexical reordering model ("lex RO") scores similarly to state-of-the-art systems of this type on the test 2007 French English data (Callison-Burch et al, 2007). This behavior appears to be consistent on the test 2007 and nc-test2007 data sets across systems (Callison-Burch et al, 2007). This evaluation was inspired by the sentence ranking evaluation in Callison-Burch et al (2007). The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008).  Callison-Burch et al (2007) show that ranking sentences gives higher inter-annotator agreement than scoring adequacy and fluency. In the WMT 2007 shared task evaluation campaign (Callison-Burch et al, 2007) domain adaptation was a special challenge. Although BLEU has played a crucial role in the progress of MT research, it is becoming evident that BLEU does not correlate with human judgement well enough, and suffers from several other deficiencies such as the lack of an intuitive interpretation of its scores. During the recent ACL-07 workshop on statistical MT (Callison-Burch et al, 2007), a total of automatic MT evaluation metrics were evaluated for correlation with human judgement. Finally, when evaluated on the datasets of the recent ACL 07 MT workshop (Callison-Burch et al, 2007). We gather the correlation results of these metrics from the workshop paper (Callison-Burch et al, 2007), and show in Table 1 the overall correlations of these metrics over the Europarl and News Commentary datasets. A complete description on WMT-07 evaluation campaign and dataset is available in Callison-Burch et al (2007). Different from Callison-Burch et al (2007), where Spear man's correlation coefficients were used, we use here Pearson's coefficients as, instead of focusing on ranking; this first evaluation exercise focuses on evaluating the significance and noisiness of the association, if any, between the automatic metrics and human-generated scores. See Callison-Burch et al (2007) for details on the human evaluation task.  Human evaluation is also often quantitative, for instance in the form of estimates of values such as adequacy and fluency, or by ranking sentences from different systems (e.g. Callison-Burch et al (2007)). Though it does, at least in principle, seem possible to mine HTER annotations for more information system comparison (Callison-Burch et al, 2007), and word alignment (Ahrenberg et al, 2003). The method of manually scoring the 11 submitted Chinese system translations of each segment is the same as that used in (Callison-Burch et al, 2007). A new human evaluation measure has been proposed to roughly estimate the productivity increase when using each of the systems in a real scenario, grounded on previous works for human evaluation of qualitative factors (Callison-Burch et al, 2007).
Minimizing Manual Annotation Cost In Supervised Training From Corpora Corpus-based methods for natural language processing often use supervised training, requiring expensive manual annotation of training corpora. This paper investigates methods for reducing annotacost by selection. this approach, during training the learning program examines many unlabeled examples and selects for labeling (annotation) only those that are most informative at each stage. This avoids redundantly annotating examples that contribute little new information. This paper extends our previous on sample selection for probabilistic classifiers. We describe a family of methods for committee-based sample selection, and report experimental results for the task of stochastic part-ofspeech tagging. We find that all variants achieve a significant reduction in annotation cost, though their computational efficiency differs. In particular, the simplest method, which has no parameters to tune, gives excellent results. We also show that sample selection yields a significant reduction in the size of the model used by the tagger.  Active learning has been successfully applied to a number of natural language oriented tasks, including text categorization (Lewis and Gale, 1994) and part-of-speech tagging (Engelson and Dagan, 1996). Engelson and Dagan (1996) experimented with QBC using HMMs for POS tagging and found that selective sampling of sentences can significantly reduce the number of samples required to achieve desirable tag accuracies. In all experiments, the agreement among the decision committee members is quantified by the Vote Entropy measure (Engelson and Dagan, 1996). Active learning also has been applied to many NLP applications, including POS tagging (Engelson and Dagan, 1996) and parsing (Baldridge and Osborne, 2003). This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s). A common metric to estimate the disagreement within an ensemble is the so-called vote entropy, the entropy of the distribution of labels li assigned to an example e by the ensemble of k classifiers (Engelson and Dagan, 1996). AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). Engelson and Dagan (1996) investigated several plausible approaches to the selection function but were unable to find significant differences among them. QBC is based on the idea to select those examples for manual annotation on which a committee of classifiers disagree most in their predictions (Engelson and Dagan, 1996). This is measured by the vote entropy (Engelson and Dagan, 1996), i.e., the entropy of the distribution of classifications assigned to an example by the classifiers. Engelson and Dagan (1996) confirm this observation that, in general, different (and even more refined) selection methods still yield similar results. 
A Clustering Approach For Nearly Unsupervised Recognition Of Nonliteral Language In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques. TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies. It also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning. We adapt a word-sense disambiguation algorithm to our task and augment it with multiple seed set learners, a voting schema, and additional features like SuperTags and extrasentential context. Detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4%. Using the TroFi algorithm, we also build the TroFi Example Base, an extensible resource of annotated literal/nonliteral examples which is freely available to the NLP research community. Such techniques either do not use any information regarding the linguistic properties of MWEs (Birke and Sarkar, 2006), or mainly focus on their noncompositionality (Katz and Giesbrecht, 2006). The idiomatic/literal token classification methods of Birke and Sarkar (2006) and Katz and Giesbrecht (2006) rely primarily on the local context of a token, and fail to exploit specific linguistic properties of non-literal language. Past work on the problem of distinguishing literal and metaphorical senses has approached it as a classical word sense disambiguation (WSD) task (Birke and Sarkar, 2006). A subset of twenty-five of the fifty verbs was used by Birke and Sarkar (2006). In our second experiment, we duplicate the setup of Birke and Sarkar (2006) so that we can compare our results with theirs. In the third experiment, we train the algorithm on the twenty-five new verbs that were not used by Birke and Sarkar (2006) and then we test it on the old verbs. Birke and Sarkar (2006) explain their scoring as follows: Literal recall is defined as (correct literals in literal cluster/ total correct literals); Literal precision is defined as (correct literals in literal cluster/ size of literal cluster). Birke-Sarkar refers to the best result reported by Birke and Sarkar (2006), using a form of active learning. NA indicates scores that were not calculated by Birke and Sarkar (2006). Instead of ten-fold cross-validation, we used the twenty-five verbs in Birke and Sarkar (2006) for testing (we call these the old verbs) and the other twenty-five verbs (the new verbs) for training.  Birke and Sarkar (2006) automatically constructed a corpus of English idiomatic expressions (words that can be used non-literally). Birke and Sarkar (2006) also used WSD. Birke and Sarkar (2006) requires WordNet. Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two automatically constructed seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set. Birke and Sarkar (2006) present a sentence clustering approach for non-literal language recognition implemented in the TroFi system (Trope Finder). Birke and Sarkar (2006) adapt this algorithm to perform a two-way classification: literal vs. non-literal, and they do not clearly define the kinds of tropes they aim to discover. Both Birke and Sarkar (2006) and Gedigan et al. (2006) focus only on metaphors expressed by a verb. Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two seed sets (one with literal and one with non-literal expressions), as signing the label of the closest set. Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one non literal), assigning the label of the closest set.
Exploiting Diversity In Natural Language Processing: Combining Parsers Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy. Two general approaches are presented and two combination techniques are described for each approach. Both parametric and non-parametric models are explored. The resulting parsers surpass the best previously published performance results for the Penn Treebank. Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy. Given a novel sentence Stest E Ctest, combine the collection of hypotheses ti = fi(Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999).  (Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers. A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999). This approach roughly corresponds to (Henderson and Brill, 1999)'s Naive Bayes parse hybridization. Henderson and Brill (1999) also reported that context did not help them to outperform simple voting. (Henderson and Brill, 1999) improved their best parser's F-measure of 89.7 to 91.3, using their naive Bayes voting on the Penn TreeBank constituent structures (16% error reduction).  Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000). Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees. Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper. Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006). Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees. (Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined. (Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents. Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework. System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999)). In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined.
Models For The Semantic Classification Of Noun Phrases Roles. In 28(3). Relation no. 1 2 3 6 7 11 13 15 16 21 25 the rest 0.06103 0.11268 0.00939 0.04225 0.39437 0.01878 0.03286 0.25822 0.04694 0.01878 0.00469 0 Table 5: Sample row from the conditional probability table where the feature pair is entity-entity. The numbers in the top row identify the semantic relations (as in Table 4). Level Level 1 Level 2 Level 3 Level 4 Number of 9 52 70 122 features Number 9 46 47 47 features No. of feature pairs 57 out of 81 189 out of 2392 204 out of 3290 250 out of 5734 Number of 1 152 181 225 with only one relation Average number 2.7692 1.291 1.1765 1.144 non-zero relations per line Table 6: Statistics for the semantic class features by level of specialization. The annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation as defined in (Moldovan et al 2004). It also provides a mapping from the FrameNet deep semantic roles to general thematic roles (list defined in (Moldovan et al 2004)), and use cases for VerbNet. The list of 35 semantic relations was presented in (Moldovan et al 2004). We have compared the performance of SVM with three other learning algorithms: (1) semantic scattering (Moldovan et al 2004), (2) decision trees (a C4.5 implementation), and (3) Naive Bayes. We considered as baseline semantic scattering which is a new learning model (Moldovan et al 2004) developed in-house for the semantic classification of noun noun pairs in NP constructions. Recent work on the automatic/semi automatic interpretation of NCs (e.g., Lapata (2002), Rosario and Marti (2001), Moldovan et al (2004) and Kim and Baldwin (2005)) has made assumptions about the scope of semantic relations or restricted the domain of interpretation. Moldovan et al (2004) used the word senses of nouns based on the do main or range of interpretation of an NC, leading to questions of scalability and portability to noveldomains/NC types. Rosario and Marti (2001) achieved about 60% using a neural net work in a closed domain, Moldovan et al (2004) achieved 43% using word sense disambiguation of the head noun and modifier over open domain data, and Kim and Baldwin (2005) produced 53% using lexical similarities of the head noun and modifier (using the same relation set, but evaluated over a different dataset). In this study, we selected three semantic similarity based models which had been found to perform strongly in previous research, and which were easy to re-implement: SENSE COLLOCATION (Moldovan et al, 2004), CONSTITUENT SIMILARITY (Kim and Baldwin, 2005) and CO-TRAINING ,e.g. using Sense COLLOCATION or CONSTITUENT SIMILAR ITY (Kim and Baldwin, 2007). We tested the original methods of Moldovan et al (2004) and Kim and Baldwin (2005), and combined them with the co-training methods of Kim and Baldwin (2007) to come up with six different hybrid systems for evaluation, as detailed in Table 1. Moldovan et al (2004) propose a 35 class scheme to classify relations in various phrases; the same scheme has been applied to noun com pounds and other noun phrases (Girju et al, 2005). Moldovan et al (2004) also use WordNet. The first method uses sense collocations as proposed by Moldovan et al (2004), and the second method uses the lexical similarity of the component words in the NC as proposed by Kim and Baldwin (2005). Moldovan et al (2004) proposed a method called semantic scattering for interpreting NCs. A wide variety of features are used by different algorithms, ranging from simple bag-of-words frequencies to WordNet-based features (Moldovan et al., 2004). Moldovan et al (2004) proposed a different scheme with 35 classes. Moldovan et al (2004) also use WordNet. Moldovan et al (2004) use SVMs as well as a novel algorithm (i.e., semantic scattering). From these, 80% were used for training and 20% for test ing. Each genitive instance was tagged with the corresponding semantic relations by two annotators, based on a list of 35 most frequently used semantic relations proposed by (Moldovan et al, 2004) and shown in Table 1. For this study, we adopt a revised version of these mantic relation set proposed by (Moldovan et al, 2004).
A Syntactic Approach To Discourse Semantics A correct structural analysis of a discourse is a prerequisite for understanding it. This paper sketches the outline of a discourse grammar which acknowledges several different levels of structure. This grammar, the &quot;Dynamic Discourse Model&quot;, uses an Augmented Transition Network parsing mechanism to build a representation of the semantics of a discourse in a stepwise fashion, from left to right, on the basis of the semantic representations of the individual clauses which constitute the discourse. The intermediate states of the parser model the intermediate states of the social situation which generates the discourse. The paper attempts to demonstrate that a discourse may indeed be viewed as constructed by means of sequencing and recursive nesting of discourse constituents. It gives rather detailed examples of discourse structures at various levels, and shows how these structures are described in the framework proposed here. STRUCTURES AT DIFFERENT LEVELS If a discourse understanding system is to be able to assemble the meaning of a complex discourse fragment (such as a story or an elaborate description) out of the meanings of the utterances constituting the fragment, it needs a correct structural analysis of it. Such an analysis is also necessary to assign a correct semantic interpretation to clauses as they occur in the discourse; this is seen most easily in cases where this interpretation depends on phenomena such as the discourse scope of temporal and locative adverbials, the movement of the reference time in a narrative, or the interpretation of discourse anaphora. The Dynamic Discourse Model, outlined in this paper, is a discourse grammar under development which analyses the structure of a discourse in order to be able to deal adequately with its semantic aspects. It should be emphasized at the outset that this system is a formal model of discourse syntax and semantics, but not a computer implementation of such a model. For a system to be able to understand a discourse, it must be able to analyse it at several different levels. 1. Any piece of talk must be assigned to one Interaction-i.e., to a socially constructed verbal exchange which has, at any moment, awell-defined set of participants. 2. Virtually every interaction is viewed by its as belonging to a particular predefined genre -be it a doctor-patient interaction, a religious ceremony, or a casual chat. Depending on the genre, certain participants may have specific roles in the verbal exchange, and there may be a predefinedagenda specifying consecutive parts of the interaction. An interaction which is socially &quot;inin such a fashion is called a Event(Hymes,1967,1972). 3. A stretch of talk within one Speech Event may be as dealing with one Topic. 4. Within a Topic, we may find one or more Dis- Units(DU's) -socially acknowledged units of talk which have a recognizable &quot;point&quot; or purpose, while at the same time displaying a specific syntactic/semantic structure. Clear examples are stories, procedures, descriptions, and jokes. 5. When consecutive clauses are combined into one syntactic/semantic unit, we call this unit a constituent unit(dcu). Examples are: lists, narrative structures, and various binary structures (&quot;A but B&quot;, &quot;A because B&quot;, etc.). Adjacency Structuresmay well be viewed as a kind of dcu, but they deserve special mention. They are two or three part conversational routines involving speaker change. The clearest examples are question-answer pairs and exchanges of greetings. 7. The smallest units which we shall deal with at discourse level are clausesand operators. Operators include &quot;connectors&quot; like &quot;and&quot;, &quot;or&quot;, &quot;because&quot;, as well as &quot;discourse markers&quot; like &quot;well&quot;, &quot;so&quot;, &quot;incidentally&quot;. The levels of discourse structure just discussed are hierarchically ordered. For instance, any DU must be part of a Speech Event, while it must be built up out of dcu's. The levels may thus be viewed as an expansion of the familiar linguistic hierarchy of phoneme, morpheme, word and clause. This does not mean, however, that every discourse is to be analysed in terms of a five level tree structure, with levels corresponding to dcu, DU, Topic, Speech Event and Interaction. To be able to describe discourse as it actually occurs, discourse constituents of various types must be allowed to be embedded in constituents of the same and other types. We shall see various examples of this in later sections. It is worth emphasizing here already that &quot;high level constituents&quot; may be embedded in &quot;low level constituents&quot;. For instance, a dcu may be interrupted by a clause which initiates another Interaction. Thus, a structural description of the unfolding discourse would include an Interaction as embedded in the dcu. In 413 this way, we can describe &quot;intrusions&quot;, &quot;asides to third parties&quot;, and other interruptions of one Interaction by another. In the description of discourse semantics, the level of the dcu's (including the adjacency structures) plays the most central role: at this level the system defines how the semantic representation of a complex discourse constituent is constructed out of the semantic representations of its parts. The other levels of structure are also of some relevance, however: - The Discourse Unit establishes higher level semantic coherence. For instance, the semantics of different episodes of one story are integrated at this level. - The Topic provides a frame which determines the interpretation of many lexical items and descriptions. - The Speech Event provides a script which describes the conventional development of the discourse, and justifies assumptions about the purposesofdiscourse participants. - The Interaction specifies referents for indexicals like &quot;I&quot;, &quot;you&quot;, &quot;here&quot;, &quot;now&quot;. II THE DYNAMIC DISCOURSE MODEL Dealina with linguistic structures above the clause level is an enterprise which differs in an essential way from the more common variant of linguistic activity which tries to describe the internal structure of the verbal symbols people exchange. Discourse linguistics does not study static verbal objects, but must be involved with the social process which produces the discourse -with the ways in which the discourse participants manipulate the obligations and possibilities of the discourse situation, and with the ways in which their talk is constrained and framed by the structure of this discourse situation which they themselves created. The structure one may assign to the text of a discourse is but a reflection of the structure of the process which produced it. Because of this, the Dynamic Discourse Model that we are developing is only indirectly involved in trying to account for the a posteriori structure of a finished discourse; instead, it tries to trace the relevant states of the social space in terms of which the discourse is constructed. This capability is obviously of crucial importance if the model is to be applied in the construction of computer systems which can enter into actual dialogs. The Dynamic Discourse Model, therefore, must construct the semantic interpretation of a discourse on a clause by clause basis, from left to right, yielding intermediate semantic representations of unfinished constituents, as well as setting the semantic parameters whose values influence the interpretation of subsequent constituents. A syntactic/semantic system of this sort may very well be fromulated as an Augmented Transition Network grammar (Woods, 1970), a non-deterministic parsing system specified by a set of transition networks which may call each other recursively. Every Speech Event type, DU type and dcu type is associated with a transition network specifying its internal structure. As a transition network processes the consecutive constituents of a discourse segment, it builds up, step by step, a representation of the meaning of the segment. This representation is stored in a register associated with the network. At any stage of the Process, this register contains a representation of the meaning of the discourse segment so far. An ATN parser of this sort models important aspects of the discourse process. After each clause, the system is in a well-defined state, characterized by the stack of active transition networks and, for each of them, the values in its registers and the place where it was interrupted. When we say that discourse participants know &quot;where they are&quot; in a complicated discourse, we mean that they know which discourse constituent is being initiated or continued, as well as which discourse constituents have been interrupted where and in what order -in other words, they are aware of the embedding structure and other information captured by the ATN configuration. The meaning of most clause utterances cannot be determined on the basis of the clause alone, but involves register values of the embedding dcu -as when a question sets up a frame in terms of which its answer is interpreted (cf. Scha, 1983) or when, to determine the temporal reference of a clause in a narrative, one needs a &quot;reference time&quot; which is established by the foregoing part of the narrative (section III B 2). From such examples, we see that the discourse constituent unit serves as a framework for the semantic interpretation of the clauses which constitute the text. By the same token, we see that the semantics of an utterance is not exhaustively described by indicating its illocutionary force and its propositional content. An utterance may also cause an update in one or more semantic registers of the dcu, and thereby influence the semantic interpretation of the following utterances. This phenomenon also gives us a useful perspective on the notion of interruption which was mentioned before. For instance, we can now see the difference between the case of a story being interrupted by a discussion, and the superficially similar case of a story followed by a discussion which is, in its turn, followed by another story. In the first case, the same dcu is resumed and all its register values are still available; in the second case, the first story has been finished before the discussion and the re-entry into a storyworld is via a different story. The first story has been closed off and its register values are no longer avilable for re-activation; the teller of the second story must re-initialize the variables of time, place and character, even if the events of the second story concern exactly the same characters and situations as the first. Thus, the notions of interruption and resumption have not only a social reality which is experienced by the interactants involved. They also have semantic consequences for the building and interpretation of texts. Interruption and resumption are often explicitly signalled by the occurrence of &quot;discourse markers&quot;. Interruption is signalled by a PUSHmarker such as &quot;incidentally&quot;, &quot;by the way&quot;, &quot;you or &quot;like&quot;. Resumption is signalled by a POP- 414 -markers such as &quot;O.K.&quot;, &quot;well&quot;, &quot;so&quot; or &quot;anyway&quot;. (For longer lists of discourse marking devices, and somewhat more discussion of their functioning, see Reichman (1981) and Polanyi and Scha(1983b).) In terms of our ATN description of discourse structure, the PUSHand POP-markers do almost exactly what their names suggest. A PUSH-marker signals the creation of a new embedded discourse constituent, while a POP-marker signals a return to an embedding constituent (though not necessarily the immediately embedding one), closing off the current constituent and all the intermediate ones. The fact that one POP-marker may thus create a whole cascade of discourse-POPs was one of Reichman's (1981) arguments for rejecting the ATN model of discourse structure. We have indicated before, however, that accommodating this phenomenon is at worst a matter of minor technical extensions of the ATNformalism (Polanyi and Scha, 1983b); in the present paper, we shall from now on ignore it. The discourse grammar used builds on [Polanyi and Scha 1984]. Such structures have received considerable attention and their models are often referred to as discourse/dialogue grammars (Polanyi and Scha, 1984) or conversational/dialogue games (Levin and Moore, 1988). It is also closer to some methods for incremental daptation of discourse structures, where additions are allowed to the right-frontier of a tree structure (e.g. Polanyi and Scha 1984).
Bidirectional Inference With The Easiest-First Strategy For Tagging Sequence Data This paper presents a bidirectional inference algorithm for sequence labeling problems such as part-of-speech tag ging, named entity recognition and text chunking. The algorithm can enumerate all possible decomposition structures andfind the highest probability sequence together with the corresponding decomposi tion structure in polynomial time. We also present an efficient decoding algorithm based on the easiest-first strategy, which gives comparably good performance tofull bidirectional inference with significantly lower computational cost. Exper imental results of part-of-speech tagging and text chunking show that the proposedbidirectional inference methods consis tently outperform unidirectional inference methods and bidirectional MEMMs give comparable performance to that achievedby state-of-the-art learning algorithms in cluding kernel support vector machines. In order to produce POS-based surprisal estimates, versions of both the training and experimental texts with their words replaced by POS were developed: The BNC sentences were parsed by the Stanford Parser, version 1.6.7 (Klein and Manning, 2003), whilst the experimental texts were tagged by an automatic tagger (Tsuruoka and Tsujii, 2005), with posterior review and correction by hand following the Penn Treebank Project Guidelines (Santorini, 1991). SSP performs POS tagging using an off-the-shelf tagger (Tsuruoka and Tsujii, 2005). Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al 2003) and (Tsuruoka and Tsujii, 2005). Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding.
Fast Decoding And Optimal Decoding For Machine Translation A good decoding algorithm is critical to the success of any statistical machine translation system. The decoder’s job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them). Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions. In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem. See (Brown et al, 1993) or (Germann et al, 2001) for a detailed discussion of this translation model and a description of its parameters. The decoding algorithm that we use is a greedy one - see (Germann et al, 2001) for details. As discussed by Germann et al (2001), the word-for-word gloss is constructed by aligning each French word fL with its most likely English translation efk. Similarly to the work by Germann et al (2001), their decoder is deterministic and explores the entire neighbourhood of a state in order to identify the most promising step. A similar problem also occurs in an ILP formulation for machine translation which treats decoding as the Travelling Salesman Problem (Germann et al, 2001). For instance, Germann et al (2001) present an ILP formulation of the Machine Translation (MT) decoding task in order to conduct exact inference. This feature distinguishes our task from other decoding problems, such as decoding in machine translation (Germann et al, 2001), that are modeled using a standard TSP formulation. We also included in the figure the performance of an IBM Model 4 word based translation system (M4), which uses a greedy decoder [Germann et al, 2001]. (Germann et al, 2001) presents a greedy approach to search for the translation that is most likely according to previously learned statitistical models. The system uses models GIZA++ and ISI ReWrite decoder (Germann et al., 2001). As a point of comparison, we also trained an IBM-4 translation model with the GIZA++ toolkit (Och and Ney, 2000), using the combined bi-phrase building and training sets, and translated the test set using the ReWrite decoder (Germann et al, 2001). Och et al (2001) and Germann et al (2001) both implemented optimal decoders and benchmarked approximative algorithms against them. Germann et al (2001) compare translations obtained by a multi-stack decoder and a greedy hill-climbing algorithm against those produced by an optimal integer programming decoder that treats decoding as a variant of the traveling-salesman problem. While acceptably fast for the kind of evaluation used in Germann et al (2001), namely sentences of up to 20 words, its speed becomes an issue for more realistic applications. In this subsection we recapitulate the greedy hill climbing algorithm presented in Germann et al (2001). Thirdly, the results of our experiments with randomized searches show that greedy decoding does not perform as well on longer sentences as one might conclude from the findings in Germann et al (2001). In this paper, we have analyzed the complexity of the greedy decoding algorithm originally presented in Germann et al (2001) and presented improvements that drastically reduce the decoder's complexity and speed to practically linear time. Germann et al (2001) suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the real-world application.  The list of zero fertility words can be obtained from the viterbi alignment of training corpus (Germann et al, 2001).
An Empirical Study of Semi-supervised Structured Conditional Models for Dependency Parsing This paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning approach. We describe an extension of semisupervised structured conditional models (SS-SCMs) to the dependency parsing problem, whose framework is originally proposed in (Suzuki and Isozaki, 2008). Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with another semi-supervised approach, described in (Koo et al., 2008). The second extension is to apply the approach to secondorder parsing models, such as those described in (Carreras, 2007), using a twostage semi-supervised learning approach. We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections: the Penn Treebank for English, and the Prague Dependency Treebank for Czech. Our best results on test data in the above datasets achieve 93.79% parent-prediction accuracy for En Finally, Suzuki et al (2009) present a very effective semi-supervised approach in which features from multiple generative models estimated on unlabeled data are combined in a discriminative system for structured prediction.  Both Suzuki et al (2009) and Chen et al (2013) adopt the higher order parsing model of Carreras (2007), and Suzuki et al (2009) also incorporate word cluster features proposed by Koo et al (2008) in their system. Also, some work has incorporated unsupervised word clusters as features, including that of Koo et al (2008) and Suzuki et al (2009), who utilized unsupervised word clusters created using the Brown et al (1992) hierarchical clustering algorithm. Although large amounts of unlabeled data are known to improve semi-supervised parsing (Suzuki et al, 2009), the best unsupervised systems use less data than is available for supervised training, relying on complex models instead: Headden et al's (2009) Extended Valence Grammar (EVG) combats data sparsity with smoothing alone, training on the same small subset of the tree-bank as the classic implementation of the DMV; Cohen and Smith (2009) use more complicated algorithms (variational EM and MBRdecoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data). We prepared a total of 3.72 billion token text data as unsupervised data following the instructions given in (Suzuki et al, 2009).     Semi-supervised models such as Ando and Zhang (2005), Suzuki and Isozaki (2008), and Suzuki et al (2009) achieve state-of-the-art accuracy.  Our system also compares favourably with the system of Carreras et al (2008) that relies on a more complex generative model, namely Tree Adjoining Grammars, and the system of Suzuki et al (2009) that makes use of external data (unannotated text).  Suzuki 2009 (Suzuki et al, 2009) reported the best reported result by combining a Semi supervised Structured Conditional Model (Suzuki and Isozaki, 2008) with the method of (Koo et al, 2008). Suzuki et al (2009) presented a semi supervised learning approach.  Suzuki et al (2009) also experiment with the same method combined with semi-supervised learning. 
Learning To Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment We address the text-to-text generation problem of sentence-level paraphrasing — a phenomenon distinct from and more difficult than wordor phrase-level paraphrasing. Our apapplies alignment sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patrepresented by lattice and automatically determines how to apply these patterns to rewrite new sentences. The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems. It's still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)). MSA is commonly used in bioinformatics to identify equivalent fragments of DNAs (Durbin et al, 1998). Following Barzilay and Lee (2003), we approach the sentence clustering task by hierarchical complete-link clustering with a similarity metric based on word n-gram overlap (n= 1, 2, 3). We adopt the scoring function for MSA from Barzilay and Lee (2003). See Barzilay and Lee (2003) for a detailed discussion about the choice of 50% according to pigeonhole principle. Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. This implies that our exact algorithm could be also used to find exact multi sequence alignments, an important problem in natural language processing (Barzilay and Lee, 2003) and computational biology (Durbin et al, 2006) that is almost always solved with approximate methods. Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003). In its ability to learn paraphrases using Multiple Sequence Alignment, our system is related to Barzilay and Lee (2003). Barzilay and Lee (2003) construct lattices over paraphrases using an iterative pairwise multiple sequence alignment (MSA) algorithm. Similar to the work of Barzilay and Lee (2003), who have applied paraphrase generation techniques to comparable corpora consisting of different newspaper articles about the same event. Barzilay and Lee (2003) proposed a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations. The latter study applied several MT metrics to paraphrase data from Barzilay and Lee's corpus-based system (Barzilay and Lee, 2003), and found moderate correlations with human adequacy judgments, but little correlation with fluency judgments. Barzilay and Lee (2003) present an approach for generating sentence level paraphrases, learning structurally similar patterns of expression from data and identifying paraphrasing pairs among them using a comparable corpus. While word and phrasal paraphrases can be assimilated to the well-studied notion of synonymy, sentence level paraphrasing is more difficult to grasp and can not be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). There exist many different string similarity measures: word overlap (Tomuro and Lytinen, 2004), longest common subsequence (Islam and Inkpen, 2007), Levenshtein edit distance (Dolan et al, 2004), word n-gram overlap (Barzilay and Lee, 2003) etc. Another direction is to build on methods to extract paraphrases from comparable corpora (Barzilay and Lee, 2003), and extend them to capture asymmetrical pairs, where entailment holds in one, but not the other, direction. Previous work aligns a group of sentences into a compact word lattice (Barzilay and Lee, 2003), a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases. Our work is closest in spirit to the two papers that inspired us (Barzilay and Lee, 2003). Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 (Barzilay and Lee, 2003) and 0.55 0.63 (Szpektor et al, 2004).
Word Sense Disambiguation With Very Large Neural Networks Extracted From Machine Readable Dictionaries In this paper, we describe a means for automatically building very large neural networks (VLNNs) from definition texts in machine-readable dictionaries, and demonslrate he use of these networks for word sense disambiguation. Our method brings together two earlier, independent approaches to word sense disambiguation: the use of machine-readable dictionaries and spreading and activation models. The automatic construction of VLNNs enables real-size xperiments with neural networks for natural language processing, which in turn provides insight into their behavior and design and can lead to possible improvements.  Another related line of work is the word sense disambiguation algorithm proposed in (Veronis and Ide, 1990), where a large neural network is built by relating words through their dictionary definitions. Current set of semantic relation types m MindNet These relation types may be contrasted with simple co-occurrence statistics used to create network structures from dictionaries by researchers including Veronis and Ide (1990), Kozima and Furugori (1993), and Wilks et al (1996). This deficiency limits the characterization of word pairs such as river bank (Wilks et al 1996) and write pen (Veronis and Ide 1990) to simple relatedness, whereas the labeled relations of MindNet specify precisely the relations river --Part -& gt; bank and write --Means --& gt; pen. Inverted semrel structure from a definition of motorist Researchers who produced spreading activation networks from MRDs, including Veronis and Ide (1990) and Kozima and Furugori (1993), typically only implemented forward links (from headwords to their definition words) in those networks. Since the model estimation has been reduced to simple update calculations, the proposed model is similar to conventional spreading activation approaches, which have been applied, for example, to word sense disambiguation (Veronis and Ide, 1990).
Non-Projective Dependency Parsing Using Spanning Tree Algorithms We formalize weighted dependency pars ing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithmof Eisner (1996) is sufficient for search ing over all projective trees in O(n3) time. More surprisingly, the representation isextended naturally to non-projective pars ing using Chu-Liu-Edmonds (Chu andLiu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing al gorithm. We evaluate these methodson the Prague Dependency Treebank using online large-margin learning tech niques (Crammer et al, 2003; McDonald et al, 2005) and show that MST parsingincreases efficiency and accuracy for lan guages with non-projective dependencies. We use three base models for dependency parsing: MST parser (McDonald et al 2005), Maltparser (Nivre et al 2006), and the ensemble parser of Surdeanu and Manning (2010).    A detailed description of the Chu Liu Edmonds algorithm for MSTs is available in McDonald et al (2005). We test these two techniques on English-Czech MT outputs using our own reimplementation of the MST parser (McDonald et al, 2005) named RUR1 parser. We have reimplemented the MST parser (McDonald et al, 2005) in order to provide for a simple insertion of the parallel features into the models. The set of monolingual features used in RUR parser follows those described by McDonald et al (2005). The first parser is a non-lexicalized version of the MST parser (McDonald et al 2005) successfully used in the multilingual context (McDonald et al 2011).   See Georgiadis (2003) for a detailed algorithmic proof, and McDonald et al (2005) for an illustrative example. They introduce maximum spanning tree (MST) parsing (McDonald et al, 2005) into phrase-based translation. For domain adaptation, we show an error reduction of up to 7.7% when adapting the second-order projective MST parser (McDonald et al 2005) from newswire to the QuestionBank domain. For dependency parsing we utilize the second-order projective MST parser (McDonald et al 2005) with the gold-standard POS tags of the corpus. Base is the second-order, projective dependency parser of McDonald et al (2005). We consider two different approaches to learning a temporal dependency parser: a shift-reduce model (Nivre, 2008) and a graph-based model (McDonald et al, 2005). Dependency tree parsing as the search for the maximum spanning tree in a directed graph was proposed by McDonald et al (2005c). The formulation works by defining in McDonald et al (2005a). The standard approach to framing dependency parsing as an integer linear program was introduced by (Riedel and Clarke, 2006), who converted the MST parser of (McDonald et al 2005) to use ILP for inference. The key idea is to build a complete graph consisting of tokens of the sentence where each edge is weighted by a learned scoring function.
The Web As A Baseline: Evaluating The Performance Of Unsupervised Web-Based Models For A Range Of NLP Tasks Previous work demonstrated that web counts can be used to approximate bigram frequencies, and thus should be useful for a wide variety of NLP tasks. So far, only two generation tasks (candidate selection for machine translation and confusion-set disambiguation) have been tested using web-scale data sets. The present paper investigates if these results generalize to tasks covering both syntax and semantics, both generation and analysis, and a larger of For the majority of tasks, we find that simple, unsupervised models perform when frequencies are obtained from the web rather than from a large corpus. However, in most cases, web-based models fail to outperform more sophisticated state-of-theart models trained on small corpora. We argue that web-based models should therefore be used as a baseline for, rather than an alternative to, standard models. Next, it looks promising to try to estimate the dictionary word frequencies using a search engine instead of text corpus, as proposed by Lapata and Keller (2004). Lapata and Keller (2004) uses the number of page hits as the web-count of the queried n gram (which is problematic according to Kilgarriff (2007)). While it is possible to exploit search engine queries for various NLP tasks (Lapata and Keller, 2004), for applications which use corpora as unsupervised training material downloadable base data is essential. This approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time ameliorates the effects of data sparseness inherent in any lexicalised DLA approach (Lapata and Keller, 2004). Lapata and Keller (2004) first used web-based co-occurrence counts for the bracketing of NCs. Aside from counting bigrams, various tasks are attainable using web based models: spelling correction, adjective ordering, compound noun bracketing, countability detection, and so on (Lapata and Keller, 2004). Therefore, it can be used easily as a baseline, as suggested by (Lapata and Keller, 2004). The results are compared against two state of the art approaches: a supervised machine learning model, Semantic Scattering (Moldovan and Badulescu, 2005), and a web based probabilistic model (Lapata and Keller, 2004). More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus. We have experimented with the support vector machines (SVM) model and compared the results against two state-of-the-art models: a supervised model, Semantic Scattering (SS), (Moldovan and Badulescu, 2005), and a web-based unsupervised model (Lapata and Keller, 2004). (Lapata and Keller, 2004)'s web-based unsupervised model classifies noun noun instances based on Lauer's list of 8 prepositions and uses the web as training corpus. Although (Lapata and Keller, 2004) used Altavista in their experiments, they showed there is almost no difference between the correlations achieved using Google and Altavista counts. They then later propose using Web counts as a baseline unsupervised method for many NLP tasks (Lapata and Keller, 2004). Lapata and Keller (2004) achieved their best accuracy (78.68%) with the dependency model and the simple symmetric score #(wi ,wj). This is confirmed by the adjacency model experiments in (Lapata and Keller, 2004) on Lauer's NC set. Lapata and Keller (2004) derived their statistics from the Web and achieved results close to Lauer's using simple lexical models. Table 3 compares our results to those of Lauer (1995) and of Lapata and Keller (2004). We have extended and improved upon the state-of-the-art approaches to NC bracketing using an unsupervised method that is more robust than Lauer (1995) and more accurate than Lapata and Keller (2004).  The vast size of the Web has been demonstrated to combat the data sparseness problem, for example, in Lapata and Keller (2004).
A Simple Pattern-Matching Algorithm For Recovering Empty Nodes And Their Antecedents This paper describes a simple patternmatching algorithm for recovering empty nodes and identifying their co-indexed antecedents in phrase structure trees that do not contain this information. The patterns are minimal connected tree fragments containing an empty node and all other nodes co-indexed with it. This paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure, which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus. Evaluating the algorithm on the output of Charniak’s parser (Charniak, 2000) and the Penn treebank (Marcus et al., 1993) shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity. Johnson (2002) proposes an algorithm that is able to find long-distance dependencies, as a post processing step, after parsing. As our interest lies in trace detection and antecedent recovery, we adopt the evaluation measures introduced by Johnson (2002).  In this section, we validate the two-step approach, by applying the parser to the output of the trace tagger, and comparing the antecedent recovery accuracy to Johnson (2002).  Comparing our results to Johnson (2002), we find that the NOINSERT model outperforms that of Johnson by 4.6% (see Table 7). Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head - dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). Finally, it is not clear that their numbers are in fact comparable to those of Dienes and Dubey on parsed data because the metrics used are not quite equivalent, particularly for (NP*) s: among other differences, unlike Jijkoun and de Rijke's metric (taken from (Johnson, 2002)), Dienes and Dubey's is sensitive to the string extent of the antecedent node, penalizing them if the parser makes attachment errors involving the antecedent even if the system recovered the long-distance dependency itself correctly. Johnson (2002) used corpus-induced patterns to insert gaps into both gold standard trees and parser output. We compare our algorithm under a variety of conditions to the work of (Johnson, 2002) and (Gabbard et al, 2006). The first metric, which was introduced by Johnson (2002), has been widely reported by researchers investigating gap insertion. Correct dependency recovery for object extraction is also difficult for shallow methods such as Johnson (2002) and Dienes and Dubey (2003). While Charniak's parser does not generate empty category information, Johnson (2002) has developed an algorithm that extracts patterns from the Treebank which can be used to insert empty categories into the parser's output. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. This approach contrasts with Johnson (2002), who treats empty/antecedent identification as a joint task, and with Dienes and Dubey (2003a, b), who always identify empties first and determine antecedents later. in an abstract sense it mediates the gap-threading information incorporated into GPSG-style (Gazdar et al., 1985) parsers, and in concrete terms it closely matches the information derived from Johnson (2002)'s connected local tree set patterns. Our algorithm's performance can be compared with the work of Johnson (2002) and Dienes and Dubey (2003a) on WSJ. For purposes of comparability with Johnson (2002) we used Charniak's 2000 parser as P. P is parser, G is string-to-context-free-gold-tree mapping, A is present remapping algorithm, J is Johnson 2002, D is the COMBINED model of Dienes 2003. To further compare the results of our algorithm with previous work, we obtained the output trees produced by Johnson (2002) and Dienes (2003) and evaluated them on typed dependency performance.
A Phrase-Based Statistical Model For SMS Text Normalization Jelinek. 1991. language modeling speech In A. Waibel and K.F. Lee, editors, Readings in Speech Recognition, pages 450-506. Morgan Kaufmann, 1991 D. Kernighan, K Church and W. Gale. 1990. spelling correction program based on a noisy model. Kukich. 1992. for automatically corwords in ACM Computing Surveys, This metaphor is, nonetheless, the one resorted to in (Aw et al, 2006), which uses a statistical phrase-based machine translation tool to convert English SMS texts into standardized English. Using this system, (Aw et al, 2006) reports a 0.81 BLEU (Papineni et al, 2001) score on a set of 5,000 English SMS. The corresponding BLEU score is close to 0,8, in line with the findings of (Aw et al, 2006) for English, and comparing favorably with the 0.68 score reported in (Guimierde Neef et al., 2007) (for French, using a different test bed). Following (Aw et al, 2006), we found that using off-the-shell statistical MT systems allows to achieve very satisfactory WER; combining this system with a system based on an analogy with the speech recognition problem yields an additional 1.5 absolute improvement in WER. (Aw et al, 2006) proposed an approach for normalizing Short Messaging Service (SMS) texts by translating it into normalized forms using Phrase-based SMT techniques on character level.  Parsing performance on noisy data can be improved by transforming the input data so that it resembles the parser's training data (Aw et al, 2006), transforming the training data so that it resembles the input data (van der Plas et al, 2009), applying semi supervised techniques such as the self-training protocol used by McClosky et al (2006), and changing the parser internals, e.g. adapting the parser's unknown word model to take into account variation in capitalisation and function word misspelling.  On this basis, Aw et al (2006) proposed a statistical machine translation model working at the phrase-level, by splitting sentences into their k most probable phrases.  In the case of text messages, text-to-speech synthesis may be particularly useful for the visually impaired; automatic translation has also been considered (e.g., Aw et al, 2006). Aw et al (2006) model text message normalization as translation from the texting language into the standard language. We propose to go beyond spell checkers, in performing deabbreviation when appropriate, and recovering the canonical word form of commonplace shorthands like b4 before, which tend to be considered beyond the remit of spell checking (Aw et al, 2006). For example, Aw et al (2006) propose a phrase-level SMT SMS normalisation method with bootstrapped phrase alignments. We reimplemented the state-of-art noisy channel model of Cook and Stevenson (2009) and SMT approach of Aw et al (2006) as benchmark methods. We compared the results using both the baseline model and the model implemented using the same training data as in Aw et al (2006).   (Aw et al, 2006) adapted a phrase-based MT model for normalizing SMS and achieved satisfying performance. 
Comlex  Syntax :  Bu i ld ing  a Computat iona l  Lex icon Ra lph  Gr i shm:m,  Cather ine  Mac leod,  and Adam Mcyers Computer  Science Depar tment ,  New York Un ivers i ty 715 Broadw,~y, 7th F loor ,  New York, NY 10003, U.S.A. {gr i s lnnan ,mac leod ,me.yers  } (@cs.nyu.e(ht Abstract We des((tile tile design of Comlex Syntax, a co,nputa- tional lexicon providing detailed syntactic iuformation ff)r approximately 38,000 English headwords.  The ANLT lexicon was derived semi-automatically from a machine readable dictionary (LDOCE), and although the COMLEX syntax dictionary (Grishman et al, 1994), which was derived with much greater amounts of human effort, has a slightly better performance, the difference is not great. We extend our previous work (Chodorowetal., 2007) by experimenting with combination features, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al, 1994). We also experimented with other features such as augmenting the model with verb-preposition preferences derived from CoMlex (Grishman et al, 1994), and querying the Google Terabyte N-gram corpus with the same patterns used in the combination features. Although a number of freely available, large scale and accurate SCF lexicons exist ,e.g. COMLEX (Grishman et al 1994), VerbNet (Kipperet al 2008) for English, availability and limitations in size and coverage remain an inherent is sue. The obtained SCFs comprise the total 163 SCF types which are originally based on the SCFs in the ANLT (Boguraev and Briscoe,1987) and COMLEX (Grishman et al, 1994) dictionaries. We obtain our SCF data using the sub categorization acquisition system of Briscoe and Carroll (1997). We expect the use of this system to be beneficial: it employs a robust statistical parser (Briscoe and Carroll, 2002) which yields complete though shallow parses, and a comprehensive SCF classifier, which incorporates 163 SCF distinctions, a super set of those found in the ANLT (Boguraev et al,1987) and COMLEX (Grishman et al, 1994) dictionaries. The lexicon is based on COMLEX (Grishman et al., 1994). We based our POS table lookup on NYU's COMLEX (Grishman et al 1994). SCF FramesThe SCFs recognized by the classifier were obtained by manually merging the frames exemplified in the COMLEX Syntax (Grishman et al, 1994), ANLT (Boguraev et al, 1987 )and/or NOMLEX (Macleod et al, 1997) dictionaries and including additional frames found by manual inspection of unclassifiable examples during development of the classifier. Carmel comes with a wide-coverage English grammar that is compatible with the wide-coverage COMLEX lexicon (Grishman et al, 1994). This fairly comprehensive classification incorporates 163 different sub categorization frames (SCFs), a superset of those listed in the ANLT (Boguraev et al, 1987) and COMLEX Syntax dictionaries (Grishman et al, 1994). was available and/or from the COMLEX Syntax dictionary (Grishman et al, 1994) all the SCFs taken by its member verbs. Important examples are LDOCE (Procter 1987), ComLex (Grishman et al 1994), PAROLE (Ruimy et al 1998). Itincorpo rates 168 SCF distinctions, a superset of those found in the COMLEX Syntax (Grishman et al, 1994) and ANLT (Boguraev et al, 1987) dictionaries. For example, we included the examination of a body part belonging to a person in the domain, and this was expressed through a Saxon genitive in English but a prepositional phrase (with the subsidiary NPs in the reverse order) in the other languages. To test our assumptions about efficiency and scalability we inferred a larger Tbox, subcategorisation frames and mappings using a pre-existing data set of verb frames for English encoded using the COMLEX subcategorisation frame inventory (Grishmanet al, 1994).  Other resources such as COMLEX syntax dictionary (Grishman et al., 1994) and English Verb Classes and Alternations (EVCA) (Levin, 1993) can provide verb subategorization information and syntactic paraphrases, but they are indexed by words thus not suitable to use in generation directly. It also exploits the classification given bythe COMLEX lexicon (Grishman et al, 1994) in order to calculate the deep-subject of infinitive verbs. the CARMEL grammar and semantic interpretation framework (Rose, 2000), and the COMLEX lexicon (Grishman et al, 1994). From the different diagnostics proposed in the literature some are quite consistent among various authors (R. Grishman et al 1994, C. Pollard and I. Sag 1987, C. Verspoor 1997).
Extracting Paraphrases From A Parallel Corpus While paraphrasing is critical both for interpretation and generation of natural language, current systems use manual or semi-automatic methods to collect paraphrases. We present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple English translations of the same source text. Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases. Some works, such as Barzilay and McKeown (2001), have acquired paraphrasing knowledge automatically. Barzilay and McKeown (2001) applied the distributionality hypothesis on such parallel sentences.  Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. Inspired by the use of parallel translations to mine paraphrasing lexicons (Barzilay and McKeown, 2001) and the use of MT engines forword sense disambiguation (Diab, 2000), we leverage existing machine translation systems to generate semantically equivalent, albeit lexically and syntactically distinct, questions. Barzilay and McKeown (2001) and Callison Burch et al (2006) extracted paraphrases from monolingual parallel corpus where multiple translations were present for the same source. Jacquemin (1999) and Barzilay and McKeown (2001) identify phrase level paraphrases, while Lin and Pantel (2001) and Shinyama et al (2002) acquire structural paraphrases encoded as templates. Barzilay and McKeown (2001) extract paraphrases from a monolingual parallel corpus, containing multiple translations of the same source. We first verify claim II by comparing our method with that of Barzilay and McKeown (2001) (BM method), Moses7 (Koehn et al, 2007) (SMT method), and that of Murata et al (2004) (Mrt method). In this experiment, following Barzilay and McKeown (2001), K is 10 and N is 1 to 3. Barzilay and McKeown (2001) induced simple POS-based paraphrase rules from paraphrase instances, which can be a good starting point. A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay and McKeown, 2001) and (Dolan et al, 2004). We compare the paraphrases we collect with paraphrases that are derivable from the same corpus using a co training-based paraphrase extraction algorithm (Barzilay and McKeown, 2001). Since the co-training-based algorithm of Barzilay and McKeown (2001) takes parallel corpus as input, we created out of the MTC corpus 55993 sentence pairs (Each equivalent translation set of cardinality 11 was mapped into (11 2) equivalent translation pairs.). Barzilay and McKeown (2001) used monolingual parallel corpora for identifying paraphrases. We find that most paraphrases extracted using the method of Barzilay and McKeown (2001) are quite short. Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al 2004) and DIRT (Lin and Pantel 2001). To generate dialogue sentences for a corresponding discourse structure we are adapting the approach to paraphrasing of Barzilay and McKeown (2001).  The approach in (Barzilay and McKeown, 2001) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours.
Language Independent Named Entity Recognition Combining Morphological And Contextual Evidence Identifying and classifying personal, geographic, institutional or other names in a text is an important task for numerous applications. This paper describes and evaluates a language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns captured in hierarchically smoothed trie models. The algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required language-specific information, tokenizers or tools. Some have assumed only partially tagged training corpora (Merialdo, 1994), while others have begin with small tagged seed word lists (such as Collins and Singer (1999) and Cucerzan and Yarowsky (1999) for named-entity tagging). The bootstrapping methods for language independent NER of Cucerzan and Yarowsky (1999) have a similar effect. Cucerzan and Yarowsky (1999) exploit morphological and contextual patterns to propose a language-independent solution to NER. Cucerzan and Yarowsky (1999) built a cross language NER, and the performance on English was low compared to supervised single-language NER such as Identi Finder. It has been previously attempted by Cucerzan and Yarowsky in their language independent NER work which used morphological and contextual evidences (Cucerzan and Yarowsky, 1999). We can also exploit what Cucerzan and Yarowsky (1999) call the one sense per discourse phenomenon, the tendency of terms to have a fixed meaning within a single document.  Both (Cucerzan and Yarowsky, 1999) and (Collins and Singer, 1999) present algorithms to obtain NEs from untagged corpora. The NER task for Hindi has been explored by Cucerzan and Yarowsky in their language independent NER work which used morphological and contextual evidences (Cucerzan and Yarowsky, 1999). Tries have previously been used in both supervised (Patrick et al, 2002) and unsupervised (Cucerzan and Yarowsky, 1999) named entity recognition. Earlier papers have taken a character-level approach to named entity recognition (NER), notably Cucerzan and Yarowsky (1999), which used prefix and suffix tries, though to our knowledge incorporating all character grams is new. Both (Cucerzan and Yarowsky, 1999) and (Collins and Singer, 1999) present algorithms to obtain NEs from untagged corpora. Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition. The NER task for Hindi has been explored by Cucerzan and Yarowsky in their language independent NER work which used morphological and contextual evidences (Cucerzan and Yarowsky, 1999). Previous work (Cucerzan and Yarowsky, 1999) was done using the complete words as features which suffers from a low recall problem. Prefix and suffix tries were also used previously (Cucerzan and Yarowsky, 1999). The core model utilized, extended and evaluated here is based on Cucerzan and Yarowsky (1999). Because the core model has been presented in detail in Cucerzan and Yarowsky (1999), this paper focuses primarily on the modifications of the algorithm and its adaptation to the current task. The bootstrapping stage (5) uses the initial or current entity assignments to estimate the class conditional distributions for both entities and contexts along their trie paths, and then re-estimates the distributions of the contexts/entity-candidates to which they are linked, recursively, until all accessible nodes are reached, as presented in Cucerzan and Yarowsky (1999). This paper has presented and evaluated an extended bootstrapping model based on Cucerzan and Yarowsky (1999) that uses a unified framework of both entity internal and contextual evidence.
Mining Very-Non-Parallel Corpora: Parallel Sentence And Lexicon Extraction Via Bootstrapping And EM We present a method capable of extracting parallel sentences from far more disparate “very-non-parallel corpora” than previous “comparable corpora” methods, by exploiting bootstrapping on top of IBM Model 4 EM. Step 1 of our method, like previous methods, uses similarity measures to find matching documents in a corpus first, and then extracts parallel sentences as well as new word translations from these documents. But unlike previous methods, we extend this with an iterative bootstrapping framework based on the principle of which claims documents found to contain one pair of parallel sentences must contain others even if the documents are judged to be of low similarity. We re-match documents based on extracted sentence pairs, and refine the mining process iteratively until convergence. This novel principle allows us to add parallel sentences from documents, to the baseline set. Experimental results show that our proposed method is nearly 50% more effective than the baseline method without iteration. We also show that our method is effective in boosting the performance of the IBM Model 4 EM lexical learner as the latter, though stronger than Model 1 used in previous work, does not perform well on data from very-non-parallel corpus.  We base our candidate generation on a method that Fung and Cheung (2004) developed for extracting loose translations (comparable sentence pairs) from quasi-comparable corpora [9], as shown in Figure 2. However, we can understand the improvement by comparing against scores obtained using the cosine-based lexical similarity metric which is typical of the majority of previous methods for mining non-parallel corpora, including that of Fung and Cheung (2004) [9]. Fung and Cheung (2004) approach the problem by using a cosine similarity measure to match foreign and English documents. The degree of parallelism can vary greatly, ranging from noisy parallel documents that contain many parallel sentences, to quasi parallel documents that may cover different topics (Fung and Cheung, 2004). Sample comparable sentences that contain parallel phrases other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. Based on this observation, dynamic program ming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. Another approach focused on sentence extraction (Fung and Cheung, 2004). A web interface was developed in order to annotate each pair, following the distinction introduced by Fung and Cheung (2004): parallel indicates sentence-aligned texts that are in translation relation; noisy characterizes two documents that are never the less mostly bilingual translations of each other; topic corresponds to documents which share similar topics, but that are not translation of each others and very-non that stands for rather unrelated texts. We would like to stress that, while conducting the manual annotation, we frequently found difficult to label pairs of articles with the classes proposed by Fung and Cheung (2004). The last case study of document and sentence alignment from ―very-non-parallel corpora is the work from Fung and Cheung (2004).  Fung and Cheung (2004a) describe corpora ranging from noisy parallel, to comparable, and finally to very non-parallel. For example, Munteanu and Mar cu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use cosine similarity.
Disambiguation Of Proper Names In Text trieving information from full text using linguisknowledge, In of the Fifteenth Online Meeting, New York, May. The only exception is in (Wacholder et al 1997) where the reported performance for the sole semantic disambiguation task of PNs is 79%. Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). Ravin and Kazi (1999) further refined the method of solving co-reference through measuring context similarity and integrated it into Nominator (Wacholder et al, 1997), which was one of the first successful systems for named entity recognition and co-reference resolution. The entity identification and in-document co reference components resemble the Nominator system (Wacholder et al, 1997). As Wacholder et al (1997) noted, it is fairly common for one of the mentions of an entity in a document to be a long, typical surface form of that entity (e.g., George W. Bush), while the other mentions are shorter surface forms (e.g., Bush).  We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997). Wacholder et al (1997) use hand-written rules and knowledge bases to classify proper names into broad categories. Approaches to this problem include Wacholder et al (1997), focusing on the variation of surface name for a given referent, and Smith and Crane (2002), resolving geographic name ambiguity. A common approach is to extract word-internal features from unknown words, for example suffix, capitalization, or punctuation features (Mikheev, 1997, Wacholder et al, 1997, Bikel et al, 1997).
Discriminative Reranking For Natural Language Parsing This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set offeatures, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the model. The new model achieved 89.75% a 13% relative decrease in measure error over the baseline model’s score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative—in terms of both simplicity and efficiency—to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation. Previous work in statistical parsing (Collins and Koo, 2005) has shown that applying reranking techniques to the n-best output of a base parser can improve parsing performance. Previous approaches (e.g., (Collins and Koo, 2005)) have used a linear model to combine the log probability under a base parser with arbitrary features derived from parse trees. In our work, we included all features described in (Collins and Koo, 2005). We have found that features in (Collins and Koo, 2005), initially developed for English parsing, also give appreciable gains in accuracy when applied to Spanish. In the reranking experiments, we follow the procedure described in (Collins and Koo, 2005) for creation of a training set with n-best parses for each sentence. Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule. A well-studied subject (e.g. the work of Charniak and Johnson (2005) and of Collins and Koo (2005)), parse reranking is concerned with the reordering of n-best ranked parse trees output by a syntactic parser. There is also work on discriminative models for parse reranking (Collins and Koo, 2005). These include text categorization (Schapire and Singer, 2000), Natural Language Parsing (Collins and Koo, 2005), English syntactic chunking (Kudo et al, 2005) and so on. Collins and Koo proposed a method only updates values of features co-occurring with a rule feature on examples at each iteration (Collins and Koo, 2005). As explained in Collins and Koo (2005), the decoding score plays an important role in reranking the candidate sentences. The n-best list for training data is produced using multi fold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005). We compare our models with a boosting-based discriminative approach (Collins and Koo, 2005) and its regularized version (Huang et al, 2007). For comparison purposes, we also showed the results of Collins and Koo (2005) its regularized versions with n-gram features. As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). We use the boosting approach of (Collins and Koo, 2005) to perform feature selection and identify good weight values. At test time, we choose an ordering using a maximum entropy reranking approach (Collins and Koo, 2005). Instead of examining and comparing rules in their entirety, this method abstracts a rule to its component parts, similar to features using information about n-grams of daughter nodes in parse reranking models (e.g., Collins and Koo, 2005). Collins and Koo (2005) proposed a reranking method for phrase structure parsing with which any type of global features in a parse tree can be used. In the discriminative reranking method (Collins and Koo, 2005), first, a set of candidates is generated using a base model (GEN).
The Stanford Typed Dependencies Representation This paper examines the Stanford typed dependencies representation, which was designed to provide a straightforward description of grammatical relations for any user who could benefit from automatic text understanding. For such purposes, we argue that dependency schemes must follow a simple design and provide semantically contentful information, as well as offer an automatic procedure to extract the relations. We consider the underlying design principles of the Stanford scheme from this perspective, and compare it to the GR and PARC representations. Finally, we address the question of the suitability of the Stanford scheme for parser evaluation. In English, this kind of typed dependencies has been introduced by de Marneffe and Manning (2008) and de Marneffe et al (2006). While previous work uses the Stanford CoreNLP toolkit to identify characters and extract typed dependencies for them, we found this approach to be too slow for the scale of our data (a total of 1.8 billion tokens); in particular, syntactic parsing, with cubic complexity in sentence length, and out-of-the-box co reference resolution (with thousands of potential antecedents) prove to be 3All categories are described using the Stanford typed dependencies (de Marneffe and Manning, 2008), but any syntactic formalism is equally applicable. Bjorne et al. showed that deep dependency analyses in the well-established Stanford Dependency (SD) scheme (de Marneffe and Manning, 2008) can successfully be utilised in extracting graphs that express semantic entities as node sand relationship arguments as edges but are limited to one node per syntactic token. The native Penn TreeBank output of Bikel's and McClosky's parser was converted to the Stanford Dependency (SD) collapsed dependency format (de Marneffe and Manning, 2008). In this stage, we connect the triggers extracted with appropriate arguments using rules defined with the Stanford dependency (SD) scheme (de Marneffe and Manning, 2008). More recent approaches to compression introduce reordering and paraphrase operations (e.g. ,dencies (Briscoe, 2006) while there are over 50 Stanford Dependencies (de Marneffe and Manning, 2008). In this paper, we describe 1) a new dependency conversion (Section 3) of the Penn Treebank (Marcus, et al, 1993) along with the associated dependency label scheme, which is based upon the Stanford parser's popular scheme (de Marneffe and Manning, 2008), and a fast, accurate dependency parser with non-projectivity support (Section 4) and additional integrated semantic annotation modules for automatic preposition sense disambiguation and noun compound interpretation (Section 5). By far the most prominent of these is the Stanford typed dependency scheme (de Marneffe and Manning, 2008). This semantic representation can be extracted from the user input by our understanding component via a robust hybrid approach: either via a number of surface patterns containing regular expressions or via patterns reflecting the syntactic analysis of a dependency parser (de Marneffe and Manning, 2008). Stanford dependencies (de Marneffe and Manning, 2008) provide a simple description of relations between pairs of words in a sentence. We now describe how we build the syntactic relatedness trie (SRT) that forms the scaffolding for the probabilistic models needed to identify sentiment-bearing words via syntactic constraints extracted from a dependency parse (Kubler et al, 2009). We use the Stanford Parser (de Marneffe and Manning, 2008) to produce a dependency graph and con sider the resulting undirected graph structure over words. 3.3.1 Dependency Structures The first set of these features include typed dependency structures (de Marneffe and Manning, 2008) which describe the grammatical relationships between words. To obtain dependency trees, we passed the Stanford constituency trees through the Stanford constituency-to-dependency converter (de Marneffe and Manning, 2008). We use both the original dependency paths and their collapsed Stanford Dependencies forms (de Marneffe and Manning, 2008). As recent work in the BioNLP 2009 shared task has shown (Kim et al,2009), domain-adapted parsing benefits information extraction systems. The native output of the C&C parser is converted into the Stanford Dependency (SD) collapsed dependency format (de Marneffe and Manning, 2008). We also establish the applicability of the PropBank scheme to the clinical sub language with its many atypical characteristics, and finally, we find that the PropBank scheme is compatible with the Stanford Dependency scheme of de Marneffeand Manning (2008a; 2008b) in which the under lying tree bank is annotated. The tree bank of Haverinen et al is annotated in the Stanford Dependency (SD) scheme of de Marneffe and Manning (2008a; 2008b). These relations are labeled using traditional grammatical concepts (subject, object, modifier) that are arranged into an inheritance hierarchy (de Marneffe and Manning, 2008a, Sec. In so far, recovering SD relations from phrase-structure (PS) trees have used a range of structural cues such as positions and phrase-labels (see, for instance, the software of de Marneffe and Manning (2008a)).
An Improved Error Model For Noisy Channel Spelling Correction The noisy channel model has been applied to a wide range of problems, including spelling correction. These models consist of two components: a source model and a channel model. Very little research has gone into improving the channel model for spelling correction. This paper describes a new channel model for spelling correction, based on generic string to string edits. Using this model gives significant performance improvements compared to previously proposed models. At ACL 2000, Brill and Moore (2000) introduced a new error model, allowing generic string-to-string edits. Brill and Moore (2000) present an improved error model for noisy channel spelling correction that goes beyond single insertions, deletions, substitutions, and transpositions. Brill and Moore (2000) showed that adding a source language model increases the accuracy significantly. The error model LTR was trained exactly as described originally by Brill and Moore (2000). In baseline speller we use a substring-based error model P dist (q0|q1) described in (Brill and Moore, 2000), the error model training method and the hypotheses generator are similar to (Duan and Hsu, 2011). Brill and Moore (2000) characterise the error model by computing the product of operation probabilities on slice-by-slice string edits. The next class of approaches applied the noisy channel model to correct single word spelling errors (Kernighan et al., 1990), (Brill and Moore, 2000). Neither do we require pairs of misspelled names and their correct spellings for learning the error model unlike (Brill and Moore, 2000) or large-coverage general purpose lexicon for unlike (Cucerzan and Brill, 2004) or pronunciation dictionaries unlike (Toutanova and Moore, 2002). Brill and Moore (2000) learn misspelled-word to correctly-spelled-word similarities for spelling correction. For example, Brill and Moore (2000) combine a character-based alignment with the Expectation Maximization (EM) algorithm to develop an improved probabilistic error model for spelling correction. The largest step towards an automatically trainable spelling system was the statistical model for spelling errors (Brill and Moore, 2000). Contrary to Brill and Moore (2000), we observe that user edits of ten have both left and right context, when editing a document. The perfect score function calculates the probability of a suggestion given the misspelled word (Brill and Moore, 2000). In addition to the simple Levenshtein distance, we also use generalized string-to-string edit distance (Brill and Moore, 2000), which we trained on aligned katakana-English word pairs in the same manner as Brill et al (2001). One interesting future avenue to consider is to use the edit distance functions in our current model to select a subset of query-candidate pairs that are similar in terms of these functions, separately for the surface and Romanized forms, and use this subset to align the character strings in these query-candidate pairs as described in Brill and Moore (2000), and add the edit operations derived in this manner to the term variation identification classifier as features. Brill and Moore (2000) introduced a model that worked on character sequences, not only on character level, and was conditioned on where in the word the sequences occurred. The approximate string matching algorithm we suggest is essentially that of Brill and Moore (2000), a modified weighted Levenshtein distance, where we allow error operations on character sequences as well as on single characters. It would have been possible to use a fast trie implementation (Brill and Moore, 2000), however. (Brill and Moore, 2000) presented an improved error model over the one proposed by (Kernighan et al, 1990) by allowing generic string-to-string edit operations, which helps with modeling major cognitive errors such as the confusion between le and al. The second is a slightly modified version of the spelling correction model of Brill and Moore (2000).
Transition-based Dependency Parsing with Rich Non-local Features Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations. In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems. In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transitionbased parsing and rivaling the best results overall. For the Chinese Treebank, they give a signficant improvement of the state of the art. An open source release of our parser is freely available. The present paper deals with five parsers evaluated within the translation frame work: three genuine dependency parsers, namely the parsers described in (McDonald et al, 2005), (Nivre et al, 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs we reconverted to dependency structures by Penn Converter (Johansson and Nugues, 2007). ZPar - Zpar parser which is basically an alternative implementation of the Malt parser, employing a richer set of non-local features as described by Zhang and Nivre (2011). The dependency parsing features are taken from the work of Zhang and Nivre (2011), and the features for joint word segmentation and POS-tagging are taken from Zhang and Clark (2010).   Our results are better than most systems on this data split, except Zhang and Nivre (2011), Li et al (2012) and Chen et al (2009). Python implementation for the labeled arc-eager system with the rich feature set of Zhang and Nivre (2011) is available on the first author's homepage. ArcS use feature set of Huang and Sagae (2010) (50 templates), and ArcE that of Zhang and Nivre (2011) (72 templates). Li11 refers to the second-order graph-based model of Li et al (2011), whereas Z&N11 is the feature-rich transition-based model of Zhang and Nivre (2011). More efficient decoding would also allow the use of the look-ahead features (Hatori et al, 2011) and richer parsing features (Zhang and Nivre, 2011). Additionally, we look at higher-order dependency arc label features, which is novel to graph-based parsing, though commonly exploited in transition-based parsing (Zhang and Nivre, 2011). The speed of our parser is 220 tokens per second, which is over 4 times faster than an exact third-order parser that at Figure 1: Example Sentence.tains UAS of 92.81% and comparable to the state-of the-art transition-based system of Zhang and Nivre (2011) that employs beam search. We compare our method to a state-of-the-art graph-based parser (Koo and Collins, 2010) as well as a state-of-the-art transition-based parser that uses a beam (Zhang and Nivre, 2011) and the dynamic programming transition-based parser of Huang and Sagae (2010). Additionally, we compare to our own implementation of exact first to third-order graph-based parsing and the transition-based system of Zhang and Nivre (2011) with varying beam sizes. We also report results for a re-implementation of exact first to third-order graph-based parsing and a re-implementation of Zhang and Nivre (2011) in order to compare parser speed. Second, at a similar toks/sec parser speed, our method achieves better performance than the transition-based model of Zhang and Nivre (2011) with a beam of 256. Here we use the identical training/validation/evaluation splits and experimental set-up as Zhang and Nivre (2011). Here we compare to our re-implementations of Zhang and Nivre (2011), exact first to third-order parsing and Rush and Petrov (2012) for the data sets in which they reported results. Zhang and Nivre is a reimplementation of Zhang and Nivre (2011) with beams of size 64 and 256. For reference, Zhang and Nivre (2011) report 86.0/84.4, which is previously the best result reported on this data set. Our feature template is an extended version of the feature template of Zhang and Nivre (2011), originally developed for the arc-eager model.
Empirical Methods For Compound Splitting Compounded words are a challenge for NLP applications such as machine translation (MT). We introduce methods to learn splitting rules from monolingual and parallel corpora. We evaluate them against a gold standard and measure their impact on performance of statistical MT systems. Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task. Splitting options for the German word Aktionsplan Aktionsplan Aktion actionplan action plan Akt ion s plan act ion plan Here, we follow an approach introduced by Koehn and Knight (2003): First, we collect frequency statistics over words in our training corpus. Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and UrduHindi (Lehal, 2010). Correctsplitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. We split large words based on word frequencies to tackle the problem of word compounds in German (Koehn and Knight, 2003). In order to reduce the source vocabulary size for the German-English translation, the source side was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003). We used the frequency-based segmentation algorithm initially introduced in (Koehn and Knight, 2003) to handle compounding. To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. The empirical approach of Koehn and Knight (2003) splits German compounds into words found in a training corpus. Popovic et al. (2006) compared the approach of Nie? en and Ney (2000) with the corpus-driven splitting of Koehn and Knight (2003) in terms of performance on an SMT task. We briefly introduce the computational morphology SMOR (section 3.1) and the corpus driven approach of Koehn and Knight (2003) (section 3.2), before we present our hybrid approach that combines the benefits of both in section 3.3. Koehn and Knight (2003) describe a method requiring no linguistically motivated morphological analysis to split compounds. Taken from (Koehn and Knight, 2003): S= split, pi= part, n= number of parts. The one-to-one correspondence gold standard (Koehn and Knight, 2003) indicates only compounds that were translated compositionally by a human translator.   Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words.  Koehn and Knight (2003) used a fixed set of two known fillers s and es for handling German compounds. We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a).
Adaptation Of Maximum Entropy Capitalizer: Little Data Can Help A Lot A novel technique for maximum “a posteriori” (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented. The technique is applied to the problem of recovering the correct capitalization of uniformly cased text: a “background” capitalizer trained on 20Mwds of Wall Street Journal (WSJ) text from 1987 is adapted to two Broadcast News (BN) test sets — one containing ABC Primetime Live text and the other NPR Morning News/CNN Morning Edition text — from 1996. The “in-domain” performance of the WSJ capitalizer is 45% better than that of the 1-gram baseline, when evaluated on a test set drawn from WSJ 1994. When evaluating on the mismatched “out-ofdomain” test data, the 1-gram baseline is outperformed by 60%; the improvement brought by the adaptation technique using a very small amount of matched BN data — 25-70kwds — is about 20-25% relative. Overall, automatic capitalization error rate 1.4%is achieved on BN data. Chelba and Acero (2004) first traina classifier on the source data. (Chelba and Acero, 2004) study the impact of using increasing amounts of training data as well as a small amount of adaptation. The first model, which we shall refer to as the PRIOR model, was first introduced by Chelba and Acero (2004). Chelba and Acero (2004) describe this approach within the context of a maximum entropy classifier, but the idea is more general. Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data. One recently proposed method (Chelba and Acero, 2004) for transfer learning in Maximum Entropy models involves modifying the mu's of this Gaussian prior. In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation. This may also be the reason that the model of Chelba and Acero (2004) did not aid in adaptation. Another piece of similar work is (Chelba and Acero, 2004), who also modify their prior. Chelba and Acero (2004) use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data. Chelba and Acero (2004) use amaximum entropy Markov model (MEMM) combining features involving words and their cases.  Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daume, 2007) and adding the source classifier' s prediction as a feature (Chelba and Acero, 2004). The approach proposed by Chelba and Acero (2004) is also related as they propose a MAP adaptation via Gaussian priors of a MaxEnt model for recovering the correct capitalization of text.
Unsupervised Learning Of Disambiguation Rules For Part Of Speech Tagging In this paper we describe an unsupervised learning algorithm for automatically training a rule-based part of speech tagger without using a manually tagged corpus. We compare this algorithm to the Baum-Welch algorithm, used for unsupervised training of stochastic taggers. Next, we show a method for combining unsupervised and supervised rule-based training algorithms to create a highly accurate tagger using only a small amount of manually tagged text. (Brill, 1995) presents a rule-based part-of-speech tagger for unsupervised training corpus. The fact that observations and prior knowledge are useful for part-of-speech tagging is well understood (Brill, 1995), but the approach of estimating an initial transition model only from unambiguous word pairs is novel. In the meantime, (Brill 1995a) (Brill 1995b) proposed a method to acquire context-dependent POS disambiguation rules and created an accurate tagger, even from a very small annotated text by combining supervised and unsupervised learning.  Later results (e.g. Brill (1995)) seemed to indicate that other methods of unsupervised learning could be more effective (although the work of Banko and Moore (2004) suggests that the difference may be far less than previously assumed). Transformation based learning (TBL) (Brill, 1995) is a machine learning approach for rule learning. In its most general setting, the TBL hypothesis is not a classifier (Brill, 1995). (Brill, 1995) uses lexicon for initial annotation of the training corpus, where each word in the lexicon has a set POS tags seen for the word in the training corpus. Brill (1995b) proposed an unsupervised tagger based on transformation based learning (Brill, 1995a), achieving accuracies of above 95%. We will also study the effect of other window sizes and the combination of this unsupervised approach with minimally-supervised approaches such as (Brill 1995) (Smith and Mann 2003). Research into unsupervised part-of-speech tagging with a tag dictionary (sometimes called weakly supervised POS tagging) has been going on for many years (cf Merialdo (1994), Brill (1995)), but generally using a fairly small tag set. Unsupervised part-of-speech tagging, as defined above, has been attempted using a variety of learning algorithms (Brill 1995, Church, 1988, Cutting et. al. 1992, Elworthy, 1994 Kupiec 1992, Merialdo 1991). For our comparison of unsupervised tagging methods, we implemented the HMM taggers described in Merialdo (1991) and Kupiec (1992), as well as the UTBL tagger described in Brill (1995).
Word-Sense Disambiguation Using Statistical Models of Rogets Categories Trained on Large Corpora David Yarowsky AT&T Bell  Laboratories 600 Mountain Avenue Murray  Hil l  N J, 07974 yarowsky@research.att .com Abst rac t This paper describes a program that disambignates English word senses in unrestricted text using statistical models of the major Rogets Thesaurus categories.   Recently, Yarowsky (1992) has found a way to extend our use of the Bayesian techniques by training on the Roget's Thesaurus (Chapman, 1977) 2 and G-rolier's Encyclopedia (1991) instead of the Canadian Hansards, thus circumventing many of the objections to our use of the Hansards. Yarowsky (1992) inputs a 100-word context surrounding a polysemous word and scores each of the 1042 Roget Categories by: 1[ P r (w lRoget Categoryi) w in context The program can also be run in a mode where it takes unrestricted text as input and tags each word with its most likely Roget Category. Table 1 shows the performance of Yarowsky (1992) on twelve words which have been previously discussed in the literature. In fact, both Black (1988) and Yarowsky (1992) report 72% performance on this very same word. In fact, Yarowsky (1992) falls below the baseline for one of the twelve words (issue), although perhaps, we needn't be too concerned about this one deviation.  Semantic tags are assigned from on-line thesauras like WordNet (Basili et al 1996) (Resnik, 1995), Roget's categories (Yarowsky 1992) (Chen and Chen, 1996), the Japanese BGH (Utsuro et al 1993), or assigned manually (Basili et al 1992) 1. Yarowsky (1992) used Roget's Thesaurus categories as classes for word senses.  The best examples of this approach has been the resent work of Yarowsky (Yarowsky, 1992), (Yarowsky, 1993), (Yarowsky, 1995). Much of the research in this area has been compromised by the fact that researchers have focused on lexical ambiguities that are not true word sense distinctions, such as words translated differently across two languages (Gale, Church, and Yarowsky, 1992) or homophones (Yarowsky, 1993). Pragmatic domain codes can be used to disambiguate (usually nominal) senses, as was shown by (Bruce and Guthrie, 1992) and (Yarowsky, 1992).  Some clusters of studies have used common test suites, most notably the 2094-word Hne data of Leacock et al (1993), shared by Lehman (1994) and Mooney (1996) and evaluated on the system of Gale, Church and Yarowsky (1992). Yarowsky (Yarowsky, 1992) used instead thesauri for their experiments. The massive network of inverted semrel structures contained in MindNet invalidates the criticism leveled against dictionary-based methods by Yarowsky (1992) and Ide and Veronis (1993) that LKBs created from MRDs provide spotty coverage of a language at best. Syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., Brown et al 1992, Yarowsky 1992), as well as in similar predicate argument structure contexts (e.g., Grishman and Sterling 1994). As in prior work including B&L, we rely on the intuition that the senses of words are hinted at by their contextual information (Yarowsky, 1992). From the perspective of a generative process, neighboring words of a target are generated by the target's underlying sense. 
A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based On Minimum Cuts analysis to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as “thumbs up” “thumbs down”. To determine this powe propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient for finding cuts in this greatly facilitates incorporation of cross-sentence contextual constraints. Moreover, Ng et al (2006) examine the FS of the weighted log-likelihood ratio (WLLR) on the movie review dataset and achieves an accuracy of 87.1%, which is higher than the result reported by Pang and Lee (2004) with the same dataset. In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al, 2007). For our experiments, we employ a large, recently introduced IMDB movie review dataset (Maas et al, 2011), in place of the smaller dataset introduced in (Pang and Lee, 2004) more commonly used for sentiment analysis. Pang and Lee (2004) use a graph-based technique to identify and analyze only subjective parts of texts. Graph based SSL learning has been successfully applied to opinion detection (Pang and Lee, 2004) but is not appropriate for dealing with large scale data sets. One of the standard data sets in opinion detection is the movie review data set created by Pang and Lee (2004). We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level. This is because certain parts-of-speech have been found to be better indicators of sentiment (Pang and Lee, 2004). Pang and Lee (2004) proposed to eliminate objective sentences before the sentiment classification of documents.  Sentence-level subjectivity detection, where training data is easier to obtain than for positive vs. negative classification, has been successfully performed using supervised statistical methods alone (Pang and Lee, 2004) or in combination with a knowledge based approach (Riloff et al, 2006). We build two classifiers based on the work of Pang and Lee (2004) to measure the polarity and objectivity of article edits. Sentiment analysis can be dependently or independently done from subjectivity detection, although Pang and Lee (2004) state that subjectivity detection performed prior to the sentiment analysis leads to better results in the latter.  In fact, it has already been established that sentence level classification can improve document level analysis (Pang and Lee, 2004). Cascaded models for fine-to-coarse sentiment analysis were studied by Pang and Lee (2004). For instance, in Pang and Lee (2004), yd would be the polarity of the document and ysi would indicate whether sentence si is subjective or objective. The local dependencies between sentiment labels on sentences is similar to the work of Pang and Lee (2004) where soft local consistency constraints were created between every sentence in a document and inference was solved using a min-cut algorithm. Alternatively, decisions from the sentence classifier can guide which input is seen by the document level classifier (Pang and Lee, 2004). On the other hand, we associate sentiment polarity to a document on the whole as opposed to Pang and Lee (2004) which deals with sentiment prediction of subjectivity content only.
Role of Word Sense Disambiguation i  Lexical Acquisition: Predicting Semantics from Syntactic Cues Bonn ie  J. Dor r  and Doug Jones Depar tment  of Computer  Sc ience and Ins t i tu te  for Advanced Computer  Stud ies Un ivers i ty  of Mary land A.V.  Hence, if we were given the perfect knowledge of the possible syntactic frames, verbs can be classified into the correct classes almost perfectly (Dorr and Jones, 1996). Dorr and Jones (1996) showed that perfect knowledge of the allowable syntactic frames for a verb allows 98% accuracy in type assignment to Levin classes.  Verb classifications have, in fact, been used to support many natural language processing (NLP) tasks, such as language generation, machine translation (Dorr, 1997), document classification (Klavans and Kan, 1998), word sense disambiguation (Dorr and Jones, 1996) and sub categorization acquisition (Korhonen, 2002).
  To some extent, function labels overlap with semantic role labels as defined in PropBank (Palmer et al, 2005). Our results confirm the findings in (Palmer et al, 2005). For our experiments on semantic role labeling we used PropBank annotations (Palmer et al, 2005). Example of Semantic Role Labeling from the PropBank dataset (Palmer et al, 2005). We focus our experimental study on the semantic role labeling problem (Palmer et al, 2005): being able to give a semantic role to a syntactic constituent of a sentence, i.e. annotating the predicate argument structure in text. FrameNet (Baker et al, 1998) and the Proposition Bank (Palmer et al, 2005), or PropBank for short, are the two main systems currently developed for semantic role-labeling annotation. The inter-annotator agreement for PropBank reported in (Palmer et al, 2005) is above 0.9 in terms of the Kappa statistic (Sidney and Castellan Jr., 1988). Recently, large corpora have been manually annotated with semantic roles in FrameNet (Fillmore et al, 2001) and PropBank (Palmer et al, 2005). Instead, we resort to Semantic Role Labeling (Palmer et al, 2005) to provide more lexicalized and semantic constraints to select the candidates. In the first step, we adopt the definitions found in PropBank (Palmer et al, 2005), defining our own frame sets for verbs not in Prop Bank, such as phosphorylate. As proposition banks are semantically annotated versions of a Penn-style tree bank, they provide consistent semantic role labels across different syntactic realizations of the same verb (Palmer et al, 2005). In addition to these syntactic structures, it was also annotated with predicate-argument structures (WSJ proposition bank) by Palmer et al (2005). Propbank (Palmer et al., 2005): This is a semantic annotation of the Wall Street Journal section of Penn Treebank-2. Despite all the known issues in semantic tagging, the major lexical resources (WordNet (Fellbaum, 1998), FrameNet (Ruppenhofer et al 2010), PropBank (Palmer et al 2005) and the word-sense part of OntoNotes (Weischedel et al 2011)) are still maintained and their annotation schemes are adopted for creating new manually annotated data (e.g. MASC, the Manually Annotated Subcorpus (Ide et al 2008)). More recent lexical resources have been built as semantic concordances from the very beginning (PropBank (Palmer et al 2005), OntoNotes word senses (Weischedel et al 2011)). Two other, somewhat different, lexical resources have to be mentioned to complete the picture: FrameNet (Ruppenhofer et al 2010) and PropBank (Palmer et al 2005). The overall IAA measured on verbs was 94% (Palmer et al 2005). For our experiments, we use an enhanced version of the CCGbank (Hockenmaier and Steedman, 2007) - a corpus of CCG derivations derived from the Penn Treebank - with Propbank (Palmer et al, 2005) roles projected onto it (Boxwell and White, 2008). The PropBank project (Palmer et al,2005) is another popular resource related to semantic role labeling. Thus, Narayanan and Harabagiu (2004) apply the argument-predicate relationship from PropBank (Palmer et al, 2005) together with the semantic frames from FrameNet (Baker et al, 1998) to create an inference mechanism to improve QA.
Preemptive Information Extraction Using Unrestricted Relation Discovery surface text patterns for a question answering system. of the 40th Annual Meeting of the As Our work is related to previous work on domain independent unsupervised relation extraction, in particular Sekine (2006), Shinyama and Sekine (2006) and Banko et al (2007). Shinyama and Sekine (2006) apply NER, coreference resolution and parsing to a corpus of newspaper articles to extract two-place relations between NEs. Like Sekine (2006) and Shinyama and Sekine (2006), we concentrate on relations involving NEs, the assumption being that these relations are the potentially interesting ones. Inference rules are an important building block of many semantic applications, such as Question Answering (Ravichandran and Hovy, 2002) and Information Extraction (Shinyama and Sekine, 2006). An alternative paradigm, Open IE, pioneered by the TextRunner system (Banko et al., 2007) and the "preemptive IE" in (Shinyama and Sekine, 2006), aims to handle an unbounded number of relations and run quickly enough to process Webscale corpora. Shinyama and Sekine proposed the "preemptive IE" framework to avoid relation-specificity (Shinyama and Sekine, 2006). Our work focuses on a recent line of exploratory work in the direction of Unrestricted Relation Discovery which is defined as: the automatic identification of different relations in text without specifying a relation or set of relations in advance (Shinyama and Sekine, 2006). Shinyama and Sekine (2006) developed an approach to preemptively discover relations in a corpus and present them as tables with all the entity pairs in the table having the same relations between them. A paradigm related to Open IE is Preemptive IE (Shinyama and Sekine, 2006). However, most algorithms (Hasegawa et al 2004, Shinyama and Sekine, 2006, Chen et. al, 2005) rely on tagging predefined types of entities as relation arguments, and thus are not well-suited for the open domain. Prior work in relation discovery (Shinyama and Sekine, 2006) has investigated the problem of finding relationships between different classes. Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). (Shinyama and Sekine, 2006) rely further on supervised methods, defining features over a full syntactic parse, and exploit multiple descriptions of the same event in newswire to identify useful relations. Shinyama and Sekine (2006) describe an approach to template learning without labeled data. Preemptive IE (Shinyama and Sekine, 2006) is a paradigm related to Open IE that first groups documents based on pairwise vector clustering, then applies additional clustering to group entities based on document clusters. Inference rules are an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006), describing a directional inference relation between two text patterns with variables. The benefit of utilizing template-based inference rules between predicates was demonstrated in NLP tasks such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006).
Applying Conditional Random Fields To Japanese Morphological Analysis This paper presents Japanese morphological analysis based on conditional random fields (CRFs). Previous work in CRFs assumed that observation sequence (word) boundaries were fixed. However, word boundaries are not clear in Japanese, and hence a straightforward application of CRFs is not possible. We show how CRFs can be applied to situations where word boundary ambiguity exists. CRFs offer a solution to the long-standing problems in corpus-based or statistical Japanese morphological analysis. First, flexible feature designs for hierarchical tagsets become possible. Second, influences of label and length bias are minimized. We experiment CRFs on the standard testbed corpus used for Japanese morphological analysis, and evaluate our results using the same experimental dataset as the HMMs and MEMMs previously reported in this task. Our results confirm that CRFs not only solve the long-standing problems but also improve the performance over HMMs and MEMMs. The lattice-conditional estimation approach was first used by Kudo et al (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. In Japanese WS, unknown words are usu ally dealt with in an on line manner with the unknown word model, which uses heuristics 183 depending on character types (Kudo et al,2004). In Japanese, our unknown word model relies on heuristics based on character types and word length to generate word nodes, similar to that of MeCab (Kudo et al., 2004).  Kudo et al (2004) use SVMs to morphologically tag Japanese. The four parallel corpora were tokenized and lemmatized, for Japanese with the MeCab morphological analyzer (Kudo et al, 2004), and for English with the Freeling analyzer (Padr? et al, 2010), with MWE, quantities, dates and sentence segmentation turned off. In our approach, N-best candidates for each training example are produced with the CRF++ software (Kudo et al, 2004). Following Kudo et al (Kudo et al, 2004), we adapted the core engine of the CRF-based morphological analyzer, MeCab1, to our POS/PROTEIN tagging task. Segmentation for Japanese is a successful field of research, achieving the F-score of nearly 99% (Kudo et al, 2004). One would notice that the baseline score is much lower than the score previously reported regarding newspaper articles (Kudo et al, 2004). This area of research may be considered almost completed, as previous studies reported the F-score of nearly 99% (Kudo et al, 2004). The sequential tagger used in this paper is CRF++ (Kudo et al, 2004). Previous work (Kudo et al, 2004) showed CRFs outperform generative Markov models and discriminative history-based methods in JWS. Kudo et al (2004) modified CRFs for non-segmented languages like Japanese which have the problem of word boundary ambiguity. As conventional sequential tagging problems, such part-of-speech tagging and phrase chunking, we employ the conditional random fields (CRF) as learners (Kudo et al, 2004). Regarding the two state-of-the-art word segmentation systems, one is JUMAN,  a rule-based word segmentation system (Kurohashi and Nagao, 1994), and the other is MECAB, a supervised word segmentation system based on CRFs (Kudo et al, 2004). The performance of the two word segmentation baselines (JUMAN and MECAB) is significantly worse in our task than in the standard word segmentation task, where nearly 99% precision and recall are reported (Kudo et al, 2004). Kudo et al (2004) studied Japanese word segmentation and POS tagging using conditional random fields (CRFs) and rule based unknown word processing. To demonstrate our method, we compare to several well-known structural learning algorithms, like CRF (Kudo et al, 2004), and SVM-HMM (Joachims et al, 2009) on two well-known data, namely, CoNLL-2000 syntactic chunking, SIGHAN-3 Chinese word segmentation tasks. Following Kudo et al (2004), we adapted the core engine of the CRF-based morphological analyzer, MeCab2, to our POS tagging task.
Investigating GIS And Smoothing For Maximum Entropy Taggers This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (Gis) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctof unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: standard Penn Treebank and the larger set of lexical types from Implementations of GIS typically use a correction feature, but following Curran and Clark (2003) we do not use such a feature, which simplifies the algorithm. Table 3 also gives the results if automatically assigned POS tags are used in the training and testing phases, using the C & C POS tagger (Curran and Clark, 2003).  When compared with other supertag sets of automatically extracted lexicalized grammars, the (effective) size of our supertag set, 1,361 lexical entries, is between the CCG supertag set (398 categories) used by Curran and Clark (2003) and the LTAG supertag set (2920 elementary trees) used by Shen and Joshi (2003). Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al, 1994). Table 1 lists the contextual predicates used in our baseline system, which are based on those used in the Curran and Clark (2003) CCG supertagger. The tagger is very similar to the Maximum Entropy POS tagger described in Curran and Clark (2003). Here we use the Maximum Entropy models described in Curran and Clark (2003). Curran and Clark (2003) describes the model and explains how Generalised Iterative Scaling, together with a Gaussian prior for smoothing, can be used to set the weights. The supertagger in Curran and Clark (2003) finds the single most probable category sequence given the sentence, and uses additional features defined in terms of the previously assigned categories. The table gives results for gold standard POS tags and, in the final 2 columns, for POS tags automatically assigned by the Curran and Clark (2003) tagger. The CCG parser results are based on automatically assigned POS tags, using the Curran and Clark (2003) tagger. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model TNT tagger (Brants, 2000) and the maximum entropy C & C tagger (Curran and Clark, 2003). The modern Bible is tagged using the C & C maximum entropy tagger (Curran and Clark, 2003), and these tags are transferred from source to target through high-confidence alignments aquired from two alignment approaches. The C & C tagger (Curran and Clark, 2003) was trained on the Wall Street Journal texts in the Penn Treebank and then used to tag the NET Bible (the source text). However, it is unclear whether multi-POS tagging will be useful in this context, since our single-tagger POS tagger is highly accurate: over 97% for WSJ text (Curran and Clark,2003). Part-of-speech (POS) tagging is done using the C & C tagger (Curran and Clark, 2003a) and lemmatisation is done using morpha (Minnen et al, 2000). We use both rule-based and machine-learning named entity recognition (NER) components, the former implemented using LT-TTT2 and the latter using the C & C maximum entropy NER tagger (Curran and Clark, 2003b). We determine weights for the features with a modified version of the Generative Iterative Scaling algorithm (Curran and Clark, 2003). It is straightforward to apply this in tasks with token-based evaluation, such as part-of-speech tagging (Curran and Clark, 2003).
Learning To Recognize Features Of Valid Textual Entailments This paper advocates a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment. Current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text, using a locally decomposable matching score. We argue that there are significant weaknesses in this approach, including flawed assumptions of monotonicity and locality. Instead we propose a pipelined approach where alignment is followed by a classification step, in which we extract features representing high-level characteristics of the entailment problem, and pass the resulting feature vector to a statistical classifier trained on development data. We report results on data from the 2005 Pascal RTE Challenge which surpass previously reported results for alignment-based systems. Pado et al. (2009) uses Textual Entailment features extracted from the Standford Entailment Recognizer (MacCartney et al, 2006). Some authors have already designed similar matching techniques, such as the ones described in (MacCartney et al, 2006) and (Snow et al, 2006). Marsi and Krahmer (2005) and MacCartney et al (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al (2006) and Hickl and Bensley (2007). We have previously emphasized (MacCartney et al, 2006) that there is more to inferential validity than close lexical or structural correspondence: negations, modals, non-factive and implicative verbs, and other linguistic constructs can affect validity in ways hard to capture in alignment. Our system is based on the stage architecture of the Stanford RTE system (MacCartney et al, 2006), but adds a stage for event coreference decision. Based on this representation, we apply a two stage entailment process similar to MacCartney et al (2006) developed for textual entailment: an alignment stage followed by an entailment stage. Given the clause representation, we follow the idea similar to MacCartney et al (2006), and predict the entailment decision in two stages of processing: (1) an alignment model aligns terms in the hypothesis to terms in the conversation segment; and (2) an inference model predicts the entailment based on the alignment between the hypothesis and the conversation segment. We base our experiments on the Stanford RTE system which uses a staged architecture (MacCartney et al, 2006). Later systems that include more linguistic features extracted from resources such as WordNet have enjoyed more success (MacCartney et al, 2006). Many of these features are inspired by MacCartney et al (2006) and Snow et al (2006), but not as sophisticated. The Stanford Entailment Recognizer (MacCartney et al, 2006) is a stochastic model that computes match and mismatch features for each premise hypothesis pair. It has a three-stage architecture similar to the RTE system of MacCartney et al (2006). MacCartney et al (2006) describe a system for doing robust textual inference.
Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages We introduce a novel precedence reordering approach based on a dependency parser to statistical machine translation systems. Similar to other preprocessing reordering approaches, our method can efficiently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding. For a set of five subject-object-verb (SOV) order languages, we show significant improvements in BLEU scores when translating from English, compared to other reordering approaches, in state-of-the-art phrase-based SMT systems. Another similar work is that of (Xu et al, 2009). They created a pre-ordering rule set for dependency parsers from English to several SOV languages. It is true that tree-based reordering cannot cover all word movement operations in language translation, previous work showed that this method is still very effective in practice (Xu et al., 2009, Visweswariah et al, 2010). In addition, a pre-reorder system using manual rules as (Xu et al, 2009) is included for the English to-Japanese experiment (ManR-PR). Xu et al (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. Recently, Xu et al (2009) and Hong et al (2009) proposed rule-based preprocessing methods for SOV languages. Hong et al (2009) used Stanford parser (de Marneffe et al, 2006), which outputs semantic head based dependencies; Xu et al (2009) also used the same representation. Xu et al (2009) used a semantic head-based dependency parser for a similar purpose. Reversing the children order (Xu et al, 2009) reconnects is and popular. We obtained four more sets of alignments from the Berkeley aligner (BA) (Liang et al, 2006), the HMM aligner (HA) (Vogel et al, 1996), the alignment based on partial words (PA), and alignment based on dependency based reordering (DA) (Xu et al,2009). (Xu et al, 2009) showed that translation between subject-verb-object (English) and subject-object-verb (Pashto) languages can be improved by reordering the source side of the parallel data. Using the rules and algorithm described in (Xu et al, 2009) we reordered all of the source side and used GIZA++ to align the sentences. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-object verb language (Xu et al, 2009). Xu et al (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. The work by Xu et al (2009) is the closest to our approach. Our approach is a true tree-to-string model and differs from (Xu et al, 2009), which uses trees only as an intermediate representation to rearrange the original sentences. We use a superset of the reordering rules proposed by Xu et al (2009), which flatten a dependency tree into SOV word order that is suitable for all three languages. On Web text, Xu et al (2009) report significant improvements applying one set of hand-crafted rules to translation from English to each of five SOV languages: Korean, Japanese, Hindi, Urdu and Turkish. Following Collins et al (2005a) and Wang (2007), Xu et al (2009) showed that when translating from English to Japanese (and to other SOV languages such as Korean and Turkish) applying reordering as a preprocessing step that manipulates a source sentence parse tree can significantly outperform state-of-the-art phrase-based and hierarchical machine translation systems. During both training and testing, the system reorders source-language sentences in a preprocessing step using a set of rules written in the framework proposed by (Xu et al, 2009) that reorder an English dependency tree into target word order. R5 Superset of rules from (Xu et al., 2009).
A Comparison of Vector-based Representations for Semantic Composition In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods. We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication. Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora. We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection. The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method. Perhaps the most direct approach is to compute a weighted linear combination of the embeddings for words that appear in the document to be classified, as done in (Maas et al, 2011) and (Blacoe and Lapata, 2012). Blacoe and Lapata (2012) compare count and predict representations as input to composition functions. The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1). Blacoe and Lapata (2012) have an extensive comparison of the performance of various vector based models on this data set to which we compare our model in Table 5.  Following standard practice in paraphrase detection studies (e.g., Blacoe and Lapata (2012)), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues: word overlap between the two sentences and difference in sentence length. Our result stands in contrast with Blacoe and Lapata (2012), the only study we are aware of that compared a sophisticated composition model (Socher et al's 2011 model) to add and mult on realistic sentences, which attained the top performance with the simple models for both figures of merit they used.
A Boosting Algorithm For Classification Of Semi-Structured Text The focus of research in text classification has expanded from simple topic identification to more challenging tasks such as opinion/modality identification. Unfortunately, the latter goals exceed the ability of the traditional bag-of-word representation approach, and a richer, more structural representation is required. Accordingly, learning algorithms must be created that can handle the structures observed in texts. In this paper, we propose a Boosting algorithm that captures sub-structures embedded in texts. The proposal consists of i) decision stumps that use subtrees as features and ii) the Boosting algorithm which employs the subtree-based decision stumps as weak learners. We also discuss the relation between our algorithm and SVMs with tree kernel. Two experiments on opinion/modality classification confirm that subtree features are important. They adopted the BACT learning algorithm (Kudo and Matsumoto, 2004) to effectively learn subtrees useful for both antecedent identification and zero pronoun detection. The details of the algorithm and its efficient implementation are given in (Kudo and Matsumoto, 2004). In contrast, this boosting-based rule learner learns a final hypothesis that is a subset of candidate rules (Kudo and Matsumoto, 2004). For this problem, we can apply a boosting technique presented in (Kudo and Matsumoto, 2004). Note that for simplicity we use bag-of-functional words and their part-of-speech intervening between a zero-pronoun and its candidate antecedent as features instead of learning syntactic patterns with the Bact algorithm (Kudo and Matsumoto, 2004). posed by Kudo and Matsumoto (2004) is designed to learn subtrees useful for classification. Therefore, in order to identify word dependencies, we followed Kudo' s rule (Kudo and Matsumoto, 2004) the original sentence. This may be accomplished with SVMs 131 using a tree kernel, or the tree boosting classifier BACT described in (Kudo and Matsumoto, 2004). To exploit subtree features in our model, we use a subtree pattern mining method proposed by Kudo and Matsumoto (2004). Even though the approach in Kudo and Matsumoto (2004) and ours are similar, there are two clear distinctions. To learn subtree features, Kudo and Matsumoto (2004) assumed supervised data{ (x i, y i)}.  An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification, ranging from the use of co-occurrence with typical positive and negative words (Turney, 2002) to bag of words (Pang et al, 2002) and dependency structure (Kudo and Matsumoto, 2004). To solve the problem efficiently, we now adopt a variant of the branch-and-bound algorithm, similar to that described in (Kudo and Matsumoto, 2004). Among various classifier induction algorithms for tree-structured data, in our experiments, we have so far examined Kudo and Matsumoto (2004)'s algorithm, packaged as a free software named BACT. These features are organized as a tree structure and are fed into a boosting-based classification algorithm (Kudo and Matsumoto, 2004).
The Second International Chinese Word Segmentation Bakeoff The second international Chinese word segmentation bakeoff was held in the summer of 2005 to evaluate the current state of the art in word segmentation.Twenty three groups submitted 130 result sets over two tracks and four different corpora. We found that the technol ogy has improved over the intervening two years, though the out-of-vocabularyproblem is still or paramount impor tance. In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al, 2005), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower. The experiments of closed tests on the second SIGHAN Bakeoff (Emerson, 2005) show that the joint model significantly outperforms the baseline models of both generative and discriminative approaches. The corpora provided by the second SIGHAN Bakeoff (Emerson, 2005) were used in our experiments. For more detailed information on the corpora, refer to Emerson (2005). Four training and testing corpora were used in the second bakeoff (Emerson, 2005), including the Academia Sinica corpus (AS), the Hong Kong City University Corpus (CU), the Peking University Corpus (PK) and the Microsoft Research Corpus (MR). We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al (2006) for comparison. SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics, successfully conducted four prior word segmentation bakeoffs, in 2003 (Sproat and Emerson, 2003), 2005 (Emerson, 2005), 2006 (Levow, 2006) and 2007 (Jin and Chen, 2007), and the bakeoff 2007 was jointly organized with the Chinese Information Processing Society of China (CIPS). In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al, 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower. For detailed info. of the corpora and these scores, refer to (Emerson, 2005). SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics, conducted two prior word segmentation bakeoffs, in 2003 and 2005 (Emerson, 2005), which established benchmarks for word segmentation against which other systems are judged. As an evidence, the CWS evaluation campaign, the Sighan Bakeoff (Emerson, 2005) has been held four times since 2004. These specs were used in the second Sighan Bakeoff (Emerson, 2005). The current state-of-the-art segmentation software developed by (Low et al, 2005), which ranks as the best in the SIGHAN bakeoff (Emerson, 2005), attains word precision and recall of 96.9% and 96.8%, respectively, on the PKU track. We used the data provided by the second SIGHAN Bakeoff (Emerson, 2005) to test the two segmentation models. In the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model. The data used was the Microsoft Research Beijing corpus from the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005), and we used the same train/test split used in the competition. We did not explicitly test the utility of CRF-type features for improving recall on out-of-vocabulary items, but we note that in the Bakeoff, the model of Tseng et al (2005), which was very similar to our CRF-only system (only containing a few more feature templates), was consistently among the best performing systems in terms of test OOV recall (Emerson, 2005). After analyzing the results presented in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we created a new Chinese word segmentation system named as 'Achilles' that consists of four modules mainly: Regular expression extractor, dictionary-based N-gram segmentation, CRF-based subword tagging (Zhang et al, 2006), and confidence-based segmentation. We conduct experiments on the SIGHAN 2003 (Sproat and Emerson, 2003) and 2005 (Emerson, 2005) bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm. The second benchmark that we adopted is the SIGHAN Bakeoff-2005 dataset (Emerson, 2005) for Chinese word segmentation.
Chunking With Support Vector Machines We apply Support Vector Machines (SVMs) to identify English base phrases (chunks). SVMs are known to achieve high generalization performance even with input data of high dimensional feature spaces. Furthermore, by the Kernel principle, SVMs can carry out training with smaller computational overhead independent of their dimensionality. We apply weighted voting of 8 SVMsbased systems trained with distinct chunk representations. Experimental results show that our approach achieves higher accuracy than previous approaches. We use a support vector machine (SVM) based chunker yamcha (Kudo and Matsumoto, 2001) for the chunking process. We used the chunker yamcha (Kudo and Matsumoto,2001), which is based on support vector machines (Vapnik, 1998). On the chunking task, the bagged model also outperforms the models of Kudo and Matsumoto (2001).  We use the chunker YamCha (Kudo and Matsumoto, 2001). We employ the technique of Support Vector Machines (SVMs) (Vapnik, 1998) as the machine learning technique, which has been successfully applied to various natural language processing tasks including chunking tasks such as phrase chunking (Kudo and Matsumoto, 2001) and named entity chunking (Mayfield et al, 2003). In this paper, we use an SVMs-based chunking tool YamCha (Kudo and Matsumoto, 2001). We use a Support Vector Machines-basedchunker, YamCha (Kudo and Matsumoto, 2001), to extract unknown words from the output of the morphological analysis. For the formulation of SVMs in the context of NLP applications, see (Kudo and Matsumoto, 2001). As regards the basic feature set for Chunking, we followed (Kudo and Matsumoto, 2001), which is the same feature set that provided the best result in CoNLL-2000. With Chunking, (Kudo and Matsumoto, 2001) reported the best F-score of 93.91 with the voting of several models trained by Support Vec tor Machine in the same experimental settings and with the same feature set. The model used to obtain the SVM baseline for concept classification was trained using YamCHA (Kudo and Matsumoto, 2001). Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al, 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al, 2004). See, for example, NPchunkers utilizing conditional random fields (Sha and Pereira, 2003) and support vector machines (Kudo and Matsumoto, 2001). We used YamCha (Kudo and Matsumoto, 2001) as a text chunker, which is based on Support Vector Machine (SVM). Therefore, we introduce a Support Vector Machine (below SVM)-based chunker (Kudo and Matsumoto, 2001) to cover the errors made by the segmenter. Wu et al (2007, 2008) presented an automatic chunk pair relation construction algorithm which can handle so-called IOB1/IOB2/IOE1/IOE2 (Kudo and Matsumoto, 2001) chunk representation structures with either left-to-right or right-to left directions. Sha and Pereira (2003) reported the Kudo and Matsumoto (2001) performance on the NP-Chunking task to be 94.39 and to be the best reported result on this task. His system is an extension of Kudo's chunking system (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks. His system is an extension of Kudo's chunking system (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks.
Tree Kernel-Based Relation Extraction with Context-Sensitive Structured Parse Tree Information This paper proposes a tree kernel with contextsensitive structured parse tree information for re lation extraction. It resolves two critical problems in previous tree kernels for relation extraction in two ways. First, it automatically determines a dynamic context-sensitive tree span for relation ex traction by extending the widely-used Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT. Second, it pro poses a context-sensitive convolution tree kernel, which enumerates both context-free and context sensitive sub-trees by considering their ancestor node paths as their contexts. Moreover, this paper evaluates the complementary nature between our tree kernel and a state-of-the-art linear kernel. Evaluation on the ACE RDC corpora shows that our dynamic context-sensitive tree span is much more suitable for relation extraction than SPT and our tree kernel outperforms the state-of-the-art Collins and Duffy?s convolution tree kernel. It also shows that our tree kernel achieves much bet ter performance than the state-of-the-art linear kernels . Finally, it shows that feature-based and tree kernel-based methods much complement each other and the composite kernel can well integrate both flat and structured features. In the news domain, the best reported results on the ACE dataset have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al, 2007). As shown in Zhou et al (2007), the context path from root to the phrase node is an effective context information feature. In this paper, we use the same settings in (Zhou et al, 2007), i.e., each phrase node is enriched with its context paths of length 1, 2, 3. We compare our method with the standard convolution tree kernel (CTK) on the state-of-the-art context sensitive shortest path-enclosed tree representation (CSPT, Zhou et al, 2007).  To resolve this problem, Zhou et al (2007) took the ancestral information of sub-trees into consideration.  Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. Zhou et al (2007) use a context sensitive kernel in conjunction with features they used in their earlier publication (GuoDong et al., 2005). To the best of our knowledge, the most recent result was reported by (Zhou and Zhu, 2011), who extended their previous work in (Zhou et al, 2007).  Zhou et al (2007) tested their system on the ACE 2004 data. Zhou et al (2007) proposed the so called context sensitive tree kernel approach based on PST, which expands PET to include necessary contextual in formation. Zhou et al (2007) further extend it to Context-Sensitive Shortest Path-enclosed Tree (CS SPT), which dynamically includes necessary predicate-linked path information. Zhou et al (2007) point out that both SPT and the convolution tree kernel are context-free. Zhou et al (2007) describe a composite kernel to integrate a context-sensitive CTK and a state-of-the-art linear kernel. Zhou et al (2007) further propose Context-Sensitive SPT (CS-SPT), which can dynamically determine the tree span by extending the necessary predicate-linked path information outside SPT. (2) CS-SPT only captures part of context sensitive information relating to predicate-linked structure (Zhou et al, 2007) and still loses much context-sensitive information. In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. Furthermore, when the UPST (FPT) kernel is combined with a linear state-of-the-state feature based kernel (Zhou et al, 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al (2007) (i.e. polynomial degree d=2 and coefficient? =0.3), we get the so far best performance of 77.1 in F-measure for 7relation types on the ACE RDC 2004 data set.
Scaling Web-Based Acquisition Of Entailment Relations Paraphrase recognition is a critical step for natural language interpretation. Accordingly, many NLP applications would benefit from high coverage knowledge bases of paraphrases. However, the scalability of state-of-the-art paraphrase acquisition approaches is still limited. We present a fully unsupervised learning algorithm for Web-based extraction an extended model of paraphrases. We focus on increased scalability and generality with respect to prior work, eventually aiming at a full scale knowledge base. Our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the Web for related syntactic entailment templates. Experiments show promising results with respect to the ultimate goal, achieving much better scalability than prior Web-based methods. Moreover, the TEASE collection of entailment rules (Szpektor et al, 2004) consists of 136 templates provided as input, plus all the learned templates.  TEASE (Szpektor et al, 2004) discovers binary relation templates from the Web base don sets of representative entities for given binary relation templates. Szpektor et al (2004) applies a similar, with no seed lists, to extract automatically entailment relationships between verbs, and Etzioni et al (2005) report very good results extracting Named Entities and relationships from the web. Lin and Pantel (2001) and Szpektor et al (2004) proposed methods to obtain entailment templates by using a single monolingual resource. DIRT (Lin and Pantel, 2001) and TEASE (Szpektor et al, 2004) report accuracies of50.1% and 44.3% respectively compared to our average accuracy across two annotators of 70.79%. In this paper we use TEASE (Szpektor et al, 2004), a state of-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules.  TEASE (Szpektor et al., 2004) discovers dependency sub-parses from the Web, based on sets of representative entities for a given lexical item. Yet the current precision of acquisition algorithms is typically still mediocre, as illustrated in Table 1 for Dirt (Lin and Pantel, 2001) and TEASE (Szpektor et al, 2004), two prominent acquisition algorithms whose outputs are publicly available. Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 (Barzilay and Lee, 2003) and 0.55 0.63 (Szpektor et al, 2004). We applied the instance-based methodology to evaluate two state-of-the-art unsupervised acquisition algorithms, DIRT (Lin and Pantel, 2001) and TEASE (Szpektor et al, 2004), whose output is publicly available. Szpektor et al (2004) describe the TEASE method for extracting entailing relation templates from the Web. Other types of relations that have been studied by pattern-based approaches include question answer relations (such as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al, 2003), general purpose analogy (Turney et al, 2003), verb relations (including similarity, strength, antonym, enable ment and temporal) (Chklovski and Pantel, 2004), entailment (Szpektor et al, 2004), and more specific relations, such as purpose, creation (Cimiano and Wenderoth, 2007), LivesIn, and EmployedBy (Bunescu and Mooney, 2007). Many recent efforts have also focused on extracting semantic relations between entities, such as entailments (Szpektor et al 2004), is-a (Ravichandran and Hovy 2002), part-of (Girju et al 2006), and other relations. Similar methods have also been used by Ibrahim et al (2003) and Szpektor et al (2004). For instance, the precisions of the para phrase patterns reported in (Lin and Pantel, 2001), (Ibrahim et al, 2003), and (Szpektor et al, 2004) are lower than 50%. On-line usage of web queries is less frequent and was used mainly in semantic acquisition applications: the discovery of semantic verb relations (Chklovski and Pantel, 2004), the acquisition of entailment relations (Szpektor et al, 2004), and the discovery of concept-specific relationships (Davidov et al, 2007). Next, we implemented a prototype that utilizes a state-of-the-art method for learning entailment relations from the web (Szpektor et al, 2004), the Minipar dependency parser (Lin, 1998) and a syntactic matching module. Taking a step further, the TEASE algorithm (Szpektor et al, 2004) provides a completely unsupervised method for acquiring entailment relations from the Web for a given input relation (see Section 5.1).
An Improved Extraction Pattern Representation Model For Automatic IE Pattern Acquisition Several approaches have been described for the automatic unsupervised acquisition of patterns for information extraction. Each approach is based on a particular model for the patterns to be acquired, such as a predicate-argument structure or a dependency chain. The effect of these alternative models has not been previously studied. In this paper, we compare the prior models and introduce a new model, the Subtree model, based on arbitrary subtrees of dependency trees. We describe a discovery procedure for this model and demonstrate experimentally an improvement in recall using Subtree patterns. Sudo et al (2003) acquired subtrees derived from dependency trees as extraction rules for IE in general domains.  The idea of a self-customizing IE system emerged recently with the improvement of pattern acquisition techniques (Sudo et al, 2003b), where the IE system customizes itself across domains given by the user's query. (Sudo et al, 2003a) consists of three phases to learn extraction patterns from the source documents for a scenario specified by the user. In other closely related work, Sudo et al (2003) use frequent dependency subtrees as measured by TF*IDF to identify named entities and IE patterns important for a given domain. Following (Sudo et al, 2003) we are interested only in the lexemes which are near neighbors of the most frequent verbs. Sudo et al (2003) evaluated how well their IE patterns captured named entities of three predefined types. In addition, Sudo et al (2003) proposed representations for IE patterns which extends the SVO representation used here and, while they did not appear to significantly improve IE, it is expected that it will be straightforward to extend the vector space model to those pattern representations. For example, Yangarber (2003) uses just subject-verb-object tuples while Sudo et al (2003) allow any subpart of the tree to act as an extraction pattern. Sudo et al (2003) compared three models in terms of their ability to identify event participants.  Subtrees: The final model to be considered is the subtree model (Sudo et al, 2003). Sudo et al (2003) extract dependency subtrees within relevant documents as IE patterns.  The subtree model considers all subtrees as pattern candidates (Sudo et al, 2003).  For example, Sudo et al (2003) used patterns consisting of a path from a verb to any of its descendents (direct or indirect) while Bunescuand Mooney (2005) suggest the shortest path between the items being related. An additional advantage of linked chain patterns is that they do not cause an unwieldy number of candidate patterns to be generated unlike some other approaches for representing extraction patterns, such as the one proposed by Sudo et al (2003) where any subtree of the dependency tree can act as a potential pattern. Given a dependency parse tree, any sub-tree can be a candidate template, setting some of its nodes as variables (Sudo et al, 2003). Three classes of syntactic template learning approaches are presented in the literature: learning of predicate argument templates (Yangarber et al, 2000), learning of syntactic chains (Lin and Pantel, 2001) and learning of sub-trees (Sudo et al, 2003).
Loosely Tree-Based Alignment For Machine Translation We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length. This is done by adding a new subtree cloning operation to either tree-to-string or tree-to-tree alignment algorithms. Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. Initially work focused on word-based alignment, but more recent research also addresses alignment at the higher levels (substrings, syntactic phrases or trees), e.g., [Gildea, 2003].  However, even after extending this model by allowing cloning operations on subtrees, Gildea (2003) found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model than with a tree-to-tree model using two trees. Additional linguistic knowledge sources such as dependency trees or parse trees were used in (Cherry and Lin, 2003) and (Gildea, 2003). Daniel Gildea (2003) dealt with the problem of the parse tree isomorphism with a cloning operation to either tree-to-string or tree-to-tree alignment models. Gildea (2003) trained a system on parallel constituent trees from the Korean-English Treebank, evaluating agreement with hand-annotated word alignments. Our model of alignment is that of Gildea (2003), reviewed in Section 2 and extended to dependency trees in Section 3. Both our constituent and dependency models make use of the "clone" operation introduced by Gildea (2003), which allows words to be aligned even in cases of radically mismatched trees, at a cost in the probability of the alignment. Syntax-based translation models, such as tree-to-string model (Yamada and Knight, 2001) and tree-to-tree model (Gildea, 2003), may be very suitable to be added into log-linear models. Works that apply the TTT model include Gildea (2003) and Zhang et al (2008). For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al (2008). We began with the tree-to-tree alignment model presented by Gildea (2003). (Gildea, 2003) and (Galley et al, 2004) discuss different ways of generalizing the tree-level cross linguistic correspondence relation, so it is not confined to single tree nodes, thereby avoiding a continuity assumption. (Gildea, 2003) performs tree-to-tree alignment, but treats it as part of a generative statistical translation model, rather than a seperate task. (Gildea, 2003) outlines an algorithm for use in syntax-based statistical models of MT, applying a statistical TSG with probabilities parameterized to generate the target tree conditioned on the structure of the source tree. However, unlike (Gildea, 2003), we treat the problem of alignment as a seperate task rather than as part of a generative translation model. Initially work focused on word-based alignment, but more and more work is also addressing alignment at the higher levels (substrings, syntactic phrases or trees), e.g., (Meyers et al, 1996), (Gildea, 2003). Yamada and Knight (2001) introduced tree-to-string alignment on Japanese data, and Gildea (2003) performed tree-to-tree alignment on the Korean Treebank, allowing for non-isomorphic structures; he applied this to word-to-word alignment. The problem of making use of syntactic trees for alignment (and translation), which is the object of our second alignment model has already received some attention, notably by (Yamada and Knight, 2001) and (Gildea, 2003).
SPMT: Statistical Machine Translation With Syntactified Target Language Phrases We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5. Galley et al.(2006) propose one solution to this problem and Marcu et al (2006) propose another, both of which we explore in Sections 5.1 and 5.2. An alternative for extracting larger rules called SPMT model 1 is presented by Marcu et al (2006). This solution requires larger applicability contexts (Marcu et al, 2006). Addressing the issues in Galley et al (2006), Marcu et al (2006) create an x Rs rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multi headed syntactic structure. Unlike previous work, our solution neither requires larger applicability contexts (Galley et al, 2006), nor depends on pseudo nodes (Marcu et al, 2006) or auxiliary rules (Liu et al, 2007). We restrict the target side to the so called well formed dependency structures, in order to cover a large set of non-constituent transfer rules (Marcu et al., 2006), and enable efficient decoding through dynamic programming. Rule Coverage Marcu et al (2006) showed that many useful phrasal rules can not be represented as hierarchical rules with the existing representation methods, even with composed transfer rules (Galley et al, 2006). (Marcu et al, 2006) and (Galley et al, 2006) introduced artificial constituent nodes dominating the phrase of interest. Inside the Moses toolkit, three different statistical approaches have been implemented: phrase based statistical machine translation (PB SMT) (Koehn et al 2003), hierarchical phrase based statistical machine translation (Chiang,2007) and syntax-based statistical machine translation (Marcu et al 2006). Moreover, syntax-based approaches often suffer from the rule coverage problem since syntactic constraints rule out a large portion of non syntactic phrase pairs, which might help decoders generalize well to unseen data (Marcu et al,2006). Syntactic analysis of texts (such as Part-OfSpeech tagging and syntactic parsing) is an ex ample of such a generic analysis, and has proved useful in applications ranging from machine translation (Marcu et al, 2006) to text mining in the bio-medical domain (Cohen and Hersh, 2005). Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even out perform them in some cases (Marcu et al, 2006). Following Galley et al (2006)' s work, Marcu et al (2006) proposed SPMT models to improve the coverage of phrasal rules, and demonstrated that the system performance could be further improved by using their proposed models. As shown in the following parts of this paper, it works very well with the existing techniques, such as rule com posing (Galley et al, 2006), SPMT models (Marcu et al, 2006) and rule extraction with k best parses (Venugopal et al, 2008). In addition to GHKM extraction, the SPMT models (Marcu et al., 2006) are employed to obtain phrasal rules that are not covered by GHKM extraction. In this system, both of minimal GHKM (Galley et al, 2004) and SPMT rules (Marcu et al., 2006) are extracted from the bilingual corpus, and the composed rules are generated by com posing two or three minimal GHKM and SPMT rules. Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al, 1993) corpora, where all nonterminals share the same generic label X. InGalley et al (2004) and Marcu et al (2006), tar get language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al (2006) use source language parse trees instead. The base feature set used for all systems is similar to that used in (Marcu et al 2006), including 14 base features in total such as 5-gram language model, bidirectional lexical and phrase based translation probabilities. Zollmann and Venugopal (2006) and Marcu et al (2006) used broken syntactic fragments to augment their grammars to increase the rule coverage. We build two translation systems: One using tree-based models without additional linguistic annotation, which are known as hierarchical phrase based models (Chiang, 2005), and another system that uses linguistic annotation on the target side, which are known under many names such as string-to-tree models or syntactified target models (Marcu et al, 2006).
Three Heads Are Better Than One  Other approaches combine lattices or N-best lists from several different MT systems (Frederking and Nirenburg, 1994). that is hopefully better than any of its ingredients, an idea pionieered in (Frederking and Nirenburg, 1994). Frederking and Nirenburg (1994) produced the first MEMT system by combining outputs from three different MT engines based on their knowledge of the inner workings of the engines. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), named entity recognition (Borthwick et al, 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al, 2003). Now back in machine translation, we do find some work addressing such concern: Frederking and Nirenburg (1994) develop a multi-engine MT or MEMT architecture which operates by combining outputs from three different engines based on the knowledge it has about inner workings of each of the component engines. Brown and Frederking (1995) is a continuation of Frederking and Nirenburg (1994) with an addition of a n-gram based mechanism for a candidate selection. The first example of this approach was the multi-engine MT system (Frederking and Nirenburg, 1994), which builds a chart using the translation units inside each input system and then uses a chart walk algorithm to find the best cover of the source sentence. Rosti et al (2007a) collect source-to-target correspondences from the input systems, create a new translation option table using only these phrases, and re-decode the source sentence to generate better translations. System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994). In Machine Translation (MT), there is a long tradition of combining multiple machine translations, as through a Multi-Engine MT (MEMT) architecture; the origins of this are generally credited to Frederking and Nirenburg (1994). System combination procedures, on the other hand, generate translations from the output of multiple component systems by combining the best fragments of these outputs (Frederking and Nirenburg, 1994). It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). Combinations of MT systems into multi-engine architectures have a long tradition, starting perhaps with (Frederking and Nirenburg, 1994).
Max-Margin Parsing We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines. Our formulation uses a factorization analogous to the standard dynamic programs for parsing. In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates. Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness. We provide an efficient algorithm for learning such models and show experimental evidence of the model’s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar. (Taskar et al., 2004) suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems. Previous work has also used surface features in their parsers, but the focus has been on machine learning methods (Taskar et al, 2004), latent annotations (Petrov and Klein, 2008a; Petrov and Klein, 2008b), or implementation (Finkel et al, 2008). We use the terminology in (Taskar et al, 2004) for a generic structured output prediction, and define a part. We follow Taskar et al (2004) and Turian and Melamed (2005) in training and testing on? 15word sentences in the English Penn Treebank (Taylor et al, 2003). To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences.   It is expensive to train the MF approximation on the whole WSJ corpus, so instead we use only sentences of length at most 15, as in (Taskar et al, 2004) and (Turian and Melamed, 2006). An other interesting model for parsing re-ranking based on tree kernel is presented in (Taskar et al, 2004). A refinement of such technique was presented in (Taskar et al, 2004). Taskar et al (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. For example, Taskar et al (2004) took several months to train on the 15 word sentences in the English Penn Treebank (Dan Klein, p.c.). We follow Taskar et al (2004) in training and testing on 15 word sentences in the English Penn Treebank (Taylor et al, 2003). To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences.  Collins and Roark (2004) and Taskar et al (2004) beat the generative baseline only after using the standard trick of using the output from a generative model as a feature. Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features (Taskar et al, 2004), giving the benefit of some input features integrally in our dynamic program. In re ranking, one can incorporate any such features, of course, but even in our dynamic programming approach it is possible to include features that decompose along the dynamic program structure, as shown by Taskar et al (2004). We use non-local span features, which condition on properties of input spans (Taskar et al, 2004). This is the approach taken by Taskar et al (2004), but their approach assumes that the loss function can be decomposed into local loss functions.
A Fast And Portable Realizer For Text Generation Systems   These realizers, along with RealPro (Lavoie and Rambow, 1997), accept tense as a parameter, but do not calculate it from a semantic representation of overlapping time intervals such as ours (though the Nigel grammar can calculate tense from speech, event, and reference time orderings, discussed below). We adopt Deep Syntactic Structures (DSyntSs) as a format for syntactic structures because they can be realized by the fast portable realizer RealPro (Lavoie and Rambow, 1997).  In this sense, they are functionally similar to the REALPRO system (Lavoie and Rambow, 1997). We automatically converted the phrase structure output of the Collins parser into the syntactic dependency representation used by our syntactic realizer, RealPro (Lavoie and Rambow, 1997). Discourse relations are essentially rhetorical structure theory (RST) relations [Mann and Thompson, 1987], and messages are represented using a deep-syntactic representation, which is loosely based on RealPro [Lavoie and Rambow, 1997]. Currently, FLO supports the LinGOrealiser (Carrollet al, 1999), but we are also looking at FLO modules for RealPro (Lavoie and Rambow, 1997) and FUF/SURGE (Elhadad et al, 1997). In the second phase, the sentence plan ranker (SPR) ranks sentence plans generated by the SPG, and then selects the top-ranked out put as input to the surface realizer, RealPro (Lavoie and Rambow, 1997). It passes propositions and a goal to the sentence-level realization module which uses templates to build the deep syntactic structures required by the RealPro realizer (Lavoie and Rambow, 1997) for generating a string that communicates the goal. As rightly pointed out by Belz (2008), traditional wide coverage realizers such as KPML (Bateman et al, 2005), FUF/SURGE (Elhadad and Robin, 1996) and RealPro (Lavoie and Rambow, 1997), which were also intended as off-the-shelf plug-in realizers still tend to require a considerable amount of work for integration and fine-tuning of the grammatical and lexical resources.  Realization is achieved with the RealPro surface realizer (Lavoie and Rambow, 1997). The top ranked sentence plan output by the SPR is input to the RealPro surface realizer which produces a surface linguistic utterance (Lavoie and Rambow, 1997). Finally, surface realization is performed by interfacing RealPro (Lavoie and Rambow, 1997) with a language model. The realizer takes each sentence in the story and reformulates it into input compatible with the RealPro (Lavoie and Rambow,1997) text generation engine. The top-ranked candidateis selected for presentation and verbalized using a language model interfaced with RealPro (Lavoie and Rambow, 1997), a text generation engine. The surface realization process is performed by RealPro (Lavoie and Rambow (1997)). Over the past several years, a significant consensus has emerged over the definition of the realisation task, through the development of realisers such as REALPRO (Lavoie and Rambow, 1997), ALETH GEN (Coch, 1996), KPML (Bateman, 1997), FUF/SURGE (Elhadad and Robin, 1996), HALO GEN (Langkilde, 2000), YAG (McRoy et al, 2000), and OPENCCG (White, 2006). Realisation involves two logically distinguishable tasks. Figure 2 illustrates a DSyntS from a meteorological application, MeteoCogent (Kittredge and Lavoie, 1998), represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework (Lavoie and Rambow, 1997).
Online Learning of Relaxed CCG Grammars for Parsing to Logical Form We consider the problem of learning toparse sentences to lambda-calculus repre sentations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG). A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar?for example allowing flexible word order, or insertion of lexical items? with learned costs. We also present a new, online algorithm for inducing a weighted CCG. Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match figure reported by He and Young (2006). We used the dataset introduced in Zettlemoyer and Collins (2007) and automatically converted their lambda-calculus expressions to attribute-value pairs following the conventions adopted by Liang et al (2009). Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. In order to provide an initial starting point, we initialize the weight vector using a similar procedure to the one used in (Zettlemoyer and Collins, 2007) to set weights for three features and a bias term. This can create problems for applications based on CCG, e.g. for the induction of stochastic CCGs from text annotated with logical forms (Zettlemoyerand Collins, 2007), where spreading probability mass over equivalent derivations should be avoided.  For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. We evaluated GUSP on end-to-end question answering using the ATIS dataset for semantic parsing (Zettlemoyer and Collins, 2007). For example, Zettlemoyer and Collins (2007) used a predicate from (f, c) to signify that flight f starts from city c. Our starting point is the work done by Zettlemoyer and Collins on parsing using relaxed CCG grammars (Zettlemoyer and Collins, 2007) (ZC07). Practically, the grammar relaxation is done via the introduction of non-standard CCG rules (Zettlemoyer and Collins, 2007). We use the set developed by Zettlemoyer and Collins (2007), which includes features that are sensitive to lexical choices and the structure of the logical form that is constructed. See the discussion by Zettlemoyer and Collins (2007) (ZC07) for the full details.  The later work of Zettlemoyer and Collins (2007), also uses hand crafted rules. The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008). We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. We follow the setup of Zettlemoyerand Collins (2007) where possible, including feature design, initialization of the semantic parser, and evaluation metrics, as reviewed below.  For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples.
Get Out The Vote: Determining Support Or Opposition From Congressional Floor-Debate Transcripts We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation. To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another. We find that the incorporation of such information yields substantial improvements over classifying speeches in isolation. In (Thomas et al, 2006), the authors use the transcripts of debates from the US Congress to automatically classify speeches as supporting or opposing a given topic by taking advantage of the voting records of the speakers. votes on the bill under discussion (Thomas et al, 2006). (Thomas et al, 2006), or personal preferences for topics (Grimmer, 2009) would enrich the model and better illuminate the interaction of influence and topic. Thomas et al (2006) achieved accuracies of 71.3% by using speaker agreement information in the graph-based MinCut/Maxflow algorithm, as compared to accuracies around 70% via an an SVM classifier operating on content alone. The same applies to the task of subgroup detection (as done by (AbuJbara et al., 2012), (Anand et al, 2011) or (Thomas et al, 2006)) . In order to produce a finer-grained model of positions, we want to develop a model that places positions stated in text along a one-dimensional scale, as done by (Slapin and Proksch, 2008) with their system called Wordfish, (Gabel and Huber, 2000), (Laver and Garry, 2000), (Laver et al, 2003) or (Sim et al, 2013). Instead of selecting sentences from the manifesto that cover a topic, the position could be extracted from the manifesto using topic models, as shown in (Thomas et al, 2006) and (Gerrish and Blei, 2011). Our second dataset is taken from segments of speech from United States Congress floor debates, first introduced by Thomas et al (2006). However, like other cascaded approaches (e.g., Thomas et al (2006), Mao and Lebanon (2006)), it can be difficult to control how errors propagate from the sentence-level subtask to the main document classification task. We also use the U.S. Congressional floor debates transcripts from Thomas et al (2006). For our experiments, we evaluate our methods using the speaker based speech-segment classification setting as described in Thomas et al (2006). Datasets in the required format for SVMsle are available at http: //www.cs.cornell.edu/ ?ainur/data.html. In the other setting described in Thomas et al (2006) (segment-based speech-segment classification), around 39% of 1051 Table 1: Summary of the experimental results for the Movie Reviews (top) and U.S. Congressional Floor Debates (bottom) datasets using SVMsle, SVMslew/ Prior and SVMslefs with and without proximity features.  Thomas et al (2006) presented a method based on support vector machines to determine whether the speeches made by participants represent support or opposition to proposed legislation, using transcripts of U.S. congressional floor debates. Our aggregation technique does, however, presuppose consistency of opinions, in a similar way to Thomas et al (2006). Thomas et al 2006 address the same problem of determining support and opposition as applied to congressional floor-debates. The first baseline is based on the work of (Thomas et al 2006). We used the speaker agreement component presented in (Thomas et al 2006) as a baseline. Other work that has considered different discourse functions in sentiment analysis, have experimented on detecting arguments (Somasundaran et al, 2007) and the stance of political debates (Thomas et al, 2006). Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al (2006), Bansal et al (2008), Burfoot et al (2011), and Anand et al (2011) proposed methods to recognize stances in on line debates. Burfoot et al, (2011) builds on the work of (Thomas et al, 2006) and proposes collective classification using speaker contextual features (e.g., speaker intentions based on vote labels).
Forest Reranking: Discriminative Parsing with Non-Local Features reranking techniques ofsuffer from the limited scope of the best list, which rules out many potentially alternatives. We instead propose a method that reranks a packed forest of exponentially many parses. Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank. Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.  All those methods fall short of re ranking parsers like Charniak and Johnson (2005) and Huang (2008), which, however, have access to many additional features, that can not be used in our dynamic program. From the presented data, we can see that indirect re ranking on LAS may not seem as good as direct re ranking on phrase-structures compared to F-scores obtained in (Charniak and Johnson, 2005) and (Huang, 2008) with one parser or (Zhang et al., 2009) with several parsers. The combination of n-best lists would not scale up and working on the ambiguous structure itself, the packed forest as in (Huang, 2008), might be necessary. A common objection to re ranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate (Huang, 2008). See Huang (2008) for more details. However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance.  With the ability to incorporate non-local phrase-structure parse features (Huang, 2008), we can recognize dependency features of arbitrary order. That is, not only is phrase+deps better at dependency recovery than its component parts, but phrase+deps+gen is also considerably better on dependency recovery than phrase+gen, which represents the previous state-of-the-art in this vein of research (Huang, 2008). Note that our model phrase+gen uses essentially the same features as Huang (2008), so the fact that our phrase+gen is noticeably more accurate on F1 is presumably due to the benefits in reduced feature under-training achieved by the MERT combination strategy.  Huang (2008) proposed to use a parse forest to incorporate non-local features. Our parser achieved an f-score of 88.4on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005).  In the second pass, we use the hyper graph reranking algorithm (Huang, 2008) to find promising translations using additional dependency features (i.e., features 810 in the list). We follow Mi and Huang (2008) to estimate the fractional count of a rule extracted from an aligned forest pair. Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyper edge away from the globally best derivation. For example, consider the probabilistic CKY algorithm as above, but using the cube decoding semiring with the non-local feature functions collectively known as "NGramTree" features (Huang, 2008) that score the string of terminals and nonterminals along the path from word j to word j+1 when two constituents CY, i, j and CZ, j, k are combined.
Temporal Ontology And Temporal Reference amp;quot;Two weeks later, Bonadea had already been his lover for a fortnight.&quot; Musil, Mann ohne Eigenschaften. A semantics of temporal categories in language and a theory of their use in defining the temporal relations between events both require a more complex structure on the domain underlying the meaning representations than is commonly assumed. This paper proposes an ontology based on such notions as causation and consequence, rather than on purely temporal primitives. A central notion in the ontology is that of an elementary event-complex called a &quot;nucleus.&quot; A nucleus can be thought of as an association of a goal event, or &quot;culmination,&quot; with a &quot;preparatory process&quot; by which it is accomplished, and a &quot;consequent state,&quot; which ensues. Natural-language categories like aspects, futurates, adverbials, and when-clauses are argued to change the temporal/aspectual category of propositions under the control of such a nucleic knowledge representation structure. The same concept of a nucleus plays a central role in a theory of temporal reference, and of the semantics of tense, which we follow McCawley, Partee, and Isard in regarding as an anaphoric category. We claim that any manageable formalism for naturallanguage temporal descriptions will have to embody such an ontology, as will any usable temporal database for knowledge about events which is to be interrogated using natural language. Forward and backward shifts correspond to introducing the consequence or preparatory phases of events [Moens and Steedman, 1988]. Every frame is represented as an example sentence, a syntactic structure and a semantic structure containing semantic predicates and their arguments and temporal information in a way similar to Moens and Steedman (1988). For example, Sue played the piano is nonculminated, while Sue played the sonata signifies a culminated event (this example comes from Moens and Steedman (1988)). The perfect is analyzed by using the notion of a nucleus (Moens and Steedman, 1988) to account for the inner structure of an eventuality.  Presuming a tripartite event structure (Moens and Steedman, 1988) consisting of a preparation phase (dynamic phase theta dyn), a culmination point (boundary tau) and a consequent state (static phase phi stat), there are three possibilities for aspect to select. We decompose each event E into a tripartite structure in a manner similar to Moens and Steedman (1988), introducing a time function for each predicate to specify whether the predicate is true in the preparatory (during (E)), culmination (end (E)), or consequent (result: (E)) stage of an event. Sony Corp. has heavily promoted the Video Walkman since the product's introduction last summer, (2) but Bob Gerson, video editor of This Week in Consumer Electronics, says (3) Sony conceives of 8 mm as a family of products, camcorders and VCR decks, SE classification is a fundamental component in determining the discourse mode of texts (Smith, 2003) and, along with aspectual classification, for temporal interpretation (Moens and Steedman, 1988). Moens and Steedman (1988), Smith (1991), Pustejovsky (1995), and others argue that it is a defining property of telic events. In addition, each predicate has a time function to show at what stage of the event the predicate holds true, in a manner similar to the event decomposition of Moens and Steedman (1988). Moens and Steedman (1988) describes temporal expressions relating to changes of state. Consider the following examples, from Ritchie (1979) and Moens and Steedman (1988). An obvious first step, which we are currently working on, is to include a linguistically motivated temporal ontology (Moens and Steedman,1988), which will be separate from the existing do main ontology. It is suggested that when clauses, by contrast, do not implicate inceptiveness; indeed they do not have any special temporal implicatures (cf. Moens and Steedman 1988). However, it can be argued that when does have a. The latter would accord with a theoretical distinction that has been made between post posed when expressing a purely temporal relation between the two clauses, and proposed when ex pressing a contingent relation between them (Moens and Steedman, 1988). Each semantic predicate in VerbNet also include a time function specifying whether the predicate is true in the preparatory (during (E)) ,culmination (end (E)), or consequent (result (E)) stage of an event, in a tripartite event structure is similar to that of Moens and Steedman (1988), which allows us to express the semantics of classes of verbs like change of state verbs whose description requires reference to a complex event structure.   The subdivision is achieved by means of three features proposed by [Bennett et al, 1990] following the framework of [Moens and Steedman, 1988]. The subdivision is achieved by means of three features proposed by [Bennett et al., 1990] following the framework of [Moens and Steedman, 1988] (in the spirit of [Dowty, 1979] and [Vendler, 1967]): +-dynamic (i.e., events vs. states, as in the Jackendoff framework), +-telic (i.e., culminative events (transitions) vs. nonculminative events (activities)), and +-atomic (i.e., point events vs. extended events).
GermaNet - A Lexical-Semantic Net For German We present the lexical-semantic net for German &quot;GermaNet&quot; which integrates conceptual ontological information with lexical semantics, within and across word classes. It is compatible with the Princeton WordNet but integrates principlebased modifications on the constructional and organizational level as well as on the level of lexical and conceptual relations. GermaNet includes a new treatment of regular polysemy, artificial concepts and of particle verbs. It furthermore encodes cross-classification and basic syntactic information, constituting an interesting tool in exploring the interacof syntax and development of such a large scale resource is particularly important as German up to now lacks basic online tools for the semantic exploration of very large corpora. Our initial experiments with the integration of GermaNet (Hamp and Feldweg, 1997), the evolving German version of WordNet, seem to confirm the positive results described for WordNet (de Buenaga Rodriguez et al, 1997) and will thus be extended. To divide adjectives into groups, Tsvetkov et al (2014) use 13 top-level classes from the adapted taxonomy of Hundsnurscher and Splett (1982), which is incorporated in GermaNet (Hamp and Feldweg, 1997). Synonyms in GermaNet (Hamp and Feldweg, 1997) receive the score 1. GermaNet (Hamp and Feldweg, 1997) is a large lexical database, where words are associated with POS in formation and semantic sorts, which are organized in a fine-grained hierarchy. For German, we address this issue by introducing two further types of features into our model based on the GermaNet resource (Hamp and Feldweg, 1997).  This paper presents an algorithm for finding systematic polysemous classes in WordNet (Miller et al 1990) and GermaNet (Hamp and Feldweg 1997) - a semantic database for German similar to WordNet. The system currently uses two different terminological onto logies WordNet (Fellbaum, 1998) and GermaNet (Hamp and Feldweg, 1997) as chaining resources which have been mapped onto the database for mat. We select the hyper parameters of our model using an independent development set, which we extract from the lexical resource GermaNet (Hamp and Feldweg, 1997).
Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms We describe new algorithms for train ing tagging models, as an alternativeto maximum-entropy models or conditional random elds (CRFs). The al gorithms rely on Viterbi decoding oftraining examples, combined with simple additive updates. We describe the ory justifying the algorithms througha modi cation of the proof of conver gence of the perceptron algorithm forclassi cation problems. We give experimental results on part-of-speech tag ging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger. We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged (the full feature set is described in (Collins 2002a)). For related work on the voted perceptron algorithm applied to NLP problems, see (Collins 2002a) and (Collins 2002b). (Collins 2002a) describes experiments on the same named-entity dataset as in this paper, but using explicit features rather than kernels. (Collins 2002b) describes how the voted perceptron can be used to train maximum-entropy style taggers, and also gives a more thorough discussion of the theory behind the perceptron algorithm applied to ranking tasks. We use the averaged perceptron (Collins, 2002) to train a global linear model and score each action. We split the Penn Treebank corpus (Marcus et al, 1994) into training, development and test sets as in (Collins, 2002).  A perceptron algorithm gives 97.11% (Collins, 2002).   Theparameters? of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and Huang (2008). We trained this model using the averaged perceptron algorithm (Collins, 2002). We adopt the perceptron algorithm (Collins, 2002) to train the re-ranker.  Collins (2002)'s perceptron training algorithm were adopted again, to learn a discriminative classifier, mapping from inputs x X to outputs y Y. The generalized perceptron proposed by Collins (2002) is closely related to CRFs, but the best CRF training methods seem to have a slight edge over the generalized perceptron. We compare those algorithms to generalized iterative scaling (GIS) (Darroch and Ratcliff, 1972), non-preconditioned CG, and voted perceptron training (Collins, 2002). Unlike other methods discussed so far, voted perceptron training (Collins, 2002) attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the best-scoring labeling of that instance according to the current model. Collins (2002) reported and we confirmed that this averaging reduces over fitting considerably. Minor variants support voted perceptron (Collins, 2002) and MEMMs (McCallum et al, 2000) with the same efficient feature encoding.
Improved Word-Level System Combination for Machine Translation Recently, confusion network decoding has been applied in machine translation system combination. Due to errors in the hypothesis alignment, decoding may result in ungrammatical combination outputs. This paper describes an improved confusion network based method to combine outputs from multiple MT systems. In this approach, arbitrary features may be added log-linearly into the objective function, thus allowing language model expansion and re-scoring. Also, a novel method to automatically select the hypothesis which other hypotheses are aligned against is proposed. A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including TER, BLEU and METEOR. The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods. We build our confusion networks using the method of Rosti et al (2007), but, instead of forming alignments using the tercom script (Snover et al, 2006), we create alignments that minimize invWER (Leusch et al, 2003), a form of edit distance that permits properly nested block movements of substrings. For this, Rosti et al (2007) use the tercom script (Snover et al, 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another. ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al (2007). Note that the algorithm of Rosti et al (2007) used n-best lists in the combination. In our experience, this approach is advantageous in terms of translation quality, e.g. by 0.7% in BLEU compared to a minimum Bayes risk primary (Rosti et al., 2007). Similar to the features in Rosti et al (2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.  A multiple CN or super-network framework was firstly proposed in Rosti et al (2007) who used each of all individual system results as the backbone to build CNs based on the same alignment metric, TER (Snover et al, 2006). The subnetworks in the latter approach may be weighted by prior probabilities estimated from the alignment statistics (Rosti et al, 2007a). This can be seen as a simplified version of (Rosti et al, 2007b). This method can also be viewed to be a hypotheses reranking model since we only use the existing translations instead of performing decoding over a confusion network as done in the word-level combination method (Rosti et al, 2007). If the systems generated N best hypotheses, a fractional increment could be added to these vectors as in (Rosti et al, 2007). Powell's method (Press et al, 2007) on N -best lists was used in system combination weight tuning in Rosti et al (2007). We re-implemented a state-of-the-art system combi nation method (Rosti et al, 2007).  The current state-of-the-art is confusion-network based MT system combination as described by Rosti and colleagues (Rosti et al, 2007a, Rosti et al., 2007b).  Similar to (Rosti et al, 2007), each word in the confusion network is associated with a word posterior probability. The availability of the TER software has made it easy to build a high performance system combination baseline (Rosti et al, 2007). The hypothesis scores and tuning are identical to the setup used in (Rosti et al, 2007).
Inducing Translation Lexicons Via Diverse Similarity Measures And Bridge Languages string date occurrence Local .125 context All .125 word distribution Narrow .125 .083 .083 .083 .125 Wide IDF RF Burstiness 1 exact-match accuracy 0.8 0.6 0.4 0.2 4 3 2 1 combo combo minus levenshtein 0 0 500 1000 1500 2000 2500 3000 3500 4000 4500 test words covered 0 500 1000 1500 2000 combo levenshtein only 2 1 0 500 1000 1500 2000 4 combo combo minus context 3 2 online + paper dictionary scoring online dictionary scoring 1 4 0 500 1000 1500 2000 3 combo combo minus date 2 1 0 500 1000 1500 2000 combo combo minus rfjdf,burstiness 4 3 2 1 4 3 0 500 1000 1500 2000 4 3 bulg+czech bulg+czech w/ retrained levenshtein bulg+czech w/ retrained levenshtein & context 2 1 0 500 1000 1500 2000 RANK CRIB. SCR. C'OMBINED STRING DATE-LOCAL WIDE-COS NARROW-C'OS BURSTINESS RF 1 0.18 protest/N (1) abhorrence/N break/V protest/V protest/N protest/N protest/N 2 0.19 opening/N (1) abomination/N resistance/N protest/N system/N reluctance/N port/N 3 0.24 break/N (1) allergy/N stress/V break/V break/V break/N opening/N 4 0.28 mouth/N (1) animosity/N protest/V hate/V protest/V kick/V stress/V 5 0.29 objection/N (1) antagonism/N escape/V opening/N antagonism/N protest/V protest/V 6 (1) antipathy/N protest/N escape/V hate/V escape/V escape/V 7 0.30 opposition/N (1) aperture/N opening/N stress/V dislike/V opposition/N resistance/N 8 0.33 reluctance/N (1) averse/J break/N system/N resentment/N mouth/N break/N 9 0.33 port/N (1) aversion/N kick/V defiance/N unit/N unit/N break/V 10 0.36 hole/N (1) bore/N system/N mouth/N disgust/V formation/N opposition/N 11 0.38 stress/N (1) bore/V opposition/N contradiction/N reluctance/N port/N unit/N 12 0.38 escape/N (1) boring/J kick/N kick/V formation/N stress/V hole/N 13 0.38 formation/N (1) boring/N formation/N resentment/N animosity/N objection/N kick/V 14 0.40 animosity/N (1) break/N punch/N dislike/V dislike/N protestation/N outlet/N 15 0.40 resentment/N (1) break/V unit/N reluctance/N escape/V hate/V column/N (1) resistance/N RANK CRIB. SCR. C'OMBINED STRING DATE-LOCAL WIDE-C'OS NARROW-COS BURSTINESS RF 1 (1) freedom/N independence/N independence/N independence/N evidence/V free/V 2 0.09 freedom/N relation/N single/J ease/N necessity/N cold/J 3 0.11 depend/V free/J cold/N irrelevant/J fair/J abandon/V 4 0.13 relation/N (4) irrelevance/N side/N side/N ease/V single/V importance/N 5 0.20 consequence/N (5) illegality/N importance/N independent/J applicability/N application/N ease/V 6 0.21 lift/V (5) illegitimacy/N depend/V consequence/N single/J independence/N licence/N 7 0.21 importance/N independent/J freedom/N disagreement/N currency/N lift/V 8 0.22 obligation/N single/J abandon/V lift/V free/V miss/N 9 0.23 ease/V life/N lack/V cold/N inadequacy/N green/N 10 0.23 independent/J freedom/N depend/V depend/V pride/N involvement/N 11 0.23 single/J irrelevant/N moment/N pride/N cold/J green/J 12 0.24 abandon/V miss/V importance/N side/N irrelevant/J consequence/N 13 0.24 integrity/N imperative/J relation/N realty/N side/V utility/N 14 0.24 necessity/N safety/N lack/N consequence/N disagreement/N lack/V 15 0.24 irrelevant/J obligation/N necessity/N drag/N independent/N independent/N (25)indpndnce/N RANK CRIB. SCR. COMBINED STRING DATE-LOCAL WIDE-COS NARROW-C'OS BURSTINESS RF 1 quarter/N currency/N currency/N exchange/V bless/V 2 0.43 chop/V (1) calibre/N good/J applaud/V praise/V making/N chop/V 3 0.45 bless/V (1) chop/N quality/N praise/N superior/J praise/N commend/V 4 0.48 applaud/V (1) chop/V class/N praise/V good/J class/N laud/V 5 0.49 exchange/V (1) class/N exchange/N good/J class/N currency/N making/N 6 (1) class/V compliment/N making/N good/N applaud/V applaud/V 7 0.56 commend/V (1) making/N superior/J bless/V quarter/N quarter/N superior/J 8 0.57 class/V (1) quality/J exchange/V superior/J quality/N superior/J praise/N 9 0.68 quarter/V (1) quality/N superior/N good/N biennial/J good/N superior/N 10 0.71 compliment/V (1) quarter/N praise/V exchange/V exchange/N quality/N compliment/N 11 0.81 scroll/V (1) quarter/V praise/N chop/V bless/V superior/N scroll/N 12 2.30 superior/J (12) applaud/V good/N exchange/N praise/N laud/V exchange/V 13 2.30 class/N (12) biennial/J bless/V quality/N exchange/V praise/V chop/N 14 2.34 quality/N (12) biennial/N currency/N class/N exchange/N good/N 15 2.35 making/N (12) bless/N caliber/N biennial/J bless/V calibre/N 17 32 (12) laud/V RANK CRIB. SCR. C'OMBINED STRING DATE-LOCAL WIDE-C'OS NARROW-COS BURSTINESS RF 1 rise/V bear/V widow/N stand/V horse/N 2 0.30 suffer/V (1) endure/V suffer/V stand/V stand/V raise/V expire/V 3 0.31 bear/V (1) expire/V stand/V leave/V leave/V suffer/V proceed/V 4 0.39 leave/V (1) leave/V limit/N suffer/V bear/V bear/V quantity/N 5 0.41 proceed/V (1) proceed/V raise/V endure/V boundary/N leave/V boundary/N 6 0.41 endure/V (1) raise/V bear/V limit/N endure/V rise/V limit/N 7 0.42 raise/V (1) rise/V leave/V raise/V limit/N proceed/V endure/V 8 0.44 rise/V (1) shallow/J horse/N quantity/N suffer/V endure/V widow/N 9 0.45 expire/V boundary/N proceed/V proceed/V limit/N bear/V 10 0.45 limit/N (1) suffer/V expire/V horse/N raise/V expire/V suffer/V 11 0.52 boundary/N (11) mischief/N quantity/N widow/N expire/V quantity/N stand/V 12 0.57 quantity/N (12) boundary/N proceed/V boundary/N rise/V widow/N mischief/N 13 0.61 widow/N (12) horse/N endure/V shallow/J horse/N horse/N raise/V 14 0.62 horse/N (12) limit/N widow/N rise/V quantity/N boundary/N shallow/J 15 0.72 shallow/J (12) quantity/N mischief/N expire/V shallow/J rise/V 5: tables show the performance of individual similarity measures as well as their combined choice, after model retraining. Correct translations are shown in bold. Note that in many cases the string-similarity-based orderings of the bridge candidates underperform individual non-string similarity measures, and they consistently underperform the weighted combiof all 8 similarity measures. Note that in the case of correct translation successfully above its quite closely related competitor almost every non-string-based similarity measure in isolation. This behavior (shown quantatively in Figure 8) illustrates the contribution of consensus modeling over this set of diverse similarity measures. Mann, G. and D. Yarowsky, 2001. Multipath translation induction via bridge languages. In This idea is similar to translation lexicon extraction via a bridge language (Schafer and Yarowsky, 2002). Nevertheless, Schafer and Yarowsky (2002) have shown that these two techniques can be combined efficiently. Schafer and Yarowsky, (2002) independently proposed using frequency, orthographic similarity and also showed improvements using temporal and wordburstiness similarity measures, in addition to context. Our setup we used cosine similarity, Rapp (1999) used l1-norm metric after normalizing the vectors to unit length, Koehn and Knight (2002) used Spearman rank order correlation, and Schafer and Yarowsky (2002) use cosine similarity. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. Schafer and Yarowsky (2002) induced translation lexicons for languages without common parallel corpora using a bridge language that is related to the target languages. For example, researchers have automatically developed translation lexicons by seeing if words from each language have similar frequencies, contexts (Koehn and Knight, 2002), burstiness, inverse document frequencies, and date distributions (Schafer and Yarowsky, 2002). Lexicon induction methods for closely related languages using phonetic similarity have been proposed by Mann and Yarowsky (2001) and Schafer and Yarowsky (2002), and applied to Swiss German data by Scherrer (2007). For instance, Schafer and Yarowsky (2002) demonstrated that word translations tend to co occur in time across languages. Various other similarity scores can be computed depending on the available monolingual data and its associated meta data (see, e.g. Schafer and Yarowsky (2002)). This can be accomplished as in Rapp (1999) and Schafer and Yarowsky (2002) by creating bag-of-words context vectors around both the source and target language words and then projecting the source vectors into the (English) target space via the current small translation dictionary. Schafer and Yarowsky (2002) created lexicons between English and a target local language (e.g. Gujarati) using a related language (e.g. Hindi) as pivot. Pivot-based methods have also been used in other related areas, such as translation lexicon induction (Schafer and Yarowsky, 2002), word alignment (Wang et al, 2006), and cross language information retrieval (Gollins and Sanderson, 2001).
Automatic Extraction Of Subcategorization From Corpora of the system that are new: the extractor, classifier and evaluator. The grammar consists of 455 phrase structure rule schemata in the format accepted by the parser (a syntactic variant of a Definite Clause Grammar with iterative (Kleene) operators). It is 'shallow' in that no atof which thetempt is made to fully analyse unbounded dependencies. However, the distinction between arguments and adjuncts is expressed, following X-bar theory (e.g. Jackendoff, 1977), by Chomsky-adjunction to maximal projections of adjuncts (XP XP Adjunct) as opposed to 'government' of arguments (i.e. arguments are sisters within projections; X1 XO Argl... ArgN). more, all analyses are rooted (in S) so the grammar assigns global, shallow and often 'spurious' analyses to many sentences. There are 29 distinct values for VSUBCAT and 10 for PSUBCAT; these are analysed in patterns along with specific closed-class lemmas of arguments, such as suband so forth, to classify patterns as evidence for one of the 160 subcategorization classes. Each of these classes can be parameterized for specific predicates by, for example, different prepositions or particles. Currently, the coverage of this grammar—the proportion of sentences for which at least one analysis is found—is 79% when applied to the Susanne corpus (Sampson, 1995), a 138K word treebanked and balanced subset of the Brown corpus. Wide coverage is important since information is acquired only from successful parses. The combined throughput of the parsing components on a Sun UltraSparc 1/140 is around 50 words per CPU second. 2.2 The Extractor, Classifier and Evaluator The extractor takes as input the ranked analyses from the probabilistic parser. It locates the subanalyses around the predicate, finding the constituents identified as complements inside each subanalysis, and the subject clause preceding it. Instances of passive constructions are recognized and treated specially. The extractor returns the predicate, the VSUBCAT value, and just the heads of the complements (except in the case of PPs, where it returns the PSUBCAT value, the preposition head, and the heads of the PP's complements). The subcategorization classes recognized by the classifier were obtained by manually merging the classes exemplified in the COMLEX Syntax and ANLT dictionaries and adding around 30 classes found by manual inspection of unclassifiable patterns for corpus examples during development of the system. These consisted of some extra patterns for phrasal verbs with complex complementation and with flexible ordering of the preposition/particle, some for non-passivizable patterns with a surface direct object, and some for rarer combinations of governed preposition and complementizer combinations. The classifier filters out as unclassifiable around 15% of patterns found by the extractor when run on all the patternsets extracted from the Susanne corpus. This demonstrates the value of the classifier as a filter of spurious analyses, as well as providing both translation between extracted patterns and two existing subcategorization dictionaries and a definition of the target subcategorization dictionary. The evaluator builds entries by taking the patterns for a given predicate built from successful parses and records the number of observations of each subcategorization class. Patterns provide several types of information which can be used to rank or select between patterns in the patternset for a given sentence exemplifying an instance of a predicate, such as the ranking of the parse from which it was extracted or the proportion of subanalyses supporting a specific pattern. Currently, we simply select the pattern supported by the highest ranked parse. However, we are experimenting with alternative approaches. The resulting set of putative classes for a predicate are filtered, following Brent (1993), 358 by hypothesis testing on binomial frequency data. Evaluating putative entries on binomial frequency data requires that we record the total number of patternsets n for a given predicate, and the number of these patternsets containing a pattern supporting an entry for given class m. These figures are straightforwardly computed from the output of the classifier; however, we also require an estimate of the probability that a pattern for class i will occur with a verb which is not a member of subcategorization class i. Brent proposes estimating these probabilities experimentally on the basis of the behaviour of the extractor. We estimate this probability more directly by first extracting the number of verbs which are members of each class in the ANLT dictionary (with intuitive estimates for the membership of the novel classes) and converting this to a probability of class membership by dividing by the total number of verbs in the dictionary; and secondly, by multiplying the complement of these probabilities by the probability of a pattern for class i, defined as the number of patterns for i extracted from the Susanne corpus divided by the total number of patterns. So, p(v -i), the probability of verb v not of class i occurring with a pattern for class i is: -i) = (1 Ipatterns_f or_i n! n, p) = mi(p im The probability of the event happening m or more times is: n,p) = i=m -i)) the probability that m or more occurrences of patterns for i will occur with a verb which is not a member of i, given n occurrences of that verb. Setting a threshold of less than or equal to 0.05 yields a 95% or better confidence that a high enough proportion of patterns for i have observed for the verb to be in class 2.3 Discussion Our approach to acquiring subcategorization classes is predicated on the following assumptions: • most sentences will not allow the application of all possible rules of English complementation; • some sentences will be unambiguous even given indeterminacy of the (1993:249-253) provides a detailed explanation and justification for the use of this measure. fact, 5% of sentences in Susanne are assigned only a single analysis by the grammar. • many incorrect analyses will yield patterns which are unclassifiable, and are thus filtered out; • arguments of a specific verb will occur with greater frequency than adjuncts (in potential argument positions); • the patternset generator will incorrectly output for certain classes more often than others; and even a highest ranked for i is only a probabilistic cue for membership of i, so membership should only be inferred if there are enough occurrences of patterns for i in the data to outweigh the error probability for i. This simple automated, hybrid linguistic/statistical approach contrasts with the manual linguistic analysis of the COMLEX Syntax lexicog- (Meyers at., who propose five criteria and five heuristics for argument-hood and six criteria and two heuristics for adjunct-hood, culled mostly from the linguistics literature. Many of these are not exploitable automatically because they rest on semantic judgements which cannot (yet) be made automatically: for example, optional arguments are often 'understood' or implied if missing. Others are syntactic tests involving diathesis alternation possibilities (e.g. passive, dative movement, Levin (1993)) which require recognition that the 'same' argument, defined usually by semantic class / thematic role, is occurring across argument positions. We hope to exploit this information where possible at a later stage in the development of our approach. However, recognizing same/similar arguments requires considerable quantities of lexical data or the ability to back-off to lexical semantic classes. At the moment, we exploit linguistic information about the syntactic type, obligatoriness and position of arguments, as well as the set of possible subcategorization classes, and combine this with statistical inference based on the probability of class membership and the frequency and reliability of patterns for classes. 3 Experimental Evaluation 3.1 Lexicon Evaluation — Method In order to test the accuracy of our system (as developed so far) and to provide empirical feedback for further development, we took the Susanne, SEC (Taylor & Knowles, 1988) and LOB corpora (Garat., total of 1.2 million words—and extracted all sentences containing an occurrence of one of fourteen verbs, up to a maximum of 1000 citations of each. These verbs, listed in Figure 2, were chosen at random, subject to the constraint that they exhibited multiple complementation patterns. The sentences containing these verbs were tagged and parsed automatically, and the extractor, classifier and evaluator were applied to the resulting lanit_verbs1 'patterns' The binomial distribution gives the probability of an with probability exactly m times out of n attempts: 359 successful analyses. The citations from which entries were derived totaled approximately 70K words. The results were evaluated against a merged entry for these verbs from the ANLT and COMLEX Syntax dictionaries, and also against a manual analysis of the corpus data for seven of the verbs. The process of evaluating the performance of the system relative to the dictionaries could, in principle, be reduced to automated report of precision of correct subcategorization classes to all classes found) of correct classes found in the dictionary entry). However, since there are disagreements between the dictionaries and there are classes found in the corpus data that are not contained in either dictionary, we report results relative both to a manually merged entry from ANLT and COMLEX, and also, for seven of the verbs, to a manual analysis of the actual corpus data. The latter analysis is necessary because precision and recall measures against the merged entry will still tend to yield inaccurate results as the system cannot acquire classes not exemplified in the data, and may acquire classes incorrectly absent from the dictionaries. We illustrate these problems with reference to there is overlap, but not agreement between the COMLEX and ANLT entries. Thus, predict that occur with a sentential complement and dummy subject, but only ANLT predicts the possibility of a `wh' complement and only COMLEX predicts the (optional) presence of a PP[to] argument with the sentential complement. One ANLT entry covers two COMLEX entries given the different treatment of the relevant complements but the classifier keeps them distinct. The corpus for examples of further classes we judge valid, in which take a and infinitive complement, as in seems to to be insane, a passive participle, as in depressed. comparison illustrates the problem of errors of omission common to computational lexicons constructed manually and also from dictionaries. All classes for exemplified in the corpus data, but for example, eight classes (out of a possible 27 in the merged entry) are not present, so comparison only to the merged entry would give an unreasonably low estimate of recall. Lexicon Evaluation Figure 2 gives the raw results for the merged entries and corpus analysis on each verb. It shows the of positives correct classes proby our system, positives incorrect proposed by our system, and negatives classes not proposed by our system, as judged against the merged entry, and, for seven of the verbs, against the corpus analysis. It also shows, in the final column, the number of sentences from which classes were extracted. Dictionary (14 verbs) Corpus (7 verbs) Precision Recall 65.7% 76.6% 35.5% 43.4% Figure 3: Type precision and recall Ranking Accuracy ask 75.0% begin 100.0% believe 66.7% cause 100.0% give 70.0% seem 75.0% swing 83.3% Mean 81.4% Figure 4: Ranking accuracy of classes Figure 3 gives the type precision and recall of our system's recognition of subcategorization classes as evaluated against the merged dictionary entries (14 verbs) and against the manually analysed corpus data (7 verbs). The frequency distribution of classes is highly skewed: for example for there are 107 instances of the most common class in the corpus data, but only 6 instances in total of the least common four classes. More generally, for the manually analysed verbs, almost 60% of the false negatives have only one or two exemplars each in the corpus citations. None of them are returned by because the binomial filter always rejects classes hypothesised on the basis of such little evidence. In Figure 4 we estimate the accuracy with which our system ranks true positive classes against the correct ranking for the seven verbs whose corpus input was manually analysed. We compute this measure by calculating the percentage of pairs of classes at positions (n, m) s.t. n < m in the system ranking that are ordered the same in the correct ranking. This gives us an estimate of the accuracy of the relative frequencies of classes output by the system. For each of the seven verbs for which we undertook a corpus analysis, we calculate the token recall of our system as the percentage (over all exemplars) of true positives in the corpus. This gives us an estimate of the parsing performance that would result from providing a parser with entries built using the system, shown in Figure 5. Further evaluation of the results for these seven verbs reveals that the filtering phase is the weak in the systeni. There are only 13 negatives which the system failed to propose, each exemplified in the data by a mean of 4.5 examples. On the other there are 67 negatives by an mean of 7.1 examples which should, ide- 360 Merged TP FP Entry Corpus TP FP Data No. of FN FN Sentences ask 9 0 18 9 0 10 390 begin 4 1 7 4 1 7 311 believe 4 4 11 4 4 8 230 cause 2 3 6 2 3 5 95 expect 6 5 3 - - - 223 find 5 7 15 - - - 645 give 5 2 11 5 2 5 639 help 6 3 8 - - - 223 like 3 2 7 - - - 228 move 4 3 9 - - - 217 produce 2 1 3 - - 152 provide 3 2 6 - - - 217 seem 8 1 4 8 1 4 534 swing 4 0 10 4 0 8 45 Totals 65 34 118 36 11 47 4149 Figure 2: Raw results for test of 14 verbs Token Recall ask 78.5% begin 73.8% believe 34.5% cause 92.1% give 92.2% seem 84.7% swing 39.2% Mean 80.9% Figure 5: Token recall have been accepted by the filter, and 11 should have been rejected. The performance of the filter for classes with less than 10 exemplars is around chance, and a simple heuristic of accepting all classes with more than 10 exemplars would have produced broadly similar results for these verbs. The filter may well be performing poorly because the probability of generating a subcategorization class for a given verb is often lower than the error probability for that class. 3.3 Parsing Evaluation In addition to evaluating the acquired subcategorization information against existing lexical resources, we have also evaluated the information in the context of an actual parsing system. In particular we wanted to establish whether the subcategorization frequency information for individual verbs could be used to improve the accuracy of a parser that uses statistical techniques to rank analyses. The experiment used the same probabilistic parser and tag sequence grammar as are present in the acquisition system (see references above)—although the experiment does not in any way rely on the Mean Recall Precision crossings 'Baseline' Lexicalised 1.00 70.7% 72.3% 0.93 71.4% 72.9% Figure 6: GEIG evaluation metrics for parser against Susanne bracketings parsers or grammars being the same. We randomly selected a test set of 250 in-coverage sentences (of lengths 3-56 tokens, mean 18.2) from the Susanne treebank, retagged with possibly multiple tags per word, and measured the 'baseline' accuracy of the unlexicalized parser on the sentences using the now standard PARSEVAL/GEIG evaluation metrics of mean crossing brackets per sentence and (unlabelled) bracket recall and precision (e.g. Gral., see figure Next, we colwords in the test corpus tagged as possibly being verbs (giving a total of 356 distinct lemmas) and retrieved all citations of them in the LOB corpus, plus Susanne with the 250 test sentences excluded. We acquired subcategorization and associated frequency information from the citations, in the process successfully parsing 380K words. We then parsed the test set, with each verb subcategorization possibility weighted by its raw frequency score, and using the naive add-one smoothing technique to allow for omitted possibilities. The GEIG measures for the lexicalized parser show a 7% improvement in the crossing bracket score (figure 6). the existing test corpus this is not statisti- 'Carroll & Briscoe (1996) use the same test set, although the baseline results reported here differ slightly due to differences in the mapping from parse trees to Susanne-compatible bracketings. 361 significant at the 95% level p = if the pattern of differences were maintained over a larger test set of 470 sentences it would be significant. We expect that a more sophisticated smoothing technique, a larger acquisition corpus, and extensions to the system to deal with nominal and adjectival predicates would improve accuracy still further. Nevertheless, this experiment demonstrates that lexicalizing a grammar/parser with subcategorization frequencies can appreciably improve the accuracy of parse ranking. 4 Related Work Brent's (1993) approach to acquiring subcategorization is based on a philosophy of only exploiting unambiguous and determinate information in unanalysed corpora. He defines a number of lexical patterns (mostly involving closed class items, such as pronouns) which reliably cue one of five subcategorization classes. Brent does not report comprehensive results, but for one class, sentential complement verbs, he achieves 96% precision and 76% recall at classifying individual tokens of 63 distinct verbs as exemplars or non-exemplars of this class. He does not attempt to rank different classes for a given verb. al. utilise a PoS tagged corpus and finite-state NP parser to recognize and calculate the relative frequency of six subcategorization classes. They report an accuracy rate of 83% (254 errors) at classifying 1565 classifiable tokens of 33 distinct verbs in running text and suggest that incorrect noun phrase boundary detection accounts for the majority of errors. They report that for 32 verbs their system correctly predicts the most frequent class, and for 30 verbs it correctly predicts the second most frequent class, if there was one. Our system rankings include all classes for each verb, from a total of 160 classes, and average 81.4% correct. Manning (1993) conducts a larger experiment, also using a PoS tagged corpus and a finite-state NP parser, attempting to recognize sixteen distinct complementation patterns. He reports that for a test sample of 200 tokens of 40 verbs in running text, the acquired subcategorization dictionary listed the appropriate entry for 163 cases, giving a token recall of 82% (as compared with 80.9% in our experiment). He also reports a comparison of acquired entries for verbs to the entries given in the Advanced Dictionary of Current English 1989) on which his system achieves a precision of 90% and a recall of 43%. His system averages 3.48 subentries (maximum 10)—less then half the number produced in our experiment. It is not clear what level of evidence the performance of Manning's system is based on, but the system was applied to 4.1 million words of text (c.f. our 1.2 million words) and the verbs are all common, so it is likely that considerably more exemplars of each verb were available. 5 Conclusions and Further Work The experiment and comparison reported above suggests that our more comprehensive subcategorization class extractor is able both to assign classes to individual verbal predicates and also to rank them according to relative frequency with comparable accuracy to extant systems. We have also demonstrated that a subcategorization dictionary built with the system can improve the accuracy of a probabilistic parser by an appreciable amount. The system we have developed is straightforwardly extensible to nominal and adjectival predicates; the existing grammar distinguishes nominal and adjectival arguments from adjuncts structurally, so all that is required is extension of the classifier. Developing an analogous system for another language would be harder but not infeasible; similar taggers and parsers have been developed for a number of languages, but no extant subcategorization dictionaries exist to our knowledge, therefore the lexical statistics we utilize for statistical filtering would need to be estimated, perhaps using the technique described by Brent (1993). However, the entire approach to filtering needs improvement, as evaluation of our results demonstrates that it is the weakest link in our current system. Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements. It also needs supplementing with information about diathesis alternation possibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. Grishman & Sterling (1992), Poznanski & Sanfilippo (1993), Resnik (1993), Ribas (1994) and others have shown that it is possible to acquire selection preferences from (partially) parsed data. Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle. In future work, we intend to extend the system in this direction. The ability to recognize that argument slots of different subcategorization classes for the same predicate share semantic restrictions/preferences would assist recognition that the predicate undergoes specific alternations, this in turn assisting inferences about control, equi and raising (e.g. Boguraev & Briscoe, 1987). The SCFs applicable to each verb are extracted automatically from corpus data using the system of Briscoe and Carroll (1997). On the other hand, we find automatic approaches to the induction of verb subcategorization information at the syntax-semantics interface for a large number of languages ,e.g. Briscoe and Carroll (1997) for English; Sarkar and Zeman (2000) for Czech; Schulteim Walde (2002a) for German; Messiant (2008) for French. Briscoe and Carroll (1997) and Korhonen et al (2000) use a grammar and a sophisticated parsing tool for argument-adjunct distinction. 67 search, but a starting point would be the approach by (Briscoe and Carroll, 1997).  (Brent 1993) estimated pe according to the acquisition system? s performance, while (Briscoe and Carroll 1997) calculated pe from the distribution of SCF types in ANLT and SCF tokens in Susanne as shown in the fol lowing equation. Further more, (Briscoe and Carroll 1997) applied their acquired English SCF lexicon to an intermediate parser, and reported a 7% improvement of both phrase-based precision and recall. We briefly outline our SCF extraction system for automatically extracting SCFs from corpora, which was based on the design proposed in Briscoe and Carroll (1997). We used the 14 verbs 4 selected by Briscoe and Carroll (1997) and evaluated our results of these verbs against the SCF entries in two gold standards: COMLEX (Grishman et al, 1994) and a manually constructed SCF set from the training data. Our experiments show that 14 verbs used in Briscoe and Carroll (1997) are ask, begin, believe, cause, expect, find, give, help, like, move, produce, provide, seem and sway. (Briscoe and Carroll, 1997) observe that in the work of (Brent, 1993), (Manning, 1993) and (Ushioda et al, 1993), the maximum number of distinct subcategorization classes recognized is sixteen, and only Ushioda et al attempt to derive relative subcategorization frequency for individual predicates. In contrast, the system of (Briscoe and Carroll, 1997) distinguishes 163 verbal subcategorisation classes by means of a statistical shallow parser, a classifier of subcategorisation classes, and a priori estimates of the probability that any verb will be a member of those classes. It is a well-documented fact (Briscoe and Carroll,1997) that subcategorisation frames (and their frequencies) vary across domains. Several substantial resources exist: e.g., hand-crafted large-scale grammars like the English Resource Grammar (ERG Flickinger (2000)) and the Dutch Alpino Grammar (Bouma et al, 2001). Unfortunately, the construction of these resources is the manual result of human efforts and therefore likely to contain errors of omission and commission (Briscoe and Carroll, 1997). The preliminary experiment on biomedical verb classification (Korhonen et al, 2006) employed basic syntactic features only: SCFs extracted from corpus data using the system of Briscoe and Carroll (1997) which operates on the output of a domain-independent robust statistical parser (RASP) (Briscoe and Carroll, 2002). (Briscoe and Carroll, 1997)) is to extract SCFs from parse trees, introducing an unnecessary dependence on the details of a particular parser. Table 2: New Verb Classes 1000 citations, on average, for each verb. Our method for SCF acquisition (Korhonen, 2002) involves first using the system of Briscoe and Carroll (1997) to acquire a putative SCF distribution for each test verb from corpus data. The text is parsed using the RASP parser (Briscoe and Carroll, 2002), and sub categorizations are extracted using the system of Briscoe and Carroll (1997). In their study, they first acquire fine-grained SCFs using the unsupervised method proposed by Briscoe and Carroll (1997) and Korhonen (2002). We propose a novel approach, which involves: (i) obtaining SCF frequency information from a lexicon extracted automatically using the comprehensive system of Briscoe and Carroll (1997) and (ii) applying a clustering mechanism to this information.
Improving Word Representations via Global Context and Multiple Word Prototypes Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models arebuilt with only local context and one represen tation per word. This is problematic becausewords are often polysemous and global con text can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learnsword embeddings that better capture the se mantics of words by incorporating both local and global document context, and 2) accountsfor homonymy and polysemy by learning mul tiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate ourmodel on it, showing that our model outper forms competitive baselines and other neural language models. 1 The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pado and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012): since werely on occurrences of words in texts, this extension should be quite straightforward by turning our word-in-context classifiers into true word sense classifiers. We also experiment with publicly released word embeddings (Huang et al, 2012), which were trained using both local and global context. In particular, many such representations are designed to capture lexical semantic properties and are quite effective features in semantic processing, including named entity recognition (Turian et al, 2009), word sense disambiguation (Huang et al., 2012), and lexical entailment (Baroni et al., 2012). Source embeddings: We employ three external embeddings (obtained from (Turian et al, 2010)) induced using the following models: 1) hierarchical log-bilinear model (HLBL) (Mnih and Hinton, 2009) and two neural network-based models, 2) Collobert and Weston's (C&W) deep-learning architecture, and 3) Huang et al's polysemous neural language model (HUANG) (Huang et al, 2012).  Huang et al (2012) compare, in passing, one count model and several predict DSMs on the standard WordSim353 benchmark (Table 3 of their paper). The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1).
Why Generative Phrase Models Underperform Surface Heuristics We investigate why weights from generative models underperform heuristic estimates in phrasebased machine translation. We first propose a simple generative, phrase-based model and verify that its estimates are inferior to those given by surface statistics. The performance gap stems primarily from the addition of a hidden segmentation variable, which increases the capacity for overfitting during maximum likelihood training with EM. In particular, while word level models benefit greatly from re-estimation, phrase-level models do not: the crucial difference is that distinct word alignments cannot all be correct, while distinct segmentations can. Alternate segmentations rather than alternate alignments compete, resulting in increased determinization of the phrase table, decreased generalization, and decreased final BLEU score. We also show that interpolation of the two methods can result in a modest increase in BLEU score. DeNero et al (2006) tried a different generative phrase translation model analogous to IBM word-translation Model 3 (Brown et al., 1993), and again found that the standard model outperformed their generative model. DeNero et al (2006) attribute the inferiority of their model and the Marcu and Wong model to a hidden segmentation variable, which enables the EM algorithm to maximize the probability of the training data without really improving the quality of the model. This avoids segmentation problems encountered by DeNero et al (2006). 140k sentences up to a certain length. DeNero et al (2006) have explored estimation using EM of phrase pair probabilities under a conditional translation model based on the original source-channel formulation. These segmentations into bilingual containers (where segmentations are taken inside the containers) are different from the monolingual segmentations used in earlier comparable conditional models (e.g., (DeNero et al, 2006)) which must generate the alignment on top of the segmentations. As it has been found out by (DeNero et al, 2006), it is not easy to come up with a simple, effective prior distribution over segmentations that allows for improved phrase pair estimates. In contrast with the model of (DeNero et al, 2006), who define the segmentations over the source sentence f alone, our model employs bilingual containers thereby segmenting both source and target sides simultaneously. Therefore, unlike (DeNeroet al, 2006), our model does not need to generate the word-alignments explicitly, as these are embedded in the segmentations. We did not explore mere EM without any smoothing or ITG prior, as we expect it will directly over fit the training data as reported by (DeNero et al, 2006). The most similar efforts to ours, mainly (DeNero et al, 2006), conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator. While theoretically sound, this approach is computationally challenging both in practice (DeNero et al, 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al, 2006), and in the end may lead to inferior translation quality (Koehn et al, 2003). thus generates a number of potentially overlapping in (DeNero et al, 2006), the ambiguity in word alignment is less prevalent than in phrase segmentation. For the German English, French English and English French language tasks we applied a forced alignment procedure to train the phrase translation model with the EM algorithm ,similar to the one described in (DeNero et al,2006). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al, 2006), without resort to the heuristic estimator of Koehn et al (2003). Phrasal SCFG models are subject to a degenerate maximum likelihood solution in which all probability mass is placed on long, or whole sentence, phrase translations (DeNero et al, 2006). DeNero et al (2006) instead proposed an exponential-time dynamic program pruned using word alignments. The heuristic method is inconsistent in the limit (Johnson, 2002) while EM is degenerate, placing disproportionate probability mass on the largest rules in order to describe the data with as few a rules as possible (DeNero et al, 2006).  If we only use the features as traditional SCFG systems, the bi parsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al, 2006). This is illustrated in Table 2, which shows the conditional probabilities for rules, obtained by locally normalising the rule feature weights for a simple grammar extracted from the ambiguous pair of sentences presented in DeNero et al (2006).
Assigning Function Tags To Parsed Text and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural lanprocessing. Linguistics, More recently, a domain independent system has been trained on general function tags such as Manner and Temporal by Blaheta and Charniak (2000). These features are generated using the Stanford parser (Klein and Manning, 2003) and a function tagger (Blaheta and Charniak, 2000). In function labeling level, EXT that signifies degree, amount of the predicates should be grouped into adverbials like in the work of (Blaheta and Charniak, 2000) and (Merlo and Musillo, 2005). As in the work of (Blaheta and Charniak, 2000) and (Merlo and Musillo, 2005), to avoid calculating excessively optimistic values, constituents bearing the O label are not counted in for computing overall precision, recall and F-score. The features we used are borrowed from feature trees described in (Blaheta and Charniak, 2000). A trivial difference is that in our system the head for prepositional phrases is defined as the prepositions themselves (not the head of object of prepositional phrases (Blaheta and Charniak, 2000)), because we think that the preposition itself is a more distinctive attribute for different semantic meanings. Blaheta and Charniak (2000) presented the first method for assigning Penn functional tags to constituents identified by a parser. As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically. Thus, it is informative to compare our results with those reported in (Blaheta and Charniak, 2000) for this same task. The difference in the accuracy is due to two reasons. First, because of the different definition of a correctly identified constituent in the parser? s output, we apply our method to a greater portion of all labels produced by the parser (95% vs. 89% reported in (Blaheta and Charniak, 2000)). TiMBL performed well on tasks where structured, more complicated and task-specific statistical models have been used previously (Blaheta and Charniak, 2000). Many researchers ((Blaheta and Charniak 2000), (Gildea and Jurafsky 2000), showed that lexical and syntactic information is very useful for predicate argument recognition tasks, such as semantic roles.  While the function of a constituent and its structural position are often correlated, they some (Blaheta and Charniak, 2000) talk of function tags. Following (Blaheta and Charniak, 2000), we refer to the first class as syntactic function labels, and to the second class as semantic function labels. Like previous work (Blaheta and Charniak, 2000), we complete the sets of syntactic and semantic labels by labelling constituents that do not bear any function label with a NULL label. Following (Blaheta and Charniak, 2000), incorrectly parsed constituents will be ignored (roughly 11% of the total) in the evaluation of the precision and recall of the function labels, but not in the evaluation of the parser. In work that predates the availability of Framenet and Propbank, (Blaheta and Charniak, 2000) define the task of function labelling for the first time and highlight its relevance for NLP. Blaheta and Charniak (2000) assume a richer input representation consisting of labelled trees produced by a tree bank grammar parser, and use the tree bank again to train a further procedure that assigns grammatical function tags to syntactic constituents in the trees. Blaheta and Charniak (2000) report an F-score of 87% for assigning grammatical function tags to constituents, but the task, and therefore the scoring method, is rather different.
A Simple Approach To Building Ensembles Of Naive Bayesian Classifiers For Word Sense Disambiguation This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co—occurring words in varying sized windows of context. Despite the simplicity of this approach, empirical results disamthe widely studied nouns show that such an ensemble achieves accuracy rivaling the best previously published results. We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classiers, each based on co-occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line. Pedersen (2000) proposed an ensemble model with multiple NB classifiers differing by context window size. It is similar to the ordinary Naive Bayes model for WSD (Pedersen, 2000). The origins of Duluth can be found in an ensemble approach based on multiple Naive Bayesian classifiers that perform disambiguation via a majority vote (Pedersen, 2000). The approach used, called combination approach, has known lots of success in speech recognition (Fiscus 1997, Schwenck and Gauvain 2000), part of speech tagging (Halteren and al. 1998, Brill and al. 1998, Marquez et Padro 1998), named entity recognition (Borthwick and al. 1998), word sense disambiguation (Pedersen, 2000) and recently in parsing (Henderson and Brill 1999), Inui and Inui 2000, Monceaux and Robba 2003). In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). They include those using Naive Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al 2000), and Naive Bayesian Ensemble (Pedersen 2000). Among these methods, the one using Naive Bayesian Ensemble (i.e., an ensemble of Naive Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Pedersen 2000). It actually employs an ensemble of the Naive Bayesian Classifiers (NBC), because an ensemble of NBCs generally performs better than a single NBC (Pedersen 2000). Table 4 shows the results achieved by some existing supervised learning methods with respect to the benchmark data (cf., Pedersen 2000). The Duluth-xLSS system was originally inspired by (Pedersen, 2000), which presents an ensemble of eighty-one Naive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features. Pedersen (2000) built an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. Locally weighted NB (LWNB, Frank et al 2003) and Ensemble NB (ENB Pedersen 2000) are two combinational approaches. Pedersen (2000) presents experiments with an ensemble of Naive Bayes classifiers, which outperform all previous published results on two ambiguous words (line and interest).
An Efficient Augmented-Context-Free Parsing Algorithm An efficient parsing algorithm for augmented context-free grammars is introduced, and its application to on-line natural language interfaces discussed. The algorithm is a generalized LR parsing algorithm, which precomputes an LR shift-reduce parsing table (possibly with multiple entries) from a given augmented context-free grammar. Unlike the standard LR parsing algorithm, it can handle arbitrary context-free grammars, including ambiguous grammars, while most of the LR efficiency is preserved by introducing the concept of a &quot;graph-structured stack&quot;. The graph-structured stack allows an LR shift-reduce parser to maintain multiple parses without parsing any part of the input twice in the same way. We can also view our parsing algorithm as an extended chart parsing algorithm efficiently guided by LR parsing tables. The algorithm is fast, due to the LR table precomputation. In several experiments with different English grammars and sentences, timings indicate a fiveto tenfold speed advantage over Earley's context-free parsing algorithm. algorithm parses a sentence strictly from left to right that is, starts parsing as soon as the user types in the first word of a sentence, without waiting for completion of the sentence. A practical on-line parser based on the algorithm has been implemented in Common Lisp, and running on Symbolics and HP Al workstations. The parser is used in the multi-lingual machine translation project at CMU. Also, a commercial on-line parser for Japanese language is being built by Intelligent Technology Incorporation, on the technique developed at  Similar to the work of Lang (1974) and Tomita (1987) extending LR parsers for arbitrary CFGs, the LR parsers for TAGs can be extended to solve by pseudo-parallelism the conflicts of moves. We developed a set of augmented context free grammar rules for general English syntactic analysis and the analyzer is implemented using Tomita LR parsing algorithm (Tomita, 1987). The RASP parser is a generalized LR parser which builds a non-deterministic generalized LALR parse table from the grammar (Tomita, 1987). a graph-structured stack (Tomita, 1987) was used to efficiently represent ambiguous index operations in a GIG stack. A context-free backbone is automatically derived from the unification grammar1 and a generalized or non-deterministic LALR(1) table is constructed from this backbone (Tomita, 1987). The parsers create parse forests (Tomita, 1987) that incorporate subtree sharing (in which identical sub-analyses are shared between differing superordinate analyses) and node packing (where sub analyses covering the same portion of input whose root categories are in a subsumption relationship are merged into a single node). In this parser, the LALR (1) technique (Aho, Sethi Ullman, 1986) is used, in conjunction with a graph-structured stack (Tomita, 1987), adapting for unification-based parsing Kipps's (1989) Tomita-like recogniser that achieves polynomial complexity on input length through caching. A forest (Tomita, 1987) compactly encodes an exponential number of parse trees.
Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model’s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines. Socher et al (2011a) and Socher et al (2011b) present a framework based on recursive neural net works that learns vector space representations for multi-word phrases and sentences.        More specifically related to our work, deep learning neural networks have been successfully employed for sentiment analysis (Socher et al, 2011) and for sentiment domain adaptation (Glo rot et al, 2011). We should emphasize that the features induced from the addressee's utterance are unique to this task and are hardly available in the related tasks that predicted the emotion of a reader of news articles (Lin and HsinYihn, 2008) or personal stories (Socher et al, 2011). Analogous to our prediction task, Lin and Hsin Yihn (2008) and Socher et al (2011) investigated predicting the emotion of a reader from the text that s/he reads. Unlike Socher et al (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Recursive Autoencoder (Socher et al, 2011c) has been proven effective in many sentiment analysis tasks by learning compositionality automatically.  To do this, we use two unsupervised recursive autoencoders (RAE) (Socher et al, 2011b), one for the source phrase and the other for the target phrase. More details can be found in (Socher et al, 2011b). By providing richer representations of meaning than what can be encompassed in a discrete representation, such approaches have successfully been applied to tasks such as sentiment analysis (Socher et al, 2011), topic classification (Klementiev et al, 2012) or word-word similarity (Mitchell and Lapata, 2008). Some previous work on classifying snippets include using pre-defined polarity reversing rules (Moilanen and Pulman, 2007), and learning complex models on parse trees such as in (Nakagawa et al., 2010) and (Socher et al, 2011).  Socher et al (2011) introduce a semi-supervised approach that uses recursive autoencoders to learn the hierarchical structure and sentiment distribution of a sentence.
METEOR: An Automatic Metric For MT Evaluation With Improved Correlation With Human Judgments  Examples of such methods are the introduction of information weights as in the NIST measure or the comparison of stems or synonyms, as in METEOR (Banerjee and Lavie, 2005). Surface-form oriented metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al, 2006), WER (Nie? en et al, 2000), and TER (Snover et al, 2006) do not correctly reflect the meaning similarities of the input sentence. Results are presented in terms of BLEU (Papineniet al, 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005 ) metrics. We consider four widely used MT metrics (BLEU, NIST, METEOR (Banerjee and Lavie, 2005) (v0.7), and TER) as our baselines. In order to attack these problems, some metrics have been proposed to include more linguistic information into the process of matching ,e.g., Meteor (Banerjee and Lavie, 2005) metric and MaxSim (Channad Ng, 2008) metrics, which improve the lexical level by the synonym dictionary or stemming technique. Another approach is taken by two other commonly used metrics, ME TEOR (Banerjee and Lavie, 2005) and TER (Snoveret al, 2006). Instead we carry out extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics: BLEU (Papineni et al, 2002), METEOR (Banerjee and Lavie, 2005), NIST (Doddington, 2002), WER, PER and TER (Snover et al, 2006). In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium? s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al, 2003), Translation Error Rate (TER) (Snover et al, 2006) 1, and METEOR (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment. Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. We also used the paraphrase database TERp for METEOR (Banerjee and Lavie, 2005). For evaluation we have selected a set of 8 metric variants corresponding to seven different families: BLEU (n= 4) (Papineni et al, 2001), NIST (n= 5) (Lin and Hovy, 2002), GTM F1-measure (e= 1, 2) (Melamed et al, 2003), 1-WER (Nie? en et al, 2000), 1-PER (Leusch et al, 2003), ROUGE (ROUGE-S*) (Lin and Och, 2004) and METEOR3 (Banerjee and Lavie, 2005). BLEU (Papineni et al, 2002), TER (Snover et al, 2006) and METEOR (Banerjee and Lavie, 2005) scores will be reported. In this paper ,translation quality is evaluated according to (1) the BLEU metrics which calculates the geometric mean of n gram precision by the system output with respect to reference translations (Papineni et al, 2002), and (2) the METEOR metrics that calculates uni gram overlaps between translations (Banerjee and Lavie, 2005). There exists a variety of different metrics ,e.g., word error rate, position-independent word error rate, BLEU score (Papineni et al, 2002), NIST score (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), GTM (Turian et al, 2003). The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (Papineni et al, 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) scores. e.g. Meteor (Banerjee and Lavie, 2005). Though we could have used a further downstream measure like BLEU, METEOR has also been shown to directly correlate with translation quality (Banerjee and Lavie, 2005) and is simpler to measure. For the evaluation of our system, we used a number of widely accepted automatic metrics, namely BLEU (Papineni et al, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al, 2006) and inverse F-Score based on token-level precision and recall. In most cases this amounts to an improvement of about 1.5 Bleu points (Papineni et al, 2002) and 1.5 Meteor points (Banerjee and Lavie, 2005). Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie,2005).
An Empirical Evaluation Of Knowledge Sources And Learning Algorithms For Word Sense Disambiguation In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data. Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations. The learning algorithms evaluated include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree algorithms. We present empirical results showing the relative contribution of the component knowledge sources and the different learning algorithms. In particular, using all of these knowledge sources and SVM (i.e., a single learning algorithm) achieves accuracy higher than the best official scores on both SENSEVAL-2 and SENSEVAL-1 test data. In this paper, we used the WSD program reported in (Lee and Ng, 2002). More precisely, we follow (Lee and Ng, 2002), a reference work for WSD, by adopting a Support Vector Machines (SVM) classifier with a linear kernel and three kinds of features for characterizing each considered occurrence in a text of the reference word. Only features based on syntactic relations are not taken from (Lee and Ng, 2002) since their use would have not been coherent with the window based approach of the building of our initial thesaurus. We adopt the feature design used by Lee and Ng (2002), which consists of the following four types: (1) Local context: n-grams of nearby words (position sensitive); (2) Global context: all the words (excluding stop words) in the given context (position-insensitive; a bag of words); (3) POS: parts-of-speech n-grams of nearby words; (4) Syntactic relations: syntactic information obtained from parser output. Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM. Our approach to building a preposition WSD classifier follows that of Lee and Ng (2002), who evaluated a set of different knowledge sources and learning algorithms for WSD. For every preposition, a baseline maxent model is trained using a set of features reported in the state-of-the-art WSD system of LeeandNg (2002). Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation.  For each ambiguous word in ELS task of SENSEVAL-3, we used three types of features to capture contextual information: part-of-speech of neighboring words with position information, unordered single words in topical context, and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations). Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002). Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002). These feature types have been widely used in WSD algorithms (see Lee and Ng (2002) for an evaluation of their effectiveness). These knowledge sources were effectively used to build a state-of-the-art WSD pro gram in one of our prior work (Lee and Ng, 2002). Similar to our previous work (Chan and Ng, 2005b), we used the supervised WSD approach described in (Lee and Ng, 2002) for our experiments, using the naive Bayes algorithm as our classifier. For the experiments reported in this paper, we follow the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local col locations, parts-of-speech (POS), and surrounding words. The features used in these systems usually include local features, such as part-of-speech (POS) of neighboring words, local collocations, syntactic patterns and global features such as single words in the surrounding context (bag-of-words) (Lee and Ng, 2002).  We adopt the same syntactic relations as (Lee and Ng, 2002). In the SVM (Vapnik, 1995) approach, we first form a training and a testing file using all standard features for each sense following (Lee and Ng, 2002) (one classifier per sense).
TextRank: Bringing Order Into Texts In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications. In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks. In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data. Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.   Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex. TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction. PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight. Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable. Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009). In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy. More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words. If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper. As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming. Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year. As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term. Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004). Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction. The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004). We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004). In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.
The Alignment Template Approach To Statistical Machine Translation A phrase-based statistical machine translation approach — the alignment template approach — is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source–channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different For the German–English speech we analyze the effect of various syscomponents. On the French–English Canadian the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese–English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems. Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004). We use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004) to all 5-grams. The retrieval results are then used as input to a standard SMT pipeline to train translation models, starting from unsupervised induction of word alignments (Och and Ney, 2000) to phrase-extraction (Och and Ney, 2004) and phrase-based decoding (Koehn et al, 2007). Based on these word-alignments, we extract phrases by applying the grow-diag-final-and heuristic and using Och and Ney (2004)'s phrase extraction algorithm as implemented in Moses (Koehn et al, 2007). Our phrase-based SMT system is similar to the alignment template system described in Och and Ney (2004). Various feature functions (Och and Ney, 2004) are then computed over the entries in the phrase table. Translation is performed using a standard dynamic programming beam-search decoder (Och and Ney, 2004). Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the phrase pairs according to the definition of Och and Ney (2004) and Chiang (2005) with the additional constraint that each phrase pair contains at most one word link. This discussion, which is partly based on Section 4.1.2 of (Och and Ney, 2004), means that the lexical translation probability pw (f|e) is another probability estimated using the word translation probability w (f|e). As described above, our base system is a phrase based statistical MT system, similar to that of Och and Ney (2004). Galley and Manning (2008) propose an algorithm that begins by running standard phrase extraction (Och and Ney, 2004) without a phrase-length limit, noting the corners of each phrase found. We consider the standard phrase-based approach to MT (Och and Ney, 2004). Breadth-first beam search algorithm of Och and Ney (2004). We assume familiarity with these operations, which are described in detail in (Och and Ney, 2004). Here we refer to Bilingual Phrase (BP) as the bilingual phrases used by Och and Ney (2004). The extraction of SSRs is similar to the well known phrase extraction algorithm (Och and Ney, 2004). For example, given a source phrase f and a target phrase e, the phrase pair (f, e) is said to be consistent (Och and Ney, 2004) with the alignment if and only if: (1) there must be at least one word in side one phrase aligned to a word inside the other phrase and (2) no words inside one phrase can be aligned to a word outside the other phrase. Och and Ney (2004) describe a phrase-extract algorithm for extracting phrase pairs from a sentence pair annotated with a 1-best alignment. The methods for calculating relative frequencies (Och and Ney, 2004) and lexical weights (Koehn et al, 2003) are also adapted for the weighted matrix case. First, many-to-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Och and Ney, 2004).
Phrasal Cohesion And Statistical Machine Translation There has been much interest in using phrasal movement to improve statistical machine translation. We explore how well phrases cohere across two languages, specifically English and French, and examine the particular conditions under which they do not. We demonstrate that while there are cases where coherence is poor, there are many regularities which can be exploited by a statistical machine translation system. We also compare three variant syntactic representations to determine which one has the best properties with respect to cohesion. The second constraint, known as the cohesion constraint (Fox, 2002), uses the dependency tree (Melcuk, 1987) of the English sentence to restrict possible link combinations. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Similar alternations are rife in bilingual data, e.g., ne pas in French (Fox, 2002) and separable prefixes in German (Collins et al 2005). In addition to lexical translation, our system models structural changes and changes to feature values, for although dependency structures are fairly well preserved across languages (Fox, 2002), there are certainly many instances where the structure must be modified. For this, we need a formalism that is expressive enough to deal with cases of syntactic divergence between source and target languages (Fox, 2002): for any given (pi, f, a) triple, it is useful to produce a derivation that minimally explains the transformation be tween pi and f, while remaining consistent with a. Fox (2002) measured phrasal cohesion in gold standard alignments by counting crossings. However, as Fox (2002) showed, even in a language pair as close as French-English, there are situations where phrasal cohesion should not be maintained. Fox (2002) showed that many common reorderings fall outside the scope of synchronous grammars that only allow the reordering of child nodes. Compare the systematic study for English-French alignments by (Fox, 2002), who compared (i) tree-bank parser style analyses, (ii) a variant with flattened VPs, and (iii) dependency structures. In real corpora, cases such as node N2 are frequent enough to be noticeable (see Fox (2002) or section 4.1 in this paper). Similar to the Pharoah package (Koehn, 2004), we extract phrase-pairs directly from word alignment together with coherence constraints (Fox, 2002) to remove noisy ones. It encodes semantic relations directly, and has the best inter-lingual phrasal cohesion properties (Fox,2002). One of assumptions of phrase-based SMT is that phrase cohere across two languages (Fox, 2002), which means phrases in one language tend to be moved together during translation. (Fox, 2002) is characterized by its simplicity, which has attracted researchers for years. Dependencies were found to be more consistent than constituent structure between French and English by Fox (2002), though this study used a tree representation on the English side only. This confirms the results of Fox (2002) and Galley et al (2004) that many translation operations must span more than one parse tree node. The advantages of modeling how a target language syntax tree moves with respect to a source language syntax tree are that (i) we can capture the fact that constituents move as a whole and generally respect the phrasal cohesion constraints (Fox, 2002), and (ii) we can model broad syntactic reordering phenomena, such as subject-verb-object constructions translating into subject-object-verb ones, as is generally the case for English and Japanese. In a very interesting study of syntax in statistical machine translation, Fox (2002) looks at how well proposed translation models fit actual translation data. Previous to Fox (2002), it had been observed that this model would prohibit certainre-orderings in certain language pairs (such as subject VP (verb-object) into verb-subject-object), but Fox carried out the first careful empirical study, showing that many other common translation patterns fall outside the scope of the child-reordering model. For this reason, we think it is important to learn from the model/data explain ability studies of Fox (2002) and to extend her results.
Algorithms for Deterministic Incremental Dependency Parsing Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a nonprojective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework. It is based on the transition-based dependency parsing paradigm (Nivre, 2008). If we consider transition-based dependency parsing (Nivre, 2008), the purely bottom-up strategy is implemented by the arc-standard model of Nivre (2004). The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). For a more detailed presentation of this subject, we refer the reader to Nivre (2008). Quite impressively, models based on deterministic shift-reduce parsing algorithms are able to rival the other computationally more expensive models (see Nivre (2008) and references therein for more details). Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al, 2007), trained on the PADT. We use McNemar's statistical significance test as implemented by Nilsson and Nivre (2008), and denote p < 0.05 and p < 0.01 with+ and ++, respectively. Nivre (2008) reports that non-projective and pseudo projective algorithms outperform the eager projective algorithm in MaltParser, but our training data did not contain any non-projective dependencies. The Nivre standard algorithm is also reported to do better on Arabic, but in a preliminary experimentation, it did similarly or slightly worse than the eager one, perhaps due to high percentage of right branching (left headed structures) in our Arabic training set, an observation already noted in Nivre (2008). Transition-based: An implementation of the transition-based dependency parsing framework (Nivre, 2008) using an arc-eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8. Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al, 2010). To prove the correctness of the system in Figure 1 for the projective dependency graph, we use the proof strategy of (Nivre, 2008a). Following Nivre (2008), we define a transition system for dependency parsing as a quadruple S= (C, T, cs, Ct), where 1. C is a set of configurations, 2. T is a set of transitions, each of which is a (partial) function t : C → C, 3. cs is an initialization function, mapping a sentence x to a configuration c ∈ C, 4. Ct ⊆ C is a set of terminal configurations. This definition of transition sequence differs from that of Nivre (2008) but is equivalent and suits our presentation better. In this paper, we assume that the resulting dependency graph for a sentence is well-formed and projective (Nivre, 2008). studies taking data-driven approaches, by (Kudo and Matsumoto, 2002), (Yamada and Matsumoto,2003), and (Nivre, 2003), the deterministic incremental parser was generalized to a state transition system in (Nivre, 2008). All of the parsing experiments reported in this study are based on the transition-based dependency parsing paradigm (Nivre, 2008). The core dependency parser we use is an implementation of a transition-based dependency parser using an arc-eager transition strategy (Nivre, 2008). There are two dominant transition based dependency parsing systems, namely the arc-standard and the arc-eager parsers (Nivre, 2008). Nivre (2008) gave a systematic description of the arc-standard and arc-eager algorithms, currently two popular transition-based parsing methods for word-level dependency parsing.
Unsupervised Part-Of-Speech Tagging Employing Efficient Graph Clustering An unsupervised part-of-speech (POS) tagging system that relies on graph clustering methods is described. Unlike in current state-of-the-art approaches, the kind and number of different tags is generated by the method itself. We compute and merge two partitionings of word graphs: one based on context similarity of high frequency words, another on log-likelihood statistics for words of lower frequencies. Using the resulting word clusters as a lexicon, a Viterbi POS tagger is trained, which is refined by a morphological component. The approach is evaluated on three different languages by measuring agreement with existing taggers.  In Biemann (2006b), the tagger output was directly compared to supervised taggers for English, German and Finnish via information-theoretic measures. We train SVM Tool and an unsupervised tagger, Unsupos (Biemann, 2006), on our training sections and apply them to the development, test and unlabeled sections. Nevertheless, many of the practically useful spell checkers incorporate context information and the current analysis on SpellNet can be extended for such spell-checkers by conceptualizing a network of words that capture the word co-occurrence patterns (Biemann, 2006).  Previous graph-theoretic work (Biemann, 2006) uses order 1 representations. In order to test the argument above, and as an attempt to improve the results from the previous experiment, POS-tags were induced using Biemann's unsupervised POS-tagger (Biemann, 2006). Additionally, we used an unsupervised part-of-speech tagger (see (Biemann, 2006)) to tag the NEGRA corpus to be able to present a complete unsupervised parsing process relying on word strings only. Mintz (2003) only uses the most frequent 45 frames and Biemann (2006) clusters the most frequent 10,000 words using contexts formed from the most frequent 150-200 words.    Perhaps due to the overly simplistic methods employed to compute morphological information, morphology has only been used as what Biemann (2006) called add-on's in existing POS induction algorithms, which remain primarily distributional in nature. Biemann (2006) described a graph-based clustering methods for word classes.
Weakly Supervised Named Entity Transliteration And Discovery From Multilingual Comparable Corpora Named Entity recognition (NER) is an important part of many natural language processing tasks. Current approaches often employ machine learning techniques and require supervised data. However, many languages lack such resources. This paper presents an (almost) unsupervised learning algorithm for automatic discovery of Named Entities (NEs) in a resource free language, given a bilingual corpora in which it is weakly temporally aligned with a resource rich language. NEs have similar time distributions across such corpora, and often some of the tokens in a multi-word NE are transliterated. We develop an algorithm that exploits both observations iteratively. The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration. Seeded with a small number of transliteration pairs, our algorithm discovers multi-word NEs, and takes advantage of a dictionary (if one exists) to account for translated or partially translated NEs. We evaluate the algorithm on an English-Russian corpus, and show high level of NEs discovery in Russian. Klementiev and Roth (2006) explore the use of a perceptron-based ranking model for the purpose of finding name transliterations across comparable corpora.  The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al, 2006) and train a discriminative classifier.  Our initial feature extraction method follows the one presented in (Klementiev and Roth, 2006a), in which the feature space consists of n-gram pairs from the two languages. As stated by (Klementiev and Roth, 2006), the projection of NER tags is easier in comparison to projecting other types of annotations such as POS-tags and BPC.  The iterative training algorithm described above is adopted from Klementiev and Roth (2006). (Klementiev and Roth, 2006) bootstrap with a classifier used interchangeably with an unsupervised temporal alignment method. Our initial feature extraction scheme follows the one presented in (Klementiev and Roth, 2006), in which the feature space consists of n-gram pairs from the two languages. We evaluated our approach in two settings; first, we compared our system to a baseline system described in (Klementiev and Roth, 2006). Note that one of the models proposed in (Klementiev and Roth, 2006b) takes advantage of the temporal information. Our best model, the unsupervised learning with all constraints, outperforms both models in (Klementiev and Roth, 2006b), even though we do not use any temporal information. The Russian data set, originally introduced in (Klementiev and Roth, 2006b), is comprised of temporally aligned news articles.  For Russian, we compare to the model presented in (Klementiev and Roth, 2006b), a weakly supervised algorithm that uses both phonetic information and temporal information. We compared our algorithm to two models described in (Klementiev and Roth, 2006b) one uses only phonetic similarity and the second also considers temporal co-occurrence similarity when ranking the transliteration candidates. This configuration is equivalent to the model used in (Klementiev and Roth, 2006b). The extraction proceeds either iteratively by starting from a few seed extraction rules (Collins and Singer, 1999), or by mining named entities from comparable news articles (Shinyama and Sekine, 2004) or from multilingual corpora (Klementiev and Roth, 2006). More recently, Klementiev and Roth (2006) also use F-index (Hetland, 2004), a score using DFT, to calculate the time distribution similarity.
Corpus-Based Induction Of Syntactic Structure: Models Of Dependency And Constituency present a generative model for the learning of dependency structures. We also describe the multiplicative combination of this dependency model with a model of linear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published figures for undependency parsing constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data.  However, previous computational models of grammar induction (Klein and Manning, 2004), including infant grammar induction (Kwiatkowski et al, 2012), have not addressed filler-gap comprehension. In the field of language acquisition computational linguists such as Klein and Manning (2004) have studied the unsupervised acquisition of syntactic structure, while linguists such as Boersma and Hayes (2001), Gold smith (2001), Pater (2008) and Albright and Hayes (2003) are developing probabilistic models of the acquisition of phonology and/or morphology, and Frank et al (2007) experimentally tests the predictions of a Bayesian model of lexical acquisition.  We use the standard generative Dependency Model with Valence (Klein and Manning, 2004). As we explain at the end of this section, without this aspect the generative story closely resembles the classic dependency model with valence (DMV) of Klein and Manning (2004). We follow an approach similar to the widely-referenced DMV model (Klein and Manning, 2004), which forms the basis of the current state-of-the-art unsupervised grammar induction model (Headden III et al, 2009). We encode more detailed valence information than Klein and Manning (2004) and condition child generation on parent valence. The resulting simplified model closely resembles DMV (Klein and Manning, 2004), except that it 1) explicitly generate words x rather than only part of-speech tags s, 2) encodes richer context and valence information, and 3) imposes a Dirichlet prior on the symbol distribution.    Finally, following (Klein and Manning, 2004) we strip out punctuation from the sentences. The maximum unsupervised accuracy it achieved on the Bulgarian data is 47.6% with initialization from Klein and Manning (2004) and this result is not stable. In the generative models of section 5, f has the form of a dependency model with valence (Klein and Manning, 2004). Although we could also try many random starting points, the initializer in Klein and Manning (2004) performs quite well. While these results are worse than those obtained previously for this model, the experiments in Klein and Manning (2004) only used sentences of 10 words or fewer, without punctuation, and with gold-standard tags. Second, consistent syntactic representations are desirable in the evaluation of unsupervised (Klein and Manning, 2004) or cross-lingual syntactic parsers (Hwa et al., 2005). Fortunately, the state of the art in broad-coverage (Lin, 1993) and unsupervised (Klein and Manning, 2004) dependency parsing allows us to treat dependency parsing merely as a preprocessing step. In DMV (Klein and Manning, 2004) and in the extended model EVG (Headden III et al, 2009), there is a STOP sign indicating that no more dependents in a given direction will be generated.
SemEval-2010 Task 14: Word Sense Induction &#x26;Disambiguation This paper presents the description and evaluation framework of SemEval-2010 Word Sense Induction & Disambiguation task, as well as the evaluation results of 26 participating systems. In this task, participants were required to induce the senses of 100 target words using a training set, and then disambiguate unseen instances of the same words using the induced senses. Systems’ answers were evaluated in: (1) an unsupervised manner by using two clustering evaluation measures, and (2) a supervised manner in a WSD task. Finally in terms of evaluation, our future work also focuses on evaluating HRGs using a fine grained sense inventory, extending the evaluation on the SemEval-2010 WSI task dataset (Manandhar et al., 2010) as well as applying HRGs to other related tasks such as taxonomy learning. Similar to Manandhar et al (2010), we use WordNet to first randomly select one synset of the first word, we then construct a set of words in various relations to the first word's chosen synset, including hypernyms, hyponyms, holonyms, meronyms and attributes. Last, we include all the nouns and verbs used in the SemEval 2010 WSI Task (Manandhar et al, 2010), which are used in our evaluation. Our primary WSI evaluation is based on the dataset provided by the SemEval-2010 WSI shared task (Manandhar et al 2010). The original task also made use of V-measure and Paired F-score to evaluate the induced word sense clusters, but have degenerate behaviour in correlating strongly with the number of senses induced by the method (Manandhar et al 2010), and are hence omitted from this paper. As a second experiment, we analyze incorrect sense assignments on SemEval-2 Task 14 (Manandhar et al., 2010) to measure whether sense-relatedness biases which sense was incorrectly selected. Our word sense induction and disambiguation model is trained and tested on the dataset of the SEMEVAL-2010 WSI/WSD task (Manandhar et al, 2010). In the paired F-Score (Artiles et al, 2009) evaluation, the clustering problem is transformed into a classification problem (Manandhar et al, 2010). However, this induction step has proven to be greatly challenging, in the most recent shared tasks, induction systems either appear to perform poorly or fail to outperform the simple Most Frequent Sense baseline (Agirre and Soroa, 2007a; Manandhar et al, 2010). We first propose evaluating ensemble configurations of Word Sense Induction models using the standard shared tasks from SemEval-1 (Agirre and Soroa, 2007a) and SemEval-2 (Manandhar et al, 2010).
A Unigram Orientation Model For Statistical Machine Translation Lebanese violate warplanes Israeli airspace A l T A } r A t A l H r b y P y l y P A l A s r A } t n t h k A l m j A l A l j w y A l l b n A n y Abstract In this paper, we present a unigram segmentation model for statistical machine translation where the segmentation units are blocks: pairs of phrases without internal structure. The segmentation model uses a novel orientation component to handle swapping of neighbor blocks. During training, we collect block uncounts with we count how often a block occurs to the left or to the right of some predecessor block. The orientation model is shown to improve translation performance over two models: 1) no block re-ordering is used, and 2) the block swapping is controlled only by a language model. We show experimental results on a standard Arabic-English translation task. A DP-based beam search procedure identical to the one used in (Tillmann,2004) is used to maximize over all oriented block segmentations. For details see (Tillmann, 2004).  We focus on improving the modelling of reordering within Hiero and include discriminative reordering features (Tillmann, 2004) and a distance cost feature, both of which are not modeled in the original Hiero system. The phrase-based reordering model (Tillmann, 2004) determines the presence of the adjacent bilingual phrase located in position (s-1,v+1) and then treats the orientation of bp as S. The phrase-based lexical reordering model (Tillmann, 2004) is also closely related to our model. One novelty this year are the introduction of lexicalized reordering models (Tillmann, 2004). Both Moses and our system are evaluated with and without lexicalized reordering (Tillmann, 2004) . (Tillmann, 2004) learns for each phrase a tendency to either remain monotone or to swap with other phrases. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a weak distance-based distortion model; and finally a word bonus model and a tuple-bonus model which compensate for the system preference for short translations. In baseline experiments we used a phrase dependent lexicalized reordering model, as proposed in Tillmann (2004). From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al, 2007). The (Tillmann, 2004) paper introduced lexical features for distortion modeling. An other obvious system improvement would be to incorporate more advanced word-based features in the DTs, such as questions about word classes (Tillmann and Zhang 2005, Tillmann 2004). Tillmann (2004) used a lexical reordering model, and Galley et al (2004) followed a syntactic-based model. 
WordNet 2 - A Morphologically And Semantically Enhanced Resource Montemagni and Structural Patterns vs string patterns for extracting semantic from dictionaries In of '92, pp 546-552 Y Ravin Disambiguating and interpreting verb def However, as various researchers have pointed out (Harabagiu et al, 1999), these networks lack information, in particular with regard to syntagmatic associations, which are generally unsystematic. Work on the Extended WordNet project (Harabagiu et al, 1999) is achieving substantial progress in making the information in WordNet more explicit. For example, in eXtended WordNet (Harabagiu et al 1999), the rich glosses in WordNet are enriched by disambiguating the nouns, verbs, adverbs, and adjectives with synsets. In the sections to follow we describe a mechanism for automating the extraction of these relationships (in the same vein as (Harabagiu et al 1999), and for using them to generative apt interpretations for metaphors involving WordNet entries. Harabagiu et al (1999) proposed a scheme for attaching sense tags to predicates within the framework of transforming WordNet glosses into a logical form. Endeavors such as that of Harabagiu et al (1999), in which each of the textual glosses in WordNet (Fellbaum, 1998) is linguistically analyzed to yield a sense-tagged logical form, is an example of the former approach. For example, in eXtended WordNet (Harabagiu et al 1999), the glosses in WordNet are enriched by disambiguating the nouns, verbs, adverbs, and adjectives with synsets. We obtain the data of the I relation from eXtended WordNet (Harabagiu et al, 1999), an automatically sense-disambiguated version of WordNet in which every term occurrence in every gloss is linked to the synset it is deemed to belong to. The transformation of WordNet into a graph based on the I relation would of course be nontrivial, but is luckily provided by eXtended WordNet (Harabagiu et al, 1999), a publicly available version of WordNet in which (among other things) each term sk occurring in a WordNet gloss (except those in example phrases) is lemmatized and mapped to the synset in which it belongs. The eXtended WordNet (Harabagiu et al, 1999) project aims to transform the WordNet glosses into a format that allows the derivation of additional semantic and logic relations. In parallel, efforts have been made to enrich WordNet by adding information in glosses (Harabagiu et al, 1999). Hence, it is not very surprising that they were criticized, as in (Harabagiu et al, 1999), for not being suitable for Natural Language Processing. They use sense disambiguated glosses provided by eXtended WordNet (Harabagiu et al, 1999) to link synsets by starting with positive (or negative) sentiment concepts in order to find other concepts with positive (or negative) sentiment values.
A Composite Kernel To Extract Relations Between Entities With Both Flat And Structured Features This paper proposes a novel composite kernel for relation extraction. The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples. The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction. Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features. Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction. To benefit from both views, a composite kernel (Zhanget al, 2006) integrates the flat features from entities and structured features from parse trees. For the similarity matrix W in section 4.1 and the kernel K in section 4.2, we used the composite kernel function (Zhang et al, 2006), which is based on structured features and entity-related features. For example, by combining tree kernels and convolution string kernels, (Zhang et al, 2006) achieved the state of the art performance on ACE (ACE, 2004), which is a benchmark dataset for relation extraction. For comparison, we use the same setting as (Zhang et al, 2006), by applying a 5-fold cross-validation. We also compare our approaches to the other state-of-the-art approaches including Convolution Tree kernel (Collinsand Duffy, 2001), Syntactic kernel (Zhao and Grishman, 2005), Composite kernel (linear) (Zhang et al, 2006) and the best kernel in (Nguyen et al, 2009). (Zhang et al, 2006) showed that by carefully choosing the weight of each component and using a polynomial expansion, they could achieve the best performance on this data: 72.1% F measure. For example, by combining tree kernels and convolution string kernels, (Zhang et al., 2006) achieved the state of the art performance on ACE data (ACE, 2004). In (Bunescu and Mooney, 2005a) dependency graph features are exploited, and in (Zhang et al, 2006a) syntactic features are employed for relation extraction. For the first time, in (Zhang et al, 2006a), this convolution tree kernel was used for relation extraction. Moreover, this tree kernel is combined with an entity kernel to form a reportedly high quality composite kernel in (Zhang et al, 2006b). AAP is computed over the MCT tree portion which is also proposed by (Zhang et al, 2006a) and is the sub-tree rooted at the first common ancestor of relation arguments. The parameter of CD'01 kernel is set to 0.4 according to (Zhang et al., 2006a). Although the path-enclosed tree portion (PT) (Zhang et al, 2006a) seems to be an appropriate portion of the syntactic tree for relation extraction, it only takes into account the syntactic information between the relation arguments, and discards many useful features (before and after the arguments features). Motivated by the work of (Zhang et al, 2006), we here examine four cases that contain different sub-structures as shown in Fig. Zhang et al (2006) discover that the Shortest Path enclosed Tree (SPT) achieves the best performance. Zhang et al (2006) describe a convolution tree kernel (CTK, Collins and Duffy, 2001) to investigate various structured information for relation extraction and find that the Shortest Path enclosed Tree (SPT) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus. Zhang et al (2006) design a composite kernel consisting of an entity linear kernel and a standard CTK, obtaining the F-measure of 72.1 on the 7 relation types in the ACE RDC 2004 corpus. Zhang et al (2006) explore five kinds of tree spans and find that the Shortest Path-enclosed Tree (SPT) achieves the best performance. In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. Experiments by Zhang et al (2006) show that linear kernel using only entity features contributes much when combined with the convolution parse tree kernel.
met*: A Method For Discriminating Metonymy And Metaphor By Computer The met* method distinguishes selected examples of metonymy from metaphor and from literalness and anomaly in short English sentences. In the met* method, literalness is distinguished because it satisfies contextual constraints that the nonliteral others all violate. Metonymy is discriminated from metaphor and anomaly in a way that [1] supports Lakoff and Johnson's (1980) view that in metonymy one entity stands for another whereas in metaphor one entity is viewed as another, [2] permits chains of metonymies (Reddy 1979), and [3] allows metonymies to co-occur with instances of either literalness, metaphor, or anomaly. Metaphor is distinguished from anomaly because the former contains a relevant analogy, unlike the latter. The met* method is part of Collative Semantics, a semantics for natural language processing, and has been implemented in a computer program called meta5. Some examples of meta5's analysis of metaphor and metonymy are given. The met* method is compared with approaches from artificial intelligence, linguistics, philosophy, and psychology. Computational approaches are mostly concerned with inferring implicitly expressed metonymic relations in English texts (Fass 1991). In his program, (Fass 1991) makes use of formal definitions of several kinds of metonymic relations. Our view of relations in a concept network being the interpretations of metonymies is strongly reminiscent of older work in metonymy resolution such as Hobbs et al1993), Fass (1991), Markert and Hahn (2002) or the use of a generative lexicon.  Fass (Fass, 1991) uses selectional preference violation technique to detect metaphors. Wilks (1978) advocates that the typically hard constraints that define a literal semantics should instead be modeled as soft preferences that can accommodate the violations that arise in metaphoric utterances, while Fass (1991) builds on this view to show how these violation scan be repaired to thus capture the literal intent be hind each metaphor. One of the first attempts to identify and interpret metaphorical expressions in text automatically is the approach of Fass (1991). Fass (1991) developed a system called met*, capable of discriminating between literalness, metonymy, metaphor and anomaly. Another system ,met* (Fass, 1991), is designed to distinguish both metaphor and metonymy from literal text, providing special techniques for processing these instances of figurative language. For example, 'the rock is becoming brittle with age' (Reddy, 1969, p. 242), has "a literal interpretation when uttered about a stone and a metaphorical one when said about a decrepit professor emeritus" (Fass, 1991, p. 54). One of the first attempts to identify and interpret metaphorical expressions in text automatically is the approach of Fass (1991). Fass (1991) developed a system called met*, capable of discriminating between literalness, metonymy, metaphor and anomaly. Almost simultaneously with the work of Fass (1991),.  A similar path-finding methodology for deriving metonymies was used by the met* system (Fass, 1991), in which connections between the sense frames of textual concepts are retrieved from a lexicon of the size of 500 word senses. He drank his glass. (Fass, 1991). 
Extracting Parallel Sentences from Comparable Corpora using Document Level Alignment The quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training. In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). One resource not yet thoroughly explored is Wikipedia, an online encyclopedia containing linked articles in many languages. We advance the state of the art in parallel sentence extraction by modeling the document level alignment, motivated by the observation that parallel sentence pairs are often found in close proximity. We also include features which make use of the additional annotation given by Wikipedia, and features using an automatically induced lexicon model. Results for both accuracy in sentence extraction and downstream improvement in an SMT system are presented. More recently, (Smith et al, 2010) reported significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model. A recent study by Smith et al (2010) extracted parallel sentences from comparable corpora to extend the existing resources. This methodology is similar to that of Smith et al (2010). Many websites are available in multiple languages, and unlike other potential sources — such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010) — it is common to find document pairs that are direct translations of one another. For these experiments, we also include training data mined from Wikipedia using a simplified version of the sentence aligner described by Smith et al (2010), in order to determine how the effect of such data compares with the effect of web mined data. Unfortunately, it is difficult to obtain meaningful results on some open domain test sets such as the Wikipedia dataset used by Smith et al (2010). For example, Smith et al (2010) mine parallel sentences from comparable documents in Wikipedia, demonstrating substantial gains on open domain translation. As this is computationally intensive, most studies fall back to heuristics, e.g., comparing news articles close in time (Munteanu and Marcu, 2005), exploiting "inter-wiki" links in Wikipedia (Smith et al., 2010), or bootstrapping off an existing search engine (Resnik and Smith, 2003). While several recent works on dealing with large bilingual collections of texts, e.g. (Smith et al., 2010), seek for extracting parallel sentences from comparable corpora, we present PARADOCS, a system designed to recognize pairs of parallel documents in a (large) bilingual collection of texts. Smith et al (2010) extended these previous lines of work in several directions. To create our dataset, we followed Smith et al (2010) to find parallel-foreign sentences using comparable documents linked by inter-wiki links. Wikipedia has become an attractive source of comparable documents in more recent work (Smith et al, 2010).
Bootstrapping POS-Taggers Using Unlabelled Data This paper investigates booststrapping part-ofspeech taggers using co-training, in which two taggers are iteratively re-trained on each other’s output. Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set. We investigate selecting examples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature. Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. We do not run self-training for POS tagging as it has been shown unuseful for this application (Clark et al 2003). paradigm in which a small amount of labeled data is available (see, e.g., Clark et al (2003)). Indeed, (Clark et al, 2003) applied self training to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increased; the same seems to be the case in Wang et al (2007) who use co-training of two diverse POS taggers. Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increases. Clark et al (2003) applies self-training to POS-tagging and reports the same outcomes. Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al, 2003). Clark et al (2003) were unable to improve the accuracy of POS tagging using self-training. Co-training has been applied to a number of NLP applications, including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). Reference resolution (Ng and Cardie 2003), POS tagging (Clark et al, 2003), and parsing (McClosky et al, 2006) were shown to be benefited from self-training. For self-training we use the definition by (Clark et al., 2003): a tagger that is retrained on its own labeled cache on each round. Steedman et al (2003) and Clark et al (2003) present co-training procedures for parsers and taggers respectively, which are effective when only very little labeled data is available. According to Clark et al (2003), self-training is a procedure in which a tagger is retrained on its own labeled cache at each round. In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used. Moreover, (Clark et al, 2003) show that a naive co-training process that does not explicitly seek to maximize agreement on unlabelled data can lead to similar performance, at a much lower computational cost. (Clark et al, 2003) provide a different definition: self-training is performed using a tagger that is retrained on its own labeled cache on each round. However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations. Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). Following Clark et al (2003), we applied self training as described in Algorithm 1, with the perceptron as the supervised learner.
Survey Article: Inter-Coder Agreement for Computational Linguistics This article is a survey of methods for measuring agreement among corpus annotators. It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff’s alpha as well as Scott’s pi and Cohen’s kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappalike measures in computational linguistics, may be more appropriate for many corpus annotation tasks—but that their use makes the interpretation of the value of the coefficient even harder. Inter-annotator agreement measures for NLP applications have been recently discussed by Artstein and Poesio (2008) who advocate for the use of chance corrected measures. Near misses occur frequently in segmentation, although manual coders often agree upon the bulk of where segment lie, they frequently disagree upon the exact position of boundaries (Artstein and Poesio, 2008, p. 40). Fournier and Inkpen (2012, p. 156-157) adapted four inter-coder agreement formulations provided by Artstein and Poesio (2008) to use S to award partial credit for near misses, but because S produces cosmetically high agreement values they grossly overestimate agreement. For more details on terminology issues, we refer to the introduction of (Artstein and Poesio, 2008). For more information on the terminology issue, refer to the introduction of (Artstein and Poesio, 2008). It is k = 0.72 on the full set, which is considered acceptable according to Artstein and Poesio (2008). Only the agreement on verbs is slightly below the acceptability threshold of 0.67 (Artstein and Poesio, 2008). Given the aims of our classification, and of STaRS.sys in general, we choose to evaluate our coding scheme by asking to a group of non experts to label a subset of the non-normalized Kremer et al's (2008) norms and measuring the inter-coder agreement between them (Artstein and Poesio, 2008), adhering to the Krippendorff's (2004, 2008) recommendations. As pointed out by Artstein and Poesio (2008), agreement doesn't ensure validity. Based on Cohen's seminal work (Cohen, 1968), Artstein and Poesio (2008) suggest the measure in (4), where k is calculated as the weighted difference between observed and expected disagreement. Our k values generally fall within the range that Landis and Koch (1977) deem "moderate agreement", but below the .8 cut-off tentatively suggested by Artstein and Poesio (2008). The individual Kappa scores largely fall into the range that Landis and Koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (Artstein and Poesio, 2008). A comprehensive overview of methods for measuring the inter-annotator agreement in various areas of computational linguistics was given in Artstein and Poesio (2008). In addition to percentage agreement, we measured Cohen's k (Artstein and Poesio, 2008) between all 3 possible annotator pairings. However, the interpretation of k is still under discussion (Artstein and Poesio, 2008). Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over 75 are excellent (Fleiss, 1971). Although mean k scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan's (1988) k has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555-556).  Artstein and Poesio (2008) note that most of a coder's judgements are non-boundaries. We calculate agreement (Apia) as pairwise mean S (scaled by each item's size) to enable agreement to quantify near misses leniently, and chance agreement (Apie) can be calculated as in Artstein and Poesio (2008).
Recovering Implicit Information This paper describes the SDC PUNDIT, (Prolog UNDerstands Integrated Text), system for processing language PUNDIT, written in Prolog, is a highly modular system consisting of distinct syntactic, semantic and pragmatics components. Each component draws on one or more sets of data, including a lexicon, a broad-coverage grammar of English, semantic verb decompositions, rules mapping between syntactic and semantic constituents, and a domain model. This paper discusses the communication between the syntactic, semantic and pragmatic modules that is necessary for making implicit linguistic information explicit. The key is letting syntax and semantics recognize missing linguistic entities as implicit entities, so that they can be labelled as such, and reference resolution can be directed to find specific referents for the entities. In this way the task of making implicit linguistic information explicit becomes a subset of the tasks performed by reference resolution. The success of this approach is dependent on marking missing syntactic constituents as missing semantic roles as that reference resolution can know when to look for referents. In early work, Palmer et al (1986) discussed filling null complements from context by using knowledge about individual predicates. While the work of Palmer et al (1986) relied on special lexicons, one might instead want to learn information about the semantic content of different role fillers and then assess for each of the potential referents in the discourse context whether their semantic content is close enough to the expected content of the null instantiated role. The first attempt for the automatic annotation of implicit semantic roles was proposed by Palmer et al (1986). A further improvement may derive from the integration of an anaphora resolution step, as first proposed by Palmer et al (1986) and more recently by Gerber and Chai (2010). Palmer et al (1986) made one of the earliest attempts to automatically recover extra-sentential arguments. Palmer et al (1986) treated unfilled semantic roles as special cases of anaphora and co reference resolution (CR).
Automatically Extracting And Representing Collocations For Language Generation Collocational knowledge is necessary for language gener- The problem collocations come in a large variety of forms. They can involve two, three or more words, these words can be of different syntactic categories and they can be involved in more or less rigid ways. This leads to two main difficulties: collocational knowledge has to be acquired and it must be represented flexibly so that it can be used for language generation. We address both problems in this paper, focusing on the acquisition problem. We describe a program, Xtract, that automatically acquires a range of collocations from large textual corpora and we describe how they can be represented in a flexible lexicon using a unification based formalism. The next set of lookup levels described, in which the structural word is no longer the primary entity but just a handle for indexing graphs, can be built on the basis of knowledge assembled from large volumes of text; the methodology employed by Smadja and McKeown (1990) would be one of many techniques possible for obtaining and organizing the various levels of model graphs. Collocation acquisition is, of course, not a goal by itself, but rather aims at creating collocation lexicons for both language processing and generation (Smadja and McKeown, 1990).  Previous studies have proposed many promising ways for this purpose, for instance, Smadja and McKeown (1990), and Frantzi and Ananiadou (1996) tried to treat more general structures like collocations.
Contextual Word Similarity And Estimation From Sparse Data In recent years there is much interest in word cooccurrence relations, such as n-grams, verbobject combinations, or cooccurrence within a limited context. This paper discusses how to estimate the probability of cooccurrences that do not occur in the training data. We present a method that makes local analogies between each specific unobserved cooccurrence and other cooccurrences that contain similar words, as determined by an appropriate word similarity metric. Our evaluation suggests that this method performs better than existing smoothing methods, and may provide an alternative to class based models. Dagan et al (1993) argue that using a relatively small number of classes to model the similarity between words may lead to substantial loss of information. As to the research on computing word sense relatedness, Dagan et al (1993) did some pilot work and Lee (1997) and Resnik (1999) contributed to the research on semantic similarity. In (Dagan et al., 1993) and (Pereira et al, 1993), clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time.
Fast Unsupervised Incremental Parsing This paper describes an incremental parser and an unsupervised learning algorithm for inducing this parser from plain text. The parser uses a representation for syntactic structure similar to dependency links which is well-suited for incremental parsing. In contrast to previous unsupervised parsers, the parser does not use part-of-speech tags and both learning and parsing are local and fast, requiring no explicit clustering or global optimization. The parser is evaluated by converting its output into equivalent bracketing and improves on previously published results for unsupervised parsing from plain text. Seginer (2007) has an incremental parsing approach using a novel representation called common-cover-links, which can be converted to constituent brackets. In addition, we also analyze CCM's sensitivity to initialization, and compare our results to Seginer's algorithm (Seginer, 2007). We also compare our method to the algorithm of Seginer (2007). The parser of Seginer (2007) performs slightly better on CTB 5.0 sentences no more than 10 words, but obviously falls behind on sentences no more than 40 words.  Incremental refers to the results reported in Seginer (2007). We note that some recent work gives a treatment to unsupervised parsing (but not of dependencies) directly from words (Seginer, 2007). As is customary in unsupervised parsing work (e.g. (Seginer, 2007)), we bounded sentence length by 10 (excluding punctuation). As pre-processing, we use an unsupervised parser that generates an unlabeled parse tree for each sentence (Seginer, 2007). As is customary in unsupervised parsing (e.g. (Seginer, 2007)), we bounded the lengths of the sentences in the corpus to be at most 10 (excluding punctuation). We start by parsing the corpus using the Seginer parser (Seginer, 2007). For example, the Seginer (2007) parser achieves an F-score of 75.9% on the WSJ10 corpus and 59% on the NEGRA10 corpus, but the percentage of individual sentences with an F-score of 100% is 21.5% for WSJ10 and 11% for NEGRA10. The unsupervised parser we use is the Seginer (2007) incremental parser, which achieves state-of-the-art results without using manually created POS tags. The incremental parser of (Seginer, 2007) does not give any prediction of its output quality, and extracting such a prediction from its internal data structures is not straightforward.  The parser we use is the incremental parser of (Seginer, 2007), POS tags are induced using the unsupervised POS tagger of ((Clark, 2003), neyessen morph model). Seginer (2007)'s common cover links model (CCL) does not need any prior tagging and is applied on word strings directly. As most unsupervised parsing models (except (Seginer, 2007)), we apply the hand annotated data of the NEGRA corpus. An exception which learns from raw text and makes no use of POS tags is the common cover link sparser (CCL, Seginer 2007). Though punctuation is usually entirely ignored in unsupervised parsing research, Seginer (2007) departs from this in one key aspect: the use of phrasal punctuation - punctuation symbols that often mark phrasal boundaries within a sentence.
Improving IBM Word Alignment Model 1 We investigate a number of simple methods for improving the word-alignment accuracy of IBM Model 1. We demonstrate reduction in alignment error rate of approximately 30% resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters.   Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported. Finally, the IBM models (Moore, 2004) impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence.  Moore (2004) also suggested adding multiple empty words to the target sentence for IBM Model 1. this example, COJO is a rare word that becomes a garbage collector (Moore, 2004) for the models in both directions. Similar to Berger and Lafferty (1999), the probability distribution of terms given a category is estimated using a normalized log-likelihood ratio (Moore, 2004), and query terms are sampled randomly from this distribution. Following Moore (2004a) rather than Munteanu and Marcu, our current notion of co-occurrence is that a data field and word co-occur if they are present in the same pair of data fields and sentence (as identified by the method described in Section 4.1 above). This allows us to compute the G2 score, for which we use the formulation from Moore (2004b) shown in Figure 2.  (Moore, 2004) translation limitation in the IBM model, due to which each word in the target document can be generated by at most one word in the question. For the same reasons, it also alleviates another related limitation by enabling translation between contiguous words across the query and documents (Moore, 2004).     Moore (2004) has found that smoothing to correct overestimated IBM1 lexical probabilities for rare words can improve word-alignment performance.  Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Graca et al, 2010).
Joint Unsupervised Coreference Resolution with Markov Logic Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data. Some unsupervised approaches have been proposed (e.g., Haghighi and Klein (2007)), but they are less accurate. In this paper, we present the first unsupervised approach that is competitive with supervised ones. This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals. On MUC and ACE datasets, our model outperforms Haghigi and Klein’s one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models. For example, Poon and Domingos (2008) has empirically reported that such global approaches achieve performance better than the ones based on incrementally processing a text. Ng (2008) used an Expectation-Maximization (EM) algorithm, and Poon and Domingos (2008) applied Markov Logic Network (MLN). In fact, Markov logic has been previously used by Poon and Domingos (2008) for coreference resolution and achieved good results, but it was used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. Poon and Domingos (Poon and Domingos, 2008) use an unsupervised technique based on joint inference across mentions and Markov logic as a representation language for their system on both MUC and ACE data. As we are not interested in unsupervised inference, the system of Poon and Domingos (2008) was unsuitable for our needs. A constraint-based graph partitioning system has been experimented by (Sapena et al., 2010) and a coreference detection system based on Markov logic networks (MLNs) has been proposed by (Poon and Domingos, 2008). Poon and Domingos (2008) introduced an unsupervised system in the framework of Markov logic.  We develop efficient learning and inference algorithms using a novel combination of two ideas from previous work on unsupervised learning with log-linear models: contrastive estimation (Smith and Eisner, 2005) and sampling (Poon and Domingos, 2008). The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). It has been successfully used in temporal relations recognition (Yoshikawa et al, 2009), co-reference resolution (Poon and Domingos, 2008), etc. Markov logic makes it possible to compactly specify probability distributions over complex relational domains, and has been successfully applied to unsupervised coreference resolution (Poon and Domingos, 2008) and other tasks. ACE2004-NWIRE: ACE 2004 Newswire set to compare against Poon and Domingos (2008). Predicate Nominatives: Another syntactic constraint exploited in Poon and Domingos (2008) is the predicate nominative construction, where the object of a copular verb (forms of the verb be) is constrained to corefer with its subject (e.g. Microsoft is a company in Redmond).  On the MUC6-TEST dataset, our system outperforms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures. Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of Poon and Domingos (2008). 
The ICSI Meeting Recorder Dialog Act (MRDA) Corpus 2 Data We describe a new corpus of over 180,000 handannotated dialog act tags and accompanying adjacency pair annotations for roughly 72 hours of speech from 75 naturally-occurring meetings. We provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data. Following the work of Shriberg et al (2004), we use the 5 general tags in our experiments: Disruption indicates the current Dialogue Act is interrupted. Like the Switchboard corpus, the ICSI Meeting Room DA (MRDA) corpus (Shriberg et al, 2004) was annotated using a variant of the DAMSL tag set, similar but not identical to the Switchboard DAMSL annotation. The differences (and a translation between the two sets) can be seen in Shriberg et al (2004). To facilitate cross-corpus classification, we will cluster these labels as described in Shriberg et al (2004). We also used additional annotation that has been developed to support higher-level analyses of meeting structure, in particular the ICSI Meeting Recorder Dialog act (MRDA) corpus (Shriberg et al., 2004). The following experiments used manual meeting transcripts and relied on manual dialogue act segmentation (Shriberg et al, 2004). All of them have been transcribed and annotated with dialog acts (DA) (Shriberg et al, 2004), topics, and abstractive and extractive summaries in the AMI project (Murray et al, 2005). While there are related tags for dialogue act tagging schema? like DAMSL (Core and Allen, 1997), which includes tags such as Action-Directive and Commit, and the ICSI MRDA schema (Shriberg et al, 2004) which includes a committag these classes are too general to allow identification of action items specifically. All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al, 2004), topics, and extractive summaries (Murray et al, 2005). First, it is rare to have sub DAs labeled in training data, and indeed this is true of the corpus (Shriberg et al, 2004) that we use. We evaluate our methods on the ICSI meeting recorder dialog act (MRDA) (Shriberg et al, 2004) corpus, and find that our novel hidden back off model can significantly improve dialog tagging accuracy. In all our models, to simplify we assume that the sentence change information is known (as is common with this corpus (Shriberg et al, 2004)). We evaluated our hidden back off model on the ICSI meeting recorder dialog act (MRDA) corpus (Shriberg et al, 2004). All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al, 2004), topics, and extractive summaries (Murray et al, 2005). In our future work, we plan to examine initiative conflicts in face-to-face multi-party conversation, such as the ICSI corpus (Shriberg et al, 2004). This GUI showed both their textual summary and the orthographic transcription, without topic segmentation but with one line per dialogue act based on the pre-existing MRDA coding (Shriberg et al, 2004). We use only the forced alignments of these meetings, available in the accompanying MRDA Corpus (Shriberg et al 2004). Experiments are performed using all train/test pairs among three conversational speech corpora: the Meeting Recorder Dialog Actcorpus (MRDA) (Shriberg et al, 2004), Switch board DAMSL (Swbd) (Jurafsky et al, 1997), and the Spanish Callhome dialog act corpus (SpCH) (Levin et al, 1998). Each channel was manually transcribed and timed, then annotated with dialogue act and adjacency pair information (Shriberg et al, 2004).
A TAG-Based Noisy-Channel Model Of Speech Repairs This paper describes a noisy channel model of speech repairs, which can identify and correct repairs in speech transcripts. A syntactic parser is used as the source model, and a novel type of TAG-based transducer is the channel model. The use of TAG is motivated by the intuition that the reparandum is a “rough copy” of the repair. The model is trained and tested on the Switchboard disfluency-annotated corpus. These steps result in an improvement of 43.98% percent relative error reduction in F-score over an earlier best result in edited detection when punctuation is included in both training and testing data [Charniak and Johnson 2001], and 20.44% percent relative error reduction in F-score over the latest best result where punctuation is excluded from the training and testing data [Johnson and Charniak 2004].  These steps result in a significant improvement in F-score over the earlier best result reported in [Charniak and Johnson 2001], where punctuation is included in both the training and testing data of the Switchboard corpus, and a significant error reduction in F-score over the latest best result [Johnson and Charniak 2004], where punctuation is ignored in both the training and testing data of the Switchboard corpus. When compared with the latest results from [Johnson and Charniak 2004], where no punctuations are used for either training or testing data, we also observe the same trend of the improved results. Noisy channel models have done well on the disfluency detection task in the past; the work of Johnson and Charniak (2004) first explores such an approach. Following Johnson and Charniak (2004), we use a noisy channel model to propose a 25-best list of possible speech disfluency analyses. Further details of the noisy channel model can be found in Johnson and Charniak (2004). To improve performance over the standard noisy channel model we use a re-ranker, as previously suggest by Johnson and Charniak (2004). As Johnson and Charniak (2004) noted, although this model performs well, a log linear re-ranker can be used to increase performance.  In this work, we use a total of 62 variables, which include 16 variables from Charniak and Johnson (2001) and Johnson and Charniak (2004), an additional 29 variables from Zhang and Weng (2005), 11 hierarchical POS tag variables, and 8 prosody variables (labels and their confidence scores). Because the edit region identification results on the original Switchboard are not directly comparable with the results on the newly segmented data, the state-of-art results reported by Charniak and Johnson (2001) and Johnson and Charniak (2004) are repeated on this new corpus by Kahn et al (2005). Speech is often disfluent, and speech repairs are known to repeat large portions of the preceding context (Johnson and Charniak, 2004). The evaluation of this system was performed on the Switchboard corpus, using the mrg annotations in directories 2 and 3 for training, and the filessw4004.mrg to sw4153.mrg in directory 4 for evaluation, following Johnson and Charniak (2004). The TAG system (Johnson and Charniak, 2004) achieves a higher EDIT-F score, largely as a result of its explicit tracking of overlapping words between reparanda and alterations. The prior probability distributions over alignment operations is estimated from data in the Switchboard in a similar manner to Johnson and Charniak (2004). Given that state-of-the-art edit detection performs at about 80% f-measure (Johnson and Charniak, 2004), much of the benefit derived here from oracle repair detection should be realizable in practice. The Johnson and Charniak (2004) approach, referred to in this document as JC04, combines the noisy channel paradigm with a tree-adjoining grammar (TAG) to capture approximately repeated elements. The output of the JC04 model (Johnson and Charniak, 2004) is included as a feature and used as an approximate baseline in the following experiments. The training of the TAG model within this system requires a very specific data format, so this system is trained not with SSR but with Switchboard (SWBD) (Godfrey et al, 1992) data as described in (Johnson and Charniak, 2004).
Text Segmentation Based On Similarity Between Words This paper proposes a new indicator of text structure, called the lexical cohesion profile (LCP), which locates segment boundaries in a text. A text segment is a coherent scene; the words in a segment are linked together via lexical cohesion relations. LCP records mutual similarity of words in a sequence of text. The similarity of words, which represents their cohesiveness, is computed using a semantic network. Comparison with the text segments marked by a number of subjects shows that LCP closely correlates with the human judgments. LCP may provide valuable information for resolving anaphora and ellipsis. Lexical cohesion techniques include similarity measures between adjacent blocks of text, as in TextTiling (Hearst, 1994, 1997) and lexical chains based on recurrences of a term or related terms, as in Morris and Hirst (1991), Kozima (1993), and Galley, et al. As in Kozima's work (Kozima, 1993), this computation operates on words belonging to a focus window that is moved all over the text. Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented. But work such as (Kozima, 1993), (Ferret, 1998) or (Kaufmann, 1999) showed that using a domain independent source of knowledge for text segmentation doesn't necessarily lead to get better results than work that is only based on word distribution in texts. This significance is defined as in (Kozima, 1993) as its normalized information in a reference corpus. He identified topic boundaries where the LCP score was low (Kozima, 1993). For example, the lexical cohesion profile (Kozima, 1993) should be perfectly usable with our fragmentation method. Ever since Morris and Hirst (1991)'s ground breaking paper, topic segmentation has been a steadily growing research area in computational linguistics, with applications in summarization (Barzilay and Elhadad, 1997), information retrieval (Salton and Allan, 1994), and text understanding (Kozima, 1993). Therefore, many approaches have concentrated on different ways of estimating lexical coherence of text segments, such as semantic similarity between words (Kozima, 1993), similarity between blocks of text (Hearst, 1994), and adaptive language models (Beeferman et al, 1999). As in Kozima (1993), the second method exploits lexical cohesion to segment exts, but in a different way. Kozima (1993), for example, used cohesion based on the spreading activation on a semantic network.
Phrasetable Smoothing For Statistical Machine Translation We discuss different strategies for smoothing the phrasetable in Statistical MT, and give results over a range of translation settings. We show that any type of smoothing is a better idea than the relativefrequency estimates that are often used. The best smoothing techniques yield consistent gains of approximately 1% (absolute) according to the BLEU metric. We speculated that this may have been due to non-smooth component models, and tried various smoothing schemes, including Kneser-Ney phrase table smoothing similar to that described in (Foster et al, 2006), and binary features to indicate phrase pair presence within different components. We also use the fix-discount method in Foster et al (2006) for phrase table smoothing. First, we used several types of phrase table smoothing in the WMT 2007 system because this proved helpful on other translation tasks: relative frequency estimates, Kneser-Neyand Zens-Ney-smoothed probabilities (Foster et al, 2006). We use a version from Foster et al (2006), modified from (Koehn et al,2003), which is an average of pairwise word translation probabilities. Foster et al (2006) applied ideas from language model smoothing to the translation model. We are only aware of one work that performs a systematic comparison of smoothing techniques in phrase-based machine translation systems (Foster et al., 2006). This may lead to an over-reliance on unreliable distributions, which can be ameliorated by smoothing (e.g., Foster et al (2006)). The maximum likelihood estimates are smoothed using Good-Turing discounting (Foster et al, 2006). The phrase translation model probabilities are smoothed according to one of several techniques as described in (Foster et al, 2006) and identified in the discussion below. In addition, a number of different phrase table smoothing algorithms were used: no smoothing, Good-Turing smoothing, Kneser-Ney 3 parameter smoothing and the log linear mixture involving two features called Zens-Ney (Foster et al, 2006). This is plausible since a similar effect (a decrease in the benefit of smoothing) has been noted with phrase table smoothing (Foster et al, 2006). In addition, we applied phrase table smoothing as described in Foster et al (2006). The phrase based translation model is similar to that of Koehn, with the exception that phrase probability estimates P (s? |t?) are smoothed using the Good-Turing technique (Foster et al, 2006). From this point of view, TMG can also be seen as a TM smoothing technique based on multiple TMs instead of single one such as Foster et al (2006). Some literatures have paid attention to this issue as well, such as Foster et al (2006) and Mylonakis and Simaan (2008). This can happen, if we define features that penalize longer phrase pairs, such as lexical weighting, or if we apply smoothing (Foster et al 2006). Therefore, the raw relative frequency estimates found in the phrase translation tables are smoothed by applying modified Kneser-Ney discounting as described in Foster et al (2006). Smoothing is obviously one possibility (Foster et al, 2006). Absolute discounting is a popular smoothing method for relative frequencies (Foster et al, 2006). The phrase translation probabilities p (e1|f )and p (f |e2) are estimated by maximum likelihood estimation and smoothed using Good-Turing smoothing (Foster et al, 2006).
Empirical Studies On The Disambiguation Of Cue Phrases phrases are linguistic expressions such as now and function as explicit indicators of structure of a discourse. For example, signal the beginning of a subtopic or a return a previous topic, while mark subsequent material as a response to prior material, or as an explanatory comment. However, while cue phrases may convey discourse structure, each also one or more alternate uses. While be used sententially as an adverbial, for example, the discourse use initiates a digression. Although distinguishing discourse and sentential uses of cue phrases is critical to the interpretation and generation of discourse, the question of how speakers and hearers accomplish this disambiguation is rarely addressed. This paper reports results of empirical studies on discourse and sentential uses of cue phrases, in which both text-based and prosodic features were examined for disambiguating power. Based on these studies, it is proposed that discourse versus sentential usage may be distinguished by intonational features, specifically, pitch accent and prosodic phrasing. A prosodic model that characterizes these distinctions is identified. This model is associated with features identifiable from text analysis, including orthography and part of speech, to permit the application of the results of the prosodic analysis to the generation of appropriate intonational features for discourse and sentential uses of cue phrases in synthetic speech. phrases, and phrases that directly signal the structure of a discourse, been variously termed words, discourse markers, discourse connectives, particles the computational linguistic and conversational analysis Empirical studies on the disambiguation of cue phrases (Hirschberg and Litman, 1993) have shown that just by considering the orthographic environment in which a discourse marker occurs, one can distinguish between sentential and discourse usages in about 80% of cases. We extract the initial unigram, bigram, and trigram of each utterance as discourse features (Hirschberg and Litman, 1993). In the literature, there is still no consistent definition for discourse markers (Hirschberg and Litman 1993). This is due to the fact that, similar to the question of cue phrase polysemy (Hirschberg and Litman 1993), many Chinese discourse markers have both discourse senses and alternate sentential senses in different utterances. Annotation includes two stages: first, we allowed two coders to choose 'explanation' relations signaled by because using (Hirschberg and Litman, 1993)'s 3-way classification. Many discourse segmentation techniques (e.g. Hirschberg and Litman, 1993) as well as some topic segmentation algorithms rely on cue words and phrases.  We use a list of cue phrases identified by Hirschberg and Litman (1993). Perhaps the most studied cue for discourse structure are lexical cues, also called cue phrases, which are defined as follows by Hirschberg and Litman (1993). For example Hirschberg and Litman (1993) found that into national phrasing and pitch accent play a role in disambiguating cue phrases, and hence in helping determine discourse structure. our own implementation of a feature concerning the presence of certain cue words (Hirschberg and Litman, 1993). Many researchers have attempted to make use of cue phrases (Hirschberg and Litman, 1993).
A Syntactic Analysis Method Of Long Japanese Sentences Based On The Detection Of Conjunctive Structures This paper presents a syntactic analysis method that first detects conjunctive structures in a sentence by checking parallelism of two series of words and then analyzes the dependency structure of the sentence with the help of the information about the conjunctive structures. Analysis of long sentences is one of the most difficult problems in natural language processing. The main reason for this difficulty is the structural ambiguity that is common for conjunctive structures that appear in long sentences. Human beings can recognize conjunctive structures because of a certain, but sometimes subtle, similarity that exists between conjuncts. Therefore, we have developed an algorithm for calculating a similarity measure between two arbitrary series of words from the left and the right of a conjunction and selecting the two most similar series of words that can reasonably be considered as composing a conjunctive structure. This is realized using a dynamic programming technique. A long sentence can be reduced into a shorter form by recognizing conjunctive structures. Consequently, the total dependency structure of a sentence can be obtained by relatively simple head-dependent rules. A serious problem concerning conjunctive structures, besides the ambiguity of their scopes, is the ellipsis of some of their components. Through our dependency analysis process, we can find the ellipses and recover the omitted components. We report the results of analyzing 150 Japanese sentences to illustrate the effectiveness of this method. we calculated the bunsetsu-based accuracies of our model and KNP (Kurohashi and Nagao, 1994) on the rst 100 sentences of the test corpus. Kurohashi and Nagao (1994) used a similar data structure in their rule-based method. Popular parsers are Cabo Cha (Kudo and Matsumoto, 2002) and KNP (Kurohashi and Nagao, 1994), which we redeveloped to analyze formal written language expressions such as that in newspaper articles. Sentences with coordinate structures are also difficult to parse (Kurohashi and Nagao, 1994). Kurohashi and Nagao (1994) proposed a method to detect conjunctive structures by calculating similarity scores between two sequences of bunsetsus. If the current modifier bunsetsu is a distinctive key bunsetsu (Kurohashi and Nagao, 1994, page 510), these features are triggered.  The state space of our model resembles that of Kurohashi and Nagao's Japanese coordination detection method (Kurohashi and Nagao, 1994). We use a syntactic parser KNP (Kurohashi and Nagao, 1994) to divide the ASR results into the phrases. An input sentence is parsed using the Japanese parser, KNP (Kurohashi and Nagao, 1994). while Sassano (2004) used richer features which are not used in the proposed method, such as features for conjunctive structures based on Kurohashi and Nagao (1994). We first process them with the Japanese morphological analyzer, JUMAN (Kurohashi et al., 1994). We have crawled 218 million web pages over three months, May July in 2007, by using the Shim Crawler, and then converted these pages into web standard format data with results of a Japanese parser, KNP (Kurohashi and Nagao, 1994), through our conversion tools. A large raw corpus is parsed by the Japanese parser, KNP (Kurohashi and Nagao, 1994b), and reliable predicate argument examples are extracted from the parse results.  Kurohashi and Nagao proposed a similarity-based method to resolve both of the two tasks for Japanese (Kurohashi and Nagao, 1994). Kurohashi and Nagao proposed a Japanese parsing method that included coordinate structure detection (Kurohashi and Nagao, 1994). Therefore, we use a method to calculate the similarity between two whole coordinate conjuncts (Kurohashi and Nagao, 1994). A similarity value between two bunsetsus is calculated on the basis of POS matching, exact word matching, and their semantic closeness in a thesaurus tree (Kurohashi and Nagao, 1994). We classified coordination keys into 52 classes ac cording to the classification proposed by (Kurohashiand Nagao, 1994).
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 183?187 Manchester, August 2008 Dependency-based Syntactic?Semantic Analysis with PropBank and NomBank Richard Johansson and Pierre Nugues Lund University, Sweden {richard, pierre}@cs.lth.se Abstract This paper presents our contribution in the closed track of the 2008 CoNLL Shared Task (Surdeanu et al., 2008).  From the results of CoNLL-2008 shared task, the top system by (Johansson and Nugues, 2008) also used two 30 different subsystems to handle verbal and nominal predicates, respectively. One is gold-standard syntactic input, and other two are based on automatically parsing results of two parsers, the state-of-the-art syntactic parser described in (Johansson and Nugues, 2008) 7 (it is referred to Johansson) and an integrated parser described as the following (referred to MST ME). Note that the reranking may slightly improve the syntactic performance according to (Johansson and Nugues, 2008). The comparison indicates that our integrated system outputs a result quite close to the state-of-the-art by the pipeline system of (Johansson and Nugues, 2008) as the same syntactic structure input is adopted. It is worth noting that our system actually competes with two independent sub-systems of (Johansson and Nugues, 2008), one for verbal predicates, the other for nominal predicates. Each sentence was parsed by the LTH dependency parser (Johansson and Nugues, 2008a), which we trained on a Swedish tree bank (Nilsson et al, 2005). Semantic role classifiers rely heavily on lexical features (Johansson and Nugues, 2008b), and this may lead to brittleness; in order to increase robustness, we added features based on hierarchical clusters constructed using the Brown algorithm (Brown et al, 1992). The dependency parser of this demonstration is a further development of Carreras (2007) and Johansson and Nugues (2008). For the predicate identification, we used the features suggested by Johansson and Nugues (2008). Toutanova et al (2008), Johansson and Nugues (2008), and Bjorkelund et al (2009) presented importance of capturing non-local dependencies of core arguments in predicate-argument structure analysis. SRL based on FrameNet is thus not a novel task, although very few systems are known capable of completing a general frame-based annotation process over raw texts, noticeable exceptions being discussed for example in (Erk and Pado, 2006), (Johansson and Nugues, 2008b) and (Coppola et al., 2009). Most of the employed learning algorithms are based on complex sets of syntagmatic features, as deeply investigated in (Johansson and Nugues, 2008b). More recently, the-state-of-art frame-based semantic role labeling system discussed in (Johansson and Nugues, 2008b) reports a 19% drop in accuracy for the argument classification task when a different test domain is targeted (i.e. NTI corpus).  In (Johansson and Nugues, 2008b) the impact of different grammatical representations on the task of frame-based shallow semantic parsing is studied and the poor lexical generalization problem is outlined. In all experiments, the FrameNet 1.3 version and the dependency based system using the LTH parser (Johansson and Nugues, 2008a) have been employed. In the evaluation of the AC task, accuracy is computed over the nodes of the dependency graph, in line with (Johansson and Nugues, 2008b) or (Coppola et al, 2009). An interesting result is that a per-node accuracy of 86.3 (i.e. only 3 points under the state of-the art on the same data set, (Johansson and Nugues, 2008b)) is achieved. The above empirical findings are relevant if compared with the outcome of a similar test on the NTI collection, discussed in (Johansson and Nugues,2008b). Notice that in this paper only the training portion of the NTI data set is employed as reported in Table 2 and results are not directly comparable to (Johansson and Nugues, 2008b).
Sentence Reduction For Automatic Text Summarization Figure 2: Sample sentence and parse tree we have an input sentence (ABCDEhas a parse tree shown in Figure 2. a human reduces the sentence to which can be translated to a series of decisions made along edges in the sentence parse tree as shown in Figure 3. The symbol &quot;y&quot; along an edge means the node it points to will be kept, and &quot;n&quot; means the node will be removed. Suppose the program reduces sentence to can be translated similarly to the annotated tree shown in Figure 4. Figure 4: Reduced form by the program We can see that along five edges (they are D—)T, D—*G, B-4A, B—>C), both the human and the program made decisions. Two out of the five decisions agree (they are D--÷B and D—>E), so the rate is 2/5 (40%). The rate defined as: man and the program have made the same decision success rate = the total # of edges along which both the human and the progam have made decisions 313 Note that the edges along which only the human or the program has made a decision (e.g., G--F and G—.>F in Figure 3 and Figure 4) are not considered in the computation of success rate, since there is no agreement issue in such cases. 3.2 Evaluation result In the evaluation, we used 400 sentences in the corpus to compute the probabilities that a phrase is removed, reduced, or unchanged. We tested the program on the rest 100 sentences. Using five-fold validation (i.e., chose different 100 sentences for testing each time and repeating the experiment five times), The program achieved an average success rate of 81.3%. If we consider the baseline as removing all the prepositional phrases, clauses, to-infinitives and gerunds, the baseline performance is 43.2%. We also computed the success rate of program's decisions on particular types of phrases. For the decisions on removing or keeping a clause, the system has a success rate of 78.1%; for the decisions on removing or keeping a to-infinitive, the system has a success rate of 85.2%. We found out that the system has a low success rate on removing adjectives of noun phrases or removing adverbs of a sentence or a verb phrase. One reason for this is that our probability model can hardly capture the dependencies between a particular adjective and the head noun since the training corpus is not large enough, while the other sources of information, including grammar or context information, provide little evidence on whether an adjective or an adverb should be removed. Given that whether or not an adjective or an adverb is removed does not affect the conciseness of the sentence significantly and the system lacks of reliability in making such decisions, we decide not to remove adjectives and adverbs. On average, the system reduced the length of the 500 sentence by 32.7% (based on the number of words), while humans reduced it by 41.8%. The probabilities we computed from the training corpus covered 58% of instances in the test corpus. When the corpus probability is absent for a case, the system makes decisions based on the other two sources of knowledge. Some of the errors made by the system result from the errors by the syntactic parser. We randomly checked 50 sentences, and found that 8% of the errors made by the system are due to parsing errors. There are two main reasons responsible for this relative low percentage of errors resulted from mistakes in parsing. One reason is that we have taken some special measures to avoid errors introduced by mistakes in parsing. For example, PP attachment is a difficult problem in parsing and it is not rare that a PP is wrongly attached. Therefore, we take this into account when marking the obligatory components using subcategorization knowledge from the lexicon (step 2) — we not only look at the PPs that are attached to a verb phrase, but also PPs that are next to the verb phrase but not attached, in case it is part of the verb phrase. We also wrote a preprocessor to deal with particular structures that the parser often has problems with, such as appositions. The other reason is that parsing errors do not always result in reduction errors. For example, given a sentence &quot;The spokesperson of the University said that ...&quot;, although that-clause in the sentence may have a complicated structure and the parser gets it wrong, the reduction system is not necessarily affected since it may decide in this case to keep that-clause as it is, as humans often do, so the parsing errors will not matter in this example. 4 Discussion and related work The reduction algorithm we present assumes generic summarization; that is, we want to generate a summary that includes the most important information in an article. We can tailor the reduction system to queries-based summarization. In that case, the task of the reduction is not to remove phrases that are extraneous in terms of the main topic of an article, but phrases that are not very relevant to users' queries. We extended our sentence reduction program to query-based summarization by adding another step in the algorithm to measure the relevance of users' queries to phrases in the sentence. In the last step of reduction when the system makes the final decision, the relevance of a phrase to the query is taken into account, together with syntactic, context, and corpus information. Ideally, the sentence reduction module should interact with other modules in a summarization system. It should be able to send feedback to the extraction module if it finds that a sentence selected by the extraction module may be inappropriate (for example, having a very low context importance score). It should also be able to interact with the modules that run after it, such as the sentence combination module, so that it can revise reduction decisions according to the feedback from these modules. Some researchers suggested removing phrases or clauses from sentences for certain applications. (Grefenstette, 1998) proposed to remove phrases in sentences to produce a telegraphic text that can be used to provide audio scanning service for the blind. (Corston-Oliver and Dolan, 1999) proposed to remove clauses in sentences before indexing documents for Information Retrieval. Both studies removed phrases based only on their syntactic categories, while the focus of our system is on deciding when it is appropriate to remove a phrase. researchers worked on the text simplifica- 314 tion problem, which usually involves in simplifying text but not removing any phrases. For example, (Carroll et al., 1998) discussed simplifying newspaper text by replacing uncommon words with common words, or replacing complicated syntactic structures with simpler structures to assist people with reading disabilities. (Chandrasekar et al., 1996) discussed text simplification in general. The difference between these studies on text simplification and our system is that a text simplification system usually not from an original sentence, although it may change its structure or words, but our system removes extraneous phrases from the extracted sentences. 5 Conclusions and future work We present a novel sentence reduction system which removes extraneous phrases from sentences that are extracted from an article in text summarization. The deleted phrases can be prepositional phrases, clauses, to-infinitives, or gerunds, and multiple phrases can be removed form a single sentence. The focus of this work is on determining, for a sentence in a particular context, which phrases in the sentence are less important and can be removed. Our system makes intelligent reduction decisions based on multiple sources of knowledge, including syntactic knowledge, context, and probabilities computed from corpus analysis. We also created a corpus consisting of 500 sentences and their reduced forms produced by human professionals, and used this corpus for training and testing the system. The evaluation shows that 81.3% of reduction decisions made by the system agreed with those of humans. In the future, we would like to integrate our sentence reduction system with extraction-based summarization systems other than the one we have developed, improve the performance of the system further by introducing other sources of knowledge necessary for reduction, and explore other interesting applications of the reduction system. Acknowledgment This material is based upon work supported by the National Science Foundation under Grant No. IRI 96-19124 and IRI 96-18797. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not  In fact, professional abstractors tend to use these operations to transform selected sentences from an article into the corresponding summary sentences (Jing, 2000). For example, Jing (2000) trained her system on a set of 500 sentences from the Benton Foundation (http: //www.benton.org) and their reduced forms written by humans. Examples include text summarisation (Jing 2000), subtitle generation from spoken transcripts (Vandeghinste and Pan 2004) and information retrieval (Olivers and Dolan 1999). Jing (2000) was perhaps the first to tackle the sentence compression problem. Our constraints are linguistically and semantically motivated in a similar fashion to the grammar checking component of Jing (2000). Jing and McKeown (H. Jing, 2000) studied a new method to remove extraneous phrase from sentences by using multiple source of knowledge to decide which phrase in the sentences can be removed. Sentence compression is the task of producing a shorter form of a single given sentence, so that the new form is grammatical and retains the most important information of the original one (Jing, 2000). Sentence compression is the task of summarizing a sentence while retaining most of the informational content and remaining grammatical (Jing, 2000). Sentence compression is the task of producing a shorter form of a grammatical source (input) sentence, so that the new form will still be grammatical and it will retain the most important information of the source (Jing, 2000). The evaluation of sentence reduction (see (Jing, 2000) for details) used a corpus of 500 sentences and their reduced forms in human-written abstracts. To overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches of Knight and Marcu (2000) and Jing (2000). Jing (2000) was perhaps the first to tackle the sentence compression problem. Table 5 shows a 5 sentence summary created using algorithm 1 for the paper A00-1043 (Jing, 2000). To overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches of Knight and Marcu (2000) and Jing (2000). In these approaches, decisions about which material to include/delete in the sentence summaries do not rely on relative frequency information on words, but rather on probability models of subtree deletions that are learned from a corpus of parses for sentences and their summaries. In addition, an automatic evaluation method based on context-free deletion decisions has been proposed by Jing (2000). Sentence compression produces a summary of a single sentence that retains the most important information while remaining grammatical (Jing, 2000). A syntactic approach considers the alignment over parse trees (Jing, 2000), and a similar technique has been used with dependency trees to evaluate the quality of sentence fusions (Marsi and Krahmer, 2005).
Reliable Measures For Aligning Japanese-English News Articles And Sentences We have aligned Japanese and English news articles and sentences to make a large parallel corpus. We first used a method based on cross-language information retrieval (CLIR) to align the Japanese and English articles and then used a method based on dynamic programming (DP) matching to align the Japanese and English sentences in these articles. However, the results included many incorrect alignments. To remove these, we propose two measures (scores) that evaluate the validity of alignments. The measure for article alignment uses similarities in sentences aligned by DP matching and that for sentence alignment uses similarities in articles aligned by CLIR. They enhance each other to improve the accuracy of alignment. Using these measures, we have successfully constructed a largescale article and sentence alignment corpus available to the public. We used a bilingual corpus (Utiyama and Isahara, 2003) to examine which semantic frames of BFN contained LUs relevant to the Japanese verb osou. The bilingual corpus used for our experiments was obtained from an automatically sentence aligned Japanese/English Yomiuri newspaper corpus consisting of 180K sentence pairs (refer to Table1) (Utiyama and Isahara, 2003). Utiyama and Isahara (2003) extract Japanese-English parallel sentences from a noisy-parallel corpus. We used 1000 sentence pairs extracted from pre-aligned data (Utiyama and Isahara, 2003) as a gold standard. For example, Munteanu and Marcu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use cosine similarity. For example, Zhao and Vogel (2002), Utiyama and Isahara (2003), and Munteanu and Marcu (2005) all acquire their comparable corpora from a collection of news articles which are either downloaded from the Web or archived by LDC. Works aimed at discovering parallel sentences include (Utiyama and Isahara, 2003), who use cross-language information retrieval techniques and dynamic programming to extract sentences from an English-Japanese comparable corpus. The translations published on MNH are used to make a parallel corpus by using a sentence alignment method (Utiyama and Isahara, 2003). These values of the parameter are determined using English sentences from Reuters articles (Utiyama and Isahara, 2003). Unlike other language pairs, the availability of Japanese-English parallel corpora is quite limited: the NTCIR patent corpus (Fujii et al, 2010) of 3 million sentence pairs (the latest NTCIR-8 version) for the patent domain and JENAAD corpus (Utiyama and Isahara, 2003) of 150k sentence pairs for the news domain. The earliest efforts in this direction are those of Zhao and Vogel (2002) and Utiyama and Isahara (2003). This database was an aggregate of several Japanese-English corpora, notably the Yomiuri newspaper corpus (Utiyama and Isahara, 2003) and the JST paper abstract corpus created at NICT (www.nict.go.jp) through (Utiyama and Isahara, 2007).
Applying Co-Training Methods To Statistical Parsing We propose a novel Co-Training method for statistical parsing. The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text. The algorithm iteratively labels the entire data set with parse trees. Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data. Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers. In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostly unsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. Feature-based integration also has points in common with co-training, which have been applied to syntactic parsing by Sarkar (2001) and Steedman et al.  Co-traininghas been successfully applied to various applications, such as statistical parsing (Sarkar, 2001). Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). (Sarkar, 2001) applied co-training to statistical parsing, where two component models are trained and the most confident parsing outputs of the existing model are incorporated into the next training. Sarkar (2001) and Steedman et al (2003 ) investigated using co-training for parsing. Sarkar (2001) applied co-training to LTAG parsing, in which the super tagger and parser provide the two views. Among others Co-Training was applied to document classification (Blum and Mitchell, 1998), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001), and statistical parsing (Sarkar, 2001). The iterative training procedure proposed in this work shares some similarity with the co-training algorithm in parsing (Sarkar, 2001), where the training procedure lets two different models learn from each other during parsing the raw text.
Limitations Of Co-Training For Natural Language Learning From Large Datasets Co-Training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data. This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data. This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels. Using base noun phrase bracketing as a case study, we find that co-training reduces by 36% the difference in error between classifiers and supervised clastrained on a labeled version all available data. However, degradation in the quality of the bootstrapped data arises as an obstacle to further improvement. To address this, we propose a moderately supervised variant of cotraining in which a human corrects the mistakes made during automatic labeling. Our analysis suggests that corrected co-training and similar moderately supervised methods may help cotraining scale to large natural language learning tasks. Similarly, in a case study on co-training for natural language tasks, Pierce and Cardie (2001) find that the degradation in data quality from automatic labelling prevents these systems from performing comparably to their fully-supervised counterparts. Pierce and Cardie (2001) have shown, however, that for tasks which require large numbers of labeled examples such as most NLP tasks co training might be inadequate because it tends to generate noisy data. We observed the same pattern in co-training; its accuracy peaked after two iterations (85.1%) and then performance degraded drastically (68% after five iterations) due in part to an increase in mislabeled data in the training set (as previously observed in (Pierce and Cardie, 2001)) and in part because the data skew is not controlled for. For naive co-training, new samples will always be added, and so there is a possibility that the noise accumulated at later stages will start to degrade performance (see Pierce and Cardie (2001)). The opposite behaviour has been observed in other applications of co-training (Pierce and Cardie, 2001).  The drop in F-measure is potentially due to the pollution of the labeled data by mislabeled instances (Pierce and Cardie, 2001). However, it has been shown (Pierce and Cardie, 2001) that semi-supervised learning is brittle for NLP tasks where typically large amounts of high quality annotations are needed to train appropriate classifiers. The use of the disagreement between taggers for selecting candidates for manual correction is reminiscent of corrected co-training (PierceandCardie, 2001). For other work on co-training, see (Muslea et al 200; Pierce and Cardie 2001). The fact that no improvement was obtained agrees with previous observations that classifiers that are too accurate can not be improved with bootstrapping (Pierce and Cardie,2001). A comparative analysis of words that benefit from basic/smoothed co-training with global parameter settings, versus words with little or no improvement obtained through bootstrapping reveals several observations: (1) Words with accurate basic classifiers can not be improved through co-training, which agrees with previous observations (Pierce and Cardie, 2001). Algorithms such as co-training (Blum and Mitchell, 1998) (Collins and Singer, 1999) (Pierce and Cardie, 2001) and the Yarowsky algorithm (Yarowsky, 1995) make assumptions about the data that permit such an approach. The table further indicates that co-training for machine translation suffers the same problem reported in Pierce and Cardie (2001): gains above the accuracy of the initial corpus are achieved, but decline as after acer tain number of machine translations are added to the training set. Pierce and Cardie (2001) showed that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a high performing model from being learned. To address the problem of data pollution by tagging errors, Pierce and Cardie (2001) propose corrected co-training. Thus, these bootstrapping methods will presumably not find the most useful unlabeled examples but require a human to review data points of limited training utility (Pierce and Cardie, 2001).  In their experiments on NP identifiers, Pierce and Cardie (2001) observed a similar effect. Co-training has been applied to a number of NLP applications ,including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001).
The English All-Words Task merl.systeml .459 .447 IR,ST-DDD-09-U .729 .441 autoPS-U .490 .433 clr04-aw .506 .431 autoPSNVs-U .563 .354 merl.system2 .480 .352 DLSI-UA-all-Nosu .343 .275 Table 2: &quot;Without U&quot; scores, sorted by recall; a -S or -U suffix after the system name indicates that the system was reported as supervised or unsupervised, respectively. System Name Email Contact autoPS dianam©sussex.ac.uk autoPSNVs dianam©sussex.ac.uk clr04-aw ken©clres.com DFA-Unsup-AW david©lsi.uned.es DLSI-UA-Nosu montoyo©dlsi.ua.es GAMBL-AW bart.decadt©ua.ac.be IR ST-DDD-00 strappaKfitc.it IRST-DDD-09 strappaKfitc.it IRST-DDD-LSI strappaKfitc.it Koc University dyuret©ku.edu.tr KUNLP-Eng-All hcseoKM1p.korea.ac.kr LCCaw parker©languagecomputer.com Meaning lluism©lsi.upc.es Meaning simple lluism©lsi.upc.es merl.systeml bhiksha©merl.com merl.system2 bhiksha©merl.com merl.system3 bhiksha©merl.com R2D2: EAW montoyo©dlsi.ua.es SenseLearner rada©cs.unt.edu UJAEN mgarcia©ujaen.es USussex-Prob3 Judita.Preiss©cl.cam.ac.uk USussex-Prob4 Judita.Preiss©cl.cam.ac.uk USussex-Prob5 Judita.Preiss©cl.cam.ac.uk upv-shmm-eaw amolina©dsic.upv.es upv-CIAOSENSO amolina©dsic.upv.es upv-CIAOSENS02 amolina©dsic.upv.es Table 3: email contact for each system; sorted alphabetically. the unsupervised systems (Palmer et al., 2001). In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria). The greatest difference between these results those of the all-words task is that a greater number of systems have now achieved scores at or above the baseline. While this result is encouraging, it seems that the best systems have a hit a wall in the 65- 70% range. This is not surprising given the typical inter-annotator agreement of 70-75% for this task. We believe that further significant progress must await the development of resources with coarser-grained sense distinctions and with glosses that draw explicit contrasts between the senses — resources more suitable for the task at hand. We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. The F-score obtained by training on SemCor (mixed-domain corpus) and testing on the two target domains without using any injections (srcb) F-score of 61.7% on Tourism and F score of 65.5% on Health is comparable to the best result reported on the SEMEVAL datasets (65.02%, where both training and testing hap pens on a mixed-domain corpus (Snyder and Palmer, 2004)). State-of-the-art systems attained a disambiguation accuracy around 65% in the Senseval-3 all-words task (Snyder and Palmer, 2004), where WordNet (Fellbaum, 1998) was adopted as a reference sense inventory. Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). While contextual evidence is required for accurate WSD, it is useful to look at this heuristic since it is so widely used as a back-off model by many systems and is hard to beat on an all words task (Snyder and Palmer, 2004). Recently, using WN as a sense repository, the organizers of the English all-words task at SensEval-3 reported an inter-annotation agreement of 72.5% (Snyder and Palmer, 2004). SensEval-38 English all-words corpus (hereinafter SE3) (Snyder and Palmer, 2004), is made up of 5,000 words, extracted from twoWSJ articles and one excerpt from the Brown Corpus. Snyder and Palmer (2004) report 62% of all word types on the English all-words task at SENSEVAL-3 were labelled unanimously. Nouns and verbal nouns (vn) have the highest agreements, similar to the results for the English all-words task at SENSEVAL-3 (Snyder and Palmer, 2004). Unsupervised learning is introduced primarily to deal with the problem, but with limited success (Snyder and Palmer, 2004). As a point of comparison, the Senseval 3 all-words task had a 75% agreement on nouns (Snyder and Palmer, 2004). Existing hand-annotated corpora like SemCor (Miller et al, 1993), which is annotated with Word Netsenses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al, 2007) English All Words data sets. First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 (Palmer et al, 2001) and Senseval-3 (Snyder and Palmer, 2004) datasets. Senseval 3 shared task data (Snyder and Palmer, 2004). We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al, 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. Existing hand-annotated corpora like Sem Cor (Miller et al, 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). S3AW task In the Senseval-3 all-words task (Snyder and Palmer, 2004) all words in three document excerpts need to be disambiguated. However, two different MFS baseline performance results are reported in Snyder and Palmer (2004), with further implementations being different still. Indeed, only 5 out of the 26 systems in the recent SENSEVAL-3 English all words task (Snyder and Palmer, 2004) outperformed the heuristic of choosing the most frequent sense as derived from SemCor (which would give 61.5% precision and recall).
Japanese Dependency Structure Analysis Based On Support Vector Machines This paper presents a method of Japanese dependency structure analysis based on Support Vector Machines (SVMs). Conventional parsing techniques based on Machine Learning framework, such as Decision Trees and Maximum Entropy Models, have difficulty in selecting useful features as well as finding appropriate combination of selected features. On the other hand, it is well-known that SVMs achieve high generalization performance even with input data of very high dimensional feature space. Furthermore, by introducing the Kernel principle, SVMs can carry out the training in high-dimensional spaces with a smaller computational cost independent of their dimensionality. We apply SVMs to Japanese dependency structure identification problem. Experimental results on Kyoto University corpus show that our sysachieves the 89.09% even with small training data (7958 sentences). The number of votes for the class obtained through the pairwise voting is used as the certain score for beam search with width 5 (Kudo and Matsumoto, 2000a). In the CoNLL-2000 shared task, we achieved the accuracy of 93.48 using IOB2-F representation (Kudo and Matsumoto, 2000b). Kudo and Matsumoto (2002) compare cascaded chunking with the CYK method (Kudo and Matsumoto, 2000).  Kudo and Matsumoto (2000) used the sigmoid function to obtain pseudo probabilities in SVMs. To cope with this problem, Kudo and Matsumoto (2000) introduced a new type of feature called dynamic features, which are created dynamically during the parsing process. We used a third degree polynomial kernel function, which is exactly the same setting in (Kudo and Matsumoto, 2000). The results for the new cascaded chunking model as well as for the previous probabilistic model based on SVMs (Kudo and Matsumoto, 2000) are summarized in Table 2. For example, Haruno et al (1999) used Decision Trees, Sekine (2000) used Maximum Entropy Models, Kudo and Matsumoto (2000) used Support Vector Machines. Therefore, our methods analyze a sentence backwards as in Sekine (2000) and Kudo and Matsumoto (2000).  Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004). Kudo and Matsumoto (2000) describe a dependency parser for Japanese and Yamada and Matsumoto (2003) an extension for English. Therefore, SVMs have shown good performance for text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2001), and dependency structure analysis (Kudo and Matsumoto, 2000). Kudo and Matsumoto (2000) also used the same backward beam search together with SVMs rather than ME. 
Chinese Syntactic Reordering for Statistical Machine Translation Syntactic reordering approaches are an effective method for handling word-order differences between source and target lan guages in statistical machine translation(SMT) systems. This paper introduces a reordering approach for translation from Chinese to English. We describe a set of syntac tic reordering rules that exploit systematic differences between Chinese and English word order. The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English in terms of their word order. We evaluated the reordering approach within the MOSES phrase-based SMT system (Koehn et al, 2007). The reordering approach improved the BLEU score for the MOSES system from 28.52 to 30.86 on the NIST 2006 evaluation data. Wealso conducted a series of experiments to an alyze the accuracy and impact of different types of reordering rules. Moreover, this rule set substantially decreased the total times of rule application about 60%, compared with a constituent-based approach (Wang et al, 2007). The most similar work to this paper is that of Wang et al (2007). We argue that even though the rules by Wang et al (2007) exist, it is almost impossible to automatically convert their rules into rules that are applicable to dependency parsers. The training data, which included those data used in Wang et al (2007), contained 1 million pairs of sentences extracted from the Linguistic Data Consortium's parallel news corpora. We implemented the constituent-based preordering rule set in Wang et al (2007) for comparison, which is called WR07 below. The overall accuracy of this rule set is 60.0%, which is almost at the same level as the WR07 rule set (62.1%), according to the similar evaluation (200 sentences and one annotator) conducted in Wang et al (2007).   There are two reasons why the syntactic reordering approach improves over the baseline phrase-based SMT system (Wang et al, 2007). Collins et al (2005) propose German clause restructuring to improve German-English SMT, while Wang et al (2007) present similar work for Chinese English SMT. Both achieve BLEU score improvements for SMT: 25.2% to 26.8% for (Collins et al, 2005) and 28.52 to 30.86 for (Wang et al, 2007). (Wang et al, 2007) uses rules very similar to our own as they use the same language pair, although they reorder the Chinese, whereas we reorder the English.  Long-distance phrase movement is a common problem in Chinese-English MT, and many MT systems try to handle it (e.g., Wang et al. 2007). There is similar work for Chinese-English (Wang et al, 2007) and quite a few other languages.   Much like Wang et al (2007), we parse the English side of our corpora and reorder using predefined rules. Chinese ordering differs from English mainly in clause ordering (Wang et al, 2007) and within the noun phrase. Clause restructuring performed with hand-crafted reordering rules for German-to-English and Chinese-to-English tasks are presented in (Collins et al, 2005) and (Wang et al, 2007), respectively.
Bilingual Parsing With Factored Estimation: Using English To Parse Korean We describe how simple, commonly understood statistical models, such as statistical dependency parsers, probabilistic context-free grammars, and word-to-word translation models, can be effectively combined into a unified bilingual parser that jointly searches for the best English parse, Korean parse, and word alignment, where these hidden structures all constrain each other. The model used for parsing is completely factored into the two parsers and the TM, allowing separate parameter estimation. We evaluate our bilingual parser on the Penn Korean Treebank and against several baseline systems and show improvements parsing Korean with very limited labeled data. Finally, the work that is the closest to ours in spirit is the idea of joint estimation (Smith and Smith, 2004). More recently, Smith and Smith (2004) proposed to merge an English parser, a word alignment model, and a Korean PCFG parser trained from a small number of Korean parse trees under a unified log linear model. We know of only one study which evaluates these bilingual grammar formalisms on the task of grammar induction itself (Smith and Smith, 2004). (Smith and Smith, 2004) tried to build a Korean parser by bilingual approach with English, and achieved labeled precision/recall around 40% for Korean. Smith and Smith (2004) explore syntactic projection further by proposing an English-Korean bilingual parser integrated with a word translation model. Smith and Smith (2004 ) consider a similar setting for parsing both English and Korean, but instead of learning a joint model, they consider a fixed combination of two parsers and a word aligner.  Another distinct body of work addresses the problem of parser bootstrapping based on syntactic dependency projection (e.g. Hwa et al 2002), often using approaches based in synchronous parsing (e.g. Smith and Smith, 2004). The lattice-conditional estimation approach was first used by Kudo et al (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. Similar results were presented by Smith and Smith (2004), using a similar estimation strategy with a model that included far more feature templates. Smith and Smith (2004) applied factored estimation to a bilingual weighted grammar, driven by data limitations. In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al (2010).
Enforcing Transitivity in Coreference Resolution A desirable quality of a coreference resolution system is the ability to handle transitivity constraints, such that even if it places high likelihood on a particular mention being coreferent with each of two other mentions, it will also consider the likelihood of those two mentions being coreferent when making a final assignment. This is exactly the kind of constraint that integer linear programming (ILP) is ideal for, but, surprisingly, previous work applying ILP to coreference resolution has not encoded this type of constraint. We train a coreference classifier over pairs of mentions, and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments. We present results on two commonly used datasets which show that enforcement of transitive closure consistently improves performance, including imof up to 3.6% using the and up to 16.5% using cluster f-measure.  On the MUC6-TEST dataset, our system outperforms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures. One could use ILP-based decoding in the style of Finkel and Manning (2008) and Song et al (2012) to attempt to explicitly find the optimal C with choice of a marginalized out, but we did not explore this option. Our formulation is equivalent to the one suggested by Finkel and Manning (2008) in a coreference resolution task. As another example, Denis and Baldridge (2007) and Finkel and Manning (2008) perform joint inference for anaphoricity determination and coreference resolution, by using Integer Linear Programming (ILP) to enforce the consistency between the output of the anaphoricity classifier and that of the coreference classifier. Second, we compare our cut-based approach with the five aforementioned approaches to anaphoricity determination (namely, Ng and Cardie (2002a), Ng (2004), Luo (2007), Denis and Baldridge (2007), and Finkel and Manning (2008)) in terms of their effectiveness in improving a learning-based coreference system. It is worth noting, in particular, that Luo (2007), Denis and Baldridge (2007), and Finkel and Manning (2008) evaluate their approaches on true mentions extracted from the answer keys.  Duplicated Finkel and Manning (2008) baseline. Examples include (Finkel and Manning, 2008), using VI, Rand index and clustering F-score for evaluating coreference resolution. Recently Finkel and Manning (2008) show that the optimal ILP solution outperforms the first and best-link methods.   Extending Denis and Baldridge (2007) and Finkel and Manning (2008)'s work, we exploit loose transitivity constraints on coreference pairs. Extending (Denis and Baldridge, 2007) and (Finkel and Manning,2008)'s work, we introduce a loose selection strategy for transitivity constraints, attempting to overcome huge computation complexity brought by transitivity closure constraints. Klenner (2007) and Finkel and Manning (2008)'s work extended the ILP framework to support transitivity constraints. Last, we note that transitive relations have been explored in adjacent fields such as Temporal Information Extraction (Ling and Weld, 2010), Ontology Induction (Poon and Domingos, 2010), and Coreference Resolution (Finkel and Manning, 2008). Transitivity was also used as an information source in other fields of NLP: Taxonomy Induction (Snow et al, 2006), Co-reference Resolution (Finkel and Manning, 2008), Temporal Information Extraction (Ling and Weld, 2010), and Unsupervised Ontology Induction (Poon and Domingos, 2010). see (Finkel and Manning, 2008) for an alternative but equivalent formalization. The model from (Finkel and Manning, 2008) utilizes transitivity, but not exclusivity.
Knowledge-Free Induction Of Inflectional Morphologies We propose an algorithm to automatically induce the morphology of inflectional languages using only text corpora and no human input. Our algorithm combines cues from orthography, semantics, and syntactic distributions to induce morphological relationships in German, Dutch, and English. Using CELEX as a gold standard for evaluation, we show our algorithm to be an improvement over any knowledge-free algorithm yet proposed. Schone and Jurafsky (2001) employ a great many sophisticated post-hoc adjustments to obtain the right conflation sets for words by pure corpus analysis without annotations. Approaches to the induction of morphology as presented in e.g. Schone and Jurafsky (2001) or Goldsmith (2001) show that the morphological properties of a small subset of languages can be induced with high accuracy, most of the existing approaches are motivated by applied or engineering concerns, and thus make assumptions that are less cognitively plausible. Schone and Jurafsky (2001) select words with frequency higher than 5 to induce morphological segmentation. Presupposing input driven learning, it has been shown in the literature that initial segmenations into words (or word-like units) is possible with unsupervised methods (e.g. Brent and Cartwright (1996)), that induction of morphology is possible (e.g. Goldsmith (2001), Schone and Jurafsky (2001)) and even the induction of syntactic structures (e.g. Van Zaanen (2001)). These models include the signature of (Goldsmith 2001), the conflation set of (Schone and Jurafsky 2001), the paradigm of (Brent et. al. 2002), and the inflectional class of (Monson 2004). In later work, Schone and Jurafsky (2001) extend their technique to identify not only suffixes but also prefixes and circumfixes by building both forward and backward tries over a corpus.  Schone and Jurafsky (2001) used latent semantic analysis to find affixes. Schone and Jurafsky (2001) use latent semantic analysis to find prefixes, suffixes and circumfixes in German, Dutch and English. (Goldsmith 2000) presents an unsupervised technique based on the expectation maximization algorithm and minimum description length to segment exactly one suffix per word, resulting in an F-score of 81.8 for suffix identification in English according to (Schone and Jurafsky 2001). (Schone and Jurafsky 2001) proposes an unsupervised algorithm capable of automatically inducing the morphology of inflectional languages using only text corpora. Often trie similarities are used as a first step followed by further processing to identify morphemes (Schone and Jurafsky, 2001). Like Schone and Jurafsky (2001), we build clusters that will have both inflectionally and derivationally related stems and evaluate them with respect to a gold standard of only inflectionally related stems. Schone and Jurafsky (2001) builds on this approach, but adds more ad hoc parameters to handle circumfixation. Further cues for morphological learning are presented in (Schone and Jurafsky, 2001) and (Yarowsky and Wicentowsky, 2000). Schone and Jurafsky (2001) employ distributions over adjacent words (yielding a syntactic distance metric) to improve the precision of their conflation sets. In addition, we measure precision, recall, and F1 as in Schone and Jurafsky (2001). Nevertheless, these metrics give us a point of comparison with Schone and Jurafsky (2001) who, using a vocabulary of English words occurring at least 10 times in a 6.7 million word newswire corpus, report F1 of 88.1 for conflation sets based only on suffixation, and 84.5 for circumfixation.
Minimum Error Rate Training In Statistical Machine Translation Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria. A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text. In this paper, we analyze various training criteria which directly optimize translation quality. These training criteria make use of recently proposed automatic evaluation metrics. We describe a new algorithm for efficient training an unsmoothed error count. We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure. Alternatively, one can train them with respect to the final translation quality measured by some error criterion (Och, 2003). The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning (Och, 2003).  We tune all feature weights automatically (Och, 2003) to maximize the BLEU (Papineni et al, 2002) score on the dev set.  Feature function scaling factors m are optimized based on a maximum likelihood approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). In our setup, we use minimum error-rate training (MERT, Och (2003)) to optimize weights of model components. The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. Feature weights were tuned with MERT (Och, 2003) to maximize BLEU on the NIST MT06 corpus. We adapt the Minimum Error Rate Training (MERT) (Och, 2003) algorithm to estimate parameters for each member model in co-decoding. Feature weights were set with minimum error rate training (Och, 2003) on a development set using BLEU (Papineniet al, 2002) as the objective function. The component features are weighted to minimize a translation error criterion on a development set (Och, 2003). All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU.  We carried out all our experiments using a state-of the-art phrase-based statistical English-to-Japanese machine translation system (Och, 2003). The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al, 2008). The model scaling factors are optimized with respect to the BLEU score similar to (Och, 2003). In addition, we do not use any discriminative training methods such as MERT for optimizing the feature weights (Och, 2003). We carried out all our experiments using a state of-the-art phrase-based statistical Japanese-to English machine translation system (Och, 2003) with pre-ordering. The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6.
A Tale of Two Parsers: Investigating and Combining Graph-based and Transition-based Dependency Parsing Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations. We study both approaches under the framework of beamsearch. By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods. More importantly, we propose a beam-search-based parser that combines both graph-based and transitionbased parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers. Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuraof respectively. a "configuration" is sometimes called a "state" (Zhang and Clark, 2008), but that term is confusing with the states in shift-reduce LR/LL parsing, which are quite different. As we will see in Section 5.1, this simpler arc-standard system performs equally well with a state-of-the-art arc-eager system (Zhang and Clark, 2008) on standard English Treebank parsing (which is never shown before). We also enhance deterministic shift-reduce parsing with beam search, similar to Zhang and Clark (2008), where k configurations develop in parallel. Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al, 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those state of-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beam search mode (k=16).  We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. Hybrid systems have improved parsing by integrating outputs obtained from different parsing models (Zhang and Clark, 2008). CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). To facilitate comparison with previous results, we follow Zhang and Clark (2008b) for data split and constituency-to-dependency conversion of CTB5.  To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MST Parser 1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transition based parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011), and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al, 2013). The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). Graph features are defined over the factors of a graph-based dependency parser, which was shown to improve the accuracy of a transition-based parser by Zhang and Clark (2008). For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), and with the same split as in Zhang and Clark (2008) and Li et al (2011).   Zhang and Clark (2008) was the first to combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explored the addition of graph based features to a transition-based parser.  The number of modifiers to a given head is used by the graph-based submodel of Zhang and Clark (2008) and the models of Martins et al (2009) and Sagae and Tsujii (2007). We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row 'Z&C08 transition'), the dynamic-programming arc-standard parser of Huang and Sagae (2010) (row 'H&S10'), and graph based models including MSTParser (McDonald and Pereira, 2006), the baseline feature parser of Koo et al. (2008) (row 'K08 baeline'), and the two models of Koo and Collins (2010).
Findings of the 2011 Workshop on Statistical Machine Translation This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics. We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics. This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake. We also conducted a pilot ‘tunable metrics’ task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality. Despite avoiding language-specific resources and using only the training data provided by the workshop, an extensive manual evaluation determined that the outputs produced were of significantly higher quality than both statistical and rule-based systems that made use of language-specific resources (Callison-Burch et al, 2011). Spearman's rank correlation coefficients on the document (system) level between all the metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the frame work of the third (Callison-Burch et al, 2008), fourth (Callison-Burch et al, 2009), fifth (Callison Burch et al, 2010) and sixth (Callison-Burch et al, 2011) shared translation tasks. And researchers have grown increasingly concerned that automatic metrics have a strong bias towards preferring statistical translation outputs; the NIST (2008, 2010), MATR (Gao et al, 2010) and WMT (Callison-Burch et al, 2011) evaluations held during the last five years have provided ample evidence that automatic metrics yield results that are inconsistent with human evaluations when comparing statistical, rule-based, and human outputs. Provided for the Sixth EMNLP Workshop on Statistical Machine Translation (Callison-Burch et al 2011) extracted from the annotated NPs in the Penn Tree bank 3.0 corpus. These systems have been dominating the area in the recent years (Callison-Burch et al, 2011). 2 Domain Source Target# Test Language Language sets Europarl (Koehn, 2005) Fr, De, Es En 4 En Fr, De, Es KFTT (Neubig, 2011) Jp, En En, Jp 2 EMEA (Tiedemann, 2009) Da, De En 4 News (Callison-Burch et al, 2011) Cz, En, Fr, De, Es Cz, En, Fr, De, Es 3Table 2: The translation systems used for the curve fitting experiments, comprising 30 language-pair and do main combinations for a total of 96 learning curves. (Callison-Burch et al, 2011) The workshop's human evaluation component has been gradually refined over several years, and as a consequence it has produced a fantastic collection of publicly available data consisting primarily of pairwise judgements of translation systems made by human assessors across a wide variety of languages and tasks. The tables in the Appendix of Callison-Burch et al (2011) report p-values of up to 1%, computed for every pairwise comparison in the dataset. For comparison, the WMT11 rows contain the results from the European languages individual systems task (Callison-Burch et al (2011), Table 7). shared pilot task, this did not hold (Callison-Burch et al, 2011). To measure the impact of different rest costs, we use the Moses chart decoder (Koehn et al 2007) for the WMT 2011 German-English translation task (Callison-Burch et al 2011). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al, 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. The goal of our experiments is to demonstrate the behaviour of the decoder and characterise its response to changes in the fundamental search parameters. The SMT models for our experiments were created with a subset of the training data for the English-French shared task at the WMT 2011 workshop (Callison-Burch et al 2011). The example is taken from the parallel corpus of English and Haitian Kre`yol text messages used in the 2010 Shared Task for the Workshop on Machine Translation (Callison-Burch et al, 2011), which is the corpus used for evaluation in this paper. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al, 2011). Inter-annotator agreement was computed using an adaptation of the kappa index with pairwise rank comparisons (Callison-Burch et al, 2011). (Callison-Burch et al, 2011). For the Haitian Creole English experiments we used the SMS corpus released for WMT11 (Callison-Burch et al, 2011). It has been shown to give better correlations than BLEU for many European languages including English (Callison-Burch et al, 2011). Extensions of the last two are included in this study together with alignments based on hidden Markov model (HMM) (Vogel et al, 1996) and inversion transduction grammars (ITG) (Wu, 1997). System combinations produced via confusion net work decoding using different hypothesis alignment algorithms have been entered into open evaluations, most recently in 2011 Workshop on Statistical Machine Translation (Callison-Burch et al, 2011).
Two Languages are Better than One (for Syntactic Parsing) We show that jointly parsing a bitext can substantially improve parse quality on both sides. In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them. Features include monolingual parse scores and various measures of syntactic divergence. Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables. The resulting bitext parser outperforms state-of-the-art monoparser baselines by 2.5 predicting side trees and 1.8 predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation. In bitext parsing, we can use the information based on "bilingual constraints" (Burkett and Klein, 2008), which do not exist in monolingual sentences. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Following the studies of Burkett and Klein (2008), Huang et al. (2009) and Chen et al. (2010), we used the exact same data split: 1-270 for training, 301-325 for development, and 271-300 for testing.  Burkett and Klein (2008) use the additional knowledge from Chinese-English parallel Treebank to improve Chinese parsing accuracy. It also improves over the discriminative, bilingual parsing model of Burkett and Klein (2008), yielding the highest joint parsing F1 numbers on this data set. We compare our parsing results to the monolingual parsing models and to the English-Chinese bilingual reranker of Burkett and Klein (2008), trained on the same dataset. In addition, our English parsing results are better than those of the Burkett and Klein (2008) bilingual reranker, the current top-performing English-Chinesebilingual parser, despite ours using a much simpler set of synchronization features. For example, Burkett and Klein (2008) show that parsing with joint models on bitexts improves performance on either or both sides. We did not compare our system with the joint model of Burkett and Klein (2008) because they reported the results on phrase structures. In bitext parsing, Burkett and Klein (2008) and Fraser et al (2009) used feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy, requiring translated tree banks.  However, although it is not essentially different, we only focus on dependency parsing itself, while the parsing scheme in (Burkett and Klein, 2008) based on a constituent representation.  For this task, we follow the setup of Burkett and Klein (2008), who improved Chinese and English monolingual parsers using parallel, hand-parsed text. Procedurally, our work is most closely related to that of Burkett and Klein (2008). We parameterize the bilingual view using at most one-to-one matchings between nodes of structured labels in each language (Burkett and Klein, 2008). We approximate this sum using the maximum-scoring matching (Burkett and Klein, 2008). First, we use the word alignment density features from Burkett and Klein (2008), which measure how well the aligned entity pair matches up with alignments from an independent word aligner. For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008).
Automatic Acquisition of Hyponyms ~om Large Text Corpora Mart i  A. Hearst Computer  Science Division, 571 Evans Hall Un ivers i ty  of Cal i fornia,  Berkeley Berkeley,  CA 94720 and Xerox Palo A l to  Research Center mart i~cs ,  berkeley, edu Abst rac t We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text.  Hearst (1992) found individual pairs of hypernyms and hyponyms from text using pattern-matching techniques. The hypernyms used to label the internal nodes of that hierarchy are chosen in a simple fashion; pattern-matching as in Hearst (1992) is used to identify candidate hypernyms of the words dominated by a particular node, and a simple voting scheme selects the hypernyms to be used. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al, 2004). Regarding pattern-based approaches to identify and distinguish lexical semantic relations in more general terms, Hearst (1992) was the first to propose lexico-syntactic patterns as empirical pointers towards relation instances, focusing on hyponymy. Automatically extracted lexico-syntactic patterns have been successfully employed in various term extraction tasks (Hearst, 1992). Phrases such as also known as, is a, part of, is an example of all indicate various of semantic relations. Such indicative phrases have been successfully applied in various tasks such as synonym extraction, hyponym extraction (Hearst, 1992) and fact extraction (Pasca et al, 2006). The idea of searching a large corpus for specific lexico-syntactic phrases to indicate a semantic relation of interest was first described by Hearst (1992). Hearst (1992) pioneered the use of lexical syntactic patterns for automatic extraction of lexical semantic relationships. Table 1: The patterns we used for entailment acquisition based on (Hearst, 1992) and (Pantel et al, 2004). The pioneering work of Hearst (1992) applied fixed patterns like NP1, especially NP2 to derive that NP1 is a hypernym of NP2. The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2The number of ranked concepts that pass CPT changes in each iteration. They build a classifier using the different hypernym patterns and find among the highest precision patterns those of (Hearst, 1992). Recently, (Ritter et al, 2009) reported hypernym learning using (Hearst, 1992) patterns and manually tagged common and proper nouns. (Hearst, 1992) If any pattern matches the structure of the dependency parse tree, the hypernym can be extracted. The procedure uses a collection of Web documents and applies some IsA extraction patterns selected from (Hearst, 1992). Hearst (1992) was the first to propose a pattern-based approach to this task using lexico-syntactic patterns to automatically extract hyponyms and this technique has frequently been used for ontology learning. Initial research on taxonomy learning focused on identifying in a given text lexico-syntactic patterns that suggest hyponymy relations (Hearst, 1992). These patterns, as those proposed by Hearst (1992) and used in many projects since, may not be 100% reliable. Hearst (1992) pioneered using patterns to extract hyponym (is-a) relations. Berland and Charniak (1999) proposed a system for part-of relation extraction, based on the (Hearst 1992) approach.
SURFACE GRAMMATICAL ANALYSIS FOR THE EXTRACTION OF TERMINOLOGICAL NOUN PHRASES Didier BOURIGAULT Ecole des Hautes Etudes en Sciences Sociales et Electlicit6 de France Direction des Etudes et Recherches 1, avenue du G6n6ral de Gaulle 92141 Clamart Cedex France Tel : +33 1 47 65 50 64 ABSTRACT LEXTER is a software package for extracting terminology.  Bourigault &apos; s LECTER [Bourigault, 1992] is a surface-syntactic analyser that extracts &apos; maximal length noun phrases &apos; -mainly sequences of determiners, premodifiers, nominal heads, and certain kinds of post modifying prepositional phrases and adjectives from French texts for terminology applications. Some methods (Bourigault (1992), Ananiadou (1994)) rely purely on linguistic information, namely morpho-syntactic features of term candidates. On the grammar-based side, Bourigault (1992) describes a system for extracting& quot; terminological noun phrases& quot; from French text. Bourigault (1992) reports a tool, LEXTER, for extracting terminologies from texts.
Text Chunking Using Transformation-Based Learning Eric Brill introduced transformation-based learning and showed that it can do part-ofspeech tagging with fairly high accuracy. The same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive &quot;baseNP&quot; chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 92% for baseNP chunks and 88% for somewhat more complex chunks that partition the sentence. Some interesting adaptations to the transformation-based learning approach are also suggested by this application. Following Ramshaw and Marcus (1995), the current dominant approach is formulating chunking as a classification task, in which each word is classified as the (B)eginning, (I)nside or (O) outside of a chunk. NP chunks in the shared task data are BaseNPs, which are non-recursive NPs, a definition first proposed by Ramshaw and Marcus (1995). Transformation-based learning (TBL) was originally introduced via the Brill part-of-speech tagger (Brill, 1992) and has since been applied to a wide variety of NLP tasks, including binary phrase structure bracketing (Brill, 1993), PP-attachment disambiguation (Brill and Resnik, 1994), base NP chunking (Ramshaw and Marcus, 1995), dialogue act tagging (Samuel et al1998), and named entity re cog nition (Black and Vasilakopoulos, 2002). Many approaches to identifying base noun phrases have been explored as part of chunking (Ramshawand Marcus, 1995), but determining sub-NP structure is rarely addressed. Ramshaw and Marcus (Ramshaw and Marcus,1995) first represented base noun phrase recognition as a machine learning problem. Both the IOB representation (Ramshaw and Marcus, 1995) and the Start/End representation (Kudo and Matsumoto, 2001) are popular. Meanwhile, it is common for NP chunking tasks to represent a chunk (e.g., NP) with two labels, the begin (e.g., B-NP) and inside (e.g., I-NP) of a chunk (Ramshaw and Marcus, 1995). They mention that the resulting shallow parse tags are somewhat different than those used by Ramshaw and Marcus (1995), but that they found no significant accuracy differences in training on either set. Training and testing were performed using the noun phrase chunking corpus described in Ramshaw & Marcus (1995) (Ramshaw and Marcus, 1995). Ramshaw and Marcus (1995) first introduced the machine learning techniques to chunking problem. After the work of Ramshaw and Marcus (1995), many machine learning techniques have been applied to the basic chunking task, such as Sup port Vector Machines (Kudo and Matsumoto, 2001), Hidden Markov Model (Molina and Pla 2002), Memory Based Learning (Sang, 2002), Conditional Random Fields (Sha and Pereira, 2003), and so on. The noun phrase extraction module uses Brill &apos; s POS tagger [Brill (1992)] and a base NPchunker [Ramshaw and Marcus (1995)]. Noun phrases were extracted using Ramshaw and Marcus &apos; s base NPchunker [Ramshaw and Marcus (1995)]. Five chunk tag sets, IOB1, IOB2, IOE1, IOE2 (Ramshaw and Marcus, 1995) and SE (Uchimoto et al, 2000), are commonly used. text chunking model (Ramshaw and Marcus, 1995), which has been previously applied to Chinesesegmentation (Peng et al, 2004). (IOB) encoding originating from (Ramshaw and Marcus, 1995). Ramshaw and Marcus (1995), Munoz et al (1999), Argamon et al (1998), Daelemans et al (1999a) find NP chunks, using Wall Street Journal training material of about 9000 sentences. Next, a rule-based text chunker (Ramshaw and Marcus, 1995) is applied on the tagged sentences to further identify phrasal units, such as base noun phrases NP and verbal units VB. Given a weight vector w, the scorew? f (x, y) ranks possible labelings of x, and we denote by Yk, w (x) the set of k top scoring labelings for x. We use the standard B, I, O encoding for named entities (Ramshaw and Marcus, 1995). The NP chunks in the shared task data are base-NP chunks which are non-recursive NPs, a definition first proposed by Ramshaw and Marcus (1995).
A Conditional Random Field Word Segmenter for Sighan Bakeoff 2005 We present a Chinese word seg mentation system submitted to the closed track of Sighan bakeoff 2005. Our segmenter was built using a condi tional random field sequence model that provides a framework to use a large number of linguistic features such as character identity, morphological and character reduplication features. Because our morphological features were extracted from the training cor pora automatically, our system was not biased toward any particular variety of Mandarin. Thus, our system does not overfit the variety of Mandarin most familiar to the system's designers. Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR). In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al, 2005), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower. Using the same approach as in (Tseng et al, 2005), we extracted the most frequent words tagged with 'B', indicating a prefix, and the last words tagged with 'I', denoting a suffix. For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NISTMT training corpora, segmented with the Stanford segmenter (Tseng et al, 2005). (Tseng et al, 2005) achieve an SF of 95.0%, 95.3% and 86.3% on PKU data from the Sighan Bakeoff 2005, PKU data from the Sighan Bakeoff 2003 and CTB data from the Sighan Bakeoff 2003 respectively.  They are selected from similar sources to the newswire articles, and are normalized (Zhang and Kahn, 2008) and word segmented (Tseng et al, 2005a). The Chinese sentences are segmented using the Stanford Chinese word segmenter (Tseng et al, 2005). We use the Stanford Chinese word segmenter (Tseng et al, 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym definition during matching.  Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al, 2005), respectively. English-Chinese: For training we used the LDC Sinorama and FBIS tests (LDC2005T10 and LDC2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005).  For the English-Chinese (E2C) baseline system, we trained on the LCD Sinorama and FBIStests (LCD2005T10 and LCD2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005).  Chinese was automatically segmented by the Stanford segmenter (Tseng et al, 2005), and traditional characters were simplified. This character-by-character method was first proposed by (Xue, 2003), and a number of discriminative sequential learning algorithms have been exploited, including structured perceptron (Jiang et al, 2009), the Passive-Aggressive algorithm (Sun, 2010), conditional random fields (CRFs) (Tseng et al, 2005), and latent variable CRFs (Sun et al, 2009). The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al, 2005). For training we used the non-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al, 2005). The FBIS-corpus was used as training corpus and all Chinese sentences were word segmented with the Stanford Segmenter (Tseng et al, 2005). Stanford Chinese word segmenter (STANFORD): The Stanford Chinese word segmenter is another well-known CWS tool (Tseng et al., 2005).
Chinese Part-Of-Speech Tagging: One-At-A-Time Or All-At-Once? Word-Based Or Character-Based? Chinese part-of-speech (POS) tagging assigns one POS tag to each word in a Chinese sentence. However, since words are not demarcated in a Chinese sentence, Chinese POS tagging requires word segmentation as a prerequisite. We could perform Chinese POS tagging strictly after word segmentation approach), or perform both word segmentation and POS tagging in a combined, single step simultaneously (all-atonce approach). Also, we could choose to assign POS tags on a word-by-word basis, making use of word features in the surrounding context (word-based), or on a character-by-character basis with character features (character-based). This paper presents an in-depth study on such issues of processing architecture and feature representation for Chinese POS tagging, within a maximum entropy framework. We found that while the all-at-once, characterbased approach is the best, the one-at-a-time, character-based approach is a worthwhile compromise, performing only slightly worse in terms of accuracy, but taking shorter time to train and run. As part of our investigation, we also built a state-of-the-art Chinese word segmenter, which outperforms the best SIGHAN 2003 word segmenters in the closed track on 3 out of 4 test corpora. So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004). If we extend these positional tags to include POS information ,segmentation and POS tagging can be performed by a single pass under a unify classification framework (Ng and Low, 2004). In the rest of the paper, we call this operation mode Joint S&T. Experiments of Ng and Low (2004) shown that, compared with performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging. As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively. The features we use to build the classifier are generated from the templates of Ng and Low (2004). The table's upper column lists the templates that immediately from Ng and Low (2004). Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004). Compared to performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004). Ac cording to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: b: the begin of the word m: the middle of the word e: the end of the word s: a single-character word. In order to perform POS tagging at the same time, we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004). Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target. Note that the templates of Ng and Low (2004) have already contained some lexical-target ones. Similar trend appeared in experiments of Ng and Low (2004), where they conducted experiments on CTB 3.0 and achieved F measure 0.919 on Joint S&T, a ratio of 96% to the F-measure 0.952 on segmentation. In addition, all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus, whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low, 2004). For example, Li et al (2010) reported that a joint syntactic and semantic model improved the accuracy of both tasks, while Ng and Low (2004) showed it is beneficial to integrate word segmentation and part-of-speech tagging into one model. In this bakeoff, our models built for the tasks are similar to that in the work of Ng and Low (2004). For this task, because of the time limitation as mentioned in the previous section, we could only port our implemented model by using a part of the feature set which was used in the word-based tagger discussed in the work of Ng and Low (2004). Our linguistic features are adopted from (Ng and Low, 2004) and (Tseng et al., 2005).  Similar to (Ng and Low, 2004), we found the overall F measure only goes up a tiny bit, but we do find a significant OOV recall rate improvement.
A Comparative Study On Reordering Constraints In Statistical Machine Translation In statistical machine translation, the generation of a translation hypothesis is computationally expensive. If arbitrary wordreorderings are permitted, the search problem is NP-hard. On the other hand, if we restrict the possible word-reorderings in an appropriate way, we obtain a polynomial-time search algorithm. In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints. This comparison includes a theoretical discussion on the permitted number of reorderings for each of these constraints. We show a connection between the ITG constraints and the since 1870 known We evaluate these constraints on two tasks: the Verbmobil task and the Canadian Hansards task. The evaluation consists of two parts: First, we check how many of the Viterbi alignments of the training corpus satisfy each of these constraints. Second, we restrict the search to each of these constraints and compare the resulting translation hypotheses. The experiments will show that the baseline ITG constraints are not sufficient on the Canadian Hansards task. Therefore, we present an extension to the ITG constraints. These extended ITG constraints increase the alignment coverage from about 87% to 96%. It has been pointed out in (Zens and Ney, 2003) that the ITG constraints can be characterized as follows: a reordering violates the ITG constraints if and only if it contains (3, 1, 4, 2) or (2, 4, 1, 3) as a subsequence. A comparison of the ITG constraints and the IBM constraints for single-word based models can be found in (Zens and Ney, 2003). (Zens and Ney, 2003) did comparative study over different reordering constraints. 6 compares our results to related work, in particular Zens and Ney (2003). Related work includes Wu (1997), Zens and Ney (2003) and Wellington et al (2006). xITGs (Zens and Ney, 2003) in part solves this problem.  Zens and Ney (2003) used GIZA++ to word-align the Verbmobil task (English and German) and the Canadian Hansards task (English and French) and tested the coverage of ITGs and xITGs, i.e. the ratio of the number of alignment configurations that could be induced by the theories and the sentences in the two tasks. ITG imposes constraints on which alignments are possible, and these constraints have been shown to be a good match for real bi text data (Zens and Ney, 2003). For a comparison and a more detailed discussion of the two approaches see (Zens and Ney, 2003). Such reordering was realized either by an additional constraint for decoding, such as window constraints, IBM constraints or ITG-constraints (Zens and Ney,2003), or by lexicalized reordering feature functions (Tillman, 2004). Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of Berger et al (1996). The ITG we apply in our experiments has more structural labels than the primitive bracketing grammar: it has a start symbol S, a single preterminal C, and two intermediate nonterminals A and B used to ensure that only one parse can generate any given word-level alignment, as discussed by Wu (1997) and Zens and Ney (2003). Some of the properties of these alignments are studied in (Zens and Ney, 2003). A comparison of both methods can be found in Zens and Ney (2003). A typical value of m is 4 (Zens and Ney, 2003), and we write IBM constraints with m= 4 as IBM (4). Zens and Ney (2003) [3] show that ITG constraints yield significantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). Because of this, Wu (1997) and Zens and Ney (2003) introduced a normal form ITG which avoids this over-counting. This source of overcounting is considered and fixed by Wu (1997) and Zens and Ney (2003), which we briefly review here. 
The Tradeoffs Between Open and Traditional Relation Extraction  We follow an open IE approach (Banko and Etzioni, 2008) and use dependencies to identify the elements. The ability to identify entities like people, organizations and geographic locations (Tjong KimSang and De Meulder, 2003), extract their attributes (Pasca, 2008), and identify entity relations (Banko and Etzioni, 2008) is useful for several applications in natural language processing and knowledge acquisition tasks like populating structured knowledge bases (KB). The information extraction oeuvre has a gamut of relation extraction methods for entities like persons, organizations, and locations, which can be classified as open or closed-domain depending on the restrictions on extractable relations (Banko and Etzioni, 2008). Downey et al (2007) starts from the output of the unsupervised information extraction system TextRunner (Banko and Etzioni, 2008), and uses language modeling techniques to estimate the reliability of sparse extractions. The point of these work is that they selected relation expressions only from the words located between given entities in the text, because as far as Englishtexts are concerned, 86% of the relation expressions of named entity pairs appear between the pair (Banko and Etzioni, 2008). In (Banko and Etzioni, 2008) a Conditional Random Field (CRF) classifier is used to perform Open Relation Extraction which improves by more than 60% the F-score achieved by the Naive Bayes model in the TextRunner system. On four common relations (Acquisition, Birthplace, InvetorOf, WonAward), Open RE attained a recall of 18.4% in comparison to 58.4% achieved by Traditional RE (Banko and Etzioni 2008). Thus, as our input, we utilize tuples extracted by TEXTRUNNER (Banko and Etzioni, 2008) when run over a corpus of 500 million web pages. We use the Open IE corpus generated by running TEXTRUNNER on 500 million high quality Webpages (Banko and Etzioni, 2008) as the source of instance data for these relations. In contrast, the recently proposed Open Information Extraction paradigm aims to detect related pairs of entities without knowing in advance what kinds of relations exist between entities in the source data and without any seeding (Banko and Etzioni, 2008). In particular, they found that almost 40% of such relations are realized by the argument-verb-argument pattern (henceforth, AVA) (see Table 1 in Banko and Etzioni (2008)). The TextRunner system (Banko and Etzioni, 2008) is trained using a CRF classifier on S-V-O tuples from a parsed corpus as positive examples, and tuples that violate phrasal structure as negative ones. TextRunner achieves P=0.94, R=0.65, and F Score=0.77 on the AVA pattern (Banko and Etzioni,2008). We run LDA-SP to compute preferences on amassive dataset of binary relations r (a1 ,a2) extracted from the Web by TEXTRUNNER (Banko and Etzioni, 2008). For all experiments we make use of a corpus of r (a1 ,a2) tuples, which was automatically extracted by TEXTRUNNER (Banko and Etzioni, 2008) from 500 million Web pages. Banko and Etzioni (Banko and Etzioni, 2008) showed that a small set of POS-tag patterns cover a large fraction of relationships in English, but never incorporated the patterns into an extractor. The TEXTRUNNER Open IE system (Banko and Etzioni, 2008) employs only shallow syntactic features in the extraction process. Overall, we provide evidence that, contrary to belief in the Open IE literature (Banko and Etzioni, 2008), semantic approaches have a lot to offer for the task of Open IE and the vision of machine reading. Although OIE often has lower precision than traditional information extraction, it is able to extract a wider variety of relations at precision levels that are often useful (Banko and Etzioni, 2008). Banko and Etzioni (2008) cite a precision score of 88% for their system.
Bayesian Unsupervised Topic Segmentation This paper describes a novel Bayesian approach to unsupervised topic segmentation. Unsupervised systems for this task are driven the tendency of wellformed segments to induce a compact and consistent lexical distribution. We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation. This contrasts with previous approaches, which relied on hand-crafted cohesion metrics. The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems. Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets. We also show that both an entropy-based analysis and a well-known previous technique can be de Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). Eisenstein and Barzilay (2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations. Eisenstein and Barzilay (2008) describe a dynamic program for linear segmentation with a space complexity of O (T) and time complexities of O (T 2) to compute the A matrix and O (TW) to fill the B matrix. Eisenstein and Barzilay (2008) use the same dataset to evaluate linear topic segmentation, though they evaluated only at the level of sections, given gold standard chapter boundaries. This is equivalent to BAYESSEG, which achieved the best reported performance on the linear segmentation of this same dataset (Eisenstein and Barzilay, 2008). Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. Unlike (Eisenstein and Barzilay, 2008), we cannot make an assumption that the number of segments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure. If the actual number of segments is known and only a linear discourse structure is acceptable, then a single move, shift of the segment border (Fig. 2(a)), is sufficient (Eisenstein and Barzilay, 2008). The second baseline is a pipeline approach (Pipeline), where we first segment the lecture transcript with BayesSeg (Eisenstein and Barzilay, 2008) and then use the pairwise alignment to find their best alignment to the segments of the story. The Bayesian framework explored by Eisenstein and Barzilay (2008) is a potential route to a richer model, and they found their richer model beneficial for a meetings corpus but not for a textbook. This insight has been used to tune supervised methods (Hsueh et al, 2006) and inspire unsupervised models of lexical cohesion using bags of words (Purver et al, 2006) and language models (Eisenstein and Barzilay, 2008). In order to illustrate why using a single gold standard reference segmentation can be problematic, we evaluate three publicly available segmenters, MinCutSeg (Malioutov and Barzilay, 2006), BayesSeg (Eisenstein and Barzilay, 2008) and APS (Kazantseva and Szpakowicz, 2011), using several different gold standards and then using all available annotations. We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). Eisenstein and Barzilay (2008) treat words in a sentence as draws from a multinomial language model. We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008).  Three automatic segmenters were trained - or had their parameters estimated upon - The Moonstone data set, including MinCut; (Malioutov and Barzilay, 2006), BayesSeg; (Eisenstein and Barzilay, 2008), and APS (Kazantseva and Szpakowicz, 2011).
Lexical Cohesion Computed By Thesaural Relations As An Indicator Of The Structure Of Text 1. virgin 31 2. pine 31 3. bush 31 (3, 1)8 4. trees 32 (4, 1)8 (4, 3)8 5. trunks 32 6. trees 33 (6, 4)8 (6, 1-3)8 Chain 6 Word Sentence Lexical Chain 1. hand-in-hand 34 2. matching 34 3. whispering 35 4. laughing 35 5. warm 38 (5, 1)8 (5, 4)8 Chain 7 Word Sentence Lexical Chain 1. first 1 2. initial 1 (2, 1)8 3. final 2 (3, 2-1)8 47 Computational Linguistics Volume 17, Number 1 Chain 8 Word Sentence Lexical Chain 1. night 2 2. dusk 3 3. darkness 3 Chain 9 Word Sentence Lexical Chain 1. environment 7 2. setting 7 3. surrounding 8 Paradigmatic similarity is based on association data extracted from thesauri [Morris and Hirst, 1991], psychological experiments [Osgood, 1952], and so on. (Xiong et al, 2013b) incorporated lexical-chain-based models (Morris and Hirst, 1991) into machine translation. An algorithm for computing lexical chains was first given by (Morris and Hirst, 1991) using the Roget's Thesaurus (Kirkpatrick, 1998). Morris and Hirst developed an algorithm (Morris and Hirst, 1991) based on lexical cohesion relations (Halliday and Hasan, 1976). Usually a lexical chain is obtained in a bottom-up fashion, by taking each candidate word of a text, and finding an appropriate relation offered by a thesaurus as Rodget (Morris and Hirst, 1991) or WordNet (Barzilay and Elhadad, 1999). Lexical cohesion analysis has been used in such NLP applications as determining the structure of text (Morris and Hirst, 1991) and automatic text summarization (Barzilay and Elhadad, 1999). Roget's Thesaurus, which was used to form the lexical chains in Morris and Hirst (1991), also gives non classically related word groups. The lexical chains of Morris and Hirst (1991) had no such restriction, and frequently nouns, verbs ,adjectives, adverbs, and verbs were joined together in one chain. In fact, each column of the matrix corresponds to a lexical chain (Morris and Hirst, 1991) for a particular term across the whole text. Global context where the semantic measures are employed to derive lexical chains, which are threads of meaning often drawn throughout an en tire text (Morris and Hirst, 1991). The most closely related method is perhaps the lexical chains algorithm (Morris and Hirst, 1991) where threads of meaning are identified throughout a text. More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al, 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. The idea of using lexical chains as indicators of lexical cohesion goes back to Morris and Hirst (1991). Two words are WordNet-related if their WordNet distance is less than 4 (this is consistent with works on lexical-cohesion, (Morris and Hirst, 1991)). WordNet is the main resource for detecting the cohesive relationships between words and their relevance to a given chain (Morris and Hirst, 1991). Ever since Morris and Hirst (1991)' s ground breaking paper, topic segmentation has been a steadily growing research area in computational linguistics. Lexical cohesion techniques include similarity measures between adjacent blocks of text, as in TextTiling (Hearst, 1994, 1997) and lexical chains based on recurrences of a term or related terms, as in Morris and Hirst (1991). The relation between entities (noun phrases) in adjacent sentences could be of type center-reference (pronoun reference or reiteration), or based on semantic relatedness (Morris and Hirst,1991). Morris and Hirst (1991) suggested building lexical chains is important in the resolution of lexical ambiguity and the determination of coherence and discourse structure. Morris and Hirst (1991) were the first to propose the concept of Lexical Chains to explore the discourse structure of a text.
Inversion Transduction Grammar for Joint Phrasal Translation Modeling We present a phrasal inversion transduction grammar as an alternative to joint phrasal translation models. This syntactic model is similar to its flatstring phrasal predecessors, but admits polynomial-time algorithms for Viterbi alignment and EM training. We demonstrate that the consistency constraints that allow flat phrasal models to scale also help ITG algorithms, producing an 80-times faster inside-outside algorithm. We also show that the phrasal translation tables produced by the ITG are superior to those of the flat joint phrasal model, producing up to a 2.5 point improvement in BLEU score. Finally, we explore, for the first time, the utility of a joint phrasal translation model as a word alignment method. For instance, incorporating WSD predictions into an MT decoder based on inversion transduction grammars (Wu, 1997) such as the Bracketing ITG based models of Wu (1996), Zens et al (2004), or Cherry and Lin (2007) would present an intriguing comparison with the present work. Cherry and Lin (2007) incorporate phrase pairs in phrase-based SMT into ITG, and Haghighi et al (2009) introduce Block ITG (BITG), which adds 1-to-many or many-to-1 terminal unary rules. Our first pruning technique is broadly similar to Cherry and Lin (2007a). For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (Cherry and Lin, 2007). Cherry and Lin (2007) and Zhang et al (2008) used synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem. The scope of iterative phrasal ITG training, therefore, is limited to determining the boundaries of the phrases anchored on the given one-to-one word alignments. The heuristic method is based on the Non Compositional Constraint of Cherry and Lin (2007). Cherry and Lin (2007) use GIZA++ intersections which have high precision as anchor points in the bitext space to constraint ITG phrases. Therefore, researches like Cherry and Lin (2007), Haghighi et al (2009) and Zhang et al (2009) tackle this problem by enriching ITG, in addition to word pairs, with pairs of phrases (or blocks). For some restricted combinatorial spaces of alignments those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al,2004) inference can be accomplished using polynomial time dynamic programs. Similarly, Cherry and Lin (2007) use ITG for pruning. Without initializing by phrases extracted from existing alignments (Cherry and Lin, 2007) or using complicated block features (Haghighi et al, 382 2009), we further reduced AER on the test set to 12.25.
Towards The Automatic Identification Of Adjectival Scales: Clustering Adjectives According To Meaning In this paper we present a method to group adjectives according to their meaning, as a first step towards the automatic identification of adjectival scales. We discuss the properties of adjectival scales and of groups of semantically related adjectives and how they imply sources of linguistic knowledge in text corpora. We describe how our system exploits this linguistic knowledge to compute a measure of similarity between two adjectives, using statistical techniques and without having access to any semantic information about the adjectives. We also show how a clustering algorithm can use these similarities to produce the groups of adjectives, and we present results produced by our system for a sample set of adjectives. We conclude by presenting evaluation methods for the task at hand, and analyzing the significance of the results obtained. Early approaches to attribute learning include Hatzivassiloglou and McKeown (1993), who cluster adjectives that denote values of the same attribute. In this paper, we concentrate on adjectives, which have received less attention (see though Hatzivassiloglou and McKeown (1993) and Lapata (2001)). As evaluation measure, we used a pair-wise measure which calculates precision, recall and a harmonic fscore as follows: Each verb pair in the cluster analysis was compared to the verb pairs in the gold standard classes, and evaluated as true or false positive (Hatzivassiloglou and McKeown, 1993). We acknowledge previous work on the computational study of adjectival scales as in (Hatzivassiloglou and McKeown, 1993), where a system could group gradation scales using a clustering algorithm.
Automatic Labeling Of Semantic Roles We present a system for identifythe semantic relationships, or sefilled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data. See (Gildea and Jurafsky, 2000) for some promising initial work in applying statistical techniques to the FrameNet database to automatically label frame elements. While a machine learning approach is used in (Gildea and Jurafsky, 2000) to determine general semantic roles, we used a simple rule-based traversal of the parse tree instead, which could also reliably determine the generic agent and patient role of a sentence, and this suffices for our current purpose. Many researchers (Blaheta and Charniak 2000), (Gildea and Jurafsky 2000), showed that lexical and syntactic information is very useful for predicate argument recognition tasks, such as semantic roles. Gildea and Jurafsky (2000, 2002) describe a statistical approach for semantic role labelling using data collected from FrameNet. (Pado et al., 2008) describe an unsupervised approach that, like ours, uses verbal argument patterns to deduce deverbal patterns, though the resulting labels are semantic roles used in SLR tasks (cf. (Gildea and Jurafsky, 2000)) rather than syntactic roles.  Starting with Gildea and Jurafsky (2000), a number of studies have developed (almost exclusively statistical) models of this task, e.g. Thompson et al. (2003) and Fleischman et al. (2003). Since direct assignment of role labels to instances fails due to the preponderance of unlabelled instances, which make up 86.7% of all instances, we follow Gildea and Jurafsky (2000) in splitting the task into two sequential subtasks: first, argument recognition decides for each instance whether it bears a semantic role or not; then, argument labelling assigns a label to instances recognised as role-bearers. There has been some related work on using the frame of FrameNet for reasoning (Chang et al, 2002) and also on the automatic annotation of English texts with regard to the relevant frames (Gildea and Jurafsky, 2000) and frame elements. To our knowledge, Gildea and Jurafsky (2000) is the only work that uses FrameNet to build a statistical semantic classifier. Gildea and Jurafsky (2000) describe a system that uses completely syntactic features to classify the Frame Elements in a sentence. We extend Gildea and Jurafsky (2000)'s initial effort in three ways. Training (32,251 sentences), development (3,491 sentences), and held out test sets (3,398 sentences) were generated from the June 2002 FrameNet release following the divisions used in Gildea and Jurafsky (2000). Due to data sparsity issues, we do not calculate this model directly, but rather, model various feature combinations as described in Gildea and Jurafsky (2000). Gildea and Jurafsky (2000) use 36995 training, 4000 development, and 3865 test sentences. As a further analysis, we have examined the performance of our base ME model on the same test set as that used in Gildea and Jurafsky (2000). Following Gildea and Jurafsky (2000), automatic extraction of grammatical information here is limited to the governing category of a Noun Phrase. (Gildea and Jurafsky, 2000) proposed a statistical approach based on FrameNet I data for annotation of semantic roles. The features used for training the labeler are a subset of the features used by Gildea and Jurafsky (2000), Xue and Palmer (2004), and Pradhan et al (2004).
Efficient Support Vector Classifiers For Named Entity Recognition Named Entity (NE) recognition is a task in whichproper nouns and numerical information are extracted from documents and are classified into cat egories such as person, organization, and date. It is a key technology of Information Extraction and Open-Domain Question Answering. First, we showthat an NE recognizer based on Support Vector Ma chines (SVMs) gives better scores than conventional systems. However, off-the-shelf SVM classifiers are too inefficient for this task. Therefore, we present a method that makes the system substantially faster.This approach can also be applied to other similar tasks such as chunking and part-of-speech tagging. We also present an SVM-based feature selec tion method and an efficient training method. For this task, we use a named entity recognizer (Isozaki and Kazawa, 2002). Discriminative classifiers, which directly model the posterior distribution of class label given features, i.e. SVM (Isozaki and Kazawa 2002) and Maximum Entropy model for NER (Chieu and Ng 2003), have been shown to outperform generative model based classifiers. Several approaches used classifiers such as decision trees or SVMs (Isozaki and Kazawa, 2002). Isozaki and Kazawa (2002) compared three commonly used methods for named entity recognition the SVM with quadratic kernel, maximal entropy method, and a rule based learning system, and showed that the SVM-based system performed better than the other two. (Since we used Isozaki's methods (Isozaki and Kazawa, 2002), the run-time complexity is not a problem.) Kudo and Matsumoto (2002) proposed an SVM-based Dependency Analyzer for Japanese sentences. In natural language applications, the size |SV| tends to be very large (Isozaki and Kazawa, 2002), often above 10,000. Kernel Expansion (Isozaki and Kazawa, 2002) is used to transform the d-degree polynomial kernel based classifier into a linear one, with a modified decision function y (x) =sgn (w xd+ b). However, even the sparse-representation version of w tends to be very large: (Isozaki and Kazawa, 2002) report that some of their second degree expanded NER models were more than 80 times slower to load than the original models (and 224 times faster to classify). This approach obviously does not scale well, both to tasks with more features and to larger degree kernels. PKE Heuristic Kernel Expansion, was introduced by (Kudo and Matsumoto, 2003). Support vector machines (e.g., Vapnik (1995), Joachims (1998)) are a different kind of kernel method that, unlike KPCA methods, have al ready gained high popularity for NLP applications (e.g., Takamura and Matsumoto (2001), Isozaki and Kazawa (2002), Mayfield et al (2003)) including the word sense disambiguation task (e.g., Cabezas et al (2001)). For example, an SVM-based NE-chunker run sat a rate of only 85byte/sec, while previous rule based system can process several kilobytes per second (Isozaki and Kazawa, 2002). Isozaki et al propose an XQK (eXpand the Quadratic Kernel) which can make their Named-Entity recognizer drastically fast (Isozaki and Kazawa, 2002). For the experiments reported here, we adopt a Support Vector Machine (SVM) learning paradigm not only because it has recently been used with success in different tasks in natural language processing (Isozaki and Kazawa, 2002), but it has been shown particularly suitable for text categorization (Kumar and Gopal, 2009) where the feature space is huge, as it is in our case. (Ikehara et al, 1997), which is similar to WordNet in English, as the attributes of the node. The chunks and their relations in the texts were analyzed by cabocha (Kudo and Matsumoto, 2002), and named entities were analyzed by the method of (Isozaki and Kazawa, 2002). Isozaki and Kazawa (2002) studied the use of SVM instead. Isozaki (Isozaki and Kazawa,2002) controls the parameters of a statistical morphological analyzer so as to produce more fine-grained output. Isozaki (Isozaki and Kazawa, 2002) introduces the thesaurus NTT Goi Taikei (Ikehara et al, 1999) to augment the Table 5: The depth of redundant analysis and the extraction accuracy Pair Wise Method Depth of morph. In this paper, we employ an SVM-based NER method in the following way that showed good NER performance in Japanese (Isozaki and Kazawa, 2002). Kernel expansion (KE) was proposed by Isozaki and Kazawa (2002) to convert Eq.  Words, chunks and their relations in the texts were analyzed by CaboCha (Kudoand Matsumoto, 2002), and named entities were analyzed by the SVM-based NE tagger (Isozaki and Kazawa, 2002).
Japanese Dependency Analysis Using Cascaded Chunking In this paper, we propose a new statistical Japanese dependency parser using a cascaded chunking model. Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model, which is not always efficient or scalable. We propose a new method that is simple and efficient, since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side. Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency. The dependency parser we apply is an implementation of a shift-reduce dependency parser which uses a bunsetsu-chunk as a basic unit for parsing (Kudo and Matsumoto, 2002). We converted the POS system used in the Kyoto Text Corpus into ChaSen's POS system because we used ChaSen, a Japanese morphological analyzer, and CaboCha3 (Kudo and Matsumoto, 2002), a dependency analyzer incorporating SVMs, as a state-of the art corpus-based Japanese dependency structure analyzer that prefers ChaSen's POS system to that of JUMAN. Next, against the results of identifying compound functional expressions, we apply the method of dependency analysis based on the cascaded chunking model (Kudo and Matsumoto, 2002), which is simple and efficient because it parses a sentence deterministically only deciding whether the current bunsetsu segment modifies the one on its immediate right hand side. Next, we further show that the dependency analysis model of (Kudo and Matsumoto, 2002) applied to the results of identifying compound functional expressions significantly outperforms the one applied to the results without identifying compound functional expressions. Unlike probabilistic dependency analysis models of Japanese, the cascaded chunking model of Kudoand Matsumoto (2002) does not require the probabilities of dependencies and parses a sentence deterministically. As a Japanese dependency analyzer based on the cascaded chunking model, we use the publicly available version of CaboCha (Kudo and Matsumoto, 2002), which is trained with the manually parsed sentences of Kyoto text corpus (Kurohashi and Nagao, 1998), that are 38,400 sentences selected from the 1995 Mainichi newspaper text. Dynamic features include bunsetsu segments modifying the current candidate modifiee (see Kudo and Matsumoto (2002) for the details). In our experiments, we used the same settings as (Kudo and Matsumoto, 2002). In previous research, we presented a state-of-the-art SVMs-based Japanese dependency parser (Kudo and Matsumoto, 2002). (Since we used Isozaki's methods (Isozaki and Kazawa, 2002), the run-time complexity is not a problem.) Kudo and Matsumoto (2002) proposed an SVM based Dependency Analyzer for Japanese sentences. Japanese dependency parsers such as Cabocha (Kudo and Matsumoto, 2002) can extract Bps and their dependencies with about 90% accuracy. The recent availability of more corpora has enabled much information about dependency relations to be obtained by using a Japanese dependency analyzer such as KNP (Kurohashi and Nagao, 1994) or CaboCha (Kudo and Matsumoto,2002). The articles were morphologically analyzed by Mecab (Kudo et al, 2003) and syntactically parsed by Cabocha (Kudo and Matsumoto, 2002).  2.4.3 Medium Parser (CaboCha-RMRS) For Japanese, we produce RMRS from the dependency parser Cabocha (Kudo and Matsumoto, 2002). The sentences were dependency parsed with Cabocha (Kudo and Matsumoto, 2002), and co occurrence samples of event mentions were extracted. When parsing with Jacy failed, comparisons could still be made with RMRS produced from shallow tools such as ChaSen (Matsumo to et al, 2000), a morphological analyser or CaboCha (Kudo and Matsumoto, 2002), a Japanese dependency parser. One of the most related models is the cascaded chunking model by (Kudo and Matsumoto, 2002). Kudo and Matsumoto (2002) give more comprehensive comparison with the probabilistic models as used in (Uchimoto et al, 1999). Note that this use of local contexts is similar to the dynamic features in (Kudo and Matsumoto, 2002) 4.
Noun Classification From Predicate-Argument Structures A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described. The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification. Hindle (1990) used noun-verb syntactic relations, and Hatzivassiloglou and McKeown (1993) used coordinated adjective-adjective modifier pairs. Hindle (1990) uses a mutual-information based metric derived from the distribution of subject, verb and object in a large corpus to classify nouns. The second class of algorithms uses co occurrence statistics (Hindle 1990, Lin 1998). NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). To extract semantic information of words such as synonyms and antonyms from corpora, previous research used syntactic structures (Hindle 1990, Hatzivassiloglou 1993 and Tokunaga 1995), response time to associate synonyms and antonyms in psychological experiments (Gross 1989), or extracting related words automatically from corpora (Grefensette 1994).   Our method is similar to (Hindle, 1990), (Lin, 1998), and (Gasperin, 2001) in the use of dependency relationships as the word features. Hindle (1990) classified nouns on the basis of co-occurring patterns of subject verb and verb-object pairs.  The method of using distributional patterns in a large text corpus to find semantically related English nouns first appeared in Hindle (1990). Hindle (1990) grouped nouns into thesaurus-like lists based on the similarity of their syntactic con texts. The only difference is that we also work on partial parsing as a task in its own right: Hindle (1990) inter alia. The classifier for learning coordinate terms relies on the notion of distributional similarity, i.e., the idea that two words with similar meanings will be used in similar contexts (Hindle, 1990). Our syntactic-relation-based thesaurus is based on the method proposed by Hindle (1990), although Hindle did not apply it to information retrieval.  In (Hindle, 1990), a small set of sample results are presented. Hindle (1990) classified nouns on the basis of co-occurring patterns of subject verb and verb-object pairs. For example, Hindle (1990) used co-occurrences between verbs and their subjects and objects, and proposed a similarity metric based on mutual information, but no exploration concerning the effectiveness of other kinds of word relationship is provided, although it is extendable to any kinds of contextual information. To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al 2005), and word similarity lists (Hindle 1990).
Simple Semi-supervised Dependency Parsing We present a simple and effective semisupervised method for training dependency parsers. We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus. We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions. For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuof in the case of Czech unlabeled second-order parsing, we from a baseline accuracy of addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance. Second, for syntactic dependency parsing, combining Brown cluster features with word forms or POS tags yields high accuracy even with little training data (Koo et al, 2008). Additional templates we include are the relative position (Bj ?orkelund et al, 2009), geneological relationship, distance (Zhao et al, 2009), and binned distance (Koo et al, 2008) between two words in the path. In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al, 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al, 2004). We did not observe the same trend in the reduction of annotation need with cluster-based features as in Koo et al (2008) for dependency parsing. Koo et al (2008) have proposed to use word clusters as features to improve graph-based statistical dependency parsing for English and Czech. The authors report 97.70% of accuracy and 90.01% for unseen data. We use the Brown et al (1992) hard clustering algorithm, which has proven useful for various NLP tasks such as dependency parsing (Koo et al, 2008) and named entity recognition (Liang, 2005). This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al, 2008) and transfer (Hwa et al, 2005) systems for prediction of linguistic structure. We observe an average absolute increase in LAS of approximately 1%, which is inline with previous observations (Koo et al, 2008). A simple method for using unlabeled data in discriminative dependency parsing was provided in (Koo et al, 2008) which involved clustering the labeled and unlabeled data and then each word in the dependency tree bank was assigned a cluster identifier. In this work, following (Koo et al, 2008), we use word cluster identifiers as the source of an additional set of features. The reader is directed to (Koo et al, 2008) for the list of cluster-based feature templates. Our first word representation is exactly the same as the one used in (Koo et al, 2008) where words are clustered using the Brown algorithm (Brown et al, 1992). In our experiments we use the clusters obtained in (Koo et al, 2008), but were unable to match the accuracy reported there, perhaps due to additional features used in their implementation not described in the paper. Terry Koo was kind enough to share the source code for the (Koo et al, 2008) paper with us, and we plan to incorporate all the features in our future work. Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with an other semi-supervised approach, described in (Koo et al, 2008). In particular, Koo et al (2008) describe a semi-supervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech. The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al, 2008). Our experiments investigate the effectiveness of: 1) the basics S-SCM for dependency parsing; 2) a combination of the SS-SCM with Koo et al (2008)'s semi supervised approach (even in the case we used the same unlabeled data for both methods); 3) the two stage semi-supervised learning approach that in 551corporates a second-order parsing model. We simply use the cluster based feature-vector representation f (x, y) introduced by (Koo et al, 2008) as the basis of our approach. These data sets are identical to the unlabeled data used in (Koo et al, 2008), and are disjoint from the training, development and test sets.
Distributional Clustering Of English Words We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical &quot;soft&quot; clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data. Class based methods (Pereira et al 1993) approximate the likelihood of unobserved words based on similar words.  Distributional clustering of words was first proposed by Pereira Tishby and Leein (Pereira et al, 1993): They cluster nouns according to their conditional verb distributions. The cluster output can then be used as classes for selectional preferences (Pereira et al, 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994). This indicates that techniques for differentiating between different senses are needed e.g., using a soft clustering technique as in (Pereira et al 1993) instead of a hard clustering technique. Hatzivassiloglou and MeKeown (1993) clustered adjectives into semantic classes, and Pereira et al (1993) clustered nouns on their appearance in verb-object pairs. Of course, this is one of many different possible similarity measures which could have been used (Pereira et al (1993)), including ones that do not depend on additional labels. So far we have used a weighted string edit distance matcher and experimented with di erent substitution weights including ones based on measures of statistical similarity between words such as the one described by Pereira et al (1993). Distributional clustering (Dcluster) (Pereira et al., 1993) measures similarity among words in terms of the similarity among their local contexts. In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate. While previous cognitively-motivated computational frameworks required structured input (e.g. Falkenhainer et al, 1989), the CP method adapts distributional clustering (Pereira et al, 1993), a standard approach applicable to unstructured data. There are a few exceptions to this tradition, such as Pereira et al (1993), Rooth et al (1999), Korhonen et al (2003), who used soft clustering methods for multiple assignment to verb semantic classes. Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al, 1993). Our approach differs from standard word clustering in that the clustering criteria is directly linked to the re ranking objective, whereas previous word clustering approaches (e.g. Brown et al (1992) or Pereira et al (1993)) have typically leveraged distributional similarity. The importance of distributional features is well known for named entity recognition and part of speech tagging (Pereira et al, 1993). For example, Pereira et al (1993) begin with a co occurrence matrix and transform this matrix into a clustering. In a similar vein, our model is both similar and distinct in comparison to the soft clustering approaches by Pereira et al (1993) and Korhonenetal. Pereira et al (1993) suggested deterministic annealing to cluster verb-argument pairs into classes of verbs and nouns.  Pereira et al (1993) used clustering to build an unlabeled hierarchy of nouns.
SemEval-2010 Task 13: TempEval-2 Tempeval-2 comprises evaluation tasks for time expressions, events and temporal relations, the latter of which was split up in four sub tasks, motivated by the notion that smaller subtasks would make both data preparation and temporal relation extraction easier. Manually annotated data were Besides the increasing availability of an notation standards (e.g., TIMEML (Pustejovsky et al., 2003a)) and corpora (e.g., TIDES (Ferro et al., 2000), TimeBank (Pustejovsky et al, 2003b)), the community has also organized three successful evaluation workshops TempEval1 (Verhagen et al, 2009), -2 (Verhagen et al, 2010), and-3 (Uzzaman et al, 2013). Following the "divide-and-conquer" approach described in Verhagen et al (2010), results from the three temporal processing steps: 1) timex normalization, 2) event-timex temporal relationship classification, and 3) event-event temporal relationship classification, are merged to obtain time lines (top half of Figure 3). We evaluate our model on all six languages in the TempEval2 Task A dataset (Verhagen et al, 2010), comparing against state-of-the-art systems for English and Spanish. Part of our task is similar to task C of TempEval-2 (Verhagen et al 2010), determining the temporal relation between an event and a time expression in the same sentence. We also evaluated our system on TempEval 2 (Verhagen et al 2010) for better comparison to the state-of-the-art. We evaluate our model against current state-of-the art systems for temporal resolution on the English portion of the TempEval-2 Task A dataset (Verhagen et al, 2010). This task is based on task A in the TempEval-2 challenge (Verhagen et al, 2010). There has been much work addressing the problems of temporal expression extraction and normalization, i.e. the systems developed in TempEval-2 challenge (Verhagen et al, 2010). In recent years a renewed interest in temporal processing has spread in the NLP community, thanks to the success of the TimeML annotation scheme (Pustejovsky et al, 2003a) and to the availability of annotated resources, such as the English and French TimeBanks (Pustejovsky et al., 2003b; Bittar, 2010) and the TempEval corpora (Verhagen et al, 2010). TempEval (Verhagen et al 2007), in 2007, and more recently TempEval-2 (Verhagen et al 2010), in 2010, were concerned with this problem. The results of TempEval-2 are fairly similar (Verhagen et al 2010), but the data used are similar but not identical.
Word Sense And Subjectivity Subjectivity and meaning are both important properties of language. This paper explores their interaction, and brings empirical evidence in support of the hypotheses that (1) subjectivity is a property that can be associated with word senses, and (2) word sense disambiguation can directly benefit from subjectivity annotations. In recent years, Akkaya et al (2009) report a successful empirical result where WSD helps improving sentiment analysis, while Wiebe and Mihalcea (2006) study the distinction between objectivity and subjectivity in each different sense of a word, and their empirical effects in the context of sentiment analysis.  As an example, Wiebe and Mihalcea (2006) prove that subjectivity information for WordNet senses can improve word sense disambiguation tasks for subjectivity ambiguous words (such as positive). Wiebe and Mihalcea (2006) label word senses in WordNet as subjective or objective. We follow Wiebe and Mihalcea (2006) in that we see subjective expressions as private states that are not open to objective observation or verification. First, (Wiebe and Mihalcea, 2006) provide evidence that word sense labels, together with contextual subjectivity analysis, can be exploited to improve performance in word sense disambiguation. We adopt the definitions of subjective and objective from Wiebe and Mihalcea (2006) (hereafter WM).  We adopt the definitions from (Wiebe and Mihalcea, 2006), who describe the annotation scheme as follows. As noted in (Wiebe and Mihalcea, 2006), sentences containing objective senses may not be objective.  Both (Wiebe and Mihalcea, 2006) and (SuandMarkert, 2008) show that even reliable subjectivity clues have objective senses. They have shown that subjectivity annotations can be helpful for word sense disambiguation when a word has distinct subjective senses and objective senses (Wiebe and Mihalcea, 2006). Wiebe and Mihalcea (2006) and Su and Markert (2008) both show that this is a well-defined concept via human annotation as well as automatic recognition.   Towards this, Wiebe and Mihalcea (2006) conduct a study on human annotation of 354 words senses with polarity and report a high inter-annotator agreement. (Wiebe and Mihalcea, 2006) define subjective expressions as words and phrases being used to express mental and emotional states, such as speculations, evaluations, sentiments, and beliefs. However, such clues often have both subjective and objective senses, as illustrated by (Wiebe and Mihalcea, 2006). 
An Algorithm For Generating Quantifier Scopings The syntactic structure of a sentence often manifests quite clearly the predicate-argument structure and relations of grammatical subordination. But scope dependencies are not so transparent. As a result, many systems for representing the semantics of sentences have ignored scoping or generated scopings with mechanisms that have often been inexplicit as to the range of scopings they choose among or profligate in the scopings they allow. This paper presents, along with proofs of some of its important properties, an algorithm that generates scoped semantic forms from unscoped expressions encoding predicate-argument structure. The algorithm is not profligate as are those based on permutation of quantifiers, and it can provide a solid foundation for computational solutions where completeness is sacrificed for efficiency and heuristic efficacy. Adding appropriate side conditions to the rules, following the constraints discussed by Hobbs and Shieber (Hobbs and Shieber, 1987) would not be difficult. Hobbs and Shieber (1987) extend this formalism to support operators (such as not) and present an enumeration algorithm that is more efficient than the naive wrapping approach. The sentence has 42 readings (Hobbs and Shieber, 1987), and it is easy to imagine how the number of readings grows exponentially (or worse) in the length of the sentence. In summary, however, the analysis is slightly more restrictive than that of Hobbs and Shieber (1987), making predictions regarding the scope of topicalized or wh-moved constituents, relative scope of embedded quantifiers, and possibly even syntactic structure of complex NPs. For example, in processing (22) (adapted from Hobbs and Shieber 1987), which Park 1995 claims to have only four readings, rather than the five predicted by their account, such a system can build both readings for the S/NP every representative of three companies saw and decide which is more likely, before building both compatible readings of the whole sentence and similarly resolving with respect to statistical or contextual support: (22) Every representative of three companies aw some sample. [Hobbs and Shieber 1987] presented an algorithm to generate quantifier scopings from a representation of predicate-argument relations and the relations of grammatical subordination. An algorithm for generating all possible quantifier scopings was detailed by Hobbs and Shieber (1987).
Sentiment Analysis Using Support Vector Machines With Diverse Information Sources This paper introduces an approach to sentiment analysis which uses support vector machines (SVMs) to bring together diverse sources of potentially pertinent information, including several favorability measures for phrases and adjectives and, where available, knowledge of the topic of the text. Models using the features introduced are further combined with unigram models which have been shown to be effective in the past (Pang et al., 2002) and lemmatized versions of the unigram models. Experiments on movie review data from Epinions.com demonstrate that hybrid SVMs which combine unigram-style feature-based SVMs with those based on real-valued favorability measures obtain superior performance, producing the best results yet published using this data. Further experiments using a feature set enriched with topic information on a smaller dataset of music reviews handannotated for topic are also reported, the results of which suggest that incorporating topic information into such models may also yield improvement. For instance, instead of representing the polarity of a term using a binary value, Mullen and Collier (2004) use Turney's (2002) method to assign a real value to represent term polarity and introduce a variety of numerical features that are aggregate measures of the polarity values of terms selected from the document under consideration. Mullen and Collier (2004) used SVMs and expanded the feature set for representing documents with favor ability measures from a variety of diverse sources.  Mullen and Collier (Mullen and Collier, 2004) integrated PMI values, Osgood semantic factors and some syntactic relations into the features of SVM.  Another method is to use proximal information of the query and the word, using syntactic structure such as dependency relations of words that provide the graphical representation of the text (Mullen and Collier, 2004). Mullen and Collier (2004) manually annotated named entities in their dataset (i.e. title of the record and name of the artist for music record reviews), and utilized presence and position features in their ML approach. Mullen and Collier (2004), for example, uses WordNet to add information about words found within text, and consequently reports improved classification performance in a sentiment analysis task.
Machine Transliteration It is challenging to translate names and technical terms across languages with different alphabets and sound inventories. These items are commonly transliterated, i.e., replaced with approximate phonetic equivalents. example, English comes out :/ — in Japanese. Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries. We describe and evaluate a method for performing backwards transliterations by machine. This method uses a generative model, incorporating several distinct stages in the transliteration process. Extending this notion, (Knight and Graehl, 1997) built five probability distributions:. Fortunately, the first two models of (Knight and Graehl, 1997) deal with English only, so we can reuse them directly for Arabic/English transliteration. We applied the EM learning algorithm described in (Knight and Graehl, 1997) on this data, with one variation. Transliterations can be generated for tokens in a source phrase (Knightand Graehl, 1997), with o (f, e) calculating phonetic similarity rather than orthographic. Transliteration of a name, for the purpose of this work, is defined as its transcription in a different language, preserving the phonetics, perhaps in a different orthography [Knight and Graehl, 1997]. In this work we treat the process of transliteration as a process of direct transduction from sequences of tokens in the source language to sequences of tokens in the target language with no modeling of the phonetics of either source or target language (Knight and Graehl, 1997). Knight and Graehl (1997) build a generative model for back ward transliteration from Japanese to English. (Knight and Graehl, 1997) proposed a generative transliteration model to transliterate foreign names in Japanese back to English using finite state transducers. For example, in order to translate names and technical terms, (Knight and Graehl, 1997) introduced a probabilistic model that replaces Japanese katakana words with phonetically equivalent English words. Previous works usually take a generative approach, (Knight and Graehl, 1997). However, back transliteration is known to be a very difficult task (Knight and Graehl, 1997). There are several studies on transliteration (e.g., (Knight and Graehl, 1997)), and they tell us that machine transliteration of language pairs that employ very different alphabets and sound systems is extremely difficult, and that the technology is still to immature for use in practical processing. (Knight and Graehl, 1997) build a generative model for backward transliteration from Japanese to English. (Knight and Graehl, 1997) and extended Markov window (Jung et al, 2000) treat transliteration as a phonetic process rather than an orthographic process. Back-transliteration is a difficult problem as exemplified in (Knight and Graehl 1997, Chen, et.al. 1998). Finite state transducers that implement transformation rules for back-transliteration from Japanese to English have been described by Knight and Graehl (1997), and extended to Arabic by Glover-Stalls and Knight (1998). Using machine transliteration can resolve part of UNK translation (Knight and Graehl, 1997). Other proposed methods include paraphrasing (Callison-Burch et al, 2006) and transliteration (Knight and Graehl, 1997) that uses the feature of phonetic similarity. One usually distinguishes between two types of transliteration (Knight and Graehl, 1997): Forward transliteration, where an originally Hebrewterm is to be transliterated to English; and Backward transliteration, in which a foreign term that has already been transliterated into Hebrew is to be recovered. Knight and Graehl (1997) have proposed to compose a set of weighted finite state transducers to solve the much more complicated problem of back-transliteration from Japanese Katakana to English.
Distant supervision for relation extraction without labeled data Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACEstyle algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression. Additionally, researchers have tried to automatically extract examples for supervised learning from resources such as Wikipedia (Weld et al,2008) and databases (Mintz et al, 2009), or attempted open information extraction (IE) (Banko et al, 2007) to extract all possible relations. It is a modification of the model proposed by Mintz et al (2009). However, Riedel et al's model (like that of previous systems (Mintz et al, 2009)) assumes that relations do not overlap there can not exist two facts r (e1, e2) and q (e1, e2) that are both true for any pair of entities, e1 and e2. We will make use of the Mintz et al (2009) sentence-level features in the expeiments, as described in Section 7. Additionally, their aggregate decisions make use of Mintzstyle aggregate features (Mintz et al, 2009), that collect evidence from multiple sentences, while we use 543 Inputs: (1)?, a set of sentences, (2) E, a set of entities mentioned in the sentences, (3) R, a set of relation names, and (4)?, a database of atomic facts of the form r (e1 ,e2) for r? R and ei? E. We use the set of sentence-level features described by Riedel et al (2010), which were originally developed by Mintz et al (2009). Mintz et al (2009) used Freebase facts to train 100 relational extractors on Wikipedia. Mintz et al (2009) use distant supervision to learn to extract relations that are represented in Freebase (Bollacker et al, 2008). This approach only has access to the text in a document and contains all the features mentioned in Wu and Weld (2010) and Mintz et al (2009). To address this limitation, a promising approach is distant supervision (DS), which can automatically gather labeled data by heuristically aligning entities in text with those in a knowledge base (Mintz et al, 2009). Craven and Kumlien (1999), Wu et al (2007) and Mintz et al (2009) were several pioneer work of distant supervision. This is a traditional DS assumption based model proposed by Mintz et al (2009). distant supervision: for each relation in the database D we assume that all the corresponding mentions are positive examples for the corresponding label (Mintz et al 2009). As discussed in Section 4.3, this model follows the "traditional" distant supervision heuristic, similarly to (Mintz et al., 2009). Contrasted to the alternative approach where annotations are document-level only, this approach has a number of important benefits, such as allowing machine learning methods for event extraction to be directly trained on fully and specifically annotated data without the need to apply frequently error prone heuristics (Mintz et al, 2009) or develop machine learning methods addressing the mapping between text expressions and document-level annotations (Riedel et al, 2010). MULTIR uses features which are based on Mintz et al (2009) and consist of conjunctions of named entity tags, syntactic dependency paths between arguments, and lexical information. Wieg and and Klakow (2011a) present an intermediate solution for opinion holder extraction inspired by distant supervision (Mintz et al 2009). A particularly attractive approach, called distant supervision (DS), creates labeled data by heuristically aligning entities in text with those in a knowledge base, such as Freebase (Mintz et al, 2009). We applied our method to Wikipedia articles using Freebase as a knowledge base and found that (i) our model identified patterns expressing a given relation more accurately than baseline methods and (ii) our method led to better extraction performance than the original DS (Mintz et al, 2009) and MultiR (Hoffmann et al., 2011), which is a state-of-the-art multi instance learning system for relation extraction (see Section 7). Our work was inspired by Mintz et al (2009) who used Freebase as a knowledge base by making the DS assumption and trained relation ex tractors on Wikipedia.
CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word–word dependencies. The resulting corpus, CCGbank, includes 99.4% of the sentences in the Penn Treebank. It is available from the Linguistic Data Consortium, and has been used to train widecoverage statistical parsers that obtain state-of-the-art rates of dependency recovery. In order to obtain linguistically adequate CCG analyses, and to eliminate noise and inconsistencies in the original annotation, an extensive analysis of the constructions and annotations in the Penn Treebank was called for, and a substantial number of changes to the Treebank were necessary. We discuss the implications of our findings for the extraction of other linguistically expressive grammars from the Treebank, and for the design offuture treebanks. For the identification and labeling steps, we train a maximum entropy classifier (Berger et al, 1996) over sections 02-21 of a version of the CCGbank corpus (Hockenmaier and Steedman, 2007) that has been augmented by projecting the Propbank semantic annotations (Boxwell and White, 2008). We used the CCGbank-style dependency output of the parser (Hockenmaier and Steedman,2007), which is a directed graph of head-child relations labelled with the head's lexical category and the argument slot filled by the child. CCGbank (Hockenmaier and Steedman, 2007) is a corpus of CCG derivations that was semiautomatically converted from the Wall Street Journal section of the Penn treebank. For example, declarative sentences are S [dcl], wh-questions are S [wq] and sentence fragments are S [frg] (Hockenmaier and Steedman, 2007). Bos et al (2009) created a CCGbank from an Italian dependency tree bank by converting dependency trees into phrase structure trees and then applying an algorithm similar to Hockenmaier and Steedman (2007). The resource used for building wide-coverage CCG parsers of English is CCGbank (Hockenmaier and Steedman, 2007), a version of the Penn Treebank in which each phrase-structure tree has been transformed into a normal-form CCG derivation. Our experiments were performed using CCGBank (Hockenmaier and Steedman, 2007), which was split into three subsets for training (Sections 02-21), development testing (Section 00) and the final test (Section 23). CCGbank was derived from the PTB, and so it might be considered that converting back to the PTB would be a relatively easy task, by essentially reversing the mapping Hockenmaier and Steedman (2007) used to create CCGbank. In our experience, trying to add number and animacy agreement constraints to a grammar induced from the CCGbank (Hockenmaier and Steedman, 2007) turned out to be surprisingly difficult, as hard constraints often ended up breaking examples that were working without such constraints, due to exceptions, sub-regularities and acceptable variation in the data. For example, CCGbank (Hockenmaier and Steedman, 2007) contains 1241 distinct supertags (lexical categories) and the most ambiguous word has 126 supertags. CCGbank was created by semiautomatically converting the Penn Treebank to CCG derivations (Hockenmaier and Steedman, 2007). By automatically transforming the constituent structure trees annotated in PTB to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (Hockenmaier and Steedman, 2007), many syntactic parser other than the CFG formalism were also developed. We agree; however, our ultimate motivation is to use this work to tackle bootstrapping from very small tag dictionaries or dictionaries obtained from linguists or resources other than a corpus, and for tag sets that are more ambiguous (e.g., super tagging for CCGbank (Hockenmaier and Steedman, 2007)). This logical form can be expressed in many ways; we will focus on the dependency representation used in CCGbank (Hockenmaier and Steedman, 2007). All experiments were conducted on CCGBank (Hockenmaier and Steedman, 2007), a right-most normal-form CCG version of the Penn Treebank. The grammar is automatically extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) with Propbank (Palmer et al, 2005) roles projected onto it (Boxwell and White, 2008). Hockenmaier and Steedman (2007) showed that a CCG corpus could be created by adapting the Penn Treebank (Marcus et al, 1993). We use CCGBank (Hockenmaier and Steedman, 2007) for experimental data. A well known work is transforming Penn Treebank into resources for various deep linguistic processing, including LTAG (Xia, 1999), CCG (Hockenmaier and Steedman, 2007), HPSG (Miyao et al, 2004) and LFG (Cahill et al, 2002). CCGbank (Hockenmaier and Steedman, 2007) extends this grammar with a set of type-changing rules, designed to strike a better balance between sparsity in the category set and ambiguity in the grammar.
Introduction To The Special Issue On The Web As Corpus  Early web-based models used search engines to collect N-gram counts, and thus could not use capitalization, punctuation, and annotations such as part-of-speech (Kilgarriff and Grefenstette, 2003). In corpus linguistics building such mega corpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003). The Web contains vast amounts of linguistic data for many languages (Kilgarriff and Grefenstette, 2003). In recent years, the Web has been increasingly used as a source of linguistic data (Kilgarriff and Grefenstette, 2003). It is now easy to gather machine-readable sentences in various domains because of the ease of publication and access via the Web (Kilgarriff and Grefenstette, 2003).  The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). For Hungarian, the highest quality (4 % threshold) stratum of the corpus contains 1.22m unique pages for a total of 699m tokens, already exceeding the 500m predicted in (Kilgarriff and Grefenstette, 2003). For collecting bilingual text data for the two sets S1, S2, the Web is an ideal source as it is large, free and available (Kilgarriff and Grefenstette, 2003). It is natural to question the appropriateness of web data for research purposes, because web data is inevitably noisy and search engines themselves can introduce certain idiosyncracies which can distort results (Kilgarriff and Grefenstette, 2003). The second corpus is a subset of the German part of the Wacky project (Kilgarriff and Grefenstette, 2003). So we have the Internet: it is immense, free, easily accessible and can be used for all manner of language research (Kilgarriff and Grefenstette, 2003). Many NLP tasks have successfully utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003).
An Empirical Approach To Conceptual Case Frame Acquisition TIME 52 process eliminates the need for a human to assign roles to the extraction patterns by hand, as had been necessary when using AutoSlog or AutoSlog-TS by themselves. For example, the pattern &quot;machinegunned <direct-obj>&quot; had strong semantic preferences for CIVILIAN, LOCATION, so was expanded to have three conceptual roles with four selectional restrictions. The expanded extraction pattern for &quot;machinegunned <direct-obj>&quot; is: &quot;machinegunned <direct-obj>&quot; -+ VEHICLE Only semantic categories that were associated with a pattern are included as selectional restric- For example, the also represents possible terrorism victims, but it was not strongly associated with the pattern. Our rationale is that an individual pattern may have a strong preference for only a subset of the categories that can be associated with a role. For example, the pattern &quot;<subject> was ambushed&quot; showed a preference for but not which makes sense because it is hard to imagine ama building. Including only as selectional restriction for targets might help eliminate incorrect building extractions. One could argue that this pattern is not likely to find building extractions anyway so the selectional restriction will not matter, but the selectional restriction might help filter out incorrect extractions due to misparses or (e.g., &quot;The White House by reporters.&quot;). Ultimately, it is an empirical question whether it is better to include all of the semantic categories associated with a conceptual role or not. Finally, we merge the expanded extraction patterns into multi-slot case frames. All extraction patterns that share the same trigger word and compatible syntactic constraints are merged into a single structure. For example, we would merge all patterns triggered by a specific verb in its passive voice. For example, the patterns &quot;<subject> was kidnapped&quot;, &quot;was kidnapped by <noun-phrase>&quot;, and &quot;was kidnapped in <noun-phrase>&quot; would be merged into a single case frame. Similarly, we would merge all patterns triggered by a specific verb in its active voice. For example, we would merge patterns for the active form of &quot;destroyed&quot; that extract the subject of &quot;destroyed&quot;, its direct object, and any prepositional phrases that are associated with it. We also merge syntactically compatible patterns that are triggered by the same noun (e.g., &quot;assassination&quot;) or by the same infinitive verb structure (e.g., &quot;to kill&quot;). When merge extraction patterns into a case frame, of the slots are simply unioned together. 4 Examples In this section, we show several examples of case frames that were generated automatically by our system. Figure 5 shows a simple case frame triggered by active forms of the verb &quot;ambushed&quot;. The subject extracted as a has a selectional of direct object is exas a has a selectional restriction that the case frame does not contain even though it is theoretically possible to ambush people. During training, the &quot;ambushed <direct-obj>&quot; pattern extracted 13 people, 11 of were recognized as Since our domain roles only list civilians and government as legitimate terrorism a victim slot was not created. This example shows how the case frames are tailored for the domain empirically. Caseframe: (active_verb ambushed) VEHICLE Figure 5: Case frame for active forms of &quot;ambushed&quot; Figure 6 shows a case frame triggered by active of &quot;blew_up&quot; This case frame extracts information from an entire sentence into a single struc- The subject object (tara prepositional phrase location) all be extracted together. Caseframe: (active_verb blew_up) subject VEHICLE Figure 6: Case frame for active forms of &quot;blew_up&quot; The case frame in Figure 7 illustrates how a semantic category can show up in multiple places. This case frame will handle phrases like &quot;the guerrillas detonated a bomb&quot;, as well as &quot;the bomb detonated&quot;. Both constructions are very common in the training corpus so the system added slots for both possibilities. It would be easy for a human to overlook some of these variations when creating case frames by hand. The case frame in Figure 8 is activated by the noun &quot;attack&quot; and includes slots for a variety of prepositional phrases. The same preposition can recognize different types of information (e.g., &quot;on&quot; can victims, locations, the same role can be filled by different prepositions military victims were classified as military incidents, not terrorism, according to the MUC-4 guidelines. represent lexicalized expressions in our phrasal lexicon. 53 Caseframe: (active_verb detonated) subject instrument subject WEAPON Figure 7: Case frame for active forms of &quot;detonated&quot; be extracted from &quot;on&quot;, &quot;against&quot;, or &quot;at&quot;). This example again shows the power of corpus-based methods to identify common constructions empirically. Anticipating all of these prepositional arguments would be difficult for a person. Caseframe: (noun attack) VEHICLE CIVILIAN GOVOFFICIAL BUILDING CIVILIAN locationpp(at) Figure 8: Case frame for noun forms of &quot;attack&quot; A disadvantage of this automated method is that inappropriate slots sometimes end up in the case frames. For example, Figure 9 shows a case frame that is activated by passive forms of the verb &quot;killed&quot;. Some of the slots are correct: the subis assigned to the and objects of the preposition &quot;by&quot; are assigned to the perpetrator and However, the remaining slots do sense. The is the result of polysemy; many person names are also location names, as &quot;Flores&quot;. The was produced by inparses of date expressions. The and (by)) slots were caused by incorrect role assignments. The list of domain roles assumes that terrorists are always perpetrators and civilians are always victims, but of course this is not true. Terrorists can be killed and civilians can be killers. killed) subject pp(by) pp(by) Figure 9: Case frame for passive forms of &quot;killed&quot; The previous example illustrates some of the problems that can occur when generating case frames automatically. Currently, we are assuming that each semantic category will be uniquely associated with a conceptual role, which may be an unrealistic assumption for some domains. One avenue for future work is to develop more sophisticated methods for mapping semantic preferences to conceptual roles. One could also have a human review the case frames and manually remove inappropriate slots. For now, we chose to avoid additional human interaction and used the case frames exactly as they were generated. The purpose of the selectional restrictions is to constrain the types of information that can be instantiated by each slot. Consequently, we hoped that the case frames would be more reliably instantiated than the extraction patterns, thereby producing fewer false hits. To evaluate the case frames, we used the same corpus and evaluation metrics as previous experiments with AutoSlog and AutoSlog- TS (Riloff, 1996b) so that we can draw comparisons between them. For training, we used the 1500 MUC- 4 development texts to generate the extraction patterns and the semantic lexicon. AutoSlog-TS generated 44,013 extraction patterns in its first pass. After discarding the patterns that occurred only once, the remaining 11,517 patterns were applied to the corpus for the second pass and ranked for manual We reviewed the top 2168 and kept 306 extraction patterns for the final dictionary. We built a semantic lexicon for nine categories aswith terrorism: CIVILIAN, GOV- OFFICIAL, MILITARYPEOPLE, LOCATION, TERROR- DATE, VEHICLE, WEAPON. reviewed the top 500 words for each category. It takes about 30 minutes to review a category assuming that the reviewer is familiar with the domain. Our final semantic dictionary contained 494 words. In total, the review process required approximately 6 person-hours: 1.5 hours to review the extraction patterns plus 4.5 hours to review the words for 9 semantic categories. From the extraction patterns and semantic lexicon, our system generated 137 conceptual case frames. important question is how to deal with unknown words during extraction. This is especially important in the terrorism domain because many of extracted items are proper names, which cannot be expected to be in the semantic lexicon. We allowed unknown words to fill all eligible slots and then used a precedence scheme so that each item was instantiated by only one slot. Precedence was based on the order of the roles shown in Figure 4. This is not a very satisfying solution and one of the weaknesses of our current approach. Handling unknown words more intelligently is an important direction for future research. We compared AutoSlog-TS' extraction patterns decided to review the top but continued down the list until there were no more ties. 54 Slot cor mis mlb dup spu R P Perp 25 31 10 18 84 .45 .31 Victim 44 23 16 24 62 .66 .47 Target 31 22 17 23 66 .58 .39 Instr 16 15 7 17 23 .52 .52 Total 116 91 50 82 235 .56 .41 Table 1: AutoSlog-TS results the case frames using 100 blind from the MUC-4 test set. The MUC-4 answer keys were used to score the output. Each extracted item was scored either mislabeled, duplicate, spurious. item was it matched against the answer An item was it matched against the answer keys but was extracted as the wrong type of object (e.g., if a victim was extracted as a perpe- An item it was coreferent with an item in the answer keys. Correct items extracted more than once were scored as duplicates, as well as correct but underspecified extractions such as instead of &quot;John F. An item it did not appear in the answer keys. All items extracted from irrelevant texts were spurious. Finally, items in the answer keys that were not were counted as Correct + missthe total number of items in the answer 1 shows the for AutoSlog-TS' extraction patterns, and Table 2 shows the results for case frames. We computed (R) cor- I (correct + missing), (P) (correct duplicate) I (correct + duplicate + misla- + spurious). extraction patterns and case frames achieved similar recall results, although the case frames missed seven correct extractions. However the case frames produced substantially fewer false hits, producing 82 fewer spurious extractions. Note that perpetrators exhibited by far the lowest precision. The reason is that the perpetrator slot received highest precedence among competing slots for unknown words. Changing the precedence relevant texts and 25 irrelevant texts from each of the TST3 and TST4 test sets. rationale for scoring coreferent phrases as duplicates instead of spurious is that the extraction pattern or case frame was instantiated with a reference to the correct answer. In other words, the pattern (or case frame) did the right thing. Resolving coreferent phrases to produce the best answer is a problem for subsequent discourse analysis, which is not addressed by the work presented here. caveat is that the MUC-4 answer keys contain some &quot;optional&quot; answers. We scored these as correct if they were extracted but they were never scored as missing, which is how the &quot;optional&quot; items were scored in MUC-4. Note that the number of possible extractions can vary depending on the output of the system. reimplemented AutoSlog-TS to use a different sentence analyzer, so these results are slightly different from those reported in (Riloff, 19966). Slot cor mis mlb dup spu R P Perp 26 30 4 17 71 .46 .36 Victim 38 28 24 12 26 .58 .50 Target 28 25 3 29 48 .53 .53 Instr 17 14 2 19 8 .55 .78 Total 109 97 33 77 153 .53 .50 Table 2: Case frame results scheme produces a bubble effect where many incorrect extractions shift to the primary default category. The case frames therefore have the potential for even higher precision if the unknown words are handled better. Expanding the semantic lexicon is one option, and additional work may suggest ways to choose slots for unknown words more intelligently. 6 Conclusions We have shown that conceptual case frames can be generated automatically using unannotated text as input, coupled with a few hours of manual review. Our results for the terrorism domain show that the case frames achieve similar recall levels as the extraction patterns, but with substantially fewer false hits. Our results are not directly comparable to the MUC-4 results because the MUC-4 systems contained additional components, such as domainspecific discourse analyzers that resolved coreferent noun phrases, merged event descriptions, and filtered out irrelevant information. The work presented here only addresses the initial stage of information extraction. However, in previous work we showed that AutoSlog-TS achieved performance comparable to AutoSlog (Riloff, 1996b), which performed very well in the MUC-4 evaluation (Lehnert et al., 1992b). Since the conceptual case frames achieved comparable recall and higher precision than AutoSlog-TS' extraction patterns, our results suggest that the case frames performed well relative to previous work on this domain. Several other systems learn extraction patterns that can also be viewed as conceptual case frames with selectional restrictions (e.g., PALKA (Kim and Moldovan, 1993) and CRYSTAL (Soderland et al., 1995)). The case frames learned by our system are not necessarily more powerful then those generated by other systems. The advantage of our approach is that it requires no special training resources. Our technique requires only preclassified training texts and a few hours of manual filtering to build the intermediate dictionaries. Given preclassified texts, it is possible to build a dictionary of conceptual case frames for a new domain in one day. Another advantage of our approach is its highly empirical nature; a corpus often reveals important patterns in a domain that are not necessarily intuitive to people. By using corpus-based methods to generate all of the intermediate dictionaries and 55 the final case frame structures, the most important words, role assignments, and semantic preferences are less likely to be missed. Our empirical approach aims to exploit the text corpus to automatically acquire the syntactic and semantic role assignments that are necessary to achieve good performance in the domain. In the Conceptual Case Frame Acquisition project (RiloffandSchmelzenbach, 1998), extraction patterns, a domain semantic lexicon, and a list of conceptual roles and associated semantic categories for the domain are used to produce multiple-slot case frames with selectional restrictions. In related unsupervised tasks, Riloff and colleagues have learned case frames for verbs (e.g., Riloff and Schmelzenbach, 1998), while Gildea (2002) has learned role slot mappings (but does not apply the knowledge for the labelling task). There are number of different existing approaches for identifying semantic roles, varying from traditional parsing approaches, for example using HPSG grammars and Lexical Functional Grammars, that strongly rely on manually developed grammars and lexicons, to data-driven approaches, for example AutoSlog (Riloff and Schmelzenbach, 1998). Applying such a model to information extraction, in AutoSlog Riloff (1993) builds a list of patterns for filling in semantic slots in a specific domain, as well as a method for automatic acquisition of case frames (Riloff and Schmelzenbach, 1998).
Collective Information Extraction With Relational Markov Networks Most information extraction (IE) systems treat separate potential extractions as independent. However, in many cases, considering influences potential extractions could improve overall accuracy. Statistical methods on models, such as random fields have been shown to be an effective approach to learning accurate IE systems. We present a new IE method that employs Relational Markov Networks (a generalization of CRFs), which can represent arbitrary dependencies between extractions. This allows for &quot;collective information extraction&quot; that exploits the mutual influence between possible extractions. Experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach. We also provide the results from Bunescu and Mooney (2004) for comparison. In contrast, the increases published by Bunescu and Mooney (2004) are relative to a baseline system which scores only 80.9% on the same task. The most relevant prior works are Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al, 2002) to explicitly models long-distance dependencies, and Sutton and McCallum (2004), who introduce skip-chain CRFs, which maintain the underlying CRF sequence model (which (Bunescu and Mooney, 2004) lack) while adding skip edges between distant nodes. Bunescu and Mooney (2004) define a Relational Markov Network (RMN) which explicitly models long-distance dependencies, and use it to represent relations between entities. The simplicity of our approach makes it easy to incorporate dependencies across the whole corpus, which would be relatively much harder to incorporate in approaches like (Bunescu and Mooney, 2004) and (Finkel et al, 2005).  We also compare our performance against (Bunescu and Mooney, 2004) and (Finkel et al, 2005) and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF. Recent work looking to directly model non-local dependencies and do approximate inference are that of Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al., 2002) to explicitly model long-distance dependencies, Sutton and McCallum (2004), who introduce skip-chain CRFs, which add additional non-local edges to the underlying CRF sequence model (which Bunescu and Mooney (2004) lack) and Finkel et al (2005) who hand-set penalties for inconsistency in labels based on the training data and then use Gibbs Sampling for doing approximate inference where the goal is to obtain the label sequence that maximizes the product of the CRF objective function and their penalty. In the following experiments, we used AImed (Bunescu and Mooney, 2004), which is a popular corpus for the evaluation of PPI extraction systems. The corpus consists of 225 biomedical paper abstracts (1970 sentences), which are sentence-split, tokenized, and annotated with proteins and PPIs.We use gold protein annotations given in the corpus. Fortunately, research in machine learning has produced methods for global inference and joint classification that can help to address this deficiency (e.g. Bunescu and Mooney (2004), Roth and Yih (2004)). In the first approach, heuristic rules are used to find the dependencies (Bunescu and Mooney, 2004) or penalties for label inconsistency are required to handset ad-hoc (Finkel et al, 2005). However, Table 1 shows that the word distance is long between interacting protein names annotated on the AImed corpus (Bunescu and Mooney, 2004), and we have to treat long-distance relations for information like protein-protein interactions. See Bunescu and Mooney (2004) and Loeliger (2004) for a detailed introduction to factor graphs.  Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). Supervised approaches (McCallum and Li 2003, Bunescu and Mooney 2004) rely on large sets of labeled examples, perform targeted extraction and employ a variety of sentence and corpus-level features. Examples of classifier-based IE systems are SRV (Freitag, 1998), HMM approaches (Freitag and McCallum, 2000), ALICE (Chieu et al, 2003), and Relational Markov Networks (Bunescu and Mooney, 2004).
]~{ECOGNI:ZING ]:F:XT GENII.ES Wl r l l  S:lb,/l:ll,I,; ~/~I,;II/I(~S USING DISCII .  Karlgren and Cutting (1994) use a combination of structural markers (e.g., noun count), lexical markers (e.g., it count), and token-level markers (e.g., words per sentence average ,type/token ratio, etc.). Work on automated genre classification was first carried out by Karlgren and Cutting (1994). Following Karlgren and Cutting (1994), I tested my approach on all three levels of granularity.  In one experiment in (Karlgren and Cutting, 1994) the sub genres under fiction are grouped together, leading to 10 gen res to classify. Results on 10-genre Brown Corpus. The result is also significantly better than prior work on the Browncor pus in (Karlgren and Cutting, 1994) (who use the whole corpus as test as well as training data).  In addition, we improve on the training accuracy of 52% reported in (Karlgren and Cutting, 1994). We are also interested in structural accuracy (S Acc) to see whether the structural SVMs make fewer& quot; big& quot; mistakes. Both works dispensed with the more complex features proposed by Karlgren and Cutting (1994) which showed promising results. The simplest kind of formality measure is based on word length, which is often used directly as an indicator of formality for applications such as genre classification (Karlgren and Cutting, 1994).
Structural Disambiguation With Constraint Propagation present a new grammatical formalism called Con- Dependency Grammar in which every rule is given constraint on wordto-word modifications. CDG parsing is formalized as a constraint satisfaction problem over a finite domain so that efficient constraint-propagation algorithms can be employed to reduce structural ambiguity without generating individual parse trees. The weak generative capacity and the computational complexity of CDG parsing are also discussed_  An architecture which fulfills this requirement is Weighted Constraint Dependency Grammar, which was based on a model originally proposed by Maruyama (1990) and later extended with weights (Schroeder, 2002). Originally, the constraints were comprised of a set of hand-written rules specifying which role values (unary constraints) and pairs of role values (binary constraints) were grammatical (Maruyama, 1990). Using constraint satisfaction techniques for natural language parsing was introduced first in (Maruyama, 1990) by defining a constraint dependency grammar (CDG) that maps nicely on the notion of a CSP.
A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model In this paper, we propose a novel string-todependency algorithm for statistical machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set. Both language-model and translation-model adaptation are implemented on top of a hierarchical Arabic-to-English translation system with string-to dependency rules as described in Shen et al (2008). In addition to the features described in Shen et al (2008), a new feature is added to the model for the bias rule weight, allowing the translation system to effectively tune the probability of the rules added by translation model adaptation in order to improve performance on the tuning set. The string-to-tree (Galleyet al 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as well as other formalisms such as Dependency Trees (Shen et al, 2008). Shen et al (2008) proposed a string-to-dependency model, which restricted the target-side of a rule by dependency structures. The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al (2008). (Shen et al, 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. (Shen et al, 2008) extends the hierarchical phrase-based model and present a string-to-dependency model, which employs string-to-dependency rules whose source side are string and the target as well-formed dependency structures. We take BBN's HierDec, a string-to-dependency decoder as described in (Shen et al, 2008), as our baseline for the following two reasons: It provides a strong baseline, which ensures the validity of the improvement we would obtain. In the original string-to-dependency model (Shen et al, 2008), a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side. Thus, we can compute the source dependency LM score in the same way we compute the target side score, using a procedure described in (Shen et al, 2008). When extracting rules with source dependency structures, we applied the same well-formedness constraint on the source side as we did on the target side, using a procedure described by (Shen et al, 2008). Following Shen et al (2008), we distinguish between fixed, floating, and ill-formed structures. Experiments show that our approach significantly outperforms both phrase-based (Koehn et al, 2007) and string-to dependency approaches (Shen et al, 2008) in terms of BLEU and TER. Following Shen et al (2008), string-to dependency rules without non-terminals can be extracted from the training example. It is easy to verify that the reduce left and reduce right actions are equivalent to the left adjoining and right adjoining operations defined by Shen et al (2008). Given a dependency tree of the target language, we are able to introduce language models that span over longer distances than the usual n-grams, as in (Shen et al, 2008). Shen et al (2008) use only phrases that meet certain restrictions. Our machine translation system is a string-to dependency hierarchical decoder based on (Shen et al., 2008) and (Chiang, 2007). Furthermore, we used a state of the art string-to-tree decoder (Shen et al, 2008) to establish the strongest possible baseline. To establish strong baselines, we used a string-to tree SMT system (Shen et al, 2008), one of the top performing systems in the NIST 2009 MT evaluation, and trained it with very large amounts of parallel and language model data.
Early Results For Named Entity Recognition With Conditional Random Fields Feature Induction And Web-Enhanced Lexicons  Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al, 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each. CRFs have been used successfully for Named Entity recognition (e.g., McCallum and Li (2003), Sarawagi and Cohen (2004)), and AutoSlog has performed well on information extraction tasks in several domains (Riloff, 1996a). NERC has been investigated using supervised (McCallum and Li, 2003), unsupervised (Etzioni et al, 2005) and semi-supervised (Pasca et al, 2006b) learning methods. CRFs have been shown to perform well in a number of natural language processing applications, such as POS tagging (Lafferty et al, 2001), shallow parsing or NP chunking (Sha and Pereira, 2003), and named entity recognition (McCallum and Li, 2003).  In recent years, conditional random fields (CRFs) (Lafferty et al, 2001) have shown success on a number of natural language processing (NLP) tasks, including shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003) and information extraction from research papers (Peng and McCallum, 2004). Standard statistical techniques for named entity recognition (NER) can be used for Step (2) (McCallum and Li, 2003). A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al 1997), Maximum Entropy methods (Borthwick et al 1998, Chieu and Ng 2002), Decision Trees (Sekine et al 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al 2002), Agent-based Approach (Ye et al 2002) and Support Vector Machines. We experimented with popular feature sets previously used for named entity (McCallum and Li, 2003) and gene (McDonald and Pereira, 2005) recognition including orthographic, part-of-speech (POS), shallow parsing and gazetteers. Part of the features we used for our CRF classifier are common features that are widely used in NER (McCallum and Li, 2003), as shown below. Conditional random fields (Lafferty et al, 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and named entity extraction (McCallum and Li, 2003). In recent years discriminative probabilistic model shave been successfully applied to a number of information extraction tasks in natural language processing (NLP), such as named entity recognition (NER) (McCallum and Li, 2003), noun phrase chunking (Sha and Pereira, 2003) and information extraction from research papers (Peng and McCallum, 2004). CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003), simplified part-of-speech (POS) tagging (Lafferty et al, 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of tabular data (Pinto et al, 2003), among other tasks. Named entities are identified by a CRF-based NER system, similar to that described in (McCallum and Li, 2003). It works quite well on NE tagging tasks (McCallum and Li, 2003). The modeling power of CRFs has been of great benefit in several applications, such as shallow parsing (Sha and Pereira, 2003) and information extraction (McCallum and Li, 2003). CRFs have been previously applied to other tasks such as name entity extraction (McCallum and Li, 2003), table extraction (Pinto et al, 2003) and shallow parsing (Sha and Pereira, 2003). CRFs have been shown to perform well on a number of NLP problems such as shallow parsing (Sha and Pereira, 2003), table extraction (Pinto et al, 2003), and named entity recognition (McCallum and Li, 2003).
Intricacies Of Collins Parsing Model M. University of Pennsylvania This article documents a large set of heretofore unpublished details Collins used in his parser, such that, along with Collins’ (1999) thesis, this article contains all information necessary to duplicate Collins’ benchmark results. Indeed, these as-yet-unpublished details account for an 11% relative increase in error from an implementation including all details to a clean-room implementation of Collins’ model. We also show a cleaner and equally well-performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about Collins’ parser. We not only analyze the effect of the unpublished details, but also reanalyze the effect of certain well-known details, revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought. Finally, we perform experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech. Subsequently, we replicated Gildea's experiment with a complete emulation of Model 2 and presented additional evidence that bilexical statistics were barely getting used during decoding (Bikel, 2004), appearing to confirm the original result. Subsequently, we duplicated Gildea's experiment with a complete emulation of Collins' Model 2, and found that when the decoder requested a smoothed estimate involving a bigram when testing on held-out data, it only received an estimate that made use of bilexical statistics a mere 1.49% of the time (Bikel, 2004). The results of (Bikel, 2004) suggested that the power of Collins-style parsing models did not lie primarily with the use of bilexical dependencies as was once thought, but in lexico-structural dependencies, that is, predicting syntactic structures conditioning on head words. Furthermore, the work in this paper relates to Bikel (2004)'s work. For Chinese, we experimented on the Penn Chinese Treebank 4.0 (CTB4) (Palmer et al, 2004) and we used the rules in (Bikel, 2004) for conversion. They were then parsed using Bikel's parser (Bikel,2004) and corrected by hand using the Penn Tree bank Bracketing Guidelines (Bies et al, 1995). Conditioning on crossing punctuation could be of help then, playing a role similar to that of comma-counting (Collins, 1997, §2.1) — and 'verb intervening' (Bikel, 2004, §5.1) - in early head-outward models for supervised parsing.     We used Bikel's reimplementation of Collins' parsing model 2 (Bikel, 2004). Our best performing model incorporates three dimensions of parametrization and our best result (75.25%) is similar to the one obtained by the parser of (Bikel, 2004) for Modern Standard Arabic (75%) using a fully lexicalized model and a training corpus about three times as large as our newest MH tree bank. This was followed by (Bikel, 2004) who showed that bilexical-information is used in only 1.49% of the decisions in Collins' Model-2 parser, and that removing this information results in an exceedingly small drop in performance. We use 2-best parse trees of Berkeley parser (Petrov and Klein, 2007) and 1-best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003) as inputs to the full parsing based system. This can be seen in state-of-the-art constituency-based parsers such as Collins (1999), Charniak (2000), and Petrov et al (2006), and the effects of different transformations have been studied by Johnson (1998), Klein and Manning (2003), and Bikel (2004). Collins's statistical parser (CBP; (Collins, 1997)), improved by Bikel (Bikel, 2004), is based on the probabilities between head-words in parse trees. The English sentences were parsed using the Bikel parser (Bikel, 2004), and the sentences were aligned with GIZA++ (Och and Ney,2000). For English we use the Bikel parser default head word rules (Bikel, 2004). 
A Comparison Of Algorithms For Maximum Entropy Parameter Estimation Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. However, the flexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Surprisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limitedmemory variable metric algorithm outperformed the other choices. Although IIS is a useful tool for estimating log linear models, we have since moved-on to estimating models using limited-memory variable-metric methods (Malouf, 2002). We feed both correct and incorrect parses licensed by the grammar to the TADM toolkit (Malouf, 2002), and learn a maximum entropy model. Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling (Malouf, 2002). We use the limited memory variable metric algorithm (Malouf, 2002) to determine the weights.  Parameter estimation was performed with the Limited Memory Variable Metric algorithm (Malouf, 2002) implemented in the Megampackage.   In particular, we use the open source TADM tool for parameter estimation (Malouf, 2002). because this method seems substantially faster than comparable methods (Malouf, 2002). We use the TADM open-source package (Malouf, 2002) for training the models, using its limited-memory variable metric as the optimization method and experimentally determine the optimal convergence threshold and variance of the prior. Recent improvements on the original incremental feature selection (IFS) algorithm, such as Malouf (2002) and Zhou et al (2003), greatly speed up the feature selection process. For parameter estimation of the disambiguation model, in all reported experiments we use the TADM2 toolkit (toolkit for advanced discriminative training), with a Gaussian prior and the (default) limited memory variable metric estimation technique (Malouf, 2002). Given the HPSG tree bank as training data, the model parameters are estimated so as to maximize the log-likelihood of the training data (Malouf, 2002). For model estimation we use the TADM3 software (Malouf, 2002). There are various optimization methods that allow one to estimate the weights of features, including generalized iterative scaling and quasi-Newton methods (Malouf, 2002). For parameter estimation, we use the open source TADM system (Malouf, 2002). Specifically, we use the open-source Toolkit for Advanced Discriminative Modeling (TADM:2 Malouf, 2002) for training, using its limited-memory variable metric as the optimization method and determining best-performing convergence thresholds and prior sizes experimentally. To maximize the above function, we use a limited memory variable method (Benson and More, 2002) that is implemented in the TAO package (Benson et al., 2002) and has been shown to be very effective in various natural language processing tasks (Malouf, 2002). For example, Malouf (2002) reports a matrix of non-zeroes that has 55 million entries for a shallow parsing experiment where 260,000 features were employed.
A Novel Use Of Statistical Parsing To Extract Information From Text Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations. Section 5 compares our approach too thiers in the literature, in particular that of (Miller et al., 2000). The basic approach we described is very similar to the one presented in (Miller et al, 2000) however there are a few major differences: in our approach the augmentation of the syn tactic tags with semantic tags is straightforward due to the fact that the semantic constituents are matched exactly 5. The semantic annotation required by our task is much simpler than that employed by (Miller et al, 2000). One possibly beneficial extension of our work suggested by (Miller et al, 2000) would be to add semantic tags describing relations between entities (slots), in which case the semantic constraints would not be structured strictly on the two levels used in the current approach, respectively frame and slot level. Similar to the approach in (Miller et al, 2000) we initialized the SLM statistics from the UPenn Tree bank parse trees. Rule-based methods (Miller et al, 2000) employ a number of linguistic rules to capture relation patterns. One interesting system that does not belong to the above class is that of Miller et al (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. (Miller et al, 2000) have combined entity recognition, parsing, and relation extraction into a jointly-trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly-labeled data is unavailable. Miller et al (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types. Whereas Miller et al (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance. The syntactic model in (Miller et al, 2000) is similar to Collins', but does not use features like subcat frames and distance measures. Similar to the approach in (Miller et al, 2000) and (Kulick et al, 2004), our parser integrates both syntactic and semantic annotations into a single annotation as shown in Figure 2. Miller et al (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels. Most of the approaches for relation extraction rely on the mapping of syntactic dependencies, such as SVO, onto semantic relations, using either pattern matching or other strategies, such as probabilistic parsing for trees augmented with annotations for entities and relations (Miller et al 2000), or clustering of semantically similar syntactic dependencies, according to their selectional restrictions (Gamallo et al, 2002). This includes parsing and relation extraction (Miller et al, 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al, 2004). For example, Miller et al (2000) showed that performing parsing and information extraction in a joint model improves performance on both tasks. Miller et al (2000) address the task of relation extraction from the statistical parsing viewpoint. Rule-based methods (Miller et al, 2000) employ a number of linguistic rules to capture relation patterns.
Counter-Training In Discovery Of Semantic Patterns This paper presents a method for unsupervised discovery of semantic patterns. Semantic patterns are useful for a variety of text understanding tasks, in particular for locating events in text for information extraction. The method builds upon previously described approaches to iterative unsupervised pattern acquisition. One common characteristic of prior approaches is that the output of the algorithm is a continuous stream of patterns, with gradually degrading precision. Our method differs from the previous pattern acquisition algorithms in that it introduces competition among several scenarios simultaneously. This provides natural stopping criteria for the unsupervised learners, while maintaining good precision levels at termination. We discuss the results of experiments with several scenarios, and examine different aspects of the new procedure. This problem is being addressed through automatic knowledge acquisition methods, such as unsupervised learning for domain-specific lexicons (Lin et al, 2003) and extraction patterns (Yangarber, 2003), which require the user to provide only a small set of lexical items of the target classes or extraction patterns for the target domain. This is termed constraint-driven learning in (Chang et al., 2007), coupled learning in (Carlson et al, 2010) and counter-training in (Yangarber, 2003). Once extraction relations were obtained for a particular set of documents, the resulting set of relations were ranked according to a method proposed in (Yangarber, 2003). We begin by outlining the general process of learning extraction patterns using a semi-supervised algorithm, similar to one presented by Yangarber (2003).    We begin by outlining the general process of learning extraction patterns, similar to one presented by (Yangarber, 2003).   For example, Yangarber (2003) uses just subject-verb-object tuples while Sudo et al (2003) allow any subpart of the tree to act as an extraction pattern. Predicate-Argument Model (SVO): A simple approach, used by Yangarber (2003) and Stevenson and Greenwood (2005), is to use subject-verb object tuples from the dependency parse as extraction patterns.   Yangarber (2003) and Etzioni et al (2005) utilize the so-called Counter-Training for detecting negative rules for a specific domain or a specific class by learning from multiple domains or classes at the same time.  The predicate-argument (SVO) model allows subtrees containing only a verb and its direct subject and object as extraction pattern candidates (Yangarber,2003). Yangarber (2003) proposed a counter-training approach to provide natural stopping criteria for unsupervised learning. Yangarber et al (2000) and Yangarber (2003) present an algorithm that can find patterns automatically, but it requires an initial seed of manually designed patterns for each semantic relation.
Learning Non-Isomorphic Tree Mappings For Machine Translation Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings. Unlike previous statistical formalisms (limited to isomorphic TSG local distortion of the tree topology. We reformulate it to permit dependency trees, and sketch EM/Viterbi algorithms for alignment, training, and decoding. (Eisner, 2003) outlines a computationally expensive structural manipulation tool which he has used for intra-lingual translation but has yet to apply to interlingual translation. (Eisner, 2003) presents a tree-mapping method for use on dependency trees which he claims can be adapted for use with PS trees. Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al, 2009). For example, Shieber and Schabes (1990) introduce synchronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003).   A related approach is taken by Kato and Matsubara (2010), who compare partial parse trees for different instances of the same sequence of words in a corpus, resulting in rules based on a synchronous Tree Substitution Grammar (Eisner, 2003). To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. If the parse tree of source sentence is provided, decoding (for tree-to-string and tree-to-tree models) can also be cast as a tree-parsing problem (Eisner, 2003). Our approach is based on synchronous tree substitution grammar (STSG, Eisner (2003)), a formalism that can account for structural mismatches, and is trained discriminatively. We hope that some of the work described here might be of relevance to other generation tasks such as machine translation (Eisner, 2003), multi-document summarisation (Barzilay, 2003), and text simplification (Carroll et al, 1999). A synchronous tree substitution grammar (STSG, Eisner (2003)) licenses the space of all possible rewrites. More expressive formalisms such as synchronous tree-substitution (Eisner, 2003) or tree adjoining grammars may better capture the pairings. Synchronous tree-substitution grammar is a formalism for synchronously generating a pair of non-isomorphic source and target trees (Eisner, 2003).  We use dynamic programming for parsing under this finite model (Eisner, 2003). Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. Our forest-based tree-to-tree model is based on a probabilistic STSG (Eisner, 2003). As tree-to-string translation takes a source parse tree as input, the decoding can be cast as a tree parsing problem (Eisner, 2003): reconstructing TAG derivations from a derived tree using tree-to-string rules that allow for both substitution and adjoining.
Bootstrapping Statistical Parsers From Small Datasets We present a practical co-training method for bootstrapping statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences. Experimental results show that unlabelled sentences can be used to improve the performance of statistical parsers. In addition, we consider the problem of bootstrapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material. We show that bootstrapping continues to be useful, even though no manually produced parses from the target domain are used. We have explored using different settings for the seed set size (Steedman et al, 2003). Specifically for parsing and POS tagging, self training (Reichart and Rappoport, 2007), co-training (Steedman et al 2003) and active learning (Hwa,2004) have been shown useful in the lightly supervised setup.   This protocol and that of Steedman et al (2003a) were applied to the problem, with the same seed, self-training and test sets. (Steedman et al, 2003a) used the first 500 sentences of WSJ training section as seed data. The self-training protocol of (Steedman et al, 2003a) does not actually improve over the baseline of using only the seed data. The only previous work that adapts a parser trained on a small dataset between domains is that of (Steedman et al, 2003a), which used co-training (no self-training results were reported there or elsewhere).  Steedman et al (2003) apply co-training to parser adaptation and find that co-training can work across domains. These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al, 2003), or by the same parser with a re ranking component in a type of self training scenario (McClosky et al, 2006). Self training is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al, 2003). Steedman et al (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results. In the iterative setting, we follow Steedman et al (2003) and parse 30 sentences from which 20 are selected in every iteration. It is not surprising that self-training is not normally effective: Charniak (1997) and Steedman et al (2003) report either minor improvements or significant damage from using self-training for parsing.  Bootstrapping was applied to syntax learning by Steedman et al (2003). Steedman et al (2003) reported some degradation using a lexicalized tree adjoining grammar parser and minor improvement using Collins lexicalized PCFG parser; however, this gain was obtained only when the parser was trained on a small labeled set.  Steedman et al (2003b) bootstrap two parsers that use different statistical models via co-training.
COOKING UP  REFERRING EXPRESS IONS Robert Dale Centre for Cognitive Science, University of Edinburgh 2 Buccleuch Place, Edinburgh EH8 9LW, Scotland email: rda~uk, ac.   Full brevity, the strongest interpretation, is underlying Dale's algorithm (Dale, 1989), which produces a description entailing the minimal number of attributes possible, at the price of suffering NP-hard complexity. Two other interpretations, the Greedy heuristic interpretation (Dale, 1989) and the local brevity interpretation (Reiter, 1990a) lead to algorithms that have polynomial complexity in the same order of magnitude.  Presented in (Dale, 1989), (Dale and Haddock, 1991) and (Reiter and Dale, 1992), we explore how well these algorithms perform in the same context. The Full Brevity algorithm (Dale, 1989) attempts to build a minimal distinguishing description by always selecting the most discriminatory property available;. Based on the Gricean Quantity maxim (Grice, 1975), and originally discussed by Appelt (1985), and further by Dale (1989), Reiter (1990) and Gardent (2002), this principle holds that descriptions should contain no more information than is necessary to distinguish an intended referent. In ASGRE, this has been translated into a criterion which determines the adequacy of an attribute set, implemented in its most straightforward form in Full Brevity algorithms which select the smallest attribute set that uniquely refers to the intended referent (Dale, 1989). The first of these is Minimality, defined as the proportion of descriptions produced by a system that are maximally brief, as per the original definition in Dale (1989). Just one system adopts the FullBrevity approach of Dale (1989), while the majority (11 systems) adopt the Dale and Reiterconvention of always adding TYPE. Yet, brevity plays a part in all GRE algorithms, sometimes in a strict form (Dale, 1989), or by letting the algorithm approximate the shortest description (for example, in the Dale and Reiter's IA). Due to its hill climbing nature, the IA avoids combinatorial search, unlike some predecessors which searched exhaustively for the briefest possible description of a referent (Dale, 1989), based on a strict interpretation of the GriceanMaxim of Quantity (Grice, 1975). Several NLG systems adapt to the user's domain expertize at different levels of generation text planning (Paris, 1987), complexity of instructions (Dale, 1989), referring expressions (Reiter, 1991), and so on. These measures were included because they are commonly named as desiderata for attribute selection algorithms in the REG field (Dale, 1989). Many studies in natural language processing are concerned with how to generate definite descriptions that evoke a discourse entity already introduced in the context. A solution to this problem has been initially proposed by Dale (1989) in terms of distinguishing descriptions and distin guishable entities. Following Dale (1989), these definite descriptions are named distinguishing descriptions.  Since the system knows which nodes in the semantic graph have already been mentioned it would also be possible to configure an external call to a GRE system (Dale, 1989) an application which infers the content of a referring expression given the current semantic context.  Much of this work takes as its starting point the characterisation of the problem expressed in (Dale, 1989).
Syntactic Constraints on Paraphrases Extracted from Parallel Corpora We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type. This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method. For example, (Bannard and Callison-Burch, 2005) and (Callison-Burch, 2008) described a method to extract paraphrases from largely available bilingual corpora. The work of Callison-Burch (2008) has shown how the monolingual context of a sentence to paraphrase can be used to improve the quality of the acquired paraphrases. Human evaluators were then asked to score each pair of an original sentence and a paraphrased sentence with the following two 5-point scale grades proposed by Callison-Burch (2008): Grammaticality: whether the paraphrased sentence is grammatical (1: horrible, 5: perfect) Meaning: whether the meaning of the original sentence is properly retained by the paraphrased sentence (1: totally different, 5: equivalent). Table 1 shows the average of the original 5-point scale scores and the percentage of examples that are judged correct based on a binary judgment (Callison-Burch, 2008): an example is considered to be correct iff the grammaticality score is 4 or above and/or the meaning score is 3 or above. Note that Callison-Burch (2008) might possibly underestimate the chance agreement and overestimate the ? values, because the distribution of human scores would not be uniform. In a more recent publication Callison-Burch (2008) improved this method by using syntactic constraints and multiple languages in parallel.  A well-known problem of phrase-based methods to paraphrase or term variation acquisition is the fact that a large proportion of the term variations or paraphrases proposed by the system are superor sub-strings of the original term (Callison-Burch, 2008). In some sense this is a sort of syntactic constraint introduced in Callison-Burch (2008).  Callison-Burch (2008) attempts to improve the ranking by limiting paraphrases to be the same syntactic type. This refinement to BiP, proposed in Callison-Burch (2008), constrains paraphrases to be the same syntactic type as the original phrase in the pivoting step of the paraphrase table construction. More recently, Callison-Burch (2008) has improved performance of this pivoting technique by imposing syntactic constraints on the paraphrases. Subsequently, Prasad et al (2010b) used Callison-Burch's technique for identifying syntax-constrained paraphrases (Callison-Burch, 2008) to identify additional discourse connectives, some of which don't appear in the PDTB corpus and some of which appear in the corpus but were not identified and annotated as discourse connectives. While syntactical constraints have been proven to helpful in identifying good paraphrases (Callison-Burch, 2008), it is insufficient in our task because it cannot properly filter the candidates for the replacement. Following Callison-Burch (2008), we refine selection by requiring both the original phrase and paraphrase to be of the same syntactic type, which leads to more grammatical paraphrases. Furthermore, we compared our fragment pair collection with Callison-Burch (2008)'s approach on the same MSR corpus, only about 21% of the extracted paraphrases appear on both sides, which shows the potential to combine different resources. As for comparison, we choose two other paraphrase collections, one is acquired from parallel bilingual corpora (Callison-Burch, 2008) and the other is using the same fragment extraction algorithm on the MSR corpus. Our proposed method is based on the automatically acquired paraphrase dictionary described in Callison-Burch (2008), in which the application of paraphrases from the dictionary encodes secret bits. The paraphrase dictionary that we use was generated for us by Chris Callison-Burch, using the technique described in Callison-Burch (2008), which exploits a parallel corpus and methods developed for statistical machine translation.
Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web’s natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multiinstance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume are for example they extract the pair This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Freebase. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level. Distant supervision is provided by the following constraint: every relation instance r (e 1, e 2) K must be expressed by at least one sentence in S (e 1, e 2), the set of sentences that mention both e 1 and e 2 (Hoffmann et al, 2011). We then extract relation instances from each parse and apply the greedy inference algorithm from Hoffmann et al, (2011) to identify the best set of parses that satisfy the distant supervision constraint. Our work is closest to Hoffmann et al 2011). They address the same problem we do (binary relation extraction) with a MIML model, but they make two approximations. However, our implementation has several advantages over the original model: (a) we model each relation mention independently, whereas Mintz et al collapsed all the mentions of the same entity tuple into a single datum; (b) we allow multi-label outputs for a given entity tuple at prediction time by OR-ing the predictions for the individual relation mentions corresponding to the tuple (similarly to (Hoffmann et al2011)). Hoffmann - This is the "MultiR" model, whic h performed the best in (Hoffmann et al2011). We applied our method to Wikipedia articles using Freebase as a knowledge base and found that (i) our model identified patterns expressing a given relation more accurately than baseline methods and (ii) our method led to better extraction performance than the original DS (Mintz et al, 2009) and MultiR (Hoffmann et al., 2011), which is a state-of-the-art multi instance learning system for relation extraction (see Section 7). We compared the following methods: logistic regression with the labeled data cleaned by the proposed method (PROP), logistic regression with the standard DS labeled data (LR), and MultiR propose din (Hoffmann et al, 2011) as a state-of-the-art multi instance learning system. For evaluating extraction accuracy, we follow the experimental setup of Hoffmann et al (2011), and use their implementation of MULTIR4 with 50 training iterations as our baseline. We use the same datasets as in Hoffmann et al (2011) and Riedel et al (2010), which include 3-years of New York Times articles aligned with Freebase. Figure 3 shows the precision/recall curves for MULTIR with and without pseudo-relevance feedback computed on the test dataset of 1000 sentence used by Hoffmann et al (2011). Note that the sentences are sampled from the union of Freebase matches and sentences from which some systems in Hoffmann et al (2011) extracted a relation. See Yao et al (2010) and Hoffmann et al (2011) for examples of such models. This is an at-least-one assumption based multi-instance learning method proposed by Hoffmann et al (2011).  This constraint is identical to the multiple deterministic-OR constraint used by Hoffmann et al (2011) to train a sentential relation extractor. The original formulation of the factors permitted tractable inference (Hoffmann et al 2011), but the Extracts function and the factors preclude efficient inference. Finally, we apply EXTRACTS to each parse, then use the greedy approximate inference procedure from Hoffmann et al 2011) for the factors. We compare our semantic parser to MULTIR (Hoffmann et al 2011), which is a state-of the-art weakly supervised relation extractor. Experimental results show that our trained semantic parser extracts binary relations as well asa state-of-the-art weakly supervised relation extractor (Hoffmann et al2011). The MultiR system allows entity tuples to have more than one relations, but still predicts each entity tuple locally (Hoffmann et al, 2011).
GEMINI: A Natural Language System For Spoken-Language Understanding  Recently, Dowding et al (1993) reported syntactic and semantic coverage of 86% for the DARPA Airline reservation corpus (Dowding et al, 1993). In a test set containing 26 repairs (Dowding et al, 1993), they obtained a detection recall rate of 42% and a precision of 84.6%; for correction, they obtained a recall rate of 30% and a recall rate of 62%. Dowding et al (1993) used a similar setup for their data. Typed Unification Grammars (TUG), like HPSG (Pollard and Sag 1994) and Gemini (Dowding et al. 1993) are a more expressive formalism in which to write formal grammars. Initial language processing is carried out Using the SRI Gemini system (Dowding et al, 1993), using a domain-independent unification. We use the Open Agent Architecture (Martin et al 1999) for communication between agents based on the Nuance speech recognizer, the Gemini natural language system (Dowding et al 1993), and Festival speech synthesis. Finally, we are developing an interface to a new large-vocabulary version of the Gemini parser (Dowding et al, 1993) which will allow us to use semantic parse information as features in the individual sub-class classifiers, and also to extract entity and event representations from the classified utterances for automatic addition of entries to calendars and to-do lists. Questions are posed to GeoLogica in a subset of English and translated into logic by a natural language parser, the system Gemini (Dowding et al, 1993). This is contrasted with the all-paths bottom-up strategy in GEMINI (Dowding et al 1993) that finds all admissable edges of the grammar. To handle the ASR results of disfluent utterances, we employ SRI's Gemini robust language parser (Dowding et al, 1993). The most likely hypothesis is input to SRI's Gemini natural language parser/generator (Dowding et al. 1993), which attempts to parse the speech recognition output. The dialogue system uses the Nuance 8.0 speech recognizer with language models compiled from a grammar (written using the Gemini system (Dowding et al, 1993)), which is also used for parsing and generation. Annotating sub-constituents with grammatical relations regularizes the syntactic structure with respect to particular grammatical rules, and allows a 'relation-to-relation' form of compositionality, as opposed to the more traditional 'rule-to-rule' version that is exemplified by such systems as Gemini (Dowding et al 1993) and the Core Language Engine (Alshawi, 1992).
Syntax Augmented Machine Translation Via Chart Parsing We present translation results on the shared task ”Exploiting Parallel Texts for Statistical Machine Translation” generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop’s baseline system. Our translation system is available open-source under the GNU General  Other models use more syntactic information (string-to-tree, tree to-string, tree-to-tree, string-to-dependency etc.) to capture the structural difference between language pairs, including (Yamada and Knight, 2001), (Zollmann and Venugopal, 2006), (Liu et. al. 2006), and (Shen et. al. 2008). In particular we have added support for Zollmann and Venugopal (2006)'s syntax-augmented machine translation. Thrax extracts both hierarchical (Chiang, 2007) and syntax-augmented machine translation (Zollmann and Venugopal, 2006) grammars. An SAMT grammar (Zollmann and Venugopal, 2006) is similar to a Hiero grammar, except that the nonterminal symbol set is much larger, and its labels are derived from a parse tree over either the source or target side in the following manner. The SAMT implementation of Zollmann and Venugopal (2006) includes a several-thousand-line Perl script to extract their rules. To date, several open-source SMT systems (based on either phrase based models or syntax-based models) have been developed, such as Moses (Koehn et al, 2007), Joshua (Li et al, 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al, 2010), cdec (Dyer et al, 2010), Jane (Vilar et al, 2010) and SilkRoad, and offer good references for the development of the NiuTrans toolkit. Table 1: Rules and their sequences of phrase pairs and nonterminals Previous work has attempted to weaken the con text free assumption of the synchronous context free grammar formalism, for example using syn tactic non-terminals (Zollmann and Venugopal, 2006). Our hierarchical systems consist of a syntax-augmented system (SAMT) that includes target-language syntactic categories (Zollmann and Venugopal, 2006) and a Hiero-style system with a single non-terminal (Chiang, 2007). For example, syntax is successfully integrated into hierarchical SMT (Zollmann and Venugopal,2006). Zollmann and Venugopal (2006) started with a complete set of phrases as extracted by traditional PBMT heuristics, and then annotated the target side of each phrasal entry with the label of the constituent node in the target-side parse tree that subsumes the span. Since many widely used SCFGs meet these criteria, including hierarchical phrase-based translation grammars (Chiang, 2007), SAMT grammars (Zollmann and Venugopal, 2006), and phrasal ITGs (Zhang et al, 2008), a detailed analysis of epsilon-containing and higher rank grammars is left to future work. Zollmann and Venugopal (2006) allow rules to be extracted where non-terminals do not exactly span a target constituent. This contrasts with the approach by (Zollmann and Venugopal, 2006) in attempting to improve the coverage of syntactic translation. Zollmann and Venugopal (2006) and Marcu et al (2006) used broken syntactic fragments to augment their grammars to increase the rule coverage; while we learn optimal tree fragments transformed from the original ones via a generative framework, they enumerate the fragments available from the original trees without learning process. One example of modifying the SCFG nonterminal set is seen in the Syntax-Augmented MT (SAMT) system of Zollmann and Venugopal (2006). SAMT (Zollmann and Venugopal, 2006) introduces heuristics to create new non-constituent labels, but these heuristics introduce many complex labels and tend to add rarely-applicable rules to the translation grammar. Syntax-Augmented Machine Translation (SAMT; Zollmann and Venugopal, 2006) solves this problem with heuristics that create new labels from the phrase structure parse: it labels for the establishment of as IN+NP+IN to show that it is the con catenation of a noun phrase with a preposition on either side. The Syntax-Augmented Machine Translation (SAMT) model (Zollmann and Venugopal, 2006) extracts more rules than the other syntactic model by allowing different labels for the rules. This restriction may be relaxed by adding constituent labels such as DET+ADJ or NPDET to group neighboring constituents or indicate constituents that lack an initial child, respectively (Zollmann and Venugopal, 2006).
Efficient Deep Processing Of Japanese We present a broad coverage Japanese grammar written in the HPSG formalism with MRS semantics. The grammar is created for use in real world applications, such that robustness and performance issues play an important role. It is connected to a POS tagging and word segmentation tool. This grammar is being developed in a multilingual context, requiring MRS structures that are easily comparable across languages.  By using a Japanese grammar (JACY: Siegel and Bender (2002)) based on a monostratal theory of grammar (HPSG: Pollard and Sag (1994)) we could simultaneously annotate syntactic and semantic structure without overburdening the annotator. JACY (Siegel and Bender, 2002) is a hand-crafted Japanese HPSG grammar that provides semantic information as well as linguistically motivated analysis of complex constructions. The grammars used for the experiments reported here are the LinGO English Resource Grammar (ERG; Flickinger (2000)) and JACY (Siegel and Bender, 2002), precision grammars of English and Japanese, respectively. In this paper, we explore the utility of different evaluation metrics at predicting parse performance through a series of experiments over two broad coverage grammars: the English Resource Grammar (ERG; Flickinger (2000)) and JACY (Siegel and Bender, 2002). JACY (Siegel and Bender, 2002) is a broad coverage linguistically precise HPSG-based grammar of Japanese. Of these, the most thorough work on Japanese honorification is seen in JACY, a Japanese HPSG grammar (Siegel 2000, Siegel and Bender 2002).  The grammar is an HPSG implementation (JACY: Siegel and Bender, 2002), which provides a high level of detail, marking not only dependency and constituent structure but also detailed semantic relations.  We take our examples from JACY (Siegel and Bender, 2002), a large grammar of Japanese built in the HPSG framework. The grammar is an HPSG implementation (JACY: Siegel and Bender, 2002), which provides a high level of detail, marking not only dependency and constituent structure but also detailed semantic relations. The Grammar Matrix was developed initially on the basis of broad coverage grammars for English (Flickinger, 2000) and Japanese (Siegel and Bender, 2002), and has since been extended and refined as it has been used in the development of broad-coverage grammars for Norwegian (Hellan and Haugereid, 2003), ModernGreek (Kordoni and Neu, 2005), and Spanish (Marimon et al, 2007), as well as being applied to 42 other languages from a variety of language families in a classroom context (Bender, 2007). There are also several grammars: e.g. ERG; the English Resource Grammar (Flickinger, 2000), Jacy; a Japanese Grammar (Siegel and Bender, 2002), the Spanish grammar, and so forth. In this section, we outline the resources targeted in this research, namely the English Resource Grammar (ERG: Flickinger (2002), Copestake and Flickinger (2000)) and the JACY grammar of Japanese (Siegel and Bender, 2002). Fujita et al (2007) add sense information to improve parse ranking with JaCy (Siegel and Bender,2002), an HPSG-based grammar which uses similar machinery to the ERG. In order to examine the cross-lingual applicability of our methods, we also use Jacy, an HPSG-based grammar of Japanese (Siegel and Bender, 2002). Language specific analyses have been implemented in deep, broad-coverage grammars for languages such as Japanese (Masuichi et al (2003), Siegel and Bender (2002)) and Portuguese (Branco and Costa (2008)). In order to produce semantic representations we are using an open source HPSG grammar of Japanese: JACY (Siegel and Bender, 2002), which we have extended to cover the dictionary definition sentences (Bond et al, 2004). We used the Japanese grammar Jacy (Siegel and Bender, 2002), a deep parsing HPSG grammar that produces RMRSs for our primary input source.
Using Multiple Knowledge Sources For Word Sense Discrimination This paper addresses the problem of how to identify the intended meaning of individual words in unrestricted texts, without necessarily having access to complete representations of sentences. To discriminate senses, an understander can consider a diversity of information, including syntactic tags, word frequencies, collocations, semantic context, role-related expectations, and syntactic restrictions. However, current approaches make use of only small subsets of this information. Here we will describe how to use the whole range of information. Our discussion will include how the preference cues relate to general lexical and conceptual knowledge and to more specialized knowledge of collocations and contexts. We will describe a method of combining cues on the basis their individual than a fixed ranking among cue-types. We will also discuss an application of the approach in a system that computes sense tags for arbitrary texts, even when it is unable to determine a single syntactic or semantic representation for some sentences. Those relations between the sense and its defining words are reflected in semantic dusters that are termed categorical, functional, and situational clusters in McRoy (1992). Moreover, those relations have been shown to be very effective knowledge sources for WSD (McRoy 1992) and interpretation of noun sequences (Vanderwende 1994). Even if most of the techniques for WSD are presented as stand-alone, it is our belief, following the ideas of (McRoy, 1992), that full-fledged lexical ambiguity resolution should combine several information sources and techniques.   (McRoy, 1992) was one of the first to use multiple kinds of features for word sense disambiguation in the semantic interpretation system, TRUMP.     We propose a tagger that makes use of several types of information (dictionary definitions, parts-of-speech, domain codes, selectional preferences and collocates) in the tradition of McRoy (McRoy, 1992) although, the information sources we use are orthogonal, unlike the sources he used, making it easier to evaluate the performance of the various modules. This approach as been recently used by (McRoy, 1992) and (Mahesh and Beale, 1996). There are a variety of methods for combining multiple knowledge sources (linguistic cues) (McRoy, 1992). McRoy (1992) describes a study of different sources useful for word sense disambiguation, including morphological information.
Learning Bilingual Lexicons from Monolingual Corpora We present a method for learning bilingual translation lexicons from monolingual corpora. Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings. Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types. Haghighi et al (2008) mention one disadvantage of using edit distance, that is, precision quickly degrades with higher recall. Haghighi et al (2008), amongst a few others, propose using canonical correlation analysis to reduce the dimension. Haghighi et al (2008) only use a small-sized bilingual lexicon containing 100 word pairs as seed lexicon. Examples can be found in Fung and Cheung (2004), followed by Haghighi et al (2008). Haghighi et al (2008) have reported that the most common errors detected in their analysis on top 100 errors were from semantically related words, which had strong context feature correlations. Earlier work, Haghighi et al (2008), proposed a method for inducing bilingual lexica using monolingual feature representations and a small initial lexicon to bootstrap with. CCA has been used in bilingual lexicon extraction from comparable corpora (Gaussier et al, 2004) and monolingual corpora (Haghighi et al., 2008). Haghighi et al (2008) use a probabilistic model over word feature vectors containing co occurrence and orthographic features.  The availability of parsers is a more stringent constraint, but our results suggest that more basic NLP methods may be sufficient for bilingual lexicon extraction. In this work, we have used a set of seed translations (unlike e.g., Haghighi et al (2008)). For example, the paper by Haghighi et al (2008) (which demonstrates how orthography and contextual information can be successfully used) reports 61.7% accuracy on the 186 most confident predictions of nouns. Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al, 2008), we are able to find translations for otherwise OOV terms. Our dictionary mining approach is based on Canonical Correlation Analysis, as used previously by (Haghighi et al, 2008). Here we describe the use of CCA to find the translations for the OOV German words (Haghighi et al, 2008). We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al, 2008). We evaluate sets of high-probability words in each topic and multilingual "synsets" by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al (2008). Previous research on bilingual lexicon induction learned translations only for a small number of high frequency words (e.g. 100 nouns in Rapp (1995), 1,000 most frequent words in Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al (2008)). Haghighi et al (2008) also used this method to show how well translations could be learned from monolingual corpora under ideal conditions, where the contextual and temporal distribution of words in the two monolingual corpora are nearly identical. Haghighi et al, (2008) made use of contextual and orthographic clues for learning a generative model from monolingual corpora and a seed lexicon. Haghighi et al (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account.
PARADISE: A Framework For Evaluating Spoken Dialogue Agents This paper presents PARADISE (PARAdigm for DIalogue System Evaluation), a general framework for evaluating spoken rlialogue agents. The framework decouples task requirements from an agent's dialogue behaviors, supports comparisons among dialogue strategies, enables the calculation of performance over subdialogues and whole dialogues, specifies the relative contribution of various factors to performance, and makes it possible to compare agents performing different tasks by normalizing for task complexity. In the remainder of this paper, we discuss the PARADISE framework (PARAdigm for Dialogue System Evaluation) (Walker et al, 1997), and that it addresses these limitations, as well as others. Instead, predictions about user satisfaction can be made on the basis of the predictor variables, which is illustrated in the application of PARADISE to sub dialogues in (Walker et al., 1997). While we discussed the representation of an information-seeking dialogue here, AVM representations for negotiation and diagnostic dialogue tasks are also easily constructed (Walker et al, 1997). The first approach to predict user judgments on the basis of interaction metrics is the well-known PARADISE model (Walker et al, 1997). There are also well-known evaluation efforts such as EAGLES (Sparck Jones and Galliers, 1996) and the Paradise evaluation framework (Walker et al, 1997). Walker et al (1997) identified three factors which carry an influence on the performance of SDSs, and which therefore are thought to contribute to its quality perceived by the user: agent factors (mainly related to the dialogue and the system itself), task factors (related to how the SDS captures the task it has been developed for) and environmental factors (e.g. factors related to the acoustic environment and the transmission channel). The PARADISE framework (Walker et al, 1997) produces such a relationship for a specific scenario, using multivariate linear regression. As stated above, the separation of environmental, agent and task factors was motivated by Walker et al (1997). In the PARADISE framework, user satisfaction is composed of maximal task success and minimal dialogue costs (Walker et al, 1997), thus a type of efficiency in the way it was defined here. User satisfaction is a function of task success and the number of user turns based on the PARADISE framework (Walker et al, 1997) and CAS refers to the proportion of repetition and variation in surface forms. Previous studies (E.g., Walker et al, 1997) use a corpus level semantic accuracy measure (semantic Accuracy) to capture the system's understanding ability. Once they had completed all tasks in sequence using one system, they filled out a questionnaire to assess user satisfaction by rating 8-9 statements, similar to those in (Walker et al, 1997), on a scale of 1-5, where 5 indicated highest satisfaction. Following the PARADISE evaluation scheme (Walker et al, 1997), we divided performance features into four groups. Some studies (e.g., (Walker et al, 1997)) build regression models to predict user satisfaction scores from the system log as well as the user survey. Future work focuses on usability tests of the prototype system, e.g. using the PARADISE evaluation framework to evaluate the general usability of the system (Walker et al, 1997). They then derived dialogue act metrics from the DATE tags and showed that when these metrics were used in the PARADISE evaluation framework (Walker et al, 1997) that they improved models of user satisfaction by an absolute 5%, and that the new metrics could be used to understand which system's dialogue strategies were most effective. Such metrics have been introduced in other fields, including PARADISE (Walker et al, 1997) for spoken dialogue systems, BLEU (Papineni et al, 2002) for machine translation, and ROUGE (Lin, 2004) for summarisation. In doing so, we are essentially exploring system behaviour in a glass box approach: this does not constitute an evaluation method for dialogue performance [Walker et al, 1997]. In particular, unlike the PARADISE framework (Walker et al, 1997), which aims to evaluate dialogue agent strategies by relating overall user satisfaction to various other metrics (task success, efficiency measures, and qualitative measures) our approach takes the agent's dialogue strategy for granted. Previous work has therefore suggested to learn a reward function from human data as in the PARADISE framework (Walker et al, 1997).
A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence. These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance. Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity. Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far. Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew. Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models. Adler and Elhadad (2006) presented an HMM-based approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of Hebrew, based on lattice parsing. Goldberg and Tsarfaty (2008) propose a generative joint model. Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach. Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text. Following (Goldberg and Tsarfaty, 2008) we deal with the ambiguous affixation patterns in Hebrew by encoding the input sentence as a segmentation lattice. The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008). A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew. Models that in addition incorporate morphological analysis and segmentation have been explored by Tsarfaty (2006), Cohen and Smith (2007), and Goldberg and Tsarfaty (2008) with special reference to Hebrew parsing. Parsing and segmentation are handled jointly by the parser (Goldberg and Tsarfaty, 2008). It is the same grammar as described in (Goldberg and Tsarfaty, 2008).  The most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task. Goldberg and Tsarfaty (2008) use a data-driven morphological analyzer derived from the tree bank. The model of Goldberg and Tsarfaty (2008) uses a morphological analyzer to constructs a lattice for each input token. Instead, we use the evaluation measure of (Tsarfaty, 2006), also used in (Goldberg and Tsarfaty, 2008), which is an adaptation of parseval to use characters instead of space-delimited tokens as its basic units.
Evaluating Discourse Processing Algorithms In order to take steps towards establishing a methodology for evaluating Natural Language systems, we conducted a case study. We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues. We present the quantitative results of handsimulating these algorithms, but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general. We illustrate the general difficulties encountered with quantitative evaluation. These are problems with: (a) allowing for underlying assumptions, (b) determining how to handle underspecifications, and (c) evaluating the contribution of false positives and error chaining. One is the case in which a pronoun x correctly says that it is coreferent with another pronoun y. However, the program misidentifies the antecedent of y. In this case (sometimes called error chaining (Walker, 1989)), both x and y are to be scored as wrong, as they both end up in the wrong coreferential chain. I use the following guidelines for the hand-simulated analysis (Walker, 1989). Following Walker (1989), a segment is defined as a paragraph unless its first sentence has a pronoun in subject position or a pronoun where none of the preceding sentence-internal oun phrases matches its syntactic features. 
The Structure And Performance Of An Open-Domain Question Answering System answers in large collections of texts: paragraph + abductive inference. Notes of the Fall AAAI Symposium on Question An  Template-based questions and summary asking inquiries cover most of the classes of question complexity proposed in (Moldovan et al, 2000). Correcting this would require a model that jointly models content and bi grams (Hardisty et al., 2010), has a co reference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al 2000).  Some Q&A systems, like (Moldovan et al, 2000) relied both on NE recognizers and some empirical indicators. (Moldovan et al, 2000) details the empirical methods used in our system for transforming a natural language question into an IR query. To decide which keywords should be expanded and what form of alternations should be used we rely on a set of heuristics which complement the heuristics that select the question keywords and generate the queries (as described in (Moldovan et al, 2000)): Heuristic 1: Whenever the first feedback loop requires the addition of the main verb of the question as a query keyword, generate all verb conjugations as well as its nominalizations. Here, we can distinguish between approaches that only return one passage per relevant document (see, for example, (Robertson et al., 1992)) and the ones that allow multiple passages per document (see, for example (Moldovan et al, 2000)). The Falcon system (Moldovan et al, 2000) uses some semantic relations from WordNet when it expands the question. Moldovan et al (2000), for instance, select as keywords all named entities that were recognized as proper nouns. In the work of Moldovan et al (2000), all why-questions share the single answer type reason.
Supertagging: An Approach To Almost Parsing In this paper, we have proposed novel methods for robust parsing that integrate the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques. Our thesis is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions (supertags) that impose complex constraints in a local context. The supertags are designed such that only those elements on which the lexical item imposes constraints appear within a given supertag. Further, each lexical item is associated with as many supertags as the number of different syntactic contexts in which the lexical item can appear. This makes the number of different descriptions for each lexical item much larger than when the descriptions are less complex, thus increasing the local ambiguity for a parser. But this local ambiguity can be resolved by using statistical distributions of supertag co-occurrences collected from a corpus of parses. We have explored these ideas in the context of the Lexicalized Tree-Adjoining Grammar (LTAG) framework. The supertags in LTAG combine both phrase structure information and dependency information in a single representation. Supertag disambiguation results in a representation that is effectively a parse (an almost parse), and the parser need &quot;only&quot; combine the individual supertags. This method of parsing can also be used to parse sentence fragments such as in spoken utterances where the disambiguated supertag sequence may not combine into a single structure. These are much finer grained than Penn Treebank preterminals tags, and more akin to those used in Tree Adjoining Grammar models (Bangalore and Joshi,1999). Supertags are the elementary structures of Lexicalized Tree Adjoining Grammars (LTAGs) (Bangalore and Joshi, 1999). We used the supertagger (Bangalore and Joshi, 1999) to supertag each word in the corpus. Insertion of further information such as supertags (Bangalore and Joshi, 1999) or word stems can also be beneficial for further processing. The process is quite similar to supertagging (Bangalore and Joshi, 1999), which assigns rich descriptions (supertags) that impose complex constraints in a local context. The notions of a supertag as a lexical category and the process of supertagging are both crucial here (Bangalore and Joshi, 1999). Bangalore and Joshi (1999) indicated that, correct disambiguation with supertagging, i.e., assignment of lexical entries before parsing, enabled effective LTAG (Lexicalized Tree-Adjoining Grammar) parsing. As methodologies deriving well-formedness of a sentence we use super tagging (Bangalore and Joshi, 1999) with lightweight dependency analysis (LDA) (Bangalore, 2000), link grammars (Sleator and Temperley, 1993) and a maximum entropy (ME) based chunk parser (Bender et al, 2003). Supertagging (Bangalore and Joshi, 1999) uses the Lexicalized Tree Adjoining Grammar formalism (LTAG). We used the supertagger of Bangalore and Joshi (1999). This is a variant of the approach above, but using super tags (Bangalore and Joshi, 1999) instead of PoS tags. It was first introduced as a means of reducing parser ambiguity by Bangalore and Joshi (1999) in the context of the LTAG formalism, and has since been applied in a similar context within the CCG formalism (Clark and Curran, 2004). It was first proposed for lexicalized tree adjoining grammar (LTAG) (Bangalore and Joshi, 1999). We condition prosody not only on word strings and their parts-of-speech but also on richer syntactic information encapsulated in the form of Supertags (Bangalore and Joshi, 1999). In addition to the POS tags, we also annotate the utterance with Supertags (Bangalore and Joshi, 1999). Supertags have been successfully applied to guide parsing in symbolic frameworks such as Lexicalised Tree-Adjoning grammar (Bangalore and Joshi, 1999). The parser uses a two-stage system, first employing a super tagger (Bangalore and Joshi, 1999) to propose lexical categories for each word, and then applying the CKY chart parsing algorithm. We automatically annotate a user's utterance with super tags (Bangalore and Joshi, 1999). The second direction concerns experiments on supertagging (Bangalore and Joshi, 1999) followed by a parsing stage the tagging stage associates to each word a supertag. Bangalore and Joshi (1999), Clark and Curran (2004) and Matsuzaki et al (2007) show that by using a supertagger before (CCG and HPSG) parsing, the space required for discriminative training is drastically reduced.
Entity-Based Cross-Document Core f erencing Using the Vector Space Model Cross-document coreference occurs when the same person, place, event, or concept is discussed in more than one text source. Computer recognition of this phenomenon is important because it helps break &quot;the document boundary&quot; by allowing a user to examine information about a particular entity from multiple text sources at the same time. In this paper we describe a cross-document coreference resolution algorithm which uses the Vector Space Model to resolve ambiguities between people having the same name. In addition, we also describe a scoring algorithm for evaluating the cross-document coreference chains produced by our system and we compare our algorithm to the scoring algorithm used in the MUC- 6 (within document) coreference task. Biographic Data Past work on this task (e.g. Bagga and Baldwin, 1998) has primarily approached personal name disambiguation using document context profiles or vectors, which recognize and distinguish identical name instances based on partially indicative words in context such as computer or car in the Clark case. Some refer to the task as cross-document co reference resolution (Bagga and Baldwin, 1998), name discrimination (Pedersen et al, 2005) or Web People Search (WebPS) (Artiles et al, 2007). In our experiments, we use the training texts to acquire co reference classifiers and evaluate the resulting systems on the test texts with respect to two commonly-used co reference scoring programs: the MUC scorer (Vilain et al, 1995) and the B-CUBED scorer (Bagga and Baldwin, 1998). Four-fold cross validation is employed and B-CUBED metric (Bagga and Baldwin, 1998) is adopted to evaluate the clustering results. We employed the B-CUBED metric (Bagga and Baldwin, 1998) to evaluate the clustering results. Bagga and Baldwin (1998) proposed entity based cross-document co-referencing which uses co-reference chains of each document to generate its summary and then use the summary rather than the whole article to select informative words to be the features of the document. BCubed (Bagga and Baldwin, 1998) is an attractive measure that addresses both completeness and homogeneity. Similar approach was developed by (Bagga and Baldwin, 1998), who created first order context vectors that represent the instance in which the ambiguous name occurs. In this paper, we present a new text semantic similarity approach for fine-grained person name categorization and discrimination which is similar to those of (Pedersen et al, 2005) and (Bagga and Baldwin, 1998), but instead of simple word co-occurrences, we consider the whole text segment and relate the deduced semantic information of Latent Semantic Analysis (LSA) to trace the text cohesion between thousands of sentences containing named entities which belong to different fine-grained categories or individuals. The original work in (Bagga and Baldwin, 1998) proposed a CDC system by first performing WDC and then disambiguating based on the summary sentences of the chains. Mann and Yarowsky (2003) have proposed a Web based clustering technique relying on a feature space combining biographic facts and associated names, whereas Bagga and Baldwin (1998) have looked for coreference chains within each document, take the context of these chains for creating summaries about each entity and convert these summaries into a bag of words. We base our work partly on previous work done by Bagga and Baldwin (Bagga and Baldwin, 1998), which has also been used in later work (Chen and Martin, 2007). To score the output of a coreference model, we employ three scoring programs: MUC (Vilain et al, 1995), B3 (Bagga and Baldwin, 1998), and 3 -CEAF (Luo, 2005). The aforementioned complication does not arise from the construction of the mapping, but from the fact that Bagga and Baldwin (1998) and Luo (2005) do not specify how to apply B3 and CEAF to score partitions generated from system mentions. We use the B3 (Bagga and Baldwin, 1998) evaluation measure as well as precision, recall, and F1 measured on the (positive) pairwise decisions. Early work in the field of name disambiguation is that of (Bagga and Baldwin, 1998) who proposed cross-document coreference resolution algorithm which uses vector space model to resolve the ambiguities between people sharing the same name. On this dataset, our proposed model yields a B3 (Bagga and Baldwin, 1998) F1 score of 73.7%, improving over the baseline by 16% absolute (corresponding to 38% error reduction). One of the first approaches to cross-document coreference (Bagga and Baldwin, 1998) uses an idf-based cosine-distance scoring function for pairs of contexts, similar to the one we use. The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) (Agirre and Edmonds, 2006) and Cross-document Coreference (CDC) (Bagga and Baldwin, 1998). Cross document coreference (CDC) (Bagga and Baldwin, 1998) is a distinct technology that consolidates named entities across documents according to their real referents.
Decision Lists For Lexical Ambiguity Resolution: Application To Accent Restoration In Spanish And French This paper presents a statistical decision procedure for lexical ambiguity resolution. The algorithm exploits both local syntactic patterns and more distant collocational evidence, generating an efficient, effective, and highly perspicuous recipe for resolving a given ambiguity. By identifying and utilizing only the single best disambiguating evidence in a target context, the algorithm avoids the problematic complex modeling of statistical dependencies. Although directly applicable to a wide class of ambiguities, the algorithm is described and evaluated in a realistic case study, the problem of restoring missing accents in Spanish and French text. Current accuracy exceeds 99% on the full task, and typically is over 90% for even the most difficult ambiguities.  Until now, many methods have been proposed for this problem including winnow-based algorithms (Golding and Roth, 1999), differential grammars (Powers, 1998), transformation based learning (Mangu and Brill, 1997), decision lists (Yarowsky, 1994). In cases like (Yarowsky, 1995), unsupervised methods offer accuracy results than rival supervised methods (Yarowsky,1994) while requiring only a fraction of the data preparation effort. Decision lists (Rivest, 1987) have been used for a variety of natural language tasks, including accent restoration (Yarowsky, 1994), word sense disambiguation (Yarowsky, 2000), finding the past tense of English verbs (Mooney and Califf, 1995), and several other problems. The standard algorithm for learning decision lists (Yarowsky, 1994) is very simple. Yarowsky (1994) suggests two improvements to the standard algorithm. Accent restoration (Yarowsky, 1994), word sense disambiguation (Yarowsky, 2000), and other problems all fall into this framework, and typically use similar feature types. They include those using Naive Bayes (Gale et al 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and 1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs. Yarowsky (1994) notes that conceptual spelling correction is part of a closely related class of problems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration. Lemmatization allows for more compact and generalizable data by clustering all inflected forms of an ambiguous word together, an effect already commented on by Yarowsky (1994). As has already been noted by Yarowsky (1994), using lemmas helps to produce more concise and generic evidence than inflected forms. Yarowsky (1994) defined a basic set of features that has been widely used (with some variations) by other WSD systems. Despite their simplicity, Decision Lists (Dlist for short) as defined in Yarowsky (1994) have been shown to be very effective for WSD (Kilgarriff & Palmer, 2000). In general, the strategy adopted to model syntagmatic relations in WSD is to provide bigrams and trigrams of collocated words as features to describe local contexts (Yarowsky, 1994).  Yarowsky (1994 and 1995), Mihalcea and Moldovan (2000), and Mihalcea (2002) have made further research to obtain large corpus of higher quality from an initial seed corpus. In general, the strategy adopted to model syntagmatic relations is to provide bigrams and trigrams of collocated words as features to describe local contexts (Yarowsky, 1994), and each word is regarded as a different instance to classify. As baseline, we report the result of a standard approach consisting on explicit bigrams and trigrams of words and POS tags around the words to be disambiguated (Yarowsky, 1994). The more recent set of techniques includes multiplicative weight update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation-based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al, 1993, Golding, 1995, Golding and Schabes, 1996). Yarowsky (1994) argued the optimal value is sensitive to the type of ambiguity.
WordNet::Similarity - Measuring The Relatedness Of Concepts WordNet::Similarity is a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts (or synsets). It provides six measures of similarity, and three measures of relatedness, all of which are based on the lexical database WordNet. These measures are implemented as Perl modules which take as input two concepts, and return a numeric value that represents the degree to which they are similar or related. Extensive research concerning the integration of semantic knowledge into NLP for the English language has been arguably fostered by the emergence of WordNet::Similarity package (Pedersen et al, 2004). Work based on WordNet-like lexical networks for building semantic similarity measures such as (Budanitsky and Hirst, 2006) or (Pedersen et al, 2004) falls into this category. For replacement using semantic similarity measures, we used WordNet::Similarity 2.05 package by Pedersen et al (2004). We use the Lesk (overlap) similarity as implemented by the WordNet::similarity package (Pedersen et al, 2004). SR-AW finds the sense of each word that is most related or most similar to those of its neighbors in the sentence, according to any of the ten measures available in WordNet::Similarity (Pedersen et al, 2004). Then the value of vi is assigned as follows, where the similarity function SIM (ti ,wj) is calculated according to the path measure (Pedersen et al., 2004) using the WordNet. We used the Wordnet::Similarity software package (Pedersen et al, 2004) to calculate the similarity between every two words at first. The measures vary from simple edge-counting to attempt to factor in peculiarities of the network structure by considering link direction, relative path, and density, such as vector, lesk, hso, lch, wup, path, res, lin and jcn (Pedersen et al, 2004). We use the WordNet word-to-word similarity metrics (Pedersen et al, 2004) and Latent Semantic Analysis (Landauer et al, 2007).  Yet, other resources of semantically-related terms can be beneficial, such as WordNet::Similarity (Pedersen et al, 2004), statistical resources like that of Lin (1998) or DIRECT (Kotlerman et al, 2010), thesauri, Wikipedia (Hu et al., 2009), ontologies (Suchanek et al, 2007) etc. where d(.) is a WordNet based relatedness measure (Pedersen et al, 2004). The thresholds were thoroughly selected depending on our analysis for the WordNet hierarchary and semantic similarity measures (Pedersen et al, 2004). For example, WordNet similarities (Pedersen et al, 2004) or Latent Semantic Analysis over a large corpus are widely used in many applications and for the definition of kernel functions. It is worth mentioning that the LSA similarity measure depends on the selected corpus but it benefits from a higher computation speed in comparison to the construction of the similarity matrix based on the WordNet Similarity package (Pedersen et al, 2004). Additionally, we used the Jiang&Conrath (J&C) distance (Jiang and Conrath, 1997) computed with wn::similarity package (Pedersen et al, 2004) to measure the similarity between T and H. Moreover, Table 4 shows that the computation of the LSA matrix on Wikipedia is faster than using the WordNet similarity software (Pedersen et al, 2004). We use the default configuration of the measure in WordNet::Similarity-0.12 package (Pedersen et al, 2004), and, with a single exception, the measure performed below Gic; see BP in table 1. We use (Pedersen et al., 2004) implementation with a minor alteration. The WordNet::Similarity package provides a flexible implementation of many of these measures (Pedersen et al, 2004).
Toward Memory--based Translation Satoshi SATO and Ma.koto NAGAO Dept.  The construction of bilingual knowledge base, in the development of example-based machine translation systems (Sato and Nagao, 1990), is vitally critical. For example, (Sato and Nagao, 1990} combine a measure of structural similarity with a measure of word distance in order to obtain the overall distance measure that is used for matching. This is the same idea as example-based machine translation (Sato and Nagao, 1990 and Furuse et .al., 1994). In the wake of the pioneering work of Nagao (1984), Brown et al (1990) and Sato and Nagao (1990), Machine Translation (MT) research has increasingly focused on the issue of how to acquire translation knowledge from aligned parallel texts.
A COMPUTATIONAL MODEL OF LANGUAGE DATA ORIENTED PARSING RENS BOlt* Department of Computational I Jnguistics University of Amsterdmn Spuistraat 134 1012 VII Amsterdam The Netherlands rens@alf.let.uva.nl PERFORMANCE: Abstract 1)ata Oriented Parsing (IX)P) is a model where no abstract rules, but language xt~riences in the ti3ru~ of all ,malyzed COlpUS, constitute the basis for langnage processing.  For a treatment of DOP in more formal terms we refer to (Bod, 1992a). In (Bod, 1992b) super strong equivalence relations between other stochastic grammars are studied. It is easy to show that an input string can be parsed with conventional parsing techniques, by applying subtrees instead of rules to the input string (Bod, 1992a). In their place we added a new feature, the probability of a rule's source side tree given its root label, which is essentially the same model used in Data-Oriented Parsing (Bod, 1992). Due to this extension, the one to one mapping between a derivation and a parse tree, which holds in CFGs, does not hold any more; many derivations might generate the same parse-tree ,rl &apos; his seemingly spurious ambiguity turns out crucial for statistical disambiguation as defined in (Bod, 1992) and in (Schabes and Waters, 1993), where the derivations are considered different stochastic processes and their probabilities all contribute to the probability of the generated parse. context-free rulesCharniak (1996) Collins (1996), Eisner (1996) context-free rules, headwords Charniak (1997) context-free rules, headwords, grandparent nodes Collins (2000) context-free rules, headwords, grandparent nodes/rules, bi grams, two-level rules, two-level bi grams, non headwords Bod (1992) all fragments within parse trees Scope of Statistical Dependencies Model Figure 4. It occurs because many systems, such as the ones proposed by (Bod, 1992), (Galley, et .al., 2004), and (Langkilde and Knight, 1998) represent their result space in terms of weighted partial results of various sizes that may be assembled in multiple ways. The Data-Oriented Parsing (DOP) method suggested in Scha (1990) and developed in Bod (19921995) is a probabilistic parsing strategy which does not single out a narrowly predefined set of structures as the statistically significant ones. In Bod (1992, 1993a), a first instantiation of this model is given, called DOP1, which uses (1) labelled trees for the utterance analyses, (2) subtrees for the fragments, (3) node substitution for combining subtrees, and (4), the sum of the probabilities of all distinct ways of generating an analysis as a def &apos ;mition of the probability of that analysis. Bod (1992, 1993a) shows that conventional context-free parsing techniques can be used in creating a parse forest for a sentence in DOP1. Data-Oriented Parsing Bothprobabil is tic and non-probabilistic DOP are based on the DOP model in Bod (1992) which extracts a Stochastic Tree-Substitution Grammar. Bod (1992) demonstrated that DOP can be implemented using conventional context-free parsing techniques. Next, parsing proceeds with the subtrees that are triggered by the dialogue context C (provided that all subtrees are converted into equivalent rewrite rules -see Bod 1992, Sima &apos; an 1995).
Three Generative Lexicalized Models For Statistical Parsing In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96). The last two counts (CAUS and ANIM) were performed on a 29-million word parsed corpus (gall Street Journal 1988, provided by Michael Collins (Collins, 1997)). These sentences were parsed with the Collins' parser (Collins, 1997). Substantial improvements have been made to parse western language such as English, and many powerful models have been proposed (Brill 1993, Collins 1997). This model is very similar to the markovized rule models in Collins (1997). Collins (1997)'s parser and its reimplementation and extension by Bikel (2002) have by now been applied to a variety of languages: English (Collins, 1999), Czech (Collins et al, 1999), German (Dubey and Keller, 2003), Spanish (Cowan and Collins, 2005), French (Arun and Keller, 2005), Chinese (Bikel, 2002) and, according to Dan Bikel's web page, Arabic. In particular, empty nodes (represented as -NONE in the tree bank) were turned into rules that generated the empty string (), and there was no collapsing of categories (such as PRT and ADVP) as is of ten done in parsing work (Collins, 1997, etc.). Answer Extraction: We select the top 5 ranked sentences and return them as Collins, 1997, can be used to capture the binary dependencies between the head of each phrase. For example, the lexicalized grammars of Collins (1997) and Charniak (1997) and the state split grammars of Petrov et al (2006) are all too large to construct unpruned charts in memory. Recently, it has gained renewed attention as empirical methods in parsing have emphasized the importance of relations between words (see, e.g., (Collins, 1997)), which is what dependency grammars model explicitly, but context-free phrase-structure grammars do not. The supervised component is Collins' parser (Collins, 1997), trained on the Wall Street Journal.  We use a mechanism similar to (Collins, 1997) but adapted to Chinese data to find lexical heads in the tree bank data. Judge et al (2006) produced a corpus of 4,000 questions annotated with syntactic trees, and obtained an improvement in parsing accuracy for Bikel's reimplementation of the Collins parser (Collins, 1997) by training a new parser model with a combination of newspaper and question data. In addition to portability experiments with the parsing model of (Collins, 1997), (Gildea, 2001) provided a comprehensive analysis of parser portability. In order to extract the linguistic features necessary for the model, all sentences were first automatically part-of-speech-tagged using a maximum entropy tagger (Ratnaparkhi, 1998) and parsed using the Collins parser (Collins, 1997). Collins (1997)'s parser and its re-implementation and extension by Bikel (2002) have by now been applied to a variety of languages: English (Collins, 1999), Czech (Collins et al. , 1999), German (Dubey and Keller, 2003), Spanish (Cowan and Collins, 2005), French (Arun and Keller, 2005), Chinese (Bikel, 2002) and, according to Dan Bikels web page, Arabic. Our model is thus a simplification of more sophisticated models which integrate PCFGs with features, such as those in Magerman (1995), Collins (1997) and Goodman (1997). This statistical technique of labeling predicate argument operates on the output of the probabilistic parser reported in (Collins, 1997). At last, the dependency parser presented in (Collins, 1997) is used to generate the full parse. For getting the syntax trees, the latest version of Collins' parser (Collins, 1997) was used.
LexPageRank: Prestige In Multi-Document Text Summarization Multidocument extractive summarization relies on the concept of sentence centrality to identify the most important sentences in a document. Centrality is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank. In this model, a sentence connectivity matrix is constructed based on cosine similarity. If the cosine similarity between two sentences exceeds a particular predefined threshold, a corresponding edge is added to the connectivity matrix. We provide an evaluation of our method on DUC 2004 data. The results show that our approach outperforms centroid-based summarization and is quite successful compared to other summarization systems. Earlier experiments with graph-based ranking algorithms for text summarization, as previously re ported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.   The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004). Lex PageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality.  Erkan and Radev (2004) and Yoshioka (2004) evaluate the relevance (similarity) between any two sentences first. We represent the sentences in A or B as a text graph constructed using the same approach as was used in Erkan and Radev (2004a, 2004b). Erkan and Radev (2004a and 2004b) represented the documents as a weighted undirected graph by taking sentences as vertices and cosine similarity between sentences as the edge weight function. In the recent years graph based techinques have become very popular in automatic text summarization (Erkan and Radev, 2004), (Mihalcea, 2005). Lex PageRank (Erkan and Radev, 2004) is one of such methods. TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. Note that although the MEAD distribution also includes an optional feature calculated using the LexRank graph-based algorithm (Erkan and Radev, 2004), this feature could not be used since it takes days to compute for very long documents such as ours, and thus its application was not tractable. Very briefly, the TextRank system (Mihalcea and Tarau, 2004) similar in spirit with the concurrently proposed LexRank method (Erkan and Radev, 2004) works by building a graph representation of the text, where sentences are represented as nodes, and weighted edges are drawn using inter-sentential word overlap. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. The underlying hypothesis of cross-document inference is that the salience of a fact should be calculated by taking into consideration both its confidence and the confidence of other facts connected to it, which is inspired by PageRank (Page et al, 1998) and LexRank (Erkan and Radev, 2004). Typical existing summarization methods include centroid-based methods (e.g., MEAD (Radev et al, 2004)), graph-ranking based methods (e.g., LexPageRank (Erkan and Radev, 2004)), non-negative matrix factorization (NMF) based methods (e.g., (Lee and Seung, 2001)), Conditional random field (CRF) based summarization (Shen et al, 2007), and LSA based methods (Gong and Liu, 2001). The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004).  
Towards Terascale Semantic Acquisition Although vast amounts of textual data are freely available, many NLP algorithms exploit only a minute percentage of it. In this paper, we study the challenges of working at the terascale. We present an algorithm, designed for the terascale, for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method. We focus on the accuracy of these two systems as a func tion of processing time and corpus size. Pantel et al (2004) proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performance and efficiency.  Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al, 2004). Lately there has been a lot of interest in acquiring such text patterns using a set of hypernymy examples ,e.g. Pantel et al (2004) and Snow (2006). However, following Pantel et al (2004), we assume that the recall of the baseline is 1 and estimate the relative recall RRS|B of the system S with respect to the baseline B using their respective precision scores PS and PB and number of instances extracted by them |S| and |B| as: RRS|B= PS? |S|PB? |B| 6.3 Gold Standard. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al,2004). KAON Text-To-Onto (Maedche and Staab, 2004) applies text mining algorithms for English and German texts to semi-automatically create an ontology, which includes algorithms for term extraction, for concept association extraction and for ontology pruning. Pattern-based approaches to extract hy ponym/hypernym relationships range from hand-crafted lexico-syntactic patterns (Hearst, 1992) to the automatic discovery of such patterns by e.g. a minimal edit distance algorithm (Pantel et al, 2004). Pantel et al (2004) extended is-a relation acquisition towards terascale, and automatically identified hypernym patterns by minimal edit distance. Many relationship classification methods utilize some language-dependent preprocessing, like deep or shallow parsing, part of speech tagging and 228 named entity annotation (Pantel et al, 2004). Useful semantic relations can be extracted from large corpora using relatively simple patterns (e.g., (Pantel et al, 2004)). Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al, 2004). The patterns we used for entailment acquisition based on (Hearst, 1992) and (Pantel et al, 2004). For example, the pattern& quot; NP1 ,a|an NP2& quot;, ranked among the top IS-A pat terns by (Pantel et al, 2004), can represent both apposition (entailing) and a list of co-hyponyms (non-entailing). Due to the need for POS tagging and/or parsing, these types of methods have been evaluated only on fixed corpora1, although (Pantel et al, 2004) demonstrated how to scale up their algorithms for the Web. Pantel et al, (2004) proposed, in the scenario of extracting is-a relations, one pattern-based approach and compared it with a baseline syntactic distributional similarity method (called syntactic co-occurrence in their paper). Patterns were shown to be very useful in all sorts of lexical acquisition tasks, giving high precision results at relatively low computational costs (Pantel et al, 2004).  Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al 2004). (Pantel et al 2004) reduce the depth of the linguistic data used but still requires POS tagging. Many papers directly target specific applications, and build lexical resources as a side effect. Named Entity Recognition can be viewed as an in stance of our problem where the desired categories contain words that are names of entities of a particular kind, as done in (Freitag, 2004) using co clustering. Note how small they are, when compared to (Pantel et al 2004), which took 4 days for a smaller corpus using the same CPU.
The NomBank Project: An Interim Report This paper describes NomBank, a project that will provide argument structure for instances of common nouns in the Penn Treebank II corpus. NomBank is part of a larger effort to add additional layers of annotation to the Penn Treebank II corpus. The University of Pennsylvania’s PropBank, NomBank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text. This paper describes the NomBank project in detail including its specifications and the process involved in creating the resource. The Chinese Nombank extends the general annotation framework of the English Proposition Bank (Palmer et al, 2005) and the English Nombank (Meyers et al, 2004) to the annotation of nominalized predicates in Chinese. NomBank (Meyers et al, 2004) is a similar resource for nominal predicates, but we do not consider it in our experiments. We then describe a novel CCG analysis of NP predicate argument structure, which we implement usingNomBank (Meyers et al, 2004). We currently do not have an analysis that allows support verbs to supply noun arguments, so we do not recover any of the long-range dependency structures described by Meyers et al (2004). Our analysis requires semantic role labels for each argument of the nominal predicates in the Penn Treebank precisely what NomBank (Meyers et al, 2004) provides. We have a list of approximately 4000 deverbal noun/ verb pairs, constructed from a combination of WordNet? s derivational links (Fellbaum,1998), NomLex (Macleod et al, 1998), NomLexPlus (Meyers et al, 2004b) and some independent curation. In recent years, NomBank (Meyers et al,2004a) has provided a set of about 200,000 manually annotated instances of nominalizations with arguments, giving rise to supervised machine learned approaches such as (Pradhan et al, 2004) and (Liu and Ng, 2007), which perform fairly wellin the overall task of classifying deverbal arguments. To extract relations we used the parser by Johansson and Nugues (2008) to annotate sentences with dependencies and shallow semantics in the PropBank (Palmer et al, 2005) and NomBank (Meyers et al, 2004) frameworks. Both parsing is formulated as a single-stage word-pair classification problem, and the latter is carried out by a search through the NomBank (Meyers et al, 2004) or the PropBank (Palmer et al, 2005). Within the context of NomBank, a project dedicated to annotation of argument structure, Meyers et al (2004a) describe the linguistics of nominalizations ,emphasizing semantic roles. This is a purely syntactic resource, but we can also include this tree bank in the category of multistratal resources since the PropBank (Palmer et al, 2005 ) and NomBank (Meyers et al, 2004) projects have an notated shallow semantic structures on top of it. In English predicate argument structure analysis, large corpora such as FrameNet (Fillmore et al, 2001), PropBank (Palmer et al, 2005) and NomBank (Meyers et al, 2004) have been created and utilized. As a complement to PropBank, NomBank (Meyers et al,2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. One of the most popular, semantic role labels (annotation and transducers based on the annotation) characterize relations anchored by select predicate types like verbs (Palmer et al, 2005), nouns (Meyers et al., 2004a), discourse connectives (Miltsakaki et al, 2004) or those predicates that are part of particular semantic frames (Baker et al, 1998). These features are marked in the NOMLEX-PLUS dictionary (Meyers et al, 2004b). NomBank annotation (Meyers et al., 2004) uses essentially the same framework as PropBank to annotate arguments of nouns. The PASbio (Wattarujeekrit et al, 2004) proposes Predicate Argument Structures (PASs), a type of linguistically-oriented semantic structures, for domain-specific lexical items, based on PASs defined in PropBank (Wattarujeekrit et al, 2004 ) and NomBank (Meyers et al, 2004). For predicate argument structure analysis, we have the following representative large corpora: FrameNet (Fillmore et al,2001), PropBank (Palmer et al, 2005), and NomBank (Meyers et al, 2004) in English, the Chinese PropBank (Xue, 2008) in Chinese, the GDA Corpus (Hashida, 2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al, 2002), and the NAIST Text Corpus (Iida et al, 2007) in Japanese. The NomBank project (Meyers et al, 2004) provides coarse annotations for some of the possessive con st ructions in the Penn Treebank, but only those that meet their criteria. A principled solution to this problem is to use an SRL system for nominal predicates trained using NomBank (Meyers et al., 2004).
ONE SENSE PER COLLOCATION David Yarowsky* Department  of  Computer  and In format ion Science Univers i ty of  Pennsy lvania Philadelphia, PA 19104 yarowsky@unagi .c is .upenn.edu ABSTRACT Previous work [Gale, Church and Yarowsky, 1992] showed that with high probability a polysemous word has one sense per discourse.  It is well known that polysemous words usually have only one sense when used as part of a collocation or technical term (Yarowsky 1993). The procedure for inducing these semantic frames is as follows: 1. apply dependency parsing to a raw corpus and extract predicate-argument structures for each verb from the automatic parses, 2. merge the predicate-argument structures that have presumably the same meaning based on the assumption of one sense per collocation (Yarowsky, 1993) to get a set of initial frames, and 3. apply clustering to the initial frames based on the Chinese Restaurant Process (Aldous, 1985) to produce verb-specific semantic frames. For this merge, we assume one sense per collocation (Yarowsky, 1993) for predicate argument structures.  Much of the research in this area has been compromised by the fact that researchers have focussed on lexical ambiguities that are not true word sense distinctions, such as words translated differently across two languages (Gale, Church, and Yarowsky, 1992) or homophones (Yarowsky, 1993).    Following Yarowsky (1993), who explicitly addresses the use of collocations in the WSD work, we adopt his definition, adapted to our purpose: A collocation is a co-occurrence of two words in a defined relation.  These methods consist of simple rules that can reliably assign a sense to certain word categories: one sense per collocation (Yarowsky, 1993), and one sense per discourse (Gale et al, 1992). In the early nineties two famous papers claimed that the behavior of word senses in texts adhered to two principles: one sense per discourse (Gale et al, 1992) and one sense per collocation (Yarowsky, 1993). In order to analyze and compare the behavior of several kinds of collocations (cf. Section 3), Yarowsky (1993) used a measure of entropy as well as the results obtained when tagging heldout data with the collocations organized as decision lists (cf. Section 4). Compared to Yarowsky (1993), who also took into account grammatical relations, we only share the content-word-to-left and the content-word-to-right collocations. It is not clear how the smoothing technique proposed in (Yarowsky, 1993) could be extended to away ambiguities. Collocations for fine-gained word-senses are sensibly weaker than those reported by Yarowsky (1993) for two-way ambiguous words. This paper shows that the one sense per collocation hypothesis is weaker for fine grained word sense distinctions (e.g. those in WordNet): from the 99% precision mentioned for 2-way ambiguities in (Yarowsky, 1993) we drop to 70% figures. A well-known issue in the WSD area is the one sense per collocation claim (Yarowsky, 1993) stating that the word meanings are strongly associated with the particular collocation in which the word is located. We admit here that, while we have been aware of the fact for long time, only after the dissemination of the closely related hypotheses of one sense per discourse (Gale, Church and Yarowsky 1992) and one sense per collocation (Yarowsky 1993), we are able to articulate the hypothesis of one tokenization per source. Yarowsky (1993) indicated that the objects of verbs play a more dominant role than their subjects in WSD and nouns acquire more stable disambiguating information from their noun or adjective modifiers.
Char Align: A Program For Aligning Parallel Texts At The Character Level have been a number of recent papers on aligning parallel texts at the sentence level, e.g., Brown al and Church (to appear), Isabelle (1992), Kay and Rosenschein (to appear), Simard al Warwick— Armstrong and Russell (1990). On clean inputs, such as the Canadian Hansards, these methods have been very successful (at least 96% correct by sentence). Unfortunately, if the input is noisy (due to OCR and/or unknown markup conventions), then these methods tend to break down because the noise can make it difficult to find boundaries, let alone sentences. This paper describes a new program, aligns texts at the level rather than at the sentence/paragraph level, based on the cognate approach proposed by Simard al. The program takes the output of char_align (Church, 1993), a robust alternative to sentence-based alignment programs, and applies word-level constraints using a version of Brown el al.'s Model 2 (Brown et al, 1993), modified and extended to deal with robustness issues.  The work by Simard, Foster and Isabelle (1993) as well as Church (1993) demonstrated that cognate-matching strategies can be highly effective in aligning text. Church (1993) uses 4-grams at the level of character sequences. Using lexical information, Kenneth Church (1993) showed that cheap alignment of text segments was still possible exploiting orthographic cognates (Michel Simard et al, 1992), instead of sentence delimiters. We use the text Dotplotting representation by (Church, 1993) and plot the cosine similarity scores between every pair of sentences in the text. Simard and Plamondon (Simard and Plamondon, 1998) used a composite method in which the first pass does alignment at the level of characters asin (Church, 1993) (itself based on cognate matching) and the second pass uses IBM Model-1, following Chen (Chen, 1993).  In previous work (Church et al 1993), we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual (Aho, Kernighan, Weinberger (1980)), using char align (Church, 1993), a method that looks for character sequences that are the same in both the source and target. Canadian Hansards that has been used in a number of other studies: Church (1993) and Simard et al (1992). This algorithm was applied to a fragment of the Canadian Hansards that has been used in a number of other studies: Church (1993) and Simard et al (1992). Currently ,word_align depends on char align (Church, 1993) to generate a starting point, which limits its applicability to European languages since char_align was designed for language pairs that share a common alphabet. Church (1993) observes that reliably distinguishing sentence boundaries for a noisy bi text obtained from an OCR device is quite difficult. The method uses length balance based alignment algorithm i.e. GaleChurch (Gale and Church, 1993), for the data collecting. Gale and Church (1993) describe a method for aligning sentences based on a simple statistical model of sentence lengths measured in number of characters. Levenshtein measure (Levenshtein, 1966) Church (1993) employs a method that induces sentence alignment by employing cognates (words that are spelled similarly across languages).
Cut And Paste Based Text Summarization We present a cut and paste based text summarizer, which uses operations derived from an analysis of human written abstracts. The summarizer edits extracted sentences, using reduction to remove inessential phrases and combination to merge resulting phrases together as coherent sentences. Our work includes a statistically based sentence decomposition program that identifies where the phrases of a summary originate in the original document, producing an aligned corpus of summaries and articles which we used to develop the summarizer. The recent approach for editing extracted text spans (Jing and McKeown,2000) may also produce improvement for our algorithm. Jing and McKeown (2000) first extract sentences, then remove redundant phrases, and use (manual) recombination rules to produce coherent output. Mean ratings for automatic compressionsnally, we added a simple baseline compression algorithm proposed by Jing and McKeown (2000) which removed all prepositional phrases, clauses, to infinitives, and gerunds. Like the work of Jing and McKeown (2000) and Mani et al (1999), our work was inspired by the summarization method used by human abstractors. Our two-step model essentially belongs to the same category as the works of (Mani et al, 1999) and (Jing and McKeown, 2000). We found that the deletion of lead parts did not occur very often in our summary, unlike the case of Jing and McKeown (2000). The task of sentence compression (or sentence reduction) can be defined as summarizing a single sentence by removing information from it (Jing and McKeown, 2000). First, splitting and merging of sentences (Jing and McKeown, 2000), which seems related to content planning and aggregation. Rewrite operations other than deletion tend to be hand-crafted and domain specific (Jing and McKeown, 2000). Jing and McKeown (2000) studied what edits people use to create summaries from sentences in the source text. Existing work in abstractive summarization has been quite limited and can be categorized into two categories: (1) approaches using prior knowledge (Radev and McKeown, 1998) (Finley and Harabagiu, 2002) (DeJong, 1982) and (2) approaches using Natural Language Generation (NLG) systems (Saggion and Lapalme, 2002) (Jing and McKeown, 2000). Close to the problem studied here is Jing and McKeown's (Jing and McKeown,2000) cut-and-paste method founded on Endres Niggemeyer's observations. Jing and McKeown (2000) manually analyzed 30 human-written summaries, and find that 19% of sentences can not be explained by cut-and-paste operations from the source text.
An Efficient Probabilistic Context-Free Parsing Algorithm That Computes Prefix Probabilities We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities. Probabilities (a) and (b) are computed incrementally in a single left-to-right pass over the input. Our algorithm compares favorably to standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars by making use of Earley's top-down control structure. It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs. This adaptation, related to that of (Stolcke 1995), involves reformulating the Earley algorithm to work with probabilistic recursive transition networks rather than with deterministic production rules. For the Berkeley grammar, we use a probabilistic Earley parser modified by Levy to calculate exact prefix probabilities using the algorithm of Stolcke (1995). Efficient algorithms for its solution have been proposed by Jelinek and Lafferty (1991) and Stolcke (1995). Nonetheless, the partition function can still be approximated to any degree of precision by iterative computation of the relation in (4), as done for instance by Stolcke (1995) and by Abney et al (1999). Note that the algorithms for the computation of prefix probabilities by Jelinek and Lafferty (1991) and Stolcke (1995) do allow incrementality, which contributes to their practical usefulness for speech recognition. Two of these, P(X) and S(X) are just the prefix and suffix probability distributions for the symbol(Stolcke, 1995): the probabilities that the string derived from begins (or ends) with a particular tag. Stolcke (1995) summarizes extensively their approach to utilize probabilistic Earley parsing. The above task has some resemblance to probabilistic context-free grammar (PCFG) parsing for which efficient algorithms are available (Stolcke, 1995), but we note that our task of finding the most probable semantic derivation differs from PCFG parsing in two important ways. Therefore, we use an Earley-style probabilistic parser, which outputs Viterbi parses (Stolcke, 1995). A third approach is to calculate the forward probability (Stolcke, 1995) of the sentence using a PCFG. The agenda algorithm does this by iterative approximation (propagating updates around any cycles in the proof graph until numerical convergence), essentially as suggested by Stolcke (1995) for the case of Earley's algorithm. In some special cases only a linear solver is needed: e.g., for unary rule cycles (Stolcke, 1995). Monolingual parsing with unary productions is fairly straightforward (Stolcke,1995), however in the transductive setting these rules can licence infinite insertions in the target string. An Earley chart is used for keeping track of all derivations that are consistent with the input (Stolcke, 1995). This problem has been studied by Jelinek and Lafferty (1991) and by Stolcke (1995) . Our solution to the problem of computing prefix probabilities is formulated in quite different terms from the solutions by Jelinek and Lafferty (1991) and by Stolcke (1995) for probabilistic context-free grammars. This contrasts with the techniques proposed by Jelinek and Lafferty (1991) and by Stolcke (1995), which are extensions of parsing algorithms for probabilistic context-free grammars, and require considerably more involved proofs of correctness. An additional complication with our construction is that finding any of the values in (3) may involve solving a system of non-linear equations, similarly to the case of probabilistic context-free grammars; see again Abney et al (1999), and Stolcke (1995). These quantities can be computed to any degree of precision, as discussed for instance in (Booth and Thompson, 1973) and (Stolcke, 1995). A probabilistic Earley parser can retrieve all possible derivations at (Stolcke, 1995).
Edge-Based Best-First Chart Parsing Best-first probabilistic chart parsing attempts to parse efficiently by working on edges that are judged &quot;best&quot; by some probabilistic figure of merit (FOM). Recent work has used probabilistic context-free grammars (PCFGs) to assign probabilities to constituents, and to use these probabilities as the starting point for the FOM. This paper extends this approach to using a probabilistic FOM to judge edges (incomplete constituents), thereby giving a much finergrained control over parsing effort. We show how this can be accomplished in a particularly simple way using the common idea of binarizing the PCFG. The results obtained are about a facof twenty improvement over the best results — that is, our parser achieves equivalent results using one twentieth the number of edges. Furthermore we show that this improvement is obtained with parsing precision and recall levels superior to those achieved by exhaustive parsing. Some of the data comes from the parsed files 2-21 of the Wall Street Journal Penn Treebank corpus (Marcus et al, 1993), and additional parsed text was obtained by parsing the Wall Street Journal text using the parser described in Charniak et al (1998). Using this information, the model described in (Charniak et al 1998) is P (s|h, t, l). Charniak et al (1998) and Caraballo and Charniak (1998) showed that, when seeking the best parse (using min= or max=), best-first parsing can be extremely effective. Therein, the idea of coarse-to-fine parsing (Charniak et al, 1998) is extended to handle the repeated parsing of the same sentences. One paper that focuses on efficiency of statistical parsing is Charniak et al (1998). Charniak et al (1998) introduce overparsing as a technique to improve parse accuracy by continuing parsing after the first complete parse tree is found. This binarization process is similar to the one described in (Charniak et al, 1998). Here, we observe an effect seen in previous work (Charniak et al (1998), Petrov and Klein (2007), Petrov et al (2008)), that a certain amount of pruning helps accuracy, perhaps by promoting agreement between the coarse and full grammars (model intersection). Two grammars are equivalent if they define the same probability distribution over strings (Charniak et al, 1998). As shown in Charniak et al (1998), we can binarize explicitly and use intermediate symbols to replace dotted rules in chart parsing. Chitrao and Grishman (1990), Caraballo and Charniak (1998), Charniak et al (1998), and Collins (1999) describe best-first parsing, which is intended for a tabular item-based framework. I-TRIE is a non-deterministic left branching trie with weights on rule entry as in Charniak et al (1998). Following (Charniak et al, 1998), we parsed unseen sentences of length 18-26 from the Penn Treebank, using the grammar induced from the remainder of the treebank. On the other hand, the more complex, tuned FOM in (Charniak et al, 1998) is able to parse all of these sentences using around 2K edges, while BF requires 7K edges. The complex FOMs in (Charniak et al, 1998) require somewhat more online computation to assemble.  In the training phase, each target-style parse tree in the training data is transformed into a binary tree (Charniak et al, 1998) and then decomposed into a (golden) action-state sequence. This measure is used by Charniak et al (1998) and Klein and Manning (2003b). With respect to chart parsing, (Charniak et al, 1998) report that their parser can achieve good results while producing about three times tile mininmm number of edges required to produce the final parse. Related approaches are used in Hall (2004) and Charniak and Johnson (2005).
Structured Models for Fine-to-Coarse Sentiment Analysis In this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Inference in the model is based on standard sequence classification techniques using constrained Viterbi to ensure consistent solutions. The primary advantage of such a model is that it allows classification decisions from one level in the text to influence decisions at another. Experiments show that this method can significantly reduce classification error relative to models trained in isolation. We used the best-performing model that fuses HCRF-Coarse and the supervised model (McDonald et al2007) by interpolation. McDonald et al (2007) also dealt with sentiment analysis, via the global joint-structural approach. More recently, McDonald et al (2007) have investigated a model for jointly performing sentence and document-level sentiment analysis, allowing the relationship between the two tasks to be captured and exploited. The data used in our initial English-only experiments were a set of 554 consumer reviews described in (McDonald et al, 2007). Finally, solutions that attempt to handle the error propagation problem have done so by explicitly optimizing for the best combination of document and sentence-level classification accuracy (McDonald et al, 2007). Alternative approaches include explicitly ac counting for this structure by treating subjective sentence extraction as a sequence-labeling problem, such as in McDonald et al (2007). McDonald et al (2007) propose a model which jointly identifies global polarity as well as paragraph and sentence-level polarity, all of which are observed in training data. While our approach uses a similar hierarchy, McDonald et al (2007) is concerned with recovering the labels at all levels, whereas in this work we are interested in using latent document content structure as a means to benefit task predictions. Subsequently, many other studies make efforts to improve the performance of machine learning-based classifiers by various means, such as using subjectivity summarization (Pang and Lee, 2004), seeking new superior textual features (Riloff et al, 2006), and employing document subcomponent information (McDonald et al, 2007). NGram Back-off Features: Similar to McDonald et al (2007), we utilize backed-off versions of lexical bi grams and trigrams, where all possible combinations of the words in the ngram are replaced by their POS tags, creating features such as w j POS k, POS j w k, POS j POS k for each lexical bigram and similarly for trigrams. Most recently, McDonald et al (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity.  Moreover, the assigned tag applies to the whole blog post while a finer grained sentiment extraction is needed (McDonald et al, 2007). McDonald et al (2007) later showed that jointly learning fine-grained (sentence) and coarse-grained (document) sentiment improves predictions at both levels. McDonald et al (2007) introduced a fully supervised model in which predictions of coarse-grained (document) and fine-grained (sentence) sentiment are learned and inferred jointly. But even in such approaches, McDonald et al (2007) note that information about the overall sentiment orientation of a document facilitates more accurate extraction of more specific information from the text. Quantitatively, subjective sentences in the product reviews amount to 78% (McDonald et al, 2007), while subjective sentences in the movie review dataset are only about 25% (Mao and Lebanon, 2006). McDonald (McDonald et al 2007) has reported some success mixing fine and course labeling in sentiment analysis. Most recently, McDonald et al (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. For example, with CRFs, Zhao et al (2008) and McDonald et al (2007) performed sentiment classification in sentence and document level.
Ontological Promiscuity To facilitate work in discourse interpretation, the logical form of English sentences should be both close to English and syntactically simple. In this paper I propose • logical notation which is first-order and nonintensional, and for which semantic translation can be naively compositional. The key move is to expand what kinds of entities one allows in one's ontology, rather than complicating the logical notation, the logical form of sentences, or the semantic translation process. Three classical problems adverbials, the distinction between and dicto belief reports, and the problem of identity in intensional contexts are examined for the difficulties they pose for this logical notation, and it is shown that the difficulties can be overcome. The paper closes with a statement about the view of semantics that is presupposed by this approach. The real problem in natural language processing is the interpretation of discourse. Therefore, the other aspects of the total process should be in the service of discourse interpretation. This includes the semantic translation of sentences into a logical form, and indeed the logical notation itself. Discourse interpretation processes, as I see them, are inferential processes that manipulate or perform deductions on logical expressions encoding the information in the text and on other logical expressions encoding the speaker's and hearer's background knowledge. These considerations lead to two principal criteria for • logical notation. Criterion I: The notation should be as close to English as possible. This makes it easier to specify the rules for translation between English and the formal language, and also makes it easier to encode in logical notation facts we normally think of in English. The ideal choice by this criterion is English itself, but it fails monumentally on the second criterion. Criterion II: The notation should be syntactically simple. Since discourse processes are to be defined primarily in terms of manipulations performed on expressions in the logical notation, the simpler that notation, the easier it will be to define the discourse operations. The development of such a logical notation is usually taken to be a very hard problem. I believe this is because researchers have imposed upon themselves several additional constraints to adhere to stringent ontological scruples, to explain a number of mysterious syntactic facts as a by-product of the notation, and to encode efficient deduction techniques in the notation. Most representational difficulties go away if one rejects these constraints, and there are good reasons for rejecting each of the constraints. Ontological scruples: Researchers in philosophy and linguistics have typically restricted themselves to very few (although a strange assortment of) kinds of entities physical objects, numbers, sets, times, possible worlds, propositions, events, and situations and all of these but the first have been controversial. Quine has been the greatest exponent of ontological chastity. His argument is that in any scientific theory, we adopt, at least insofar as we are reasonable, the simplest conceptual scheme into which the disordered fragments of our experience can be fitted arranged.&quot; (Quine, 1953, P.16.) But he goes on to say that 'simplicity ... is not a clear and unambiguous idea; and it is quite capable of presenting a double or multiple standard.&quot; 17.) Minimizing kinds of entities is not the only way to achieve simplicity in a theory. The aim in this enterprise is achieve simplicity by minimizing the complexity of the in the system. It turns out this can be achieved by multiplying kinds of entities, bY allowing as an entity everything that can be referred to by a noun phrase. Syntactic explanation: The argument here is easy. It would be pleasant if an explanation of, say, the syntactic behavior of count nouns and mass nouns fell out of our underlying ontological stricture at no extra cost, but if the extra cost is great complication in statements of discourse operations, it would be quite unpleasant. In constructing a theory of discourse interpretation, it doesn't make sense for us to tie our hands by requiring syntactic explanations as well. The problem of discourse is at least an order of magnitude harder than the problem of syntax, and syntax shouldn't be in the driver's seat. Efficient deduction: There is a long tradition in artificial intelligence of building control information into the notation, and indeed much work in knowledge representation is driven by consideration. Semantic networks and other notational sysaround hierarchies (Quillian, 1968; Simmons, Hendrix, 1975) implicitly assign a low cost to certain types of syllogistic reasoning. The KL-ONE representation language and Brachman, 1982) has a variety of notational dewith an associated efficient deduction procedure. Hayes (1979) has argued that frame representations (Minsky, 1975; Bobrow and Winograd, 1977) should be viewed as sets of predicate calculus axioms together with a control component for drawing certain kinds of inferences quickly. In quite a different vein, Moore (1980) uses a possible worlds notation to model and action in part to avoid inefficiencies in theorem- 61 proving. By contrast, I would argue against building efficiencies into the notation. From a psychological point of view, this allows us to abstract away from the details of implementation on a particular computational device, increasing the generality of the theory. From a technological point of view, it reflects a belief that we must first determine empirically the most common classes of inferences required for discourse processing and only then seek algorithms for optimizing them. In this paper I propose a flat logical notation with an ontologically promiscuous semantics. One's first naive guess as to how to represent a simple sentence like A boy builds a boat. is as follows: This simple approach seems to break down when we encounter the more difficult phenomena of natural language, like tense, intensional contexts, and adverbials, as in the sentence A boy wanted to build a boat quickly. These phenomena have led students of language to introduce significant complications in their logical notations for representing sentences. My approach will be to maintain the syntactic simplicity of the logical notation and expand the theory of the world implicit in the semantics to accommodate this simplicity. The representation of the above sentence, as is justified below, is es, es, y)Paagel z, es)Aquickqes, es) y) occurred in the past, where is es, is the quickness of es, which is z's building of a boy and a boat. In brief, the logical form of natural language sentences will be a conjunction of atomic predications in which all variables are existentially quantified with the widest possible scope. Predicates will be identical or nearly identical to natural language morphemes. There will be no functions, functional., nested quantifiers, disjunction., negations, or modal or intensional operators. 2 The Logical Notation Davidson (1967) proposed a treatment of action sentences in which events are treated as individuals. This facilitated the representation of sentences with time and place adverbials. Thus we can view the sentences John ran on Monday. John ran in San Francisco. as asserting the existence of a running event by John and asserting a relation between the event and Monday or San Francisco. We can similarly view the sentence John ran slowly. as expressing an attribute about a running event. Treating events as individuals is also useful because they can be arguments of statements about causes: Because he wanted to get there first, John ran. Because John ran, he arrived sooner than anyone else. They can be the objects of propositional attitudes: Bill was surprised that John ran. this approach accomodates the facts that events can nominalized and can be referred to pronominally: John's running tired him out. John ran, and Bill saw it. But virtually every predication that can be made in natural language can be specified as to time and place, be modified adverbially, function as a cause or effect of something else, be the object of a propositional attitude, be nominalized, and be referred to by a pronoun. It is therefore convenient to extend Davidson's approach to all predications. That is, corresponding any predication that be made in natural language, we will say there is an event, or state, or condition, or situation, or *eventuality&quot;, or whatever, in the world that it refers to. approach might be called 'ontological One ontological scruples. Thus we would like to have in our logical notation the possibility of an extra argument in each predication referring to the &quot;condition&quot; that exists when that predication is true. However, especially for expository convenience, we would like to retain the option of not specifying that extra argument when it is not needed and would only get in our way. Hence, I propose a logical that provides two predicates that are systemby introducing might be called a '. Corresponding io n-ary predicate there will he an n + first. argument be thought of as condition that holds when p is true the subsequent arguments. Thus, run(J) means that John ( E J) that running event by John. or running. If slippery(F) means that floor slippery, F) that condition of F's being slippery, or F's slipperiness. The effect of this notational mato provide handles by which various can grasped by higher predications. similar approach has been used in many Al systems. discourse one not only makes such epheevents, states and conditions. also refers to entities do not actually Our notation must thus have a way referring to such entities. We therefore take model to be a Platonic universe which contains everything that can be spoken of objects, events, states, conditions whether they exist in the real world or not. It then may or may not be a property of such entities that they exist in the real world. In the sentence (1) John worships Zeus, the worshipping event and John, but not Zeus, exist in the real world, but all three exist in the (overpopulated) Platonic universe. Similarly, in John wants to fly. 62 John's flying exists in the Platonic universe but not in the real The logical notation then is just first-order predicate calculus, where the universe of discourse is a rich set of individuals, which are real, possible and even impossible objects, events, conditions, eventualities, and so on. Existence and truth in the actual universe are treated as predications about individuals in the Platonic universe. For this purwe use a predicate formula J 0 N ) says the individual in the Platonic universe denoted JOHN in the actual The formula Exist(E) runi(E, the condition John's running exists the actual universe, or more simply that &quot;John runs&quot; is true, or still more simply, that John runs. A shorter way to write it is N). Although for a simple sentence like 'John runs&quot;, a logical form like (2) seems a bit overblown, when we come to real sentences in English discourse with their variety of tenses, modalities and adverbial modifiers, the more elaborated logical form is necessary. Adopting the notation of (2) has the effect of splitting a into its propositional content rtire(E, and assertional claim - E xist( E). This frequently out to be useful, as the latter is often in doubt until substantial work has been done by discourse interpretation processes. An entire sentence may be embedded within an indirect proof or other extended counterfactual. We are now in a position to state formally the systematic relation between the unprimed and primed predicates as an axiom schema. For every n-ary predicate p, (Vi, D (36)Ezisf(c)AptIc, is, p is true of , there is a condition e of p's true of and e exists. Conversely, , , , is. if e is the condition of p's being true of then p is true of can compress these axiom schemas into one formula: :=7 A sentence in English asserts the existence of one or more eventualities in the real world, and this may or may not imply the existence of other individuals. The logical form of sentence (1) is JOHN, ZEUS) H N) but not Exist(Z S). Similarly. form of &quot;John wants to fly&quot; is need not adhere to Platonism to accept Platonic universe. It be viewrti as a socially constituted, or conventional, construction, which is nevertheless highly constrained by the way the (not directly accessible) world is. The degree of constraint is variable. We constrained by the material world to believe in trees and chairs, less so to believe in patriotism or ghosts. 'The reader might choose to think of the Platonic universe as the universe possible individuals, although I do not want to exclude impossible individuals, such as the condition John believes to exist when he believes 6 + 7 = 15. 'McCarthy (1977) employs a similar technique. JOHN) Exist(J0 N) but not When the existence of the condition corresponding to some predication implies the existence of one of the arguments of the predication, we say that the predicate is transparent in that argument, worship and want are transparent in their first arguments and opaque in their second arguments. In if a predicate p is transparent in its nth argument this can be encoded by the axiom is, if e is p's being true of x e then Equivalently, z, ...) In the absence of such axioms, predicates are assumed to be opaque. The following sentence illustrates the extent to which we must have a way of representing existent and nonexistent states and ordinary discourse. (4) The government has repeatedly refused to deny that Prime Minister Margaret Thatcher vetoed the ltannel Tunnel at summit meeting with President Mitterand on 18 Scientist last week.&quot; In addition to the ordinary individuals Margaret Thatcher and Mitterand and the corporate entity are intervals of timr 18 May and the entity, the 'hannel Tunnel. an iii (i', event and the complex event if I he ing, actually occurred, a set, of real refusals distributed across time in a particular way, a denial event which did not occur, and a event which may or may not us Post( mean that existed in t he past f ect( mean what the perfect roughly. existed in the past. and may not yet be iltlj liii The representation of just the verb, nominalit ations. adverbials and of ) is as El) K 1) f denyl (;OUT, E3) A ) A ( ) 18M U )A VS. Ks) week( Of the variotis entities referred to, the sentence. via Imprinted asserts the existence of a typiriel refusal in a ,et refusals anti the revelation Tile ext,tence the existence (if the ov,'rnmIni,'iiI Ii (,,', not howl% the of the denial; (nine the oppte-tie Ii way •■14..■!..7t.-1 of the veto, but certainly doe, Hot i pl■ it . The revelaimplies the existence of both the Scientist properly, we should say &quot;existentially transparent&quot; and opaque&quot;, since this notion not coincide exactly with referential transparency. in this notation is aiways entities in the Platonic uni• Existence in real world is expressed by predicates, in particular predicate sentence is taken from the 1%2 indebted to Paul Martin calling to attention. 63 the at relation E4, which in turn implies the existence of the veto and the meeting. These then imply the existence of Thatcher President Mitterand not Channel course, we know about the existence of some of these entities, such as Margaret Thatcher and President Mitterand, for reasons other than the transparency of predicates. Sentence (4) shows that virtually anything can be embedded in a higher predication. This is the reason, in the logical notation, for flattening everything into predications about individuals. There are four serious problems that must be dealt with if this approach is to work quantifiers, opaque adverbials, the between and dicto of belief reports, and the problem of identity in intensional contexts. I have described a solution to the quantifier problem elsewhere (Hobbs, 1983). Briefly, universally quantified variables are reified as typical elements of sets, existential quantification inside the scope of universally quantified variables are handled by means of dependency functions, and the quantifier structure of sentences is encoded in indices on predicates. In this paper I will address only the other three problems in detail. 3 Opaque Adverbials It seems reasonably natural to treat transparent adverbials as properties of events. For opaque adverbials, like 'almost&quot;, it seems less natural, and one is inclined to follow Reichenbach (1947) in treating them as functionals mapping predicates into predicates. Thus, John is almost a man. would be represented almost(man)(/) That is, almost maps the predicate man into the predicate &quot;almost a man', which is then applied to John. This representation is undesirable for our purposes since it is not first-order. It would be preferable to treat opaque operators as we do transparent ones, as properties of events or conditions. The sentence would be represented J) But does this get us into difficulty? First note that this representation does not imply that John is a man, for we have not asserted E's existence in the real world, and almost is opaque and does not imply its argument's existence. is there enough information in allow one to determine truth value of isolation; without appeal to other facts? The answer is that there could be. We can construct model in which for every functional is a corresponding predicate that z)) The existence of the model shows that this condition is not necessarily contradictory. the universe of discourse the class of finite sets built out of a finite set of urelements. The interpretation of a constant be some element of call it I(X). interpretation a monadic predicate p will a subset of call it I(p). such that plE, X), we define the interpretation of < 1(p), >. suppose we have a functional predicates into We can define the corresponding predicate be such that true if there are a predicate p and a constant the interpretation of is < 1(p), I(X) > true. fact that we can define such a predicate a moderately rich model means that we are licensed to treat opaque adverbials as properties of events and conditions. The purpose of this exercise is only to show the viability of approach. I am not claiming that a running event pair of the runner and the all runners, although it should be harmless enough for those irredeemably committed to set-theoretic semantics to view it like that. It should be noted that this treatment of adverbials has confor the individuating eventualities. We can say &quot;John is almost a man&quot; without wishing to imply 'John is almost a mammal,&quot; so we would not want to say that John's bea man is the being mammal. We are forced, though not unwillingly, into a position of individuating eventualities according to very fine-grained criteria. Re Dicto Reports The next problem concerns the distinction (due to Quine (1956)) ditto reports. A belief report like (5) John believes a man at the next table is a spy. two interpretations. The diet° likely in the circumstance in which John and some man are at adjacent and John observes suspicious behavior. re interlikely if some man is sitting at the table next the speaker of the sentence, and John is nowhere around but knows man otherwise and suspects him to be a A sentence very nearly forces the re believes Bill's mistress is whereas the sentence believes Russian consulate are spies. indicates a dicto the de re reading (5), John is not necessarily taken to know that the man is in at the next table, but he is assumed to to the man somehow. More on below. In the John believes there is a who is both at the table and a spy, but may be unable to identify man. The re (5) usually taken to support the inference There is someone John believes to be a the supports the weaker inference (7) John believes that someone is a spy. is due to Moore and Hendrix (1982). 64 As Quine has pointed out, as usually interpreted, the first of these sentences is false for most of us, the second one true. A common notational maneuver (though one that Quine rejects) is to represent this distinction as a scope ambiguity. Sentence (6) is encoded as (8) and (7) as (9): (8) (3x)believe(J , spy(x)) (9) believe(J,(3x)spy(x)) If one adopts this notation and stipulates what the expressions mean, then there are certainly distinct ways of representing the two sentences. But the interpretation of the two expressions is not obvious. It is not obvious for example that (8) could not cover the case where there is an individual such that John believes him to be a spy but has never seen him and knows absolutely nothing else about him not his name, nor his appearance. nor his location at any point in time beyond the fact that he is a spy. In fact, the notation we propose takes (8) to be the most representation. Since quantification over in the Platonic universe, (8) says that there is some entity in the Platonic universe such that John believes of that entity that it is a spy. Expression (8) commits us to no other beliefs on the part of John. When understood in this way, expression (8) is representation of what is conveyed in de dirto report. Translated into the flat notation and introducing a constant for the existentially quantified variable, (8) becomes believe( J P) S) Anything else that John believes about this entity must be explicitly. In particular, the dicto of be represented by something like believe(J, P) S) J , Q) , S,T) the next table. That is, John believes that a and that the table. John may know many other about still fall short of knowing is. There is a range of possibilities for John's knowledge, from hare statements of (10) and (11) that correspond to a to the full-blown knowledge of S's identity that is present in a In fact, an FBI agent would progress through just such a range of belief states on his way to identifying the spy. To state John's knowledge of S's identity properly, we would have to state explicitly John's belief in a potentially very large collection of properties of the spy. To arrive at a succinct way of representing knowledge of identity in our notation, let its contwo of equivalent sentences: that? Identify that. The FBI doesn't know who the spy is. The FBI doesn't know the spy's identity. The answer to the question 'Who are you?&quot; and what is rebefore we can say that we know is or that we know their identity is a highly context-dependent matter. Several years ago, before I had ever seen Kripke, if someone had me whether I knew who Saul was, 1 have 'Yes. He's the author of Naming and once I was at a workshop which I knew was being attended by Kripke, but I didn't yet know what he looked like. If someone had asked me whether I knew who Kripke was, I would have had to say, &quot;No.&quot; The relevant property in that context was not his authorship of some paper, but any property that distinguished him from the others present, such as the man in the back row holding a cup of coffee&quot;. Knowledge of a person's identity is then a matter of knowing some context-dependent essential property that serves to identify that person for present purposes that is, a matter of or she is. Therefore, we need a kind of place-holder predicate to stand for this essential property, that in any particular context can be specified more precisely. It happens that English has a morthat serves just this function morpheme Let posit a predicate stands for the context ially determined property or conjunction of properties that would count identification in that particular context. reading of generally taken to John's of the identity of the spy. Assuming report would be as a conjunction of two beliefs, one for the tnaM predication and the other expressing knowledge of the essential property. the what-ness, of the of predication. J P) .spy'( P, X) A al] I. ci wh'((j, is, John a spy and John knows who let us probe just a little more deeply in particular call into quest it knowledge of is really part of the meaning of the sentence in the re The representation of of F). I said. is P) P, S) J , S • T) us represent the reading as .1 Apy'l P. Exi.0)(,)) A ,IP(y. 13 J. common (12) (1:') t he I. 11. P, 5) There ambil.Tuit y exists the real world is merely believed John dicta). addition, ri) includes conjuncts J, R) line (13b). are these part of the int erpretat 5? The following example (10111)1 his ..;tilipose entire Rotary (lob is at t he table nest iit he 5. but Joliddoesn't know t his. John belie% i' —.me memof the rhib a sp■. but 111, i),' a 111(11 5 this lint ion. and only ( holds. not (12). Judgments are uncertain to sentence 5 is appropriate in circumstances, it is certain that the sentence John believes someone at the next table is a spy. is appropriate, and that is sufficient for the argument. seems then that the conjuncts R) R. S) are not part of what we want in the initial logical form of the sentence,' but only a very common conversational implicature. The reason the implicature is very common is that if of it: they are not part of the meaning of sentence. 65 John doesn't know that the man is at the next table, there must be some other description under which John is familiar with the man. The story I just told provides such a description, but not one sufficient for identifying the man. analysis is attractive since it allows us to view the re dicto problem as just one instance of a much more general problem, namely, the existential status of the grammatically subordinated material in sentences. Generally, such material takes on the tense of the sentence. Thus, in The boy built the boat. building event by z of place in the past, and we assume a boy in the past, at the time of the building. But in Many rich men studied computer science in college. the most natural reading is not that the men were rich when they were studying computer science but that they are rich now. In The flower is artificial. there is an entity z which is described as a flower, and z exists, its 'flower-nese does not exist in the real world. is a which is embedded in the opaque predicate was stated above that the representation (10) for the conveys no properties of than that John believes him to be a spy. In particular, it does not convey S's in the real world. refers to a possible individual, may turn out to if, for example. John ever comes to be able to identify the person whom he believes to be the spy, or if there is some actual spy who has given John good cause for his suspicions. not be actual, only possible. Suppose this is the case. One common objection to possible individuals is that they may seem to violate the Law of the Excluded Middle. Is or not married? Our intuition is that the question is inappropriate, and indeed the answer given in our formalism this flavor. By axiom is really just an abfor marrierr(E,S) is false, for the of the real world would imply the existence of also false. But its falsity has nothing to do with S's marital status, only his existential status. The predication unmarried(S) is false for the same reason. The primed predicates are basic, and for them the problem of the excluded does not arise. The predication married( S) true false depending on whether is condition of S's being married. An unprimed, transparent predicate carries along with it the existence of its arguments, and it can fail to be true of an entity either through the entity being actual but not having that property or through the nonexistence of the entity. 5 Identity in Belief Contexts final problem I will consider arises in dicto reports. It is the problem of identity in intensional contexts, raised by (1892). One way of stating the problem is this. Why it that if (14) John believes the Evening Star is rising. and if the Evening Star is identical to the Morning Star, it is not necessarily true that (15) John believes the Morning Star is rising. Leibniz's Law, we ought to to substitute for an entity any entity that is identical to it. This puzzle survives translation into the logical notation, if John knows of the existence of the Morning Star and if proper are unique. The representation for (the dicto of) sentence (14) is believe(J, rise( ES) John's belief in the Morning Star would he represented A existence of the Evening Star and the Morning Star expressed by uniqueness of the proper name 'Evening is expressed by the axiom Ettentrig-Star(y) x y The identity of the Evening Star and the Morning Star is expressed all of this we can infer that the Morning also Evening Star and hence identical to hence can into rise( give have is a representation for the paradoxical (15). There are three possibilities for dealing with this problem. first is to or restrict Leibniz's Law. The second is to deny that the Evening Star and the Morning Star are identical as entities in the Platonic universe; they only happen to he identical in the real world, and that is not sufficient for intersubstitutivity. third is to deny that expression (16) represents (14) because 'the Evening Star&quot; in (14) does not refer to what it seems to refer to. The first possibility is the approach of researchers who treat belief as an operator rather than as a predicate, and then re• strict substitution inside the operator? We cannot avail ourselves of this. solution because of the flatness of our notation. predicate rise is surely referentially transparent, so if S identical, MS can he substituted for the , give riselP,„t/S). Then the exnot even require substitution to he belief about the Morning In any case, this approach does not seem wise in view of the central importance played in discourse interpretation by the identity of differently presented entities, i.e. by coreference. Free intersubstitutibility of identicals seems a desirable property to preserve. The second possible answer to Frege's problem is to say that in the Platonic universe, the Morning Star and the Evening Star 'This 4 a purely syntactic approach, and when one tries to construct a semantics for it, one is generally driven to the third possibility. 66 are different entities. It just happens that in the real world they identical. But it is not true that = MS, equality, like quantification, is over entities in the Platonic universe. The fact identical in the real world (call this relation rw-identieel) must be stated explicitly, say, by the expression MS) properly, rwidentieal(z, y) For reasoning about 're-identical* entities, that is, Platonic entities that are identical in the real world, we may take the following approach. Substitution in referentially transparent contexts would be achieved by use of the axiom schema , ...)pi , ...) A ...) A rin-identical(es, is the kth argument of p and p is referentially transin its That is, if p's being true of and identical in the real world, then there is a es of p's being true of 64, and es is identical to the real world. Substitution of &quot;rw-identicals' in a condition results not in the same condition but in an &quot;rw-identical' condition. would be such an axiom for the first argument believe but not for its referentially opaque second argument. Axioms will express the fact that rw-identieal is an equivalence relation: y) D 14v-identical(y, z) y, s)rvridentieagz, y) z) ,v-identieal(z. z) Finally, the following axiom, together with axiom (17), would express Leibniz's Law: es) (Ezist(et all we can prove that if the Evening Star rises then the Morning Star rises, but we cannot prove from John's that the Evening that John believes the Morning Star rises. If John knows the Morning Star and the Evening Star are identical, and he knows axiom (17), then his belief that the Morning Star rises can be proved as one would prove his belief in the consequences of any other syllogism whose premises he believed, in accordance with a treatment of reasoning about developed in a longer version this This solution is in the spirit of our whole representational approach in that it forces us to be painfully explicit about everything. The notation does no magic for us. There is a significant cost associated with this solution, however. When proper names are represented as predicates and not as constants, the natural way to state the uniqueness of proper names is by means of foilowing sort: tar(z) tar(y) z since from the axioms for we can show that tar( M S), it follow that We thus restate the axiom for the uniqueness of proper names as rel-identieal(z, y) A similar modification must be made for functions. Since we are using only predicates, the uniqueness of the value of a function must be encoded with an axiom like y, ather(y, z = and y are both fathers of z and y are the same. This have to by the axiom z)A f ather(y, z) y) The very common problems involving reasoning about equality, which can be done efficiently, are thus translated into problems involving reasoning about the predicate rw-identical, which is very cumbersome. way to view this second solution is as a fix to the first solution. For &quot;=1&quot; we substitute the relation rw-identical, and by means of axiom schema (17), we force substitutions to propagate to the eventualities they occur in, and we force the distinction between referentially transparent and referentially opaque predicates to be made explicitly. It is thus an indirect way of rejecting Leibniz' Law. third solution is to say that 'the Evening Star' sentence (14) does not really refer to the Evening Star, but to some abstract entity somehow related to the Evening Star. That is, sentence (14) is really an example of metonymy. This may seem counterintuitive, and even bizarre, at first blush. But in fact the most widely accepted classical solutions to the problem of are of this flavor. For Frege (1892) 'the Evening in sentence (14) does not refer to the Evening Star but to the the phrase 'the Evening Star'. In a more recent approach. Zalta (1983) takes such noun phrases to refer to 'abstract objects' related to the real object. In both approaches noun phrases in intensional contexts refer to senses or abstract objects, while other noun phrases refer to actual entities, and so it is necessary to specify which predicates are intensional. In a Montagovian approach, 'the Evening Star' would be taken to refer to the intension of the Evening Star, .not its eztenaion in real world, and noun phrases would to refer to intensions, although for nonintensional predicates there would be meaning postulates that make this equivalent to reference to extensions. Thus, in all these approaches intensional and extensional predmust be distinguished explicitly, and noun phrases in intensional contexts are systematically interpreted metonymically. It would be easy enough in our framework to implement these approaches. We can define a function a of three arguments the entity, and the condition used to entity the sense, or intension, abstract corresponding to actual for that cognizer and Sentence would be represented, not as (16), but as A ri eel( , o( ES , J, J ,Q AEveninpS tari(Q , ES) tend to prefer to think of the value as abstract Whatever it is, it is necessary that the of J, Qi)be something different from the value Qs) where is, different objects must correspond to the condition of being Star and the condition of being the Morning It because of this feature that we escape the problem 67 intersubstitutivity of identicals, for substitution of (18) yields `... 41)) rather than A S, J, A , would be the representation of sentence (1$). The difficulty with this approach is that it makes the interpretation of noun phrases dependent on their embedding context: Intensional context metonymic interpretation Extensional context — nonmetonymic interpretation It thus violates, though not seriously, the naive compositionality that I have been at so many pains to preserve. Metonymy is a very common phenomenon in discourse, but I prefer to think of it as occurring irregularly, and not as signalled systematically by other elements in the sentence. Having laid out the three possible solutions and their shortcomings, I find that I would like to avoid the problem of identity altogether. The third approach suggests a ruse for doing so. We can assume that, in general, (16) is the representation of sentence (14). We invoke no extra complications where we don't have to. When, in interpreting the text, we encounter a difficulty resulting (rota the problem of identity, we can go back and revise our interpretation of (14), by assuming the reference must have been • metonymic one to the abstract entity and not the actual entity. In those cases it would be as are saying, 'John couldn't believe about the Evening Star itself that it is rising. The paradox shows that he is insufficiently acquainted with the Evening Star to refer to it directly. He must be talking about an abstract entity related to the Evening Star. * My guess is that we will not have to resort to this ruse often, for I suspect the problem rarely arises in actual discourse interpretation. 6 The Role of Semantics Let me close by making some comments about ways of doing Semantics is the attempted specification relation between language and the world. However, this requires theory of the world. There of choices one can in this regard. one of the spectrum let's say the right end one can adopt the 'correct' theory of the world, the theory given by quantum mechanics and the other sciences. If one does this, semantics becomes impossible because it is no less than all of science, a fact that has led Fodor (1980) to express some despair. There's too much of • mismatch between the way we view the world and the way the world really is. At the left one can assume a theory world that is isomorphic to way we talk about it. What been doing in this paper, fact, is an effort to work out the details such theory. In this case, semantics becomes very nearly trivial. Most activity in semantics today is slightly to the right of the extreme left end of this spectrum. One makes certain assumptions about the nature of the world that closely redeet language, and doesn't make other sesumptiosui. Where one has failed to necessary assumptions, puzzles appear, and semantics becomes effort to solve those puzzles. Nevertheless, rails move far enough away from language to represent significant progress toward the right end of the spectrum. The position I advocate is that there is no reason to make our tisk more difficult. We will have puzzles enough to solve when we get to discourse. Events are represented by event variables, as in [Hobbs, 1985], so that see' (e1, x1, x2) means e1 is a seeing event by x1 of x2. First, as originally advocated by Hobbs (1985), we adopt an ONTOLOGICALLY PROMISCUOUS representation that includes a wide variety of types of entities. Second, in keeping with ontological promiscuity (Hobbs, 1985), we represent the importance of attributes by the salience of events and states in the discourse model - these states and events now have the same status in the discourse model as any other entities. [Hobbs 1985] describes a similar approach by introducing what he calls 'nominalization'. Note that the predicate language representation utilized by Carmel-Tools is in the style of Davidsonian event based semantics (Hobbs, 1985).   See Hobbs (1985a) for explanation of this notation for events. For justification for this kind of logical form for sentences with quantifiers and intensional operators, see Hobbs (1983) and Hobbs (1985a). We adopt their idea of an utterance as a description, generated from a communicative goal, and also use an "ontologically promiscuous" formalism for representing meaning [Hobbs, 1985]. [Hobbs, 1985] framework for representing the propositional content (or meaning) of an expression as an ontologically richly sorted, relational structure. The underlying core theories are expressed as axioms in this notation (Hobbs, 1985). First, we adopt an ONTOLOGICALLY PROMISCUOUS representation (Hobbs, 1985) that includes a wide variety of types of entities. Doing inference with representations close to natural language has also been advocated by Jerry Hobbs, as in (Hobbs, 1985). Nevertheless, as (Hobbs, 1985) and others have argued, semantic representations for natural language need not be higher-order in that ontological promiscuity can solve the problem. Moreover, as stated in (Hobbs, 1985), we assume that the alleged predicate is existentially opaque in its second argument. All of the following discussion is based on a model of semantic analysis similar to that proposed in (Hobbs, 1985). We do not completely rule out the possibility that some more sophisticated, ontologically promiscuous, first-order analysis (perhaps along the lines of (Hobbs, 1985)) might account for these kinds of monotonicity inferences. An add-on to ESG converts the parse tree into an LF in the style of Hobbs (1985).
Distributed Word Clustering for Large Scale Class-Based Language Modeling in Machine Translation In statistical language modeling, one technique to reduce the problematic effects of data sparsity is to partition the vocabulary into equivalence classes. In this paper we investigate the effects of applying such a technique to highermodels trained on large corpora. We introduce a modification of the exchange clustering algorithm with improved efficiency for certain partially class-based models and a distributed version of this algorithm to efficiently obtain automatic word classifications large vocabularies million words) ussuch large training corpora billion tokens). The resulting clusterings are then used in training partially class-based language models. We show that combining them with wordmodels in the log-linear model of a state-of-the-art statistical machine translation system leads to improvements in translation quality as indicated by the BLEU score. (Uszkoreit and Brants, 2008) used partially class based LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. However, rather than the more commonly used model of Brown et al (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008). By ignoring class-to-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008). While the use of class-to-class transitions can lead to more compact models, which is often useful for conquering data sparsity, when clustering large datasets we can get reliable statistics directly on the word to-class transitions (Uszkoreit and Brants, 2008). We use the following fairly standard features in our tagger: current word, suffixes and prefixes of length 1, 2 and 3; additionally we use word cluster features (Uszkoreit and Brants, 2008) for the current word, and transition features of the cluster of the current and previous word. We use the following features for our tagger: current word, suffixes and prefixes of length 1 to 3; additionally we use word cluster features (Uszkoreit and Brants, 2008) for the current word, and transition features of the cluster of the current and previous word. Uszkoreit and Brants (2008) proposed a distributed clustering algorithm with a similar objective function as the Brown algorithm. To our knowledge, Uszkoreit and Brants (2008) are the only recent authors to show an improvement in a state-of-the-art MT system using class-based LMs. However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes.
HHMM-Based Chinese Lexical Analyzer ICTCLAS This document presents the results from Inst. of Computing Tech., CAS in the ACL- SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff. The authors introduce the unified HHMM-based frame of our Chinese lexical analyzer ICTCLAS and explain the operation of the six tracks. Then provide the evaluation results and give more analysis. Evaluation on ICTCLAS shows that its performance is competitive. Compared with other system, ICTCLAS has ranked top both in CTB and PK closed track. In PK open track, it ranks second position. ICTCLAS BIG5 version was transformed from GB version only in two days; however, it achieved well in two BIG5 closed tracks. Through the first bakeoff, we could learn more about the development in Chinese word segmentation and become more confident on our HHMM-based approach. At the same time, we really find our problems during the evaluation. The bakeoff is interesting and helpful. (Note that the top participant of CTBc (Zhang et al, 2003) used additional named entity knowledge/data in their word segmenter). ICTCLAS Segmenter: this model, trained by Zhang et al (2003), is a hierarchicalHMM segmenter that incorporates parts-of speech (POS) information into the probability models and generates multiple HMM mod els for solving segmentation ambiguities. ICTCLAS (Zhang et al., 2003), a tool developed by the Institute of Computing Technology of Chinese Academy of Sciences (ICT), is used for word segmentation and part-of-speech tagging. Then we apply a hierarchical Hidden Markov Model (HMM) based Chinese lexical analyzer ICTCLAS (Zhang et al, 2003) to extract named entities, noun phrases and events. HMMsegmenter (Zhang et al, 2003) that uses the specifications of PKU. Most word-based segmenters in Chinese IR are either rule-based models, which rely on a lexicon, or statistical-based models, which are trained on manually segmented corpora (Zhang et al,2003). Its segmentation model is a 3The query set and relevance judgements are available at http: //www.cs.ualberta.ca/ ?yx2/research.html 59 class-based hidden Markov model (HMM) model (Zhang et al, 2003). Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al, 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (Max Ent) (Xue and Shen, 2003), support vector machine. Decrease in H (XjX n) for Chinese characters when n is increased software such as (Zhang et al, 2003) whose performance is also high. Hence the need for automatic word segmentation systems (Zhang et al, 2003). Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al, 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al, 2004). Then for every path of the N+1paths4 (N best paths and the atom path), we perform a process of Roles Tagging with HMM model (Zhang et al 2003). The Chinese sentences in both the development and test corpus are segmented and POS tagged by ICTCLAS (Zhang et al, 2003). Both ICTCLAS and Stanford segmenters utilise machine learning techniques, with Hidden Markov Models for ICT (Zhang et al, 2003) and conditional random fields for the Stanford segmenter (Tseng et al, 2005). In this work, we resort to ICTCLAS (Zhang et al, 2003), a widely used tool in the literature. The posts were then part-of speech tagged using a Chinese word segmentation tool named ICTCLAS (Zhang et al, 2003). Their system only does IWR, using the CWS and POS tagging output of the ICTCLAS segmenter (Zhang et al, 2003) as in put.  ICTCLAS is developed by Chinese Academy of Science, the precision of which is 97.58% on tagging general words (Huaping Zhang et al, 2003). The Chinese word segmentation tool is ICTCLAS (Zhang et al 2003) and Google Translator is the MT for the source language.
Language Independent NER Using A Maximum Entropy Tagger Entity Recognition systems need to integrate a wide variety of information for optimal performance. This paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy. The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch. We use the C&C tools (Curran and Clark, 2003) for POS and NE tagging and the and the Berkeley Parser (Petrov and Klein, 2007), trained with default parameters. Tokenisation and sentence splitting is followed by part-of speech tagging with the Maximum Entropy Markov Model (MEMM) tagger developed by Curran and Clark (2003) (here after referred to as C&C) for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003), trained on the MedPost data (Smith et al, 2004). As the vanilla C&C tagger (Curran and Clark, 2003) is optimised for performance on newswire text, various modifications were applied to improve its performance for biomedical NER. The named entity recognizer of Curran and Clark (2003) is also used to recognize the standard set of muc entities, including person, location and organisation. These are based on those found in (Curran and Clark, 2003). The part-of-speech tagging uses the Curran and Clark POS tagger (Curran and Clark, 2003) trained on MedPost data (Smith et al, 2004), whilst the other preprocessing stages are all rule based. The NER module uses the Curran and Clark NER tagger (Curran and Clark, 2003), augmented with extra features tailored to the biomedical domain. Malouf (2002) and Curran and Clark (2003) condition the label of a token at a particular position on the label of the most recent previous instance of that same token in a prior sentence of the same document. A number of NER systems have made effective use of how the same token was tagged in different parts of the same document (see (Curran and Clark, 2003) and (Mikheev et al, 1999)).  Further linguistic markup is added using the morpha lemmatiser (Minnen et al, 2000) and the C&C named entity tagger (Curran and Clark, 2003) trained on the data from MUC-7. Part-of-speech (POS) tagging is done using the C&C tagger (Curran and Clark, 2003a) and lemmatisation is done using morpha (Minnen et al, 2000). We use both rule-based and machine-learning named entity recognition (NER) components, the former implemented using LT-TTT2 and the latter using the C&C maximum entropy NER tagger (Curran and Clark, 2003b). We use different strategies for the identification of the two classes of entities: for the domain-specific ones we use hand-crafted LT TTT rules, while for the non domain-specific ones we use the C&C named entity tagger (Curran and Clark, 2003) trained on the MUC-7 data set. The part-of-speech tagging uses the Curran&Clark maximum entropy Markov model tagger (Curran and Clark, 2003) trained on MedPost data (Smith et al., 2004), whilst the other preprocessing stages are all rule-based. We use different strategies for the identification of the two classes of entities: for the domain-specific ones we use hand-crafted LT TTT rules, while for the non-domain-specific ones we use the C& amp; C named entity tagger (Curran and Clark, 2003) trained on the MUC7 data set. For this we have used the C&C named entity recogniser (Curran and Clark, 2003), which is run on pos-tagged and chunked documents in the corpus to identify and extract named entities as potential topics. Malouf (2002) and Curran and Clark (2003) condition the label of a token at a particular position on the label of the most recent previous in stance of that same token in a previous sentence of the same document. By training the C&C tagger (Curran and Clark, 2003) on the gold-standard corpora an dour new Wikipedia-derived training data, we evaluate the usefulness of the latter and explore the nature of the training corpus as a variable in NER. We trained the C&C NER tagger (Curran and Clark,2003) to build separate models for each gold standard corpus.
Multi-Source Transfer of Delexicalized Dependency Parsers We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al, 2011). Part-of-speech tags are known to contain significant amounts of information for unlabeled dependency parsing (McDonald et al, 2011), so we find it reassuring that our latest grammar inducer is less dependent on gold tags than its predecessors. More recently, McDonald et al (2011) demonstrated an impressive alternative to grammar induction by projecting reference parse trees from languages that have annotations to ones that are resource-poor. If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McDonald et al, 2011). It also retains compatibility with any refinement procedures similar to projected transfer (McDonald et al, 2011) that may have been designed to work in conjunction with direct model transfer. It remains to be seen which one would produce the best results and how multi-source feature representation projection would compare to, for example, multi-source projected transfer (McDonald et al, 2011). McDonald et al (2011) showed that such projection produce better structures than the current unsupervised parsers do. A more generally applicable class of methods exploits the notion of universal part of speech tags (Petrov et al., 2011; Das and Petrov, 2011) to train parsers that can run on any language with no adaptation (McDonald et al., 2011) or unsupervised adaptation (Cohen et al., 2011). By instantiating the basic MST Parser features over coarse parts of speech, we construct a state-of-the-art delexicalized parser in the style of McDonald et al (2011). We emphasize again that these baseline features are entirely standard, and all the DELEX feature set does is recreate an MST Parser-based analogue of the direct transfer parser described by McDonald et al (2011). The first variable we consider is whether we have access to a small number of target language trees or only pre-existing tree banks in a number of other languages; while not our actual target language, these other tree banks can still serve as a kind of proxy for learning which features generally transfer useful in formation (McDonald et al., 2011). Following McDonald et al (2011), we strip punctuation from all tree banks for the results of Section 3.3.  Following the procedure from McDonald et al (2011), for each language, we train both our DELEX and DELEX+PROJ features on a concatenation of 2000 sentences from each other CoNLL training set, plus 2000 sentences from the Penn Treebank. We include for reference the baseline results of McDonald et al (2011) and Ta?ckstro?m et al (2012) (multi-direct transfer and no clusters) and the improvements from their best methods using lexical information (multi-projected transfer and cross lingual clusters).  The first parser is a non-lexicalized version of the MST parser (McDonald et al 2005) successfully used in the multilingual context (McDonald et al 2011). Surprisingly, our model slightly outperforms the mapping of (Petrov et al 2011), yielding an average accuracy of 56.7% as compared to the 55.4% achieved by its manually constructed counterpart for the direct transfer method (McDonald et al 2011). Specifically, McDonald et al (2011) have demonstrated that projecting from a single oracle chosen language can lead to good parsing performance, and our technique may allow such projection without an oracle. The first section of the table is for the direct transfer of the MST parser (McDonald et al 2011).
Using Universal Linguistic Knowledge to Guide Grammar Induction We present an approach to grammar induction that utilizes syntactic universals to improve dependency parsing across a range of languages. Our method uses a single set of manually-specified language-independent rules that identify syntactic dependencies between pairs of syntactic categories that commonly occur across languages. During inference of the probabilistic model, we use posterior expectation constraints to require that a minimum proportion of the dependencies we infer be instances of these rules. We also automatically refine the syntactic categories given in our coarsely tagged input. Across six languages our approach outperforms state-of-theart unsupervised methods by a significant mar USR is the weakly supervised system of Naseem et al (2010).  Other orthogonal dependency grammar induction techniques - including ones based on universal rules (Naseem et al2010) - may also benefit in combination with DBMs. The second class of techniques assumes knowledge about identities of part-of-speech tags (Naseem et al., 2010), i.e., which word tokens are verbs, which ones are nouns, etc.  Unsupervised methods attempt to infer linguistic structure without using any annotated data (Klein and Manning, 2004) or possibly by using a set of linguistically motivated rules (Naseem et al, 2010) or a linguistically informed model structure (Berg-Kirkpatrick and Klein, 2010). In particular it builds on the idea that unsupervised parsing can be informed by universal dependency rules (Naseem et al, 2010). We reformulate the universal dependency rules used in Naseem et al (2010) in terms of the universal tags provided in the shared task (Figure 2), but unlike them, we do not engage in grammar induction.  Naseem et al (2010) used universal syntactic categories and rules to improve grammar induction, but their model required expert hand written rules as constraints. We compare to the following three systems that do not augment the tree banks and report results for some of the languages that we considered: USR: The weakly supervised system of Naseem et al (2010), in which manually defined universal syntactic rules (USR) are used to constrain a probabilistic Bayesian model. For instance, Naseem et al (2010) explicitly encode these similarities in the form of universal rules which guide grammar induction in the target language. Among several available fine-to-coarse mapping schemes, we employ the one of Naseem et al (2010) that yields consistently better performance for our method and the baselines than the mapping proposed by Petrov et al (2011). The second approach takes the universal rules of Naseem et al (2010) but rather than estimating a probabilistic model with these rules, a rule based heuristic is used to select a parse rather.  Gillenwater et al (2010) is a fully unsupervised extension of the approach described in Klein and Manning (2004), whereas Naseem et al (2010) rely on hand-written cross-lingual rules.  In fact, the universal tag sets manually induced by Petrov et al (2011) and by Naseem et al (2010) disagree on 10% of the tags. Naseem et al (2010) obtain slightly better results, but only evaluate on six languages. Naseem et al (2010) report better results than ours on Portuguese, Slovene, Spanish and Swedish, but worse on Danish.
Large Language Models in Machine Translation This paper reports on the benefits of largescale statistical language modeling in machine translation. A distributed infrastruc ture is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. Itis capable of providing smoothed probabilities for fast, single-pass decoding. We in troduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases. 5-gram word language models in English are trained on a variety of monolingual corpora (Brants et al, 2007). In the case of language models, we often have to remove low-frequency words because of a lack of computational resources, since the feature space of k grams tends to be so large that we sometimes need cutoffs even in a distributed environment (Brantset al, 2007). To scale LMs to larger corpora with higher-order dependencies, researchers have considered alternative parameterizations such as class-based models (Brown et al, 1992), model reduction techniques such as entropy-based pruning (Stolcke, 1998), novel represent ion schemes such as suffix arrays (Emami et al, 2007), Golomb Coding (Church et al, 2007) and distributed language models that scale more readily (Brants et al, 2007).  Here we choose to work with stupid back off smoothing (Brants et al, 2007) since this is significantly more efficient to train and deploy in a distributed framework than a context dependent smoothing scheme such as Kneser-Ney. Previous work (Brants et al, 2007) has shown it to be appropriate to large-scale language modeling. Brants et al (2007) have shown that each doubling of the training data from the news domain (used to build the language model), leads to improvements of approximately 0.5 BLEU points. For example, Brants et al (2007) used 1500 machines for a day to compute the relative frequencies of n-grams from 1.8TB of web data. We build sentence specific zero-cutoff stupid-back off (Brants et al., 2007) 5-gram language models, estimated using 4.7B words of English newswire text, and apply them to rescore each 10000-best list. However, adding more data in an unsupervised sense is unlikely to significantly improve results (Brants et al, 2007). We aim to allow the estimation of large scale distributed models, similar in size to the ones in Brantset al (2007). In relation to language models, Brants et al (2007) recently proposed a distributed MapReduce infrastructure to build Ngram language models having up to 300 billion n-grams. Log-linear interpolation is particularly popular in statistical machine translation (e.g., Brants et al., 2007), because the interpolation weights can easily be discriminatively trained to optimize an end-to-end translation objective function (such as BLEU) by making the log probability according to each language model a separate feature function in the overall translation model. This was expected, as it has been observed before that very simple smoothing techniques can perform well on large data sets, such as web data (Brants et al, 2007). Since standard spelling correction dictionaries (e.g. ASpell) are not able to capture the rich language used in web queries, large-scale knowledge sources such as Wikipedia (Li et al 2011), query logs (Chen et al 2007), and large n-gram corpora (Brants et al., 2007) are employed. Since that time, however, increasingly large amounts of language model training data have become available ranging from approximately one billion words (the Gigaword corpora from the Linguistic Data Consortium) to trillions of words (Brants et al, 2007). We implemented an N -gram indexer/estimator using MPI inspired by the MapReduce implementation of N-gram language model indexing/estimation pipeline (Brants et al, 2007). In machine translation, improved language models have resulted in significant improvements in translation performance (Brants et al., 2007). This makes the storage requirement for higher-quality modified Kneser-Ney smoothing comparable to stupid back off (Brants et al 2007). Our distributed 4-gram language model was trained on 600 million words of Arabic text, also collected from many sources including the Web (Brants et al, 2007).
The Penn Discourse TreeBank 2.0. This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the temporal collocations capture the real world semantics and real world events over time. 1. 2. This paper considers differences in texts in the well known Penn TreeBank (hereafter, PTB) and in particular, how these differences show up in the Penn Discourse TreeBank (Prasad et al, 2008). Genre differences at the level of discourse in the PTB can be seen in the manual annotations of the Penn Discourse TreeBank (Prasad et al, 2008). The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) provides annotations for the arguments and relation senses of one hundred pre-selected discourse connectives over the news portion of the Penn Treebank corpus (Marcus et al, 1993). The PDTB adopts a lexically grounded approach to discourse relation annotation (Prasad et al, 2008). The set of discourse connectives is taken from the Penn Discourse Treebank (Prasad et al, 2008), thus creating a list of 240 potential connectives. Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al, 2008) benefits the researchers with a large discourse annotated corpora, using a comprehensive scheme for both implicit and explicit relations. The Penn Discourse Treebank (Prasad et al, 2008) (see Section 3.1 below) includes around 100 connective types, but the exact number varies across studies, 194 depending on the discourse theory used to classify them. One of the very few available discourse annotated corpora is the Penn Discourse Treebank (PDTB) in English (Prasad et al, 2008). One of the most important resources for discourse connectives in English is the Penn Discourse Treebank (Prasad et al, 2008). The last example in Table 1 is a sentence from the Penn Discourse Treebank (Prasad et al., 2008). One of the few available discourse annotated corpora in English is the Penn Discourse Treebank (PDTB) (Prasad et al, 2008). The Penn Discourse Treebank (PDTB) (Prasad et al, 2008) is such a corpus which provides a discourse-level annotation on top of the Penn Treebank, following a predicate argument approach (Webber, 2004). Later, with the release of manually annotated corpus, such as Penn Discourse Treebank 2.0 (PDTB) (Prasad et al, 2008), recent studies performed implicit discourse relation recognition on natural (i.e., genuine ) implicit discourse data (Pitler et al, 2009) (Lin et al,2009) (Wang et al, 2010) with the use of linguistically informed features and machine learning algorithms.  PDTB (Prasad et al, 2008) is the largest hand annotated corpus of discourse relation so far. It is also considered to be the anchor of discourse relations, in the sense of the Penn Discourse Treebank (PDT) (Prasad et al, 2008). The goal is not only to annotate the data, but also to compare the representation of these relations in the Prague Dependency Treebank with the annotation done at the Penn Treebank, which was carried out at University of Pennsylvania (Prasad et al., 2008). As described e.g. in Mladova et al. (2009), the annotation framework that we use is based on the knowledge obtained from studying various other systems, especially the Penn Discourse Treebank (Prasad et al., 2008), but naturally it has been adjusted to specific needs of the Czech language and PDT. To attempt an answer to this question, we utilized the end-to-end discourse parser proposed by Lin et al (2010) to extract PDTB-styleddiscourse relations (Prasad et al, 2008) from RT data. In the realm of discourse annotation, the Penn Discourse TreeBank (PDTB) (Prasad et al, 2008) separates itself by adopting a lexically grounded approach.
Stochastic Attribute-Value Grammars Probabilistic analogues of regular and context-free grammars are well known in computational linguistics, and currently the subject of intensive research. To date, however, no satisfactory probabilistic analogue of attribute-value grammars has been proposed: previous attempts have failed to define an adequate parameter-estimation algorithm. In the present paper, I define stochastic attribute-value grammars and give an algorithm for computing the maximum-likelihood estimate of their parameters. The estimation algorithm is adapted from Della Pietra, Della Pietra, and Lafferty (1995). To estimate model parameters, it is necessary to compute the expectations of certain functions under random fields. In the application discussed by Della Pietra, Della Pietra, and Lafferty (representing English orthographic constraints), Gibbs sampling can be used to estimate the needed expectations. The fact that attribute-value grammars generate constrained languages makes Gibbs sampling inapplicable, but I show that sampling can be done using the more general Metropolis-Hastings algorithm. On the other hand, as Abney (1997) points out, the context-sensitive dependencies that unification-based constraints introduce render the relative frequency estimator suboptimal. Abney (1997) proposes a Markov Random Field or log linear model for SUBGs, and the models described here are instances of Abney's general framework. Abney (1997) proposes a gradient ascent, based upon a Monte Carlo procedure for estimating E0 (fj). Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). As Abney (1997) shows, we can not use relatively simple techniques such as relative frequencies to obtain a model for estimating derivation probabilities in attribute-value grammars. PCFG-based models can only approximate LFG and similar constraint-based formalisms (Abney, 1997). Simple PCFG based models, while effective and computationally efficient, can only provide approximations to LFG and similar constraint-based formalisms (Abney,1997).  Unfortunately, most of the proposed probability models are not mathematically clean in that the probabilities of all possible UBG readings do not sum to the value 1, a problem which is discussed intensively by Eisele (1994), Abney (1997). We would like the parsing model to include long-range dependencies, but this introduces problems for generative parsing models similar to those described by Abney (1997) for attribute-value grammars. For example, when we apply a unification-based grammar, LPCFG-like modeling results in an inconsistent probability model because the model assigns probabilities to parsing results not allowed by the grammar (Abney, 1997). In particular, when we apply Feature-Based LTAG (FBLTAG), the above probability is no longer consistent because of the non-local constraints caused by feature unification (Abney, 1997). Such constraints are known 83 to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997). As is well known (Abney, 1997), DAG-like dependencies can not in general be modeled with a generative approach of the kind taken here. Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations. Abney (1997) pointed out that the non-context free dependencies of a unification grammar require stochastic models more general than Probabilistic Context-Free Grammars (PCFGs) and Markov Branching Processes, and proposed the use of log linear models for defining probability distributions over the parses of a unification grammar. The following section reviews stochastic unification grammars (Abney, 1997) and the statistical quantities required for efficiently estimating such grammars from parsed training data (Johnson et al, 1999). (Abney, 1997) and has the advantage of elegantly bypassing the issue of loosing probability mass to failed derivations due to unification failures.  Abney gives fuller details (Abney, 1997).
Stanfordâ€™s Multi-Pass Sieve Coreference Resolution System at the CoNLL-2011 Shared Task This paper details the coreference resolution system submitted by Stanford at the CoNLL- 2011 shared task. Our system is a collection of deterministic coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information. All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. We participated in both the open and closed tracks and submitted results using both predicted and gold mentions. Our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track. In fact, the Stanford coreference resolver (Lee et al 2011), which won the CoNLL-2011 shared task on coreference resolution, adopts the once-popular rule-based approach, resolving pronouns simply with rules that encode the aforementioned traditional linguistic constraints on coreference, such as the Binding constraints and gender and number agreement. The infrequency of occurrences of difficult pronouns in these standard evaluation corpora by no means undermines their significance, however.  Our second baseline is the Stanford resolver (Lee et al2011), which achieves the best performance in the CoNLL 2011 shared task (Pradhan et al2011). Lee et al (2011) use rules to extract appositions for co reference resolution, selecting only those that are explicitly flagged using commas or parentheses. After obtaining all the extracted noun phrases, we also use a rule-based method to remove some erroneous candidates based on previous studies (e.g. Lee et al (2011), Uryupina et al (2011)). They also outperform the learning-based systems of Sapena et al (2011) and Chang et al (2011), and perform competitively with Lee's system (Lee et al 2011). In this paper, we have chosen two coreference resolution systems: Stanford's Multi-Pass Sieve Coreference Resolution System (Lee et al, 2011) (henceforth, Stanforddcoref) and ARKref (O'Connor and Heilman, 2011). which, along with the absence of Froggy in the name gazetteer for the system (Lee et al, 2011), would lead to both precision and recall errors for Froggy, as we observed. attributes and features such as ANIMACY in the 2 According to Lee et al (2011), Stanforddcoref correctly. The Stanford tools perform part of speech tagging (Toutanova et al, 2003), constituent and dependency parsing (Klein and Manning, 2003), named entity recognition (Finkel et al, 2005), and coreference resolution (Lee et al, 2011). We use the part-of speech (POS) tagger, the named-entity recognizer, the parser (Klein and Manning, 2003), and the coreference resolution system (Leeetal., 2011). Our system is an extension of Stanford's multi-passsieve system, (Raghunathan et al,2010) and (Lee et al, 2011), by adding novel constraints and sieves. Another example which omits today in the phrase for the predicted mention is mentioned in (Lee et al, 2011) and this boundary mismatch also accounts for precision and recall errors. For Proper HeadWordMatch mentioned in (Lee et al, 2011), the Pronoun distance which indicates sentence distance limit between a pronoun and its antecedent.  For this purpose, existing work on coreference resolution (Lee et al, 2011) may prove to be useful. We filter arcs by simply adapting the sieves method proposed in (Lee et al, 2011). Sieves 2 to 7 are obtained from (Lee et al, 2011). Note that this and several other rules rely on coreference information, which we obtain from two sources: (1) chains generated automatically using the Stanford Deterministic Coreference Resolution System (Lee et al 2011) 5, and (2) manually identified coreference chains taken directly from the annotated Switchboard dialogues. In particular, the top performing system in the CoNLL 2011 shared task (Pradhan et al, 2011) is a multi-pass system that applies tiers of deterministic co reference sieves from highest to lowest precision (Lee et al, 2011).
Improving Accuracy In Word Class Tagging Through The Combination Of Machine Learning Systems We examine how differences in language models, learned by different data-driven systems performing the same NLP task, can be exploited to yield a higher accuracy than the best individual system. We do this by means of experiments involving the task of morphosyntactic word class tagging, on the basis of three different tagged corpora. Four well-known tagger generators (hidden Markov model, memory-based, transformation rules, and maximum entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second-stage classifiers. All combination taggers outperform their best component. The reduction in error rate varies with the material in question, but can be as high as 24.3% with the LOB corpus. This simple idea has been applied to a variety of classification problems ranging from optical character recognition to medical diagnosis, part-of-speech tagging (see Dietterich 1997 and van Halteren et al 2001 for overviews), and notably supervised WSD (Florian et al, 2002). For example, the state of the art POS tagger is an ensemble of individual taggers (van Halterenet al, 2001), each of which must process the text separately. Illustrating the negative impact of annotation errors on computational uses of annotated corpora, van Halteren et al (2001) compare taggers trained and tested on the Wall Street Journal (WSJ, Marcus et al, 1993) and the Lancaster-Oslo-Bergen (LOB, Johansson, 1986) corpora and find that the results for the WSJ perform significantly worse. Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. Given that the latest literature on POS tagging using Penn Treebank reports an accuracy of around 97% with in-domain training data (van Halteren et al, 2001), we achieve a very reasonable performance, considering these errors. This data set was also used in (van Halteren et al, 2001), therefore the second experiment will allow for a comparison of the results with previous work on tagging Dutch. These sentences received no contextual labels and thus not all of the training data used in (van Halteren et al, 2001) could be used in the Wotan experiment. In the Wotan experiment, 36K sentences (628K words) are used for training (compared to 640K words in (van Halteren et al, 2001)), and 4176 sentences (72K words) are used for testing. The Wotan data is annotated with a tag set consisting of 345 tags (although a number of 341 is reported in (van Halteren et al, 2001)). This result is very similar to the 92.06% reported by Van Halteren, Zavrel and Daelemans in (van Halteren et al, 2001) who used the TnT trigram tagger (Brants, 2000) on the same training and testing data. We have experimented with various classifier combination methods, such as those described in (Brill and Wu, 1998) or (van Halteren et al., 2001), and got improved results, as expected.
Text And Knowledge Mining For Coreference Resolution AND (y is one of the hypernyms of x) AND AND (z is SYNONYM of y) AND AND (z is SYNONYM of anaphor) then Cast_in_Chain(Anaphor,antecedent) (Riloff and Jones 1999) note that the performance of the mutual bootstrapping algorithm can deteriorate rapidly if erroneous rules are entered. To make the algorithm more robust we use the same solution by introducing a second level of bootrapping. outer level, called most reliable based on semantic consistency and discard all the others before restarting the mutual bootstrapping loop again. In our experiments we have retained only those rules for which the new performance, given by the F-measure was larger than the median of the past four loops. The formula for the van Rijsbergen's F-measure combines precision the recall = 6 Evaluation To measure the performance of COCKTAIL we have trained the system on 30 MUC-6 and MUC-7 texts and tested it on the remaining 30 documents. computed the the Fperformance measures have been obtained automatically using the MUC-6 coreference scoring program (Vilain et al. 1995). Table 4 lists the results. Precision Recall F-measure rules 87.1% 61.7% 72.3% rules combined 91.3% 58.6% 71.8% +bootstrapping 92.0% 73.9% 81.9% Table 4: Bootstrapping effect on COCKTAIL Table 4 shows that the seed set of rules had good precision but poor recall. By combining the rules with the entropy-based measure, we obtained further enhancement in precision, but the recall dropped. The application of the bootstrapping methodology determined an enhancement of recall, and thus of the F-measure. In the future we intend to compare the overall effect of rules that recognize referential expressions on the overall performance of the system. 7 Conclusion We have introduced a new data-driven method for corefresolution, implemented in the system. Unlike other knowledge-poor methods for corefresolution (Baldwin 1997) (Mitkov 1998), COCKits most performant rules through massive data, generated by its component. Furthermore, by using an entropy-based method we determine the best partition of corefering expressions chains. rules are learned by applying a bootstrapping methodology that uncovers additional semantic consistency data. References Breck Baldwin. 1997. CogNIAC: high precision coreference with limited knowledge and linguistic resources. The approach is a fully automated variant of the example selection algorithm introduced in Harabagiu et al (2001). Similar observations are made by Harabagiu et al (2001), who point out that intelligent selection of positive instances can potentially minimize the amount of knowledge required to perform coreference resolution accurately. Examples of such scoring functions include the Dempster Shafer rule (see Kehler (1997) and Bean and Riloff (2004)) and its variants (see Harabagiu et al (2001) and Luo et al (2004)). To expand our set of candidate partitions, we can potentially incorporate more high-performing coreference systems into our framework, which is flexible enough to accommodate even those that adopt knowledge-based (e.g., Harabagiu et al (2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)). Vieira & Poesio (2000), Harabagiu et al (2001), and Markert & Nissim (2005) explore the use of WordNet for different coreference resolution subtasks, such as resolving bridging reference, other and definite NP anaphora, and MUC-style coreference resolution. These measures are not specifically developed for coreference resolution but simply taken off-the-shelf and applied to our task without any specific tuning i.e. in contrast to Harabagiu et al. (2001), who weight WordNet relations differently in order to compute the confidence measure of the path. In numerous articles the usefulness of this data and software ensemble has been demonstrated (e.g., for word sense disambiguation (Patwardhan et al, 2003), the analysis of noun phrase conjuncts (Hogan, 2007), or the resolution of coreferences (Harabagiu et al, 2001)). Harabagiu et al (2001) use paths through Wordnet, using not only synonym and is-a relations, but also parts, morphological derivations, gloss texts and polysemy, which are weighted with a measure based on the relation types and number of path elements. For example, motivated by the fact that some coreference relations are harder to identify than the others (see Harabagiu et al (2001)), Ng and Cardie (2002a) present a method for mining easy positive instances, in an attempt to avoid the inclusion of hard training instances that may complicate the acquisition of an accurate coreference model. Results presented in Harabagiu et al (2001) are higher than those reported here, but assume that all and only the noun phrases involved in coreference relationships are provided for analysis by the coreference resolution system. We also plan to investigate previous work on common noun phrase interpretation (e.g. Sidner (1979), Harabagiu et al (2001)) as a means of improving common noun phrase resolution, which remains a challenge for state-of-the-art coreference resolution systems. In (Harabagiu et al, 2001), the path patterns in WordNet are utilized to compute the semantic consistency between NPs.
Fast Methods For Kernel-Based Text Analysis Kernel-based learning (e.g., Support Vector Machines) has been successfully applied to many hard problems in Natural Language Processing (NLP). In NLP, although feature combinations are crucial to improving performance, they are heuristically selected. Kernel methods change this situation. The merit of the kernel is that feature combinaimplicitly expanded without loss of generality and increasing the computational costs. Kernel-based text analysis shows an excellent performance in terms in accuracy; however, these methods are usually too slow to apply to large-scale text analysis. In this paper, we extend Mining to convert a kernel-based classifier into a simple and fast linear classifier. Experimental results on English BaseNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel-based classifiers. For natural language problems in general, of course, it is widely recognized that significant accuracy gains can often be achieved by generalizing over relevant feature combinations (e.g., Kudo and Matsumoto (2003)). There are two types of correct summary according to the character length, "long" and "short", All series of documents were tagged by CaboCha (Kudo and Matsumoto, 2003). We used person name, organization, place and proper name extracted from NE recognition (Kudo and Matsumoto, 2003) for event detection, and noun words including named entities for topic detection. Kudo and Matsumoto (2003) proposed polynomial kernel inverted (PKI), which builds inverted indices h(fj ) ≡ {s | s ∈ S, fj ∈ s} from each feature fj to support vector s ∈ S to only consider support vector s relevant to given x such that s Tx 6= 0.  Following (Kudo and Matsumoto, 2003), we use a trie (hereafter, weight trie) to maintain conjunctive features. PKI - Inverted Indexing (Kudo and Matsumoto, 2003), stores for each feature the support vectors in which it appears. PKE - Heuristic Kernel Expansion, was introduced by (Kudo and Matsumoto, 2003). Our approach is similar to the PKE approach (Kudo and Matsumoto, 2003), which used a basket mining approach to prune many features from the expansion. We use Yamcha (Kudo and Matsumoto, 2003), a support-vector machine-based sequence tagger. We use Yamcha (Kudo and Matsumoto, 2003), an implementation of support vector machines which includes Viterbi decoding. Second, we replace the YAMCHA (Kudo and Matsumoto, 2003) implementation of Support Vector Machines (SVMs) with SVMTool (Gimenez and Marquez, 2004) as our machine learning tool, for reasons of speed, at the cost of a slight decrease in accuracy. We used YamCha (Kudo and Matsumoto, 2003) to detect named entities, and we trained it on the SemEval full-text training sets. readers may refer to Kudo and Matsumoto (2003) for the detailed computation for obtaining w. The number of support vectors of SVMs was 71,766 ± 9.2%, which is twice as many as those used by Kudo and Matsumoto (2003) (34,996) in their experiments on the same task. This result conforms to the results reported in (Kudo and Matsumoto, 2003).  In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al, 2001) is used to efficiently mine the features in a low degree polynomial kernel space.  We use the following tools for syntactic processing: OpenNLP4 for POS tagging, YamCha (Kudo and Matsumoto, 2003) for constituent chunking, and the MALT parser (Nivre et al, 2007) for dependency parsing.
Exploiting Semantic Role Labeling WordNet And Wikipedia For Coreference Resolution In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns. The use of predicate-argument structure has been explored by Ponzetto and Strube (2006b; 2006a). This last conjecture is somewhat validated by Ponzetto and Strube (2006b), who reported that including predicate argument pairs as features improved the performance of a coreference resolver. Also, on WS-353, our hybrid sense-filtered variants and word-cos-ll obtained a correlation score higher than published results using WordNet-based measures (Jarmasz and Szpakowicz, 2003) (.33 to .35) and Wikipedia based methods (Ponzetto and Strube, 2006) (.19 to .48); and very close to the results obtained by thesaurus-based (Jarmasz and Szpakowicz, 2003) (.55) and LSA-based methods (Finkelstein et al, 2002) (.56). However, the use of related verbs is similar in spirit to Bean and Riloff's (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006). Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al). Our baseline coreference system implements the standard machine learning approach to coreference resolution (see Ng and Cardie (2002b), Ponzetto and Strube (2006), Yang and Su (2007), for instance), which consists of probabilistic classification and clustering, as described below. Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al (2001), Ng and Cardie (2002b), Kehler et al (2004), Ponzetto and Strube (2006)). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). The use of semantic knowledge for coreference resolution has been studied before in a number of works, among them (Ponzetto and Strube, 2006), (Bengtson and Roth, 2008), (Lee et al, 2011), and (Rahman and Ng, 2011). Semantic features: semantic class agreement, governing verb and its grammatical role, predicate (Ponzetto and Strube, 2006) For English, the number agreement and gender agreement features can be obtained through the gender corpus provided. Recently, Ponzetto and Strube (2006) suggest to mine semantic relatedness from Wikipedia, which can deal with the data sparseness problem suffered by using WordNet. Some researchers simply use the first sense (Soon et al, 2001) or all possible senses (Ponzetto and Strube, 2006a), while others overcome this problem with word sense disambiguation (Nicolae and Nicolae, 2006). Knowledge has also been mined from Wikipedia for measuring the semantic relatedness of two NPs, NPj and NPk (Ponzetto and Strube (2006a; 2007)). Contextual roles (Bean and Riloff, 2004), semantic relations (Ji et al, 2005), semantic roles (Ponzetto and Strube, 2006b; Kong et al, 2009), and animacy (Orasan and Evans, 2007) have also been exploited to improve coreference resolution. We use Wikipedia differently from (Ponzetto and Strube, 2006) who focus on using WikiRelate, a Wikipedia-based relatedness metric (Strube and Ponzetto, 2006).  As a result, researchers have re-adopted the once-popular knowledge-rich approach ,investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). Following Ponzetto and Strube (2006), we consider an anaphoric reference, NPi, correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition. Following previous work (e.g., Soon et al (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+1, NPi+2,..., NPj-1. In alternative, it has been recently shown that Wikipedia can be a promising source of semantic knowledge for coreference resolution between nominals (Ponzetto and Strube, 2006).
Supervised Noun Phrase Coreference Research: The First Fifteen Years The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago. As recently discussed in (Ng, 2010), the so called mention-pair model suffers from several design flaws which originate from the locally confined perspective of the model: Generation of (transitively) redundant pairs, as the formation of co reference sets (co reference clustering) is done after pairwise classification. (Ng, 2010) discusses the entity-mention model which operates on emerging co reference sets to create features describing the relation of an anaphor candidate and established co reference sets. This shortcoming has been addressed by entity-mention models, which relate a candidate mention to the full cluster of mentions predicted to be co referent so far (for more discussion on the model types, see, e.g., (Ng, 2010)). A better idea of the progress in the field can be obtained by reading recent survey articles (Ng, 2010) and tutorials (Ponzetto and Poesio, 2009) dedicated to this subject. In computational linguistics, the increasing availability of annotated coreference corpora has led to developments in machine learning approaches to automatic co reference resolution (see Ng, 2010). The task of automatic NP coreference resolution is to determine "which NPs in a text [...] refer to the same real-world entity" (Ng, 2010, p. 1396). In principle, this algorithm is too greedy and sometimes results in unreasonable partition (Ng, 2010). For a historical account and assessent of work in automated anaphora resolution in this period and afterwards, we direct the reader to Strube (2007), Ng (2010) and Stede (2012). Ng (2010) provides an excellent overview of the history and recent developments within the field. We follow the standard architecture where mentions are extracted in the first step, then they are clustered using a pair-wise classifier (see e.g., (Ng, 2010)). The two most common decoding algorithms often found in literature are the so-called BestFirst (henceforth BF) and ClosestFirst (CF) algorithms (Ng, 2010). Interested readers can refer to the literature review by Ng (2010). Excellent surveys are provided by Strube (2007) and Ng (2010). Unresolved anaphora can add significant translation ambiguity, and their incorrect translation can significantly decrease a reader's ability to understand a text. For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. 
Multi-Prototype Vector-Space Models of Word Meaning Current vector-space models of lexical semantics create a single “prototype” vector to represent the meaning of a word. However, due to lexical ambiguity, encoding word meaning with a single vector is problematic. This paper presents a method that uses clustering to produce multiple “sense-specific” vectors for each word. This approach provides a context-dependent vector representation of word meaning that naturally accommodates homonymy and polysemy. Experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models. More recently, Reisinger and Mooney (2010) present a method that uses clustering to produce multiple sense-specific vectors for each word. Following Reisinger and Mooney (2010), we also evaluated mixture models that combine the output of models with varying parameter settings. It is difficult to relate our results to Reisinger and Mooney (2010), due to differences in the training data and the vector representations it gives rise to. As a comparison, a baseline configuration with tf-idf weighting and the cosine similarity measure yields a correlation of 0.38 with our data and 0.49 in Reisinger and Mooney (2010). In general, our approach is quite close to the multi prototype models of Reisinger and Mooney (2010). Reisinger and Mooney (2010b) introduced a multi-prototype VSM where word sense discrimination is first applied by clustering contexts, and then prototypes are built using the contexts of the sense-labeled words. Instead of using only one representation per word, Reisinger and Mooney (2010b) proposed the multi prototype approach for vector-space models, which uses multiple representations to capture different senses and usages of a word. Pruned tf-idf (Reisinger and Mooney, 2010b) and ESA (Gabrilovich and Markovitch, 2007) are also included. Reisinger and Mooney (2010b) found pruning the low-value tf-idf features helps performance. We first tried movMF as in Reisinger and Mooney (2010b), but were unable to get decent results (only 31.5). Reisinger and Mooney (2010b) combined the two approaches and applied them to vector-space models, which was further improved in Reisinger and Mooney (2010a). The multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012): since we rely on occurrences of words in texts, this extension should be quite straightforward by turning our word-in-context classifiers into true word sense classifiers. A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. The top-down multi-prototype approach determines a number of senses for each word, and then clusters the occurrences of the word (Reisinger and Mooney, 2010) into these senses. Contextual term-vectors created using the Wikipedia corpus have shown to perform well on measuring word similarity (Reisinger and Mooney, 2010). Related work to ours is (Reisinger and Mooney, 2010) where exemplars of a word are first clustered and then prototype vectors are built.  Although in previous work, researchers try to capture word senses using different vectors (Reisinger and Mooney, 2010) from the same text corpus, this is in fact difficult in practice.
Unsupervised Semantic Role Labeling We present an unsupervised method for labelling the arguments of verbs with their semantic roles. Our bootstrapping algorithm makes initial unambiguous role assignments, and then iteratively updates the probability model on which future assignments are based. A novel aspect of our approach is the use of verb, slot, and noun class information as the basis for backing off in our probability model. We achieve 50–65% reduction in the error rate over an informed baseline, indicating the potential of our approach for a task that has heretofore relied on large amounts of manually generated training data. Swier and Stevenson (2004) innovated with an unsupervised approach to the problem, using a boot strapping algorithm, and achieved 87% accuracy. VerbNet and its semantic features have been used in a variety of NLP applications, such as semantic role labeling (Swier and Stevenson, 2004), inferencing (Zaenen et al, 2008), verb classification (Joanis et al, 2008), and information extraction (Maynard et al, 2009). Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the Verb Net verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface. There has been little research on semi-supervised learning for SRL.We refer to He and Gildea (2006) who tested active learning and co-training methods, but found little or no gain from semi-supervised learning, and to Swier and Stevenson (2004), who achieved good results using semi-supervised methods, but tested their methods on a small number of Verb Net roles, which have not been used by other SRL systems. To the best of our knowledge no system was able to reproduce the successful results of (Swier and Stevenson, 2004) on the PropBank role set. There are also some methods for unsupervised semantic role labeling (Swier and Stevenson, 2004), (Abend et al, 2009) that easily adapt across domains but their performances are not comparable to supervised systems. For example, VerbNet (derived from Levin? s [1993] work, Kipper et al, 2008) is widely used for a number of semantic processing tasks, including semantic role labeling (Swier and Stevenson, 2004), the creation of semantic parse trees (Shi and Mihalcea, 2005), and implicit argument resolution (Gerber and Chai, 2010). Swier and Stevenson (2004) and Swier and Stevenson (2005) presented the first model that does not use an SRL annotated corpus. Swier and Stevenson (2004) present an unsupervised method for labeling the arguments of verbs with their semantic roles. Swier and Stevenson (2004) were the first to introduce unsupervised SRL in an approach that used the VerbNet lexicon to guide unsupervised learning. Finally, Swier and Stevenson (2004) per form unsupervised semantic role labeling by using hand-crafted verb lexicons to replace supervised semantic role training data. Swier and Stevenson (2004, 2005), while addressing an unsupervised SRL task, greatly differ from us as their algorithm uses the VerbNet (Kipper et al, 2000) verb lexicon, in addition to supervised parses. Swier and Stevenson (2004) induce role labels with a bootstrapping scheme where the set of labeled instances is iteratively expanded using a classifier trained on previously labeled instances. Swier and Stevenson (2004) were the first to introduce an unsupervised semantic role labeling system. This is achieved by adopting the scoring method of Swier and Stevenson (2004), in which we compute the portion Frame of frame slots that can be mapped to an extracted argument, and the portion% Sent of extracted arguments from the sentence that can be mapped to the frame. For comparison, we also apply the iterative algorithm developed by Swier and Stevenson (2004), using the same bootstrapping parameters. For ease of comparison, we use the same verbs as in Swier and Stevenson (2004), except that we measure performance over a much larger superset of verbs. As a point of comparison, we apply the iterative back off model from Swier and Stevenson (2004), trained on 20% of the BNC, with our frame matcher and test data. Early unsupervised approaches to the SRL task include (Swier and Stevenson, 2004), where theVerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface.
Machine Transliteration Of Names In Arabic Texts We present a transliteration algorithm based on sound and spelling mappings using finite state machines. The transliteration models can be trained on relatively small lists of names. We introduce a new spelling-based model that is much more accurate than state-of-the-art phonetic-based models and can be trained on easier-toobtain training data. We apply our transliteration algorithm to the transliteration of names from Arabic into English. We report on the accuracy of our algorithm based on exact-matching criterion and based on human-subjective evaluation. We also compare the accuracy of our system to the accuracy of human translators. The spelling-based model we propose (described in detail in (Al-Onaizan and Knight, 2002)) directly maps English letter sequences into Arabic letter sequences, which are trained on a small English/Arabic name list without the need for English pronunciations. The adapted spelling-based generative model is similar to (Al-Onaizan and Knight, 2002). Transliteration from English to Arabic and Chinese is complicated (Al-Onaizan and Knight, 2002). Section 3 presents the letter-based transducer approach to Arabic-English transliteration proposed in (Al-Onaizan and Knight,2002), which we use as the main point of comparison for our substring-based models. Al-Onaizan and Knight (2002) find that a model mapping directly from English to Arabic letters outperforms the phoneme-to-letter model. The main point of comparison for the evaluation of our substring-based models of transliteration is the letter-based transducer proposed by (Al-Onaizan and Knight, 2002). Al-Onaizan and Knight (Al-Onaizan and Knight, 2002) proposed a spelling-based model which directly maps English letter sequences into Arabic letter sequences. of these entities is in most cases actually a (more or less phonetic) transliteration, see for example (Al-Onaizan and Knight, 2002). Al-Onaizan and Knight (2002) transliterated named entities in Arabic text to English by combining phonetic-based and spelling-based models, and re-ranking candidates with full-name web counts, named entities co-reference, and contextual web counts. These results are also comparable to other state-of-the-art statistical Arabic name trans literation systems such as (Al-Onaizan and Knight, 2002). A spelling-based model that directly maps English letter sequences into Arabic letters was developed by Al-Onaizan and Knight (2002). (Al-Onaizan and Knight 2002) showed that use of outside linguistic resources such as WWW counts of transliteration candidates can greatly boost transliteration accuracy. A spelling-based model is described in (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c) that directly maps English letter sequences into Arabic letter sequences with associated probability that are trained on a small English/Arabic name list without the need for English pronunciations. The phonetics-based and spelling-based models have been linearly combined into a single transliteration model in (Al-Onaizan and Knight, 2002b) for transliteration of Arabic named entities into English. To improve this performance, we plan to enrich the Arabic lexicon with more proper names, using either name recognition (Maloney and Niv, 1998) or a back translation approach after name recognition in English texts (Al-Onaizan and Knight, 2002). Subsequently, the performance of this system was greatly improved by combining different spelling and phonetic models (Al-Onaizan and Knight,2002). For example, the work of (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. Al-Onaizan and Knight (2002b) suggested that pronunciation can be skipped and the target language letters can be mapped directly to source language letters. Similarly, Al-Onaizan and Knight (2002a; 2002b) only made use of transliteration information alone and so was not directly comparable. 
Single Malt or Blended? A Study in Multilingual Parser Optimization We describe a two-stage optimization of the MaltParser system for the ten languages in the multilingual track of the CoNLL 2007 shared task on dependency parsing. The first stage consists in tuning a single-parsersystem for each language by optimizing parameters of the parsing algorithm, the fea ture model, and the learning algorithm. Thesecond stage consists in building an ensemble system that combines six different parsing strategies, extrapolating from the opti mal parameters settings for each language. When evaluated on the official test sets, the ensemble system significantly outperforms the single-parser system and achieves the highest average labeled attachment score.   One system (Hall et al, 2007b) extends this two-stage approach to a three-stage architecture where the parser and labeler generate an n-best list of parses which in turn is reranked.  This model is used by Marinov (2007) and in component parsers of the Nilsson ensemble system (Hall et al, 2007a). The most extreme case is the top performing Nilsson system (Hall et al, 2007a), which reached rank 1 for five languages and rank 2 for two more languages. However, Hall et al (2007a) point out that the official results for Chinese contained a bug, and the true performance of their system was actually much higher.  The same technique was also used by the winning team of the CoNLL 2007 Shared Task (Hall et al, 2007), combining six transition-based parsers. An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Halletal., 2007). Both Hall et al (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models. We implement a left-to-right arc-eager parsing model in a way that the parser scan through an input sequence from left to right and the right dependents are attached to their heads as soon as possible (Hall et al, 2007). Only one model was used for syntactic parsing in our system, in contrast to the existing work using an ensemble technique for further performance enhancement, e.g., (Hall et al, 2007).  This model is simple and works very well in the shared-tasks of CoNLL2006 (Nivre et al, 2006) and CoNLL2007 (Hall et al, 2007). In this work, we adopt a left-to-right arc-eager parsing model, that means that the parser scans the input sequence from left to right and right dependents are attached to their heads as soon as possible (Hall et al, 2007).  For the final system, feature models and training parameters were adapted from Hall et al (2007). The single parses were blended following the procedure of Hall et al (2007). system for English described in Hall et al (2007) was used as a baseline, and then optimized for this new task, focusing on feature selection.
Overview of BioNLP&rsquo;09 Shared Task on Event Extraction The paper presents the design and implementation of the BioNLP’09 Shared Task, and reports the final results with analysis. The shared task consists of three sub-tasks, each of which addresses bio-molecular event extraction at a different level of specificity. The data was developed based on the GENIA event corpus. The shared task was run over 12 weeks, drawing initial interest from 42 teams. Of these teams, 24 submitted final results. The evaluation results are encouraging, indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges. Due to the increasing demand on deep understanding of natural language text, negation recognition has been drawing more and more attention in recent years, with a series of shared tasks and workshops, however, with focus on cue detection and scope resolution, such as the BioNLP 2009 shared task for negative event detection (Kim et al, 2009) and the ACL 2010 Workshop for scope resolution of negation and speculation (Morante and Sporleder, 2010), followed by a special issue of Computational Linguistics (Morante and Sporleder, 2012) for modality and negation. In the BioNLP 09 Shared Task on Event Extraction (Kim et al, 2009), the most frequent predicates were nominals. The BioNLP 09 Shared Task on Event Extraction (Kim et al, 2009), the first large scale evaluation of biomedical event extraction systems, drew the participation of 24 groups and established a standard event representation scheme and datasets. The BioNLP 09 Shared Task (Kim et al., 2009) was the first shared task that provided a consistent data set and evaluation tools for extraction of such biological relations. The inclusion of full papers in the datasets is the only difference from Task of the BioNLP 2009 shared task (BioNLP09ST1) (Kim et al., 2009), which used the same task definition and abstracts dataset. This is in the tradition of the Textual Entailment or Information Validation paradigm (Dagan et al., 2009), and in contrast to aboutness annotation such as semantic roles (Carreras and Marquez, 2004) or the BioNLP2009 task (Kim et al, 2009) where negated relations are also labelled as positive. In recent years, several challenges and shared tasks have included the extraction of negations, typically as part of other tasks (e.g. the BioNLP 09 Shared Task 3 (Kim et al 2009)). We use molecular events as a case study and experiment on the BioNLP 09 data, which comprises a gold standard corpus of research abstracts manually annotated for events and negations (Kim et al 2009). The corpus used in this study is provided by the BioNLP 09 challenge (Kim et al 2009). The recent BioNLP 09 Shared Task on Event Extraction (Kim et al, 2009a) (below, BioNLP shared task) represented the first community-wide step toward the extraction of fine-grained event representations of information from biomolecular domain publications (Ananiadou et al, 2010). To estimate the capacity of the newly annotated resource to support the extraction of the targeted PTM events and the performance of current event extraction methods at open-domain PTM extraction, we performed a set of experiments using an event extraction method competitive with the state of the art, as established in the BioNLP shared task on event extraction (Kim et al, 2009a; Bjorne et al., 2009). We note that while these results fall notably below the best result reported for Phosphorylation events in the BioNLP shared task, they are comparable to the best results reported in the task for Regulation and Binding events (Kim et al, 2009a), suggesting that the dataset allows the extraction of the novel PTM events with Theme and Site arguments at levels comparable to multi-argument shared task events. Lee et al (2008) present E3Miner, a tool for automatically extracting information related to ubiquitination, and Kim et al (2009b) present a preliminary study adapting the E3Miner approach to the mining of acetylation events. It should be noted that while studies targeting single specific PTM types report better results than found in the initial evaluation presented here (in many cases dramatically so), different 25extraction targets and evaluation criteria complicate direct comparison. For the BioNLP 09 Shared Task (Kim et al, 2009), the first in the ongoing series, the organisers provided the participants with automatically generated syntactic analyses for the sentences from the annotated data. The term hedging is often used as an umbrella term to refer to an array of extra-factual phenomena in natural language and is the focus of the CoNLL-2010 Shared Task on Hedge Detection. The CoNLL-2010 Shared Task on Hedge Detection (Farkas et al, 2010) follows in the steps of the recent BioNLP? 09 Shared Task on Event Extraction (Kim et al, 2009), in which one task (speculation and negation detection) was concerned with notions related to hedging in biomedical abstracts. The BioNLP? 09 Shared Task on Event Extraction (Kim et al, 2009) dedicated a task to detecting negation and speculation in biomedical abstracts, based on the GENIA event corpus annotations. The majority of our corpora are available in the common stand-off style format introduced for the BioNLP 2009 Shared Task (BioNLP 09 ST) (Kim et al, 2009). A recent shared task in biomedical text mining, the BioNLP 09 Shared Task on Event Extraction (Kim et al, 2009), showed that the biomedical natural language processing (BioNLP) community is greatly interested in heading towards the extraction of deep, semantically rich relationships. The approach of classifying identified events into whether they fall under negation or speculation was followed by Sauri and Pustejovsky (2009) and the participants of the BioNLP? 09 Shared Task (Kim et al., 2009). The BioNLP 2009 Shared Task, a recent bio-molecular event extraction task, is one such task: analysis showed that the application of a parser correlated with high rank in the task (Kim et al, 2009).
A Noisy-Channel Approach To Question Answering We introduce a probabilistic noisychannel model for question answering and we show how it can be exploited in the context of an end-to-end QA system. Our noisy-channel system outperforms a stateof-the-art rule-based QA system that uses similar resources. We also show that the model we propose is flexible enough to accommodate within one mathematical framework many QA-specific resources and techniques, which range from the exploitation of WordNet, structured, and semi-structured databases to reasoning, and paraphrasing. Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. At a high level, the QA task boils down to only two essential steps (Echihabi and Marcu, 2003). Examples of 22 cases where the bag-of-words approach fails abound in QA literature; here we borrow an example used by Echihabi and Marcu (2003). a noisy-channel model which selects the most likely answer to a question (cf. (Echihabi and Marcu, 2003)). In contrast, Echihabi and Marcu (2003) introduce an SMT-based method for extracting the concrete answer in factoid QA. The approach of Echihabi and Marcu (2003) that uses translation probabilities to rank the answers achieves higher results on the same data set (an MRR of 0.325 versus our 0.141). In (Echihabi and Marcu, 2003) another form of combining strategies for advanced QA is proposed: (1) a knowledge-based Q/A implementation based on syntactic/semantic processing is combined using a maximum-entropy framework with (2) a statistical noisy-channel algorithm for Q/A and (3) a pattern-based approach that learn from Web data. We propose to study and develop several kernel methods that can operate in Support Vector Machines for determining the optimal strategies and compare the results with the Maximum Entropy combinations reported in (Echihabi and Marcu, 2003). As any QA system can virtually be decomposed into two major high-level components, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. In (Echihabi and Marcu, 2003) a noisy channel model for Q/A was introduced. For the experiments, we used PropBank (www.cis.upenn.edu/? ace) along with Penn TreeBank3 2 (www.cis.upenn.edu/? tree bank) (Echihabi and Marcu, 2003). Echihabi and Marcu (2003) align all paths in questions with trees for heuristically pruned answers.
Top Accuracy and Fast Dependency Parsing is not a Contradiction In addition to a high accuracy, short parsing and training times are the most important properties of a parser. However, pars ing and training times are still relatively long. To determine why, we analyzed thetime usage of a dependency parser. We il lustrate that the mapping of the features onto their weights in the support vectormachine is the major factor in time complexity. To resolve this problem, we implemented the passive-aggressive percep tron algorithm as a Hash Kernel. The Hash Kernel substantially improves the parsing times and takes into account thefeatures of negative examples built dur ing the training. This has lead to a higher accuracy. We could further increase theparsing and training speed with a paral lel feature extraction and a parallel parsing algorithm. We are convinced that the HashKernel and the parallelization can be ap plied successful to other NLP applicationsas well such as transition based depen dency parsers, phrase structrue parsers, and machine translation. In this paper, we use the following baseline parsers: MaltParser (Nivre et al, 2007) for transition-based parsing; MSTParser (McDonald et al, 2005) (with sibling 2-edge factors) and Bohnet Parser (Bohnet, 2010) (with general 2-edge factors) for graph-based parsing; and BerkeleyParser (Petrov et al, 2006) for constituency-based parsing. Parser: We used the second-order graph-based parser available in Mate-tools12 (Bohnet, 2010).  This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012). We also implement the feature mapping function as a hash kernel (Bohnet, 2010) and apply averaging (Collins, 2002), though for brevity we omit this from the pseudo code. It is difficult to compare our system for LAS because most systems evaluate on gold data (part-of-speech, lemmas and morphological information) like Bohnet (2010). we compare our system with the MATE parser (Bohnet, 2010), an improvement over the MST parser (McDonald et al, 2005) with hash kernels, using the MELT part-of-speech tagger (Denis and Sagot, 2009) and our own lemma tiser. Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionally negative features. The dependency labels were provided by the Bohnet parser (Bohnet, 2010) for English and by magyarlanc 2.0 (Zsibrita et al, 2013) for Hungarian. For POS tagging and lemmatization, we use TreeTagger (Schmid, 1994) and determine grammatical gender with the morphological layer of the MATE Tools (Bohnet, 2010). It uses an approximate exhaustive search for unlabeled parsing, then a separate arc label classifier is applied to label each arc. The Mateparser (Bohnet, 2010) is an efficient second order dependency parser that models the interaction between siblings as well as grandchildren (Carreras, 2007). The three dependency parsers are: MaltParser (Nivre et al, 2006), Mate (Bohnet, 2010) 2 and MSTParser (McDonald and Pereira, 2006). For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0.We conducted our dependency-based pre ordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al, 2012). The three methods outperformed the baseline (the state of the art parser for French which is a second order graph based method) (Bohnet, 2010). For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig. Please refer to Table 4 of Bohnet (2010) for the complete feature list. We add for each verb in VerbNet and for each noun in CoreLex its base class or basic type as an additional feature where words tagged by the mate tagger (Bohnet, 2010) as NN.* are treated as nouns and words tagged as VB.* as verbs.  Our system not only out performs the best single system (Bjorkelundetal., 2013) by 1.4%, but it also tops the ensemble system that combines three powerful parsers: the Mate parser (Bohnet, 2010), the Easy-First parser (Goldberg and Elhadad, 2010) and the Turbo parser (Martins et al, 2013) Impact of Sampling Methods We compare two sampling methods introduced in Section 3.2 with respect to their decoding efficiency. As a result, the hash kernel often improves accuracy as well as efficiency compared to traditional techniques that only make use of features that occur in gold standard parses (Bohnet, 2010).
An Annotation Scheme For Free Word Order Languages We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme. The resulting scheme reflects a stratificational notion of language, and makes only minimal assumpabout the interrelation of the particu- •lar representational strata. This type of model is used to facilitate the syntactic annotation of the NEGRA corpus of German newspaper texts (Skut et al, 1997). For our experiments, we use the NEGRA corpus (Skut et al, 1997). As data we use version 2 of the Negra (Skut et al1997) tree bank, with the common training.  According to Skut et al (1997) tree banks have to meet the following requirements: 1. that could best deal with the free word order displayed by Basque syntax (Skut et al, 1997). In contrast, some other tree banks, such as the German NeGra and TIGER tree banks allow annotation with crossing branches (Skut et al, 1997). Non-local dependencies can then be expressed directly by grouping all dependent elements under a single node. Our data source is the German NeGra tree bank (Skut et al, 1997). The parsing models we present are trained and tested on the NEGRA corpus (Skut et al, 1997), a hand parsed corpus of German newspaper text containing approximately 20,000 sentences. The present paper addresses this question by proposing a probabilistic parsing model trained on Negra (Skut et al, 1997), a syntactically annotated corpus for German. The annotation scheme (Skut et al, 1997) is modeled to a certain extent on that of the Penn Treebank (Marcuset al, 1993), with crucial differences.  German is considerably more in ectional which means that discarding functional information is more harmful, and which explains why the NEGRA an notation has been conceived to be quite at (Skut et al, 1997). The factors used in the algorithms and the algorithms themselves are evaluated on a German corpus annotated with syntactic and co reference in formation (Negra) (Skut et al, 1997).  CKK uses the Dubey and Keller (2003) parser, which is trained on the Negra corpus (Skut et al, 1997). Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negra treebank (Skut et al, 1997) reports that lexicalization of PCFGs decrease the parsing accuracy when parsing Negra's flat constituent structures. We next tested UML-DOP on two additional domains which were also used in Klein and Manning (2004) and Bod (2006): the German NEGRA 10 (Skut et al 1997) and the Chinese CTB10 (Xue et al 2002) both containing 2200+ sentences 10 words after removing punctuation. A comparison of unlexicalised PCFG parsing (Kubler, 2005) trained and evaluated on the German NEGRA (Skut et al, 1997) and the Tu? Ba D/Z (Telljohann et al, 2004) tree banks using LoPar (Schmid, 2000) shows a difference in parsing results of about 16%, using the PARSEVAL metric (Black et al, 1991). 
Transformation Based Learning In The Fast Lane Transformation-based learning has been successfully employed to solve many natural language processing problems. It achieves state-of-the-art performance on many natural language processing tasks and does not overtrain easily. However, it does have a serious drawback: the training time is often intorelably long, especially on the large corpora which are often used in NLP. In this paper, we present a novel and realistic method for speeding up the training time of a transformation-based learner without sacrificing performance. The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems: a standard transformation-based learner, and the ICA system (Hepple, 2000). The results of these experiments show that our system is able to achieve a significant improvement in training time while still achieving the same performance as a standard transformation-based learner. This is a valuable contribution to systems and algorithms which utilize transformation-based learning at any part of the execution. For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed. The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. We replaced the English part-of-speech tags with those generated by a transformation-based learner (Ngai and Florian, 2001). A TB Lchunker trained on Wall Street Journal corpus (Ngai and Florian, 2001) maps each word to an associated chunk tag, encoding chunk type and relative word position (beginning of an NP, inside a VP, etc.). Then non-recursive, or basic, noun phrases (NPB) are identified using the TBL method reported in (Ngai and Florian, 2001). The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). In order to benchmark our results with the CRF models, we reimplemented the supertagger model proposed by Baldwin (2005b) which simply takes FNTBL 1.1 (Ngai and Florian, 2001) off the shelf and trains it over our particular training set. With the POS extraction method, we first Penn tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001), training over the Brownand WSJ corpora with some spelling, number and hyphenation normalisation. The sentence was first part-of-speech tagged and chunked with the fnTBL transformation based learning tools (Ngai and Florian, 2001). This package, and the TBL framework itself, are described in detail by Ngai and Florian (2001). The feature set used in the TBL algorithm is similar to those used in the NP Chunking task in (Ngai and Florian, 2001). We use the LMR tagging output to train a Transformation Based learner, using fast TBL (Ngai and Florian, 2001). For our purposes, we use a Penn tree bank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001). Essentially, we used a POS tagger and chunker (both built using fnTBL 1.0 (Ngai and Florian, 2001)) to first (re) tag the BNC. With the POS extraction method, we first tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001) trained over the Brown and WSJ corpora and based on the Penn POS tag set. Chunking An in-house chunker implemented with fnTBL, a transformation based learner (Ngaiand Florian, 2001), and trained on the British National Corpus (BNC). The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian,2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state ma chines for very fast tagging. We used Florian and Ngai's Fast TBL system (fnTBL) (Ngai and Florian, 2001) to train rules using dis fluency annotated conversational speech data. Similarly, the space efficient algorithm using compound questions at the end of Section 2.2.1 can be thought of as a static probabilistic version of the efficient TBL of Ngai and Florian (2001).
Investigating Regular Sense Extensions based on Intersective Levin Classes In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases. We see verb classes as the key to making generalizations about regular extensions of meaning. Current approaches to English classification, Levin classes and WordNet, have limitations in their applicability that impede their utility as general classification schemes. We present a refinement of Levin classes, intersective sets, which are a more fine-grained classification and have more coherent sets of syntactic frames and associated semantic components. We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the original Levin classes. We also have begun to examine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties. Complicating the issue is the phenomenon of regular sense extensions (Dang et al, 1998), where what once may have been coercion has become entrenched and is now seen as a different sense of the verb. Dang et al (1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intersective Levin classes, which are a more fine-grained, syntactically and semantically coherent refinement of basic Levin classes. VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al, 1998), aimed at achieving more coherent classes both semantically and syntactically. Dang et al (1998) have supplemented the taxonomy with intersective classes: special classes for verbs which share membership of more than one Levin class because of regular polysemy. One can grasp this easily by looking at intersective Levin classes (Dang et al., 1998), created by grouping together subsets of existing classes with overlapping members. Several clusters were produced which represent linguistically plausible inter sective classes (e.g. A3) (Dang et al, 1998) rather than single classes. A very good candidate seems to be the Intersective Levin class (ILC) (Dang et al, 1998) that can be found as well in other predicate resources like PB and VerbNet (VN) (Kipper et al, 2000). This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levin's classification, called Intersective Levin classes (ILCs) (Dang et al, 1998). A very good candidate seems to be the Intersective Levin classes (Dang et al, 1998) that can be found as well in other predicate resources like PropBank and VerbNet (Kipper et al., 2000). This constraint of having the same semantic roles is further ensured inside the VerbNetlexi con that is constructed based on a more refined version of the Levin classification called Intersective Levin classes (Dang et al, 1998). Palmer (1999) and Dang et al (1998) argue that the use of syntactic frames and verb classes can simplify the definition of different verb senses. We also plan to experiment with different classification schemes for verb semantics uch as WordNet (Miller et al, 1990) and intersective Levin classes (Dang et al, 1998). This reorganization, which was facilitated by the use of inter sective Levin classes (Dang et al, 1998), refined the classes to account for semantic and syntactic divergences within a class. We first tried using the verb classes in VerbNet (Dang et al, 1998).
Fully Unsupervised Discovery of Concept-Specific Relationships by Web Mining We present a web mining method for discovering and enhancing relationships in which a specified concept (word class) participates. We discover a whole range of relationships focused on the given concept, rather than generic known relationships as in most previous work. Our method is based on clustering patterns that contain concept words and other words related to them. We evaluate the method on three different rich concepts and find that in each case the method generates a broad variety of relationships with good precision. (Davidov et al, 2007) introduce the use of term frequency patterns for relationship discovery. For example, Pantel and Pennacchiotti (2008) linked instantiations of a set of semantic relations into existing semantic ontologies and Davidov et al (2007) employed seed concepts from a given semantic class to discover relations shared by concepts in that class. As a pre-requisite to extracting relations among pairs of classes, the method described in (Davidov et al., 2007) extracts class instances from unstructured Web documents, by submitting pairs of instances as queries and analyzing the contents of the top 1,000 documents returned by a Web search engine. The evaluation methodology is also quite different, as the instance sets acquired based on the input seed instances in (Davidov et al, 2007) are only evaluated for three hand-picked classes, with precision scores of 90% for names of countries, 87% for fish species and 68% for instances of constellations. (Davidov et al, 2007) proposed a method for unsupervised discovery of concept specific relations, requiring initial word seeds. Pattern driven search engine queries allow to access such information and gather the required data very efficiently (Davidov et al, 2007). For example, (Etzioni et al, 2004) discovered a set of countries and (Davidov et al, 2007) discovered diverse country relationships, including location relationships between a country and its capital and a country and its rivers. Following (Davidov et al, 2007) we seek symmetric patterns to retrieve concept terms. To use clusters for classification we define a HITS measure similar to that of (Davidov et al, 2007), reflecting the affinity of a given nominal pair to a given cluster. Davidov et al (2007) developed a web mining approach for discovering relations in which a specified concept participates based on clustering patterns in which the concept words and other words appear. The precision observed for this task is comparable to precision obtained for Country-Capital and Country-Language in a previous single-language acquisition study (Davidov et al, 2007). On-line usage of web queries is less frequent and was used mainly in semantic acquisition applications: the discovery of semantic verb relations (Chklovski and Pantel, 2004), the acquisition of entailment relations (Szpektor et al, 2004), and the discovery of concept-specific relationships (Davidov et al, 2007). Our web mining part follows common pattern based retrieval practice (Davidov et al, 2007). Following (Davidov et al, 2007), we seek symmetric patterns to retrieve concept terms.
Vector-based Models of Semantic Composition This paper proposes a framework for representing the meaning of phrases and sentences in vector space. Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments. Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and K additional knowledge. In both experiments, we compare the SVS model against the state-of-the art model by Mitchell and Lapata 2008 (henceforth M&L; cf.  While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations. Note that Spearman's ? is often a little lower than Pear son? s (Mitchell and Lapata, 2008). Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression. Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting. And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors. Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor). As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now. Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition. As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition. In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008). Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995). Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001). The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression. For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset. We use other WSM settings following Mitchell and Lapata (2008). Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition. We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008).
Improved Source-Channel Models For Chinese Word Segmentation This paper presents a Chinese word segmentation system that uses improved sourcechannel models of Chinese sentence generation. Chinese words are defined as one of the following four types: lexicon words, morphologically derived words, factoids, and named entities. Our system provides a unified approach to the four fundamental features of word-level Chinese language processing: (1) word segmentation, (2) morphological analysis, (3) factoid detection, and (4) named entity recognition. The performance of the system is evaluated on a manually annotated test set, and is also compared with several state-ofthe-art systems, taking into account the fact that the definition of Chinese words often varies from system to system. The superiority of the unified approach has been demonstrated empirically in Gao et al (2003), and will also be discussed in Section 5. All feature functions in Figure 1, except the NW function, are derived from models presented in (Gao et al, 2003).  The class mode score we used can be written as generate Score ()= P (|) P ()## w w c c The P (C) and P (?#? C) is similar to the one defined by Gao et al (2003). A Chinese resume C=c1',c2',...,ck' is first tokenized into C= w1,w2,...,wk with a Chinese word segmentation system LSP (Gao et al., 2003). Then, we use a back-off schema (Katz, 1987) to deal with the data sparseness problem when estimating the probability P (L) (Gao et al, 2003). We selected SVMlight (Joachims, 1999) as the SVM classifier toolkit and LSP (Gao et al, 2003) for Chinese word segmentation and named entity identification. In Gao et al (2003), an approach based on source-channel model for Chinese word segmentation was proposed. The word segmentation system is developed based on a source-channel model similar to that described in (Gao et al, 2003). That is if we collect all words seen in the training data and store them into a lexicon, then each word in a test set is either a lexicon word or an OOV (out of vocabulary) word (Gao et al., 2003). In our experiments we identify SL (Chinese) NEs implicitly found by the word segmentation algorithm stated in Gao et al (2003), and the dictionaries for translating NEs include the same one used for QSL-TFIDF, and the LDC Chinese/English NE dictionary. The Chinese side of all corpora are segmented into words by our implementation of (Gao et al, 2003). Gao et al (2003) uses class-based language for word segmentation where some word category information can be incorporated. To identify entities, we use a CRF-based named entity tagger (Finkel et al, 2005) and a Chinese word breaker (Gao et al, 2003) for English and Chinese corpora, respectively.
Parser Combination By Reparsing We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers. We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers. Parser combination has been shown to be a powerful way to obtain very high accuracy in dependency parsing (Sagae and Lavie, 2006). In the second approach, combination of the three dependency parsers is done according to the maximum spanning tree combination scheme of Sagae and Lavie (2006), which results in high accuracy of surface dependencies. To illustrate how this framework allows for improvements in the accuracy of dependency parsing to be used directly to improve the accuracy of HPSG parsing, we showed that by combining the results of different dependency parsers using the search-based parsing ensemble approach of (Sagae and Lavie, 2006), we obtain improved HPSG parsing accuracy as a result of the improved dependency accuracy. Constant et al (2013) proposed to combine pipeline and joint systems in a reparser (Sagae and Lavie, 2006), and ranked first at the Shared Task. A key difference with previous work on shift reduce dependency (Nivre et al, 2006) and CFG (Sagae and Lavie, 2006b) parsing is that, for CCG, there are many more shift actions - a shift action for each word-lexical category pair. Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007). The heterogeneous parser used in this paper is based on the shift-reduce parsing algorithm described in Sagae and Lavie (2006a) and Wang et al (2006). Moreover, beam search strategies can be used to expand the search space of a shift-reduce-based heterogeneous parser (Sagae and Lavie, 2006a). This group of features are completely identical to those used in Sagae and Lavie (2006a). Sagae and Lavie (2006) demonstrated that a simple combination scheme of the outputs of different parsers can obtain substantially improved accuracies. (Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents. Sagae and Lavie (2006) improve this second scheme by introducing a threshold for the constituent count, and search for the tree with the largest number of count from all the possible constituent combination. Sagae and Lavie (2006) combine 5 parsers to obtain a score of 92.1, while they report a score of 91.0 for the best single parser in their paper. Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006). However, as suggested in (Sagae and Lavie 2006), this feature favours precision over recall. To solve this issue, Sagae and Lavie (2006) use a threshold to balance them.  We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be obtained, for example, by using different learning techniques or changing parsing direction from forward to backward (of course, even greater gains may be achieved when different algorithms are used, although this is not pursued here).
A Mention-Synchronous Coreference Resolution Algorithm Based On The Bell Tree This paper proposes a new approach for resolution which uses the tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained. Luo et al (2004) also apply beam search at test time, but use a static assignment of antecedents and learns log-linear model using batch learning. Luo et al (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree. As observed by Luo et al (2004), if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score - significantly higher than any published system. To cope with this computational complexity, Luo employs the algorithm proposed in Luo et al (2004) to heuristically search for the most probable partition by performing a beam search through a Bell tree. Details of this process can be found in Luo et al (2004).  Luo et al (2004) pointed out that one can obtain a very high MUC score simply by lumping all mentions together.    Luo et al (2004) propose a system that performs coreference resolution by doing search in a large space of entities. As a base line, we follow the solution proposed in (Luo et al, 2004) to design a set of first-order features. For example, Luo et al (2004) apply the ANY predicate to generate cluster-level features for their entity-mention model, which does not perform as well as the mention-pair model. Memorization features have been used as binary-valued features indicating the presence or absence of their words (Luo et al, 2004) or as probabilistic features indicating the probability that the two heads are coreferent according to the training data (Ng, 2007b).  Our existing co-reference module is a state-of the-art system that produces very competitive results compared to other existing systems (Luo et al., 2004). Distances have been used in e.g. Luo et al (2004).  Luo et al (2004) perform the clustering step within a Bell tree representation. They report considerable improvements over state-of the-art systems including Luo et al (2004).
Supertagged Phrase-Based Statistical Machine Translation Until quite recently, extending Phrase-based Statistical Machine Translation (PBSMT) with syntactic structure caused system performance to deteriorate. In this work we show that incorporating lexical syntactic descriptions in the form of supertags can yield significantly better PBSMT systems. We describe a novel PBSMT model that integrates supertags into the target language model and the target side of the translation model. Two kinds of supertags are employed: those from Lexicalized Tree-Adjoining Grammar and Combinatory Categorial Grammar. Despite the differences between these two approaches, the supertaggers give similar improvements. In addition to supertagging, we also explore the utility of a surface global grammaticality measure based on combinatory operators. We perform various experiments on the Arabic to English NIST 2005 test set addressing issues such as sparseness, scalability and the utility of system subcomponents. Our best result (0.4688 BLEU) improves by 6.1% relative to a state-of-theart PBSMT model, which compares very favourably with the leading systems on the NIST 2005 task. This is a simple way of including syntactic information in a phrase-based model, and has also been suggested by Hassan et al (2007). For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation. Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. Supertagging (Hassan et al, 2007b): incorporating lexical syntactic descriptions, in the form of supertags, to the language model and target side of the translation model in order to better inform decoding. We have previously shown this approach to be very effective for both case and punctuation restoration (Hassan et al, 2007a). (Huang and Knight, 2006) and (Hassan et al, 2007) introduce relabeling and supertagging on the target side, respectively. Two kinds of supertags, from Lexicalized Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG), have been used as lexical syntactic descriptions (Hassan et al, 2007) for phrase based SMT (Koehn et al, 2007). Birch et al (2007) and Hassan et al (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. Hassan et al (2007) improve the statistical phrase based MT model by injecting supertags, lexical information such as the POS tag of the word and its subcategorization information, into the phrase table, resulting in generalized phrases with placeholders in them. Hassan et al (2007) and Birch et al (2007) use supertag n-gram LMs. This analysis then lets us abstract and encode many local and some nonlocal syntactic structures as complex tags (dynamically, as opposed to the static complex tags as proposed by Birch et al (2007) and Hassan et al (2007)). A similar approach based on supertagging was proposed by Hassan et al (2007). They used both CCG supertags and LTAG supertags in Arabic-to-English phrase-based translation and have reported about 6% relative improvement in BLEU scores. Hassan et al (2007) noticed that the target side POS sequences could be scored, much as we do in this work.
Wide-Coverage Semantic Representations From A CCG Parser This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser. Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation. We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP. For evaluation the parser returns dependency structures, but we have also developed a module which builds first order semantic representations from the derivations, which can be used for inference (Bos et al, 2004). CCG-based syntactic parsing (Bos et al, 2004). Bos et al (2004) present an algorithm that learns CCG lexicons with semantics but requires fully specified CCG derivations in the training data. This leads a traditional parsing system with a direct mapping from the parse tree to a semantic representation to fail to achieve a parse on 35% percent of the stories, and as such could not be used (Bos et al, 2004). The nlp tools used by ASKNet are the C& amp; C parser (Clark and Curran, 2007) and the semantic analysis program Boxer (Bos et al, 2004), which operates on the ccg derivations output by the parser to produce a first-order representation. It makes use of two NLP tools, the Clark and Curran parser (Clark and Curran, 2004) and the semantic analysis tool Boxer (Bos et al, 2004), both of which are part of the C& amp; C Toolkit1. The parser is based on Combinatory Categorial Grammar (CCG) and has been trained on 40,000 manually annotated sentences of the WSJ. Bos et al (2004) derive semantic interpretations from a wide-coverage categorial grammar. There are several differences between this and RASP-RMRS, but the most important arise from the differences between CCG and RASP. On the practical side, we have corpora with CCG derivations for each sentence (Hockenmaier and Steed man, 2007), a wide-coverage parser trained on that corpus (Clark and Curran, 2007) and a system for converting CCG derivations into semantic representations (Bos et al, 2004). However, despite being treated as a single unified grammar formalism, each of these authors use variations of CCG which differ primarily on which combinators are included in the grammar and the restrictions that are put on them. One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations ,e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al, 1998), and first-order logical forms (Bos et al, 2004). Recently, the state of the art in wide coverage parsing has made wide-coverage semantic processing come into the reach of research in computational semantics (Bos et al., 2004). For example, Bos et al (2004) build semantic representations from the parse derivations of a CCG parser, and the English Resource Grammar (ERG) (Toutanovaet al, 2005) provides a semantic representation using minimal recursion semantics. Additionally, Bos et al (2004) consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon. Despite some recent advances in this direction (Bos et al, 2004), it is still the case that it is hard to obtain deep semantic analyses which are accurate enough to support logical inference (Lev et al, 2004). As mentioned at the beginning of this paper, the conversion of unrestricted text to some logical form has experienced a recent revival recently (Bos et al, 2004). At the other end of the spectrum, Bos et al (Bos et al., 2004) have developed a broad-coverage parser to translate sentences to a logic based on discourse representation theory. Bos et al (2004) present an algorithm for building semantic representations from CCG parses but requires fully specified CCG derivations in the training data. For example, a system that translates the output of a robust CCG parser into semantic representations has been developed (Bosetal., 2004). In this section, we present a method to derive TDL semantic representations from HPSG parse trees, adopting, in part, a previous method (Bos et al, 2004). We used the Penn Treebank (Marcus, 1994) Section 22 (1,527 sentences) to develop and evaluate the proposed method and Section 23 (2,144 sentences) as the final test set. We measured the coverage of the construction of TDL semantic representations, in the manner described in a previous study (Bos et al, 2004). Although this number is slightly less than 92.3%, as reported by Bos et al, (2004), it seems reasonable to say that the proposed method attained a relatively high coverage, given the expressive power of TDL. The construction of TDL semantic representations failed for 11.7% of the sentences.
Alignment By Agreement We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions. Moreover, including predictions of bi-directional IBM Model 4 and model of Liang et al (2006) as features, we achieve an absolute AER of 3.8 on the English French Hansards alignment task - the best AER result published on this task to date. By also including as features the posteriors of the model of Liang et al (2006), we achieve AER of 3.8, and 96.7/95.5 precision/recall. Liang et al (2006) propose agreement-based learning, which jointly learns probabilities by maximizing a combination of likelihood and agreement between two directional models. Training is performed using the agreement-based learning method which encourages the directional models to overlap (Liang et al, 2006). Concerning the former, we trained an unsupervised model with the Berkeley aligner, an implementation of the symmetric word-alignment model described by Liang et al (2006). The state-of-the-art unsupervised Berkeley aligner (Liang et al, 2006) with default setting is used to construct word alignments. We used version two of the Berkeley alignment model (Liang et al, 2006), with the posterior threshold set at 0.5. The reordering metrics require alignments which were created using the Berkeley word alignment package version 1.1 (Liang et al., 2006), with the posterior probability to being 0.5. As is standard in unsupervised alignment models, we initialized the translation parameters pt by first training 5 iterations of IBM Model 1 using the joint training algorithm of Liang et al (2006), and then trained our model for 5 EM iterations. Previous evaluation of Addicter shows that hypothesis-reference alignment coverage (in terms of discovered word pairs) directly influences error analysis quality; to increase alignment coverage we used Berkeley aligner (Liang et al, 2006) and trained it on and applied it to the whole set of reference-hypothesis pairs for every language pair. They could reach an AER of 3.8 on the same task, but only if they also included the posteriors of the model of Liang et al (2006). We used two well studied unsupervised aligners, GIZA++ (Och and Ney, 2003) and HMM (Liang et al, 2006) and one supervised aligner, ITG (Haghighi et al, 2009) as representatives in this work. We used three aligners in this work: GIZA++ (Och and Ney, 2003), jointly trained HMM (Liang et al, 2006), and ITG (Haghighi et al, 2009). The HMM aligner used in this work was due to Liang et al (2006). Second, an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al, 2006). As suggested by Liang et al (2006), we can group the distortion parameters into a few buckets. As an additional experiment, we tested the Cross EM aligner (Liang et al, 2006) from the Berkeley Aligner package on the MSR data. Exceptions where discriminative SMT has been used on large training data are Liang et al (2006a) who trained 1.5 million features on 67,000 sentences. Training data for discriminative learning are prepared by comparing a 100-best list of translations against a single reference using smoothed per sentence BLEU (Liang et al, 2006a). We performed word alignment using a cross EM word aligner (Liang et al, 2006).
Generating Referring Expressions: Boolean Extensions Of The Incremental Algorithm This paper brings a logical perspective to the generation of referring expressions, addressing the incompleteness of existing algorithms in this area. After studying references to individual objects, we discuss references to sets, including Boolean descriptions that make use of negated and disjoined properties. To guarantee that a distinguishing description is generated whenever such descriptions exist, the paper proposes generalizations and extensions of the Incremental Therefore, van Deemter (2002) has extended the set of descriptors to boolean combinations of attributes, including negations. Recent extensions address some of its shortcomings, such as negated and disjoined properties (van Deemter, 2002) and an account of salience for generating contextually appropriate shorter REs (Krahmer and Theune, 2002). The Context-Sensitive extension (Krahmer and Theune, 2002) is able to generate referring expressions for the most salient entity in a context; the Boolean Expressions algorithm (van Deemter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have a handle; and the Sets algorithm (van Deemter, 2002) extends the basic approach to references to sets, as in the red cups. Recently, algorithms have been applied to the identification of sets of objects rather than individuals [Bateman 1999, Stone 2000, Krahmer, v. Erk, and Verweg 2001], and the repertoire of descriptions has been extended to boolean combinations of attributes, including negations [van Deemter 2002]. This is because these operators appear only in embedded boolean combinations [van Deemter 2002], which are the basis for building larger varieties of expressions [Horacek 2004]. Subsequent work on referring expression generation has expanded the logical framework to allow reference by negation (the dog that is not black) and references to multiple entities (the brown or black dogs) (van Deemter, 2002), explored different search algorithms for finding the minimal description (e.g., Horacek (2003)) and offered different representation frameworks like graph theory (Krahmer et al, 2003) as alternatives to AVMs. Transformation Rules: In connection with reference to sets, it has been proposed to use the Q-M algorithm (McCluskey,) to find the shortest formula equivalent to a given input formula (van Deemter, 2002). GRE has been dominated by Dale and Reiter's (1995) Incremental Algorithm (IA), one version of which, generalised to deal with non-disjunctive plural references, is shown in Algorithm 1 (van Deemter, 2002). Such a description would be returned by a generalised version of Algorithm 1 proposed by van Deemter (2002). Unlike van Deemter (2002), we only focus on disjunction, leaving negation aside. Evaluation results showed that these principles are on the right track, with significantly better performance over a previous model (van Deemter, 2002). For instance, the classical Dale and Reiter algorithms compute purely conjunctive formulas; van Deemter (2002) extends this language by adding the other propositional connectives, whereas Dale and Haddock (1991) extends it by allowing existential quantification. Although we agree with van Deemter (2002) and others that the careful use of negation and disjunction can improve REs, these connectives must not be overused. Recently, algorithms have also been developed to the identification of sets of objects rather than individuals (Bateman 1999, Stone 2000, Krahmer, v. Erk, and Verweg 2001), and the repertoire of descriptions has been extended to boolean combinations of attributes, including negations (van Deemter 2002). An exception to this method is the work by Paraboni and van Deemter (2002) who use hierarchical object representations to refer to parts of a book (figures, sections., etc.).
Centering: A Framework For Modeling The Local Coherence Of Discourse the original motivations for centering, the basic definitions underlying the centering framework, and the original theoretical claims. This paper attempts to meet that need. To accomplish this goal, we have chosen to remove descriptions of many open research questions posed in Grosz, Joshi, and Weinstein (1986) as well as solutions that were only partially developed. We have also greatly shortened the discussion of criteria for and constraints on a possible semantic theory as a foundation for this work. The centering model (Grosz et al, 1995) focuses on the resolution of inter-sentential anaphora. Possible strategies for treating sentence-level anaphora within the centering framework are processing sentences linearly one clause at a time (as suggested by Grosz et al (1995)). To summarize the results of our empirical evaluation, we claim that our proposal based on functional criteria leads to substantively better results for languages with free word order than the linear approach suggested by Grosz et al (1995) and the two approaches which prefer inter-sentential or intra-sentential ntecedents. Crucial for the evaluation of the centering model (Grosz et al, 1995) and its applicability to naturally occurring discourse is the lack of a specification concerning how to handle complex sentences and internal sentential anaphora. One popular model, the centering model (Grosz et al, 1995), uses a ranking of discourse entities realized in particular sentence sand computes transitions between adjacent sentences to provide insight in the felicity of texts. Linguists have also studied various aspects of text flow like the centering theory (Grosz et al, 1995) among the most influential contributions. Most models attempting to capture local coherence between sentences were based on or inspired by centering theory (Grosz et al, 1995), which postulated strong links between the center of attention in comprehension of adjacent sentences and syntactic position and form of reference. Finally, Poesio examined the hypothesis that finding the anchor of a BD involves knowing which entities are the CB and the CP in the sense of Centering (Grosz et al, 1995). Centering Theory (CT) characterises the local coherence of a text on the basis of the discourse entities in a text and the way in which they are introduced (Grosz et al, 1995). Centering Theory (CT, Grosz et al 1995) is an entity-based theory of local coherence, which claims that certain entities mentioned in an utterance are more central than others and that this property constrains a speaker's use of certain referring expressions. Centering Theory (Grosz et al, 1995) has been an influential framework for modelling entity coherence in computational linguistics in the last two decades. Entity-based theories of discourse (e.g., (Grosz et al, 1995)) claim that a coherent text segment tends to focus on a specific entity. Summarised below are some issues specific to anaphora resolution in spoken dialogues (see also Byron and Stent (1998) who mention some of these problems in their account of the Centering model (Grosz et al, 1995)). Byron and Stent (1998) present extensions of the centering model (Grosz et al, 1995) for spoken dialogue and identify several problems with the model.  The main new aspect of the markup scheme, especially as far as our studies of salience were concerned, are the elements used to annotate potential utterance sin the sense of Centering (Grosz et al, 1995). Our model is inspired by Centering (Grosz et al, 1995) and other entity-based models of coherence (Barzilay and Lapata, 2005) in which an entity is in focus through a sequence of sentences. Grosz et al (1995) give the following example of the Am SHIFT pattern. Centering's Rule 1 states that if any element of the previous utterance's forward looking center list is realized in the current utterance as a pronoun, then the backward looking center must be realized as a pronoun as well (Grosz et al, 1995). pronouns involve local focusing while full lexical forms involve global focusing (Grosz et al, 1995).
Language Model Based Arabic Word Segmentation  The segmentation model is similar to the one presented by Lee et al (2003), and obtains an accuracy of about 98%. For training, we used the non-UN portion of the NIST training corpora, which was segmented using an HMMsegmenter (Lee et al, 2003). The Arabic data was preprocessed using an HMM segmenter that splits off attached prepositional phrases, personal pronouns, and the future marker (Lee et al, 2003). Lee et al (2003) demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation.  As in (Lee et al, 2003), we used unsupervised training data which is automatically segmented to discover previously unseen stems. context sensitive Arabic stemmer (Lee et al 2003) to overcome the morphological complexity of Arabic. To separate the Arabic white-space delimited words into segments, we use a segmentation model similar to the one presented by (Lee et al, 2003). We propose in the following an extension to the aforementioned FST model, where we jointly determines not only diacritics but segmentation into affixes as described in (Lee et al, 2003). An Arabicsegmenter similar to (Lee et al, 2003) provides the segmentation features. This produces a segmentation view of the arabic source words (Lee et al., 2003). In (Lee et al, 2003) a statistical approach for Arabic word segmentation was presented.  The algorithm is inspired with the work on the segmentation of Arabic words (Lee et al, 2003). Lee et al (2003) use a corpus of manually segmented words, which appears to be a subset of the first release of the ATB (110,000 words), and thus comparable to our training corpus. Lee et al (2003) show that the unsupervised use of the large corpus for stem identification increases accuracy. Lee et al (2003) addressed supervised word segmentation in Arabic and have some aspects similar to our approach. As estimated by (Lee et al, 2003), we set the probability of ?u/k? to be 1E? 9. We found that the value proposed by (Lee et al, 2003) for Arabic gives good results also for Hebrew. Moving on to Arabic, Lee et al (2003) describe a word segmentation system for Arabic that uses an n gram language model over morphemes.
Disambiguating Nouns Verbs And Adjectives Using Automatically Acquired Selectional Preferences Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information. We evaluate WSD using selectional preferences acquired for English adjective–noun, subject, and direct object grammatical relationships with respect to a standard test corpus. The selectional preferences are specific to verb or adjective classes, rather than individual word forms, so they can be used to disambiguate the co-occurring adjectives and verbs, rather than just the nominal argument heads. We also investigate use of the one-senseper-discourse heuristic to propagate a sense tag for a word to other occurrences of the same word within the current document in order to increase coverage. Although the preferences perform well in comparison with other unsupervised WSD systems on the same corpus, the results show that for many applications, further knowledge sources would be required to achieve an adequate level of accuracy and coverage. In addition to quantifying performance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance. They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). Selectional preferences do not only play an important role in human sentence processing (McRae et al, 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al, 2007). Note that these are the two extremes of semantic granularity in WordNet, and we plan to experiment with intermediate representation levels in future research (c.f. Li and Abe (1998), McCarthy and Carroll (2003), Xiong et al (2005), Fujita et al (2007)). Agirre and Martinez (2001) and Zheng et al (2007) adopted the Class-only Model in research, while in McCarthy and Carroll (2003) and Merlo and Stevenson (2001) the Word-Class Model is employed. McCarthy and Carroll (2003) reports that the Word-Class Model performs well in unsupervised WSD. One known unsupervised learning approach for WSD in SP is McCarthy and Carroll (2003) which addresses the issue via conditional probability. This experiment is used as baseline as the approach is also used in McCarthy and Carroll (2003) for verb and adjective disambiguation. McCarthy and Carroll (2003) also uses this type of redundancy for disambiguation in SP. The approach is also used in (McCarthy and Carroll 2003) to disambiguate verbs and adjectives in collocations. To be particular, the method used by McCarthy and Carroll (2003) is formula (6). For example: McCarthy and Carroll (2003) use Li and Abe's method in a word sense disambiguation setting; Schulteim Walde et al (2008) use their MDL approach as part of a system for syntactic and semantic subcategorisation frame learning; Shutova (2010) deploys Resnik's method for metaphor interpretation. Examples include information retrieval (Manning et al, 2008), word sense discrimination (Schtze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. McCarthy and Carroll (2003) have shown the effectiveness of the selectional preference information for WSD. McCarthy and Carroll (2003) also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes. The most similar approach to the one we describe, that has been tested on Senseval-2, is the one described in (McCarthy and Carroll, 2003).
USING A SEMANTIC CONCORDANCE FOR SENSE IDENTIFICATION George A. Miller, Martin Chodorow*, Shari Landes, Claudia Leacock, and Robert G. Thomas Cognit ive Science Laboratory Pr inceton Univers i ty Pr inceton, NJ 08542 ABSTRACT This paper proposes benchmarks for systems of automatic sense identification.  Also (Miller et al, 1994) tagging semantically SemCor by hand, measure an error rate around 10% for polysemous words. First, the similarity and relatedness measures used in the system may rely on SemCor data (Miller et al., 1994). The sense-tagged corpus SEMCOR, prepared by (Miller et al, 1994), contains a substantial subset of the Brown corpus tagged with the refined senses of WORDNET. However, as reported in (Miller et al, 1994), there are not enough training examples per word in SP. Among the existing sense-tagged corpora, the SEMCOR corpus (Miller et al, 1994) is one of the most widely used. It is a balanced corpus and it has more than 200K words that are manually sense tagged as a product of the semantic concordance (SemCor) effort using WordNet [Miller et al 1994]. Among the few currently available manually sense-annotated corpora for WSD, the SEMCOR (SC) corpus (Miller et al, 1994) is the most widely used. Bentivogli et al (2004) proposed an approach to create an Italian sense tagged corpus (MultiSemCor) based on the transference of the annotations from the English sense tagged corpus SemCor (Miller et al, 1994), by means of word alignment methods. (Miller et al, 1994) found that automatic assignment of polysemous words in Brown Corpus to senses in WordNet was 58% correct with a heuristic of most frequently occurring sense. The widely used SEMCOR (SC) corpus (Milleret al, 1994) is one of the few currently available manually sense-annotated corpora for WSD. We gathered training examples from parallel corpora, SEMCOR (Miller et al, 1994), and the DSO corpus. The SEMCOR corpus (Miller et al, 1994) is one of the few currently available, manually sense annotated corpora for WSD. The verbs are tagged with respect to senses in WordNet (Miller 1990), which has become widely used, for example in corpus-annotation projects (Miller et al 1994, Ng & Hian 1996, and Grishman et al 1994) and for performing disambiguation (Resnik 1995 and Leacock et al). SEMCOR (Miller et al 1994) leaves these uses untagged. Here attested frequencies from SemCor (Miller et al, 1994) are used, so all ancestors are considered. Based on the lexicalized grammars, Bikel (2000) attempts at combining parsing and word sense disambiguation in a unified model, using a subset of SemCor (Miller et al, 1994).
Automatic Detection Of Text Genre As the text databases available to users become larger and more heterogeneous, genre becomes increasingly important for computational linguistics as a complement to topical and structural principles of classification. We propose a theory of genres as of correlate with various surface cues, and argue that genre detection based on surface cues is as successful as detection based on deeper structural properties. We use a different kind of model here, logistic regression, which is especially well suited for categorical data analysis (cf. eg. Agresti (1990) or Kessler et al (1997)). Kessler et al (1997) mention that parsing and word-sense disambiguation can also benefit from genre classification. Like Kessler et al (1997) and Argamon et al (1998) after them, they exploit (partly) hand-crafted sets of features, which are specific to texts in English. However, additional aspects of a document in uence its relevance, including, e.g., the evidential status of the material presented, and the attitudes expressed about the topic (Kessler et al, 1997). Apart from two notable exceptions, namely Kessler et al (1997) and Rehm (2006) whose implementations require extensive manual annotation (Kessler et al, 1997) or analysis (Rehm, 2006), genres are usually classified as single-label discrete entities, relying on the simplified assumption that a document can be assigned to only one genre. We plan to investigate more types of document collections pairs, e.g., the document collections from different text genres (Kessler et al, 1997). (Kessler et al, 1997) combine these views by saying that a genre should not be so broad that the texts belonging to it don't share any distinguishing properties. ... we would probably not use the term "genre" to describe merely the class of texts that have the objective of persuading someone to do something, since that class which would include editorials, sermons, prayers, advertisements, and so forth has no distinguishing formal properties (Kessler et al, 1997, p. 33). This is where the caveat from (Kessler et al, 1997) be comes relevant: A particular genre shouldn't betaken so broadly as to have no distinguishing features, nor so narrowly as to have no general applicability. Kessler et al (1997) avoid structural markers since they require tagged or parsed text and replace them with character-level markers (e.g., punctuation mark counts) and derivative markers, i.e., ratios and variation measures derived from measures of lexical and character-level markers. Another straightforward method is the assumption that Latinate prefixes and suffixes are indicators of formality in English (Kessler et al, 1997), i.e. informal words will not have Latinate affixes such as -ation and intra-.
Named Entity Recognition Through Classifier Combination This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions. When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data. Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. We also applied the classifier combination technique discussed in this paper to English and German (Florian et al, 2003b).    Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc.  Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al, 2002), as well as voting over sets of different classifiers (Florian et al, 2003).  Next, we compare the extraction quality of the customized CoreNER for CoNLL03 and Enron3 with the corresponding best published results by (Florian et al, 2003) and (Minkov et al, 2005). It is worthwhile noting that the best published results for CoNLL03 (Florian et al, 2003) were obtained by using four different classifiers (Robust Risk Minimization, Maximum Entropy, Transformation-based learning, and Hidden MarkovModel) and trying six different classifier combination methods. Table 1 presents the results of our system using three learning algorithms, the uneven margins SVM, the standard SVM and the PAUM on the CONLL 2003 test set, together with the results of three participating systems in the CoNLL-2003 shared task: the best system (Florian et al, 2003), the SVM-based system (Mayfield et al, 2003) and the Perceptron-based system (Carreras et al, 2003). Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). Florian et al (2003) employed the same technique in a combination of learners. Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al, 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each. Florian et al (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best. One participating team has used externally trained named entity recognition systems for English as a part in a combined system (Florian et al, 2003). The inclusion of extra named entity recognition systems seems to have worked well (Florian et al, 2003). For English, the combined classifier of Florian et al (2003) achieved the highest overall F1 rate. Florian et al (2003) have also obtained the highest F1 rate for the German data.
Dialogue Act Modeling For Automatic Tagging And Recognition Of Conversational Speech So do you go to college right now? Are yo-, Yeah, it's my last year [laughter]. You're a, so you're a senior now. Yeah, I'm working on my projects trying to graduate [laughter]. Oh, good for you. Yeah. That's great, um, is, is N C University is that, uh, State, What did you say? This paper describes the parameters and tagsets (analogous to "Dialogue Act" tagging (Stolcke et al2000)), which they and three other groups have developed for this highly specialised domain. These are similar to some of the Dialogue Act labels used in NLP work: Stolcke et al's (2000) agreement, response acknowledgement, summarize, or VERBMOBIL's suggest, confirm, clarify (Jekat et al1995). Tremendous amounts of work has focused on this aspect (Stolcke et al, 2000). Stolcke et al (2000) used HMMs as a general model of discourse with an application to speech acts (or dialog acts) in conversations. These notions have been particularly fruitful in the dialog community, where dialog act tagging is a major topic of research; to cite just one prominent example: (Stolcke et al, 2000). Stolcke et al (2000) point out that the use of dialogue acts is a useful first level of analysis for describing discourse structure. We chose 12 tags by manually labelling the dialogue corpus using tags that seemed appropriate from the 42 tags used by Stolcke et al (2000) based on the Dialog Act Markup in Several Layers (DAMSL) tag set (Core and Allen, 1997). A super set of dialogue acts that covers all domains would necessarily be a large number of tags (at least the 42 identified by Stolcke et al (2000)) with many tags not being appropriate for other domains. Although this is higher than the results from 13 recent studies presented by Stolcke et al (2000) with accuracy ranging from 40% to 81.2%, the tasks, data, and tag sets used were all quite different, so any comparison should be used as only a guideline. Another main approach to robust dialogue processing has been statistical models for identifying dialogue acts (e.g., Stolcke et al (2000)). For instance, Stolcke et al (2000) explore n-gram models based on transcribed words and prosodic information for SWBD-DAMSL dialogue acts in the Switchboard corpus (Godfrey et al, 1992). The work of Stolcke et al (2000) found benefits to using Markov sequence models and prosodic features in addition to word features, but those benefits were relatively small, so for simplicity our experiments here use only word features and classify utterances in isolation. In this sort of situations, tag categories are often collapsed when running experiments so as to get meaningful frequencies (Stolcke et al, 2000). (Stolcke et al, 2000) reports an impressive 71% accuracy on transcribed Switchboard dialogues, using a tag set of 42 DAs. (Stolcke et al, 2000) employs a combination of HMM, neural networks and decision trees trained on all available features (words, prosody, sequence of DAs and speaker identity). Work in dialogue act tagging is also relevant, as it seeks to describe the actions and moves with which speakers display these types of positioning (Stolcke et al, 2000). This task involves defining the role of each contribution based on its function (Stolcke et al, 2000). As (Stolcke et al., 2000) report good accuracy (87%) for statement vs. question classification on manual Switchboard transcripts, such coarse-grained information might be reliably available. They model engagement using manually coded dialogue acts based on the SWBDL-DAMSL scheme (Stolcke et al, 2000). Classification over the Switchboard corpus has been demonstrated using Decision Trees (Verbree et al, 2006), Memory-Based Learning (Rotaru, 2002) and Hidden Markov Models (HMM) (Stolcke et al, 2000).
Dynamic Programming for Linear-Time Incremental Parsing Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a problem: the search is only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming. We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging “equivalent” stacks based on feature values. Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy. Better search also leads to better learning, and our final parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster. In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986). Two examples of feature functions are the word form associated with the topmost and second-topmost node on the stack; adopting the notation of Huang and Sagae (2010), we will write these functions as s0: w and s1: w, respectively. In the following, we take this second approach, which is also the approach of Huang and Sagae (2010). Huang and Sagae (2010) use this quantity to order the items in a beam search on top of their dynamic programming method. Alternatively, we can also use the more involved calculation employed by Huang and Sagae (2010), which allows them to get rid of the left context vector from their items. The essential idea in the calculation by Huang and Sagae (2010) is to delegate (in the computation of the Viterbi score) the scoring of sh transitions to the inference rules for la/ra. The basic idea behind our technique is the same as the one implemented by Huang and Sagae (2010) for the special case of the arc-standard model, but instead of their graph-structured stack representation we use a tabulation akin to Lang's approach to the simulation of pushdown automata (Lang, 1974). However, Huang and Sagae (2010) have provided evidence that the use of dynamic programming on top of a transition-based dependency parser can improve accuracy even without exhaustive search. To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming (Huang and Sagae, 2010), and propose an efficient method of character-based decoding over candidate structures. The incremental framework of our model is based on the joint POS tagging and dependency parsing model for Chinese (Hatori et al, 2011), which is an extension of the shift-reduce dependency parser with dynamic programming (Huang and Sagae, 2010). The feature set of our model is fundamentally a combination of the features used in the state-of-the-art joint segmentation and POS tagging model (Zhang and Clark, 2010) and dependency parser (Huang and Sagae, 2010), both of which are used as baseline models in our experiment. W21, and T01? 05 are taken from Zhang and Clark (2010), and P01? P28 are taken from Huang and Sagae (2010). Dep?: the state-of-the-art dependency parser by Huang and Sagae (2010). Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 1137 1147) sets. H&S10 refers to the results of Huang and Sagae (2010). In particular, "Huang 10" and "Zhang 11" denote Huang and Sagae (2010) and Zhang and Nivre (2011), respectively. Since our algorithm is transition-based, many existing techniques such as k-best ranking (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010) designed to improve transition-based parsing can be applied. We implement three transition-based dependency parsers with three different parsing algorithms: Nivre's arc standard, Nivre's arc eager (see Nivre (2004) for a comparison between the two Nivre algorithms), and Liang's dynamic algorithm (Huang and Sagae, 2010). This has led to the development of various data-driven dependency parsers, such as those by Yamada and Matsumoto (2003), Nivre et al2004), McDonald et al2005), Martins et al2009), Huang and Sagae (2010) or Tratz and Hovy (2011), which can be trained directly from annotated data and produce ac curate analyses very efficiently.
Why Doesn't EM Find Good HMM POS-Taggers? This paper investigates why the HMMs es timated by Expectation-Maximization (EM) produce such poor results as Part-of-Speech(POS) taggers. We find that the HMMs es timated by EM generally assign a roughlyequal number of word tokens to each hid den state, while the empirical distribution of tokens to POS tags is highly skewed. This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution. We investigate Gibbs Sampling (GS) and Variational Bayes(VB) estimators and show that VB con verges faster than GS for this task and thatVB significantly improves 1-to-1 tagging ac curacy over EM. We also show that EM does nearly as well as VB when the number of hidden HMM states is dramatically reduced. We also point out the high variance in all of these estimators, and that they requiremany more iterations to approach conver gence than usually thought. [vbhmm]: Bayesian HMM with variational Bayes (Johnson, 2007). We fixed the and parameters to 0.1, values that appeared to be reasonable based on Johnson (2007), and which were also used by Graca et al (2009). We evaluate on a 1-to-1 mapping between unsupervised tags and gold labels, as well as many-to-1 (M-to-1), corresponding to the evaluation mappings used in Johnson (2007). These figures are the MCMC settings that provided the best results in Johnson (2007). There is much potential for further work in this direction, experimenting with more training data or more estimation iterations, or even looking at different estimators as suggested in Johnson (2007) and Ravi et al (2010b). The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson (2007), and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed. Johnson (2007) and Gao & Johnson (2008) assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags. Johnson (2007) reports results for different numbers of hidden states but it is unclear how to make this choice a priori, while Goldwater & Griffiths (2007) leave this question as future work. As Johnson (2007) clearly explained, training the HMM with EM leads to poor results in PoS tagging. The fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g. Goldwate & Griffiths (2007) versus Johnson (2007)) supports this claim. In particular, Toutanova and Johnson (2007) demonstrate good performance on unsupervised part-of-speech tagging (using a dictionary) with a Bayesian model similar to our own. Johnson (2007) observed that EM tends to create word clusters of uniform size, which does not reflect the way words cluster into parts of speech in natural languages. We evaluated POS tagging accuracy using the lenient many-to-1 evaluation approach (Johnson, 2007). This mapping technique is based on the many-to-one scheme used for evaluating unsupervised part-of-speech induction (Johnson, 2007). In related fields of NLP lately Dirichlet priors have been investigated, e.g. (Johnson, 2007).    For instance, on unsupervised part-of speech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson, 2007). (Johnson, 2007) criticizes the standard EM based HMM approaches because of their poor performance on the unsupervised POS tagging and their tendency to assign equal number of words to each hidden state.
Headline Generation Based On Statistical Translation Extractive summarization techniques cannot generate document summaries shorter than a single sentence, something that is often required. An ideal summarization system would understand each document and generate an appropriate summary directly from the results of that understanding. A more practical approach to this problem results in the use of an approximation: viewing summarization as a problem analogous to statistical machine translation. The issue then becomes one of generating a target document in a more concise language from a source document in a more verbose language. This paper presents results on experiments using this approach, in which statistical models of the term selection and term ordering are jointly applied to produce summaries in a style learned from a training corpus. One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate βi's from the document to be abstracted and rely on recent information ordering techniques to sort the βi fragments (Lapata, 2003). Some researchers (Banko et al, 2000) have developed simple statistical models for aligning documents and headlines. The second baseline is based on the noisy-channel generative (flat generative, FG) model proposed by Banko et al, (2000). Our method for estimation of selection and ordering preferences is based on the technique described in (Banko et al, 2000). Even a larger beam size such as 80 (as used by Banko et al (2000)) does not match the title quality of the optimal decoder. Our first abstractive model builds on and extends a well-known probabilistic model of headline generation (Banko et al, 2000). Banko et al (2000) propose a bag-of-words model for headline generation. Following Banko et al (2000), we approximated the length distribution with a Gaussian. This approach has been explored in (Zajic et al, 2002) and (Banko et al, 2000). For example, Banko et al (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. Another approach, presented by (Banko et al, 2000), consists in generating coherent summaries that are shorter than a single sentence. Banko et al (2000) uses beam search to identify approximate solutions. In subsequent work to Witbrock and Mittal (1999), Banko et al (2000) describe the use of information about the position of words within four quarters of the source document. From early in the field, it was pointed out that a purely extractive approach is not good enough to generate headlines from the body text (Banko et al, 2000). For this reason, most early headline generation work focused on either extracting and reordering n-grams from the document to be summarized (Banko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003).
A Rule-Based Approach To Prepositional Phrase Attachment Disambiguation I:n this paper, we describe a new corpus-based ap- proach to prepositional phrase attachment disam- biguation, and present results colnparing peffo> mange of this algorithm with other corpus-based approaches to this problem. This technique has previously been used not only for part-of-speech tagging (Brill, 1994), but also for prepositional phrase attachment disambiguation (Brill and Resnik, 1994), and assigning unlabeled binary-branching tree structure to sentences (Brill, 1993a). Brill and Resnik (1994) trained a transformation-based learning algorithm on 12,766 quadruples from WSJ, with modifications similar to those by Collins and Brooks (1995). Since Eric Brill first introduced the method of Transformation-Based Learning (TBL) it has been used to learn rules for many natural anguage processing tasks, such as part-of-speech tagging [Brill, 1995], PP attachment disambiguation [Brill and Resnik, 1994], text chunking [Ramshaw and Marcus, 1995], spelling correction [Mangu and Brill, 1997], dialogue act tagging [Samuel et al, 1998] and ellipsis resolution [Hardt, 1998]. This alternative, which we have yet to try, has the advantage of fitting into the transformation-based error-driven paradigm (Brill and Resnik, 1994) more cleanly than having a translation stage. Brill and Resnik (1994) used the supervised transformation-based learning method and lexical and conceptual classes derived from WordNet, achieving 82% precision on 500 randomly selected examples. For example, in (Brill and Resnik, 1994) clustering PP heads according to WordNetsynsets produced only a 1% improvement in a PP disambiguation task, with respect to the non-clustered method. This huge number of tokens can be explained by the fact that the lexicon used for tokenization and tagging integrates many multi-word expressions which are not part of these mantic lexicon for (Brill and Resnik, 1994) and 0.77 for (LauerandDras, 1994)), but a direct comparison is difficult inasmuch as only three-word sequences (V N P, for (Brill and Resnik, 1994) and N N N for (Lauer and Dras, 1994)) were used for evaluation in those works, and the language studied is English. In later stages of processing, a corpus-based approach (Brill and Resnik, 1994) is used to deal with ambiguities that cannot be solved with syntactic information only, in particular attachments of prepositional phrases, gerunds and infinitive constructions. For example, the sentence Congress accused the president of peccadillos is classified according to the attachment site of the prepositional phrase: attachment toN: accused [the president of peccadillos] attachment to V: (4) accused [the president] [of peccadillos] The UPenn Treebank-II Parsed Wall Street Journal corpus includes PP-attachment information, and PP-attachment classifiers based on this data have been previously described in Ratnaparkhi, Reynar, Roukos (1994), Brill and Resnik (1994), and Collins and Brooks (1995). A non-statistical supervised approach by Brill and Resnik (1994) yielded 81.8% accuracy using a transformation-based approach (Brill, 1995) and incorporating word-class information.    Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maximum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998). Brill and Resnik (1994) applied Error-Driven TransformationBased Learning, Ratnaparkhi, Reynar and Roukos (1994) applied a Maximum Entropy model, Franz (1996) used a Loglinear model, and Collins and Brooks (1995) obtained good results using a BackOff model. The other methods for which results have been reported on this dataset include decision trees, Maximum Entropy (Ratnaparkhi, Reynar, and Roukos, 1994), and Error-Driven TransformationBased Learning (Brill and Resnik, 1994), which were clearly outperformed by both IB1 and IBI-IG, even though e.g. Brill~ Resnik used more elaborate feature sets (words and WordNet classes). Brill and Resnik (1994) applied Error-Driven Transformation-Based Learning to this task, using the verb, noun1, preposition, and noun2 features. Supervised methods are as varied as the Back off approach by Collins and Brooks (1995) and the Transformation-based approach by Brill and Resnik (1994). We use the PPA data created by (Brill and Resnik, 1994) and (Ratnaparkhi et al, 1994) to objectively compare the performances of the systems. (10) The lower bound for the B&R data is 63% (Brill and Resnik, 1994) and for the IBM data is 52% (Ratnaparkhi et al, 1994).
Tuning Support Vector Machines For Biomedical Named Entity Recognition We explore the use of Support Vector Machines (SVMs) for biomedical named entity recognition. To make the SVM training with the available largest corpus – the GENIA corpus – tractable, we propose to split the non-entity class into sub-classes, using part-of-speech information. In addition, we explore new features such as word cache and the states of an HMM trained by unsupervised learning. Experiments on the GENIA corpus show that our class splitting technique not only enables the training with the GENIA corpus but also improves the accuracy. The proposed new features also contribute to improve the accuracy. We compare our SVMbased recognition system with a system using Maximum Entropy tagging method.  In addition, we split the GENIA 1.1 subset into the test dataset of 80 abstracts used in Kazama et al (2002) and the training dataset of the remaining 590 abstracts. Results of protein name recognition in Kazama et al (2002) using GENIA 1.1 are 0.492, 0.664 and 0.565 for precision, recall, f-score respectively. In the case of term classification, Kazama et al (2002) used a more exhaustive feature set containing lexical information, POS tags, affixes and their combinations in order to recognise and classify terms into a set of general biological classes used within the GENIA project (GENIA, 2003). Experiments with augmented tag sets in the biomedical domain also show performance loss with respect to smaller tagsets; e.g., Kazama et al (2002) report an F score of 56.2% on a tag set of 25 Genia classes, compared to the 75.9% achieved on the simplest binary case. Kazama et al (2002) reported an F-measure of 56.5% on the GENIA corpus (Version 1.1) using Support Vector Machines. (Kazama et al, 2002) proposed a machine learning approach to BNE tagging based on support vector machines (SVM), which was trained on the GENIA corpus and reported an F-measure score of 0.73 for different mixtures of models tested on 20 abstracts.  In previous research, (Kazama et al 2002) make use of POS information and conclude that it only slightly improves performance.  On V1.1, we use the same training and testing data and capture the same NE classes as (Kazama et al 2002). Kazama et al (2002) addressed the data imbalance problem and sped up the training process by splitting the type O instances into sub classes using part-of-speech information. In general, machine learning based methods to relation extraction perform very well for any task where sufficient, representative and high quality training data is available (Kazama et al., 2002).
TextRunner: Open Information Extraction on the Web  We turned to TextRunner (Yates et al, 2007) as a large source of background knowledge about pre-existing relations between nominals. Yates (2009) considers the output from an open information extraction system (Yates et al, 2007) and clusters predicates and arguments using string similarity and a combination of constraints. Two example systems implementing this paradigm are TEXTRUNNER (Yates et al, 2007) and REVERB (Fader et al, 2011). In contrast, when research focuses on any relation, as in TextRunner (Yates et al, 2007), there is no standardized manner for re-using the pattern learned. The online demo of TextRunner (Yates et al, 2007) actually allowed us to collect the arguments for all our semantic relations.
Dependency Parsing by Belief Propagation We formulate dependency parsing as a graphical model with the novel ingredient of global constraints. We show how to apply loopy belief propagation (BP), a simple and tool for and inference. As a parsing algorithm, BP is both asymptotically and empirically efficient. Even with second-order features or latent variables, which would make exact parsing considerslower or NP-hard, BP needs only with a small constant factor. Furthermore, such features significantly improve parse accuracy over exact first-order methods. Incorporating additional features would increase the runtime additively rather than multiplicatively. Note that finding taxonomy trees is a structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008). The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). We present a unified view of two state-of-the-art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al (2009). In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al,2009). The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). Our contributions are not limited to dependency parsing: we present a general method for inference in factor graphs with hard constraints, which extends some combinatorial factors considered by Smith and Eisner (2008). Smith and Eisner (2008) proposed a factor graph representation for dependency parsing (Fig. 1). Fortunately, for all the hard constraint factors in rows 3-5 of Table 1, this computation can be done in linear time (and polynomial for the TREE factor) - this extends results presented in Smith and Eisner (2008). Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig. 1) in which they run loopy BP. Smith and Eisner (2008) also proposed other variants with more factors, which we omit for brevity.   However, as observed in Smith and Eisner (2008), we can encapsulating common dynamic programming algorithms within special-purpose factors to efficiently globally constrain variable configurations. Let DEP-TREE be a global combinatorial factor, as presented in Smith and Eisner (2008), which attaches to all Link (i, j) variables and similarly contributes a factor of 1 iff the configuration of Link variables forms a valid projective dependency graph. To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al, 1985), previously applied to dependency parsing by Smith and Eisner (2008). Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. In this work we follow Smith and Eisner (2008) and train the models with stochastic gradient descent on the conditional log-likelihood of the training data, using belief propagation in order to calculate approximate gradients. This is a natural extension to the use of complex factors described by Smith and Eisner (2008) and Dreyer and Eisner (2009). One behavior we observe in the graph is that the DD results tend to incrementally improve in accuracy while the BP results quickly stabilize, mirroring the result of Smith and Eisner (2008).
Mixed Initiative in Dialogue: An Investigation into Discourse Segmentation Marilyn Walker University of Pennsylvania* Computer Science Dept.  As initiative passes back and forth between discourse participants, control over the conversation similarly transfers from one speaker to another (Walker and Whittaker, 1990). Walker and Whittaker (1990) found a correlation between initiative switches and discourse segments. After experimenting with several tagging methods, we concluded that the approach presented in Walker and Whittaker (1990) adopted from (Whittaker and Stenton, 1988) best captured the aspects of the dialogue we were interested in and, as with the DAs, could be tagged reliably on our data. Before embarking on an exhaustive manual annotation of initiative, I chose to get a sense of whether initiative may indeed affect learning in this context by automatically tagging for initiative using an approximation of Walker and Whittaker's utterance based allocation of control rules (Walker and Whittaker, 1990). Walker and Whittaker claim that initiative encompasses both dialogue control and task control (Walker and Whittaker, 1990), however, several others disagree. For dialogue initiative annotation, I am using Walker and Whittaker's utterance based allocation of control rules (Walker and Whittaker, 1990), which are widely used to identify dialogue initiative. Whittaker and Stenton (1988) looked at the correlation of control boundaries to discourse markers, and Walker and Whittaker (1990) looked at anaphoric reference. Walker and Whittaker (1990) suggested that changes in initiative correspond to change sin discourse structure, but they did not determine the exact relationship between them. For dialogue initiative annotation, we used the well-known utterance-based rules for allocation of control from (Walker and Whittaker, 1990). Once the dialogue acts had been automatically annotated, two coders, one of the authors and an out side annotator, coded 24 dialogues (1449 utterances, approximately 45% of the corpus) for dialogue initiative, by using the four control rules from (Walker and Whittaker, 1990).
Minimum Risk Annealing For Training Log-Linear Models 10 restarts 1 restart 793 Optimization Procedure labeled dependency acc. [%] Slovenian Bulgarian Dutch Max. like. 27.78 47.23 36.78 Min. error 22.52 54.72 36.78 Ann. min. risk 31.16 54.66 36.71 2: dependency accuracy on parsing 200sentence test corpora, after training 10 experts on 1000 senand fitting their weights 200 more. For Slovenian, minimum risk annealing is significantly better than the other training methods, while minimum error is significantly worse. For Bulgarian, both minimum error and annealed minimum risk training achieve significant gains over maximum likelihood, but are indistinguishable from each other. For Dutch, the three methods are indistinguishable. than either and in some cases significantly helped. Note, however, that annealed minimum risk training results in a deterministic classifier just as these other training procedures do. The orthogonal of Bayes risk decoding achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004). In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000). We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder. Another training approach that incorporates arloss functions is found in the in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004). Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones. The distinction is in using a loss function to calculate the required margins. 8 Conclusions Despite the challenging shape of the error surface, we have seen that it is practical to optimize task-specific error measures rather than optimizing likelihood—it produces lower-error systems. Different methods can be used to attempt this global, non-convex optimization. We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to optimization performs significantly better than a previous line-search method that does not smooth the error surface. It never does significantly worse. With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP. References L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer. 1988. A new algorithm for the estimation of hidden model parameters. In pages 493–496. E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best and maxent discriminative reranking. In pages 173–180. S. F. Chen and R. Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical report, CS Dept., Carnegie Mellon University. K. Crammer, R. McDonald, and F. Pereira. 2004. New large algorithms for structured prediction. In Structured Outputs M. Dreyer, D. A. Smith, and N. A. Smith. 2006. Vine parsing and minimum risk reranking for speed and precision. In G. Elidan and N. Friedman. 2005. Learning hidden variable The information bottleneck approach. 6:81–127. V. Goel and W. J. Byrne. 2000. Minimum Bayes-Risk auspeech recognition. Speech and Lan- 14(2):115–135. J. T. Goodman. 1996. Parsing algorithms and metrics. In pages 177–183. Hinton. 1999. Products of experts. In of volume 1, pages 1–6. K.-U. Hoffgen, H.-U. Simon, and K. S. Van Horn. 1995. trainability of single neurons. of Computer and 50(1):114–125. D. S. Johnson and F. P. Preparata. 1978. The densest hemiproblem. Comp. 6(93–107). S. Katagiri, B.-H. Juang, and C.-H. Lee. 1998. Pattern recognition using a family of design algorithms based upon the probabilistic descent method. 86(11):2345–2373, November. P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrasetranslation. In pages 48–54. S. Kumar and W. Byrne. 2004. Minimum bayes-risk decodfor statistical machine translation. In J. Lafferty, A. McCallum, and F. C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting labeling sequence data. In F. J. Och. 2003. Minimum error rate training in statistical translation. In pages 160–167. K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. A method for automatic evaluation of machine In pages 311–318. K. A. Papineni. 1999. Discriminative training via linear In A. Rao and K. Rose. 2001. Deterministically annealed deof Hidden Markov Model speech recognizers. on Speech and Audio 9(2):111–126. K. Rose. 1998. Deterministic annealing for clustering, compression, classification, regression, and related optimizaproblems. 86(11):2210–2239. N. A. Smith and J. Eisner. 2004. Annealing techniques for statistical language learning. In pages 486–493. The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006). A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface. Deterministic Annealing was suggested by Smith and Eisner (2006) where the authors propose to minimize the expected loss or risk. This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score. In the geometric interpolation above, the weight controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006).  Deterministic Annealing: In this system, in stead of using the regular MERT (Och, 2003) whose training objective is to minimize the one best error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique). Gradient-based techniques require a differentiable objective, and expected sentence BLEU is the most popular choice, beginning with Smith and Eisner (2006). The N-best list based expected BLEU tuning (Rosti et al, 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices. The objective function is defined by replacing the n-gram statistics with expected n gram counts and matches as in (Smith and Eisner, 2006), and brevity penalty with a differentiable approximation. Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temperature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface.  In future work, we would like to investigate other objectives with a more direct task loss, such as max margin (Taskar et al, 2004), risk (Smith and Eisner, 2006) or soft max-loss (Gimpel and Smith, 2010), and different regularizers, such as L1-norm for a sparse solution. Although 1-best systems are not differentiable functions, we can approach their behavior during ERM training by annealing the training objective (Smith and Eisner, 2006). This is consistent with Jansche (2005) and Smith and Eisner (2006), who observed similar improvements when using approximate f-score loss for other problems.  An annealed minimum risk approach is presented in (Smith and Eisner, 2006) which outperforms both maximum likelihood and minimum error rate training. We can therefore support the claim of (Smith and Eisner, 2006) that MBR tends to have better generalization capabilities. In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. Cunei's built-in optimization code closely follows the approach of (Smith and Eisner, 2006), which minimizes the expectation of the loss function over the distribution of translations present in the n best list.
CogNIAC: High Precision Coreference With Limited Knowledge And Linguistic Resources This paper presents a high precision pronoun resolution system that is capable of greater than 90% precision with 60% and better recall for some pronouns. It is suggested that the system is resolving a sub-set of anaphors that do not require general world knowledge or sophisticated linguistic processing for successful resolution. The system does this by being very sensitive to ambiguity, and only resolving pronouns when very high confidence rules have been satisfied. The system is capable of 'noticing' ambiguity because it requires that there be a unique antecedent within a salience ranking, and the salience rankings are not total orders, i.e. two or more antecedents can be equally salient. Given the nature of the systems rules, it is very likely that they are largely domain independent and that they reflect processing strategies used by humans for general language comprehension. The system has been evaluated in two distinct experiments which support the overall validity of the approach. We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's CogNIAC (Baldwin 1997) approach which features. This result is comparable with the results described in (Baldwin 1997). For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%). The approach for applying the rules is similar to the one proposed by Baldwin (1997). The CogNIAC algorithm (Baldwin, 1997) was designed for high-precision AR. RAP (Kennedy and Boguraev, 1996), Baldwin's pronoun resolution method (Baldwin, 1997) and Mitkov's knowledge-poor pronoun resolution approach (Mitkov, 1998b). Since the original version of CogNiac is non-robust and resolves only anaphors that obey certain rules, for fairer and comparable results we implemented the resolve-all version as described in (Baldwin, 1997). Baldwin's CogNiac (Baldwin, 1997) is a knowledge poor approach to anaphora resolution based on a set of high confidence rules which are successively applied over the pronoun under consideration. This definition is slightly different from the one used in (Baldwin, 1997) and (Gaizauskas and Humphreys, 2000). As expected, the results reported in Table 1 do not match the original results published by Kennedy and Boguraev (1996), Baldwin (1997) and Mitkov (1998b) where the algorithms were tested on different data, employed different pre-processing tools, resorted to different degrees of manual intervention and thus provided no common ground for any reliable comparison. Evaluation results for both evaluation modes are 76 given in traditional precision, recall and f-score, which are similar to (Baldwin, 1997). Pronoun resolution is carried out using the Glencova Pronoun Resolution algorithm (Halpin et al, 2004), based on a series of rules similar to the CogNIAC engine (Baldwin, 1997), but without gender information based rules since this is not provided by the Penn Treebank tag set. As a consequence, current anaphor resolution implementations mainly rely on constraints and preference heuristics which employ information originated from morphosyntactic or shallow semantic analysis ,e.g. in Baldwin (1997).
Synchronous Tree-Adjoining Grammars The unique properties of lree-adjoining grammars (TAG) present a challenge for the application of 'FAGs beyond the limited confines of syntax, for instance, to the task of semantic interpretation or automatic translation of nat- ural h'mguage. We present a variant of "FAGs, called synchronous TAGs, which chmacterize correspondences between languages. "lq\]e formalism's intended usage is to relate expressions of natural anguages to their associ- ated semantics represented in a logical tbrm language, or to their translates in another natural anguage; in sum- mary, we intend it to allow TAGs to be used beyond their role in syntax proper. We discuss the application of synchronous TAGs to concrete examples, mention- ing primarily in passing some computational issues that tu:ise in its interpretation. Synchronous tree adjoining grammars (TAG) (Shieber and Schabes, 1990) are a good candidate. This channel model is formalised using a Synchronous Tree Adjoining Grammar (STAG) (Shieber and Schabes, 1990), which matches words from the reparandum to the repair. between syntax and semantics (Shieber and Schabes, 1990). The states could be more refined than those shown above: the state for the subject, for example, should probably be not NP but a pair (Npl, NP3s) .STSG is simply a version of synchronous tree adjoining grammar or STAG (Shieber and Schabes, 1990) that lacks the adjunction operation. For further background, refer to the work of Shieber and Schabes (1990) and Shieber (1994). In theory, stochastic tree transducers and some versions of synchronous grammars provide solutions for the non-isomorphic tree based transduction problem and hence possible solutions for MT. Synchronous Tree Adjoining Grammars, proposed by (Shieber and Schabes, 1990), were introduced primarily for semantics but were later also proposed for translation. A synchronous derivation process for the two syntactic structures of both languages suggests the level of cross-lingual isomorphism between the two trees (e.g. Synchronous Tree Adjoining Grammars (Shieber and Schabes, 1990)). Shieber and Schabes (1990) describe a synchronous tree adjoining grammar. At the same time, grammar theoreticians have proposed various generative synchronous grammar formalisms for MT, such as Synchronous Context Free Grammars (S-CFG) (Wu, 1997) or Synchronous Tree Adjoining Grammars (S-TAG) (Shieber and Schabes, 1990). For example, Synchronous TAGs, proposed by (Shieber and Schabes, 1990), which were introduced primarily for semantics but were later also proposed for translation. The TAG channel model defines a stochastic mapping of source sentences X into observed sentences Y. There are several ways to define transducers using TAGs such as Shieber and Schabes (1990), but the following simple method, inspired by finite-state transducers, suffices for the application here. The tree fragments that are obtained by decomposing the synchronous trees in this fashion are similar to the Synchronous Tree Insertion Grammar of (Shieber and Schabes, 1990). We developed a tree traversal algorithm that decomposes parallel trees into all minimal tree fragments. One natural solution is to restrict our candidate triples to those given by a synchronous context free grammar (SCFG) (Shieber and Schabes, 1990). Our mapping between semantic and syntactic constituents bears resemblance to the pairings in Synchronous TAG (Shieber and Schabes, 1990). Recently there is a swing back to incorporating more linguistic information again, but this time linguistic insight carefully guides the setup of empirically learned models. Shieber (2007) recently argued that probabilistic Synchronous Tree Adjoining Grammars (Shieber and Schabes, 1990) have the right combination of properties that satisfy both linguists and empirical MT practitioners. Shieber and Schabes (1990) offer a synchronous version of TAG (STAG), allowing the construction of a pair of trees in lockstep fashion using the TAG operations of substitution and adjunction on tree pairs.  STAG (Shieber and Schabes, 1990) (Synchronized TAG) for each member of the TAG (Tree Adjoining Grammar) family. The meaning of the word& quot; synchronized& quot; here is exactly the same as in STAG (Shieber and Schabes, 1990). As first described by Shieber and Schabes (1990), STAG can be used to provide a semantics for a TAG syntactic analysis by taking the tree pairs to represent a syntactic analysis synchronized with a semantic analysis. For example, Figure 2 (a) contains a sample English syntax/semantics grammar fragment that can be used to analyze the sentence? John apparently likes Mary?.
A Study On Convolution Kernels For Shallow Statistic Parsing In this paper we have designed and experimented novel convolution kernels for automatic classification of predicate arguments. Their main property is the ability to process structured representations. Support Vector Machines (SVMs), using a combination of such kernels and the flat feature kernel, classify Prop- Bank predicate arguments with accuracy higher the current argument classification state- Additionally, experiments on FrameNet data have shown that SVMs are appealing for the classification of semantic roles even if the proposed kernels do not produce any improvement. In our experiments, we implement the feature-enriched tree kernel by extending the SVMlight (Joachims, 1998) with the proposed tree kernel function (Moschitti, 2004). The kernel that we employed in our experiments is based on the SCF structure devised in (Moschitti, 2004).  We used support vector machines (Vapnik, 1995) with (a) polynomial kernels to learn the semantic role classification and (b) Tree Kernels (Moschitti, 2004) for learning both frame and ILC classification.  The kernel that we employed in our experiments is based on the SCF structure devised in (Moschitti, 2004).  As shown in (Moschitti, 2004), we can label semantic roles by classifying the smallest subtree that includes the predicate with one of its arguments, i.e. the so called PAF structure.  For learning, the SVM-Light software (Joachims, 1999) was employed with the convolution tree kernel implemented by Moschitti (2004). In (Moschitti, 2004), an alternative to the SCF extraction was proposed, i.e. the SCF kernel (SK). A preliminary study on the benefit of such kernels was measured on the classification accuracy of semantic arguments in (Moschitti, 2004). The convolution kernel that we have experimented was devised in (Moschitti, 2004) and is characterized by two aspects: the semantic space of the subcategorization structures and the kernel function that measure their similarities. The evaluations were carried out with the SVMlight-TK software (Moschitti, 2004) available at http: //ai-nlp.info.uniroma2.it/moschitti/ which encodes the tree kernels in the SVM-light software (Joachims, 1999). Here we use the same convolution parse tree kernel as described in Collins and Duffy (2001) for syntactic parsing and Moschitti (2004) for semantic role labeling. In our implementation, we use the binary SVMLight (Joachims, 1998) and Tree Kernel Tools (Moschitti, 2004). For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004). Moschitti (2004) and Che et al (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel for SRL under the framework of convolution tree kernel. In our implementation, we use the binary SVMLight (Joachims, 1998) and modify the Tree Kernel Tools (Moschitti, 2004) to a grammar driven one.
Is It Harder To Parse Chinese Or The Chinese Treebank? Table 3: Frequency of parse error types. hancements that can be used to address it. 4.1 Analysis by error type and PCFG-enrichment fixes Multilevel VP adjunction errors (Figure 5) are common in models without parent annotation, although even with parent annotation the presence of VP coordination would give multilevel VP adjunction nonzero probability. We address this error by taking advantage of the CTB's principled VP annotation practices, marking adjunction, complementation, and coordination VP levels, which builds the flat adjunction constraint back into the structure of the head daughter. NP-NP modification, depicted in NNM± in Figure 4, was the most common error seen; the greater prevalence of false positives is likely a result of the overall PCFG parsing preference for flatter structures. This type of parse ambiguity is grounded in the semantic ambiguity of compound noun interpretation. This semantic ambiguity exists in English as well, as in the NP-NP modification false positive/negative PNM{+/—} (non-NP) prenominal mod. false positive/negative CRD{H,L}{V,N} incorrect {high/low} coordination attachment of righthand {verbal/nominal} material X/Y incorrect adjunction into X; rect site was Y mistag X/Y category Y mistagged as X that only mistaggings leading to constituentlevel parse errors were tallied. VP (-ADJ) ADVP ADVP VP (-COMP) NP AD AD VV NP 1 1 1 NP i• Itg 4k 4g* A A Fi' positively investigate profession NR NN NR I I I NN VP i4 A * 01 ADVP VP Shanghai customs Chorigm rig office AD ADVP VP I NR NN NR NN i• AD VV NP I I I I i4 A * 01 044' Itg 4k 4g* A A Fi' Shanghai customs Chorigming office positively investigate high-risk profession NP NNM±* Figure 5: Flat (corpus) versus multilevel (incorrect-parse) adjunction. Parenthesized material is category-modification. CP NP I exports NP VP -** NP VP -** VV NP realized I gross exports CRDHN* NP PU NP NP problems CRDLN CP NP NP PU NP unmet • fig_ conditions ë problems VP NP ADVP VP nationwide most long NP VP ADVP VP nationwide -------most long Figure 4: Major parse ambiguities. Starred examples are correct in corpus; alternates are parse errors. string speculator Richard Denthese structures are typically bracketed flat in the ETB, underspecifying the semantic relations relative to the CTB. In CTB parsing, this type of ambiguity is difficult to resolve; different compound NP parses differ in dependency structure, so the dependency model resolves errors when word frequencies are large enough to be reliable, but this is often not possible. We found that the internal distributions of (i) NP modifiers of NPs and (ii) left-modified NPs both differ from the internal distribution of NPs in general; we take advantage of this in the PCFG model by marking both types (i) and (ii), which reduces the bias against NP-NP modification in compound NPs. Prenominal modification errors, illustrated in PNM of Figure 4, are rather infrequent, despite the natural parallel with PP attachment ambiguity in English. Due to the highly articulated structure of prenominal modifiers, it seems difficult to address this problem directly; one measure we found somewhat successful is to mark IP daughters of prenominal modification. Coordination scope errors occured in two major varieties: those where the misattached right conjunct is verbal (a VP or IP), and those where it is nominal the latter case is illustrated in and CRDLN in Figure The equivverbal coordination is generally marked with commas, whereas nominal coordination is marked with conjoiners or the mostly noun-conjoining punctuation mark &quot;, &quot; IP NP heretofore unmet conditions NP IP IP NP VP IP PU IP PU IP NP VP he I I president may VV him say president may he I meet him 120, say Figure 6: Ambiguity between communication verb subcategorization frame (left; corpus) and high coordination attachment (right; incorrect parse). ocal majority of low over high verbal attachment errors contrasts qualitatively with ETB parsing, where low attachment is more common and parsers tend to err toward high attachment. There are two major sources of ambiguous attachment sites: (i) any VP can be parsed as an IP plus a unary IP—NP, so due to pro-drop any VP coordination is ambiguous with a higher IP coordination; (ii) VPs are multilevel, giving rise to ambiguities of scope over adjuncts. It seems that (i) is a difficult problem; in some cases, certain &quot;discourse-level&quot; adverbs such as IP modification and are thus strong indicators of high attachment. To capture this we mark those adverbs possessing an IP grandparent. We address (ii) to some extent by marking VPs as adjunction or complementation structures, as shown before in Figure 5; in training data, only like-type VPs are coordinated. With nominal coordination scope errors, the situation is different: we found no false low attachments. False high scopings can be reduced by marking NP conjuncts. (Charniak, 2000) claims that a similar strategy proved effective for WSJ parsing. Because of these characteristics, Chinese has a rather different set of salient ambiguities from the perspective of statistical parsing (Levy and Manning, 2003). We use the SVM-Light Toolkit version 6.02 (Joachims, 1999) for the implementation of SVM, and use the Stanford Parser version 1.6 (Levy and Manning, 2003) as the constituent parser and the constituent-to-dependency converter. To run the DE classifiers, we use the Stanford Chinese parser (Levy and Manning, 2003) to parse the Chinese side of the MT training data, the devset and test set. In this work, we use the Stanford Parser (Levy and Manning 2003). Levy and Manning (2003) used a factored model that combines an unlexicalized PCFG model with a dependency model. While it is uncommon to offer an error analysis for probabilistic parsing, Levy and Manning (2003) argue that a careful error classification can reveal possible improvements. Verb mistagging is also a problem for other languages: Levy and Manning (2003) describe a similar problem in Chinese for noun/verb ambiguity. The closest previous work is the detailed manual analysis performed by Levy and Manning (2003).   See Levy and Manning (2003) for a similar discussion of Chinese and the Penn Chinese Treebank. We use the SVM-Light Toolkit (Joachims, 1999) for the implementation of SVM, and use the Stanford Parser (Levy and Manning, 2003) as the parser and the constituent-to-dependency converter. Noun/verb mis-taggings are a frequent error case for PCFG parsing on PCTB data, compounded in Chinese by the lack of function words and morphology (Levy and Manning, 2003). It should be noted that it is straightforward to simultaneously do POS tagging and constituent parsing, as POS tags can be regarded as non-terminals in the constituent structure (Levy and Manning, 2003). Levy and Manning (2003) established that properties of Chinese such as noun/verb ambiguity contribute to the difficulty of Chinese parsing. greatly affects parsing accuracy (Levy and Manning, 2003). We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al, 2009). Adapting unlexicalized parsers appears to be equally difficult: Levy and Manning (2003) adapt the unlexicalized parser of Klein and Manning (2003) to Chinese, but even after significant efforts on choosing category splits, only modest performance gains are reported. As pointed out in (Levyand Manning, 2003), there are many linguistic differences between Chinese and English, as well as structural differences between their corresponding tree banks, and some of these make it a harder task to parse Chinese. First of all, we adopt the head finding rules for Chinese used in (Levy and Manning, 2003), and this affects sieve 4, 6 and 7 which are all take advantage of the head words.
Head-Driven Statistical Models For Natural Language Parsing This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram dependencies, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collectingfrequencies ofvarious structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models. This figure shows the amount of time (excluding any startup overhead) spent parsing or bracketing using this system (the two lowest lines) versus the parsers of Collins (2003) and Charniak (2000) run with default settings. Reranking of n-best lists has recently become popular in several natural language problems, including parsing (Collins, 2003), machine translation (Och and Ney, 2002) and web search (Joachims, 2002). Collins (2003) uses both Markov Random Fields and boosting, Och and Ney (2002) use a maximum entropy ranking scheme, and Joachims (2002) uses a support vector approach. All the COL03 systems are results obtained using the restriction of the output of Collins (2003) parser. My guess is that the features used in e.g., the Collins (2003) or Charniak (2000) parsers are probably close to optimal for English Penn Treebank parsing (Marcus et al, 1993), but that other features might improve parsing of other languages or even other English genres. The corpora are first parsed using Collins's parser (Collins, 2003) with the boundaries of all the entity mentions kept. To generate parse trees, we use the Berkeley parser (Petrov et al, 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. As pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation might discard useful information. We suspect that identities of punctuation marks (Collins, 2003, Footnote 13) - both sentence-final and sentence-initial - could be of extra assistance in grammar induction, specifically for grouping imperatives, questions, and so forth. The first, proposed by (Johnson, 1998), is the annotation of parental history, and the second encodes a head-outward generation process (Collins, 2003). Collins (2003) proposes to generate the head of a phrase first and then generate its sisters using Markovian processes, thereby exploiting head/sister-dependencies. A formal overview of the transformation and its correspondence to (Collins, 2003)'s models is available at (Hageloh, 2007). In a second set of experiments we use an unlexicalized head driven baseline a la (Collins, 2003) located on the (0, 0, 0) coordinate. In our next set of experiments we evaluate the contribution of the depth dimension to extensions of the head-driven unlexicalized variety a la (Collins,2003). This dimension is orthogonal to the vertical (Collins, 2003) and horizontal (Johnson, 1998) dimensions previously outlined by Klein and Manning (2003), and it can not be collapsed into any one of the previous two. Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account. Inspired by the work of Collins (2003), the generative model builds trees by recursively creating nodes at each level according to a Markov process. Motivated by Collins syntactic parsing models (Collins, 2003), we consider the generation process for a hybrid sequence from an MR production as a Markov process. For the experiments reported in this paper, we use as parser P, our in-house implementation of the Collins parser (Collins, 2003), to which various speed-related enhancements (Goodman, 1997) have been applied. First, we consider the integration of the generative model for phrase-structure parsing of Collins (2003), with the second-order discriminative dependency parser of Koo et al (2008).
Word Identification For Mandarin Chinese Sentences Keh- J iann Chen Sh ing- l luan Liu Institute of lnfl~rmation Science Academia Sinica Chinese sentences are composed with string of characters without blanks to mark words. However the basic unit for sentence parsing and understanding is word. Therefore the first step of processing Chinese sentences i to identify the words. The difficulties of identifying words include (l) the identification of com- plex words, such as Determinative-Measure, redupli- cations, derived words etc., (2) the identification of proper names,(3) resolving the ambiguous segmenta- tions. In this paper, we propose the possible solutions for the above difficulties. We adopt a matching algo- rithm with 6 different heuristic rules to resolve the am- biguities and achieve an 99.77% of the success rate. The statistical data supports that the maximal match- ing algorithm is the most effective heuristics. In general, the word segmentation program utilizes the word entries, part-of-speech (POS) information (Chen and Liu, 1992) in a monolingual dictionary, segmentation rules (Palmer, 1997), and some statistical information (Sproat, et al, 1994). Early word-based segmentation work employed simple heuristics like dictionary-lookup maximum matching (Chen and Liu, 1992). Here, the Forward Maximum Matching algorithm (Chen and Liu, 1992) based on a dictionary is adopted; c) Stop words filtering. In our segmentation system, a hybrid strategy is applied (Figure 1): First, forward maximum matching (Chen and Liu, 1992), which is a dictionary-based method, is used to generate asegmentation result. In our system, the well-known forward maximum matching algorithm (Chen and Liu, 1992) is implemented. (Chen and Liu,1992) and many subsequent works), or uses the position of characters in a word as the basis for segmentation (Xue, 2003). In terms of processing model, this is a contradiction since segmentation should be the pre-requisite of dictionary lookup and should not presuppose lexical information. The classical model, described in (Chen and Liu, 1992) and still adopted in many recent works, considers text segmentation as a tokenization. This word-by-word approach ranges from naive maximum matching (Chen and Liu, 1992) to complex solution based on semi-Markov conditional random fields (CRF) (Andrew, 2006). Take for example maximum matching, which was a popular algorithm at the early stage of research (Chen and Liu, 1992).
Aligning A Parallel English-Chinese Corpus Statistically With Lexical Criteria We describe our experience with automatic alignment of sentences in parallel English-Chinese texts. Our report concerns three related topics: (1) progress on the HKUST English-Chinese Parallel Bilingual Corpus; (2) experiments addressing applicability of Gale (1991) lengthbased statistical method to the task of alignment involving a non-Indo-European language; and (3) an improved statistical method that also incorporates domain-specific lexical cues. In byte-length ratio approaches, the presence of long stretches of blocks that have roughly similar lengths can be problematic, and some improvement can be achieved by augmenting the byte-length measure by scores derived from lexical feature matching (Wu, 1994). For instance, the statistical correlation of sentence length between English and Chinese is not as high as that between two Indo European languages (Wu, 1994). As shown in Chen (1993) and Wu (1994), however, sentence length based methods suffer when the texts to be aligned contain small passages, or the languages involved share few cognates. There are basically three kinds of approaches on sentence alignment: the length-based approach (Gale and Church 1991 and Brown et al 1991), the lexical approach (Key and Roscheisen 1993), and the combination of them (Chen 1993, Wu 1994 and Langlais 1998, etc.). The first sentence alignment model used to align English-Chinese bilingual texts is proposed by Wu (1994). We use parts of the HKUST English-Chinese Bilingual Corpora for our experiments (Wu 1994), consisting of transcriptions of the Hong Kong Legislative Council debates in both English and Chinese. Work on sentence alignment of English and Chinese texts (Wu 1994), indicates that the lengths of English and Chinese texts are not as highly correlated as in French-English task, leading to lower success rate (85-94%) for length-based aligners. (DeKai Wu, 1994) offered that (1:1) sentence beads occupied 89% in English-Chinese as well. The approaches by (Wu, 1994), (Haruno and Yamazaki, 1996), (Ma, 2006) and (Gautam and Sinha, 2007) require an externally supplied bilingual lexicon. Wu (Wu, 1994) also used lexical cues from corpus-specific bilingual lexicon for better alignment. Existing parallel corpora have illustrated their particular value in empirical NLP research, e.g., Canadian Hansard Corpus (Gale and Church, 1991b), HK Hansard (Wu, 1994), INTERSECT (Salkie, 1995), ENPC (Ebeling, 1998), the Bible parallel corpus (Resnik et al, 1999) and many others. All experiments reported in this paper were performed on sentence-pairs from the HKUST English-Chinese Parallel Bilingual Corpus, which consists of governmental transcripts (Wu 1994).  Previous studies such as (Fung and Wu, 1994) have commented 268 that methods developed for Indo-European language pairs using alphabetic haracters have not addressed important issues which occur with European-Asian language pairs. It has also been suggested by (Wu, 1994) that sentence length ratio correlations may arise partly out of historic cognate-based relationships between Indo-European languages. (Wu, 1994), about whether the byte length model by itself can perform well.
Exploiting Syntactic and Shallow Semantic Kernels for Question Answer Classification We study the impact of syntactic and shallow semantic information in automatic classification of questions and answers and answer re-ranking. We define (a) new tree structures based on shallow semantics encoded in Predicate Argument Structures (PASs) and (b) new kernel functions to exploit the representational power of such structures with Support Vector Machines. Our experiments suggest that syntactic information helps tasks such as question/answer classification and that shallow semantics gives remarkable contribution when a reliable set of PASs can be extracted, e.g. from answers. Shallow semantic representations, bearing a more compact information, can prevent the sparseness of deep structural approaches and the weakness of BOW models (Moschitti et al, 2007). There is a widely held belief in the NLP and computational linguistics communities that identifying and defining roles of predicate arguments in a sentence has a lot of potential for and is a significant step toward improving important applications such as document retrieval, machine translation, question answering and information extraction (Moschitti et al., 2007). Shallow semantic representations, bearing a more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOW models (Moschitti et al, 2007). (Moschitti et al, 2007) solve this problem by designing the Shallow Semantic Tree Kernel (SSTK) which allows to match portions of a ST.  Counting the number of matched dependencies is essentially a simplified tree kernel for QA (e.g., see (Moschitti et al, 2007)) matching only trees of depth 2. In particular, in (Moschitti et al, 2007) kernels for the processing of PASs (in PropBank1 format (Kingsbury and Palmer, 2002)) extracted from question/answer pairs were proposed. Then, we design a novel shallow semantic kernel which is far more efficient and also more accurate than the one proposed in (Moschitti et al, 2007). To overcome this problem, a Shallow Semantic Tree Kernel (SSTK) was designed in (Moschitti et al, 2007). Thus, ? can recursively be an SRK (and evaluate Nested PASs (Moschitti et al, 2007)) or any other potential kernel (over the arguments). The result is 89.05? 1.25 and 83.73? 1.61 for 6 and 50 classes, which outperforms the best result of 86.1? 1.1 for 6 classes as reported in (Moschitti et al, 2007). In (Moschitti et al, 2007) it was shown that the use of TK improves QC of 1.2 percent points, i.e. from 90.6 to 91.8: further analysis of these fragments may help us to device compact, less sparse syntactic features and design more accurate models for the task. In (Moschitti et al, 2007), we proposed the Shallow Semantic Tree Kernel (SSTK) designed to encode PASs1 in SVMs. Instead, the similar PAS-SSTK representation in (Moschittiet al, 2007) does not take argument order into account, thus it fails to capture the linguistic rationale expressed above. Instead, the similar PAS-SSTK representation in (Moschittiet al, 2007) does not take argument order into account, thus it fails to capture the linguistic rationale expressed above. In future work, we intend to expand our analysis of both the gold-standard answer and the student answers beyond the bag-of-words paradigm by considering basic logical features in the text (i.e., AND, OR, NOT) as well as the existence of shallow grammatical features such as predicate argument structure (Moschitti et al, 2007) as well as semantic classes for words.
Indirect-HMM-based Hypothesis Alignment for Combining Outputs from Machine Translation Systems This paper presents a new hypothesis alignment method for combining outputs of multiple machine translation (MT) systems. An indirect hidden Markov model (IHMM) is proposed to address the synonym matching and word ordering issues in hypothesis alignment. Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the of the IHMM are estimated a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty. The IHMM-based method significantly outperforms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets. Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the Translation Edit Rate (TER, Snover et al (2006)) based alignment proposed in Sim et al (2007) is often taken as the baseline, and a couple of other approaches, such as the Indirect Hidden Markov Model (IHMM, He et al (2008)) and the ITG-based alignment (Karakos et al. (2008)), were recently proposed with better results reported. Our incremental alignment approaches adopt the same heuristics for alignment normalization stated in He et al (2008). The various parameters in the IHMM model are set as the optimal values found in He et al (2008). The comparison between the two pair-wise alignment methods shows that IHMM gives a 0.7 BLEU point gain over TER, which is a bit smaller than the difference reported in He et al (2008).  To compute scores for word pairs, we perform pair-wise hypothesis alignment using the indirect HMM (He et al 2008) for every pair of input hypotheses. The baselines include a pair-wise hypothesis alignment approach using the indirect HMM (IHMM) proposed by He et al (2008), and an incremental hypothesis alignment approach using the incremental HMM (IncHMM) proposed by Li et al (2009). He et al (2008) proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a variety of sources. We compute the association score from a linear combination of two clues: surface similarity computed as Equation (2) and position difference based distortion score by following (He et al, 2008). IHMM-based: He et al (2008) propose an indirect hidden Markov model (IHMM) for hypothesis alignment. We compute the distortion model by following (He et al, 2008) for IHMM and CLA-based methods. We compared our approach with the state-of-the-art confusion-network-based system (He et al, 2008) and achieved a significant absolute improvement of 1.23 BLEU points on the NIST 2005 Chinese-to-English test set and 0.93 BLEU point on the NIST 2008 Chinese-to-English test set. Since the candidate hypotheses are aligned using Indirect-HMM-based (IHMM-based) alignment method (He et al, 2008) in both direction, we briefly review the IHMM-based alignment method first.  On NIST MT05 test set, the lattice-based system gave better results with an absolute improvement of 1.23 BLEU points over the confusion network-based system (He et al, 2008) and 3.73 BLEU points over the best single system. Aligning translation hypotheses can be challenging and has a substantial effect on combination performance (He et al, 2008).  Aligning translation hypotheses accurately can be challenging, and has a substantial effect on combination performance (He et al, 2008). He et al (2008) proposed using an indirect hidden Markov model (IHMM) for pairwise alignment of system outputs.
Improvements In Automatic Thesaurus Extraction The use of semantic resources is comin modern but methods to extract lexical semantics have only recently begun to perform well enough for practical use. We evaluate existing and new similarity metrics for thesaurus extraction, and experiment with the tradeoff between extraction performance and efficiency. We propose an approximation based on attributes and coarseand fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty. Curran and Moens (2002b) evaluate thesaurus extractors based on several different models of context on large corpora. All the systems use the JACCARD similarity metric and TTEST weighting function that were found to be most effective for thesaurus extraction by Curran and Moens (2002a). Curran and Moens (2002b) have demonstrated that more complex and constrained contexts can yield superior performance, since the correlation between context and target term is stronger than simple window methods. We worked with an implementation of the log likelihood ratio (g-Score) as proposed by Dunning (1993) and two variants of the t-score, one considering all values (t-score) and one where only positive values (t-score+) are kept following the results of Curran and Moens (2002). Another venue of research may be to exploit different thesauri, such as the ones automatically derived as in (Curran and Moens, 2002). Several researchers (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)) have used large monolingual corpora to extract distributionally similar words. Monolingual syntax-based distributional similarity is used in many proposals to find semantically related words (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)). Curran and Moens (2002) report on a large scale evaluation experiment, where they evaluated the performance of various commonly used methods. Vander Plas and Bouma (2005) present a similar experiment for Dutch, in which they tested most of the best performing measures according to Curran and Moens (2002). Curran and Moens (2002) show that synonymy extraction for lexical semantic resources using distributional similarity produces continuing gains in accuracy as the volume of input data increases. Curran and Moens (2002) introduces a vector of canonical attributes (of bounded length k m), selected from the full vector, to represent the term. Comparisons made with these low frequency terms are unreliable (Curran and Moens, 2002). Recently, there has been much interest in finding words which are distribution ally similar e.g., Lin (1998), Lee (1999), Curran and Moens (2002), Weeds (2003) and Geffet and Dagan (2004). In these experiments we have used a variant of Dice, proposed by Curran and Moens (2002). Pereira et al (1993), Curran and Moens (2002) and Lin (1998) use syntactic features in the vector definition. Also, because it has been shown (Curran and Moens, 2002) that negative PMI values worsen the distributional similarity performance, we bound PMI so that PMI (wi, cj)= 0 if PMI (wi, cj) < 0. A first major algorithmic approach is to represent word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Curran and Moens, 2002). Curran and Moens (2002b) have demonstrated that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality. For these experiments we use the JACCARD (1) measure and the TTEST (2) weight, as Curran and Moens (2002a) found them to have the best performance in their comparison of many distance measures. Curran and Moens (2002a) propose an initial heuristic comparison to reduce the number of full O(m) vector comparisons.
A Regression Model of Adjective-Noun Compositionality in Distributional Semantics In this paper we explore the computational modelling of compositionality in distributional models of semantics. In particular, we model the semantic composition of pairs of adjacent English Adjecand Nouns from the National We build a vector-based semantic space from a lemmatised version of the BNC, where the most frequent A-N lemma pairs are treated as single tokens. We then extrapolate three different models of compositionality: a simple additive model, a pointwise-multiplicative model and a Partial Least Squares Regression (PLSR) model. We propose two evaluation methods for the implemented models. Our study leads to the conclusion that regression-based models of compositionality generally out-perform additive and multiplicative approaches, and also show a number of advantages that make them very promising for future research. Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. Guevara (2010) proposed a related method of learning composition which used linear regression to learn how components compose. Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. The general equation for the two functions is the following, where B is a matrix of weights that is multiplied by the noun vector v to produce the AN vector p. p= Bv (5) In the linear map (lim) approach proposed by Guevara (2010), one single matrix B is learnt that represents all adjectives. Following Guevara (2010), we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented inthe Rpls package (Mevik and Wehrens, 2007), setting the latent dimension parameter of PLSR to 300. Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators. We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs: the simple additive and multiplicative models of Mitchell and Lapata (2008), and the linear-map-based models of Guevara (2010) and Baroni and Zamparelli (2010). The main innovation of Guevara (2010), who focuses on adjective-noun combinations (AN), is to use the co-occurrence vectors of corpus-observed ANs to train a supervised composition model. In the linear map (lm) approach proposed by Guevara (2010), a composite AN vector is obtained by multiplying a weight matrix by the concatenation of the adjective and noun vectors, so that each dimension of the generated AN vector is a linear combination of dimensions of the corresponding adjective and noun vectors. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al (2012) and Collobert et al (2011) have been proposed. Guevara (2010), Mitchell and Lapata (2010), Socher et al (2011) and Zanzotto et al (2010) generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model. This work extends the basic methodology presented in Guevara (2010) with new data collection techniques, improved evaluation metrics and new case studies. Guevara (2010) and Baroni and Zamparelli (2010) introduce a different approach to model semantic compositionality in distributional spaces by extracting context vectors from the corpus also for the composed vector. The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression. Let us start by setting the syntactic relation that we want to focus on for the purposes of this study: following Guevara (2010) and Baroni and Zamparelli (2010), I model the semantic composition of adjacent Adjective-Noun pairs expressing attributive modification of a nominal head. Guevara (2010) and Mitchell and Lapata (2010). A more general form of the additive model (full add) has been proposed by Guevara (2010) (see also Zanzotto et al (2010)). Guevara (2010) and Zanzotto et al (2010) propose the full additive model (full add), where the two vectors to be added are pre-multiplied by weight matrices: c= Au+Bv Since the Mitchell and Lapata and full add models were developed for phrase composition, the two input vectors were taken to be, very straightforwardly, the vectors of the two words to be com posed into the phrase of interest. Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. Although we are not bound to a specific composition model, throughout this paper we use the method proposed by Guevara (2010) and Zanzottoet al (2010) which defines composition as application of linear transformations to the two constituents followed by summing the resulting vectors: f comp R (~u, ~v)= W 1 ~u+W 2 ~v. We will further use the following equivalent formulation: f comp R (~u, ~v)= W R [~u; ~v] where W R? R d? 2dand [~u; ~v] is the vertical concatenation of the two vectors (using Matlab notation).
Scalable Inference And Training Of Context-Rich Syntactic Translation Models Statistical MT has made great progress in the last few years, but current translation models are weakon re-ordering and target language fluency. Syn tactic approaches seek to remedy these problems.In this paper, we take the framework for acquiring multi-level syntactic translation rules of (Gal ley et al, 2004) from aligned tree-string pairs, and present two main extensions of their approach: first, instead of merely computing a single derivation that minimally explains a sentence pair, we constructa large number of derivations that include contextually richer rules, and account for multiple interpretations of unaligned words. Second, we pro pose probability estimates and a training procedure for weighting these rules. We contrast differentapproaches on real examples, show that our esti mates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated, and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules. If we analyze these three models in terms of expressive power, the Galley et al (2006) model is more expressive than the SPMT models, which in turn, are more expressive than Chiang's model. The xRS formalism utilized by Galley et al (2006) allows for the use of translation rules that have multi-level target tree annotations and discontinuous source language phrases. The parameters of the SPMT models presented in this paper are easier to estimate than those of Galley et als (2006) and can easily exploit and expand on previous research in phrase-based machine translation.  Syntax-driven (Galley et al, 2006) and hierarchical translation models (Chiang, 2005) take advantage of probabilistic synchronous context free grammars (PSCFGs) to represent structured, lexical reordering constraints during the decoding process. (Galley et al, 2006) use syntactic constituents for the PSCFG nonterminal set and (Zollmann and Venugopal, 2006) take advantage of CCG (Steedman, 1999) categories, while (Chiang, 2005) uses a single generic nonterminal. Similarly, the tree-to-string syntax-based transduction approach offers a complete translation framework (Galley et al, 2006).  Six MT systems were combined: three (A, C, E) were phrase based similar to (Koehn, 2004), two (B, D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al, 2006). In this paper, we focus on syntactic translation with tree-transducer rules (Galley et al, 2006). GIZA++ union alignments have been used in the state-of-the-art syntax-based statistical MT system described in (Galley et al, 2006) and in the hierarchical phrase-based system Hiero (Chiang, 2007). Using these alignments, which we refer to as GIZA++ union + link deletion, we train a syntax-based translation system similar to that described in (Galley et al., 2006). For example, Galley et al (2004) initially built a syntax-based system using only minimal rules, and subsequently reported (Galley et al, 2006) that composing rules improves Bleu by 3.6 points, while increasing grammar size 60-fold and decoding time 15-fold. Galley et al (2006 )argued that breaking a single tree pair into multiple decompositions is important for correct probability modeling. For example, Galley et al (2006) proposed the idea of rule composing which composes two or more rules with shared states to form a larger, composed rule. Following Galley et al (2006)'s work, Marcu et al (2006) proposed SPMT models to improve the coverage of phrasal rules, and demonstrated that the system performance could be further improved by using their proposed models. As shown in the following parts of this paper, it works very well with the existing techniques, such as rule composing (Galley et al, 2006), SPMT models (Marcu et al, 2006) and rule extraction with k best parses (Venugopal et al, 2008). In this work, the issue of translation rule extraction is studied in the string-to-tree model proposed by Galley et al (2006). Finally, the rule composing method (Galley et al, 2006) is used to compose two or more minimal GHKM or SPMT rules having shared states to form larger rules. Our baseline MT system is built based on the string-to-tree model proposed in (Galley et al, 2006).
A Compression-Based Algorithm For Chinese Word Segmentation Chinese is written without using spaces or other word delimiters. Although a text may be thought of as a corresponding sequence of words, there is considerable ambiguity in the placement of boundaries. Interpreting a text as a sequence of words is beneficial for some information retrieval and storage tasks: for example, full-text search, word-based compression, and key phrase extraction. We describe a scheme that infers appropriate positions for word boundaries using an adaptive language model that is standard in text compression. It is trained on a corpus of presegmented text, and when applied to new text, interpolates word boundaries so as to maximize the compression obtained. This simple and general method performs well with respect to specialized schemes for Chinese language segmentation. Using a statistical model called prediction by partial matching (PPM), Teahan et al (2000) reported a significantly better result. Teahan (Teahan et al, 2000) has successfully applied escape method D to segment Chinese text. Treating each character individually as in (Teahan et al, 2000) requires a large amount of training data in order to calculate all the probabilities in the tables, as well as a large amount of table space and time to lookup data from the tables. Machine learning approaches are more desirable and have been successful in both unsupervised learning (Peng and Schuur mans, 2001) and supervised learning (Teahan et al, 2000). For example, the N-gram generative language modeling based approach of Teahan et al (2000) does not use domain knowledge. In addition, compared to simple models like n-gram language models (Teahan et al, 2000), another shortcoming of CRF-based segmenters is that it requires significantly longer training time. This approach reduces to the cross-entropy/compression-based approach of (Teahan et al 2000). Second, the probabilistic models used in these methods (e.g. Teahan et al, 2000) are trained on a segmented corpus which is not always available. This is in contrast to supervised word segmentation algorithms (e.g., Teahan et al, 2000), which are typically used for segmenting text in documents written in languages that do not put spaces between their words like Chinese.
Probabilistic Text Structuring: Experiments With Sentence Ordering Ordering information is a critical task for natural language generation applications. In this paper we propose an approach to information ordering that is particularly suited for text-to-text generation. We describe a model that learns constraints on sentence order from a corpus of domainspecific texts and an algorithm that yields the most likely order among several alternatives. We evaluate the automatically generated orderings against authored texts from our corpus and against human subjects that are asked to mimic the model’s task. We also assess the appropriateness of such a model for multidocument summarization. Furthermore, to provide some assessment of the quality of the predicted orderings themselves, we follow Lapata (2003) in employing Kendall's t, which is a measure of how much an ordering differs from the OSO --- the underlying assumption is that most reasonable sentence orderings should be fairly similar to it.   The same data and similar methods were used by Barzilay and Lee (2004) to compare their probabilistic approach for ordering sentences with that of Lapata (2003). It is typically applicable in the text generation field, both for concept-to-text generation and text-to text generation (Lapata, 2003), such as multiple document summarization (MDS), question answering and so on. For works taking no use of source document, Lapata (2003) proposed a probabilistic model which learns constraints on sentence ordering from a corpus of texts. The probability model originates from (Lapata, 2003), and we implement the model with four features of lemmatized noun, verb, adjective or adverb, and verb and noun related dependency. In contrast, more recent research has focused on stochastic approaches that model discourse coherence at the local lexical (Lapata, 2003) and global levels (Barzilay and Lee, 2004), while preserving regularities recognized by classic discourse theories (Barzilay and Lapata, 2005).   In contrast, the greedy algorithm of Lapata (2003) makes grave search errors. The genetic algorithms of Mellish et al (1998) and Karamanis and Manarung (2002), as well as the greedy algorithm of Lapata (2003), provide no theoretical guarantees on the optimality of the solutions they propose. Adjacency of sentences has been previously used to model local coherence (Lapata, 2003). Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. In this respect, this is similar to work by Lapata (2003), who builds a conditional model of words across adjacent sentences, focusing on words in particular semantic roles. Lapata (2003) has suggested a probabilistic model of text structuring and its application to the sentence ordering. Even though we could not compare our experiment with the probabilistic approach (Lapata, 2003) directly due to the difference of the text corpora, the Kendall coefficient reported higher agreement than Lapata's experiment (Kendall=0.48 with lemmatized nouns and Kendall=0.56 with verb-noun dependencies). Lapata (2003) proposed an algorithm that computes the probability of two sentences being adjacent for ordering sentences. Lapata (2003) employed the probability of two sentences being adjacent as determined from a corpus. As the features, Lapata (2003) proposed the Cartesian product of content words in adjacent sentences.
Multi-Criteria-Based Active Learning For Named Entity Recognition In this paper, we propose a multi-criteriabased active learning approach and effectively apply it to named entity recognition. Active learning targets to minimize the human annotation efforts by selecting examples for labeling. To maximize the contribution of the selected examples, we the multiple criteria: informativepropose measures to quantify them. More comprehensively, we incorporate all the criteria using two selection strategies, both of which result in less labeling cost than single-criterion-based method. The results of the named entity recognition in both MUC-6 and GENIA show that the labeling cost can be reduced by at least 80% without degrading the performance. It is only recently employed in NER (Shen et al, 2004). This issue was previously addressed in Shen et al (2004) in the context of named-entity recognition, where they used a two-step procedure to first select the most informative and representative samples, followed by a diversity filter. In a more recent study, Shen et al (2004) consider AL for entity recognition based on Support Vector Machines.  Given the variety of methods that are available for generating training data efficiently automatically using extant domain resources (Morgan et al, 2004) or semi-automatically (active learning approaches like Shen et al (2004) or systems using seed rules such as Mikheev et al. Active learning, which has been applied to the problem of NER in (Shen et al, 2004), is used in situations where a large amount of unlabeled data exists and data labeling is expensive. Diversity measures as proposed by (Shen et al, 2004) might help in mitigating this effect, but our experiments show that there are fundamental differences between text classification and NER. It has been applied to various NLP/IE tasks, including named entity recognition (Shen et al, 2004) and parse selection (Baldridge and Osborne, 2004) with rather impressive results in reducing the amount of annotated training data. In order to circumvent this obstacle several approaches have been presented, among them active learning (Shen et al, 2004) and rule-based systems encoding domain specific knowledge (Gaizauskas et al, 2003). Shen et al (2004) combine multiple criteria to measure the informativeness, representativeness, and diversity of examples in active learning for named entity recognition. Therefore, in order to avoid recursion and over-complexity, we employ a diversity-motivated intra-stratum sampling scheme (Shen et al, 2004), called KDN (K-diverse neighbors), which aims to maximize the training utility of all seeds from a stratum. (Collins and Singer, 1999) classified NEs through co-training, (Kozareva et al, 2005a) used self-training and co-training to detect and classify named entities in news domain, (Shen et al, 2004) conducted experiments with multi-criteria-based active learning for biomedical NER.  Shen et al (2004) proposed an approach to selecting examples based on informativeness, representativeness and diversity criteria.
Template-Based Information Extraction without the Templates Standard algorithms for template-based information extraction (IE) require predefined template schemas, and often labeled data, to learn to extract their slot fillers (e.g., an the a template). This paper describes an approach to template-based IE that removes this requirement and performs extraction without knowing the template structure in advance. Our algorithm instead learns the template structure automatically from raw text, inducing template schemas as sets of linked events (e.g., include off, deassociated with semantic roles. We also solve the standard IE task, using the induced syntactic patterns to extract role fillers from specific documents. We evaluate on the MUC-4 terrorism dataset and show that we induce template structure very similar to handcreated gold structure, and we extract role fillers with an F1 score of .40, approaching the performance of algorithms that require full knowledge of the templates. Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. Most recently, (Chambers and Jurafsky, 2011) acquire event words from an external resource, group the event words to form event scenarios, and group extraction patterns for different event roles. (Chambers and Jurafsky, 2011) (C+J) created an event extraction system by acquiring event words from WordNet (Miller, 1990), clustering the event words into different event scenarios, and grouping extraction patterns for different event roles. The use of case frames is well grounded in a variety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004), and information extraction (Chambers and Jurafsky, 2011), where they serve the central unit of semantic analysis. We propose a method for inferring event templates based on word clustering according to their proximity in the corpus and syntactic function clustering. For example, Patwardhan and Riloff (2007) and Chambers and Jurafsky (2011) consider an IE approach where the extraction targets are MUC-4 style document-level templates (Sundheim, 1991), the former a supervised system and the latter fully unsupervised. Efficiently storing such structures is an important step in integrating document-level statistics into downstream tasks, such as characterizing complex scenarios (Chambers and Jurafsky, 2011), or story understanding (Gordon et al, 2011). Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. (Chambers and Jurafsky, 2011) (Poon and Domingos, 2010) (Chen et al 2011) focus on extracting frame-like structures (Baker et al 1998) by defining two types of clusters, event clusters and role clusters. (Chambers and Jurafsky, 2011) is similar to our approach in that it learns a semantic model, called template, from unlabelled news articles and then uses the template to extract information. There has been work that attempts to fill predefined templates using Bayesian nonparametrics (Haghighi and Klein, 2010) and automatically learns template structures using agglomerative clustering (Chambers and Jurafsky, 2011). Our first baseline is PROFINDER, a state of-the-art template inducer which Cheung et al (2013) showed to outperform the previous heuristic clustering method of Chambers and Jurafsky (2011).
Randomised Language Modelling for Statistical Machine Translation A Bloom filter (BF) is a randomised data structure for set membership queries. Its space requirements are significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability. Here we explore the use of BFs for language modelling in statistical machine translation. show how a BF containing can enable us to use much larger corpora and higher-order models complementing a con- LM within an SMT system. We also consider (i) how to include approximate frequency information efficiently within a BF and (ii) how to reduce the error rate of these models by first checking for sub-sequences in candidate grams. Our solutions in both cases retain the one-sided error guarantees of the BF while taking advantage of the Zipf-like distribution of word frequencies to reduce the space requirements. Our framework makes use of the log-frequency Bloom filter presented in (Talbot and Osborne, 2007), and described briefly below, to compute smoothed conditional n-gram probabilities on the fly. Recent work (Talbot and Osborne, 2007) presented a scheme for associating static frequency information with a set of n-grams in a BF efficiently. As noted in Talbot and Osborne (2007), errors for this log-frequency BF scheme are one-sided: frequencies will never be underestimated. There is a potential risk of redundancy if we represent related statistics using the log-frequency BF scheme presented in Talbot and Osborne (2007). We refer to (Talbot and Osborne, 2007) for empirical results establishing the performance of the log frequency BF-LM: overestimation errors occur with a probability that decays exponentially in the size of the overestimation error. We hope the present work will, together with Talbot and Osborne (2007), establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics. Work by Talbot and Osborne (2007), Van Durme and Lall (2009) and Goyal et al (2009) considered the problem of building very large language models via the use of randomized data structures known as sketches. Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. However, if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size (Carter et al, 1978), (Talbot and Osborne, 2007a). Recent work (Talbot and Osborne, 2007b) has used lossy encodings based on Bloom filters (Bloom, 1970) to represent logarithmically quantized corpus statistics for language modeling. Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al, 2007) no errors are possible for n grams stored in the model. Following (Talbot and Osborne, 2007a) we can avoid unnecessary false positives by not querying for the longer n-gram in such cases. We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007). All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data. RANDLM (Talbot and Osborne, 2007) performs well and scaled to the full data with improvement (resulting in our best overall system). For language modeling, we use RandLM (Talbot and Osborne, 2007). The system we submitted corresponds to the GIZA++ and SBLITG (only news) system, but with RandLM (Talbot and Osborne, 2007) as language model rather than SRILM. RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures. Lossy compressed models RandLM (Talbot and Osborne, 2007) and Sheffield (Guthrie and Hepple,2010) offer better memory consumption at the expense of CPU and accuracy. There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately.
Measuring The Semantic Similarity Of Texts This paper presents a knowledge-based method for measuring the semanticsimilarity of texts. While there is a large body of previous work focused on finding the semantic similarity of concepts and words, the application of these wordoriented methods to text similarity has not been yet explored. In this paper, we introduce a method that combines wordto-word similarity metrics into a text-totext metric, and we show that this method outperforms the traditional text similarity metrics based on lexical matching. The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. Most similar to our approach are the methods of Islam and Inkpen (2008) and Corley and Mihalcea (2005), who performed a word-to-word similarity alignment; however, they did not operate at the sense level. Although these implications are uncontroversial, their automatic recognition is complex if we rely on models based on lexical distance (or similarity) between hypothesis and text ,e.g., (Corley and Mihalcea, 2005).   Corley and Mihalcea (2005) proposed a hybrid method by combining six existing knowledge-based methods. Then, we use this cross-pair similarity with more traditional intra-pair similarities (e.g., (Corley and Mihalcea, 2005)) to define a novel kernel function. In line with many other researches (e.g., (Corley and Mihalcea, 2005)), we determine these anchors using different similarity or relatedness dec tors: the exact matching between tokens or lemmas, a similarity between tokens based on their edit distance, the derivation ally related form relation and the verb entailment relation in WordNet, and, finally, a WordNet-based similarity (Jiang and Conrath, 1997). Experimental results lexical similarity siml (T, H) as defined in (Corley and Mihalcea, 2005). First, as observed in (Corley and Mihalcea, 2005) the lexical-based distance kernel Kl shows an accuracy significantly higher than the random baseline ,i.e. 50%. The first backup strategy is a straightforward BoW method that we will not present in this paper (see more details in (Corley and Mihalcea, 2005)). In practice, we implement WBOW by using the text similarity measure defined in (Corley and Mihalcea, 2005) as the single feature in the SVM classifier that, as in BOW, learns the threshold on this single feature. The system performance reported in (CM05; (Corley and Mihalcea, 2005)), which is among the best we are aware of, is also included for comparison. This simple feature is the lexical similarity between T and H computed using WordNet-based metrics as in (Corley and Mihalcea, 2005). Otherwise, WordNet (Miller, 1995) similarities (as in (Corley and Mihalcea, 2005)) and different relation between words such as verb entailment and derivational morphology are applied. Although, there are asymmetric measures such as the Monge-Elkan measure (1996) and the measure proposed by Corley and Mihalcea (Corley and Mihalcea, 2005), they are outnumbered by the symmetric measures. Also, we will try different similarity score functions for both the clustering and the anchor approaches, as those surveyed in Corley and Mihalcea (2005). The semantic similarity formula from (Corley and Mihalcea, 2005) defines the similarity of a pair of documents differently depending on with respect to which text it is computed.
The Berkeley FrameNet Project is a NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, &quot;Tools for Lexicon Building&quot;). The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics. The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between &quot;frame elements&quot; and their syntactic realizations (e.g. grammatical function, phrase type, and other syntactic traits). This report will present the project's goals and workflow, and information about the computational tools that have been adapted or created in-house for this work. Our statistical algorithms are trained on a hand-labeled dataset: the FrameNet database (Baker et al, 1998). Recently, the integration of NLP systems with manually-built resources at the predicate argument-level, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al, 2005) has received growing interest. Building on this frame-semantic model, the Berkeley FrameNet project (Baker et al, 1998) has been developing a frame-semantic lexicon for the core vocabulary of English since 1997. Several lexical resources have been built manually, most notably WordNet (Fellbaum, 1998), FrameNet (Baker et al, 1998) and VerbNet (Baker et 122al., 1998). Therefore, many large human-annotated corpora have been constructed to support related research, such as FrameNet (Baker et al, 1998), PropBank (Kings bury and Palmer, 2002), NomBank (Meyers et al,2004), and so on. UBY currently contains nine resources in two languages: English WordNet (WN, Fellbaum (1998), Wiktionary2 (WKT-en), Wikipedia3 (WP en), FrameNet (FN, Baker et al (1998)), and VerbNet (VN, Kipper et al (2008)); German Wiktionary (WKT-de), Wikipedia (WP-de), and Ger ma Net (GN, Kunze and Lemnitzer (2002)), and the English and German entries of OmegaWiki4 (OW), referred to as OW-en and OW-de. In formalisms such as Frame Semantics (Baker et al, 1998), semantic roles generalize across semantically similar predicates belonging to the same frame. Other groups however have shown that this can be done, e.g., in Framenet (Baker et al, 1998) and more recently in PropBank (Kingsbury and Palmer, 2002). This type of approach has led to two highly valued semantic resources: the Prince ton WordNet (Fellbaum, 1998) and the Berkeley Framenet (Baker et al, 1998). FrameNet (FN; Baker et al (1998), Ruppenhofer et al (2010)) is a lexical resource based on FS. (6) To deal with these errors, we may use rich knowledge about verbs such as VerbNet (Kipper et al, 2000) and FrameNet (Baker et al, 1998) in order to judge whether a verb is transitive or intransitive. The current computational scene has witnessed an increased interest in the creation and use of semantically annotated computational lexica and their associated annotated corpora, like PropBank (Palmeretal., 2005), FrameNet (Baker et al, 1998) and NomBank (Meyers, 2007), where the proposed annotation scheme has been applied in real contexts. Specifically, the task included first the identification of frames and frame elements in a text following the FrameNet paradigm (Baker et al, 1998), then the identification of locally uninstantiated roles (NIs). In Rule 14, we use FrameNet (Baker et al 1998) to determine whether med/situation should be assigned to an NP, NPi. The modeling of Process as a kind of event that is continuous and homogeneous in nature, follows the frame semantic analysis used for generating the FRAMENET data (Baker et al, 1998). For predicate structure annotation, we followed the FrameNet model (Baker et al., 1998) (see Section 2.2). We carried out predicate argument structure annotation applying the FrameNet paradigm as described in (Baker et al, 1998). Driven by annotation resources such as FrameNet (Baker et al, 1998) and PropBank (Palmer et al, 2005), many systems developed in these studies have achieved argument F1 scores near 80% in large-scale evaluations such as the one reported by Carreras and Marquez (2005). FrameNet (Baker et al, 1998) and OpenCyc are other valuable resources for English, also hand-created, that contain a rich set of relations between words and concepts. In our experiments we used two datasets: FN extracted from FrameNet II 1.1 (Baker et al., 1998) TB2 extracted from Penn Treebank-II.
Cheap and Fast â€“ But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon’s Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense. Previous work has shown that data collected through the Mechanical Turk service is reliable and comparable in quality with trusted sources (Snow et al, 2008). Previous studies using this on line task marketplace have shown that the collective judgments of many workers are comparable to those of trained annotators on labeling tasks (Snow et al, 2008) although these judgments can be obtained at a fraction of the cost and effort. Snow et al (2008) compared the quality of labels produced by non-expert Turkers against those made by experts for a variety of NLP tasks and found that they required only four responses per item to emulate expert annotations. Human judgments for the triples were collected through the Amazon Mechanical Turk (AMT) crowd sourcing platform (Snow et al, 2008).  Mechanical Turk has also been used to create labeled data sets for word sense disambiguation (Snow et al, 2008) and even to modify sense inventories. Along this spirit, Snow et al (2008) show that obtaining multiple low quality labels (through Mechanical Turk) can approach high-quality editorial labels. MTurk is becoming a popular means of eliciting and collecting linguistic intuitions for NLP research; see Snow et al (2008) for an overview and a further discussion.  This might suggest that a further increase in the number of HIT assignments would outperform expert ITA scores, as was previously reported in (Snow et al, 2008). In this paper, we address these issues by using Amazon's Mechanical Turk (MTurk), online non-expert annotators (Snow et al, 2008). Recently, however, attempts have been made to leverage non-expert annotations provided by Amazon's Mechanical Turk (MTurk) service to create large training corpora at a fraction of the usual costs (Snow et al 2008). Commonly, MTurk has been used for the classification task (Snow et al. 2008) or for straightforward data entry. Snow et al (2008) have validated AMT as a valid data source by comparing non-expert with gold-standard expert judgments. It is common to do such filtering when using crowd sourced data by using the majority or median vote as the final judgment or to calibrate judges using expert judgments (Snow et al 2008). Most of our measures were scalar; we chose to do this because previous work on estimating the relationship between MTurk annotations and expert an notations suggest that taking the means of scalar annotations could be a good way to reduce noise in MTurk annotations (Snow et al, 2008). Recently, AMT has been shown to be an effective tool for annotation and evalatuation in NLP tasks ranging from word similarity detection and emotion detection (Snow et al, 2008) to Machine Translation quality evaluation (Callison-Burch, 2009). We intend to experiment with different guidelines and instructions, and to screen (Callison Burch, 2009) and weight Turkers' responses (Snow et al, 2008), in order to lower the number of Turkers required for this task. We have used the dataset created by Snow et al (2008) for the task of recognising textual entailment, originally proposed by Dagan et al (2006) in the PASCAL Recognizing Textual Entailment (RTE) Challenge. Snow et al (2008) work with a majority rule where ties are broken uniformly at random and report an observed agreement (accuracy) between the majority rule and the gold standard of 89.7%.
A Maximum Entropy Model for Prepositional Phrase Attachment Adwait Ratnaparkhi, Jeff  Reynar,* and Salim Roukos IBM Research  D iv is ion Thomas  J. Watson  Research  Center York town Heights ,  NY  10598 1.  Also, Ratnaparkhi et al (1994) conducted human experiments with a subset of their corpus. Collins and Brooks (1995) introduced modifications to the Ratnaparkhi et al (1994) dataset meant to combat data sparsity and used the modified version to train their backed-off model. Stetina and Nagao (1997) trained on a version of the Ratnaparkhi et al (1994) dataset that contained modifications similar to those by Collins and Brooks (1995) and excluded forms not present in WordNet. The system achieved 89.0% on a similarly modified Ratnaparkhi et al (1994) dataset. They used a version of the Ratnaparkhi et al (1994) dataset that had all words lemmatized and all digits replaced by @. Their final system had 85.0% precision and 91.8% recall on the Ratnaparkhi et al (1994) dataset. The best performing systems for many tasks in natural language processing are based on supervised training on annotated corpora such as the Penn Treebank (Marcus et al, 1993) and the prepositional phrase dataset first described in (Ratnaparkhi et al, 1994). We did not use the PP data set described by (Ratnaparkhi et al, 1994) because we are using more context than the limited context available in that set (see below). For English, the average human performance on pp-attachment for the (v, n1, p, n2) problem formulation is just 88.2% when given only the four head-words, but increases to 93.2% when given the full sentence (Ratnaparkhi et al, 1994). A benchmark dataset of 27,937 such quadruples was extracted from the Wall Street Journal corpus by Ratnaparkhi et al (1994) and has been the basis of many subsequent studies comparing machine learning algorithms and lexical resources. Ratnaparkhi et al (1994) trained a maximum entropy model on (v, n1, p, n2) quadruples extracted from the Wall Street Journal corpus and achieved 81.6% accuracy. The maximum entropy approach of Ratnaparkhiet al (1994) uses the mutual information clustering algorithm described in (Brown et al, 1992). For our experiments we use the Wall Street Journal dataset created by Ratnaparkhi et al (1994). Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maximum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998). Ratnaparkhi et al (1994) created a benchmark dataset of 27,937 quadruples (v, n1, p, n2), extracted from the Wall Street Journal. It has been used in a variety of difficult classification tasks such as part-of-speech tagging (Ratnaparkhi, 1996), prepositional phrase attachment (Ratnaparkhi et al, 1994) and named entity tagging (Borthwick et al, 1998), and achieves state of the art performance. A year later (Ratnaparkhi et al, 1994) published a supervised approach to the PP attachment problem. (Ratnaparkhi et al, 1994) used 20,801 tuples for training and 3097 tuples for evaluation. The difference in noun attachments between these two sets is striking, but (Ratnaparkhi et al, 1994) do not discuss this (and we also do not have an explanation for this). But it makes obvious that (Ratnaparkhi et al, 1994) were tackling a problem different from (Hindle and Rooth, 1993) given the fact that their baseline was at 59% guessing noun attachment (rather than 67% in the Hindle and Rooth experiments).
A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation chl, dozhang@microsoft.com mhli@insun.hit.edu.cn muli, mingzhou@microsoft.com guanyi@insun.hit.edu.cn Abstract Inspired by previous preprocessing approaches to SMT, this paper proposes a novel, probabilistic approach to reordering which combines the merits of syntax and phrase-based SMT. Given a source sentence and its parse tree, our method generates, tree operations, an list of reordered inputs, which are then fed to standard phrase-based decoder to produce the optimal translation. Experiments show that, for the NIST MT-05 task of Chinese-to- English translation, the proposal leads to BLEU improvement of 1.56%. Li et al (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents. More recently, Li et al (2007) use a maximum entropy system to learn reordering rules for binary trees (i.e., whether to keep or reorder for each node). This is one of the main problems addressed in the present work. (Li et al, 2007) use weighted n-best lists as in put for the decoder. A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (Zhang et al, 2007a) and the impact of syntax on reordering is difficult to single out (Li et al, 2007). This paper follows the term convention of global reordering and local reordering of Li et al (2007), between which the distinction is solely defined by reordering distance (whether beyond four source words) (Li et al, 2007). Rules created from a syntactic parser are also utilized to form weighted n-best lists which are fed into the decoder (Li etal., 2007). To overcome the problem, Li et al (2007) proposed k-best approach. A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. Li et al (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation. Li et al (2007) used N-best reordering hypotheses to overcome the reordering ambiguity. Syntax-based SMT is better suited to cope with long-distance dependencies, however there also are problems, some of them originated from the linguistic motivation itself, incorrect parse trees, or reordering that might involve blocks that are not constituents (Li et al, 2007).  Li et al (2007) used a parser to get the syntactic tree of the source language sentence.
Combining Distributional And Morphological Information For Part Of Speech Induction In this paper we discuss algorithms for clustering words into classes from unlabelled text using unsupervised algorithms, based on distributional and morphological information. We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.   We used Alexander Clarke's software, based on (Clark, 2003), to cluster the words, and then allow each word to be labeled with any part of speech tag seen in the data with any other word in the same cluster. Since induction is founded to some extent upon disambiguating contexts, this work has some bearing on the evaluation of induced categories with corpus annotation; not only is there more than one tag set in existence (see discussion in Clark, 2003), but annotation schemes make distinctions that morphosyntactic contexts can not readily capture. As a base tagger, we modify a leading unsupervised POS tagger (Clark, 2003) to constrain the distributions of word types across clusters to be Zipfian, allowing us to utilize a perplexity-based quality test. Figure 1 demonstrates this phenomenon for a leading POS induction algorithm (Clark, 2003). We focus here on Clark's tagger (Clark, 2003) (CT), probably the leading POS induction algorithm (see Table 3). Clark (2003) proposed a perplexity based test for the quality of his POS induction algorithm. In this paper we show that for the tagger of (Clark, 2003) such a method provides mediocre results (Table 2) even when the training criterion (likelihood or data probability for this tagger) is evaluated on the test set.  For example, (Sch?utze, 1993) induces 200 clusters and (Clark, 2003) chooses between 16-128; and most of these induced categories are difficult to associate with a specific POS tag.  We continue by tagging the corpus using Clark's unsupervised POS tagger (Clark, 2003) and the unsupervised Prototype Tagger (Abendetal., 2010). In the 'Fully Unsupervised' scenario, prepositions and verbs were identified using Clark's tagger (Clark, 2003).  Using a large corpus of abstracts from PubMed (30,963,886 word tokens of 335,811 word types), we cluster words by their syntactic contexts and morphological contents (Clark, 2003). Toutanova et al. (2003) describe a wide variety of morphological and distributional features useful for POS tagging, and Clark (2003) proposes ways of incorporating some of these in an unsupervised tagging model. As Clark (2003) points out, many-to-1 accuracy has several defects.  Both of the older systems discussed by Christodoulopoulos et al (2010), i.e., Clark (2003) and Brown et al (1992), included this constraint and achieved very good performance relative to token-based systems.
Centroid-Based Summarization Of Multiple Documents: Sentence Extraction Utility-Based Evaluation And User Studies We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization. Obviously, intrinsic evaluation is more reliable, but it remains an extremely laborious process, where inter-judge disagreement is still an issue, see (Radev et al, 2000). The first of these, relative utility (RU) (Radev et al,2000) allows model summaries to consist of sentences with variable ranking. For details see (Radev et al, 2000). Three judges have assessed the sentences in each cluster and have provided a score on a scale from 0 to 10 (i.e. utility judgement), expressing how important the sentence is for the topic of the cluster (Radev et al, 2000). However, this experiment differs from prior work in that we use judges to determine the relevance of sentences to sub-events rather than to evaluate summaries (Radev et al, 2000).  Following (Radev et al, 2000), we used relative utility as our metric. MEAD (Radev et al, 2000): MEAD is a centroid-based extractive summarizer that scores sentences based on sentence-level and inter-sentence features which indicate the quality of the sentence as a summary sentence. As in (Radev et al, 2000), in order to create an extract of a certain length, we simply extract the top scoring sentences that add up to that length. MEAD (Radev et al, 2000): MEAD is a centroid-based extractive summarizer that scores sentences based on sentence-level andinter-sentence features which indicate the quality of the sentence as a summary sentence. 3.1.3 Relative Utility Relative Utility (RU) (Radev et al, 2000) is tested on a large corpus for the first time in this project. This is in contrast with a method proposed by Radev (Radev et al, 2000), where the centroid of a cluster is selected as the representative one. A number of techniques for choosing the right sentences to extract have been proposed in the literature, ranging from word counts (Luhn, 1958), key phrases (Edmundson, 1969), naive Bayesian classification (Kupiec et al, 1995), lexical chains (Barzilay and Elhadad, 1997), topic signatures (Hovy and Lin, 1999) and cluster centroids (Radev et al, 2000). We use the Relative Utility (RU) method (Radev et al, 2000) to compare our various summaries. These summarizers have been found to produce quantitatively similar results, and both significantly outperform a baseline summarizer, which is the MEAD summarization framework with all options set to the default (Radev et al, 2000) .Both summarizers rely on information extraction from the corpus. The extractive approach is represented by MEAD*, which is adapted from the open source summarization framework MEAD (Radev et al., 2000). First, our method focuses on subject shift of the documents from the target event rather than the sets of documents from different events (Radev et al, 2000). Finally, MEAD is a widely used MDS and evaluation platform (Radev et al, 2000). We have implemented an extractive summarizer for educational science content, COGENT, based on MEAD version 3.11 (Radev et al, 2000). Radev et al (2000) use it in their MDS system MEAD.
Broad-Coverage Sense Disambiguation And Information Extraction With A Supersense Sequence Tagger In this paper we approach word sense disambiguation and information extraction as a unified tagging problem. The task consists of annotating text with the tagset defined by the 41 Wordnet supersense classes for nouns and verbs. Since the tagset is directly related to Wordnet synsets, the tagger returns partial word sense disambiguation. Furthermore, since the noun tags include the standard named entity detection classes – person, location, organization, time, etc. – the tagger, as a by-product, returns extended named entity information. We cast the problem of supersense tagging as a sequential labeling task and investigate it empirically with a discriminatively-trained Hidden Markov Model. Experimental evaluation on the main sense-annotated datasets available, i.e., Semcor and Senseval, shows considerable improvements over the best known “first-sense” baseline. Supersense tagging A WordNet-based supersense tagger (Ciaramita and Altun, 2006).  Columns 1 - 3 were predicted using the tagger of Ciaramita and Altun (2006). grained distinctions of WN (Hearst and Schutze,1993) (Peters et al, 1998) (Mihalcea and Moldovan, 2001) (Agirre et al, 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al, 1997) (Ciaramita and Johnson, 2003) (Villarejo et al, 2005) (Curran, 2005) (Ciaramita and Altun, 2006). In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al, 1997), (Ciaramita and Johnson, 2003), (Villarejo et al, 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006). Wherever applicable, we explore different syntactic and semantic representations of the textual content, e.g., extracting the dependency-based representation of the text or generalizing words to their WordNet supersenses (WNSS) (Ciaramita and Altun, 2006). Sentences were annotated with WNSS categories, using the tagger of Ciaramita and Altun (2006), which annotates text with a 46-label tag set. We used the implementation available from http: //sourceforge.net/projects/supersensetag, more details on this tagger can be found in (Ciaramita and Altun, 2006). This result is particularly interesting as a supersense tagger can easily provide a satisfactory accuracy (Ciaramita and Altun, 2006). This annotation was performed automatically using the SuperSense Tagger (Ciaramita and Altun, 2006) and includes 1183 named-entities and WordNet Super-Senses. We recommend mate-tools (Bjorkelund et al, 2009) and SuperSenseTagger (Ciaramita and Altun, 2006). We used Ciaramita and Altun's Su per Sense Tagger (Ciaramita and Altun, 2006) to tag the supersenses. This is the coarse lexicographic category label, elsewhere denoted supersense (Ciaramita and Altun, 2006), which is the terminology we use. we use the out puts of SuperSense Tagger (Ciaramita and Altun,2006), which is optimised for assigning the super senses described above, and can outperform a WNF style baseline on at least some datasets. We use SuperSenseTagger (Ciaramita and Altun,2006) as our NER tagger. SUPERSENSE LEARNER brings together under one system the features previously used in the SENSELEARNER (Mihalcea and Csomai, 2005) and the SUPERSENSE (Ciaramita and Altun, 2006) all-words word sense disambiguation systems. A detailed description of the features used and the tagger can be foundin (Ciaramita and Altun, 2006). In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al, 1997), (Ciaramita and Johnson, 2003), (Villarejo et al, 2005), (Curran, 2005) and (Ciaramita and Altun, 2006). WSD are those reported by (Ciaramita and Altun, 2006). Relations and POS tags are obtained using a dependency parser Tratz and Hovy (2011), supersense tags using sstlight Ciaramita and Altun (2006), and lemmas us 468.
A STATISTICAL APPROACH TO LANGUAGE TRANSLAT ION P. BROWN,  J. COCKE,  S. DEL I ,A  PIETRA,  V. DELLA P IETRA, F. JEL INEK,  R, MF, RCF, R, and P. ROOSSIN IBM Research Divis ion T.J. Watson  Research Center Depar tment  of Computer  Science P.O.  Consider, for example, the following sentence, taken from the Hansard corpus of the proceedings of the Canadian parliament [Brown et al 1988]: (1) They know full well that the companies held tax money aside for collection later on the b~sis that the government said it was going to collect it. Parallel corpora have received a lot of attention since the advent of statistical machine translation (Brown et al, 1988) where they serve as training material for the underlying alignment models. The most successful translation models that are found in the literature exploit finite-state machinery. The approach started with the so-called IBM models (Brown et al, 1988), implementing a set of elementary operations, such as movement, duplication and translation, that independently act on individual words in the source sentence. Developing a better TM is a fundamental issue for those applica tions. Researchers at IBM first described such a statistical TM in (Brown et al, 1988). Most algorithms for bilingual word alignment to date have been based on the probabilistic translation models first proposed by Brown et al (1988, 1990), especially Model I and Model 2. Brown et al (1988) suggested that MT can be statistically approximated to the transmission of information through a noisy channel. As mentioned above, the MDI2B model is closely related to the IBM2 model (Brown et al, 1988). In the field of eomputationa.1 linguistics, mutual information [Brown et al, 1988],  2 [Church and Hanks, 1990], or a likelihood ratio test [Dunning, 199a] are suggested. In general a statistical machine translation system is composed of three components: a language model, a translation model, and a decoder (Brown et al, 1988). The language model tells how probable a given sentence is in the source language, the translation model indicates how likely it is that a particular target sentence is a translation of a given source sentence, and the decoder is what actually takes a source sentence as input and produces its translation as output. This is the task of finding for a word in one language words of a similar meaning in a second language. The results of this can be used to aid manual construction of resources or directly aid translation. This task was first approached as a distributional similarity-like problem by Brown et al (1988).
Guided Learning for Bidirectional Sequence Classification In this paper, we propose guided learning, a new learning framework for bidirectional sequence classification. The tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single Perceptron like learning algorithm. We apply this novel learning algorithm to POS tagging. It obtains an error rate of 2.67% on the standard PTB test set, which represents 3.3% relative error reduction over the previous best result on the same data set, while using fewer features. For our POS tagging experiments, we used the Wall Street Journal in PTB III (Marcus et al, 1994) with the same data split as used in (Shen et al., 2007). In POS tagging, the previous best performance was reported by (Shen et al, 2007) as summarized in Table 7. Feature templates are shown in Table 3, which are based on those of Ratnaparkhi (1996) and Shen et al (2007). For the experimental evaluations we use the Bidirectional Tagger with Guided Learning presented in Shen et al (2007). For the implementation, we used bpos (Shen et al., 2007) for the POS tagging. For comparison, our best model, the PLMRF, achieved a 96.8% in-domain accuracy on sections 22-24 of the Penn Treebank, about 0.5% shy of a state-of-the-art in-domain system (Shen et al, 2007) with more sophisticated supervised learning. The idea of bidirectional parsing is related to the bidirectional sequential classification method described in (Shen et al, 2007). Similar to bidirectional labelling in (Shen et al, 2007), there are two learning tasking in this model. The learning algorithm for level-0 dependency is similar to the guided learning algorithm for labelling as described in (Shen et al, 2007). The only preprocessing step needed is POS tagging of the data, for which we used the system of Shen et al (2007). It is competitive to CRF in tagging accuracy but requires much less training time (Shen et al, 2007). We propose a new category of dependency parsing algorithms, inspired by (Shen et al, 2007): non directional easy-first parsing. Indeed, one major influence on our work is Shen et.al's bi-directional POS-tagging algorithm (Shen et al, 2007), which combines a perceptron learning procedure similar to our own with beam search to produce a state-of-the-art POStagger, which does not rely on left-to-right processing.  Note that Shen et al (2007) employ contextual features up to 5-gram which go beyond our local trigram window. (Shen et al, 2007) developed new algorithms based on the easiest-first strategy (Tsuruoka and Tsujii, 2005) and the perceptron algorithm. Shen et al, (2007) report an accuracy of 97.33% on the same data set using a perceptron-based bidirectional tagging model. Shen et al (2007) have further shown that better results (97.3 % accuracy) can be obtained using guided learning, a framework for bidirectional sequence classification, which integrates token classification and inference order selection into a single learning task and uses a perceptron-like (Collins and Roark, 2004) passive-aggressive classifier to make the easiest decisions first. We used the feature set defined in (Shen et al 2007), which includes the following: 1. 
Prototype-Driven Learning For Sequence Models investigate for primarily unsupervised sequence modeling. Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label. This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model. On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work. For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints. We also compare to semi-supervised learning and discuss the system’s error trends. Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. We report greedy one-to-one mapping accuracy (1-1) (Haghighi and Klein, 2006) and the information-theoretic score V measure (V-m), which also varies from 0 to 100% (Rosenberg and Hirschberg, 2007). For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate. Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006), Druck et al (2008)). Similarly, prototype-driven learning (PDL) (Haghighi and Klein, 2006) optimizes the joint marginal likelihood of data labeled with prototype input features for each label. We use the same feature processing as Haghighi and Klein (2006), with the addition of context features in a window of 3. The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al (2005) (74.4%), and matches the results of Mann and McCallum (2008) with GE with more accurate sampled label distributions and 10 labeled examples. Another interesting idea is to select some exemplars (Haghighi and Klein, 2006). (Haghighi and Klein, 2006) exploits prototype words (e.g., close, near, shoping for the NEIGHBORHOOD attribute) in an unsupervised setting. Haghighi and Klein (2006) ask the user to suggest a few prototypes (examples) for each class and use those as features. For comparison, Haghighi and Klein (2006) report an unsupervised baseline of 41.3%, and a best result of 80.5% from using hand-labeled prototypes and distributional similarity. In the same spirit, Christodoulopoulos et al (2010) used the output of a number of unsupervised PoS tagging methods to extract seeds for the prototype-driven model of Haghighi and Klein (2006). The approach of Christodoulopoulos et al. (2010) falls between pre-processing and data exploration, as the clusters of tokens produced are semi-automatically processed in order to produce seeds which were then used by the prototype-driven model of Haghighi and Klein (2006). Still, however, such techniques often require "seeds", or "prototypes" (c.f., (Haghighi and Klein, 2006)) which are used to prune search spaces or direct learners. Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly. For instance, the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model; the tagged words in IGT can be used to label the resulting clusters produced by the word clustering approach; the frequent and unambiguous words in the target lines can serve as prototype examples in the prototype-driven approach (Haghighi and Klein, 2006). In the monolingual setting, Smith and Eisner (2005) showed similarly that a POS induction model can be improved with spelling features (prefixes and suffixes of words), and Haghighi and Klein (2006) describe an MRF-based monolingual POS induction model that uses features. Monolingual MRF tag model (Haghighi and Klein, 2006) of automatically distorted training examples which are compactly represented in WFSTs. Specifically we use the features described in Haghighiand Klein (2006), namely initial-capital, contains hyphen, contains-digit and we add an extra feature contains-punctuation. We can clearly see that morphological features are helpful in both languages; however the extended features of Haghighi and Klein (2006) seem to help only on the English data.
Improving Statistical Machine Translation Using Word Sense Disambiguation We show for the first time that incorporatingthe predictions of a word sense disambigua tion system within a typical phrase-based statistical machine translation (SMT) model consistently improves translation qualityacross all three different IWSLT ChineseEnglish test sets, as well as producing sta tistically significant improvements on the larger NIST Chinese-English MT task? and moreover never hurts performance on any test set, according not only to BLEUbut to all eight most commonly used au tomatic evaluation metrics. Recent work has challenged the assumption that word sense disambiguation (WSD) systems areuseful for SMT. Yet SMT translation qual ity still obviously suffers from inaccurate lexical choice. In this paper, we addressthis problem by investigating a new strategy for integrating WSD into an SMT sys tem, that performs fully phrasal multi-worddisambiguation. Instead of directly incor porating a Senseval-style WSD system, weredefine the WSD task to match the ex act same phrasal translation disambiguation task faced by phrase-based SMT systems.Our results provide the first known empirical evidence that lexical semantics are in deed useful for SMT, despite claims to the contrary. ?This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants  Unlike a full blown machine translation task (Carpuat and Wu, 2007), annotators and systems are not required to translate the whole context but just the target word. (Carpuat and Wu, 2007) report an improvement in translation quality by incorporating a WSD system directly in a phrase-based translation system. The output of this model is incorporated into the machine translation system by providing the WSD probabilities for a phrase translation as extra features in a log-linear model (Carpuat and Wu, 2007). (Chan and Ng, 2007) introduce a system very similar to that of (Carpuat and Wu, 2007), but as applied to hierarchical phrase-based translation. Another WSD approach incorporating context-dependent phrasal translation lexicons is given in (Carpuatand Wu, 2007) and has been evaluated on several translation tasks. Second, instead of disambiguating phrase senses as in (Carpuat and Wu, 2007), we model word selection independently of the phrases used in the MT models. Carpuat and Wu (2007b) and Chan et al. (2007) showed improvents by integrating word-sense-disambiguation (WSD) system into a phrase-based (Koehn, 2004) and a hierarchical phrase-based (Chiang, 2005) SMT system, respectively.  Carpuat and Wu (2007) approached the issue as a Word Sense Disambiguation problem. In Chan and Ng (2007) a classifier exploits information such as local collocations, parts-of-speech or surrounding words to determine the lexical choice of target words, while Carpuat and Wu (2007) use rich context features based on position, syntax and local collocations to dynamically adapt the lexicons for each sentence and facilitate the choice of longer phrases. Unlike a full blown machine translation task (Carpuat and Wu, 2007), annotators and systems will not be required to translate the whole context but just the target word. As described in (Carpuat and Wu, 2007), the disambiguation model plays an important role in the machine translation task. In the study more closely related to our work, Carpuat and Wu (2007) propose a novel method to train a phrasal lexical disambiguation model to benefit translation candidates selection in machine translation. Current work in machine translation has shown that word sense disambiguation can play an important role by using the surrounding words as context to disambiguate terms (Carpuat and Wu, 2007) (Apidianaki, 2009). Another WSD approach incorporating context-dependent phrasal translation lexicons is presented by Carpuat and Wu (2007) and has been evaluated on several translation tasks. In Carpuat and Wu (2007), another state-of-the-art WSD engine (a combination of naive Bayes, maximum entropy, boosting and Kernel PCA models) is used to dynamically determine the score of a phrase pair under consideration and, thus, let the phrase selection adapt to the context of the sentence. The key insight here is that Word Sense Disambiguation and Machine Translation (MT) are highly intertwined tasks, as previously shown by Carpuat and Wu (2007) and Chan et al (2007), who successfully used sense information to boost state-of-the-art statistical MT. Carpuat and Wu (2007a, 2007b) and Chan et al (2007) have integrated word sense disambiguation (WSD) and phrase sense disambiguation (PSD) into SMT systems. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT.
Exploiting Parallel Texts For Word Sense Disambiguation: An Empirical Study A central problem of word sense disambiguation (WSD) is the lack of manually sense-tagged data required for supervised learning. In this paper, we evaluate an approach to automatically acquire sensetagged training data from English-Chinese parallel corpora, which are then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task. Our investigation reveals that this method of acquiring sense-tagged data is promising. On a subset of the most difficult SENSEVAL-2 nouns, the accuracy difference between the two approaches is only 14.0%, and the difference could narrow further to 6.5% if we disregard the advantage that manually sense-tagged data have in their sense coverage. Our analysis also highlights the importance of the issue of domain dependence in evaluating WSD programs. To tackle this problem, in one of our recent work (Ng et al, 2003), we had gathered training data from parallel texts and obtained encouraging results in our evaluation on the nouns of SENSEVAL-2 English lexical sample task (Kilgarriff, 2001). To gather training examples from these parallel texts, we used the approach we described in (Ng et al, 2003) and (Chan and Ng, 2005b). Other similar work includes that in (Ng et al, 2003), where a sense-annotated corpus was automatically generated from a parallel corpus. For example, Ng et al (2003) acquired sense examples using English-Chinese parallel corpora, which were manually or automatically aligned at sentence level and then word-aligned using software. There is a growing number of methods that use data available in one language to build text processing tools for another language, for diverse tasks such as word sense disambiguation (Ng et al, 2003), syntactic parsing (Hwa et al, 2005), information retrieval (Monz and Dorr, 2005), subjectivity analysis (Mihalcea et al, 2007), and others. For the translation of ambiguous English words Ng et al (2003) made use of the fact that the various senses are often translated differently. Ng et al (2003) show that it is possible to use automatically word aligned parallel corpora to train accurate supervised WSD models. Similarly, (Ng et al, 2003) report a research study which uses an English-Chinese parallel corpus in order to extract sense-tagged training data.  Similarly, Ng et al (2003) employ English Chinese parallel word aligned corpora to identify a repository of senses for English. For example, Ng et al (2003) proposed to train a classifier on sense examples acquired from word-aligned English-Chinese parallel corpora. Ng et al (2003) address word sense disambiguation by manually annotating WordNet senses with their translation in the target language (Chinese), and then automatically extracting labeled examples for word sense disambiguation by applying the IBM Models to a bilingual corpus. Moreover, some studies present multilingual WSD systems that attain state-of-the-art performance in all-words disambiguation (Ng et al, 2003). For instance, Ng et al (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. Ng et al (2003) extend this approach further and demonstrate that it is feasible for large scale WSD. Unlike Ng et al (2003) our algorithm works on monolingual corpora, which are much more abundant than parallel ones, and is fully automatic. To gather examples from these parallel corpora, we followed the approach in (Ng et al, 2003). For instance, Ng et al (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. To gather examples from parallel corpora, we followed the approach in (Ng et al, 2003). As described in (Ng et al, 2003), when several senses of an English word are translated by the same Chinese word, we can collapse these senses to obtain a coarser-grained, lumped sense inventory.
Moses: Open Source Toolkit for Statistical Machine Translation where many sources of information about the project can be found. Moses was the subject of this year’s Johns Hopkins University Workshop on Machine Translation (Koehn et al. 2006). The decoder is the core component of Moses. To minimize the learning curve for many researchers, the decoder was developed as a drop-in replacement for Pharaoh, the popular phrase-based decoder. In order for the toolkit to be adopted by the community, and to make it easy for others to contribute to the project, we kept to the following principles when developing the decoder: • Accessibility • Easy to Maintain • Flexibility • Easy for distributed team development • Portability It was developed in C++ for efficiency and followed modular, object-oriented design. 3 Factored Translation Model Non-factored SMT typically deals only with the surface form of words and has one phrase table, as shown in Figure 1. Translate: i am buying you a green cat vous achète un vert using phrase dictionary: i am buying vous green chat Figure 1. Non-factored translation In factored translation models, the surface forms may be augmented with different factors, such as POS tags or lemma. This creates a factored representation of each word, Figure 2. ⎛⎞⎛ ⎞⎛ vous achet ⎞⎛ chat ⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟ ⎜⎟⎜ ⎟⎜ PRO PRO VB ART NN ⎟⎜ ⎟⎜ ⎟ ⎜⎟⎜ ⎟⎜ je vous acheter ⎟⎜ chat ⎟⎜ ⎟⎜ ⎟⎜ ⎝⎠⎝ ⎠⎝ st present masc / ⎠⎝ ⎛ ⎞⎛ buy ⎞⎛ ⎞⎛ ⎞ you a cat ⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟ ⎜⎟⎜ VB ⎟⎜ ⎟ PRO ART NN ⎜ ⎟⎜ tobuy ⎟⎜ ⎟⎜ ⎟ you a cat ⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟⎜ ⎟ ⎝⎠⎝ Figure 2. Factored translation Mapping of source phrases to target phrases may be decomposed into several steps. Decomposition of the decoding process into various steps means that different factors can be modeled separately. Modeling factors in isolation allows for flexibility in their application. It can also increase accuracy and reduce sparsity by minimizing the number dependencies for each step. For example, we can decompose translating from surface forms to surface forms and lemma, as shown in Figure 3. je achète you a un a une vert cat 178 Figure 3. Example of graph of decoding steps By allowing the graph to be user definable, we can experiment to find the optimum configuration for a given language pair and available data. The factors on the source sentence are considered fixed, therefore, there is no decoding step which create source factors from other source factors. However, Moses can have ambiguous input in the form of confusion networks. This input type has been used successfully for speech to text translation (Shen et al. 2006). Every factor on the target language can have its own language model. Since many factors, like lemmas and POS tags, are less sparse than surface forms, it is possible to create a higher order language models for these factors. This may encourage more syntactically correct output. In Figure 3 we apply two language models, indicated by the shaded arrows, one over the words and another over the lemmas. Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006). 4 Confusion Network Decoding Machine translation input currently takes the form of simple sequences of words. However, there are increasing demands to integrate machine translation technology into larger information processing systems with upstream NLP/speech processing tools (such as named entity recognizers, speech recognizers, morphological analyzers, etc.). These upstream processes tend to generate multiple, erroneous hypotheses with varying confidence. Current MT systems are designed to process only one input hypothesis, making them vulnerable to errors in the input. In experiments with confusion networks, we have focused so far on the speech translation case, where the input is generated by a speech recognizer. Namely, our goal is to improve performance of spoken language translation by better integrating speech recognition and machine translation models. Translation from speech input is considered more difficult than translation from text for several reasons. Spoken language has many styles and genres, such as, formal read speech, unplanned speeches, interviews, spontaneous conversations; it produces less controlled language, presenting more relaxed syntax and spontaneous speech phenomena. Finally, translation of spoken language is prone to speech recognition errors, which can possibly corrupt the syntax and the meaning of the input. There is also empirical evidence that better translations can be obtained from transcriptions of the speech recognizer which resulted in lower scores. This suggests that improvements can be achieved by applying machine translation on a large set of transcription hypotheses generated by the speech recognizers and by combining scores of acoustic models, language models, and translation models. Recently, approaches have been proposed for improving translation quality through the processing of multiple input hypotheses. We have implemented in Moses confusion network decoding as discussed in (Bertoldi and Federico 2005), and developed a simpler translation model and a more efficient implementation of the search algorithm. Remarkably, the confusion network decoder resulted in an extension of the standard text decoder. 5 Efficient Data Structures for Translation Model and Language Models With the availability of ever-increasing amounts of training data, it has become a challenge for machine translation systems to cope with the resulting strain on computational resources. Instead of simply buying larger machines with, say, 12 GB of main memory, the implementation of more efficient data structures in Moses makes it possible to exploit larger data resources with limited hardware infrastructure. A phrase translation table easily takes up gigabytes of disk space, but for the translation of a single sentence only a tiny fraction of this table is needed. Moses implements an efficient representation of the phrase translation table. Its key properare a tree for source words and demand i.e. only the fraction of the phrase table that is needed to translate a sentence is loaded into the working memory of the decoder. 179 For the Chinese-English NIST task, the memory requirement of the phrase table is reduced from 1.7 gigabytes to less than 20 mega bytes, with no loss in translation quality and speed (Zens and Ney 2007). The other large data resource for statistical machine translation is the language model. Almost unlimited text resources can be collected from the Internet and used as training data for language modeling. This results in language models that are too large to easily fit into memory. The Moses system implements a data structure for language models that is more efficient than the canonical SRILM (Stolcke 2002) implementation used in most systems. The language model on disk is also converted into this binary format, resulting in a minimal loading time during start-up of the decoder. An even more compact representation of the model is the result of the the word prediction and back-off probabilities of the language model. Instead of representing these probabilities with 4 byte or 8 byte floats, they are sorted into bins, resulting in (typically) 256 bins which can be referenced with a single 1 byte index. This quantized language model, albeit being less accurate, has only minimal impact on translation performance (Federico and Bertoldi 2006). 6 Conclusion and Future Work This paper has presented a suite of open-source tools which we believe will be of value to the MT research community. We have also described a new SMT decoder which can incorporate some linguistic features in a consistent and flexible framework. This new direction in research opens up many possibilities and issues that require further research and experimentation. Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007). This could be addressed by the so-called factored phrase-based model as implemented in the Moses decoder (Koehn et al, 2007). For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. Subsequently, we extracted the bi-lingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Finally, we extract the semantic phrase table from the augmented aligned corpora using the Moses toolkit (Koehn et al, 2007). Bidirectional lexical scores for all rules with lexical items, calculated from a unigram lexicon over Viterbi-aligned word pairs as in the Moses decoder (Koehn et al, 2007). The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al, 2007). Our baseline system is phrase-based Moses (Koehn et al, 2007) with feature weights trained using MERT. We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec. For our experiments we use the phrase-based machine translation techniques described in (Koehn, 2004) and (Koehn et al, 2007), integrating our models within a log-linear framework (Och and Ney, 2002). Each instance of the decoder is a standard phrase based machine translation decoder that operates according to the same principles as the publicly available PHARAOH (Koehn, 2004) and MOSES (Koehn et al, 2007) SMT decoders. (Koehn et al, 2007) was also included in the scores for partial hypothesis during the decoding. An automatic extraction of bilingual MWEs is carried out by Ren et al (2009), using a log likelihood ratio based hierarchical reducing algorithm to investigate the usefulness of bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al, 2007). The effectiveness of the MWE-aligned and chunk aligned parallel corpus is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phrase-extraction heuristics described in (Koehn et al, 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model trained using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) and the Moses decoder (Koehn et al, 2007). We then trained the Moses phrase-based system (Koehn et al, 2007) on the segmented and marked text. In all the experiments conducted in this paper, we used the Moses5 phrase-based translation system (Koehn et al, 2007), 2008 version. Backward 2-gram and 3-gram source and target log probabilities: as proposed by Duchateau et al (2002) Log probability of target segments on 5-gram MT-output-based LM: using MOSES (Koehn et al, 2007) trained on the provided parallel corpus, we translated the English side of this corpus into Spanish, assuming that the MT output contains mistakes. When an ambiguous connective is explicitly translated by another connective, the incorrect rendering of its sense can lead to erroneous translations, as in the second and third examples in Table 1, which are translated by the Moses SMT decoder (Koehn et al., 2007) trained on the Europarl corpus. We used two decoders, Matrax (Simardetal., 2005) and Moses (Koehn et al, 2007), both standard statistical phrase based decoders. In this paper we describe the speed improvements to the Moses decoder (Koehn et al,2007), as well as a novel framework to specify reordering constraints with XML markup, which we tested with punctuation-based constraints. on parallel sentences from the Prague Czech-English Dependency Treebank (PCEDT) 2.0 (Bojar et al, 2012), comparing the gold standard Czech translations to the output of an SMT system (Koehn et al, 2007) and estimating the Maximum Likelihood probabilities of errors for each part-of-speech tag.
Revisiting Readability: A Unified Framework for Predicting Text Quality We combine lexical, syntactic, and discourse features to produce a highly predictive model of human readers’ judgments of text readability. This is the first study to take into account such a variety of linguistic factors and the first to empirically demonstrate that discourse relations are strongly associated with the perceived quality of text. We show that various surface metrics generally expected to be related to readability are not very good predictors of readability judgments in our Wall Street Journal corpus. We also establish that readability predictors behave differently depending on the task: predicting text readability or ranking the readability. Our experiments indicate that discourse relations are the one class of features that exhibits robustness across these two tasks. In any case, this aspect of readability may be worth further investigation (Pitler and Nenkova, 2008). In another related work, (Pitler and Nenkova, 2008) investigated the impact of certain surface linguistic features, syntactic, entity coherence and discourse features on the readability of Wall Street Journal (WSJ) Corpus. We use the syntactic features used in (Pitler and Nenkova, 2008) as baselines for our experiments on grammaticality in this paper. Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al, 2003).  When readability is targeted towards adult competent language users a more prominent role is played by discourse features (Pitler and Nenkova, 2008).  Pitler and Nenkova (2008) used the Penn Discourse Treebank (Prasad et al, 2008) to examine discourse relations. Pitler and Nenkova (2008) used the same features to evaluate how well a text is written. This approach was subsequently pursued by Pitler and Nenkova (2008) in their readability study. Measures of cohesion have also been used in a variety of NLP tasks such as measuring text readability (e.g. (Pitler and Nenkova, 2008)), measuring stylistic differences in text (Mccarthy et al, 2006), and for topic segmentation in tutorial dialog (Olney and Cai, 2005). To answer this question, we performed an additional experiment on Wall Street Journal articles from the Penn Treebank that were previously used in experiments for assessing overall text quality (Pitler and Nenkova, 2008). Five annotators had previously assess the overall text quality of each article on a scale from 1 to 5 (Pitler and Nenkova, 2008). Discourse apects and language model features that have been extensively studied in prior work are indeed much more indicative of overall text quality (Pitler and Nenkova, 2008). Pitler and Nenkova (2008) and Kate et al (2010), for example, average out results collected from different readers. Nevertheless, recent works in the NLP community investigating the impact of entity grids (Barzilay and Lapata, 2008) or of discourse relations (Pitler and Nenkova, 2008) on text coherence and readability go in the same direction as research on Coh-Metrix, in that they aim at identifying the linguistic features that best express readability at syntactic, semantic and discourse level. Syntactic features from PCFG parse trees have also been used for gender attribution (Sarawgi et al 2011), genre identification (Stamatatos et al 2000), native language identification (Wong and Dras, 2011) and readability assessment (Pitler and Nenkova, 2008). These methods identify regularities in words (Barzilay and Lee, 2004), entity coreference (Barzi lay and Lapata, 2008) and discourse relations (Pitler and Nenkova, 2008) from a large collection of articles and use these patterns to predict the coherence. An exception to this trend is the work of Pitler and Nenkova (2008) who reported non-significant correlation for the mean number of words per sentence (r= 0.1637, p= 0.3874) and the mean number of characters per word (r= 0.0859, p= 0.6519).
A Maximum Entropy Approach To Natural Language Processing The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing. The approach made use of a maximum entropy model (Berger et al, 1996) formulated from frequency information for various combinations of the observed features. This is not a issue for the MaxEnt classifier as it can deal with arbitrary overlapping features (Berger et al, 1996). We performed feature selection by incrementally growing a log-linear model with order0 features f (x,yt) using a forward feature selection procedure similar to (Berger et al, 1996). For the ACL data, where response is the binary cited-or-not variable we use logistic regression, often referred to as a "maximum entropy" model (Berger et al., 1996) or a log-linear model. Berger et al (1996) and Jelinek (1997) make this same point and arrive at the same estimator, albeit through a maximum entropy argument. A common choice for the local probabilistic classifier is maximum entropy classifiers (Berger et al, 1996). For local classifiers, we used a maximum entropy model which is a common choice for incorporating various types of features for classification problems in natural language processing (Berger et al, 1996). The log-linear model (LLM), or also known as maximum-entropy model (Berger et al, 1996), is a linear classifier widely used in the NLP literature. We consider three learning algorithms, namely, the C4.5 decision tree induction system (Quinlan, 1993), the RIPPER rule learning algorithm (Cohen, 1995), and maximum entropy classification (Berger et al, 1996). Maximum entropy classification (MaxEnt) is a technique which has proven effective in a number of natural language processing applications (Berger et al, 1996). Berger et al (1996) propose to estimate the weight wi of the candidate feature fi, while assuming that the weights of features in S stay constant. the gain-informed selection method proposed by Berger et al (1996) still recalculates the weights of all the candidate features during every cycle. The maximum entropy approach (Berger et al, 1996) is known to be well suited to solve the classification problem. The NLU uses a maximum-entropy model (Berger et al, 1996) to classify utterances as one of the user SAs using shallow text features. we use the general technique of choosing the maximum entropy (maxent) distribution that estimates the average of each feature over the training data (Berger et al, 1996). Uses Maximum Entropy (Berger et al, 1996) classification, trained on JNLPBA (Kim et al, 2004) (NER). An especially well-founded framework for doing this is maximum entropy (Berger et al, 1996). It can be proven that the probability distribution p satisfying the above assumption is the one with the highest entropy, is unique and has the following exponential form (Berger et al 1996). Since its introduction to the Natural Language Processing (NLP) community (Berger et al, 1996), ME-based classifiers have been shown to be effective in various NLP tasks. Maximum entropy (ME) models have been used in bilingual sense disambiguation, word reordering, and sentence segmentation (Berger et al, 1996),.
Automatic Evaluation And Uniform Filter Cascades For Inducing N-Best Translation Lexicons This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources. The knowledge sources are cast as filters, so that any subset of them can be cascaded in a uniform framework. A new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades. The best filter cascades improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance. Drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used. This makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable. Moreover, three of the four filters prove useful even when used with large training corpora. We employ several orthographic metrics widely used in this research area: the edit distance (Levenshtein, 1965), the longest common subsequence ratio (Melamed, 1995) and the XDice metric (Brew and McKelvie, 1996). Accuracy and coverage roughly correspond to Melamed's precision and percent correct respectively (Melamed, 1995). Estimated clues are derived from the parallel data using, for example, measures of co-occurrence (e.g. the Dice coefficient (Smadja et al, 1996)) ,statistical alignment models (e.g. IBM models from statistical machine translation (Brown et al, 1993)), or string similarity measures (e.g. the longest common sub-sequence ratio (Melamed,1995)). This measurement is called longest common subsequence ratio [Melamed, 1995]. Similarity is normalised by dividing the length of the common subsequence by the length of the longer string (Melamed, 1995). Melamed (1995) used the ratio (LCSR) between the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: LCSR (s1 ,s2)= |LCS (s1 ,s2) |max (|s1|, |s2|) where LCS (s1 ,s2) is the longest common subsequence of s1 and s2, and |s| is the length of s. Following Nakov et al (2007), we combined the LCSR similarity measure with competitive linking (Melamed, 2000) in order to extract potential cog 76nates from the training bi-text. We used the Longest Common Subsequence Ratio (LCSR) to measure similarity (Melamed, 1995). The aligner linked two words to each other only if neither of them was on the function word list and their longest common subsequence ratio (Melamed, 1995) was at least 0.75. recognition on the bag-of-words translation task was measured directly, using Bitext-Based Lexicon Evaluation (BIBLE: Melamed, 1995). 104 kind of BIBLE evaluation has been estimated at 62% precision and 60% recall (Melamed, 1995). Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). Melamed (1995) used the ratio (LCSR) between the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them. Following Melamed (1995), we measured the orthographic similarity using the longest common subsequence ratio (LCSR), defined as follows: LCSR (s 1, s 2)= |LCS (s 1, s 2)| max (|s 1|, |s 2|) where LCS (s 1, s 2) is the longest common subsequence of s 1 and s 2, and |s| is the length of s. We retained as likely cognates all pairs for which LCSR was 0.58 or higher; that value was found by Kondrak et al (2003) to be optimal for a number of language pairs in the Europarl corpus. Al-Onaizan et al (1999) extracted such likely cognates for Czech-English using one of the variations of LCSR (Melamed,1995) described in (Tiedemann, 1999) as a similarity measure.
Message Unders tand ing  Conference  - 6: A Br ie f  H is tory Ralph Grishman Dept.  In the case of English, partially motivated by Message Understanding Conferences (MUCs) (Grishman and Sundheim, 1996), a number of coreference resolution methods have been proposed. A condition of attending the MUC workshops was participation in a required evaluation (bakeoff) task of filling slots in templates about events, and began (after an exploratory MUC-1 in 1987) with MUC-2 in 1989, followed by MUC-3 (1991), MUC-4 (1992), MUC-5 (1993) and MUC 6 (1995) (Grishman and Sundheim, 1996). Discourse references have been the subject of attention in both the Message Understanding Conference (Grishman and Sundheim, 1996) and the Automatic Content Extraction program (Strassel et al., 2008). There have been a very large number of NE tagger implementations since this task was introduced at MUC-6 (Grishman and Sundheim, 1996). The automated construction of semantically typed lexicons (terms classified into their appropriate semantic class) from unstructured text is of great importance for various kinds of information extraction (Grishman and Sundheim, 1996), question answering (Moldovan et al, 1999), and ontology population (Suchanek et al, 2007). The topic of the sixth MUC (MUC-6) was management succession events (Grishman and Sundheim, 1996). Figure 2 is a simplified event from the the MUC-6 evaluation similar to one described by Grishman and Sundheim (1996). As demonstrated by prior work (Grishman and Sundheim, 1996), grammar-based IE systems can be effective in many scenarios. When it was introduced, in the 6th Message Understanding Conference (Grishman and Sundheim, 1996), the named entity recognition task comprised three entity identification and labeling subtasks: ENAMEX (proper names and acronyms designating persons, locations, and organizations), TIMEX (absolute temporal terms) and NUMEX (numeric expressions, monetary expressions, and percentages). The NER task was introduced with the 6th Message UnderstandingConference (MUC) in 1995 (Grishman and Sundheim, 1996). The problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the MUC 6 Evaluation (Grishman and Sundheim, 1996).  In a previous attempt to define predicate-argument structure, Semeval, the effort was abandoned because so many constructs would require detailed attention and resolution, and because most information-extraction systems did not generate full predicate-argument structures (most likely because the task did not require it) (Grishmanand Sundheim, 1996). Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003).  We used 6 categories of entity types, which are the major categories defined in the MUC (Grishman and Sundheim 1996) or ACE project (ACE homepage). For illustration purposes, we extended it here by MUC (Grishman and Sundheim, 1996) entity types such as Person, Organization, etc. Diversity IE traditionally targets a selected event type (Grishman and Sundheim, 1996).
Guiding Semi-Supervision with Constraint-Driven Learning Over the last few years, two of the main research directions in machine learning of natural language processing have been the study of semi-supervised learning algorithms as a way to train classifiers when the labeled data is scarce, and the study of ways to exploit knowledge and global information in structured learning tasks. In this paper, we suggest a method for incorporating domain knowledge in semi-supervised learning algorithms. Our novel framework unifies can exploit several kinds of specific The experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks. We achieve competitive performance in comparison to alternate model families, in particular generative models such as MRFs trained with EM (Haghighi and Klein, 2006) and HMMs trained with soft constraints (Chang et al, 2007). Another recent method that has been proposed for training sequence models with constraints is Chang et al (2007).  Most constraints that prove useful for SRL (Chang et al., 2007) also require customization when applied to a new language, and some rely on language specific resources, such as a valency lexicon. Constraint driven learning (CoDL) was first introduced in Chang et al [2007], and has been used also in Chang et al [2008].  Likewise, Chang et al (2007) use constraints at multiple levels, such as sentence-level constraints to specify field boundaries and global constraints to ensure relation-level consistency. Chang et al (2007) use a set of domain specific rules as automatic implicit feedback for training information extraction system. We compare our CRF model integrated with VE with two state-of-the-art models, i.e., constraint driven learning (Chang et al, 2007) and generalized expectation criteria (Mann and McCallum, 2008). Constraint-driven learning (Chang et al, 2007) expresses several kinds of constraints in a unified form.  (Chang et al 2007) incorporates domain specific constraints in semi-supervised learning. The learning algorithm in Figure 2 is an instance of augmented-loss training (Hall et al, 2011) which is closely related to the constraint driven learning algorithms of Chang et al (2007). Note that the objective function in Equation 5, if written in the additive form, leads to a cost function reminiscent of the one used in constraint-driven learning algorithm (CoDL) (Chang et al, 2007) (and similarly, posterior regularization (Ganchev et al, 2010), which we will discuss later at Section 6). Constraint-driven learning (CoDL) (Chang et al, 2007) and posterior regularization (PR) (Ganchev et al., 2010) are both primarily semi-supervised models. Most semi-supervised learning algorithms rely on marginals (GE, Mann and McCallum, 2008) or MAP assignments (CODL, Chang et al, 2007). corresponds to constraint satisfaction weights used in (Chang et al, 2007). Chang et al propose constraint-driven learning (CODL, Chang et al, 2007) which can be interpreted as a variation of self-training: Instances are selected for supervision based not only on the model's prediction, but also on their consistency with a set of user-defined constraints. We use the same token label constraints as Chang et al (2007). We also report supervised results from (Chang et al, 2007) and SampleRank.
A Critique And Improvement Of An Evaluation Metric For Text Segmentation metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms. However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size dis- We propose a simple modification to the that remedies these problems. This new metric—called WindowDiff—moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text. Since it has been argued in (Pevzner and Hearst, 2002) that Pk has some weaknesses, we also include results according to the WindowDiff (WD) metric (which is described in the same work). To select an automatic segmenter for a particular task, a variety of segmentation evaluation metrics have been proposed, including Pk (Beeferman and Berger, 1999, pp. 198-200), WindowDiff (WD; Pevzner and Hearst 2002, p. 10), and most recently Segmentation Similarity (S; Fournier and Inkpen 2012, p. 154-156). Pevzner and Hearst (2002, pp. 3-4) explain Pk well: a window of size k — where k is half of the mean manual segmentation length — is slid across both automatic and manual segmentations. Pevzner and Hearst (2002, pp. 5-10) identified that Pk: i) penalizes false negatives (FNs)2 more than false positives (FPs); ii) does not penalize full misses within k units of a reference boundary; iii) penalize near misses too harshly in some situations; and iv) is sensitive to internal segment size variance. To solve Pk's issues, Pevzner and Hearst (2002, pp. 10) proposed a modification referred to as WindowDiff (WD). Hence the standard Pk (Beeferman et al, 1997) and WinDiff (Pevzner and Hearst, 2002) measures for text segmentation are not so suitable for our task. For the TDT data we use the error metric pk (Beeferman et al, 1999) and WindowDiff (Pevzner and Hearst, 2002) which are implemented in the LCseg toolkit. Precision is the proportion of boundaries chosen that agree with a reference segmentation, and recall is the proportion of boundaries chosen that agree with a reference segmentation out of all boundaries in the reference and hypothesis (Pevzner and Hearst, 2002, p. 3). To attempt to mitigate the shortcomings of Pk, Pevzner and Hearst (2002, p. 10) proposed a modified metric which changed how penalties were counted, named WindowDiff (WD). When Pevzner and Hearst (2002) proposed WD, they demonstrated that it was not as sensitive as Pk to variations in the size of segments inside a segmentation. To show this, they simulated how WD performs upon a segmentation comprised of 1000 segments with four different uniformly distributed ranges of internal segment sizes (keeping the mean at approximately 25 units) in comparison to a hypothesis segmentation with errors (false positives, false negatives, and both) uniformly distributed within segments (Pevzner and Hearst, 2002, pp. 11-12). We also give the value of WindowDiff (WD), a variant of Pk proposed in (Pevzner and Hearst, 2002) that corrects some of its insufficiencies. In these tables, we report the F-measure of identifying the precise location of a story boundary as well as three metrics designed specifically for this type of segmentation task: the pk metric (Beeferman et al, 1999), WindowDiff (Pevzner and Hearst, 2002) and Cseg (Pseg= 0.3) (Doddington, 1998). As a last measure for segmentation quality we used WindowDiff (Pevzner and Hearst, 2002), which only evaluates segment boundaries not the labels assigned to them. We assessed segmentation performance using the Pk and WindowDiff (WD) error measures proposed by (Beeferman et al, 1999) and (Pevzner and Hearst, 2002) respectively. Pevzner and Hearst (2002) highlighted several problems of the Pk metric. Pevzner and Hearst (2002) propose the alternative metric called WindowDiff. However, unlike Pk and Pk, WindowDiff takes into account how many boundaries fall within the window and is penalizing in 'how many discrepancies occur between the reference and the system results' rather than 'determining how often two units of text are incorrectly labeled as being in different segments' (Pevzner and Hearst, 2002). Another issue regarding WindowDiff is that it is not clear 'how does one interpret the values produced by the metric' (Pevzner and Hearst, 2002). To measure the quality of segmentation of the lecture transcript, we use two standard metrics, Pk (Beeferman et al, 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002), but both metrics disregard the alignment links (i.e. the topic labels).
Generating Natural Language Summaries From Multiple On-Line Sources We present a methodology for summarization of news about current events in the form of briefings that include appropriate background (historical) information. The system that we developed, SUMMONS, uses the output of systems developed for the DARPA Message Understanding Conferences to generate summaries of multiple documents on the same or related events, presenting similarities and differences, contradictions, and generalizations among sources of information. We describe the various components of the system, showing how information from multiple articles is combined, organized into a paragraph, and finally, realized as English sentences. A feature of our work is the extraction of descriptions of entities such as people and places for reuse to enhance a briefing.  For examples of work in producing abstract-like summaries, see Radev and McKeown (1998), which combines work in information extraction and natural language processing. Radev and McKeown (1998) point out when summarizing interesting news events from multiple sources, one can expect reports with contradictory and redundant information. Notable systems are SUMMONS (Radev and McKeown, 1998). While template-based representations have been proposed for information merging in the past (Radev and McKeown, 1998), they considered only domain-specific scenarios. the main previous body of work on biographical summarization is that of (Radev and McKeown 1998). This is particularly problematic in the case of multi-document summarization, where sentences extracted from related documents are very likely to express similar information in different ways (Radev and McKeown, 1998). Existing work in abstractive summarization has been quite limited and can be categorized into two categories: (1) approaches using prior knowledge (Radev and McKeown, 1998). Since data producing these summaries can be sourced in different documents, summary fusion techniques as proposed in (Radev and McKeown, 1998) can be employed. Since (Radev and McKeown, 1998) describes the summary fusion mechanisms, Class 3 of questions can be reduced in this paper to Class 2, which deals with the processing of the template. (Radev and McKeown, 1998) and (Harabagiu and Lacatusu, 2004) define agreement (when two sources report the same information), addition (when a second source reports additional information), contradiction (when two sources report conflicting information). Recently, advanced QA systems defined relationships (equivalence, contradiction, ...) between Web page extracts or texts containing possible answers in order to combine them and to produce a single answer (Radev and McKeown, 1998), (Harabagiu and Lacatusu, 2004), (Webber et al., 2002). These include comparing templates filled in by extracting information using specialized, domain specific knowledge sources from the document, and then generating natural language summaries from the templates (Radev and McKeown, 1998). Similarly, Radev and McKeown (1998) used IE combined with Natural Language Generation (NLG) in their SUMMON system.
Adding Noun Phrase Structure to the Penn Treebank The Penn Treebank does not annotate base noun phrases committing only to flat structures that ignore the of English This means that tools trained on Treebank data cannot the correct internal structure of This paper details the process of adding gold-standard bracketing within each noun phrase in the Penn Treebank. We then examine the consistency and reliability of our annotations. Finally, we use resource to determine using several statistical approaches, thus demonstrating the utility of the corpus. This adds detail to the Penn Treebank that necessary for many For WSJ parsing, we use the standard train (02-21) / dev (22) / test (23) split and apply the NP bracketing patch by Vadas and Curran (2007). We apply an automatic conversion process using the gold-standard NP data annotated by Vadas and Curran (2007a). Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing.  The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below: (NP (NML (NN lung) (NN cancer)) (NNS deaths)). PropBank (Palmer et al, 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. This section describes the process of converting the Vadas and Curran (2007a) data to CCG derivations. This simple heuristic captures NP structure not explicitly annotated by Vadas and Curran (2007a). Vadas and Curran (2007a) describe using NE tags during the annotation process, suggesting that NER based features will be helpful in a statistical model. Vadas and Curran (2007a) experienced a similar drop in performance on Penn Tree bank data, and noted that the F-score for NML and JJP brackets was about 20% lower than the overall figure. Recent annotations by Vadas and Curran (2007a) added NP structure to the PTB.  Our training and testing data are derived from recent annotations by Vadas and Curran (2007a). Vadas and Curran (2007a) annotated NP-internal structure by adding annotations whenever there is a left-bracketing.    We use Vadas and Curran (2007a)'s annotations (Section 3) to create training, development and testing data for base NPs, using standard splits of the Penn Treebank (Table 1). Due to the annotation and work of Vadas and Curran (2007a; 2007b; 2008), we are now able to create Natural Language Processing (NLP) systems that take advantage of the internal structure of noun phrases in the Penn Treebank. Vadas' internal noun phrase structure has been used in previous work on constituent parsing using Collins parser (Vadas and Curran, 2007c), but has yet to be analyzed for its effects on dependency parsing.
Attention Intentions And The Structure Of Discourse In this paper we explore a new theory of discourse structure that stresses the role of purpose and processing in discourse. In this theory, discourse structure is composed of three separate but interrelated components: the structure of the sequence of utterances (called the linguistic structure), a structure of purposes (called the intentional structure), and the state of focus of attention (called the attentional state). The linguistic structure consists of segments of the discourse into which the utterances naturally aggregate. The intentional structure captures the discourse-relevant purposes, expressed in each of the linguistic segments as well as relationships among them. The attentional state is an abstraction of the focus of attention of the participants as the discourse unfolds. The attentional state, being dynamic, records the objects, properties, and relations that are salient at each point of the discourse. The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases, referring expressions, and interruptions. The theory of attention, intention, and aggregation of utterances is illustrated in the paper with a number of example discourses. Various properties of discourse are described, and explanations for the behavior of cue phrases, referring expressions, and interruptions are explored. This theory provides a framework for describing the processing of utterances in a discourse. Discourse processing requires recognizing how the utterances of the discourse aggregate into segments, recognizing the intentions expressed in the discourse and the relationships among intentions, and tracking the discourse through the operation of the mechanisms associated with attentional state. This processing description specifies in these recognition tasks the role of information from the discourse and from the participants' knowledge of the domain. As a discourse progresses, an adequate discourse model must represent the relevant entities, and the relationships between them (Grosz and Sidner, 1986), A speaker may then felicitously refer anaphorically to an object (subject to focusing or centering constraints (Grosz et al, 1983, Sidner 1981, 1983, Brennan et al 1987)) if there is an existing DE representing it, or if a corresponding DE may be directly inferred from an existing DE. It was reported that the lexical chains closely correlated to the intentional structure (Grosz and Sidner, 1986) of the texts, where the start and end of chains coincided with the intention ranges. The most well-known of work from this period is that of Mann and Thompson (1988), Grosz and Sidner (1986b), Moore and Moser (1996), Polanyi and van den Berg (1996), and Asher and Lascarides (2003). That is, as Grosz and Sidner (1986b) argued several years ago, the sentences may only be related by their communicative intentions one sentence intended to draw the reader's attention to the specific error that was made (so that the reader knows what was mis-stated), the other intended to correct it. This belief that different aspects of discourse would be related, is what led Grosz and Sidner (1986b) to propose a theory that linked what they called the intentional structure of discourse, with its linguistic structure and with the reader or listener's cognitive attentional structure. As Grosz and Sidner (Grosz and Sidner, 1986) pointed out, cue phrases such as now and well serve to indicate a topic change. Indeed, the COLLAGEN architecture, like that of the Queen's Communicator, manages discourse using a focus stack, a classical idea in the theory of discourse structure (Grosz and Sidner, 1986). This structure, in turn, identifies domains for interpretation; many systems for anaphora resolution rely on some notion of locality (Grosz and Sidner, 1986). Examples of such models and their implementation are the information state-update approach (an implemented system is described in (Larsson, 2002)), or more linguistically oriented approaches like the adjacency-pair models or intentional models such as GROSZ and SIDNER's (see (Grosz and Sidner, 1986)). DAR presupposes the discourse structure described by Grosz and Sidner (1986). In order to cover coreference over discourse segments the centering model was extended by a stack mechanism (Grosz and Sidner, 1986). Since a more general theory to referring expressions is needed, an extension is presented by Grosz and Sidner (1986). This message constitutes the primary communicative or dis course goal (Grosz and Sidner, 1986) of the graphic and captures its main contribution to the overall dis course goal of the entire document. Therefore, it is appropriate to close the initial summary with propositions from the computational class so that the whole graphic is in the user's focus of attention (Grosz and Sidner, 1986). structure of discourse segment purposes in the view of (Grosz and Sidner, 1986). This structure, in turn, identifies domains for interpretation; many systems for anaphora resolution rely on some notion of locality (Grosz and Sidner, 1986). Recognizing the structure of text is an essential task in text understanding. [GroszandSidner, 1986]. Third, the knowledge of textual structure helps to interpret the meaning of entities in a text (Grosz and Sidner 1986). Grosz and Sidner (1986) proposed a theory of discourse structure to account for why an utterance was said and what was meant by it. We start by discussing the work of Grosz and Sidner (1986), which ties speaker's intentions to linguistic structure.
Supervised Models for Coreference Resolution Traditional learning-based coreference reoperate by training a mentionfor determining whether two mentions are coreferent or not. Two independent lines of recent research have attempted to improve these mention-pair one by learning a mentionto rank preceding mentions for a given anaphor, and the other training an to determine whether a preceding cluster is coreferent with a given mention. We propose a cluster-ranking approach to coreference resolution that combines the strengths of mention rankers and entitymention models. We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution. Experimental results on the ACE data sets demonstrate its superior performance to competing approaches.   For comparison purposes, the B3None variant used on A05RA is calculated slightly differently than other B3None results; see Rahman and Ng (2009). The Stoyanov et al (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest performing cluster ranking model. The set of features that we use, listed in Table 5, is an extension of the set by Rahman and Ng (2009). Our system is based on a cluster-ranking model proposed by Rahman and Ng (2009), with novel semantic features based on recent re search on narrative event schema (Chambers and Jurafsky, 2009). We created a baseline system based on the cluster-ranking model proposed by Rahman and Ng (2009). Rahman and Ng (2009) in particular propose the cluster-ranking model which we used in our baseline. In each query we include a null-cluster instance, to allow joint learning of discourse-new detection, following (Rahman and Ng, 2009). We follow Rahman and Ng (2009) in jointly learning to detect anaphoric mentions along with resolving coreference relations. We follow the procedure described in (Rahman and Ng, 2009). In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010).  Also, the B3 variant used by Rahman and Ng (2009) is slightly different from other systems (they remove all and only the singleton twinless system mentions, so it is neither B3All nor B3None). The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). To extract NPs from the ACE-annotated documents, we train a mention extractor on the training texts (see Section 5.1 of Rahman and Ng (2009) for details), which recalls 83.6% of the NPs in the test set. Since space limitations preclude a description of these features, we refer the reader to Rahman and Ng (2009) for details. Details of the CR model can be found in Rahman and Ng (2009). Also, the results show that the CR model is stronger than the MP model, corroborating previous empirical findings (Rahman and Ng, 2009). The cluster ranking model of Rahman and Ng (2009) proceeds in a left-to-right fashion and adds the current discourse old mention to the highest scoring preceding cluster.
Extraposition Grammars Extraposition grammars are an extension of definite clause grammars, and are similarly defined in terms of logic clauses. The extended formalism makes it easy to describe left extraposition of constituents, an important feature of natural language syntax. For the sake of comparison, let us consider the following example which Pereira 1981 uses in order to illustrate Extraposition Grammar. A number of nonconcatenative grammar formalisms has been put forward, such as head-wrapping grammars (HG) (Pollard, 1984), extra position grammars (XG) (Pereira, 1981). Other non-concatenative formalisms are head-wrapping grammars (HG) (Pollard, 1984), extra position grammars (XG) (Pereira, 1981) and various exotic forms of tree adjoining grammar (Kroch and Joshi, 1986). Whereas head grammars provide for an account of verb fronting and cross-serial dependencies, Pereira, introducing extraposition grammars in (Pereira, 1981), is focused on displacement of noun phrases in English. XP -* NP While XG allows for elegant accounts of cross-serial dependencies and topicalization, it seems again hard to simultaneously account for verb and noun movement, especially if the bracketing constraint introduced in (Pereira, 1981), which requires that XG derivation graphs have a planar representation, is not relaxed. 
Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora We address the creation of cross-lingual textual entailment corpora by means of crowdsourcing. Our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocessing tools or already annotated monolingual datasets. In line with recent works emphasizing the need of large-scale annotation efforts for textual entailment, our work aims to: the scarcity of data available to train evaluate systems, and the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality. We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators. The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German. In our first experiment we used the English-German portion of the CLTE corpus described in (Negri et al., 2011), consisting of 500 multi-directional entailment pairs which we equally divided into training and test sets. The corpora used in the experiments comes from a cross-lingual Textual Entailment dataset presented in (Negri et al, 2011), and provided by the task organizers. Defining "entailment" is quite difficult when dealing with expert annotators and still more with non-experts, as was noted by Negri et al. (2011). Afterwards, the creation of CLTE corpus by using Mechanical Turk is described on (Negri et al, 2011) and a corpus freely available for CLTE is published (Castillo, 2011). The dataset provided by the organizers consists of 500 CLTE pairs translated to four languages following the crowdsourcing-based methodology proposed in (Negri et al., 2011). The dataset was created following the crowdsourcing methodology proposed in (Negri et al, 2011), which consists of the following steps. Two datasets were provided by the organization of SemEval 2012 (Negri et al, 2011): a training set and a test set, both composed by a set of 500 pairs of sentences. There was one training set for each French-English, German-English, Italian-English, Spanish-English language combination (Negri et al, 2011). The values of the parameters were chosen based on the CLTE development dataset (Negri et al, 2011) and were as follows.
A Joint Source-Channel Model For Machine Transliteration Most foreign names are transliterated into Chinese, Japanese or Korean with approximate phonetic equivalents. The transliteration is usually achieved through intermediate phonemic mapping. This paper presents a new framework that allows direct orthographical mapping (DOM) between two different languages, through a joint source-channel model, also transliteration model (TM). the TM model, we automate the orthographic alignment process to derive the aligned transliteration units from bilingual dictionary. The TM under the DOM framework greatly reduces system development effort and provides a quantum leap in improvement in transliteration accuracy over that of other state-of-the-art machine learning algorithms. The modeling framework is validated through several experiments for English-Chinese language pair. (Li et al 2004) introduced the joint transliteration model whose variant augmented with adaptive re-ranking we used in our experiments.  Li et al (2004) presented a framework allowing direct orthographical mapping of transliteration units be tween English and Chinese, and an extended model is presented in Ekbal et al (2006). One Chinese Pinyin string can correspond to several Chinese characters (Li et al, 2004). For this reason, it has been reported that English-to-Chinese transliteration without Chinese phonemes outperforms that with Chinese phonemes (Li et al, 2004). We performed alignment between E G and E P and between E P and C P in a similar manner presented in Li et al (2004).  We used the same test set used in Li et al (2004) for our testing. (Xinhua News Agency, 1992), which includes names in English, French, German, and many other foreign languages (Li et al., 2004). Table 6 represents the overall performance of one system in a previous work (Li et al, 2004) and eighteen systems based on the transliteration models defined in this paper. To compare Li et al (2004) and transliteration models defined in this paper under the same condition, we also carried out experiments with the same training data in Li et al (2004). Since the training data used in Li et al (2004) is identical as the union of our training and development data, we denoted it as TRAIN+DEV in Table 6. The grapheme-based approach, also known as direct orthographical mapping (Li et al, 2004), which treats transliteration as a statistical machine translation problem under monotonic constraints, has also achieved promising results. Other transliteration systems focus on alignment for transliteration, for example the joint source channel model suggested by Li et al (2004). The grapheme-based approach, which treats transliteration as statistical machine translation problem under monotonic constraint, aims to obtain a direct orthographical mapping (DOM) to reduce possible errors introduced in multiple conversions. Phoneme-based approaches are usually not good enough, because name entities have various etymological origins and transliterations are not always decided by pronunciations (Li et al, 2004). Li et al (2004) propose a letter-to-letter n-gram transliteration model for Chinese-English transliteration in an attempt to allow for the encoding of more contextual information. Many transliterated words are proper names, whose pronunciation rules may vary depending on the language of origin (Li et al, 2004). So grapheme-based (Li et al, 2004) approach has gained lots of attention recently. Direct orthographic mapping (e.g. Li et al, 2004), making use of individual Chinese graphemes, tends to overcome the problem and model the character choice directly.
MBT: A Memory-Based Part Of Speech Tagger-Generator We introduce a memory-based approach to part of speech tagging. Memory-based learning is a form of supervised learning based on similarity-based reasoning. The part of speech tag of a word in a particular context is extrapolated from the most similar cases held in memory. Supervised learning approaches are useful when a tagged corpus is available as an example of the desired output of the tagger. Based on such a corpus, the tagger-generator automatically builds a tagger which is able to tag new text the same way, diminishing development time for the construction of a tagger considerably. Memory-based tagging shares this advantage with other statistical or machine learning approaches. Additional advantages specific to a memory-based approach include (i) the relatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii) explanation capabilities, (iv) flexible integration of information in case representations, (v) its non-parametric nature, (vi) reasonably good results on unknown words without morphological analysis, and (vii) fast learning and tagging. In this paper we show that a large-scale application of the memory-based approach is feasible: we obtain a tagging accuracy that is on a par with that of known statistical approaches, and with attractive and time complexity properties when using tree-based formalism for indexing and searching huge case bases. The use of IGTree has as additional advantage that optimal context size for disambiguation is dynamically computed. The first four lines show the token-level accuracy for standard POS tagging tools trained and evaluated on the BulTreeBank:2 TreeTagger (Schmid, 1994), which uses decision trees, TnT (Brants, 2000), which uses a hidden Markov model, SVMtool (Gimenez and Ma`rquez, 2004), which is based on support vector machines, and ACOPOST (Schroder, 2002), implementing the memory-based model of Daelemans et al (1996). We use the Memory Based Tagger (Daelemans et al, 1996) System Headline Source Florida executes notorious serial killer PBMT Serial killer executed in Florida Word Sub. To investigate the effect of using automatically assigned tags, we trained MBT, a memory-based tagger (Daelemans et al,1996), on the training portions of our 10-fold cross validation experiment for the maximal data and let it predict tags for the test material. In that table, TBL stands for Brill &apos; s transformation-based error-driven tagget (Brill, 1995), ME stands for a tagger based on the maximum entropy modelling (Ratnaparkhi, 1996), SPATTER stands for a statistical parser based on decision trees (Magerman, 1996), IGTREE stands for the memory-based tagger by Daelemans et al (1996), and, finally, TComb stands for a tagger that works by combination of a statistical trigram-based tagger, 59 Tagger TBL ME SPATTER IGTREE TComb STT+ (CPD+ENS) Train Test 950 Kw 150 Kw. The English data was automatically labeled with part-of-speech and chunk tags from the memory-based tagger and chunker (Daelemans et al, 1996), and the German data was labelled with the decision-tree-based TreeTagger (Schmidt, 1994). Direct feedback loops that copy a predicted output label to the input representation of the next example have been used in symbolic machine-learning architectures such as the the maximum-entropy tagger described by Ratnaparkhi (1996) and the memory-based tagger (MBT) proposed by Daelemans et al (1996). Comparison of generalization performances in terms of F-score of MBL on the three test sets, with and without a feedback loop, and the error reduction attained by the feedback-loop method, the F-score of the trigram-class method, and the F-score of the combination of the two methods. approach was proposed in the context of memory based learning for part-of-speech tagging as MBT (Daelemans et al, 1996). For the part of speech tagging, the memory-based tagger MBT (Daelemans et al, 1996), trained on the Wall Street Journal corpus2, was used. The CoNLL data differs slightly from the original Alpino tree bank as the corpus has been part-of-speech tagged using a Memory-Based-Tagger (Daelemans et al, 1996). The annotator has followed the MITRE and SAIC guidelines for named entity recognition (Chinchor et al, 1999) as well as possible. The data consists of words, entity tags and part of-speech tags which have been derived by a Dutch part-of-speech tagger (Daelemans et al, 1996). We used the provided POS annotation in Dutch (Daelemans et al, 1996) and a minimally supervised tagger (Yarowsky and Cucerzan, 2002) for Spanish to restrict the space of words accepted by the discriminators (e.g .is_B_candidate rejects prepositions, conjunctions, pronouns, adverbs, and those determiners that are the first word in the sentence). We employ MBT, a memory-based tagger-generator and tagger (Daelemans et al, 1996) to produce apart-of-speech (PoS) tagger based on the ATB1corpus2. A particular instantiation, MBT, was proposed in (Daelemans et al, 1996). We use the Memory-Based Tagger (Daelemans et al, 1996) trained on the Brown corpus to compute the part-of speech tags. Memory-based learning has been applied to a wide range of natural language processing tasks including part-of-speech tagging (Daelemans et al, 1996), dependency parsing (Nivre, 2003) and word sense disambiguation (Kubler and Zhekova, 2009). This fact prohibits the feeding of the training algorithms with patterns that have the form: (Tagi_2, Tagi_b Tagi, Tagi.~, Manual_Tagi), which is the ease for similar systems that learn POS disambiguation (e.g., Daelemans et al, 1996). Comparing our tree-induction algorithm and IGTREE, the algorithm used in MBT (Daelemans et al, 1996), their main difference is that IGTREE produces oblivious decision trees by supplying an a priori ordered list of best features instead of re-computing the best feature during each branching, which is our case. The MBT POS tagger (Daelemans et al, 1996) is used to provide POS information. This material was POS-tagged using MBT (Memory Based Tagger) (Daelemans et al,1996). Its original PoS tag set is very coarse and the PoS and the word stem information is not very reliable. We therefore decided to retag the tree bank automatically using the Memory-Based Tagger (MBT) (Daelemans et al, 1996) which uses a very fine-grained tag set.
Lexical Normalisation of Short Text Messages: Makn Sens a #twitter hanb@student.unimelb.edu.au tb@ldwin.net Abstract Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words. Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn’t require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter. We use unsupervised methods to build a pipeline that identifies ill-formed English SMS word tokens and builds a dictionary of their most likely normalized forms. Hanand Baldwin (2011) use a classifier to detect ill formed words, and then generate correction candidates based on morphophonemic similarity. w2wN: The output of the word-to-word normalization of Han and Baldwin (2011). 5.2.1 Twitter To evaluate the performance on Twitter data, we use the dataset of randomly sampled tweets produced by (Han and Baldwin, 2011). We expect that our language model could improve other Social Media tasks, for example lexical normalisation (Han and Baldwin, 2011) or even event detection (Lin et al., 2011). Han and Baldwin (2011) use a classifier to detect ill-formed words, and generate correction candidates based on morphophonemic similarity. Recently, Han and Baldwin (2011) and Gouwsetal2011) propose two-step unsupervised approaches to normalisation, in which lexical variants are first identified, and then normalised. They approach lexical variant detection by using a context fitness classifier (Han and Baldwin, 2011) or through dictionary lookup (Gouws et al 2011). In contrast to the normalisation dictionaries of Han and Baldwin (2011) and Gouws et al 2011) which focus on very frequent lexical variants, we focus on moderate frequency lexical variants of a minimum character length, which tend to have unambiguous standard forms; our intention is to produce normalisation lexicons that are complementary to those currently available. To further narrow the search space, we only consider IV words which are morphophonemic ally similar to the OOV type, following settings in Han and Baldwin (2011). Given the re-ranked pairs from Section 5, here we apply them to a token-level normalisation task using the normalisation dataset of Han and Baldwin (2011). In addition, the contribution of these dictionaries in hybrid normalisation approaches is also presented, in which we first normalise OOVs using a given dictionary (combined or otherwise), and then apply the normalisation method of Gouws et al2011) based on consonant edit distance (GHM-norm), or the approach of Han and Baldwin (2011) based on the summation of many unsupervised approaches (HB-norm), to the remaining OOVs. the Internet slang dictionary (HB-dict) from Han and Baldwin (2011), and combinations of these dictionaries. In addition, we combine the dictionaries with the normalisation method of Gouws et al2011) (GHM-norm) and the combined unsupervised approach of Han and Baldwin (2011) (HB-norm). 6.2.3 Hybrid Approaches The methods of Gouws et al2011) (i.e. GHM-dict+GHM-norm) and Han and Baldwin (2011) (i.e. HB-dict+HB-norm) have lower precision and higher false alarm rates than the dictionary based approaches; this is largely caused by lexical variant detection errors. The present lexical normalisation used by our system is the dictionary lookup method of Hanand Baldwin (2011) which normalises noisy tokens only when the normalised form is known with high confidence (e.g. you for u). Ultimately, however, we are interested in performing context sensitive lexical normalisation, based on a reimplementation of the method of Han and Baldwin (2011). (Hanand Baldwin, 2011) reported an average of 127 candidates per nonstandard token with the correct-word coverage of 84%. (Han and Baldwin, 2011) developed classifiers for detecting the ill-formed word sand generated corrections based on the morphophonemic similarity. 
Coping With Syntactic Ambiguity Or How To Put The Block In The Box On The Table we construct a table so that the entry in the tells the parser how to parse i occurrences of 9. An Example Suppose for example that we were given the following grammar: (40a) S NP VP ADJS (40b) S V NP (PP) ADJS ADJS (40c) VP -0. V NP (PP) ADJS (40d) PP P NP (40e) NP NI NP PP ADJS adj ADJS I (In this example we will assume no lexical ambiguity V, P, inspection, we notice that NP are Catalan grammars and that ADJS is a Step grammar. PP = E i>0 NP = N ADJS = With these observations, the parser can process PPs, and by counting the number of occurrencof terminal symbols and looking up numbers in the appropriate tables. We now substitute (41a-c) into (40c). (42) VP = V NP (1 + PP)ADJS V (N E N)')(E (E and simplify the convolution of the two Catalan functions VP = V (N E adj') so that the parser can also find VPs by just counting coccurrences of terminal symbols. Now we simplify so that can also be parsed by just counting occurrences of terminal symbols. translate (40a-b) into the equation: (44) S = NP VP ADJS + V NP (1+PP) ADJS ADJS and then expand VP using (42) (45) S = NP (V NP (1+PP) ADJS) ADJS + V NP (1+PP) ADJS ADJS and factor S = (NP + 1) V NP (1+PP) That can be simplified considerably because NP (1 + PP) = N E E N E and (48) = E adj' E adj' = (i + so that S = (N E + 1) N E Cat. 14- (i + has the following Jump Jump • • • • • (1, (i + 1+ (50) The entire example grammar has now been compiled into a form that is easier for parsing. This formula says that sentences are all of the form: (51) S (N (P N)*) V N (P N)* adj* which could be recognized by the following finite state machine: (52) ›c) Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 Kenneth Church and Ramesh Patil Coping with Syntactic Ambiguity Furthermore, the number of parse trees for a given input sentence can be found by multiplying three numbers: (a) the Catalan of the number of P N's before the verb, (b) the Catalan of one more than the number of P N's after the verb, and (c) the ramp of the number of adj's. For example, the sentence (53) The man on the hill saw the boy with a telescope yesterday in the morning. Cat * * 3 = 6 parses. That is, there is one way to parse &quot;the man on the hill,&quot; two ways to parse &quot;saw the boy with a telescope&quot; (&quot;telescope&quot; is either a complement of &quot;see&quot; as in (54a-c) or is attached to &quot;boy&quot; as in (54d-f)), and three ways to parse the adjuncts (they could both attach to the S (54a,d), or they could both attach to the VP (54b,e), or they could split (54c,f)). (54a) [The man on the hill [saw the boy with a telescope] [yesterday in the morning.]] (54b) The man on the hill [[saw the boy with a telescope] [yesterday in the morning.]] (54c) The man on the hill [[saw the boy with a telescope] yesterday] in the morning. (54d) [The man on the hill saw [the boy with a telescope] [yesterday in the morning.]] (54e) The man on the hill [saw [the boy with a telescope] [yesterday in the morning.]] (54f) The man on the hill [saw [the boy with a telescope] yesterday] in the morning. All and only these possibilities are permitted by the grammar. 10. Conclusion We began our discussion with the observation that certain grammars are &quot;every way ambiguous&quot; and suggested that this observation could lead to improved parsing performance. Catalan grammars were then introduced to remedy the situation so that the processor can delay attachment decisions until it discovers some more useful constraints. Until such time, the processor can do little more than note that the input sentence is &quot;every way ambiguous.&quot; We suggested that a table lookup scheme might be an effective method to implement such a processor. We then introduced rules for combining primitive grammars, such as Catalan grammars, into composite grammars. This linear systems view &quot;bundles up&quot; all the parse trees into a single concise description capable of telling us everything we might want to know about the parses (including how much it might cost to ask a particular question). This abstract view of ambiguity enables us to ask questions in the most convenient order, and to delay asking until it is clear that the pay-off will exceed the cost. This abstraction was strongly influenced by the notion of binding. We have presented combination rules in three different representation systems: power series, ATNs, and context-free grammars, each of which contributed its own insights. Power series are convenient for defining the algebraic operations, ATNs are most suited for discussing implementation issues, and context-free grammars enable the shortest derivations. Perhaps the following quotation best summarizes our motivation for alternating among these three representation systems: thing or idea seems meaningful only when we have different ways to represent it — different perspectives and different associations. Then you can turn it around in your mind, so to speak; however, it seems at the moment you can see it another way; you never come to a full stop. (Minsky 1981, p. 19) In each of these representation schemes, we have introduced five primitive grammars: Catalan, Unit Step, 1, and 0, and terminals; and four composition rules: addition, subtraction, multiplication, and division. We have seen that it is often possible to employ these analytic tools in order to re-organize (compile) the grammar into a form more suitable for processing efficiently. We have identified certain where the ambiguity is combinatoric, and have sketched a few modifications to the grammar that enable processing to proceed in a more efficient manner. In particular, we have observed it to be important for the grammar to avoid referencing quantities that are not easily determined, such as the dividing point between a noun phrase and a prepositional phrase as in (55) Put the block in the box on the table in the kitchen ... We have seen that the desired re-organization can be achieved by taking advantage of the fact that the autoconvolution of a Catalan series produces another Caseries. This reduced processing time from to almost linear time. Similar analyses have been discussed for a number of lexically and structurally ambiguous constructions, culminating with the example in section 9, where we transformed a grammar into a form that could be parsed by a single left-to-right pass over the terminal elements. Currently, these grammar reformulations have to be performed by hand. It ought to be possible to automate this process so that the reformulations could be performed by a grammar compiler. We leave this project open for future research. 11. Acknowledgments We would like to thank Jon Allen, Sarah Ferguson, Lowell Hawkinson, Kris Halvorsen, Bill Long, Mitch Marcus, Rohit Parikh, and Peter Szolovits for their very useful comments on earlier drafts. We would Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 especially like to thank Bill Martin for initiating the project. In addition to the cases of left-linear and right linear grammars discussed in Section 3, our algorithm is exact in a variety of interesting cases, including the examples of Church and Patil (1982), which illustrate how typical attachment ambiguities arise as structural ambiguities on regular string sets. In contrast, our method will produce an exact FSA for many interesting grammars generating regular languages, such as those arising from systematic attachment ambiguities (Church and Patil, 1982). As the NP length increases, the number of possible binary trees (parses) increases with the Catalan numbers (Church and Patil, 1982). Coordination ambiguity is under-explored, despite being one of the three major sources of structural ambiguity (together with prepositional phrase attachment and noun compound bracketing), and belonging to the class of ambiguities for which the number of analyses is the number of binary trees over the corresponding nodes (Church and Patil, 1982), and despite the fact that conjunctions are among the most frequent words. The number of possible binary-branching parses of a sentence is defined by the Catalan number, an exponential combinatoric function (Church and Patil, 1982), so dynamic programming is crucial for efficiency.
A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches This paper presents and compares WordNetbased and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses. Agirre et al (2009) split thews set into similarity (wss) and relatedness (wsr) subsets. Interestingly, the same overall result pattern is observed if we limit evaluation to the WordSim subsets that Agirre et al (2009) have identified as semantically similar (e.g., synonyms or coordinate terms) and semantically related (e.g., meronyms or topically related concepts). Spearman's rank correlation (ρ) with average human judgements (Agirre et al., 2009) was used to measure the quality of various models. Multiple prototypes improve Spearman correlation on WordSim-353 compared to previous methods using the same underlying representation (Agirre et al., 2009). Earlier work on reducing the polysemy of sense inventories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of word senses (Agirre and Lopez, 2003; McCarthy, 2006). In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2,000 cores.  We also compare our results against the state-of-the-art results (Agirre) for distributional similarity (Agirre et al, 2009). Recent systems have, however, shown improved results using extremely large corpora (Agirre et al, 2009), and existing large-scale resources such as Wikipedia (Strube and Ponzetto, 2006). In a recent paper, Agirre et al (2009) parsed 4 billion documents (1.6 Terawords) crawled from the web, and then used a search function to extract syntactic relations and context windows surrounding key words.   While our results are not competitive with the best corpus based methods, we can note that our current corpus is an order of magnitude smaller - 2 million sentences versus 1 million full Wikipedia articles (Gabrilovich and Markovitch, 2007) or 215MB versus 1.6 Terabyte (Agirre et al, 2009). We also evaluated our scores separately on the semantically similar versus the semantically related subsets of WordSim-353 following Agirre et al (2009). This is remarkable as different methods tend to be more appropriate to calculate either one or the other (Agirre et al, 2009). Agirre et al (2009) compared DS approaches with WordNet-based methods in computing word similarity and relatedness; and they also studied the combination of them. For instance, Agirre et al (2009) derive a WordNet-based measure using PageRank and combined it with several corpus based vector space models using SVMs. Examining the relations between the words in each pair, Agirre et al (2009) further split this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the former contains synonyms, antonyms, identical words and hyponyms/hypernyms and the latter capture other word relations.  
The Complexity of Phrase Alignment Problems Many phrase alignment models operate over combinatorial space of phrase We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard. On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient. More recently, DeNero and Klein (2008) have proven the NP-completeness of the phrase alignment problem. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial for a sub-class of weighted synchronous context free grammars (Wu, 1997). Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). Extending A1-1 to include blocks is problematic, because finding a maximal 1-1 matching over phrases is NP-hard (DeNero and Klein, 2008). (DeNero and Klein, 2008) gives an integer linear programming formulation of another alignment model based on phrases. Recently, DeNero and Klein (2008) addressed the training problem for phrase-based models by means of integer linear programming, and proved that the problem is NP-hard. While theoretically sound, this approach is computationally challenging both in practice (DeNero et al, 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al, 2006), and in the end may lead to inferior translation quality (Koehn et al, 2003). The first challenge is with inference: computing alignment expectations under general phrase models is #P-hard (DeNero and Klein, 2008). Our application is also atypical for an NLP application in that we use an approximate sampler not only to include Bayesian prior in formation (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). The general phrase alignment problem under an arbitrary model is known to be NP-hard (DeNero and Klein, 2008). However, this has been shown to be infeasible for real-world data (DeNero and Klein, 2008). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. A similar approach is employed by DeNero and Klein (2008) for finding optimal phrase-based alignments for MT. However, these models are unsuccessful largely due to intractable estimation (DeNero and Klein, 2008).
Domain Adaptation with Active Learning for Word Sense Disambiguation When a word sense disambiguation (WSD) system is trained on one domain but applied to a different domain, a drop in accuracy is frequently observed. This highlights the importance of domain adaptation for word sense disambiguation. In this paper, we first show that an active learning approach can be successfully used to perform domain adaptation of WSD systems. Then, by using the predominant sense predicted by expectation-maximization (EM) and adopting a count-merging technique, we improve the effectiveness of the original adaptation process achieved by the basic active learning approach. Chan and Ng (2007) performed supervised domain adaptation on a manually selected subset of 21 nouns from the DSO corpus.  In building training dataset by active learning, we use uncertainty sampling like (Chan and Ng, 2007) (Figure 1 line 30-31). Instance weighting (Jiang and Zhai, 2007) and active learning (Chan and Ng, 2007) are also employed in domain adaptation. AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the results of (Dang, 2004) raise doubts as to whether AL can successfully be applied to a fine-grained annotation scheme, where Inter Annotator Agreement (IAA) is low and thus the consistency of the human annotations decreases. Active learning has been widely used for NLP tasks such as part of speech tagging (Ringger et al, 2007), parsing (Tang et al, 2002) and word sense disambiguation (Chan and Ng, 2007). Here, we take inspiration from the target-word specific results reported by Chan and Ng (2007) where by using just 30% of the target data they obtained the same performance as that obtained by using the entire target data.  Domain specific WSD for selected target words has been attempted by Ng and Lee (1996), Agirre and de Lacalle (2009), Chan and Ng (2007), Koeling et al (2005) and Agirre et al (2009b). Our main inspiration comes from the target word specific results reported by Chan and Ng (2007) and Agirre and de Lacalle (2009). With the exception of (Chan and Ng, 2007) which tried to adapt a WSD system trained on the BC part of the DSO corpus to the WSJ part of the DSO corpus, the other researchers simply applied active learning to reduce the annotation effort required and did not deal with the issue of adapting a WSD system to a new domain. It has also been applied to the problem of domain adaptation for word sense disambiguation in (Chan and Ng, 2007). Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning.
The CoNLL 2007 Shared Task on Dependency Parsing The Conference on Computational Natural Language Learning features a shared task, inwhich participants train and test their learn ing systems on the same data sets. In 2007, as in 2006, the shared task has been devoted to dependency parsing, this year with both a multilingual track and a domain adaptation track. In this paper, we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages. In addition, we characterize the different approaches of the participating systems, report the test results, and provide a first analysis of these results.    For example, of ten treebanks for CoNLL-2007 shared task, none includes more than 500K tokens, while the sum of tokens from all tree banks is about two million (Nivre et al, 2007). The CoNLL-2007 shared tasks include two tracks: the Multilingual Track and Domain AdaptationTrack (Nivre et al, 2007). Our second-order parser still does not reproduce the state-of-the art results presented by similar systems (Nivre et al, 2007). Some dependency parsing systems prefer two-stage architecture: unlabeled parsing and dependency classification (Nivre et al, 2007). Data-driven dependency parsing has recently received extensive attention in the parsing community and impressive results have been obtained for a range of languages (Nivre et al, 2007). The CoNLL-X (Buchholz and Marsi, 2006) and CoNLL 2007 (Nivre et al, 2007) shared tasks focused on multilingual dependency parsing. The organizers of the CoNLL 2007 shared task noted that languages with free word order and high morphological complexity are the most difficult for dependency parsing (Nivre et al, 2007). Morphologically rich languages present new challenges, as the use of state of the art parsers for more configurational and non-inflected languages like English does not reach similar performance levels in languages like Basque, Greek or Turkish (Nivre et al, 2007a).   For the experiments, we used the best configuration for English at the CoNLL 2007 Shared Task on Dependency Parsing (Nivre et al, 2007) as our baseline. We'll use a simple example sentence to illustrate how our feature sets are extracted from CONLL formatted data (Nivre et al, 2007). First, we investigate the impact of using different flavours of Covington's algorithm (Covington, 2001) for non projective dependency parsing on the ten different languages provided for CoNLL-X Shared Task (Nivre et al, 2007). In this paper we showed the performance of three flavours of Covington's algorithm for non-projective dependency parsing on the ten languages provided for the CoNLL-X Shared Task (Nivre et al, 2007). We evaluated our system using the standard evaluation script provided by the organizers (Nivre et al, 2007). The DS representation is taken from the conversion procedure used in the CoNLL 2007 Shared Task on dependency parsing (Nivre et al, 2007). Dependency trees are representations of the syntactic structure of a sentence (Nivre et al 2007).
Probabilistic Top-Down Parsing And Language Modeling This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition. The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling. A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers. A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity. Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model. A small recognition experiment also demonstrates the utility of the model. Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001)). Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition.  The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank. We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search. In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the argmax, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999). The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights.  Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word. A good example of this is the Roark parser (Roark, 2001) which works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning. At the end one has a beam-width's number of best parses (Roark, 2001). To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses. The n-best lists were provided by Brian Roark (Roark, 2001).  Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001).  For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags). We modified the Roark (2001) parser to calculate the discussed measures, and the empirical results in ?4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time. In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here. At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures.
Calibrating Features For Semantic Role Labeling This paper takes a critical look at the features used in the semantic role tagging literature and show that the information in the input, generally a syntactic parse tree, has yet to be fully exploited. We propose an additional set of features and our experiments show that these features lead to fairly significant improvements in the tasks we performed. We further show that different features are needed for different subtasks. Finally, we show that by using a Maximum Entropy classifier and fewer features, we achieved results comparable with the best previously reported results obtained with SVM models. We believe this is a clear indication that developing features that capture the right kind of information is crucial to advancing the stateof-the-art in semantic analysis. Syntactic frame as described by Xue and Palmer (2004) Table 3. The candidate argument extraction method used for the FrameNet data, (as mentioned in 4) was adapted from the algorithm of Xue and Palmer (2004) applied to dependency trees. For PropBank we use the algorithm of Xue and Palmer (2004) applied to dependency trees. Also, we considered some of the features designed by (Pradhan et al, 2005): First and Last Word/POS in Constituent, Subcategorization, Head Word of Prepositional Phrases and the Syntactic Frame feature from (Xue and Palmer, 2004). Hence we now prune our set, by keeping only the siblings of all of the verb's ancestors, as is common in supervised SRL (Xue and Palmer, 2004). The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al, (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). For example, Xue and Palmer (2004) reported that SRL performance dropped more than 10% when they used syntactic features from an automatic parser instead of the gold standard parsing trees. In order to reduce the number of candidate arguments in the identification step, I apply the filtering technique of Xue and Palmer (2004), trivially adopted to the dependency syntax formalism. To save time, we use a pruning stage (Xue and Palmer, 2004) to filter out the constituents that are clearly not semantic arguments to the predicate. Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling. Though several pruning algorithms have been raised (Xue and Palmer, 2004), the policies are all in global style. In this paper, a statistical analysis of Penn Prop Bank indicates that arguments are limited in a local syntax sub-tree rather than a whole one. Also, we considered some of the features designed by (Pradhan et al, 2004): First and Last Word/POS in Constituent, Subcategorization, Head Word of Prepositional Phrases and the Syntactic Frame feature from (Xue and Palmer, 2004). This is from a general belief that each step requires a different set of features (Xue and Palmer, 2004), and training these steps in a pipeline takes less time than training them as a joint-inference task. To reduce the complexity, Zhao et al (2009) reformulated a pruning algorithm introduced by Xue and Palmer (2004) for dependency structure by considering only direct dependents of a predicate and its ancestors as argument candidates. (Xue and Palmer, 2004) found out that different features suited for different sub-tasks of SRL ,i.e. argument identification and classification. As for the former (hereafter it is referred to synPth), we continue to use a dependency version of the pruning algorithm of (Xue and Palmer, 2004). Note that this pruning algorithm is slightly different from that of (Xue and Palmer, 2004), the predicate itself is also included in the argument candidate list as the nominal predicate sometimes takes itself as its argument. The effectiveness of the proposed additional pruning techniques may be seen as a significant improvement over the original algorithm of (Xue and Palmer, 2004). For our baseline SRL model, we adopt the features used in other state-of-the-art SRL systems, which include the seven baseline features from the original work of Gildea and Jurafsky (2002) ,additional features taken from Pradhan et al (2005), and feature combinations which are inspired by the system in Xue and Palmer (2004). For instance, many systems used the pruning strategy described in (Xue and Palmer, 2004) and other systems used the soft pruning rules described in (Pradhan et al, 2005a).
Latent Semantic Analysis For Text Segmentation This paper describes a method for linear text segmentation that is more accurate or at least as accurate as state-of-the-art methods (Utiyama and Isahara, 2001; Choi, 2000a). Inter-sentence similarity is estimated by latent semantic analysis (LSA). Boundary locations are discovered by divisive clustering. Test results show LSA is a more accurate similarity measure than the  PLSA is the probabilistic variant of latent semantic analysis (LSA) (Choi et al, 2001), and offers a more solid statistical foundation. Choi at al. used LSA for segmentation (Choi et al., 2001). (Choi et al, 2001) used all vocabulary words to compute low-dimensional document vectors. (Choi et al, 2001) used clustering to predict boundaries whereas we used the average similarity scores. It is a very powerful technique already used for NLP applications such as information retrieval (Berry et al, 1995) and text segmentation (Choi et al, 2001) and, more recently, multi and single-document summarization. In CWM (Choi et al, 2001), a variant of C99, each word of a sentence is replaced by its representation in a Latent Semantic Analysis (LSA) space.  While extensive research has been conducted in topic segmentation for monologues (e.g., (Malioutov and Barzilay, 2006), (Choi et al, 2001)) and synchronous dialogs (e.g., (Galley et al, 2003), (Hsueh et al, 2006)), none has studied the problem of segmenting asynchronous multi-party conversations (e.g., email). The CWM algorithm (Choi et al, 2001) applies the same procedure to a similarity matrix of LSI vectors. As a preliminary test of the error measure, I evaluated two algorithms from Choi et al (2001) on the standard segmentation data set that Choi (2000) compiled. The C99 (Choi, 2000) and CWM (Choi et al, 2001) algorithms were evaluated. Because of these differences, the implementation of HCWM reported here differs somewhat from the implementation of CWM reported by Choi et al (2001). (12.5%) matches what Choi et al (2001) reported (12%), while the error for HCWM (12.1%) is higher than that reported for the version with a paragraph-based 500-dimension LSI space (9%) but appears comparable to their sentence-based 400-dimension LSI space. And the result for NONE (46.1%) agrees with Choi et al (2001)'s results for their NONE (46%) base line. In addition, LSA has been applied to a number of NLP tasks, such as text segmentation (Choi et al, 2001).
Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning. Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences. We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations. Following previous work (Kwiatkowski et al, 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs. Kwiatkowski et al (2010) described an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. Our approach for learning factored PCCGs extends the work of Kwiatkowski et al (2010), as reviewed in Section 7. Our Factored Unification Based Learning (FUBL) method extends the UBL algorithm (Kwiatkowski et al, 2010) to induce factored lexicons, while also simultanously estimating the parameters of a log linear CCG parsing model. The overall approach is closely related to the UBL algorithm (Kwiatkowski et al, 2010), but includes extensions for updating the factored lexicon, as motivated in Section 6.  An area for future work is developing an automated way to produce this lexicon, perhaps by extending the recent work on automatic lexicon generation (Kwiatkowski et al2010) to the weakly supervised setting. For evaluation, following from Kwiatkowski et al (2010), we reserve 280 sentences for test and train on the remaining 600. WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system. As a starting point, we used the UBL system developed by Kwiatkowski et al (2010) to learn a semantic parser based on probabilistic Combinatory Categorial Grammar (PCCG). We show that it outperforms a state-of-the-art semantic parser (Kwiatkowski et al 2010) when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by Kwiatkowski et al).  In particular, our approach is closely related that of Kwiatkowski et al (2010) but, whereas that work required careful initialisation and multiple passes over the training data to learn a discriminative parsing model, here we learn a generative parsing model without either. This set is generated with the functional mapping T:{t}= T (s, m), which is defined, following Kwiatkowski et al (2010), using only the CCG combinators and a mapping from semantic type to syntactic category (presented in in Section 4). Here we review the splitting procedure of Kwiatkowski et al (2010) that is used to generate CCG lexical items and describe how it is used by T to create a packed chart representation of all parses {t} that are consistent with s and at least one of the meaning representations in {m}. For comparison we use the UBL semantic parser of Kwiatkowski et al (2010) trained in a similar setting i.e., with no language specific initialisation. Kwiatkowski et al (2010) initialise lexical weights in their learning algorithm using corpus-wide alignment statistics across words and meaning elements. Figure 4 shows accuracy for our approach with and without guessing, for UBL when run over the training data once (UBL1) and for UBL when run over the training data 10 times (UBL10) as in Kwiatkowski et al (2010). We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL; ts Vb (Jonesetal., 2012), which uses variational Bayesian inference to learn weights for a tree transducer; UBL (Kwiatkowski et al, 2010), which learns a CCGlexicon with semantic annotations; and hybrid tree (Lu et al, 2008), which learns a synchronous generative model over variable-free MRs and NL strings. The remaining system discussed in this paper, UBL (Kwiatkowski et al, 2010), leverages the fact that the MRL does not simply encode trees, but rather - calculus expressions.
Domain Adaptation With Structural Correspondence Learning Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resourcerich source domain to a resource-poor domain. We introduce learning automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger. Following (Blitzer et al, 2006), we present an application of structural correspondence learning to non-projective dependency parsing (McDonald et al, 2005). In this paper, we investigate the effectiveness of structural correspondence learning (SCL) (Blitzer et al, 2006) in the domain adaptation task given by the CoNLL 2007. Prettenhofer and Stein (2010) investigate cross lingual sentiment classification from the perspective of domain adaptation based on structural correspondence learning (Blitzer et al, 2006). There has been a lot of work in domain adaption for NLP (Dai et al, 2007) (Jiang and Zhai, 2007) and one suitable choice for our problem is the approach based on structural correspondence learning (SCL) as in (Blitzer et al, 2006) and (Blitzer et al, 2007b). These features can either be identified with heuristics (Blitzer et al, 2006) or by automatic selection (Blitzer et al, 2007b). However, another approach is to train a separate out-of-domain parser, and use this to generate additional features on the supervised and unsupervised in-domain data (Blitzer et al, 2006). Recent work by McClosky et al (2006) and Blitzer et al (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. SCL is the structural correspondence learning technique of Blitzer et al (2006). Note that there are some similarities between our two-stage semi-supervised learning approach and the semi-supervised learning method introduced by (Blitzer et al, 2006), which is an extension of the method described by (Ando and Zhang, 558 2005). SCL (Blitzer et al, 2006) is one feature representation approach that has been effective on certain high-dimensional NLP problems, including part-of-speech tagging and sentiment classification. The other dimensions of both domains were difficult to interpret. We experimented with using the SCL features together with the raw features (n-grams and length), as suggested by (Blitzer et al, 2006). As in (Blitzer et al, 2006), we found it necessary to scale up the SCL features to increase their utilization in the presence of the raw features; however, it was difficult to guess the optimal scaling factor without having access to labeled target data. In this way our problem resembles the part-of-speech tagging task (Blitzer et al, 2006), where the category of each word is predicted using values of the left, right, and current word token. (Blitzer et al, 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. Of course, it is a known fact that machine learning techniques do not transfer well across different domains (e.g., Blitzer et al (2006)). Finally, we compare two domain adaptation approaches to utilize unlabeled speech data: bootstrapping, and Blitzer et al's Structural Correspondence Learning (SCL) (Blitzer et al, 2006). In contrast with bootstrapping, SCL (Blitzer et al,2006) uses the unlabeled target data to learn domain independent features. Blitzer et al (2006) used Structural Correspondence Learning and unlabeled data to adapt a Part-of Speech tagger. For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al, 2006). Blitzer et al (2006) append the source domain labeled data with predicted pivots (i.e. words that appear in both the source and target domains) to adapt a POS tagger to a target domain.
Using Predicate-Argument Structures For Information Extraction In this paper we present a novel, customizable IE paradigm that takes advantage of predicate-argument structures. We also introduce a new way of automatically identifying predicate argument structures, which is central to our IE paradigm. It is based on: (1) an extended set of features; and (2) inductive decision tree learning. The experimental results prove our claim that accurate predicate-argument structures enable high quality IE results.  Indeed, the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from information extraction (Surdeanu et al, 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al, 2005). The benefit of semantic roles has already been demonstrated for a number of tasks, among others for machine translation (Boas, 2002), information extraction (Surdeanu et al, 2003), and question answering (Narayanan and Harabagiu, 2004). We use a semantic parser (described in (Surdeanu et al, 2003)) that recognizes predicate-argument structures. We have found that the identification of the association between a candidate answer and a question depends on (a) the recognition of predicates and entities based on both the output of a named entity recognizer and a semantic parser (Surdeanu et al, 2003) and their structuring into predicate-argument frames. However, it is a daunting task for people to find out information they are interested in from such a huge number of news tweets, thus motivating us to conduct some kind of information extraction such as event mining, where SRL plays a crucial role (Surdeanu et al, 2003). Surdeanu et al (2003) applied semantic parsing to capture the predicate-argument sentence structure. In the same line, some systems also use features of the content words of the argument, using the heuristics of Surdeanu et al (2003). Concerning lexicalization of the argument, most of the techniques rely on head word rules based on Collins, or content word rules as in Surdeanu et al (2003). The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al, (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Due to the sparsity of the head word feature, we also use the part-of-speech of the head word, following Surdeanu et al (2003). They are a combination of features introduced by Gildea and Jurafsky (2002), ones proposed in Pradhan et al (2004), Surdeanu et al (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Figure 1: A comparison of frames for buy.v defined in PropBank and FrameNet (Moschitti et al, 2007), and information extraction (Surdeanu et al, 2003) . Content words, which add informative lexicalized information different from the head word, were detected using the heuristics of (Surdeanu et al, 2003). Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al 2003), and Machine Translation (Boas 2002). SCFs can be useful for many NLP applications, such as parsing (John Carroll and Briscoe, 1998) or information extraction (Surdeanu et al, 2003). Surdeanu et al (2003) employ predicate-argument structures for information extraction. Semantic role analysis has the potential of benefiting a wide spectrum of applications ranging from information extraction (Surdeanu et al, 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al, 2005). For instance, information extraction (Surdeanu et al, 2003), question answering (Narayanan and Harabagiu, 2004) and machine translation (Boas, 2002) could stand to benefit from broad coverage semantic processing. Our method first converts the extracted answers into a series of open-domain templates, which are based on predicate-argument frames (Surdeanu et al 2003).
Factored Translation Models We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ?may it be linguistic markup or automatically generated word classes. In a num ber of experiments we show that factoredtranslation models lead to better translation performance, both in terms of auto matic scores, as well as more grammatical coherence. Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al 2006) and (Koehn and Hoang 2007). Eventually, we would like to replace the functionality of factored translation models (Koehn and Hoang, 2007) with lattice transformation and augmentation. We assume that the reader is familiar with the basics of phrase-based statistical machine translation (Koehn et al, 2003) and factored statistical machine translation (Koehn and Hoang, 2007). Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data (Koehn and Hoang, 2007). Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). We used factored translation (Koehn and Hoang, 2007), with both surface words and part-of-speech tags on the target side, with a sequence model on part-of-speech. A tight integration of morpho syntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. Koehn and Hoang (2007) generalise the phrase-based model's representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search. Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it down into several factors which are further combined using a log-linear model (Och and Ney, 2002). Unlike with factored models (Koehn and Hoang, 2007) or additional translation lexicons (Schwenk et al, 2008), we do not generate the surface form back from the lemma translation, which means that tense, gender and number information are lost. Factored translation models (Koehn and Hoang, 2007) facilitate a more data-oriented approach to agreement modeling. Factored models are introduced in (Koehn and Hoang, 2007) for better integration of morpho syntactic information. Gime ?nez and Ma`rquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-Englishcorpus, but only standard tokens are used in decoding.  Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). Koehn and Hoang (2007) have reported an in crease of 0.86 BLEU points for German-to-English translation for small training data. While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity, e.g., lemma-token and word-token LMs, it requires a 1:1 correspondence between the tokens in the different factors, which clearly is not our case. Our approach is built on top of the factor-based SMT model proposed by Koehn and Hoang (2007), as an extension of the traditional phrase based SMT framework. We also construct a Hebrew-to-English MT system using Moses' factored translation model (Koehn and Hoang, 2007). Factored translation models (Koehn and Hoang, 2007) approach the idea of integrating annotation into translation from the opposite direction. We translate from Spanish into English using phrase-based decoding with Moses (Koehn and Hoang, 2007) as our decoder.
A Simple and Effective Hierarchical Phrase Reordering Model While phrase-based statistical machine translation systems currently deliver state-of-the art performance, they remain weak on word order changes. Current phrase reordering models can properly handle swaps between adjacent phrases, but they typically lack theability to perform the kind of long-distance re orderings possible with syntax-based systems. In this paper, we present a novel hierarchical phrase reordering model aimed at improvingnon-local reorderings, which seamlessly in tegrates with a standard phrase-based system with little loss of computational efficiency. Weshow that this model can successfully han dle the key examples often used to motivate syntax-based systems, such as the rotation of a prepositional phrase around a noun phrase. We contrast our model with reordering models commonly used in phrase-based systems, and show that our approach provides statistically significant BLEU point gains for two language pairs: Chinese-English (+0.53 on MT05 and +0.71 on MT08) and Arabic-English (+0.55 on MT05). All differences between DE-Annotated and BASELINE are significant at the level of 0.05 with the approximate randomization test in (Riezler and Maxwell, 2005) conduct additional experiments with a hierarchical phrase reordering model introduced by Galley and Manning (2008). We use a phrase-based system similar to Moses (Koehn et al, 2007) based on a set of common features including maximum likelihood estimates p ML (e|f) and p ML (f |e), lexically weighted estimates p LW (e|f) and p LW (f |e), word and phrase-penalties, a hierarchical reordering model (Galley and Manning, 2008), a linear distortion feature, and a modified KneserNey language model trained on the target-side of the parallel data. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle long distance word-block permutations. From the word-to-word alignments, the system extracts a phrase table (Koehn et al, 2003) and hierarchical reordering model (Galley and Manning, 2008). Instead of just looking at the reordering relationship between individual phrases, the new feature examines the reordering of blocks of adjacent phrases (Galley and Manning, 2008) and improves translation quality when the material being reordered cannot be captured by single phrase. Galley and Manning (2008) present a hierarchical phrase reordering model aimed at improving non-local reorderings. Our MOS concept is also closely related to hierarchical reordering model (Galley and Manning, 2008) in phrase-based decoding, which computes o of b with respect to a multi-block unit that may go beyond b?. Given no constraint on maximum phrase length, the hierarchical phrase reordering model (Galley and Manning, 2008) also analyzes the adjacent bilingual phrases for bp and identifies its orientation as S. We plan to apply our method to the complex lexicalized reordering models, for example, the hierarchical reordering model (Galley and Manning, 2008) and the MEBTG reordering model (Xiong et al, 2006). In (Galley and Manning, 2008) a hierarchical orientation model is introduced that captures some non-local phrase reordering by a shift reduce algorithm. This method reduces the complexity to O (nbdmax) but fails to capture long distance reorderings (Galley and Manning, 2008). For example Galley and Manning (2008) propose a shift-reduce style method to allow hieararchical non-local reorderings in a phrase-based decoder. In addition to the baseline Phrasal feature set, we used the lexicalized re-ordering model of Galley and Manning (2008). Permutation parsers have been used to implement hierarchical re-ordering models (Galley and Manning, 2008) and to enforce inversion transduction grammar (ITG) constraints (Fengetal., 2010). Thus far, they have been used to enable a hierarchical re-ordering model, or HRM (Galley and Manning, 2008), as well as an ITG constraint (Feng et al, 2010). The HRM (Galley and Manning, 2008) maintains similar re-ordering statistics, but determines orientation differently. Galley and Manning (2008) introduce a deterministic shift-reduce parser into decoding, so that the decoder always has access to the largest possible previous block, given the current translation history. Galley and Manning (2008) propose an algorithm that begins by running standard phrase extraction (Och and Ney, 2004) without a phrase-length limit, noting the corners of each phrase found. In all experiments we use phrase-orientation lexicalized reordering (Galley and Manning, 2008) which models monotone, swap, discontinuous orientations from both reordering with previous phrase pair and with the next phrase pair. Galley and Manning (2008) introduce three orientation models for lexicalized reordering: word-based, phrase-based and hierarchical orientation model.
Senseval-3 Task: Automatic Labeling Of Semantic Roles The SENSEVAL-3 task to perform automatic labeling of semantic roles was designed to encourage research into and use of the FrameNet dataset. The task was based on the considerable expansion of the FrameNet data since the baseline study of automatic labeling of semantic roles by Gildea and Jurafsky. The FrameNet data provide an extensive body of “gold standard” data that can be used in lexical semantics research, as the basis for its further exploitation in NLP applications. Eight teams participated in the task, with a total of 20 runs. Discussions among participants during development of the task and the scoring of their runs contributed to a successful task. Participants used a wide variety of techniques, investigating many aspects of the FrameNet data. They achieved results showing considerable improvements from Gildea and Jurafsky’s baseline study. Importantly, their efforts have contributed considerably to making the complex FrameNet dataset more accessible. They have amply demonstrated that FrameNet is a substantial lexical resource that will permit extensive further research and exploitation in NLP applications in the future. To date PropBank and FrameNet are the two main resources in English for training semantic role labelling systems, as in the CoNLL-2004 shared task (Carreras and Marquez, 2004) and SENSEVAL-3 (Litkowski, 2004). Also from these 321 frames only 100 were considered to have enough training data and were used in Senseval-3 (see Litkowski, 2004 for more details). Although the strict measures are more interesting, we include these figures for comparison with the systems participating in the Senseval-3 Restricted task (Litkowski, 2004). This task is a more advanced and realistic version of the Automatic Semantic Role Labeling task of Senseval-3 (Litkowski, 2004). Noun predicates also appear in FrameNet semantic role labeling (Gildea and Jurafsky, 2002), and many FrameNet SRL systems are evaluated in Senseval-3 (Litkowski, 2004). The same problem was again highlighted by the results obtained with and without the frame information in the Senseval-3 competition (Litkowski,2004) of FrameNet (Johnson et al, 2003) role labeling task. Also from these 321 frames only 100 were considered to have enough training data and were used in Senseval-3 (see (Litkowski, 2004) for more details). This task has been the subject of a previous Senseval task (Automatic Semantic Role Labeling, Litkowski (2004)) and two shared tasks on semantic role labeling in the Conference on Natural Language Learning (Carreras Marquez (2004) and Carreras Marquez (2005)).  In addition to the CoNLL-2004 shared task, another evaluation exercise was conducted in the Senseval-3 workshop (Litkowski, 2004). Beginning with work by Gildea and Jurafsky (2002), there has been a large interest in semantic role labelling, as evidenced by its adoption as a shared task in the Senseval-III competition (FrameNet data, Litkowski, 2004) and at the CoNLL-2004 and 2005 conference (PropBank data, Carreras and Mrquez, 2005).
Is Knowledge-Free Induction Of Multiword Unit Dictionary Headwords A Solved Problem? We seek a knowledge-free method for inducing multiword units from text corpora for use as machine-readable dictionary headwords. We provide two major evaluations of nine existing collocation-finders and illustrate the continuing need for improvement. We use Latent Semantic Analysis to make modest gains in performance, but we show the significant challenges encountered in trying this approach. Schone and Jurafsky (2001) applied LSA to the analysis of MWEs in the task of MWE discovery, by way of rescoring MWEs extracted from a corpus. Indeed, Schone and Jurafsky (2001) provide evidence that suggests that WordNet is as effective an evaluation resource as the web for MWE detection methods, despite its inherent size limitations and static nature. Schone and Jurafsky (Schone and Jurafsky, 2001) applied Latent-Semantic Analysis (LSA) to the analysis of MWEs in the task of MWE discovery, by way of rescoring MWEs extracted from the corpus. Regarding (i), Schone and Jurafsky (2001) compare the semantic vector of a phrase p and the vectors of its component words in two ways: one includes the contexts of p in the construction of the semantic vectors of the parts and one does not. Statistical association measures are frequently used for MWU detection and collocation extraction (e.g. Schone and Jurafsky (2001), Evert and Krenn (2001), Pecina (2010)). We use all measures used by Schone and Jurafsky (2001) that can be derived from a phrase's contingency table. As our baseline, we use two methods of comparing semantic vectors: sj1 and sj2, both introduced by Schone and Jurafsky (2001). Examples include automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Schutze, 1998) and disambiguation (McCarthy et al, 2004), collocation extraction (Schone and Jurafsky, 2001), text segmentation (Choi et al, 2001), and notably information retrieval (Salton et al, 1975). By contrast, Schone and Jurafsky (2001) evaluate the identification of phrasal terms without grammatical filtering on a 6.7 million word extract from the TREC databases, applying both WordNet and on line dictionaries as gold standards. Since Schone and Jurafsky (2001) demonstrated similar results whether WordNet or on line dictionaries were used as a gold standard, WordNet was selected. There have also been a number of papers focusing on the detection of semantic non-compositional items in recent years beginning with the work of Schone and Jurafsky (2001). Furthermore, lists of topical unigrams are often made only marginally interpretable by virtue of their non-compositionality, the principle that a collocation's meaning typically is not derivable from its constituent words (Schone and Jurafsky, 2001).
METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more used It is one of several automatic metrics used in this year’s shared task within the ACL WMT-07 workshop. This paper recaps the technical details underlying the metric and describes recent improvements in the metric. The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and Its flexible matching was extended to French, Spanish, German and Czech for this workshop (Lavie and Agarwal, 2007). We evaluate using BLEU4 (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). The translation accuracy reported in Table 3, as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007), also shows significant improvement and approaches the quality achieved using gold standard data. For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). We report case-insensitive scores on version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006). The translation quality is measured by three MT evaluation metrics: TER (Snover et al, 2006), BLEU (Papineni et al, 2002), and METEOR (Lavie and Agarwal, 2007). We report case-insensitive scores for version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006). For evaluation of the given task, we have incorporated evaluation techniques based on current evaluation techniques used in machine translation, BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task. In this case, we ID Description1-4 n-gram precisions against pseudo references (1? n? 4) 5-6 PER and WER 7-8 precision, recall, fragmentation from METEOR (Lavie and Agarwal, 2007) 9-12 precisions and recalls of nonconsecutive bigrams with a gap size of m (1? m? 2) 13-14 longest common subsequences 15-19 n-gram precision against a target corpus (1? n? 5) Table 1: Feature sets for regression learning can easily retrain the learner under different conditions, therefore enabling our method to be applied to sentence-level translation selection fro many sets of translation systems without any additional human work. The translation accuracy as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) also shows improvement over baseline and approaches gold standard quality. This problem is similar to the task of automatic translation output evaluation and so we use METEOR (Lavie and Agarwal,2007), an automatic MT evaluation metric for com paring two sentences. The free parameters can be tuned to maximize correlation with various types of human judgments (Lavie and Agarwal, 2007). Table 1 compares the new HTER parameters to those tuned for other tasks including adequacy and fluency (Lavie and Agarwal, 2007) and ranking (Agarwal and Lavie, 2008). Our evaluation metric (METEOR-NEXT-hter) was tested against the following established metrics: BLEU (Papineni et al, 2002) with a maximum N gram length of 4, TER (Snover et al, 2006), versions of METEOR based on release 0.7 tuned for adequacy and fluency (METEOR-0.7-af) (Lavie and Agarwal, 2007), ranking (METEOR-0.7-rank) (Agarwal and Lavie, 2008), and HTER (METEOR-0.7-hter). In an MT evaluation setting, sense clusters have been integrated into an Mt evaluation metric (METEOR) (Lavie and Agarwal, 2007) and brought about an increase of the metric's correlation with human judgments of translation quality in different languages (Apidianaki and He, 2010). In particular, our framework might be useful with translation metrics such as TER (Snover et al, 2006) or METEOR (Lavie and Agarwal, 2007). In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with word sand hidden tree structures. The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavieand Agarwal, 2007) 12. Moreover, the overall BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) scores, as well as numbers of exact string matches (as measured against to the original sentences in the CCG bank) are higher for the hyper tagger-seeded realizer than for the preexisting realizer. This paper is structured as follows: Section 2 provides background on chart realization in Open CCG using a corpus-derived grammar. Instead of NIST scores, other MT evaluation scores can be plugged into this formula, such as METEOR (Lavie and Agarwal, 2007) for languages for which paraphrase data is available.
The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts. The motivation behind this task was that distinguishing factual and uncertain information in texts is of essential importance in information extraction. This paper provides a general overview of the shared task, including the annotation protocols of the training and evaluation datasets, the exact task definitions, the evaluation metrics employed and the overall results. The paper concludes with an analysis of the prominent approaches and an overview of the systems submitted to the shared task. The recent CoNLL-2010 shared task (Farkas et al, 2010), aimed at detecting uncertainty cues in texts, focused on these phrases in trying to determine whether sentences contain uncertain information. Whereas the CoNLL-2010 shared task (Farkas et al, 2010) annotated all occurrences of weasels as uncertainty markers, we acknowledge the possibility of sources (e.g. citations) that actually nullify the weasel. We are also interested in understanding whether, and which, linguistic features of the discussion are important for dispute detection. Drawing inspiration from studies of human mediation of on line conflicts (e.g. Billings and Watts (2010), Kittur et al (2007), Kraut and Resnick (2012)), we hypothesize that effective methods for dispute detection should take into account the sentiment and opinions expressed by participants in the collaborative endeavor. We extract the initial unigram, bigram, and trigram of each utterance as dis Lexical Features Syntactic/Semantic Featuresunigram/bigramunigram with POS tag number of words all uppercased dependency relation number of words Conversation Features Discourse Features quote overlap with target initial uni-/bi-/tri-gram TFIDF similarity with target repeated punctuations (remove quote first) hedging phrases collected from Sentiment Features Farkas et al (2010) connective+ sentiment words number of negators sentiment dependency relation sentiment words Table 2: Features used in sentence-level sentiment prediction.  We also compare with two state-of-the-art methods that are used in sentiment prediction for conversations: (1) an SVM (RBF kernel) that is employed for identifying sentiment-bearing sentences (Hassan et al, 2010), and (dis) agreement detection (Yin et al, 2012) in on line debates; (2) a Linear CRF for (dis) agreement identification in broadcast conversations (Wang et al, 2011). Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al, 2010) targeted negation mostly on those subfields. Councill et al (2010) present a supervised scope detector using their own annotation. As annotation tool, we use Jubilee (Choi et al,2010). This approach was first used by Morante et al (2008) and subsequently in many of the studies presented in the CoNLL-2010 Conference Shared Task (Farkas et al, 2010a), and is the one used in this paper. Task 2 of the CoNLL-2010 Conference Shared Task (Farkas et al, 2010b) proposed solving the problem of in-sentence hedge cue phrase identification and scope detection in two different domains (biological publications and Wikipedia articles), based on manually annotated corpora. The best result on hedge cue identification (Tanget al, 2010) obtained an F-score of 81.3 using a supervised sequential learning algorithm to learn BIOclasses from lexical and shallow parsing information, also including certain linguistic rules. For scope detection, Morante et al (2010) obtained an F-score of 57.3, using also a sequence classification approach for detecting boundaries (tagged in FOL format, where the first token of the span is marked with an F, while the last one is marked with an L). Similarly, Kilicoglu and Bergler (2010) used a pure rule-based approach based on constituent parse trees in addition to syntactic dependency relations, and achieved the fourth best F score for scope detection, and the highest precision of the whole task (62.5). The best results so far for this task used a token classification approach or sequential labelling techniques, as Farkas et al (2010b) note.   The goal of the CoNLL 2010 Shared Task (Farkas et al, 2010) was to develop linguistic scope detectors as well. The shared task at the 2010 Conference on Natural Language Learning (CoNLL) focused on speculation detection for the domain of biomedical research literature (Farkas et al, 2010), with data sets based on the BioScope corpus (Vincze et al, 2008) which annotates so called speculation cues along with their scopes. Prabhakaran et al (2010) report experiments with belief tagging, which in many ways is similar to factuality detection.
Improvements In Phrase-Based Statistical Machine Translation In statistical machine translation, the currently best performing systems are based in some way on phrases or word groups. We describe the baseline phrase-based translation system and various refinements. We describe a highly efficient monotone search algorithm with a complexity linear in the input sentence length. We present translation results for three tasks: Verbmobil, Xerox and the Canadian Hansards. For the Xerox task, it takes less than 7 seconds to translate the whole test set consisting of more than 10K words. The translation results for the Xerox and Canadian Hansards task are very promising. The system even outperforms the alignment template system. We use a phrase-based translation approach as described in (Zens and Ney, 2004). We extended the monotone search algorithm from (Zens and Ney, 2004) such that reorderings are possible. We exchange the baseline lexical scoring with a noisy-or (Zens and Ney, 2004) lexical scoring variant. The core of our engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004). There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table. Each includes relative frequency estimates and lexical estimates (based on Zens and Ney, 2004) of forward and backward conditional probabilities.  In order to complete the conversion from a pipeline approach to a joint approach, we fold our input segmentation step into the exact search framework by replacing a separate segmentation module (#2) with a monotone phrasal decoder (Zens and Ney, 2004). In the joint approach (Figure 1c), we perform segmentation and L2P prediction simultaneously by applying the monotone search algorithm developed for statistical machine translation (Zens and Ney, 2004). (Logic MONOTONE) This is the algorithm of Zens and Ney (2004). The First d Uncovered Words strategy (FdUW) is described by Tillman and Ney (2003) and Zens and Ney (2004), who call it the IBM Constraint. In (Zens and Ney, 2004) the downhill simplex method is used to estimate the weights; around 200 iterations are required for convergence to occur.  For tractability, we followed standard practice with this technique and considered only monotonic alignments when decoding (Zens and Ney, 2004). combination method (Zens and Ney, 2004) which has shown good performance in calculating similarities between bags-of-words in different languages. This finding fails to echo the promising results in the previous study (Zens and Ney,2004). The source text, annotated with name translations, is then passed to a statistical, phrase-based MT system (Zens and Ney, 2004). The following methods were investigated: (Monotone) Phrase-based MT on character level: A state-of-the-art phrase-based SMT system (Zens and Ney, 2004) was used for name transliteration, i.e. translation of characters instead of words. We use the RWTH Aachen Chinese-to-English statistical phrase-based machine translation system (Zens and Ney, 2004) for these purposes.
Statistical Sentence Condensation Using Ambiguity Packing And Stochastic Disambiguation Methods For Lexical-Functional Grammar We present an application of ambiguity packing and stochastic disambiguation techniques for Lexical-Functional Grammars (LFG) to the domain of sentence condensation. Our system incorporates a linguistic parser/generator for LFG, a transfer component for parse reduction operating on packed parse forests, and a maximum-entropy model for stochastic output selection. Furthermore, we propose the use of standard parser evaluation methods for automatically evaluating the summarization quality of sentence condensation systems. An experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings. Overall summarization quality of the proposed system is state-of-the-art, with guaranteed grammaticality of the system output due to the use of a constraint-based parser/generator. Furthermore, we consider an F-score measure that is adapted from dependency-based parsing (Crouch et al., 2002) and sentence-condensation (Riezler et al,2003). The intrinsic evaluation measures used in our experiments are the well-known BLEU (Papineni et al., 2001) and NIST (Doddington, 2002) metrics, and an F-score measure that adapts evaluation techniques from dependency-based parsing (Crouch et al., 2002) and sentence-condensation (Riezler et al, 2003) to machine translation. We stand in a marked contrast to previous 'grafting' approaches which more or less rely on an ad-hoc collection of transformation rules to generate candidates (Riezler et al, 2003).  Our results show that grammatical relations based F-score (Riezler et al 2003) correlates reliably with human judgements and could thus be used to measure compression performance automatically. Riezler et al (2003) present a discriminative sentence compressor over the output of an LFG parser that is a packed representation of possible compressions. It is an important and growing field of natural language processing with applications in areas such as transfer based machine translation (Riezler and Maxwell, 2006) and sentence condensation (Riezler et al, 2003). Riezler et al (2003) applied linguistically rich LFG grammars to a sentence compression system. One of the most widely used automatic metrics is the F1 measure over grammatical relations of the gold standard compressions (Riezler et al, 2003). As an automated metric of quality, we compute F-score based on grammatical relations (relational F1, or RelF1) (Riezler et al, 2003). We also report results using F1 computed over grammatical relations (Riezler et al, 2003). The first evaluation is dependency base devaluation same as Riezler et al (2003).
Probabilistic CFG With Latent Annotations This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA. This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables. Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm. Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared. In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F, sentences 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection. Matsuzaki et al (2005) independently introduce a similar approach and present empirical results that rival ours.  These scores are the same as the variational rule scores of Matsuzaki et al (2005).  Matsuzaki et al (2005) introduced a model for such learning: PCFG-LA.  Just as Collins manually split the S nonterminal label into S and SG for sentences with and without subjects, Matsuzaki et al (2005) split S into S [1], S [2],. Before extracting the backbone PCFG and running the constrained inside-outside (EM) training algorithm, we preprocessed the Treebank using center-parent binarization Matsuzaki et al (2005). Matsuzaki et al (2005) used a markovized grammar to get a better unannotated parse forest during decoding, but they did not markovize the training data. Matsuzaki et al (2005) note that the best annotated parse is in fact NP-hard to find.  'Basic' models are trained on a non-markovized tree bank (as in Matsuzaki et al (2005)); all others are trained on a markovized tree bank. With these techniques we reach a parsing accuracy similar to Matsuzaki et al (2005), but with an order of magnitude less parameters, resulting in more efficient parsing. We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al, 2005), or secondary linksMij (not constrainedby TREE/PTREE) that augment the parse with representations of control, binding, etc.   These expectations can be easily computed from the inside/outside scores, similarly as in the maximum bracket recall algorithm of Goodman (1996), or in the variational approximation of Matsuzaki et al (2005). The tree bank data is right-binarized (Matsuzaki et al, 2005) to construct grammars with only unary and binary productions. Later, automated methods for nonterminal refinement were introduced, first splitting all categories equally (Matsuzaki et al, 2005), and later refining nonterminals to different degrees (Petrov et al,2006) in a split-merge EM framework. The resulting memory limitations alone can prevent the practical learning of highly split grammars (Matsuzaki et al, 2005).
Exploiting Shallow Linguistic Information For Relation Extraction From Biomedical Literature We propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information. We use a combination of kernel functions to integrate two different information sources: (i) the whole sentence where the relation appears, and (ii) the local contexts around the interacting entities. We performed experiments on extracting gene and protein interactions from two different data sets. The results show that our approach outperforms most of the previous methods based on syntactic and semantic information. We propose a hybrid kernel by combining the proposed feature based kernel (outlined above) with the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) and the Path-enclosed Tree (PET) kernel (Moschitti, 2004). An interesting finding is that the Shallow Linguistic (SL) kernel (Giuliano et al 2006) (to be discussed in Section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings. The Shallow Linguistic (SL) kernel was proposed by Giuliano et al (2006).   A similar finding can be seen, for example, in the relatively flat learning curve of Giuliano et al (2006).  The former approach can produce higher performance: the evaluation of Giuliano et al (2006) includes both alternatives, and their method achieves an F-score of 63.9% under the former criterion, which they term One Answer per Relation in a given Document (OARD). Our method outperforms most studies using similar evaluation methodology, with the exception being the approach of Giuliano et al (2006). Airola et al. (2008) repeat the method published by Giuliano et al (2006) with a correctly preprocessed AIMed and reported an F1-score of 52.4%.  In addition to word features, Giuliano et al (2006) extract shallow linguistic information such as POS tag, lemma, and orthographic features of tokens for PPI extraction. On the LLL data set, the LA method using distributional similarity measures significantly outperforms both baselines and also yields better results than an approach based on shallow linguistic information (Giuliano et al, 2006). Giuliano et al (2006) use no syntactic information.  In contrast, work reported in (Giuliano et al, 2006) does not make use of syntactic information which on the data without coreferences yields higher recall. For RE, we use AImed, previously used to train protein interaction extraction systems ((Giuliano et al, 2006)). We use the KGC kernel from (Giuliano et al, 2006), one of the highest-performing systems on AImed to date and perform 10-fold cross validation. The starting point of our research is an approach for identifying relations between named entities exploiting only shallow linguistic information, such as tokenization, sentence splitting, part-of-speech tagging and lemmatization (Giuliano et al, 2006). Bunescu and Mooney (2005) and Giuliano et al (2006) successfully exploited the fact that relations between named entities are generally expressed using only words that appear simultaneously in one of the following three contexts.
A Linear Observed Time Statistical Parser Based On Maximum Entropy Models 3. A search heuristic which attempts to find the highest scoring parse tree for a given input sentence. Abstract This paper presents a statistical parser for natural language that obtains a parsing accuracy—roughly 87% precision and 86% recall—which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. running time of the parser on test sentence linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring a dramatically higher accuracy of 93% precision and recall. In this section, we investigate the performance of two maximum entropy classifiers (Ratnaparkhi, 1997), one for determining whether a noun phrase has a determiner or not and the other for selecting the appropriate determiner if one is needed. There are two canonical parsers that fall into this category: the decision-tree parser of (Magerman, 1995), and the maximum-entropy parser of (Ratnaparkhi, 1997). The closest relative of our framework is the maximum-entropy parser of Ratnaparkhi (Ratnaparkhi, 1997). The Chinese parse trees are produced by a maximum entropy based parser (Ratnaparkhi, 1997). We demonstrate this by comparing our k-best lists to those in (Ratnaparkhi,1997), (Collins, 2000) and the parallel work by Charniak and Johnson (2005) in several ways, including oracle reranking and average number of found parses. Ratnaparkhi (1997) introduced the idea of oracle re ranking: suppose there exists a perfect re ranking scheme that magically picks the best parse that has the highest F-score among the top k parses for each sentence.  A maximum entropy parser (Ratnaparkhi, 1997) parser is then built and tested. The maximum entropy parser (Ratnaparkhi, 1997) is used in this study, for it offers the flexibility of integrating multiple sources of knowledge into a model. The maximum entropy parser (Ratnaparkhi, 1997) parses a sentence in three phases: (1) it first tags the input sentence. The SLM was trained on 20M words of WSJ text automatically parsed using the parser in (Ratnaparkhi, 1997), binarized and enriched with headwords and NT/POS tag information as explained in Section 2.2 and Section 3. The techniques of Charniak (1997), Collins (1997), and Ratnaparkhi (1997) achieved roughly comparable results using the same sets of training and test data. The probability models of Charniak (1997), Magerman (1995) and Ratnaparkhi (1997) dier in their details but are based on similar features. Even though some parsers effectively exhibit linear behavior in sentence length (Ratnaparkhi, 1997), fast statistical parsers such as (Henderson, 2004) still take around 1.5 seconds for sentences of length 35 in tests that we made. This is a true pipeline approach, as was done in other successful parsers, e.g. (Ratnaparkhi, 1997), in that the classifiers are trained on individual decisions rather than on the overall quality of the parser, and chained to yield the global structure. Finally, our parser is in many ways similar to the parser of Ratnaparkhi (1997).  On the application side, (log) linear parsing models have the potential to supplant the currently dominant lexicalized PCFG models for parsing by allowing much richer feature sets and simpler smoothing, while avoiding the label bias problem that may have hindered earlier classifier-based parsers (Ratnaparkhi, 1997). The English parse tree used for the syntactic reordering was produced by a maximum entropy based parser (Ratnaparkhi, 1997). Chunks as a separate level have also been used in Collins (1996) and Ratnaparkhi (1997).
Parsing Strategies with Lexicalized Grammars: Appl icat ion to Tree Adjoining Grammars * Yves SCHABES,  Anne  ABE ILLE**and  Arav ind  K. JOSHI Department of Computer and Information Science University of Pennsylvania Philadelphia PA 19104-6389 USA schabes~linc.cis.upenn.edu abeille~cis.upenn.edu joshi~eis.upenn.edu ABSTRACT In this paper we present a general parsing strategy that arose from the development of an Earley-type parsing al- gorithm for TAGs (Schabes and Joshi 1988) and from re- cent linguistic work in TAGs (Abeille 1988).  Consequently, lexicalized grammars offer significant parsing benefits (Schabes et al, 1988) as the number of applications of productions (i.e., derivation steps) is clearly bounded by the length of the input string. Restrictions on possible relations are attached to the words ,e.g., expressed as valencies in the case of dependency relations, yielding a strictly lexicalized grammar in the sense of Schabes et al (1988). The lexical distribution of grammatical knowledge one finds in many lexiealized grammar formalisms (e.g., LTAGS (Schabes et al, 1988) or HPSG (Pollard& amp; Sag, 1994)) is still constrained to declarative notions. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982), Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al, 1988), Head driven Phrase Structure Grammar (HPSG) (Pollardand Sag, 1994) and Combinatory Categorial Grammar (CCG) (Steedman, 2000), which represent deep syntactic structures that can not be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing.  However, virtually all Tree Adjoining Grammars (TAG ,seee.g., (Schabes et al, 1988)) used in NLP applications can (almost) be seen as lexicalized Tree Insertion Grammars (TIG), which can be converted into strongly equivalent CFGs (Schabes and Waters,1995). They employed a CCG (Steedman, 2000) or LTAG (Schabes et al, 1988) parser to acquire syntactic/semantic structures, which would be passed to statistical classifier as features. Various parsing techniques have been developed for lexicalized grammars such as Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al,1988), and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994). Head-Driven Phrase Structure Grammar (HPSG) is classified into lexicalized grammars (Schabes et al, 1988). The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al, 1988)) for generating a word lattice containing realizations and selects the best one using a trigram model. White and Baldridge (2003) developed a chart generator for CCG (Combinatory Categorial Grammar (Steedman, 2000)) and proposed several techniques for efficient generation such as best-first search, beam thresholding and chunking the input logical forms (White, 2004). Conventional approaches to subcategorization, such as Definite Clause Grammar (Pereira and Warren, 1980), Categorial Grammar (Ades and Steedman, 1982), PATR-II (Shieber, 1986), and lexicalized TAG (Schabes et al 1988) all deal with complementation by including in one form or another a notion of& quot; subcategorization frame& quot; that specifies a sequence of complement phrases and constraints on them. Derivation trees, the structural description in LTAG (Schabes et al, 1988), represent the association of lexical items i.e., elementary trees. In lexicalized grammatical formalisms such as Lexicalized Tree Adjoining Grammar (Schabes et al, 1988, LTAG), Combinatory Categorial Grammar (Steedman, 2000, CCG) and Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994, HPSG), it is possible to separate lexical category assignment? the assignment of informative syntactic categories to linguistic objects such as words or lexical predicates? from the combinatory processes that make use of such categories? such as parsing and surface realization.
Extracting Product Features And Opinions From Reviews Consumers are often forced to wade through many on-line reviews inorder to make an informed prod uct choice. This paper introducesOPINE, an unsupervised informationextraction system which mines reviews in order to build a model of important product features, their evalu ation by reviewers, and their relative quality across products. Compared to previous work, OPINE achieves 22% higher precision (with only 3% lower recall) on the feature extraction task. OPINE?s novel use ofrelaxation labeling for finding the semantic orientation of words in con text leads to strong performance on the tasks of finding opinion phrases and their polarity. Popescu and Etzioni (2005) achieved high-precision opinion phrases extraction by using relaxation labeling. (Popescu and Etzioni, 2005) and (Qiu et al, 2011) designed syntactic patterns to perform this task. Popescu and Etzioni (2005) proposed a relaxed labeling approach to utilize linguistic rules for opinion polarity detection.   A more NLP-oriented approach is proposed in (Popescu and Etzioni 2005), where noun phrases are extracted from online user reviews.  Popescu and Etzioni (2005) investigated the same problem. The dictionary-based method utilizes Wikipedia (Popescu and Etzioni, 2005) to find an entry page for a phrase or a single term in a query.  In (Popescu and Etzioni, 2005), Popescu and Etzioni not only analyzed polarity of opinions regarding product features but also ranked opinions based on their strength. Popescu and Etzioni (2005) present a method that identifies product features for using corpus statistics, WordNet relations and morphological cues. The relevance ranking and extraction was then performed with different statistical measures: Pointwise Mutual Information (Popescu and Etzioni, 2005), the Likelihood Ratio Test (Yi et al, 2003) and Association Mining (Hu and Liu, 2004).   Unlike most previous work on polarity classification, which has largely focused on exploiting adjective-noun (AN) relations (e.g., Dave et al (2003), Popescu and Etzioni (2005)), we hypothesized that subject-verb (SV) and verb-object (VO) relations would also be useful for the task. Common solutions to this problem involve clustering with the help of knowledge-rich methods, involving manually-constructed rules, semantic hierarchies, or both (e.g., Popescu and Etzioni 2005, Fahrni and Klenner 2008). In the IE setting, Popescu and Etzioni (2005) extract frequent terms, and cluster them into aspects.  Popescu and Etzioni applied relaxation labeling to polarity identification (Popescu and Etzioni, 2005).
Co-Training for Cross-Lingual Sentiment Classification The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classification. However, there are many freely available English sentiment corpora on the Web. This paper focuses on the problem of cross-lingual sentiment classification, which leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data. Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem. We propose a cotraining approach to making use of unlabeled Chinese data. Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers. To overcome the shortcomings of available resources and to take advantage of ensemble systems, Wan (2008) and Wan (2009) explored methods for developing a hybrid system for Chinese using English and Chinese sentiment analyzers. To reduce this kind of error introduced by the translator, Wan in (Wan, 2009) applied a co-training scheme. But as the conditional distribution can be quite different for the original language and the pseudo language produced by the machine translators, these two strategies give poor performance as reported in (Wan, 2009). For comparsion, we use the same data set in (Wan, 2009): Test Set (Labeled Chinese Reviews): The data set contains a total of 886 labeled product reviews in Chinese (451 positive reviews and 435 negative ones). The features we used are bigrams and unigrams in the two languages as in (Wan, 2009). We compare our procedure with the co-training scheme reported in (Wan, 2009): CoTrain: The method with the best performance in (Wan, 2009). More recently, Wan (2009) proposed a co training approach to tackle the problem of cross lingual sentiment classification by leveraging an available English corpus for Chinese sentiment classification. Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). A related, yet more sophisticated technique is proposed in (Wan,2009), where a co-training approach is used to leverage resources from both a source and a target language. Wan (2009) combined the annotated English reviews, unannotated Chinese reviews and their translations to co-train two separate classifiers for each language, respectively. In the document-level review polarity classification experiment, we used the dataset adopted in (Wan, 2009). In the review polarity classification experiment, we use unigram, bigram of Chinese words as features which is suggested by (Wan, 2009). The original annotations 1104 can be produced either manually or automatically. Wan (2009) constructs a multilingual classifier using co-training. Methods which follow this two step approach include the EM-based approach by Rigutini et al (2005), the CCA approach by Fortuna and Shawe-Taylor (2005), the information bottleneck approach by Ling et al (2008), and the co-training approach by Wan (2009). The proposed co-regression algorithm can make full use of both the features in the source language and the features in the target language in a unified framework similar to (Wan 2009). Wan (2009) constructs a multilingual classifier using co-training. Wan (2009) proposes to use ensemble method to train better Chinese sentiment classification model on English labeled data and their Chinese translation. SVM: We train a SVM classifier on the Chinese labeled data.MT-Cotrain: This is the co-training based approach described in (Wan, 2009). Wan (2009) also leveraged an available English corpus for Chinese sentiment classification by using the co-training approach to make full use of both English and Chinese features in a unified framework. Sentiment classification can be performed on words, sentences or documents, and is generally categorized into lexicon-based and corpus-based classification method (Wan, 2009).
Using Emoticons To Reduce Dependency In Machine Learning Techniques For Sentiment Classification Sentiment Classification seeks to identify a piece of text according to its author’s general feeling toward their subject, be it positive or negative. Traditional machine learning techniques have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the training and test data with respect to topic. This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with training data labeled with emoticons, which has the potential of being independent of domain, topic and time. In sentiment analysis research, Read (2005) used emoticons in newsgroup articles to extract instances relevant for training polarity classifiers. We therefore experiment with multiple such conventions with apparently similar meanings - here, emoticons (following (Read, 2005)) and Twitter hash tags - allowing us to examine the similarity of classifiers trained on independent labels but intended to detect the same underlying class. The regulating aspects of semantic orientation of a text are natural language context information (Pang et al, 2002) language properties (Wiebe and Mihalcea, 2006), domain pragmatic knowledge (Aue and Gamon, 2005) and lastly most challenging is the time dimension (Read, 2005). We have built a corpus of tweets written in English following the procedure described in (Read, 2005) and (Go et al, 2009). According to (Read, 2005), when authors of an electronic communication use an emotion, they are effectively marking up their own text with an emotional state. The regulating aspects which govern the lexical level semantic orientation are natural language context (Pang et al, 2002), language properties (Wiebe and Mihalcea, 2006), domain pragmatic knowledge (Aue and Gamon, 2005), time dimension (Read, 2005), colors and culture (Strapparava and Ozbal, 2010) and many more unrevealed hidden aspects. We seed the graph using the polarity values in the OpinionFinder lexicon (Wilson et al, 2005), the known polarity of emoticons, and a maximum entropy classifier trained on 1.8 million tweets with automatically assigned labels based on the presence of positive and negative emoticons, like Read (2005) and Go et al (2009). The regulating aspects which govern the lexical level semantic orientation are natural language context (Pang et al, 2002), language properties (Wiebe and Mihalcea, 2006), domain pragmatic knowledge (Aue and Gamon, 2005), time dimension (Read, 2005), colors and culture (Strapparava and Ozbal, 2010) and many more unrevealed hidden aspects. It is not a static sentiment lexicon set [polarity changes with time (Read, 2005)] as it is updated regularly. We use emoticons as indicators of an emotion (Read, 2005) to automatically classify texts into positive or negative sets. The approach is similar to the one in (Read, 2005).
Lexicalized Markov Grammars for Sentence Compression We present a sentence compression system based on synchronous context-free grammars (SCFG), following the successful noisy-channel approach of (Knight and Marcu, 2000). We define a headdriven Markovization formulation of SCFG deletion rules, which allows us to lexicalize probabilities of constituent deletions. We also use a robust approach for tree-to-tree alignment between arbitrary document-abstract parallel corpora, which lets us train lexicalized models with much more data than previous approaches relying exclusively on scarcely available document-compression corpora. Finally, we evaluate different Markovized models, and find that our selected best model is one that exploits head-modifier bilexicalization to accurately distinguish adjuncts from complements, and that produces sentences that were judged more grammatical than those generated by previous work. Modifications of this model are reported in Turner and Charniak (2005) and Galley and McKeown (2007) with improved results. (Galley and McKeown, 2007) extended the noisy-channel approach and proposed a head-driven Markovization formulation of synchronous context free grammar (SCFG) deletion rules. The last sentence compression method we use is the lexicalized Markov grammar-based approach (Galley and McKeown, 2007) with edit word detection (Charniak and Johnson, 2001).  Where the tree pairs are isomorphic, synchronous context-free grammars (SCFG) may suffice, but in general, non-isomorphism can make the problem of rule extraction difficult (Galley and McKeown,2007). Galley and McKeown (2007) show improvements to the noisy-channel approach based on rule lexicalization and rule Markovization. We use the sentence pairs available in the Ziff-Davis Tree Alignment corpus (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content.
Recognizing Contextual Polarity In Phrase-Level Sentiment Analysis This paper presents a new approach to phrase-level sentiment analysis that firstdetermines whether an expression is neu tral or polar and then disambiguates the polarity of the polar expressions. With thisapproach, the system is able to automat ically identify the contextual polarity for a large subset of sentiment expressions,achieving results that are significantly bet ter than baseline. Wilson et al (2005) proposed supervised learning, dividing the resources into prior polarity and context polarity, which are similar to polar atoms and syntactic patterns in this paper, respectively. For example, AbuJbara et al (2013) and Jochim and Schutze (2012) find the list of polar words from Wilson et al (2005) to be useful, and neither study lists dependency relations as significant features. In addition, we used subjectivity clues extracted from the lexicon by Wilson et al (2005). Wilson et al (2005) present a two-step process to recognize contextual polarity that employs machine learning and a variety of features. Some lexicons assign real number scores to indicate sentiment orientations and strengths (i.e. probabilities of having positive and negative sentiments) (Esuli and Sebastiani, 2006) while other lexicons assign discrete classes (weak/strong, positive/negative) (Wilson et al, 2005).  Further, there are analyses (Wiebe et al, 2005) and experiments (Wilson et al, 2005) that indicate that lexicon-lookup approaches to subjectivity analysis will have limited success on general texts. The former were based on the General Inquirer lexicon (Wilson et al, 2005), the MontyLingua part-of-speech tagger (Liu, 2004) and co-occurrence statistics of words with a set of predefined reference words. More specifically, we use the terms in the lexicon constructed from (Wilson et al, 2005) as the indicators to identify the substructures for the convolution kernels, and extract different sub-structures according to these indicators for various types of parse trees (Section 3). To solve this problem, we define the indicators in this task as subjective words in a polarity lexicon (Wilson et al, 2005). We use a manually constructed polarity lexicon (Wilson et al, 2005), in which each entry is annotated with its degree of subjectivity (strong, weak), as well as its sentiment polarity (positive, negative and neutral).  PRIOR-POLARITY & PRIOR-INTENSITY: We obtain these prior-attributes from the polarity lexicon populated by Wilson et al (2005).  Polar word Count (PC) Number of words that are polar (strong subjective words from the lexicon (Wilson et al, 2005)). We also used two datasets for the evaluation purpose: the MPQA (Wilson et al, 2005) and IQAPs (Marneffe et al, 2010) datasets. In this work we use MPQA (Wilson et al, 2005). The MPQA lexicon contains separate lexicons for subjectivity clues, intensifiers and valence shifters (Wilson et al, 2005), which are used for identifying opinion roots, modifiers and negation words. They use the Opinion Finder lexicon (Wilson et al, 2005) and two bilingual English-Romanian dictionaries to translate the words in the lexicon. To generate the initial explanations, one can use an off-the shelf sentiment classifier such as OpinionFinder2 (Wilson et al, 2005).
Efficient Unsupervised Discovery Of Word Categories Using Symmetric Patterns And High Frequency Words We present a novel approach for discovering word categories, sets of words sharing a significant aspect of their meaning. We utilize meta-patterns of highfrequency words and content words in order to discover pattern candidates. Symmetric patterns are then identified using graph-based measures, and word categories are created based on graph clique sets. Our method is the first pattern-based method that requires no corpus annotation or manually provided seed patterns or words. We evaluate our algorithm on very large corpora in two languages, using both human judgments and WordNetbased evaluation. Our fully unsupervised results are superior to previous work that used a POS tagged corpus, and computation time for huge corpora are orders of magnitude faster than previously reported.  In Section 2, we describe a corpus statistics approach, previously applied for web mining (Davidov and Rappoport, 2006), which we extend for relation discovery. To discover them, we use a slightly modified version of the method presented in (Davidov and Rappoport, 2006). It also significantly outperforms the single-language pattern-based method introduced by (Davidov and Rappoport, 2006), which achieves average precision of 79.3 on a similar set in English (in comparison to 86.7 in this study). Following (Davidov and Rappoport, 2006), we classified words into high-frequency words (HFWs) and content words (CWs). Unlike (Davidov and Rappoport, 2006), we consider all punctuation characters as HFWs. To specify patterns, following (Davidov and Rappoport, 2006) we classify words into high frequency words (HFWs) and content words (CWs). We discover such words by scanning our corpora and querying the web for symmetric patterns (obtained automatically from the corpus as in (Davidov and Rappoport, 2006)) that contain w1 or w2. Davidov and Rappoport (2006) developed a framework which discovers concepts based on high frequency words and symmetry-based pattern graph properties. Our algorithm is based on the concept acquisition method of (Davidov and Rappoport, 2006).  Our basic comparison was to (Davidov and Rappoport, 2006) (we have obtained their data and utilized their algorithm), where we can estimate if incorporation of parser data can solve some fundamental weaknesses of their framework. The Russian corpus (Davidov and Rappoport, 2006) was assembled from web-based Russian repositories, to yield 33GB and 4G words. All of these corpora were also used by (Davidov and Rappoport, 2006) and BNC was used in similar settings by (Widdows and Dorow, 2002). We have improved the evaluation framework for Russian by using the Russian WordNet (Gelfenbeyn and et al, 2003) instead of back-translations as done in (Davidov and Rappoport, 2006). We do this as follows, essentially implementing a simplified version of the method of Davidov and Rappoport (2006). It was shown in (Davidov and Rappoport, 2006) that pairs of words that often appear together in such symmetric patterns tend to belong to the same class (that is, they share some notable aspect of their semantics). Note that our method differs from that of Davidov and Rappoport (2006) in that here we provide an initial seed pair, representing our target concept, while there the goal is grouping of as many words as possible into concept classes. For automated extraction of patterns, we followed the pattern definitions given in (Davidov and Rappoport, 2006). Unlike (Davidov and Rappoport, 2006), we consider all single punctuation characters or consecutive sequences of punctuation characters as HFWs.
Introduction To The CoNLL-2000 Shared Task: Chunking  This data set was used for CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000). Text chunking consists of dividing text into syntactically related non overlapping groups of words (Tjong Kim Sang and Buchholz, 2000). Unlike the shallow phrases defined for the CoNLL-2000 Shared Task (Tjong Kim Sang and Buchholz, 2000), base phrases correspond directly to constituents that appear in full parses, and hence can provide a straightforward constraint on edges within a chart parser. For the chunker, we ran fnTBL over the lemmatised tagged data, training over CoNLL 2000 style (Tjong Kim Sang and Buchholz, 2000) chunk converted versions of the full Brown and WSJ corpora. The data set, extracted from the WSJ Penn Tree bank, and first used in the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000), contains 211,727 training examples and 47,377 test instances. We annotate the same set of 800 tweets mentioned previously with tags from the CoNLL shared task (Tjong Kim Sang and Buchholz,2000). Our chunks were derived from the Tree bank trees using the conversion described by Tjong Kim Sang and Buchholz (2000). The shallow parse tags define non hierarchical base constituents (chunks), as defined for the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000). For the chunker, we ran fnTBL over the lemmatised tagged data, training over CoNLL 2000 style (Tjong Kim Sang and Buchholz, 2000) chunk converted versions of the full Brown and WSJ corpora. (Tjong Kim Sang and Buchholz, 2000) Unlike a parse tree, a set of syntactic chunks has no hierarchical information on how sequences of words relate to each other. We used the same data set as the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000). The project provided a data set for this task at the CoNLL-2000 workshop (Tjong Kim Sang and Buchholz, 2000). Chunking was the shared task of CoNLL-2000, the workshop on Computational Natural Language Learning, held in Lisbon, Portugal in 2000 (Tjong Kim Sang and Buchholz, 2000). The task was extended to additional phrase types for the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000), which is now the standard evaluation task for shallow parsing. NP chunking results have been reported on two slightly different data sets: the original RM data set of Ramshawand Marcus (1995), and the modified CoNLL-2000 version of Tjong Kim Sang and Buchholz (2000). Headwords are obtained from a parse tree with the script used for the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000). In the chunk inventory devised for the CoNLL-2000 test chunking shared task (Tjong Kim Sang and Buchholz, 2000), a dedicated particle chunk type once again exists. Indeed, all the best systems in the CoNLL shared task competitions (e.g. Chunking (Tjong Kim Sang and Buchholz, 2000)) make extensive use of lexical information. The conversion program is the same as used for the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000). Tjong Kim Sang and Buchholz (2000) give an overview of the CoNLL shared task of chunking.
Multitext Grammars And Synchronous Parsers translational equivalence between their components. link generated by a D-MTG has components. Some (but not all) components of a link may be empty. An empty component indicates that an expression vanishes in translation. To express empty we add a special terminal to special nonterminal MTG applications, the different components of a link will typically come largely disjoint subsets of vocabularies or sets of grammatical categories from different languages. Each MTG also has a set of production rules (or just &quot;productions&quot; for short), which fall into one of two categories.' YIELD productions have the form t X is a link of and t is a link of is empty if and only if empty, < D. productions have the form X=NPM (2) where M is a non-empty vector of nonterminal links, is a non-empty vector of and is a rendering function, explained The rank of an MTG production is the number of nonterminal links on its RHS. The rank of an MTG is the maximum rank of its production rules. is the class of MTGs of rank Each row of P and M corresponds to a different component of multitext. Each permutation is written as a row in P, and each link is written as a column M, as in Equation 3 below. If empty, then the dth component of every link in M must be empty If not empty, then at least one of the links in M must have a non-empty dth component. The position of a non-empty terminal or nonterminal relative to other non-empty elements of its component is its role. If there are m non-empty nonterminals component (row) M then a permutation roles from 1 to Pd empty if and only if is empty. The D-MTG derivation process begins with the link $, which is a vector of of the start symbol $ derivation continues with nondeterministic application of production rules. The semantics of = are the usual semantics of rewriting systems, i.e., that the expression on the LHS can be rewritten as the expression on the RHS. Following convention, we let be the reflexive transitive closure of dichotomy imposes a convenient normal form, without loss of generality. rendering function is a notational convenience; MTGs can be defined without it. When no more productions can be applied, i.e., when all nonterminals have been rewritten into terminals, the rendering functions are evaluated in order. The rearranges the nonempty terminals in each row of a link vector according to that row's permutation. For example, c [1,3,2,4] =wxyz t Env vut By reordering the terminals independently in each component, the join operator hides information about which terminals were derived from the same link. Thus, the translational equivalence represented by links is not observable in MTG yields, just as it is not observable in raw multitext. To avoid spurious ambiguity, we stipulate a normal form for components of P: In each permutation, first appearance of role precede the first of role y for all y, except where the arrangement is incompatible with a preceding permutation in P. We could, for example, obtain the result above if we put put and switch their roles in the 2nd and 3rd permutations. However, the normal form requires the 2nd to be [1, 3, 2, 4], not [4, 3, 2, 1], so must be listed last. Let Q be an MTG derivation where no more production rules can be applied. Let Render(Q) be the result of evaluating all the N's in Q. The (formal) an MTG the set of multithat can be generated by applying zto the link of then evaluating all the joins. I.e., = : $ z Due to the importance of lexical information in disambiguating syntactic structure, we shall pay special attention to lexicalized MTGs (LMTGs) of the variety A bilexical MTG has a set A of &quot;delexicalized&quot; nonterminal labels. Intuitively, A corresponds to the nonterminal set of an CFG. Then, every nonterminal in form some terminal some label The terminal the lexical head of its constituent, or just the head. One link on the RHS each production serves as the heir of the link on the LHS. Each component of the heir link inherits the lexical head of its parent nonterminal. An of a derivation is in Figure 1. nonterminal always lexicalized with the ternonterminals may also be lexicalized represent empty categories. The special start nonterminal $ is lexicalized with the special start terminal S. Following Eisner ..4z Satta (1999), we can then that the language of interest is actually : Q'$ E (3) ix[1, 2] (S[fed] $[S] 0.1[1,2] [1, 2, 3] ( NP[cat] [1, 2] S[kormil] [1, 2] [1,3, 2] NP[kota]) S[S]) 0.1[1, 2] [1, 0.1[1,2] N[cat] 2] [1, 3, 2] Pro[ya] V[kormil] [1] ) S[S]) ix[1, 2] [1, 2, 3] ( I fed ) (the cat )) $) 0.1[1, 2] ( (I fed the cat) $) I fed the cat $ 2] [1, 3, 2] ya kormil [1] $ kota kormil) s) kota kormil Figure 1: A 2-L2MTG derivation in English and transliterated Russian: (4-5) DEPEND productions; (6) YIELD productions, followed by rendering. and superclasses of MTG have been studied before. The non-lexicalized 2-MTG(2) is equivalent to ITG (Wu, 1997). Alal. &quot;collections of finite-state head transducers&quot; can be viewed as a subclass of 2-LMTG where, among other restrictions, A contains only one (dummy) nonterminal label. &quot;Syntax-directed transof order 1969) are equivalent to k-MTG(2). On the other hand, MTG is a subof Multiple CFG (Seki al., where the functions that render the RHS of production rules may not mix symbols from different components. 3 Synchronous Parsers Inference of synchronous structures requires a synparser. A parser an algorithm that can infer the syntactic structure of each component text in a multitext and simultaneously infer the correspondence relation between these struc- To facilitate complexity analysis (below), we specify our parsers using declarative inference rules.' &quot;X :- Y, Z&quot; means that X can be inferred from Y and Z. V means the same thing. An item that appears in an inference rule stands for the proposition that the item is in the parse chart. A production rule that appears in an inference rule stands for the proposition that the production is in the grammar. Such specifications are nondeterministic: they do not indicate the order in which a parser should attempt inferences. A deterministic parsing strategy can always be chosen later, to suit the application. Any reasonable parsing strategy will have the same asymptotic complexity (McAllester, 2002). 3.1 Naive Synchronous Bilexical Parsers For expository purposes, we begin with Parser R2D2A, which is a naive CKY-style parser for The chart of suitable set of monolingual parsers can also infer the syntactic structure of each component, but cannot infer the correspondence relation between these structures. use both Horn clauses and sequents to save space. R2D2A can be compared to Wu (1997)'s procedure for parsing non-lexicalized ITGs, which runs in —1 22 j1 j1 j2 X7. [hl] X2 [h2] X2 [h2]/Z2 [h2] i2 —1 iz hook seeds hedge Figure 2: Items used by our parsers for 2-L2MTG(2). R2D2A is with &quot;seed&quot; items, illustrated in Figure 2. A one-dimensional seed is put in the chart for every word in every component of the input. After initialization, the parser can translational equivalence between seeds components by firing Y inference rules: —1 —1 hi i2 inference rules infer rules. Each two-dimensional instantiation expresses the equivalence of two word tokens, at positions and in their respective components. One-dimensional Y inferences assert that a word vanishes in translation. E.g. : i2 6[6] i2 i2 X2 [h2] ' 21_2 [(12] 122 „ L R2D2A most of its time compospairs of non-seed items into larger A bottom-up one-dimensional parser composes onedimensional items until it infers an item that covers the input text. A bottom-up synchronous parser composes multi-dimensional items until it infers an item that covers the multitext space spanned by the input multitext. The items composed by synchronous parsers are called short. The 2D hedges composed by Parser R2D2A are shown in Figure 2. The particular hedge in the figure represents a constituent beword boundaries ji of the first compo- As Eisner & Satta (1999) have shown, yields of bilexical grammars are generally more expensive to parse than their nonlexicalized counterparts. term to any partial parse. [pi.] [p2] assert in different X1 [hl] X2 [h2] i2 —1 h2 X2 [h2] h2 7 22 1h Z h 2 2 Z h 1 1 Z h 2 2 1 Y g 1 Y g 1 1 2g 2g Y g 1 1 Z h 1 1 Z h 2 2 Z h 1 1 Z h 2 2 2g Y g 1 1 2g I go there quite often 3 2 4 1 I J’ often quite go there y vais 1 2 3 souvent y vais 2 1 souvent 3 4 Pat went home early Pat went home early ghar Pat−nay ghar Pat−nay juldee gayee juldee gayee a gift for you from France a gift from France for you un cadeau de France pour vous un cadeau de France pour vous rank with distinguished heir without 2 2 2 3 3 2 4 or 5 3 3 6 4 3 7 to 9 4 4 Table 1: Highest possible cardinality of minimizing decompositions over all 2D productions of the given rank. Figures 6 and 5 exemplify highest-cardinality productions of ranks 3 and 4, respectively. As we shall see in Section 5, however, bad binarization can worsen recognition complexity. The Binarization Rules apply deterministically,' but there are multiple ways to decompose the RHS of a non- DEPEND production into nested Some decompositions may give rise to more discontinuities than others. Let the cardinality of an RTV be the total number of partitions in all its components, and let the cardinality of a decomposithe cardinality of the RTVs that it contains. A minimizing decomposition for a given production is one of those with lowest cardi- Then, the cardinality of a production cardinality of its minimizing decomposition. cardinality of a production is bounded by its rank, as Table 1 shows for the 2D case. Finally, the caran MTG the maximum of the cardinalities of its productions. 5 Inference of Discontinuous Constituents Parser A is a parser for arbitrary MTGs. It initializes its chart and fires Y inferences just like Parser R2A. It then composes pairs of items into larger items using inference rule A.0 (see below). Just like items in ordinary parsers, Parser A items need to know their positions in the input multitext, but not their internal structure. However, items with discontinuities need to remember all their boundaries, not just the outermost ones. Expanding on Johnson (1985), we define a discontinuous span (or dshort) as a of zero or more intervals = . ; where the left boundaries and the are right boundaries between word positions in a text, so /, for 1 • < for 1 < < m which means that the intervals do not overlap. predefined equivalence classes for new nonterminals. &quot;For correct binarization of productions with a distinguished heir, the decomposition must put the heir in the most deeply nested DLV. This requirement tends to increase the cardinality of L2MTGs, as shown in Table 1. In addition, we say that a d-span is in normal form all the inequalities between and are strict, i.e. there is a gap between each pair of consecutive Now, a hedge item Parser A is d-link with a vector of d-spans normal form. The cardinality of an item is the total number of intervals in its d-span vector. Binarized MTG productions can be inferred under generalizations of the ID and LP constraints described in Section 3. We use two helper functions to express these constraints. + is the concatenation operator for d-spans: Given two d-spans, it outthe union of their intervals in normal The 0 function computes the role template that describes the relative positions of the intervals in two E.g., if v = (1, 3; 8, 9) and = then + = (1, 3; 7, 9) and v 0 = [1], [2, 1]. Both operators apply componentwise to vectors of d-spans. With their help, we state the composition inference rule of Parser A: Y(v),X =N (Y,Z) + The space complexity of Parser A is a function of the maximum number of boundaries stored in its signatures, and the number INIof nonterminals in the grammar. The maximum number of required boundaries is exactly twice the cardinality of the MTG, and each of the boundaries can range over positions. Thus, the space complexity Parser A for an MTG in 0(1Ni (G) is bilexical, then the number of possible nonterminals a factor of , the space complexity of A to +2C (G)) The time complexity of Parser A depends on how many boundaries are shared between antecedent items in A.0 rules. In the best case, all the boundaries are shared except the two outermost boundaries in each dimension, and the inferred item is contiguous. In the worst case, no boundaries are shared, and the inferred item stores all the boundaries of antecedent items. In any case, if cardinalities of the composed items, and the cardinality of the inferred item, then the number of boundaries in an A.0 inference is + z. Thus, in the worst case, the number of free boundinvolved in an A.0 inference is beeach boundary can range over values, where n is the length of the longest component of the input multitext. We still have 3 nonterminal labels per dimension per inference. Also, each inference now needs to compute an RTV at a cost inputs of ± must have no overlapping intervals, or else the output is undefined. A.C: (C (G)). the time complexity of Parser A in (C (G) For a binarized which also needs to keep track of two lexical heads per dimension per inference, this complexity rises to (G)13D Parser B is a generalization of Parser R2B for biof arbitrary rank. It decomposes inference rule A.0 into ID and LP subrules, using generalized hooks that carry an RTV. The decomposition can happen in one of two ways, depending on the heir's role (1 or 2) in the DLV. X[h] Y[g]) Z [h] 0 \Z [h] + X[h] Z [II]) 0 X[h](v)[v + The rules in Section 3.2 are simple examples of B.ID1 and B.LP1. Parser B is faster than Parser A, but takes more space. The hooks of Parser B must keep track of one more nonterminal label per dimension than hedges. The size of an RTV is bounded by the cardinality of the grammar. Thus, the space complexity of B is in (C (G)12D ) On the other hand, The B.ID rules involve only one d-span instead of two, reducing the number of free variables by (C (G)) . B.LP rules again involve only one lexical head instead of two, reducing the number of free by a factor of < turns out that the worst-case running time of Parser B is than that of Parser A by a factor of under of any rank and dimensionality. 6 Conclusion We have proposed Multitext Grammars (MTGs) as a convenient and relatively expressive foundation for building practical models of translational equivalence. To encourage their use for this purpose, we have explored algorithms for parsing bilexical MTGs of arbitrary rank and dimensionality. Our exploration highlighted some little-known properties of synchronous parsing: (1) some optimizations of monolingual parsers generalize to the synchronous case, but others do not; (2) discontinuous constituents are essential for parsing bitexts even in similar Western languages; (3) different binarization schemes lead to different time and space complexity. There are many aspects of translational equivalence that MTG cannot express, such as some of those described by Dorr (1994). In future work, we hope to extend the formalism to cover some of the aspects that would not raise the computational complexity of its recognition, such as discontinuous and/or phrasal terminals. Concurrently, we shall explore the empirical properties of MTG, by inducing stochastic MTGs from real multitexts. Acknowledgments Thanks to Jason Eisner, Sanjeev Khudanpur, Owen Rambow, Giorgio Satta, and members of NYU's Proteus Project for helpful discussions. The idea of treating binarization as an optimization problem is due to Wei Wang. Dan Klein proposed the term &quot;hook.&quot; This research was supported by the DARPA TIDES program, by an NSF CAREER award, and by a gift from Sun Microsystems. The joint model used by our bilingual parser is an instance of a stochastic bilingual multi text grammar (2MTG), formally defined by Melamed (2003). This speedup could not, however, be applied to the bilingual parsing algorithm since a split parsing algorithm will preclude inference of certain configurations of word alignments that are allowed by a non-split parser (Melamed, 2003). In more formal work, Melamed (2003) proposes multi text grammars and algorithms for parsing them. Melamed (2003) discussed the applicability of the hook trick for parsing bilexical multi text grammars. The assumption here (following (Melamed, 2003)) is that lexicalization is not considered as just affecting the grammar constant, but that in parsing, every terminal symbol has to be considered as the potential head of every phrase of which it is a part. While our simulation may be significantly slower than a direct implementation of the algorithm (especially when some of the optimizations discussed in (Melamed, 2003) are taken into account), the fact that it is orders of magnitude slower does indicate that our correspondence-guided approach is a promising alternative for an application context in which a word alignment is available. Melamed (2003, 2004) formalized the MT problem as synchronous parsing based on multi text grammars. Melamed (2003) presents algorithms for synchronous parsing with more complex grammars, discussing how to parse grammars with greater than binary branching and lexicalization of synchronous grammars. Variations of SCFGs go back to Aho and Ullman (1972)'s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multi text Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al (2004). The vehicle for the present guided tour shall be multi text grammar (MTG), which is a generalization of context-free grammar to the synchronous case (Melamed, 2003). This normal form allows simpler algorithm descriptions than the normal forms used by Wu (1997) and Melamed (2003).   Under such an MTG, the logic of word alignment is the one in Melamed (2003)'s Parser A, but without Compose inferences.  These switches correspond to discontinuous constituents (Melamed, 2003) in general bitext parsing. Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. However, modeling complete bilingual bilexical dependencies as theorized in Melamed (2003) implies a huge parameter space of O (|V| 2 |T| 2), where |V| and |T| are the vocabulary sizes of the two languages. Melamed (2003) discussed the applicability of the so-called hook trick for parsing bilexical multi text grammars. (2) cannot represent all possible permutations of concepts that may occur during translation, because some permutations will require discontinuous constituents (Melamed, 2003).
Extracting Semantic Orientations Of Words Using Spin Model We propose a method for extracting semantic orientations of words: desirable or undesirable. Regarding semantic orientations as spins of electrons, we use the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. We also propose a criterion for parameter selection on the basis of magnetization. Given only a small number of seed words, the proposed method extracts semantic orientations with high accuracy in the experiments on English lexicon. The result is comparable to the best value ever reported. Most recently, (Takamura et al, 2005) reports on the use of spin models to infer the semantic orientation of words. Hashimoto et al's method constructs a network of templates based on their co-occurrence in web sentences with a small number of polarity-assigned seed templates and infers the polarity of all the templates in the network by a constraint solver based on the spin model (Takamura et al, 2005). Semantic Orientation Lexicon (Takamura et al, 2005): We used the words listed as having positive or negative polarity to produce +/- states, when they occur with the designated part-of-speech. The proposed method in (Takamura et al, 2005) extracts semantic orientations from a small number of seed words with high accuracy in the experiments on English as well as Japanese lexicons. Takamura et al (2005) extracted semantic orientations of words. In more recent work, Takamura et al (2005) used the spin model to extract word semantic orientation. Semantic Orientation Lexicon (Takamura et al., 2005): The positive/negative words in this list are assigned +/- tags, when they occur with the designated part-of-speech. The syntactic network is defined in a way similar to previous work such the Spin Model (Takamura et al, 2005) and Latent Semantic Analysis to compute the association strength with seed words (Turney and Litman, 2003). Takamura et al (2005) proposed using spin models for extracting semantic orientation of words.  Takamura et al built lexical network from not only such co-occurrence but other resources including thesaurus (Takamura et al, 2005). Takamura et al (2005) used the spin model to extract word semantic orientation. Takamura et al (2005) used the Ising model to extract semantic orientations of words (not phrases).   Takamura et al (2005) determine term orientation (for Japanese) according to a spin model, i.e. a physical model of a set of electrons each endowed with one between two possible spin directions, and where electrons propagate their spin direction to neighbouring electrons until the system reaches a stable configuration. Previous work on identifying the semantic orientation of words has addressed the problem as both a semi-supervised (Takamura et al, 2005) and an unsupervised (Turney and Littman, 2003) learning problem. Takamura et al (2005) proposed using spin models for extracting semantic orientation of words.  Under this setting, we compare our method to the spin model described in (Takamura et al, 2005).
Sentiment Analysis of Twitter Data We examine sentiment analysis on Twitter data. The contributions of this paper are: (1) We introduce POS-specific prior polarity features. (2) We explore the use of a tree kernel to obviate the need for tedious feature engineering. The new features (in conjunction with previously proposed features) and the tree kernel perform approximately at the same level, both outperforming the state-of-the-art baseline. They used a combination of a minimum word frequency threshold and Categorical Proportional Difference as a feature selection method and achieved the highest accuracy of 83.33% on a hand labeled test dataset. (Agarwal et al, 2011) performed three class (positive, negative and neutral) classification of tweets. We use the emoticons list provided by (Agarwal et al, 2011) in their research. We use the emoticons list provided by (Agarwal et al, 2011) in their research. Other resources for sentiment detection include the Dictionary of Affect in Language (DAL) to score the prior polarity of words, as in Agarwal et al (2011) on social media data. Johansson and Moschitti (2010) and Agarwal et al (2011) process sentences and tweets respectively. In (Agarwal et al, 2011) a study was conducted on a reduced corpus of tweets labelled manually.
Experiments in Domain Adaptation for Statistical Machine Translation 2: Test set performance of our systems: and output/reference length ratio. 4.3 Training and decoding parameters We tried to improve performance by increasing some of the limits imposed on the training and decoding setup. During training, long sentences are removed from the training data to speed up the GIZA++ word alignment process. Traditionally, we worked with a sentence length limit of 40. We found that increasing this limit to about 80 gave better results without causing undue problems with running the word alignment (GIZA++ increasingly fails and runs much slower with long sentences). We also tried to increase beam sizes and the limit on the number of translation options per coverage span (ttable-limit). This has shown to be successful in our experiments with Arabic–English and Chinese–English systems. Surprisingly, increasing the maximum stack size to 1000 (from 200) and ttable-limit to 100 (from 20) has barely any efon translation performance. The changed only by less than 0.05, and often worsened. 4.4 German–English system The German–English language pair is especially challenging due to the large differences in word order. Collins et al. (2005) suggest a method to reorder the German input before translating using a set of manually crafted rules. In our German–English submissions, this is done both to the training data and the input to the machine translation system. 5 Conclusions Our submission to the WMT 2007 shared task is a fairly straight-forward use of the Moses MT system using default parameters. In a sense, we submitted baseline performance of this system. for all our systems on the test sets are displayed in Table 2. Compared to other submitted systems, these are very good scores, often the best or second highest scores for these tasks. We made a special effort in two areas: We explored domain adaptation methods for the News- Commentary test sets and we used reordering rules for the German–English language pair. Factored translation models have also been used for the integration of CCG supertags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007). The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). Work on domain adaptation for statistical machine translation (Koehn and Schroeder, 2007) tries to bring solutions to this issue. Based on Koehn and Schroeder (2007) we adapted our system from last year, which was focused on Europarl, to perform well on test data. Although the Moses decoder is able to work with two phrase tables at once (Koehn and Schroeder, 2007), it is difficult to use this method when there is more than one additional model. Genre adaptation is one of the major challenges in statistical machine translation since translation models suffer from data sparseness (Koehn and Schroeder, 2007). Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before. The pooled model for pairing data from abstracts and claims is trained on data composed of 250,000 sentences from each text section. Another approach to exploit commonalities between tasks is to train separate language and translation models on the sentences from each task and combine the models in the global log-linear model of the SMT framework, following Foster and Kuhn (2007) and Koehn and Schroeder (2007). See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009). Separate 5-gram language models were built from the target side of the two data set sand then they were interpolated using weights chosen to minimise the perplexity on the tuning set (Koehn and Schroeder, 2007). We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estve, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation. In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but considering only additional source data. (Koehn and Schroeder, 2007) used two language models and two translation models: one in-domain and other out-of-domain to adapt the system. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. One is the log-linear combination of TMs trained on each subcorpus (Koehn and Schroeder, 2007), with weights of each model tuned under minimal error rate training using MIRA. Researchers such as Foster and Kuhn (2007) and Koehn and Schroeder (2007) have investigated mixture model approaches to adaptation. Koehn and Schroeder (2007) learn mixture weights for language models trained with in-domain and out of-domain data respectively by minimizing the perplexity of a tuning (development) set and interpolating the models. Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). There were three different adaptation measures: First, the turker-generated development set was used for optimizing the weights of the decoding meta parameters, as introduced by Koehn and Schroeder (2007). 
Which Side Are You On? Identifying Perspectives At The Document And Sentence Levels In this paper we investigate a new problem identifying the which a document is written. By perspective we mean a point of view, for example, from the perspective of Democrats or Republicans. Can computers learn to identify the perspective of a document? Not every sentence is written strongly from a perspective. Can computers learn to identify which sentences strongly convey a particular perspective? We develop statistical models to capture how perspectives are expressed at the document and sentence levels, and evaluate the proposed models on articles about the Israeli-Palestinian conflict. The results show that the proposed models successfully learn how perspectives are reflected in word usage and can identify the perspective of a document with high accuracy. Given a pair of document collections A and B, our goal is not to construct classifiers that can predict if a document was written from the perspective of A or B (Lin et al, 2006), but to determine if the document collection pair (A, B) convey opposing perspectives. In some cases, the author may also introduce their own perspective (Lin et al, 2006) through the use of framing (Greene and Resnik, 2009). Hierarchical Bayesian modelling has recently gained notable popularity in many core areas of natural language processing, from morphological segmentation (Goldwater et al, 2009) to opinion modelling (Lin et al, 2006). We show that using adaptive naive Bayes improves on state of the art classification using the Bitter Lemons corpus (Lin et al, 2006), a document collection that has been used by a variety of authors to evaluate perspective classification. The support vector machine (SVM), NB B and LSPM results are taken directly from Lin et al (2006). We report 4-fold cross-validation (DP-4) using the folds in Greene and Resnik (2009), where training and testing data come from different websites for each of the sides, as well as 10-fold cross-validation performance on the entire corpus, irrespective of the site. Bitter Lemons (BL): We use the GUEST part of the BitterLemons corpus (Lin et al, 2006), containing 296 articles published in 2001-2005 on http: //www.bitterlemons.org by more than 200 different Israeli and Palestinian writers on issues re lated to the conflict. Bitter Lemons International (BL-I): We collected 150 documents each by a different per2Ratings are from :http: //www.OnTheIssues.org/.  For these features we replace the opinion words with their positive or negative polarity equivalents (Lin et al, 2006). They attempt to identify a position of a debate, such as ideological (Somasundaran et al, 2010, Lin et al, 2006) or product comparison debate (Somasundaran et al, 2009). These experiments were conducted in political debate corpus (Lin et al 2006). (Lin et al, 2006) explores relationships between sentence-level and document-level classification for a stance-like prediction task. Among the literature on ideological subjectivity, perhaps most similar to our work is (Somasundaran and Wiebe, 2010). Some sentences are written from a certain perspective (Lin et al, 2006) or point of view. As it is easy for a human to identify the perspective of an author (Lin et al, 2006), this measure facilitated the annotation task. In our second study, we make a direct comparison with prior state-of-the-art classification using the Bitter Lemons corpus of Lin et al (2006). Israeli-Palestinian Conflict In order to make a direct comparison here with prior state-of-the-art work on sentiment analysis, we re port on sentiment classification using OPUS features in experiments using a publicly available corpus involving opposing perspectives, the Bitter Lemons corpus introduced by Lin et al (2006) .Corpus. Within computational linguistics, what we call implicit sentiment was introduced as a topic of study by Lin et al (2006) under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science. They often belong to controversial subjects (e.g., religion, terrorism, etc.) where the same event can beseen from two or more opposing perspectives, like the IsraeliPalestinian conflict (Lin et al, 2006).
Information Fusion In The Context Of Multi-Document Summarization We present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents. Our approach is unique in its usage of language generation to reformulate the wording of the summary. As new applications appear, that cannot start generation from a semantic input because such an input is not available (for example re-generation of sentences from syntactic fragments to produce summaries (Barzilay et al, 1999) or generation of complex NPs in a hybrid template system for business letters (Gedalia, 1996)), this motivation has lost some of its strength. Barzilay et al (1999) were one of the first to use time for multi-document summarization. Extractive summarization is a simple but robust method for text summarization and it involves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting those with highest scores, while abstraction summarization usually needs information fusion (Barzilay et al, 1999), sentence compression (Knight and Marcu, 2002) and reformulation (McKeown et al, 1999). Then one can apply the method inspired by (Barzilay et al, 1999) to identify common phrases across sentences and use language generation to form a more coherent summary. Paraphrase here includes sentences generated in an Information Fusion task (Barzilay et al, 1999). Barzilay et al (1999) introduce a combination of extracted similar phrases and a reformulation through sentence generation. In this work, we use the clusters of event related sentences from the Information Fusion work by Barzilay et al [1999]. For our evaluation cases, we use the Information Fusion data collected by [Barzilay et al, 1999]. ). Semantic Role Labeling (SRL) has been implemented or suggested as a means to aid several Natural Language Processing (NLP) tasks such as information extraction (Kogan et al, 2005), multi document summarization (Barzilay et al, 1999) and machine translation (Quantz and Schmitz, 1994. As a text-to-text approach, our work is more similar to work on Information Fusion (Barzilay et al., 1999), a sub-problem in multi-document summarisation.  Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multi Gen (Barzilay et al, 1999), and MEAD (Radev et al, 2004). A famous effort in this direction is the information fusion approach proposed in Barzilay et al (1999). Barzilay et al (1999) does not explicitly extract facts, but instead picks out relevant repeated elements and combines them to obtain a summary which retains the semantics of the original. Nevertheless, one might imagine that such output forms the basis for generating coherent query-focused summaries with sentence rewrite techniques, e.g., (Barzilay et al, 1999). Since paraphrases capture the variations of linguistic expressions while preserving the meaning, they are very useful in many applications, such as machine translation (Marton et al, 2009), document summarization (Barzilay et al, 1999), and recognizing textual entailment (RTE) (Dagan et al, 2005). Lexical chains have been used in text summarization (Barzilay et al, 1999), and our linear time algorithm (Silber and McCoy, 2002) makes their computation feasible even for large texts. It has been observed that in the context of multi-document summarization of news articles, extraction may be inappropriate because it may produce summaries which are overly verbose or biased towards some sources (Barzilay et al, 1999). The merging task is a more logic-based approach than similar techniques like information fusion used in multi-document summarization (Barzilay et al, 1999).
Models Of Translational Equivalence Among Words Parallel texts (bitexts) have properties that distinguish them from other kinds of parallel data. First, most words translate to only one other word. Second, bitext correspondence is typically only partial—many words in each text have no clear equivalent in the other text. This article presents methods for biasing statistical translation models to reflect these properties. Evaluation with respect to independent human judgments has confirmed that translation models biased in this fashion are significantly more accurate than a baseline knowledge-free model. This article also shows how a statistical translation model can take advantage of preexisting knowledge that might be available about particular language pairs. Even the simplest kinds of languagespecific knowledge, such as the distinction between content words and function words, are shown to reliably boost translation model performance on some tasks. Statistical models that reflect knowledge about the model domain combine the best of both the rationalist and empiricist paradigms. The idea of a computer system for translating from one language to another is almost as old as the idea of computer systems. Warren Weaver wrote about mechanical translation as early as 1949. More recently, Brown et al. (1988) suggested that it may be possible to construct machine translation systems automatically. Instead of codifying the human translation process from introspection, Brown and his colleagues proposed machine learning techniques to induce models of the process from examples of its input and output. The proposal generated much excitement, because it held the promise of automating a task that forty years of research have proven very labor-intensive and error-prone. Yet very few other researchers have taken up the cause, partly because Brown et al. 's (1988) approach was quite a departure from the paradigm in vogue at the time. Brown et al. (1988) built statistical models of equivalence models', short). In the context of computational linguistics, translational equivalence is a relation that holds between two expressions with the same meaning, where the two expressions are in different languages. Empirical estimation statistical translation models is typically based on texts of texts that are translations of each other. As with all statistical models, the best translation models are those whose parameters correspond best with the sources of variance in the data. Probabilistic translation models whose parameters reflect universal properties of translational equivalence and/or existing knowledge about particular * D1-66F, 610 Opperman Drive, Eagan, MN 55123. E-mail: dan.melamed@twestgroup.com 1 The term translation model, which is standard in the literature, refers to a mathematical relationship two data sets. hi this context, the term implies nothing about the translation between natural languages, automated or otherwise. © 2000 Association for Computational Linguistics Computational Linguistics Volume 26, Number 2 languages and language pairs benefit from the best of both the empiricist and rationalist traditions. This article presents three such models, along with methods for efficiently estimating their parameters. Each new method is designed to account for an additional universal property of translational equivalence in bitexts: 1. Most word tokens translate to only one word token. I approximate this tendency with a one-to-one assumption. 2. Most text segments are not translated word-for-word. I build an explicit noise model. 3. Different linguistic objects have statistically different behavior in translation. I show a way to condition translation models on different word classes to help account for the variety Quantitative evaluation with respect to independent human judgments has shown that each of these three estimation biases significantly improves translation model accuracy over a baseline knowledge-free model. However, these biases will not produce the best possible translation models by themselves. Anyone attempting to build an optimal translation model should infuse it with all available knowledge sources, including syntactic, dictionary and cognate information. My goal here is only to demonstrate the value of some previously unused kinds of information that are always available for translation modeling, and to show how these information sources can be integrated with others. Melamed (2000) proposed statistical translation models to improve the techniques of word alignment by taking advantage of preexisting knowledge, which was more effective than a knowledge-free model. A typical case is the indirect association problem (Melamed, 2000), as shown in Figure 2 in which we want to translate the term s1 (s=s1). To reduce such errors and enhance the reliability of the estimation, a competitive linking algorithm, which is extended from Melamed's work (Melamed, 2000), is developed to determine the most probable translations. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: LCSR (s1, s2)= |LCS (s1, s2) |max (|s1|, |s2|) where LCS (s1, s2) is the longest common subsequence of s1 and s2, and |s| is the length of s. We chose this statistic because it has previously been found to be effective for automatically constructing translation lexicons (e.g., Melamed, 2000). The utility of the constraint for parallel corpora has already been evaluated by Melamed (2000). Melamed (2000) has already established that most source words in parallel corpora tend to translate to only one target word. A direct association, as defined in (Melamed, 2000), is an association between two words (in this setting found by the TI+Cue method) where the two words are indeed mutual translations. A translation model is induced between phonemes in two word lists by combining the maximum similarity alignment with the competitive linking algorithm of Melamed (2000). One good overview is Melamed (2000). Chen et al (2008) used Competitive Linking Algorithm (CLA) (Melamed, 2000) to align the words to construct confusion network. CLA-based: Chen et al (2008) used competitive linking algorithm (CLA) (Melamed, 2000) to build confusion network for hypothesis regeneration. This view of alignment as graph matching is not, in itself, new: Melamed (2000) uses competitive linking to greedily construct matchings where the pair score is a measure of word-to-word association, and Matusov et al (2004) find exact maximum matchings where the pair scores come from the alignment posteriors of generative models. With just this feature on a pair of word tokens (which depends only on their types), we can already make a stab at word alignment, aligning, say, each English word with the French word (or null) with the highest Dice value (see (Melamed, 2000)), simply as a matching-free heuristic mode. As observed in Melamed (2000), this use of Dice misses the crucial constraint of competition: a candidate source word with high association to a target word may be unavailable for alignment because some other target has an even better affinity for that source word. In particular, the first alignment model we will present has already been described in (Melamed, 2000). As previously mentioned, this model is mostly identical to one already proposed in (Melamed, 2000). Additionally, as already argued in (Melamed, 2000), there are ways to determine the boundaries of some multi-words phrases (Melamed, 2002), allowing to treat several words as a single token. In order to compare the efficiency of the BP procedure to a more simple one, we reimplemented the Competitive Link Algorithm (abbreviated as CLA from here on) that is used in (Melamed, 2000) to train an identical model. 
Statistical Phrase-Based Translation We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems. A word link extension algorithm similar to the one presented in this paper is given in (Koehn et al, 2003). Following phrase-based methods in statistical machine translation (Koehn et al., 2003). Modern phrasal SMT systems such as (Koehn et al., 2003) derive much of their power from being able to memorize and use long phrases. We compared our system to Pharaoh, a leading phrasal SMT decoder (Koehn et al, 2003), and our tree let system. Consider the lexical model pw (ry|rx), defined following Koehn et al (2003). All conditions use word alignments produced by sequential iterations of IBM model 1, HMM, and IBM model 4 in GIZA++ , followed by 'diag-and' symmetrization (Koehn et al., 2003). We obtained word alignments of training data by first running GIZA++ (Och and Ney, 2003) and then applying the refinement rule "grow-diag-final-and" (Koehn et al., 2003). Koehn et al (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data. we achieved results similar to Koehn et al (2003a). Recent work in SMT has shown that simple phrase-based MT systems can outperform more sophisticated word-based systems (e.g. Koehn et al. 2003). We apply STIR as a pre-ordering step in a state of-the-art phrase-based translation system from English to Japanese (Koehn et al, 2003). This paper proposes a method for building a bilingual lexicon through a pivot language by using phrase-based statistical machine translation (SMT) (Koehn et al, 2003). This system is based on phrase-based statistical machine transliteration (SMT) (Finch and Sumita, 2008), an approach initially developed for machine translation (Koehn et al, 2003), where the SMT system's log-linear model is augmented with a set of features specifically suited to the task of transliteration. In addition to the most popular techniques such as Phrase-Based Machine Transliteration (Koehnet al, 2003), CRF, re-ranking, DirecTL-pde coder, Non-Parametric Bayesian Co-segmentation (Finch et al, 2011), and Multi-to-Multi Joint Source Channel Model (Chen et al, 2011) in the News 2011, we are delighted to see that several new techniques have been proposed and explored with promising results reported, including RNN-based LM (Finch et al, 2012), English Segmentation algorithm (Zhang et al, 2012), JLIS reranking method (Wu et al, 2012) ,improved m2m-aligner (Okuno, 2012), multiple reference optimized CRF (Ammar et al, 2012), language dependent adaptation (Kondrak et al, 2012) and two-stage CRF (Kuo et al, 2012).  Many research groups use a decoder based on a log-linear approach incorporating phrases as main paradigm (Koehn et al, 2003). Traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (Koehn et al, 2003). The translation quality of statistical phrase-based systems (Koehn et al, 2003) is heavily dependent on the quality of the translation and reordering models generated during the phrase extraction algorithm (Ling et al, 2010). As for the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002): GIZA++ implementation of IBM word alignment model 4, the refinement and phrase extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al, 2007) to decode. 
Word Reordering And A Dynamic Programming Beam Search Algorithm For Statistical Machine Translation In this article, we describe an efficient beam search algorithm for statistical machine translation based on dynamic programming (DP). The search algorithm uses the translation model presented in Brown et al. (1993). Starting from a DP-based solution to the traveling-salesman problem, we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efficient search algorithm. Word reordering restrictions especially useful for the translation direction German to English are presented. The restrictions are generalized, and a set offour parameters to control the word reordering is introduced, which then can easily be adopted to new translation directions. The beam search procedure has been successfully tested on the Uerbmobil task (German to English, 8,000-word vocabulary) and on the Canadian Hansards task (French to English, 100,000-word vocabulary). For the medium-sized Uerbmobil task, a sentence can be translated in a few seconds, only a small number of search errors occur, and there is no performance degradation as measured by the word error criterion used in this article. We use a DP-based beam search procedure similar to the one presented in (Tillmann and Ney, 2003). The same beam-search pruning as described in (Tillmann and Ney, 2003) is used. Related works either deal with reordering in general as (Kanthak et al, 2005) or deal with local reordering as (Tillmann and Ney, 2003). Wand and Waibel (1997) and Tillmann and Ney (2003) proposed breadth-first search methods, i.e. beam search. A monotone decoder similar to (Tillmann and Ney, 2003) with a trigram language model is set up for translations. However, their decoder is outperformed by phrase-based decoders such as (Koehn, 2004), (Och et al, 1999), and (Tillmann and Ney, 2003). The phrase-based decoder we use is inspired by the decoder described in (Tillmann and Ney, 2003) and similar to that described in (Koehn, 2004). The beam search algorithm attempts to find the translation (i.e., hypothesis that covers all source words) with the minimum cost as in (Tillmann and Ney, 2003) and (Koehn, 2004). The distortion cost is added to the log-linear mixture. The word reorderings that are explored by the search algorithm are controlled by two parameters s and w as described in (Tillmann and Ney, 2003). Machine Translation Performance using the NIST 2005 Bleu scorer scribed in (Tillmann and Ney, 2003). A beam search decoder similar to phrase-based systems (Tillmann and Ney, 2003) is used to translate the Arabic sentence into English. For details, see (Tillmann and Ney, 2003). Our implementation of a monotone-at-punctuation reordering constraint (Tillmann and Ney,2003) requires that all input words before clause separating punctuation have be translated, be forewords afterwards are covered. In (Tillmann and Ney, 2003), a beam-search algorithm used for TSP is adapted to work with an IBM-4 word-based model and phrase-based model respectively. Dynamic-programming based beam search algorithms are discussed for both word-based and phrase-based models by Tillmann and Ney (2003) and Tillmann (2006). In (Tillmann and Ney, 2003) and (Tillmann, 2006), the authors modify a certain Dynamic Programming technique used for TSP for use with an IBM4 word-based model and a phrase-based model respectively. For further details see e.g. (Tillmann and Ney, 2003). Investigations on the IBM constraints (Berger et al, 1996) for single-word based statistical machine translation can be found e.g. in (Tillmann and Ney, 2003). The paper contains the following original contributions: 1) the DP-based decoding algorithm in (Tillmann and Ney, 2003) is extended in a formal way to handle phrases and a novel pruning strategy with increased translation speed is presented 2) a novel alignment algorithm is presented that computes a phrase alignment efficiently in the case that it is consistent with an underlying word alignment. In these algorithms, a shortest-path search is carried out in one pass over some input along a specific 'direction': in speech recognition the search is time synchronous, the single-word based search algorithm in (Tillmann et al., 1997) is (source) position-synchronous or left-to-right, the search algorithm in (Niessen et al., 1998) is (target) position-synchronous or bottom-to-top, and the search algorithm in (Tillmann and Ney, 2003) is so-called cardinality-synchronous.
New Ranking Algorithms For Parsing And Tagging: Kernels Over Discrete Structures And The Voted Perceptron This paper introduces new learning al gorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficientlyapplied to exponential sized representations of parse trees, such as the ?all sub trees? (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence.We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named entity extraction from web data. (Collins and Duffy 2002) describe the voted perceptron applied to the named-entity data in this paper, but using kernel-based features rather than the explicit features described in this paper. The vector is trained using the perceptron algorithm in combination with the averaging method to avoid over fitting; see Freund and Schapire (1999) and Collins and Duffy (2002) for details. This is essentially the syntactic tree kernel (STK) proposed in (Collins and Duffy, 2002) in which syntactic fragments from constituency trees can be matched even if they only differ in the leaf nodes (i.e. they have different surface forms).  Instead, the method employed by many rerankers following Collins and Duffy (2002) directly learn a scoring function that is trained to maximize performance on the reranking task. A viable alternative has been proposed in (Collins and Duffy, 2002), where convolution kernels were used to implicitly define a tree substructure space. Thepastc uses the tree kernel function defined in (Collins and Duffy, 2002). The tree kernel used in this article was proposed in (Collins and Duffy, 2002) for syntactic parsing reranking. Tree kernels evaluate the similarity between two trees in terms of their overlap, generally measured as the number of common substructures (Collins and Duffy, 2002). delta can be efficiently computed with the algorithm proposed in (Collins and Duffy, 2002). Collins (2000) and Collins and Duffy (2002) also succeed in finding algorithms for training discriminative models which balance tractability with effectiveness, showing improvements over a generative model. Given the semantic objects defined in the previous section, we design a convolution kernel in a way similar to the parse-tree kernel proposed in (Collins and Duffy, 2002).  It is worth noting that even if the above equations define a kernel function similar to the one proposed in (Collins and Duffy, 2002), the substructures on which it operates are different from the parse-tree kernel. For this purpose, kernel methods, and in particular tree kernels allow for representing trees in terms of all possible subtrees (Collins and Duffy, 2002). In contrast, tree kernels (Collins and Duffy, 2002) can be used to efficiently generate the huge space of tree fragments but, to generate the space of pairs of tree fragments, a new kernel function has to be defined.  In this perspective, String Kernel (SK) proposed in (Shawe Taylor and Cristianini, 2004) and the Syntactic Tree Kernel (STK) (Collins and Duffy, 2002) allow for modeling structured data in high dimensional spaces. delta function counts the number of subtrees rooted in n1 and n2 and can be evaluated as follows (Collins and Duffy, 2002). the reranker learns directly from a scoring function that is trained to maximize the performance of the reranking task (Collins and Duffy, 2002).
A Model-Theoretic Coreference Scoring Schem e Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, Lynette Hirschman The MITRE Corporation 202 Burlington Rd .  Using the MUC co reference scoring algorithm (see Vilain et al 1995). Many evaluation metrics have been proposed in the past two decades, including the MUC measure (Vilain et al, 1995), B-cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and, more recently, BLANC gold (Recasens and Hovy, 2011). For evaluation of coreference relations, we calculated re call and precision based on the MUC score (Vilain et al, 1995). For evaluation, Vilain et al (1995)'s scoring algorithm was adopted to compute the recall and precision of the whole co reference resolution. We utilized MUC (Vilain et al, 1995), B3All (Stoyanov et al, 2009), B3None (Stoyanov et al, 2009), and Pairwise F1. For this purpose, we obtain the recall, the precision and the F-measure using the standard MUC scoring program (Vilain et al 1995) for the coreference resolution task. For tests on the MUC data, we report both F-measure using the official MUC score (Vilain et al, 1995 ) and ECM-F. Three metrics have been commonly used for evaluating coreference performance over an unrestricted set of entity types: i) The link based MUC metric (Vilain et al, 1995), ii) The mention based B-CUBED metric (Bagga and Baldwin, 1998) and iii) The entity based CEAF (Constrained Entity Aligned F-measure) metric (Luo, 2005).  So, the evaluation programs, in an obvious extension of what was pro posed in (Vilain et al, 1995) for identity. For coreference resolution, MUC (Vilain et al 1995), B-CUBED (Bagga and Baldwin, 1998) and CEAF-E (Luo, 2005) are used for evaluation. Another possible choice is the MUC F-measure (Vilain et al., 1995). For coreference resolution, we report the performance in terms of recall, precision, and F1-measure using the commonly-used model theoretic MUC scoring program (Vilain et al, 1995). Results are reported in terms of recall (R), precision (P), and F-measure (F), obtained using two coreference scoring programs: the MUC scorer (Vilain et al., 1995). We report recall, precision, and F1 for MUC (Vilain et al, 1995). Vilain et al (1995) introduced the link-based MUC evaluation metric for the MUC-6 and MUC 7 coreference tasks.  Results are shown in Table 2 (Duplicated Soon Baseline) where performance is reported in terms of recall, precision, and F-measure using the model theoretic MUC scoring program (Vilain et al, 1995). We report in the following tables the MUC score (Vilain et al, 1995). MUC (Vilain et al, 1995).
Identifying Word Correspondences in Parallel Texts William A. Gale Kenneth W. Church I AT&T Bell Laboratories Murray Hill, N.J., 07974 gale@research.att.com 1.   We also used the phi2 score (Gale and Church, 1991) as a word association model, and as a POS-tags association model.   Motivated by the need to reduce on the memory requirement and to insure robustness in estimation of probability, Gale and Church (1991) proposed an alternative algorithm in which probabilities are not estimated and stored for all word pairs. When trained with a corpus only one-tenth the size of the corpus used in Gale and Church (1991), the algorithm aligns over 80% of word pairs with comparable precision (93%). These methods often involve using a statistic such as ?2 (Gale and Church, 1991) or the log likelihood ratio (Dunning, 1993) to create a score to measure the strength of correlation between source and target words. We produce an initial alignment using the same algorithm described in Section 3, except we maximize summed phi2 link scores (Gale and Church, 1991), rather than alignment probability.  (Gale and Church, 1991) has used the ?2 statistics as the correspondence level of the word pairs and has showed that it was more effective than the mutual information. Existing parallel corpora have illustrated their particular value in empirical NLP research, e.g., Canadian Hansard Corpus (Gale and Church, 1991b), HK Hansard (Wu, 1994), INTERSECT (Salkie, 1995), ENPC (Ebeling, 1998), the Bible parallel corpus (Resnik et al, 1999) and many others. The first cost function is the correlation measure (cf the use of phi2 in Gale and Church (1991)) computed as follows:= (bc ad )x/ (a+ b) (c+ d) (a+ c) (b+ d) where a =nv -n~, i~v b =nw, y c= Nnvnw +nw, v d =nwnw, v N is the total number of bi texts, nv the number of bi texts in which V appears in the target, nw the number of bi texts in which W appears in the source, and nw, y the number of bi texts in which W appears in the source and V appears in the target. They are used in multilingual NLP as a basis for the creation of translation models (Brown et .al., 1990), lexical acquisition (Gale and Church, 1991) as well as for cross-language information retrieval (Chen and Nie, 2000). Gale and Church (1991) do not follow the EM model, but rather find French translations of English words using a phi2-like measure of association. A wide variety of ways of LTP estimation have been proposed in the literature of computational linguistics, including Dice coefficient (Kay and Roscheisen 1993), mutual information, phi2 (Gale and Church 1991b), dictionary and thesaurus Table 1. Gale and Church (1991) made what may be the first application of word association to word alignment. Third, it makes the correct judgment on Gale and Church's well known chambre-communes problem (Gale and Church, 1991). Many previous efforts have used a similar methodology but were only able to focus on word to word correspondences (Gale and Church, 1991). The algorithm creates an initial alignment using search, constraints, and summed phi2 correlation-based scores (Gale and Church, 1991). In this work, the sentence level alignment algorithm given in (Gale and Church,1991) has been used for applying segment constraint.
Shallow Parsing With Conditional Random Fields Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods. We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model. Improved training methods based on modern optimization algorithms were critical in achieving these results. We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models. Many machine learning techniques have been successfully applied to chunking tasks, such as Regularized Winnow (Zhang et al, 2001), SVMs (Kudo and Matsumoto, 2001), CRFs (Sha and Pereira, 2003), Maximum Entropy Model (Collins, 2002), Memory Based Learning (Sang, 2002) and SNoW (Munoz et al., 1999). For chunking, we follow Sha and Pereira (2003) for the set of features, including token and POS information. the perceptron performance is comparable to that of Conditional Random Field models (ShaandPereira, 2003). The linear CRF chunker of Sha and Pereira (2003) is a standard near-state-of-the-art baseline chunker. In fact, many off-the-shelf CRF implementations now replicate Sha and Pereira (2003), including their choice of feature set. This evaluation was also used in (Sha and Pereira, 2003). The second-order encoding used in our NER experiments is the same as that described in (Sha and Pereira, 2003) except removing IOB-tag of previous position label. CRFs have been applied with impressive empirical results to the tasks of noun phrase chunking (Sha and Pereira, 2003). In recent years discriminative probabilistic model shave been successfully applied to a number of information extraction tasks in natural language processing (NLP), such as named entity recognition (NER) (McCallum and Li, 2003), noun phrase chunking (Sha and Pereira, 2003). Since the task is basically identical to shallow parsing by CRFs, we follow the feature sets used in the previous work by Sha and Pereira (2003). The difference between our CRF chunker and that in (Sha and Pereira, 2003) is that we could not use second-order CRF models, hence we could not use trigram features on the BIO states. Although not directly comparable, Sha and Pereira (2003) report almost the same level of accuracy (94.38%) on noun phrase recognition, using a much smaller training set. we use a standard gradient-descent method to find the weight vector that maximizes the log likelihood n i log P (yi|xi) (Sha and Pereira, 2003). For more information on current training methods for CRFs, see Sha and Pereira (2003).  The form of the objective and gradient are quite similar to the traditional fully observed training scenario for CRFs (Sha and Pereira, 2003). Although this non-concavity prevents efficient global maximization of equation (3), it still allows us to incorporate incomplete an notations using gradient ascent iterations (Sha and Pereira, 2003). Chunking approach in this paper is closely similar to the work of Sha and Pereira (2003). In the field of English text chunking (Sha and Pereira, 2003), the step 1, 3, and 4 have been studied sufficiently, whereas the step 2, how to select optimal feature template subset efficiently, will be the main topic of this paper. 
An Unsupervised Method For Word Sense Tagging Using Parallel Corpora tion using statistical models of Roget's categories on large corpora. In of the The research most similar to ours is the work of Diab and Resnik (2002). TransCont is an enhancement over an existing approach that leverages multilingual evidence through projection, SALAAM, described in detail in (Diab and Resnik, 2002). TransCont is based on the WSD system SALAAM (Diab and Resnik, 2002), henceforth (DR02). The parallel data we experiment with are the same standard data sets as in (Diab and Resnik, 2002), namely, Senseval 2 English AW data sets (SV2AW) (Palmer et al, 2001), and Seneval 3 English AW (SV3AW) data set. Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al, 2001), and statistical machine translation. Diab and Resnik (2002) presented an unsupervised method for WSD using the same type of resource. Diab and Resnik (2002) present an unsupervised approach to WSD that exploits translational correspondences in parallel corpora that were artificially created by applying commercial MT systems on a sense-tagged English corpus. This assumption of similar meaning when multiple phrases map onto a single foreign language phrase is the converse of the assumption made in the word sense disambiguation work of Diab and Resnik (2002) which posits different word senses when a single English word maps onto different words in the foreign language. As mentioned in Section 1, the way that we extract paraphrases is the converse of the methodology employed in word sense disambiguation work that uses parallel corpora (Diab and Resnik, 2002). Diab and Resnik (2002) use multilingual information to create an English sense tagged corpus to train a monolingual WSD approach. In order to accomplish these steps, Bhattacharya et al (2004) used the pseudo-translation approach of Diab and Resnik (2002): they created the model using an English-Spanish parallel corpus constructed by using Systran to translate a large collection of English text, and they obtained parallel Spanish text for the test items in the same fashion. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. A wide range of annotations from part of speech (Hi and Hwa, 2005) and chunks (Yarowsky et al, 2001) to word senses (Diab and Resnik, 2002), dependencies (Hwa et al, 2002) and semantic roles (Pado and Lapata, 2009) have been successfully transferred between languages. They have been employed in word sense disambiguation (Diab and Resnik, 2002), automatic construction of bilingual dictionaries (McEwan et al, 2002), and inducing statistical machine translation models (Koehn et al., 2003). Cross-language tagging is the goal of the work by Diab and Resnik (2002), who present a method for word sense tagging both the source and target texts of parallel bilingual corpora with the WordNet sense inventory. Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al, 2001), and statistical machine translation. The first model, which we call the Sense model, builds on the work of Diab and Resnik (2002) that uses both parallel text and a sense inventory for the target language, and recasts their approach in a probabilistic framework. The main inspiration for our work is Diab and Resnik (2002), who use translations and linguistic knowledge for disambiguation and automatic sense tagging. Bengio and Kermorvant (2003) present a graphical model that is an attempt to formalize probabilistically the main ideas in Diab and Resnik (2002). We show that this improves on the results of Diab and Resnik (2002).
CCG Supertags in Factored Statistical Machine Translation Combinatorial Categorial Grammar (CCG) supertags present phrase-based machine translation with an opportunity to access rich syntactic information at a word level.The challenge is incorporating this informa tion into the translation process. Factoredtranslation models allow the inclusion of supertags as a factor in the source or target language. We show that this results in an im provement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings. This alternate decoding path model was developed by Birch et al (2007). However, Birch et al (2007) showed that this approach captures the same re-ordering phenomena as lexicalized re-ordering models, which were not included in the baseline. Birch et al (2007) then investigated source-side CCG super tag features, but did not show an improvement for Dutch-English. Lastly, Koehn and Schroeder (2007) reported improvements from using multiple decoding paths (Birch et al, 2007) to pass both tables to the Moses SMT decoder (Koehn et al, 2003), instead of directly combining the phrase tables to perform domain adaptation. We have also shown in passing that the linear interpolation of translation models may work less well for translation model adaptation than the multiple paths decoding technique of (Birch et al, 2007). Finally, Birch et al (2007) exploit factored phrase-based translation models to associate each word with a supertag, which contains most of the information needed to build a full parse. Birch et al (2007) also reported a significant improvement for Dutch-English translation by applying CCG supertags at a word level to a factorized SMT system (Koehn et al, 2007). We built two separate phrase tables for the two bi-texts, and we used them in the alter native decoding path model of Birch et al (2007). Birch et al (2007) and Hassan et al (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side ,translating a morphologically-poor language (English) to a morphologically-rich language (Greek). Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the super tags on the target language side, English. Probabilistic models for using only source tags were investigated by Birch et al (2007), who attached syntax hints in factored SMT models by having Combinatorial Categorial Grammar (CCG) super tags as factors on the input words, but in this case English was the target language. In order to reduce these problems, decoding needed to consider alternative paths to translation tables trained with less or no factors (as Birch et al (2007) suggested), so as to cover instances where a word appears with a factor which it has not been trained with. Supertagging encapsulates more contextual information than POS tags and Birch et al (2007) report improvements when comparing a super tag language model to a baseline using a word language model only. Hassan et al (2007) and Birch et al (2007) use super tag n-gram LMs. Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. Factored translation models have also been used for the integration of CCG super tags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007).
A Systematic Comparison Of Various Statistical Alignment Models We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methodsfor combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented. Brown et al (1993a) stopped after only one iteration of EM in using Model 1 to initialize their Model 2, and Och and Ney (2003) stop after five iterations in using Model 1 to initialize the HMM word-alignment model. The trial and test data had been manually aligned at the word level, noting particular pairs of words either as 'sure' or 'possible' alignments, as described by Och and Ney (2003). We report the performance of our different versions of Model 1 in terms of precision, recall, and alignment error rate (AER) as defined by Och and Ney (2003). It is interesting to contrast our heuristic model with the heuristic models used by Och and Ney (2003) as baselines in their comparative study of alignment models. However, it is not clear that AER as defined by Och and Ney (2003) is always the appropriate way to evaluate the quality of the model, since the Viterbi word alignment that AER is based on is seldom used in applications of Model 1. The translation model was trained by first creating unidirectional word alignments in both directions using GIZA++ (Och and Ney, 2003), which are then symmetrized by the grow-diag-final-and method (Koehn et al,2005). The word alignments needed for reordering were created using GIZA++ (Och and Ney, 2003), an implementation of the IBM models (Brown et al., 1993) of alignment, which is trained in a fully unsupervised manner based on the EM algorithm (Dempster et al, 1977). Our translation models were trained using GIZA++ (Och and Ney, 2003), which we modified as necessary for the morpheme-based experiments. To obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training, i.e. GIZA++ and heuristics for phrase extraction (Och and Ney, 2003). The main tools are Moses (Koehn et al 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al 2011) for resampling and significance testing. Then the procedure is quite standard: We run GIZA++ (Och and Ney, 2003) for bi-directional word alignment, and then obtain the lexical translation table and phrase table. Word alignment scores: source-target and target-source MGIZA++ (Gao and Vogel, 2008) force-alignment scores using IBM Model 4 (Och and Ney, 2003). The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003). GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al, 1996) where EM algorithm is employed to estimate the model parameters. In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al, 2003) and assigned numerical weights. Our method is to use human alignment as the oracle of supervised learning and compare its performance against that of GIZA++ (Och and Ney 2003), a state of the art unsupervised aligner. The study of the relation between alignment quality and MT performance can be traced as far as to Och and Ney, 2003. Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection. It invokes GIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003).
Inducing Syntactic Categories By Context Distribution Clustering This paper addresses the issue of the automatic induction of syntactic categories from unannotated corpora. Previous techniques give good results, but fail to cope well with ambiguity or rare words. An algorithm, context distribution clustering (CDC), is presented which can be naturally extended to handle these problems. Preliminary experiments with tags derived automatically using distributional clustering (Clark, 2000), have shown essentially the same results. It has often been proposed that children might make use of information about the contextual distribution of usage of words to induce the parts-of-speech of their native language (e.g. Maratsos and Chalkley, 1980), and work by, e.g., Redington, Chater and Finch (1998) and Clark (2000), showed that parts-of-speech can indeed be induced by clustering together words that are used in similar contexts in a corpus. Schutze (1995) and Clark (2000) apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters. Firstly, each word is annotated with a distributional similarity tag, from a distributional similarity model (Clark, 2000) trained on 100 million words from the British National Corpus and English Gigaword corpus. Our work builds on two older part-of-speech inducers word clustering algorithms of Clark (2000) and Brown et al (1992) that were recently shown to be more robust than other well-known fully unsupervised techniques (Christodoulopoulos et al, 2010).  Our main purely unsupervised results are with a flat clustering (Clark, 2000) that groups words having similar context distributions, according to Kullback Leibler divergence.  We found that Brown et al's (1992) older information-theoretic approach, which does not explicitly address the problems of rare and ambiguous words (Clark, 2000) and was designed to induce large numbers of plausible syntactic and semantic clusters, can perform just as well.  Unsupervised word clustering techniques of Brown et al (1992) and Clark (2000) are well-suited to dependency parsing with the DMV. We trained the model described in (Clark, 2000), with code downloaded from his website, on several hundred million words from the British national corpus, and the English Gigaword corpus. Clark (2000) also builds distributional profiles, introducing an iterative clustering method to better handle ambiguity and rare words. Clark (2000) reports results on a corpus containing 12 million terms, Schutze (1993) on one containing 25 million terms, and Brown, et al (1992) on one containing 365 million terms. Clark (2000) presentsa framework which in principle should accommodate lexical ambiguity using mixtures, but includes no evidence that it does so. We trained a variant of our system without gold part-of-speech tags, using the unsupervised word clusters (Clark, 2000) computed by Finkel and Manning (2009). This approach is taken by Clark (2000), where the perplexity of a finite-state model is used to compare different category sets. To overcome this problem, Clark (2000) proposes a bootstrapping approach, in which he (1) clusters the most distributionally reliable words, and then (2) incrementally augments each cluster with words that are distributionally similar to those already in the cluster. It is perhaps not immediately clear why morphological information would play a crucial role in the induction process, especially since the distributional approach has achieved considerable success for English POS induction (see Lamb (1961), Schutze (1995) and Clark (2000)). In fact, Jardino and Adda (1994), Schutze (1997) and Clark (2000) have attempted to address the ambiguity problem to a certain extent.
Discriminative Word Alignment With Conditional Random Fields In this paper we present a novel approach for inducing word alignments from sentence aligned data. We use a Conditional Random Field (CRF), a discriminative model, which is estimated on a small supervised training set. The CRF is conditioned on both the source and target texts, and thus allows for the use of arbitrary and overlapping features over these data. Moreover, the CRF has efficient training and decoding processes which both find globally optimal solutions. We apply this alignment model to both French-English and Romanian-English language pairs. We show how a large number of highly predictive features can be easily incorporated into the CRF, and demonstrate that even with only a few hundred word-aligned training sentences, our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively. These models are roughly clustered into two groups: generative models, such as those proposed by Brown et al (1993), Vogel et al (1996), and Och and Ney (2003), and discriminative models, such as those proposed by Taskar et al (2005), Moore (2005), and Blunsom and Cohn (2006). Such approaches have been shown to be effective in log-linear word alignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006). The one most similar to ours is the one presented by Blunsom and Cohn (2006). Labeled alignments are also used by Blunsom and Cohn (2006) to train a CRF word alignment model. The model is similar to the discriminative CRF-based word alignment model of (Blunsom and Cohn, 2006). Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional random field jointly normalized over the entire sequence of alignments of (Blunsom and Cohn, 2006). The model is trained by gradient ascent using the l-BFGS method (Liu and Nocedal, 1989), which has been successfully used for training log linear models (Blunsom and Cohn, 2006) in many natural language tasks, including alignment. (Blunsom and Cohn, 2006) do word alignment by combining features using conditional random fields. Reported work includes improved model variants (e.g., Jiao et al, 2006) and applications such as web data extraction (Pinto et al, 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourse level chunking (Feng et al, 2007). Our work is heavily influenced by the bilingual alignment literature, especially the discriminative model proposed by Blunsom and Cohn (2006).
Learning Semantic Correspondences with Less Supervision A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state. To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty—Robocup sportscasting, weather forecasts (a new domain), and NFL recaps. We use the model of Liang et al (2009) to automatically induce the correspondences between words in the text and the actual database records mentioned. We achieved a BLEU score of 51.5 on the combined task of content selection and generation, which is more than a two-fold improvement over a model similar to that of Liang et al (2009). We used the dataset created by Liang et al (2009). By defining features on the entire field set, we can capture any correlation structure over the fields; in contrast, Liang et al (2009) generates a sequence of fields in which a field can only depend on the previous one. In particular, a word is chosen from the parameters learned in the model of Liang et al (2009). We use the model of Liang et al (2009) to impute the decisions. We found that using the unsupervised model of Liang et al (2009) to automatically produce aligned training scenarios (Section 4.3.1) was less effective than it was in the other two domains due to two factors: (i) there are fewer training examples in SUMTIME and unsupervised learning typically works better with a large amount of data; and (ii) the alignment model does not exploit the temporal structure in the SUMTIME world state. Liang et al (2009) introduces a generative model of the text given the world state, and in some ways is similar in spirit to our model. On the other hand, Liang et al (2009) introduced a probabilistic generative model for learning semantic correspondences in ambiguous training data consisting of sentences paired with observed world states. On the other hand, Liang et al (2009) proposed a probabilistic generative approach to produce a Viterbi alignment between NL and MRs. They use a hierarchical semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts. Chen et al (2010) recently reported results on utilizing the improved alignment produced by Liang et al (2009)'s model to initialize their own iterative retraining method. Motivated by this prior research, our approach combines the generative alignment model of Liang et al (2009) with the generative semantic parsing model of Lu et al (2008) in order to fully exploit the NL syntax and its relationship to the MR semantics.  Like Liang et al (2009)'s generative alignment model, our model is designed to estimate P (w|s), where w is an NL sentence and s is a world state containing a set of possible MR logical forms that can be matched to w. The natural language generation model covers the roles of both the field choice model and word choice models of Liang et al (2009). We also tried using a Markov model to order arguments like Liang et al (2009), but preliminary experimental results showed that this additional component actually decreased performance rather than improving it.  In particular, our proposed model outperforms the generative alignment model of Liang et al (2009), indicating that the extra linguistic information and MR grammatical structure used by Lu et al (2008)'s generative language model make our overall model more effective than a simple Markov+ bag-of-words model for language generation. Compared to Liang et al (2009), our more accurate (i.e. higher F-measure) matchings provide a clear improvement in both semantic parsing and tactical generation. In particular, we showed that our alignments provide a better foundation for learning accurate semantic parsers and tactical generators compared to those of Liang et al (2009), whose generative model is limited by a simple bag-of-words assumption.
A Simple Rule-Based Part Of Speech Tagger Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rulebased methods. In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another. Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rule-based tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below. Candidates considered in the semantic tagging process are noun phrases NP, proposition phrases PP, verb phrases VP, adjectives ADJ and adverbs ADV. To gather these candidates we used the Brill transformational tagger (Brill, 1992) for the part-of speech step and the CASS partial parser for the parsing step (Abney, 1994). The noun phrase extraction module uses Brill's POS tagger [Brill (1992)] and a base NPchunker [Ramshaw and Marcus (1995)].  In order to include features describing verb tense, we use Brill's part-of-speech tagger (Brill, 1992). Each part of speech (POS) is taken to be a feature, whose value is a count of the number of occurrences in the given utterance. In order to extract the linguistic features necessary for the ME model in WSD tasks, all sentences containing the target word are automatically part-of speech (POS) tagged using the Brill POS tagger (Brill, 1992). In the part-of-speech literature, whether taggers are based on a rule-based approach (Klein and Simmons, 1963), (Brill, 1992), (Voutilainen, 1993), or on a statistical one (Bahl and Mercer, 1976), (Leech et al, 1983), (Merialdo, 1994), (DeRose, 1988), (Church, 1989), (Cutting et al, 1992), there is a debate as to whether more attention should be paid to lexical probabilities rather than contextual ones. Part of speech tags are assigned by Brill's tagger (Brill, 1992). We looked at three different lemmatizers: the lemmatizing backend of the XTAG project (XTAG Re search Group, 2001) 4, Celex (Baayen et al, 1995), and the lemmatizing component of an enhancedTBL tagger (Brill, 1992). TBL tagger (Brill, 1992) 7, and a TnT-style trigram tagger (Halacsy et al, 2007). All corpora were stemmed (Karp et al, 1992) and part-of-speech tagged (Brill, 1992). In order to extract the linguistic features necessary for the ME model, all sentences containing the target word were automatically part of-speech (POS) tagged using the Brill POS tagger (Brill, 1992). All 15,863 documents were tagged by a part-of-speech tagger (Brill, 1992) and stemmed using WordNet information (Fellbaum, 1998).  Compared to learning rule-based approaches such as the one by Brill (1992), a k-nn approach provides a uniform approach for all disambiguation tasks, more flexibility in the engineering of case representations, and a more elegant approach to handling of unknown words (see e.g. Cardie 1994).  Rule based POS tagging methods extract rules from training corpus and use these rules to tag new sentences (Brill, 1992) (Brill, 1994). Automated part of speech tagging (Brill, 1992) is a useful technique in term extraction (Frantziet al, 2000), a domain closely related to named entity recognition. In future work, we plan to identify such adjectives in Google excerpts using a Part of Speech tagger (Brill, 1992). The CONLL 2000 tags against which we measure our own results are in fact assigned by the Brill tagger (Brill 1992), and while these may not correlate perfectly with those that would have been assigned by a human linguist, we believe that the correlation is likely to be good enough to allow for an informative evaluation of our method. For the part-of-speech tagging problem, it is known that assigning the most common part of speech for each lexical item gives a baseline of 90% accuracy [Brill, 1992].
A Uniform Architecture for Parsing and Generation Stuart M. SHIEBER Artificial Intelligence Center SRI International Menlo Park, California, USA* Abst rac t The use of a single grammar for both parsing and generation is an idea with a certain elegance, the desirability of which several researchers have noted.  We relate the algorithm to proposals by Shieber (1988). Shieber (1988) claims that the problem of logical equivalence reduces to the knowledge representation problem. Two further problems are the treatment of unary rules and functors with what Shieber (1988) calls vestigial semantics, which we prefer to call identity semantics. The obvious solution is to use a lemma table or chart (as discussed by Pereira and Warren 1984 and Shieber 1988). Shieber (1988) states that to guarantee completeness in using a precomputed entry in the chart, the entry must subsume the formula being generated top-down. And Bi-directional Grammar Shieber proposed auniform architecture for sentence parsing and generation based on the Early type deduction mechanism (Shieber,1988). Shieber (1988) gave the first use of Earley's algorithm for generation, but this algorithm does not. As Shieber (1988) noted, the main shortcoming of Earley generation is a lack of goal-directedness that results in a proliferation of edges. In generation, as Shieber (1988) and Appelt (1989) observe, a situation may arise in which the representation supplied as input to the process (perhaps by another program) is not itself directly suitable, but is logically equivalent to one that is. The use of distinct grammars for parsing and generation could provide a solution to this problem, but it raises others connected with management of the resulting system. As (Shieber, 1988) showed, the problem with such words is that they can not be selected on the basis of the input semantics. In (Shieber, 1988), a chart-based bottom-up generator is presented which is devoid of an indexing scheme: all word edges leave and enter the same vertex and as a result, interactions must be considered explicitly between new edges and all edges currently in the chart. Shieber (1988) showed that parsing charts can be also used in generation and raised the question, which we take up again here, of whether they constitute a natural uniform architecture for parsing and generation. In this respect, it differs from the proposal of Shieber (1988) which starts with all word edges leaving and entering a single vertex. Shieber (1988) has generalized this method so as to adapt to sentence generation as well. In generation, examples of such extended processing strategies are head corner generation with its semantic linking (Shieber et al, 1990) or bottom-up (Earley) generation with a semantic filter (Shieber, 1988). The use of such a semantic filter in bottom-up evaluation requires the grammar to obey the semantic monotonicity constraint in order to ensure completeness (Shieber, 1988) (see below). In order to obtain a generator similar to the bottom-up generator as described in Shieber (1988) the compilation process can be modified such that only lexical entries are extended with magic literals. The first demonstration of using charts for generation appeared in Shieber (1988). In particular, unlike an Earley deduction generator (Shieber, 1988), it allows use of semantically non monotonic grammars, yet unlike top down methods, it also permits left-recursion. In previous work (Shieber, 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural-language processes.
Generalizing Case Frames Using A Thesaurus And The MDL Principle A new method for automatically acquiring case frame patterns from large corpora is proposed. In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed. In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as &quot;cuts&quot; in the thesaurus tree, thus reducing the generalization problem to that of estimating a &quot;tree cut model&quot; of the thesaurus tree. An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity. Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods.   After extracting the argument heads of the target slots of each verb (e.g., the intransitive subject and the transitive object for the causative alternation), she then determined their selectional profiles using a minimum description length tree cut model (Li and Abe, 1998). The method addresses conceptual problems of an earlier measure proposed by McCarthy (2000), which was limited to tree cut models (Li and Abe, 1998) and failed to distinguish detailed semantic differences between them. Li and Abe (1998) use a minimum description length-based algorithm to find an optimal tree cut over WordNet for each classification problem, finding improvements over both lexical association (Hindle and Rooth, 1993) and conceptual association, and equaling the transformation-based results. W ealso plan to compare the results to the tree cut algorithm reported in (Li and Abe, 1998), which allows different levels to be identified for different subtrees. Our selectional preference model relies on Li and Abe (1998), applying the MDL principle to determine selectional preferences of verbs and their arguments, by means of a concept hierarchy ordered by hypernym/hyponym relations. Li and Abe (1998) model selectional preferences of a verb (for an argument position) as a set of nodes in the semantic class hierarchy with a probability distribution over them.   In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al (2007). Initially our project began as an application of the closely related MDL approach of Li and Abe (1998), but was hindered by sparse data. Li and Abe (1998) used a tree cut model over WordNet, based on the principle of MinimumDescription Length (MDL).  McCarthy determines the sense profile of a verb/slot pair using a minimum description length tree cut model over the frequency-populated hierarchy (Li and Abe, 1998). We suspect the problem is two-fold, arising from the dependence of her method on tree cut models (Li and Abe, 1998). The method addresses conceptual problems of an earlier measure proposed by McCarthy (2000), which was limited to tree cut models (Li and Abe, 1998) and failed to distinguish detailed semantic differences between them. The MDL-based tree cut model was originally introduced for handling the problem of generalizing case frames using a thesaurus (Li and Abe, 1998). This leads to the notion of "cutting" the hierarchy at one or more positions (Li and Abe, 1998). Li and Abe (1998) propose a model in which the appropriate cut c is selected according to the Minimum Description Length principle; this principle explicitly accounts for the trade-off between generalisation and accuracy by minimising a sum of model description length and data description length.
The Reliability Of A Dialogue Structure Coding Scheme This paper describes the reliability of a dialogue structure coding scheme based on utterance function, game structure, and higher-level transaction structure that has been applied to a corpus of spontaneous task-oriented spoken dialogues.  These strategies can be seen as transactions made up of conversational games (Carletta et al., 1997). Each dialogue is divided into short, clearly defined dialogue acts Initiations I and Acknowledgments A based on the top of the hierarchy given in Carletta et al (1997). This is an empirically very well-founded distinction: the ICSI-MR group have provided some inter-annotator agreement figures (Carletta et al, 1997) for a very similar task and report a kappa of 0.79. As indicated in Table 9, the unanimous agreement of just 16.6% and 19.5% in the ad hoc and categorization tasks respectively is low: the agreement data has Kappa (Carletta et al 1997) of .38 for ad hoc and .29 for categorization 4. To put Table 8 in perspective, note that expert human coders achieved= 0.83 on DA classification for MapTask, but also had available the speech source (Carletta et al, 1997). We also compared the confusion matrix from (Carletta et al, 1997) with the confusion matrix we obtained for our best result on MapTask (FLSA using Game+ Speaker). The same type of structures is also used in the analysis of dialogues, e.g. (Carletta et al., 1997). Another common agreement metric is the kappa coefficient which normalises token level accuracy by chance, e.g. Carletta et al (1997). Other tagging schemes, such as the Maptask scheme (Carletta et al., 1997), are also too general for our purposes. For dialog act labeling, we built models from our corpus and from the Maptask (Carletta et al, 1997). This kind of multi-level assessment corresponds to that described and used in Carletta et al, (1997). Many such models focus on other aspects of a dialog such as coordinated activities ,i.e. turn-taking and grounding, (Traum and Hinkelman, 1992) and regular patterns in the dialog (Carletta et al, 1997) rather than the domain-specific information communicated by participants. Note that we are interested in the process of designing a domain-specific tag set from the definitions of task, subtask, and concept provided by the framework, not in the process of using an existing tag set to annotate data (see for example (Carletta et al, 1997)). Again, according to the work of Carletta et al (1997), a minimum kappa score of 0.67 is required to draw tentative conclusions. This position was taken by other computational linguists as well (Carletta et al, 1997, p. 25). For example, Carletta et al (1997) computed agreement on a coarse segmentation level that was constructed on the top of finer segments, by determining how well coders agreed on where the coarse segments started, and, for agreed starts, by computing how coders agreed on where coarse segments ended. Redo our experiments on other corpora, such as Map Task (Carletta et al., 1997). The monologue side has been annotated with discourse relations, using an adaptation of the annotation guidelines of Carlson and Marcu (2001), whereas the dialogue side has been marked up with dialogue acts, using tags inspired by the schemes of Bunt (2000), Carletta et al. (1997) and Core and Allen (1997).
Inducing A Semantically Annotated Lexicon Via EM-Based Clustering We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evalutated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries. Two-dimensional EM-based clustering has been applied to tasks in syntax (Rooth et al, 1999), but so far this approach has not been used to derive models of higher dimensionality and, to the best of our knowledge, this is the first time that it is being applied to speech. EM-based clustering has been derived and applied to syntax (Rooth et al, 1999). We evaluated our 3-dimensional clustering models on a pseudo-disambiguation task similar to the one described by Rooth et al (1999), but specied to onset, nucleus, and coda ambiguity. e. g. Rooth et al (1999) and Erk (2007). Similar models have been used to learn sub categorization information (Rooth et al, 1999) or properties of verb argument slots (Yao et al,2011). Rooth et al (1999) referring to a direct object noun for describing verbs, or used any syntactic relationship detected by a chunker or a parser (such as Lin (1998) and McCarthy et al (2003). Rooth et al (1999) generalize over seen head words using EM-based clustering rather than WordNet. Experimental design Like Rooth et al (1999) we evaluate selectional preference induction approaches in a pseudo disambiguation task. Rooth et al (1999) tested 3000 random (v, n) pairs, but required the verbs and nouns to appear between 30 and 3000 times in training. EM-based clustering, originally introduced for the induction of a semantically annotated lexicon (Rooth et al, 1999), regards classes as hidden variables in the context of maximum likelihood estimation from incomplete data via the expectation maximisation algorithm. Usually, the classes are from WordNet (Miller et al, 1990), although they can also be inferred from clustering (Rooth et al., 1999).  Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al (1993) and Rooth et al (1999). Rooth et al (1999) introduced a model of selectional preference induction that casts the problem in a probabilistic latent-variable framework. Semantic classes assigned to predicate arguments in sub categorization frames are either derived automatically through statistical clustering techniques (Rooth et al (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the WordNet hierarchy or LDOCE semantic classes. Rooth et al (1999) present a fundamentally different approach to selectional preference induction which uses soft clustering to form classes for generalisation and does not take recourse to any hand-crafted resource. The strategy of our model to derive generalisations directly from corpus data, without recourse to re sources, is similar to another family of corpus-driven selectional preference models, namely EM-based clustering models (Rooth et al, 1999).  We explore a variant of the pseudo-disambiguation task of Rooth et al (1999) which has been applied to SP acquisition by a number of recent papers. Three and five-dimensional EM-based clustering has been applied to monolingual phonological data (Muller et al, 2000) and two-dimensional clustering to syntax (Rooth et al, 1999).
Retrieving Collocations From Text: Xtract Natural languages are full of collocations, recurrent combinations of words that co-occur more often than expected by chance and that correspond to arbitrary word usages. Recent work in lexicography indicates that collocations are pervasive in English; apparently, they are common in all types of writing, including both technical and nontechnical genres. Several approaches have been proposed to retrieve various types of collocations from the analysis of large samples of textual data. These techniques automatically produce large numbers of collocations along with statistical figures intended to reflect the relevance of the associations. However, none of these techniques provides functional information along with the collocation. Also, the results produced often contained improper word associations reflecting some spurious aspect of the training corpus that did not stand for true collocations. In this paper, we describe a set of techniques based on statistical methods for retrieving and identifying collocations from large textual corpora. These techniques produce a wide range of collocations and are based on some original filtering methods that allow the production of richer and higher-precision output. These techniques have been implemented and resulted in a tool, techniques are described and some results are presented on a 10 corpus of stock market news reports. A lexicographic evaluation of a retrieval tool has been made, and the estimated precision of 80%. Smadja (1993) illustrates this by presenting 8 different ways of referring to the Dow Jones index, among which only 4 are used. Smadja (1993) classifies them according to syntactic function while Sag et al (2002) classify them according to flexibility. Among early work on developing methods for MWE identification, there is that of Smadja (1993). Therefore, sublanguage techniques such as Sager (1981) and Smadja (1993) do not work. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Future work will include: (i) applying the method to retrieve other types of collocations (Smadja,1993).  It is true that various term extraction systems have been developed, such as Xtract (Smadja 1993). Schone & Jurafsky's results indicate similar results for log-likelihood & T-score, and strong parallelism among information-theoretic measures such as ChiSquared, Selectional Association (Resnik 1996), Symmetric Conditional Probability (Ferreira and Pereira Lopes, 1999) and the Z-Score (Smadja 1993). One aspect of VPCs that makes them difficult to extract (cited in ,e.g., Smadja (1993)) is that the verb and particle can be non-contiguous. Note, this is the same as the maximum span length of 5 used by Smadja (1993). One of the earliest attempts at extracting 'interrupted collocations' (i.e. non-contiguous collocations, including VPCs), was that of Smadja (1993). Lastly, collocations are domain-dependent (Smadja 1993) and language-dependent. There have been many statistical measures which estimate co-occurrence and the degree of association in previous researches, such as mutual information (Church 1990, Sporat 1990), t-score (Church 1991), dice matrix (Smadja 1993, 1996).   Smadja (Smadja 1993) proposed a statistical model by measuring the spread of the distribution of co occurring pairs of words with higher strength. This is an example of a collocation ,i.e. a sequence of words that tend to occur together and whose interpretation generally crosses the boundaries between words (Smadja, 1993). In terms of practical MWE identification systems, a well known approach is that of Smadja (1993), who uses a set of techniques based on statistical methods, calculated from word frequencies, to identify MWEs in corpora. It would therefore be interesting to conduct this study on a larger scale, using more general MWE definitions such as automatically learned collocations (Smadja, 1993) or verb-noun constructions (Diab and Bhutada, 2009).
Two Statistical Parsing Models Applied To The Chinese Treebank This paper presents the first-ever results of applying statistical parsing models to the newly-available Chinese Treebank. We have employed two models, one extracted and adapted from BBN's SIFT System (Miller et al., 1998) and a TAGbased parsing model, adapted from (Chiang, 2000). On sentences with <40 words, the former model performs at 69% precision, 75% recall, and the latter at 77% precision and 78% recall. A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al,2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). Xiong et al (2005) used a similar model to the BBN's model in (Bikel and Chiang, 2000), and augmented the model by semantic categorical information and heuristic rules. The XH test data we selected was identical to the test set used in previous parsing research by Bikel and Chiang (2000). The English sentences in the 2-best lists were parsed using the Collins parser (Collins, 1999), and the Chinese sentences were parsed using a Chinese parser provided to us by D. Bikel (Bikel and Chiang, 2000). We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study.  This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). Neither Collins et al (1999) nor Bikel and Chiang (2000) compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages.  Collins et al (1999) and Bikel and Chiang (2000) do not compare their models with an unlexicalized baseline; hence it is unclear if lexicalization really improves parsing performance for these languages. Besides, Bikel and Chiang (2000) applied two lexicalized parsing models developed for English to Penn Chinese Treebank.  The third experiment was on the Chinese Tree bank, starting with the same head rules used in (Bikel and Chiang, 2000). 
Better Evaluation for Grammatical Error Correction We present a novel method for evaluating grammatical error correction. The core of method, which we call is an algorithm for efficiently computing the sequence of phrase-level edits between a source sentence and a system hypothesis that achieves the highest overlap with the goldstandard annotation. This optimal edit seis subsequently scored using mea- We test our on the Helping Our Own (HOO) shared task data and show that our method results in more accurate evaluation for grammatical error correction. Dahlmeier and Ng (2012) propose an alternative evaluation scheme which, along with other properties, overcomes this by operating in terms of tokens rather than character offsets. Unlike evaluating grammar error correction systems (Dahlmeier and Ng, 2012), correction detection cannot refer to a gold standard.   In order to overcome this problem, the Max Match (M2) scorer was proposed in (Dahlmeier and Ng, 2012b).   
Local Phrase Reordering Models For Statistical Machine Translation We describe stochastic models of localphrase movement that can be incorporated into a Statistical Machine Translation (SMT) system. These models pro vide properly formulated, non-deficient, probability distributions over reorderedphrase sequences. They are implemented by Weighted Finite State Trans ducers. We describe EM-style parameter re-estimation procedures based on phrase alignment under the complete translationmodel incorporating reordering. Our ex periments show that the reordering modelyields substantial improvements in trans lation performance on Arabic-to-English and Chinese-to-English MT tasks. We also show that the procedure scales as the bitext size is increased. A reordering model in the framework of weighted finite state transducers is described in (Kumar and Byrne, 2005). A reordering model with reordering constraints, such as ITG constraints (Wu, 1997), IBM constraints (Berger et al1996), and local constraints (Kumar and Byrne, 2005) can account for the syntactic differences. There are additional variants, such as the Maximum Jump d strategy (MJd), a polynomial-time strategy described by Kumar and Byrne (2005), and possibly others.  Kumar and Byrne (2005) define two local reordering models for their Translation Template Model (TTM): In the first one, called MJ-1, only adjacent phrases are allowed to swap, and the movement has to be done within a window of 2. Iglesias et al. (2009) show that exact FST decoding is feasible for a phrase-based system with limited reordering (the MJ1 model (Kumar and Byrne, 2005)), and de Gispert et al (2010) show that exact FST decoding is feasible for a specific class of hierarchical grammars (shallow-1 grammars). One might try to address this issue by limiting a priori the amount of re-ordering, in the spirit of (Kumar and Byrne, 2005), which would allow to approximate a phrase-based model by a standard transducer, but this would introduce further issues. The system implements either a monotone phrase order translation, or an MJ1 (maximum phrase jump of 1) reordering model (Kumar and Byrne, 2005).
Semantic Role Labeling Using Different Syntactic Views Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels. In this paper we present a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classifiers. We show improvements on this system by: i) adding new features including features extracted from dependency parses, ii) performing feature selection and calibration and iii) combining parses obtained from semantic parsers trained using different syntactic views. Error analysis of the baseline system showed that approximately half of the argument identification errors resulted from parse errors in which there was no syntactic constituent that aligned with the correct argument. In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements. In Pradhan et al (2005), we reported on a first attempt to overcome this problem by combining semantic role labels produced from different syntactic parses. Pradhan et al (2005) combined systems that are based on phrase-structure parsing, dependency parsing, and shallow parsing.   However, (Pradhan et al., 2005a) uses some additional information since it deals with incorrect parser output by using multiple parsers.    In (Pradhan et al, 2005b), some experiments were conducted on SRL systems trained using different syntactic views. More details of this system can be found in Pradhan et al, (2005). To solve these errors, we need to explore more, such as using n-best parses and the use of several syntactic views (Pradhan et al, 2005b). Some other work paid much attention to the robust SRL (Pradhan et al, 2005b) and post inference (Punyakanok et al, 2004). Kernel Setup: We use the Constituent, Predicate, and Predicate-Constituent related features, which are reported to get the best-reported performance (Pradhan et al, 2005a), as the baseline features.  Some other works paid much attention to the robust SRL (Pradhan et al, 2005b) and post inference (Punyakanok et al, 2004). Pradhan et al (2005) combine the outputs of multiple parsers to extract reliable syntactic information, which is translated into features for a machine learning experiment in assigning semantic roles. Most state-of-the-art methods for the latter two tasks use a cascaded architecture: they employ syntactic parsers and re-cast the corresponding tasks as pattern matching (Johnson, 2002) or classification (Pradhan et al, 2005) problems.  In Table 4 we compare our system for semantic roles labeling with the output of Charniak's parser to the state-of-the-art system of (Pradhan et al, 2005). 
Integrating Graph-Based and Transition-Based Dependency Parsers Previous studies of data-driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference. In this paper, we show how these results can be exploited to improve parsing accuracy by integrating a graph-based and a transition-based model. By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task. Both Hall et al (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models.  For example, Nivre and McDonald (2008) present the combination of two state of the art dependency parsers feeding each another, showing that there is a significant improvement over the simple parsers. Nivre and McDonald (2008) present an application of stacked learning to dependency parsing, in which a second predictor is trained to improve the performance of the first. We will also explore ways of combining graph-based and transition-based parsers along the lines of Nivre and McDonald (2008). A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard tree bank annotations (Nivre and McDonald, 2008).  Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers. Therefore, the integration of both techniques as in Nivre and McDonald (2008) seems to be very promising.  Although Mate's performance was not significantly better than Berkeley's in our setting, it has the potential to tap richer features and other advantages of dependency parsers (Nivre and McDonald, 2008) to further boost accuracy, which may be difficult in the generative framework of a typical constituent parser. Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers. Nivre and McDonald (2008) use different kinds of learning paradigms, but the general idea can be carried over to a situation using the same learning mechanism. We experimented on French using a part-of-speech tagger but we could also use another parser and either use the methodology of (Johnsonand Ural, 2010) or (Zhang et al, 2009) which fusion n-best lists form different parsers, or use stacking methods where an additional parser is used as a guide for the main parser (Nivre and McDonald, 2008). Stacked learning has been applied as a system ensemble method in several NLP tasks, such as named entity recognition (Wu et al, 2003) and dependency parsing (Nivre and McDonald, 2008). More recently, approaches of combining these two parsers achieved even better dependency accuracy (Nivre and McDonald, 2008).  In the previous work, Nivre and McDonald (2008) have integrated MST Parser and Malt Parser by feeding one parser's output as features into the other.  Nivre and McDonald (2008) explore a parser stacking approach in which the output of one parser is fed as an input to a different kind of parser.
Discourse Segmentation By Human And Automated Means The need to model the relation between discourse structure and linguistic features of utterances is almost universally acknowledged in the literature on discourse. However, there is only weak consensus on what the units of discourse structure are, or the criteria for recognizing and generating them. We present quantitative results of a two-part study using a corpus of spontaneous, narrative monologues. The first part of our paper presents a method for empirically validating multiutterance units referred to as discourse segments. We report highly significant results of segmentations performed by naive subjects, where a commonsense notion of speaker intention is the segmentation criterion. In the second part of our study, data abstracted from the subjects' segmentations serve as a target for evaluating two sets of algorithms that use utterance features to perform segmentation. On the first algorithm set, we evaluate and compare the correlation of discourse segmentation with three types of linguistic cues (referential noun phrases, cue words, and pauses). We then develop a second set using two methods: error analysis and machine learning. Testing the new algorithms on a new data set shows that when multiple sources of linguistic knowledge are used concurrently, algorithm performance improves. Passonneau and Litman (1997) describe an experiment where seven untrained annotators were asked to find discourse segments in a corpus of transcribed narratives about a movie. We turned the segmentation task into a classification task by using boundaries between dialogue acts as one class and non-boundaries a the other (see Passonneau and Litman (1997) for a similar practice). By using multiple markers and machine learning methods, topic segmentation algorithms may be developed using this second approach that have a higher accuracy than methods using a single marker alone (Passonneau and Litman, 1997). Even annotating linear segmentation is challenging, particularly in the vicinity of segment boundaries (PassonneauandLitman, 1997). These referential and lexical features build on the work of Passonneau and Litman (1997), who use them in discourse segmentation. First, if non-canonicals are indicators of attentional stack pops, they should be more likely at segment boundaries; hence, we expect an increased presence of cue words (Passonneau and Litman, 1997) on non-canonicals compared to canonicals. In this sense, we are performing low-level discourse segmentation, as opposed to segmenting text into chunks or topics (e.g., Passonneau and Litman (1997)). Passonneau and Litman (1997) found that pause length correlates with discourse segment boundaries. This makes it different from others using surface features like (Passonneau and Litman, 1997). Automatic discourse segmentation, as shallow annotation of discourse structure, also provides a testing grounds for linguistic theories of discourse (Passonneau and Litman, 1997) and provides a natural unit of measure in linguistic corpora (Biber et al, 2004). Human annotators demonstrate frequent disagreement about the number of segments and exactly where the transitions between segments occur, while still demonstrating statistically significant agreement (Passonneau and Litman, 1997).
Distributional Memory: A General Framework for Corpus-Based Semantics Research into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus. As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems. In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes. Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods. The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature. We use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in (Bourigault, 2002) from a structured model (Baroni and Lenci, 2010), i.e. using syntactic contexts. Several approaches have investigated the above mentioned problem: (Baroni and Lenci, 2010) use a representation based on third order tensors and provide a general framework for distributional semantics in which it is possible to represent several aspects of meaning using a single data structure. Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived. Example entries in Baroni and Lenci (2010)'s tensor is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w, v and a connecting link-word l. The 11 most frequent contexts in Baroni and Lenci (2010)'s tensor (v and j represent verbs and adjectives, respectively). We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional sub space, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC). In the case of vectors obtained from Baroni and Lenci (2010)'s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1).  Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010). We add to the models we constructed the freely available Distributional Memory (DM) model, that has been shown to reach state-of-the-art performance in many semantic tasks (Baroni and Lenci,2010). LMI proved to be a good measure for different semantic tasks, see for example the work of Baroni and Lenci, 2010. A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)). Their cognitive relevance for language has been supported by studies of child lexical development (Li et al, 2004), category-related deficits (Vigliocco et al, 2004), selectional preferences (Erk, 2007), event types (Zarcone and Lenci, 2008) and more (see Landauer et al (2007) and Baroni and Lenci (2010) for a review). we used local mutual information (LMI) as proposed by Baroni and Lenci (2010). The DM model is described in detail by Baroni and Lenci (2010), where it is referred to as TypeDM. Refer to Baroni and Lenci (2010) for how the surface realizations of a feature are determined. Baroni and Lenci (2010) make the vectors of their best-performing Distributional Memory (dm) model available. The mcrae set (McRae et al, 1998) consists of 100 noun-verb pairs, with top performance reached by the DepDM system of Baroni and Lenci (2010), a count DSM relying on syntactic information. Indeed, in several cases they are close, or even better than those attained by dm, a linguistically-sophisticated count based approach that was shown to reach top performance across a variety of tasks by Baroni and Lenci (2010).
Dependency Grammar Induction via Bitext Projection Constraints Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poor languages. The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext. We consider generative and discriminative models for dependency grammar induction that use word-level alignments and a source language parser (English) to constrain the space of possible target trees. Unlike previous approaches, our framework does not require full projected parses, allowing partial, approximate transfer through linear expectation constraints on the space of distributions over trees. We consider several types of constraints that range from generic dependency conservation to language-specific annotation rules for auxiliary verb analysis. We evaluate our approach on Bulgarian and Spanish CoNLL shared task data and show that we consistently outperform unsupervised methods and can outperform supervised learning for limited training data. Hwa et al (2005) and Ganchev et al (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. Ganchev et al (2009) presented a parser projection approach via parallel text using the posterior regularization framework (Graca et al, 2007). In 1597 Spanish and Bulgarian projected data extracted by Ganchev et al (2009), the figures are 3.2% and 12.9% respectively. Ganchev et al (2009) handle partial projected parses by avoiding committing to entire projected tree during training. While Hwa et al (2005) requires full projected parses to train their parser, Ganchev et al (2009) and Jiang and Liu (2010) can learn from partially projected trees. However, the discriminative training in (Ganchev et al, 2009) doesn't allow for richer syntactic context and it doesn't learn from all the relations in the partial dependency parse. We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in (Ganchev et al, 2009) for comparison. While the Hindi projected tree bank was obtained using the method described in section 4, Bulgarian and Spanish projected datasets were obtained using the approach in (Ganchev et al, 2009). P (GNPPA) is the percentage of relations in the data that are learned bythe GNPPA parser satisfying the contiguous partial tree constraint and P (E-GNPPA) is the per Exactly 10K sentences were selected in order to compare our results with those of (Ganchev et al, 2009). For Bulgarian and Spanish, we used the same test data that was used in the work of Ganchev et al (2009). Table 4 compares our accuracies with those reported in (Ganchev et al, 2009) for Bulgarian and Spanish.  Table 4: Comparison of baseline, GNPPA and E GNPPA with baseline and discriminative model from (Ganchev et al, 2009) for Bulgarian and Spanish. Ganchev et al (2009)'s baseline is similar to the first iteration of their discriminative model and hence performs better than ours.  Since posterior regularization is closely related to constraint driven learning, this makes our algorithm also similar to the parser projection approach of Ganchev et al (2009). An empirical comparison to Ganchev et al (2009) is given in Section 5. PR: The posterior regularization (PR) approach of Ganchev et al (2009), in which a supervised English parser is used to generate constraints that are projected using a parallel corpus and used to regularize a target language parser. The PR system of Ganchev et al (2009) is similar to ours as it also projects syntax across parallel corpora. Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009).
A Discriminative Global Training Algorithm For Statistical MT This paper presents a novel training algorithm for a linearly-scored block sequence translation model. The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder. No translation, language, or distortion model probabilities are used as in earlier work on SMT. Therefore our method, which employs less domain specific knowledge, is both simpler and more extensible than previous approaches. Moreover, the training procedure treats the decoder as a black-box, and thus can be used to optimize any decoding scheme. The training algorithm is evaluated on a standard Arabic-English translation task.  In (Tillmann and Zhang, 2006) the model is optimized to produce a block orientation and the target sentence is used only for computing a sentence level BLEU. It might be the case that a larger k-best, or revisiting previous strategies for y+ and y− selection, such as bold updating, local updating (Liang et al, 2006b), or maxBLEU updating (Tillmann and Zhang, 2006) might have a greater impact. Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al. (2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. Tillmann and Zhang (2006) use a BLEU oracle decoder for discriminative training of a local reordering model. The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006). Tillmann and Zhang (2006) describe a perceptron style algorithm for training millions of features. Both Liang, et al (2006), and Tillmann and Zhang (2006) report on effective machine translation (MT) models involving large numbers of features with discriminatively trained weights.   Tillmann and Zhang (2006) trained their feature set using an on line discriminative algorithm. Tillmann and Zhang (2006) avoided the problem by precomputing the oracle translations in advance.  For instance, some max-margin methods restrict their computations to a set of examples from a feasible set, where they are expected to be maximally discriminative (Tillmann and Zhang, 2006). This might prove beneficial for various discriminative training methods (Tillmann and Zhang, 2006). This is the main motivation of (Tillmann and Zhang,2006), where the authors compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple (local) reordering models. Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations. This is referred to in past work as maxBLEU (Tillmann and Zhang, 2006) (MB).
SemEval-2007 Task 07: Coarse-Grained English All-Words Task This paper presents the coarse-grained En glish all-words task at SemEval-2007. We describe our experience in producing acoarse version of the WordNet sense inven tory and preparing the sense-tagged corpusfor the task. We present the results of participating systems and discuss future direc tions. To address this issue, a coarse-grained English all-words task (Navigli et al, 2007) was conducted during SemEval-2007. For example, in the English coarse grained all words task (Navigli et al,2007) at the recent SemEval Workshop the base line of choosing the most frequent sense using the first WordNet sense attained precision and recall of 78.9% which is only a few percent lower than the top scoring system which obtained 82.5%. The problem of how to cluster fine-grained senses into coarse senses is hard, especially if consensus is required (Navigli et al 2007). Similarly, the performance of WSD systems clearly indicates that WSD is not easy unless one adopts a coarse-grained approach, and then systems tagging all words at best perform a few percentage points above the most frequent sense heuristic (Navigli et al, 2007). Finally, results are presented from the SemEval-2007 coarse grained all-words task (Navigli et al, 2007), and we explore the influence of various types of selectors on the algorithm in order to draw insight for future improvement of Web-based methods. The sense inventory was created by mapping senses in WordNet 2.1 to the Oxford Dictionary of English (Navigli et al., 2007). For SemEval 2007, all systems performed better than the random base line of 53.43%, but only 4 of 13 systems achieved an F1 score higher than the MFS baseline of 78.89% (Navigli et al, 2007). Table 2 lists the results of applying the generalized Web selector algorithm described in this paper in a straight-forward manner, such that all scale (T) are set to 1. This will allow to asses its applicability to realistic tasks, such as query processing or document indexing. Experimental Set-up In order to measure accuracy, the Senseval 2007 coarse WSD dataset (Navigli et al, 2007) has been employed. We report our results in terms of precision, recall and F1-measure on the Semeval-2007 coarse-grained all-words dataset (Navigli et al, 2007).  For the SemEval workshop, only 6 of 15 systems performed better than this baseline on the nouns (Navigli et al, 2007), all of which used MFS as a back off strategy and an external sense tagged data set. All systems performing better than the MFS used the heuristic as a back off strategy when unable to output a sense (Navigli et al, 2007). Currently supervised methods achieve the best disambiguation quality (about 80% precision and recall for coarse-grained WSD in the most recent WSD evaluation conference SemEval 2007 (Navigli et al, 2007)). In the most recent SemEval 2007 (Navigli et al, 2007), the best unsupervised systems only achieved about 70% precision and 50% recall. We have evaluated our method using SemEval-2007 Task 07 (Coarse-grained English All-words Task) test set (Navigli et al, 2007). Two authors of (Navigli et al, 2007) independently and manually annotated part of the test set (710 word instances), and the pairwise agreement was 93.80%. We benchmark our API by performing knowledge based WSD with BabelNet on standard SemEval datasets, namely the SemEval-2007 coarse-grained all-words (Navigli et al, 2007, Coarse-WSD, hence forth) and the SemEval-2010 cross-lingual (Lefever and Hoste, 2010, CL-WSD) WSD tasks. Table 1: Performance on SemEval-2007 coarse-grained all-words WSD (Navigli et al, 2007).
Bayesian Word Sense Induction Sense induction seeks to automatically identify word senses directly from a corpus. A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning. Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word’s contexts into different classes, each representing a word sense. Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words. The Bayesian framework provides a principled way to incorporate a wide range of features beyond lexical cooccurrences and to systematically assess their utility on the sense induction task. The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset. More related to our work are (Brody and Lapata, 2009) or (Toutanova and Johnson, 2008) who use LDA-based models which induce latent variables from task-specific data rather than from simple documents. (Brody and Lapata, 2009) apply such a model for word sense induction on a set of 35 target nouns. Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). Tiered clustering is a discrete clustering method, as opposed to methods such as (Brody and Lapata, 2009) that assign a distribution of word senses to each word instance. We extracted pseudo documents from a 10-word window centered on the corresponding word token for each word type following Brody and Lapata (2009). This introduces the complication of choosing the right number of possible senses, hence a Bayesian approach to WSI was proposed which deals with this problem within a principled generative framework (Brody and Lapata, 2009). Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al, 2007). LDA model has also been applied to WSI (Brody and Lapata, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009). It has been demonstrated to be highly effective in a wide range of tasks, including multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008), information retrieval (Wei and Croft, 2006) and image labelling (Feng and Lapata, 2010). In the short time since its inception, topic modelling (Blei et al, 2003) has become a mainstream technique for tasks as diverse as multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008) and information retrieval (Wei and Croft, 2006). Later, Brody and Lapata (2009) combined different feature sets using a probabilistic Word Sense Induction model and found that only some combinations produced an improved system. We consider three latent models: the Singular Value Decomposition (SVD) (Schu?tze, 1998), Non-negative Matrix Factorization (NMF) (Vande Cruysand Apidianaki, 2011), and Latent Dirichlet Allocation (Brody and Lapata, 2009). Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al, 2008). Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling - using Latent Dirichlet Allocation (LDA: Blei et al (2003)) and derivative approaches - and use the topic model to determine the appropriate sense granularity. In the remainder of this section, we refer to Brody and Lapata (2009) as BL, and Yao and Durme (2011) as YVD. In their work, they model the target word instances as samples from a multinomial distribution over senses which are successively characterized as distributions over words (Brody and Lapata, 2009).  Results are shown through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task. Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval 2007 (Agirre and Soroa, 2007).
Named Entity Recognition Using An HMM-Based Chunk Tagger This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities. Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature. In this way, the NER problem can be resolved effectively. Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively. It shows that the performance is significantly better than reported by any other machine-learning system. Moreover, the performance is even consistently better than those based on handcrafted rules.  Zhou and Su (2002) integrated four different kinds of features, which convey different semantic information, for a classification model based on the Hidden Markov Model (HMM). However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. The additional orthographic features have proved useful in other systems, for example Carreras et al (2002), Borthwick (1999) and Zhou and Su (2002). Among them, NE recognition, part-of-speech tagging and text chunking adopt the same HMM based engine with error-driven learning capability (Zhou and Su, 2002). For instance, Zhou and Su trained HMM with a set of attributes combining internal features such as gazetteer information, and external features such as the context of other NEs already recognized (Zhou and Su, 2002). The named entity recognition component (Zhou and Su 2002) recognizes various types of MUC-style named entities, that is, organization, location, person, date, time, money and percentage. In this paper, it is tackled by a named entity recognition component, as in Zhou and Su (2002), using the following name alias algorithm in the ascending order of complexity. Research on named-entity recognition was addressed in the nineties at the Message Understanding Conferences (Chinchor, 1998) and is continued for example in (Zhou and Su, 2002). This will be done by integrating the relation extraction system with our previously developed NER system as described in Zhou and Su (2002). Additionally, Zhou and Su (2002) trained classifiers for Named Entity extraction and reported that performance degrades rapidly if the training corpus size is below 100KB. In this paper, we will study how to adapt a general Hidden Markov Model (HMM)-based NE recognizer (Zhou and Su 2002) to biomedical domain. Our system is adapted from a HMM-based NE recognizer, which has been proved very effective in MUC (Zhou and Su 2002). An alternative back-off modeling approach by means of constraint relaxation is applied in our model (Zhou and Su 2002). Furthermore, some constraints on the boundary category and entity category between two consecutive tags are applied to filter the invalid NE tags (Zhou and Su 2002). Furthermore, we evaluate these features and compare with those used in MUC (Zhou and Su, 2002). The reported result of the simple deterministic features used in MUC can achieve F measure of 74.1 (Zhou and Su 2002), but when they are used in biomedical domain, they only get F-measure of 24.3. In the previous NER research in newswire domain, part-of-speech (POS) features were stated not useful, as POS features may affect the use of some important capitalization information (Zhou and Su 2002). Moreover, if we can map the abbreviation to its full form in the current document, the recognized abbreviation is still helpful for classifying the same forthcoming abbreviations in the same document, as in (Zhou and Su 2002).
Two Languages Are More Informative Than One This paper presents a new approach for resolving lexical ambiguities in one language using statistical data on lexical relations in another language. This approach exploits the differences between mappings of words to senses in different languages. We concentrate on the problem of target word selection in machine translation, for which the approach is directly applicable, and employ a statistical model for the selection mechanism. The model was evaluated using two sets of Hebrew and German examples and was found to be very useful for disambiguation.  It is well known that two languages are more informative than one (Dagan et al, 1991). However, bilingual parallel corpora have mostly been used for tasks related to word sense disambiguation such as target word selection (Dagan et al, 1991) and separation of senses (Dyvik, 1998). For example, Brown et al (1991) and Gale et al (1992a, 1993) used the parallel, aligned Hansard Corpus of Canadian Parhamentary debates for WSD, and Dagan et al (1991) and Dagan and Ital (1994) used monolingual corpora of Hebrew and German and a bilingual dictionary.
Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms. Using statistical machine translation techniques, a semantic parser based on a synchronous grammar augmented with operators is learned given a set of training sentences and their correct logical forms. The resulting parser is shown to be the bestperforming system so far in a database query domain. This was pursued further, for instance, by Zettlemoyer and Collins (2005) and Wong and Mooney (2007), aimed at learning log-linear models, or (in the latter case) synchronous CF grammars augmented with lambda operators, for mapping English queries to DB queries. They solved the problem of aligning sentences and meanings by iteratively retraining an existing supervised semantic parser, WASP (Wong and Mooney, 2007b) or KRISP (Kate and Mooney, 2006), or an existing supervised natural-language generator, WASP-1 (Wong and Mooney, 2007a).    It may also alleviate the non-isomorphism issue that was commonly faced by researchers when mapping meaning representations and sentences (Wong and Mooney, 2007b).    Our approach outperforms the previous system WASP? 1++ (Wong and Mooney, 2007a) significantly, and achieves comparable or slightly better performance as compared to Lu et al (2009). We follow the standard evaluation procedure for Geo250, using 10-fold cross validation experiments with the same splits of the data as Wong and Mooney (2007). For GeoQuery, this includes the ZC05, ZC07 (Zettlemoyer and Collins, 2005, 2007), ?-WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011).  Wong and Mooney (2007) and Chen and Mooney (2008) use synchronous grammars that map a logical form, represented as a tree, into a parse of the text.  The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008).  Note the results for SCISSOR, KRISPandLU on GEOQUERY are based on a different meaning representation language, FUNQL, which has been shown to produce lower results (Wong and Mooney, 2007). 
Conundrums in Noun Phrase Coreference Resolution: Making Sense of the State-of-the-Art We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions, the assumptions made in evaluation methodologies, and inherent differences in text corpora. First, we examine three subproblems that play a role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection. We measure the impact of each subproblem on coreference resolution and confirm that certain assumptions regarding these subproblems in the evaluation methodology can dramatically simplify the overall task. Second, we measure the performance of a state-of-the-art coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets. Since we use automatically extracted markables, it is possible that some extracted markables and the gold standard markables are unmatched, or twinless as defined in Stoyanov et al (2009). In this paper, we adopt the B3all variation proposed by Stoyanov et al (2009), which retains all twin less markables.  A04CU: Train/dev/test split of the newswire portion of the ACE 2004 training set7 utilized in Culotta et al (2007), Bengston and Roth (2008) and Stoyanov et al (2009). A05ST: Train/test split of the newswire portion of the ACE 2005 training set utilized in Stoyanov et al (2009).  We utilized MUC (Vilain et al, 1995), B3All (Stoyanov et al, 2009), B3None (Stoyanov et al, 2009), and Pairwise F1. The Stoyanov et al (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest performing cluster ranking model. While researchers who evaluate their resolvers on gold NPs point out that the results can more accurately reflect the performance of their co reference algorithm, Stoyanovet al (2009) argue that such evaluations are unrealistic, as NP extraction is an integral part of an end-to-end fully-automatic resolver. To apply these scorers to automatically extracted NPs ,different methods have been proposed (see Rahman and Ng (2009) and Stoyanov et al (2009)). The majority of well studied co reference features (e.g. Stoyanov et al (2009)) are actually positive co reference indicators. A more detailed study of Reconcile-based co reference resolution systems in different evaluation scenarios can be found in Stoyanov et al (2009). We contrast our work with (Stoyanov et al, 2009), who show that the co-reference resolution problem can be separated into different parts ac cording to the type of the mention. Our baseline differs most substantially from Stoyanov et al (2009) in using a decision tree classifier rather than an averaged linear perceptron. We find the decision tree classifier to work better than the default averaged perceptron (used by Stoyanov et al (2009)), on multiple datasets using multiple metrics (see Section 4.3). Note that B3 has two versions which handle twinless (spurious) mentions in different ways (see Stoyanov et al (2009) for details). We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al (2009).  B3 here is the B3All version of Stoyanov et al (2009). The perceptron baseline in this work (Reconcile settings: 15 iterations, threshold= 0.45, SIG for ACE04 and AP for ACE05, ACE05 ALL) has different results from Stoyanov et al (2009) because their current publicly available code is different from that used in their paper (p.c.).
Using Encyclopedic Knowledge For Named Entity Disambiguation We present a new method for detecting anddisambiguating named entities in open do main text. A disambiguation SVM kernel is trained to exploit the high coverage and rich structure of the knowledge encoded in an online encyclopedia. The resultingmodel significantly outperforms a less in formed baseline.  With respect to the use of Wikipedia as a resource for natural language processing tasks, the work that is most closely related to ours is perhaps the name entity disambiguation algorithm proposed in (Bunescu and Pasca, 2006), where an SVM kernel is trained on the entries found in Wikipediafor ambiguous named entities.  The first approach in this line was Bunescu and Pasca (2006), who measure similarity between the textual context of the NE mention and the Wikipedia categories of the candidate.   Based on the aforementioned resources of information, we follow the method presented in (Bunescu and Pasca, 2006) to build a dictionary called ViDic.   (Bunescu and Pasca, 2006) showed that external information from Wikipedia can improve the disambiguation performance. After the task of EL was initiated with Wikipedia-based works on entity disambiguation, in particular by Cucerzan (2007) and Bunescu and Pasca (2006), numerous systems have been developed, encouraged by the TAC 2009 KB population task (McNamee and Dang, 2009). This is addressed by various methods, such as setting a threshold of minimal similarity for an entity selection (Bunescu and Pasca, 2006), or training a separate binary classifier to judge whether the returned top candidate is the actual denotation (Zheng et al, 2010).    We then employ standard entity linking techniques including string matching, prominence priors (Fader et al2009), and context matching (Bunescu and Pasca, 2006) to link the noun phrase subjects into Wikipedia.  Culotta et al (2006) deal with learning contextual patterns for extracting family relation ships from Wikipedia.  
A Discriminative Matching Approach To Word Alignment We present a discriminative, large margin approach to feature-based matching for word alignment. In thisframework, pairs of word tokens re ceive a matching score, which is basedon features of that pair, including mea sures of association between the words,distortion between their positions, sim ilarity of the orthographic form, and soon. Even with only 100 labeled train ing examples and simple features whichincorporate counts from a large unlabeled corpus, we achieve AER perfor mance close to IBM Model 4, in muchless time. Including Model 4 predic tions as features, we achieve a relativeAER reduction of 22% in over inter sected Model 4 alignments. For parameter optimization for the word alignment task, Taskar, Simon and Klein (Taskar et al, 2005) used a large margin approach by factoring the structure level constraints to constraints at the level of an alignment link. It should be noted that previous word-alignment experiments such as Taskar, Simon and Klein (Taskar et al, 2005) have been done with very large datasets and there is little word-order variation in the languages involved. These models are roughly clustered into two groups: generative models, such as those proposed by Brown et al (1993), Vogel et al (1996), and Och and Ney (2003), and discriminative mod els, such as those proposed by Taskar et al (2005), Moore (2005), and Blunsom and Cohn (2006). Similarly, Taskar et al (2005) cast word alignment as a maximum weighted matching problem and propose a framework for learning word pair scores as a function of arbitrary features of that pair. We followed the work in (Taskar et al, 2005) and split the original test set into 347 test examples, and 100 training examples for parameters tuning. Methods like competitive linking (Melamed, 2000) and maximum matching (Taskar et al, 2005) use a one-to-one constraint, where words in either sentence can participate in at most one link. Though Tsochantaridis et al (2004) provide several ways to incorporate loss into the SVM objective, we will use margin re-scaling, as it corresponds to loss usage in another max-margin alignment approach (Taskar et al, 2005). To create a matching alignment solution, we reproduce the approach of (Taskar et al, 2005) within the framework described in Section 4.1: 1. We use the same feature representation as (Taskar et al, 2005), with some small exceptions. The first baseline, matching is the matching SVM described in Section 4.2.1, which is a re-implementation of the state-of-the art work in (Taskar et al, 2005). The first thing to note is that our Matching baseline is achieving scores in line with (Taskar et al, 2005), which reports an AER of 0.107 using similar features and the same training and test sets. Our work borrows heavily from (Taskar et al, 2005), which uses a max-margin approach with a weighted maximum matching aligner. (Taskar et al, 2005) cast the problem of alignment as a maximum weight bipartite matching problem, where nodes correspond to the words in the two sentences. Dice Coefficient of the source word and the target word (Taskar et al, 2005). Commitments are then sent to a Commitment Selection module, which uses a weighted bipartite matching algorithm first described in (Taskar et al, 2005b) in order to identify the commitment from the t which features the best alignment for each commitment extracted from the h. Following Commitment Extraction, we used an word alignment technique first introduced in (Taskar et al, 2005b) in order to select the commitment extracted from t (henceforth, ct) which represents the best alignment for each of the commitments extracted from h (henceforth, ch). As with (Taskar et al, 2005b), we use the large-margin structured prediction model. Less general approaches based on matching have been proposed in (Matusov et al, 2004) and (Taskar et al., 2005). Permutation space methods include weighted maximum matching (Taskar et al, 2005), and approximations to maximum matching like competitive linking (Melamed, 2000). (Taskar et al, 2005), instead set parameters to maximize alignment accuracy against a hand-aligned development set.
Automatic Verb Classification Based On Statistical Distributions Of Argument Structure Automatic acquisition of lexical knowledge is critical to a wide range of natural language processing tasks. Especially important is knowledge about verbs, which are the primary source of relational information in a sentence—the predicate-argument structure that relates an action or state to its participants (i.e., who did what to whom). In this work, we report on supervised learning experiments to automatically classify three major types of English verbs, based on their argument structure—specifically, the thematic roles they assign to participants. We use linguistically-motivated statistical indicators extracted from large annotated corpora to train the classifier, achieving 69.8% accuracy for a task whose baseline is 34%, and whose expert-based upper bound we calculate at 86.5%. A detailed analysis of the performance of the algorithm and of its errors confirms that the proposed features capture properties related to the argument structure of the verbs. Our results validate our hypotheses that knowledge about thematic relations is crucial for verb classification, and that it can be gleaned from a corpus by automatic means. We thus demonstrate an effective combination of deeper linguistic knowledge with the robustness and scalability of statistical techniques. (Merlo and Stevenson, 2001) use grammatical features (acquired from corpora) to classify verbs into three semantic classes: unergative, unaccusative, and object-drop. there has been some work on the acquisition of thematic roles, (e.g., Merlo and Stevenson, 2001). However, those works targeted a small subset of Levin classes, and a limited number of monosemous verbs; for example, Merlo and Stevenson (2001) studied three classes and 59 verbs, and Joanis et al (2008) focused on 14 classes and 835 verbs. The value of this feature is computed by the method of Merlo and Stevenson (2001). As a cue to this alternation, Merlo and Stevenson (2001) create a bag of head nouns for each of the two potentially alternating slots, and compare them. The method used by Merlo and Stevenson (2001) has the advantage of directly capturing similarity between slots (in terms of use of identical nouns [lemmas]), but fails to generalize over the nouns, lending itself to sparse data problems. Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulteim Walde, 2006 for German semantic verb classes. In particular, Merlo and Stevenson (2001) present a classification experiment which bears similarities to ours. The results reported in Tables 2 and 3 are significantly higher than those of Merlo and Stevenson (2001). Merlo and Stevenson (2001) present a method for verb classification which relies only on distributional statistics taken from corpora in order to train a decision tree classifier to distinguish between three groups of intransitive verbs. This paper presents experiments in automatic classification of the animacy of unseen Norwegian common nouns, inspired by the method for verb classification presented in Merlo and Stevenson (2001).  Based on our observation and previous studies (Merlo and Stevenson, 2001), we assume that each ILC has a distinct frequency distribution of roles on different grammatical slots. (Merlo and Stevenson, 2001) approximates diathesis alternations by hand-selected grammatical features. Merlo and Stevenson (2001) use corpus-based thematic role information to identify and classify unergative, unaccusative, and object-drop verbs. For open, the first three words in either position are the same. For the verb play, on the other hand, classified as an "object-drop" verb by Merlo and Stevenson (2001), we would expect overlap between the subject of transitive and intransitive uses. Merlo and Stevenson (2001) presented an automatic classification of three types of English intransitive verbs, based on argument structure and heuristics to thematic relations. This was the approach taken by Merlo and Stevenson (2001), who worked with a Decision Tree and selected linguistic cues to classify English verbs into three classes: unaccusative, unergative and object-drop. Based on our observation and previous studies (Merlo and Stevenson, 2001), we assume that each Levin class has a distinct frequency distribution of roles on different grammatical slots. In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes.
An Evaluation Exercise For Word Alignment BiBr.EF.1 BiBr.EF.2 BiBr.EF.3 BiBr.EF.4 BiBr.EF.5 BiBr.EF.6 BiBr.EF.7 BiBr.EF.8 Limited Unlimited Unlimited Limited Unlimited Unlimited Limited Unlimited baseline of bilingual bracketing baseline of bilingual bracketing + English POS tagging baseline of bilingual bracketing + English POS tagging and base NP reverse direction of BiBr.EF.1 reverse direction of BiBr.EF.2 reverse direction of BiBr.EF.3 intersection of BiBr.EF.1 & BiBr.EF.3 intersection of BiBr.EF.3 & BiBr.EF.6 ProAlign.EF.1 Unlimited cohesion between source and target language + English parser + distributional similarity for English words Ralign.EF.1 Limited Giza (IBM Model 2) + recursive parallel segmentation UMD.EF.1 Limited IBM Model 2, trained with 1/20 of the corpus, distortion 2, iterations 4 XRCE.Base.EF.1 XRCE.Nolem.EF.2 XRCE.Nolem.EF.3 Limited GIZA++ (IBM Model 4) with English and French lemmatizer GIZA++ only (IBM Model 4), trained with 1/4 of the corpus GIZA++ only (IBM Model 4), trained with 1/2 of the corpus Table 2: Short description for English-French systems System Resources Description BiBr.RE.1 BiBr.RE.2 BiBr.RE.3 Limited Unlimited Unlimited baseline of bilingual bracketing baseline of bilingual bracketing + English POS tagging baseline of bilingual bracketing + English POS tagging and base NP  We use the data provided forthe French-English shared task of the 2003 HLTNAACL Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003). The evaluation is done using the performance measures described in (Mihalcea and Pedersen, 2003): precision, recall and F-score on the probable and sure alignments, as well as the Alignment Error Rate (AER), which in our case is a weighted average of the recall on the sure alignments and the precision on the probable. HLT-03 best F is Ralign.EF.1 and best AER is XRCE.Nolem.EF.3 (Mihalcea and Pedersen, 2003). HLT-03 best is Ralign.EF.1 (Mihalcea and Pedersen, 2003). hlt-03 best is xrce.nolem (mihalcea and pedersen, 2003). We evaluated the alignment performance of the proposed models with two tasks: Japanese English word alignment with the Basic Travel Expression Corpus (BTEC) (Takezawa et al, 2002) and French-English word alignment with the Hansard dataset (Hansards) from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). As a point of comparison, the SMT community has been evaluating performance of word-alignment systems on an even smaller dataset of 447 pairs of non-overlapping sentences (Mihalcea and Pedersen, 2003). (Mihalcea and Pedersen, 2003). This is the same training data set as used in the 2003 word alignment evaluation (Mihalcea and Pedersen, 2003). The evaluation was run with respect to precision, recall, F-measure, and alignment error rate (AER) considering sure and probable alignments but not NULL ones (Mihalcea and Pedersen, 2003). For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003). To choose the regularization strength and the initial learning rate 0,3 we trained several models on a 10,000-sentence-pair subset of the French English Hansards, and chose values that minimized the alignment error rate, as evaluated on a 447 sentence set of manually created alignments (Mihalcea and Pedersen, 2003). This comes back to the task of word alignment, which is a very difficult task for computers (Mihalcea and Pedersen, 2003). In this section, we report experiments conducted with Canadian Hansards data from the 2003 HLT NAACL word-alignment workshop (Mihalcea and Pedersen, 2003). We applied this matching algorithm to word level alignment using the English-French Hansards data from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). We used the same training and test data as in our previous work, a subset of the Canadian Hansards bilingual corpus supplied for the bilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003). The French/English data were those used by Mihalcea and Pedersen (2003). The alignments produced by MEBA were compared to the ones produced by YAWA and evaluated against the Gold Standard (GS) annotations used in the Word Alignment Shared Tasks (Romanian-English track) organized at HLT-NAACL2003 (Mihalcea and Pedersen 2003). The 2003 HLT-NAACL Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003) reflected the increasing importance of the word alignment task, and established standard performance measures and some benchmark tasks.
Joint Word Segmentation and POS Tagging Using a Single Perceptron Chinese word segmentation is a preliminary step. To avoid error propagation and improve segmentation by utilizing segmentation and tagging can be performed simultaneously. A challenge for this joint approach is the large combined search space, which makes efficient decoding very hard. Recent research has explored integration of segmentation and tagging, by decoding under restricted versions of the full combined search space. In this paper, propose a joint segmentation and tagging model that does not impose any hard constraints on the interaction between word and Fast decoding is achieved by using a novel multiple-beam search algorithm. The system uses a discriminative statistical model, trained using the generalized perceptron algorithm. The joint model gives an error reduction in segmentation accuracy of an error reduction in tagging acof compared to the traditional pipeline approach.  Zhang and Clark (2008) generated CTB 3.0 from CTB 4.0. Zhang and Clark (2008) indicated that their results cannot directly compare to the results of Shi and Wang (2007) due to different experimental settings. We decided to follow the experimental settings of Jiang et al (2008a; 2008b) on CTB 5.0 and Zhang and Clark (2008) on CTB 4.0 since they reported the best performances on joint word segmentation and POS tagging using the training materials only derived from the corpora. Following Zhang and Clark (2008), we first generated CTB 3.0 from CTB 4.0 using sentence IDs. Table 8 compares the F1 results of our baseline model with Nakagawa and Uchimoto (2007) and Zhang and Clark (2008) on CTB 3.0. We relax the max operation by beam-search, resulting in a segment-based decoder similar to the multiple-beam algorithm in (Zhang and Clark, 2008). Zhang and Clark (2008) used a segment-based decoder for word segmentation and pos tagging.  Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b). Zhang and Clark (2008) employed the generalized perceptron algorithm to train a statistical model for joint segmentation and POS tagging, and applied multiple-beam search algorithm for fast decoding.  We show that the standard beam-search algorithm can be used as an efficient decoder for the global linear model of Zhang and Clark (2008) for joint word segmentation and POS-tagging, achieving a significant speed improvement. In our 10-fold cross validation experiments with the Chinese Tree bank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. In our previous work (Zhang and Clark, 2008), which we refer to as Z&C08 from now on, we used an approximate decoding algorithm that keeps track of a set of partially built structures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning. Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. We also use Penn2Malt and the head-finding rules of (Zhang and Clark 2008) to convert constituency trees into dependencies. For POS tagging features, we follow the work of Zhang and Clark (2008a). Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931and 1148-1151), and test (secs 816-885 and 1137 1147) sets.
A Statistical Semantic Parser That Integrates Syntax And Semantics We introduce a learning semantic parser,SCISSOR, that maps natural-language sentences to a detailed, formal, meaning representation language. It first usesan integrated statistical parser to pro duce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label. A compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation. We evaluate the system in two domains, a natural-language database interface and an interpreter for coaching instructions in robotic soccer. We present experimentalresults demonstrating that SCISSOR produces more accurate semantic representa tions than several previous approaches.  SCISSOR (Ge and Mooney, 2005), a system that learns an integrated syntactic-semantic parser. Figure 6 shows the performance of WASP com pared to four other algorithms: SILT (Kate et al,2005), COCKTAIL (Tang and Mooney, 2001), SCIS SOR (Ge and Mooney, 2005) and Zettlemoyer and Collins (2005). Of course, other annotations (Ge and Mooney, 2005) carry more explicit forms of semantics. The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008). Ge and Mooney (2005) introduced an approach, SCISSOR, where the composition of meaning representations is guided by syntax. Extended back-off levels for the semantic parameter PL1 (Li| ...), using the same notation as in Ge and Mooney (2005). Here, we only describe changes made to SCISSOR for re ranking, for a full description of SCISSOR see Ge and Mooney (2005). Baseline ResultsTable 2 shows the results comparing the baseline learner SCISSOR using both the back-off parameters in Ge and Mooney (2005) (SCISSOR) and the revised parameters in Section 2.2 (SCISSOR+). Ge and Mooney also presented a statistical method (Ge and Mooney, 2005) by merging syn tactic and semantic information. Structure representation A tree structure representation incorporated with semantic and syntactic information is named semantically augmented parse tree (SAPT) (Ge and Mooney, 2005). As defined in (Ge and Mooney, 2005), in an SAPT, each internal node in the parse tree is annotated with a semantic label. The procedure of generating a logical form using a SAPT structure originally proposed by (Ge and Mooney, 2005) and it is expressed as Algorithm 1.  SCISSOR (Ge and Mooney, 2005) takes syntactic parses rather than NL strings and attempts to translate them into MR expressions.
The Parallel Grammar Project We report on the Parallel Grammar (ParGram) project which uses the XLE parser and grammar development platform for six languages: English, Over the past few years, there has been considerable progress in the ability of manually created large-scale grammars, such as the English Resource Grammar (ERG, Copestake and Flickinger (2000)) or the ParGram grammars (Butt et al, 2002), to parse wide-coverage text and assign it deep semantic representations. These tools have been developed to serve an Arabic Lexical Functional Grammar parser using XLE (Xerox Linguistics Environment) platform as part of the ParGram Project (Butt et al 2002). The experiments reported in this paper use the English LFG grammar constructed as part of the ParGram project (Butt et al, 2002). We parse a tree bank with the XLE platform (Crouch et al, 2008) and the English grammar developed within the ParGram project (Butt et al, 2002). Aspects of this research have often had their own separate fora, such as the ACL 2005 workshop on deep lexical acquisition (Baldwin et al, 2005), as well as the TAG+ (Kallmeyer and Becker, 2006), Alpino (van der Beek et al, 2005), ParGram (Butt et al, 2002) and DELPH-IN (Oepen et al, 2002) projects and meetings. A full system using the Lexical-Functional Grammar (LFG) parsing system XLE and the grammars from the Parallel Grammar development project (ParGram; (Butt et al., 2002)) has been implemented, and we present preliminary results on English-to-German translation with a tree-labeling system trained on a small subsection of the Europarl corpus. In the work described in this paper, we employ the XLE platform using the grammars available for English and German from the ParGram project (Butt et al, 2002).  The experiments reported in this paper use the German LFG grammar constructed as part of the ParGram project (Butt et al, 2002). This problem was illustrated using a German LFG grammar (Rohrer and Forst, 2006) constructed as part of the ParGram project (Butt et al, 2002). We compared the output of the XLE system, a deep-grammar-based parsing system using the English Lexical-Functional Grammar previously constructed as part of the Pargram project (Butt et al, 2002), to the same gold standard. The grammar used for this experiment was developed in the ParGram project (Butt et al, 2002). Within the ParGram project (Butt et al, 2002), Kim et al (2003) were able to directly port the argument optionality related rules from a Japanese grammar to Korean. The only rule-based approach to German LFG-parsing we are aware of is the hand-crafted German grammar in the ParGram Project (Buttet al, 2002). In order for our approach to work, the coverage of the precision grammars must be broad enough to parse a large corpus of grammatical sentences, and for this reason, we choose the XLE (Maxwell and Kaplan, 1996), an efficient and robust parsing system for Lexical Functional Grammar (LFG) (Kaplanand Bresnan, 1982) and the ParGram English grammar (Butt et al, 2002) for our experiments. The ParGram English LFG is a hand-crafted broad-coverage grammar developed over several years with the XLE platform (Butt et al, 2002). The tree bank is based on the output of individual deep LFG (LexicalFunctional Grammar) grammars that were developed independently at different sites but within the overall framework of ParGram (the Parallel Grammar project) (Butt et al, 1999a; Butt et al, 2002).
Statistical Models for Unsupervised Prepositional Phrase Attachment several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task. Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information. It is therefore less resource-intensive and more portable than previous corpus-based algorithm proposed for this task. We present results for prepositional phrase attachment in both English and Spanish. We rely on Gsearch to provide moderately accurate information about verb frames in the same way that Hindle and Rooth (1993) relied on Fidditch to provide moderately accurate information about syntactic structure, and Ratnaparkhi (1998) relied on simple heuristics defined over part-of-speech tags to deliver information early as useful as that provided by Fidditch. (Ratnaparkhi, 1998) first assumes noun attachment for all of-PPs and then applies his disambiguation methods to all remaining PPs.  The most prominent unsupervised methods are the Lexical Association score by Hindle and Rooth (1993) and the co occurrence values by Ratnaparkhi (1998). The co occurrence values for verb V and noun N correspond to the probability estimates in (Ratnaparkhi, 1998) except that Ratnaparkhi includes a back-off to the uniform distribution for the zero denominator case. The current unsupervised state of the art achieves 81.9% attachment accuracy (Ratnaparkhi, 1998). As in (Ratnaparkhi, 1998), we constructed a training data set consisting of only unambiguous.    More recently, Ratnaparkhi (1998) developed an unsupervised method that collects statistics from text annotated with part-of-speech tags and morphological base forms. Ratnaparkhi (1998) notes that the test set contains errors, but does not correct them. Using an adaptation of the algorithm proposed by Ratnaparkhi (1998) for PP-attachment, she achieves P=72% (baseline P=64%), R=100.00%. A second model relevant to our discussion is the one proposed in (Ratnaparkhi,1998), addressing the problem of unsupervised learning for PP attachment resolution in VERB NOUN PP sequences. The model proposed in (Ratnaparkhi, 1998) is similar to a version of our model based solely on equation (9), with no semantic information. equation (9) captures both the contribution from the random variable used in (Ratnaparkhi, 1998) to denote the presence or absence of any preposition that is unambiguously attached to the noun or the verb in question, and the contribution from the conditional probability that a particular preposition will occur as unambiguous attachment to the verb or to the noun. (Ratnaparkhi, 1998) solved this problem by regarding only prepositions in syntactically unambiguous configurations.
Japanese Discourse and the Process of Centering Mar i lyn  Walker* University of Pennsylvania Sharon  Cotes University of Pennsylvania Masayo  I ida t Stanford University This paper has three aims: (1) to generalize a computational ccount of the discourse process called CENTERING, (2) to apply this account o discourse processing in Japanese so that it can be used in computational systems for machine translation or language understanding, and (3) to provide some insights on the effect of syntactic factors in Japanese on discourse interpretation.  Hence, the most important single construct of the centering model is the ordering of the list of forward-looking centers (Walker et al, 1994).   In this paper, I will review and assess the recent centering approach to the interpretation of Japanese zero pronouns (Walker et al 1994) as a case study.  The BFP-algorithm (c.f. Walker et al (1994)) consists of three basic steps: 1 GENERATE possible Cb-Cf combinations. 2 FILTER by constraints, e.g., contra-indexing, sortal predicates, centering rules and constraints. 3 RANK by transition orderings. Walker et al (1994) proposed forward center ranking for Japanese.  
Translating Named Entities Using Monolingual And Bilingual Resources Named entity phrases are some of the most difficult phrases to translate because new phrases can appear from nowhere, and because many are domain specific, not to be found in bilingual dictionaries. We present a novel algorithm for translating named entity phrases using easily obtainable monolingual and bilingual resources. We report on the application and evaluation of this algorithm in translating Arabic named entities to English. We also compare our results with the results obtained from human translations and a commercial system for the same task. Web counts are frequently used to automatically re-rank candidate lists for various NLP tasks (Al-Onaizan and Knight, 2002). Specifically, (Al-Onaizan and Knight, 2002) uses transliteration to generate candidates and then web corpora to identify translations. (Al-Onaizan and Knight 2002) showed that use of outside linguistic resources such as WWW counts of transliteration candidates can greatly boost transliteration accuracy.  A spelling-based model is described in (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c) that directly maps English letter sequences into Arabic letter sequences with associated probability that are trained on a small English/Arabic name list without the need for English pronunciations. The phonetics-based and spelling-based models have been linearly combined into a single transliteration model in (Al-Onaizan and Knight, 2002b) for transliteration of Arabic named entities into English. Yaser Al-Onaizan (Al-Onaizan and Knight, 2002) transliterated an NE in Arabic into several candidates in English and ranked the candidates by comparing their counts in several English corpora. Al-Onaizan and Knight (2002) used Web statistics information to validate the translation candidates generated by language model, and obtained the accuracy of 72.6% in Arabic-English OOV word translation. Al-Onaizan and Knight (2002) describe a system which combines a phonetic based model with a spelling model for transliteration. For example, the work of (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. Al-Onaizan and Knight (2002b) suggested that pronunciation can be skipped and the target language letters can be mapped directly to source language letters. Similarly, Al-Onaizan and Knight (2002a; 2002b) only made use of transliteration information alone and so was not directly comparable. For example, the work of (Al Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002b; Knight and Graehl, 1998) used only the pronunciation or spelling of w in translation. Previous work on defining subtasks within statistical machine translation has been performed on, e.g., noun-noun pair (Cao and Li, 2002) and named entity translation (Al-Onaizan and Knight, 2002). Al-Onaizan and Knight (2002), Huang (2003) and Ji and Grishman (2007) investigated the general name entity translation problem, especially in the context of machine translation.
Contrastive Estimation: Training Log-Linear Models On Unlabeled Data Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and namedentity extraction (McCallum and Li, 2003). CRFs allowing the incorporation of arbifeatures into the model. To train on we require methods for log-linear models; few exist. We describe a novel We show that the new technique can be intuitively understood as exnegative evidence is computationally efficient. Applied to a sequence labeling problem—POS tagging given a tagging dictionary and unlabeled text—contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features. Recent work by Smith and Eisner (2005) on contrastive estimation suggests similar techniques to generate local neighborhoods of a parse; however, the purpose in their work is to define an approximation to the partition function for log-linear estimation (i.e., the normalization factor in a MaxEnt model). Smith and Eisner (2005a; 2005b) generate negative evidence for their contrastive estimation method by moving or removing a word in a sentence.  We compare the output to two annotation schemes: the fine grained PTB WSJ scheme, and the coarse grained tags defined in (Smith and Eisner, 2005). Smith and Eisner (2005) initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. The settings of the various experiments vary in terms of the exact gold annotation scheme used for evaluation (the full WSJ set was used by all authors except Goldwater and Griffiths (2007) and the GGTP-17 model which used the set of 17coarse grained tags proposed by (Smith and Eisner, 2005)) and the size of the test set. Evaluation was done against the POS-tag annotations of the 45-tag PTB tag set (hereafter PTB45), and against the Smith and Eisner (2005) coarse version of the PTB tag set (hereafter PTB17). We follow this method, but also attempt to identify negative examples that are semantically similar to the positive ones in order to improve the discriminative power of the classifier (Smith and Eisner, 2005).  Smith and Eisner (2005) show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods, and we posit that the same holds here. The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005).  System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999)). Smith and Eisner (2005) design a contrastive estimation technique which yields a higher accuracy of 88.6%. Contrastive estimation (CE) (Smith and Eisner, 2005a) is another log-linear framework for primarily unsupervised structured prediction. The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon et al, 2009), where good neighborhoods can be identified. Smith and Eisner (2005) use neighborhoods of related instances to figure out what makes found instances good. One example of the kind of operator used is the transposition operator proposed by Smith and Eisner (2005). This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005).
Computing Consensus Translation For Multiple Machine Translation Systems Using Enhanced Hypothesis Alignment This paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (MT) systems. The outputs are combined and a possibly new translation hypothesis can be generated. Similarly to the well-established ROVER approach of (Fiscus, 1997) for combining speech recognition hypotheses, the consensus translation is computed by voting on a confusion network. To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering. The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment. The proposed alignment and voting approach was evaluated on several machine translation tasks, including a large vocabulary task. The method was also tested in the framework of multi-source and speech translation. On all tasks and conditions, we achieved significant improvements in translation quality, increasing e. g. the BLEU score by as much as 15% relative. Combining multiple MT outputs to increase performance is, in general, a difficult task (Matusov et al, 2006) when significantly different engines compete for producing the best outputs. Note that the approach by Matusov et al (2006) attempts to align synonyms and different morphological forms of words to each other but this is done implicitly, relying on the parallel text to learn word alignments. The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al, 2006) or edit distance alignments allowing shifts (Rosti et al, 2007). The second, sys comb giza, corresponds to the pair-wise symmetric HMM alignments from GIZA++ described in (Matusov et al, 2006). Thus, when Matusov et al (2006) use this procedure, they deterministically reorder each translation prior to the monotone alignment. This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al, 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation.    In (Matusov et al, 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). Tuning is fully automatic, as opposed to (Matusov et al, 2006) where global system weights were set manually. Similar combination of multiple confusion networks was presented in (Matusov et al, 2006). Matusov et al (2006) propose using a statistical word alignment algorithm as a more robust way of aligning (monolingual) outputs into a confusion network for system combination. Experiments combining several kinds of MT systems have been presented in (Matusov et al, 2006), based only on the single best output of each system. The basic concept of the approach has been described by Matusov et al (2006).  (Matusov et al, 2006) computes consensus translation by voting on a confusion network, which is created by pairwise word alignment of multiple baseline MT hypotheses. And confusion network generates new hypotheses based on confusion network decoding (Matusov et al, 2006), where the confusion network is built on the original N-best translations. Matusov et al (2006) let every hypothesis play the role of the skeleton once and used GIZA++ to get word alignment. 
An Efficient Method For Determining Bilingual Word Classes In statistical natural language processing we always face the problem of sparse data. One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling. In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation. We develop an optimization criterion based on a maximumlikelihood approach and describe a clustering algorithm. We will show that the usage of the bilingual word classes we get can improve statistical machine translation. The word classes are computed automatically using another statistical training procedure (Och, 1999) which often produces word classes including words with the same semantic meaning in the same class. We use the publicly available implementation MK CLS3 (Och, 1999) to train this model. Word alignment was estimated with GIZA++ tool (Och, 2003), coupled with mk cls3 (Och, 1999), which allows for statistical word clustering for better generalization. This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code. The word classes for the class-based features are trained using the mkcls tool (Och, 1999). We use the publicly available implementation MKCLS (Och, 1999) to train this model. The SMR technique works with statistical word classes (Och, 1999) instead of words themselves (particularly, we have used 200 classes in all experiments). This feature implements a 5-gram language model of target statistical classes (Och, 1999). This model has been popular for language modelling and bilingual word alignment, and an implementation with improved inference called mkcls (Och, 1999) has become a standard part of statistical machine translation systems. As a baseline we report the performance of mkcls (Och, 1999) on all test corpora. A later study by (Och, 1999) showed improvements on perplexity of bilingual corpus, and word translation accuracy using a template-based translation model. This approach was shown to give the best results in (Och, 1999). Practically, we can use word alignment as used in (Och, 1999). First, we cluster the words in the corpus using the MKCLS algorithm (Och, 1999) given a number of classes. Och (1999) described a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words. Word clusters have previously been used for SMT for improving word alignment (Och, 1999), in a class-based language model (Costa-jussa et al., 2007) or for extracting gappy patterns (Gimpel and Smith, 2011). For the unsupervised tags, we used clustered word classes obtained using the mkcls software, which implements the approach of Och (1999). Och (1999) showed a method for inducing bilingual word classes that placed each phrase pair into a two-dimensional equivalence class. However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. This feature consists of a 5-gram model of words classes, which is trained from the target side of the bilingual corpus using the statistical classes from (Och, 1999).
Graph-Based Generation Of Referring Expressions This article describes a new approach to the generation of referring expressions. We propose to formalize a scene (consisting of a set of objects with various properties and relations) as a labeled directed graph and describe content selection (which properties to include in a referring expression) as a subgraph construction problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The current approach has four main advantages: (1) Graph structures have been studied extensively, and by moving to a graph perspective we get direct access to the many theories and algorithms for dealing with graphs; (2) many existing generation algorithms can be reformulated in terms of graphs, and this enhances comparison and integration of the various approaches; (3) the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches. Identifying sets of objects originally followed the incremental algorithm (Dale and Reiter 1995), as in (Bateman 1999), (Stone 2000) and (Krahmer et al 2003), with limited coverage, since only few attributes typically apply to all intended referents and to none of the potential distractors. For the first step, attribute selection, we use a version of the Graph-based REG algorithm of Krahmer et al (2003). (in line with the human preference for basic level values; cf. Krahmer et al 2003. We are currently exploring whether Krahmer et als (2003) graph-based approach to GRE is able to provide a better coverage of the data: this algorithm provides the ability to make use of different search strategies and weighting mechanisms when adding properties to a description, and such a mechanism might be used, for example, to counterbalance the Relational Algorithm's heavy bias towards the relations in this domain. The major exceptions here may be (a) van Deemter's (2002) algorithm for sets; recall that we excluded from the human data used here 16 references that involved sets; and, as noted above, (b) Krahmer et als (2003) graph-based approach to GRE, which may perform better than the Relational Algorithm on descriptions using relations. They distinguish three types of preciseness, i.e. precise, imprecise, or very imprecise pointing, and integrate pointing into the graph-based algorithm proposed by [Krahmer et al, 2003]. Algorithms for the generation of referring expressions commonly use this as a starting point, proposing that properties are organized in some linear order (Dale and Reiter, 1995) or weighted order (Krahmer et al, 2003) as input. Similarly, one of the strengths of the Graph-Based Algorithm (Krahmer et al, 2003) is its ability to generate expressions that involve relations between objects, and these include spatial ones (next to, on top of, etc.). In addition, we plan to experiment with assigning costs to planning operators in a metric planning problem (Hoffmann, 2002) in order to model the cognitive cost of an RE (Krahmer et al, 2003) and compute minimal-cost instruction sequences. (Krahmer et al, 2003) describe an approach to GRE in which a cost function guides search for a suitable description, and show that some existing GRE algorithms fit into this framework.  Recent work has formalised NLG algorithms for referring expression generation in terms of algorithms for finding an appropriate subgraph of a graph representing the domain knowledge [Krahmer et al, 2003]. The Graph based algorithm (Krahmer et al, 2003), for example, searches for the cheapest description for a target, and distinguishes cheap attributes (such as color) from more expensive ones (orientation). The use of vertex facts indicates that the representation is inspired by the Graph approach to referring expression generation [Krahmer et al, 2003]. We use the Graph-based algorithm of Krahmer et al (2003) for attribute selection. For attribute selection, we use the graph-based algorithm of Krahmer et al (2003), one of the highest scoring attribute selection methods in the TUNA 2008 Challenge (Gatt et al (2008), table 11). Although Krahmer et al claim that their method can handle n-ary relations (Krahmer et al, 2003), they provide node tails. A possible direction would be to enhance the algorithm proposed by Krahmer et al (Krahmer et al., 2003). The graph-based REG algorithm (Krahmer et al, 2003), for example, models preferences in terms of costs, where cheaper is more preferred. In the graph-based algorithm (Krahmer et al, 2003), which we refer to as Graph, information about domain objects is represented as a labelled directed graph, and REG is modeled as a graph-search problem.
Experimental Support for a Categorical Compositional Distributional Model of Meaning Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model. We discuss three ways of constructing such matrices, and evaluate each method in a disambiguation task developed by Grefenstette and Sadrzadeh (2011). In (Grefenstette and Sadrzadeh, 2011), we developed and implemented one such method on the data from the British National Corpus. The experiment is on the dataset developed in (Grefenstette and Sadrzadeh, 2011). Furthermore, the success of both of these methods relative to the others examined in Table 1 shows that it is the extra information provided in the matrix (rather than just the diagonal, representing the lexical vector) that encodes the relational nature of transitive verbs, thereby validating in part the requirement suggested in Coecke et al (2010) and Grefenstette and Sadrzadeh (2011) that relational word vectors live in a space the dimensionality of which be a function of the arity of the relation. While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic composition by generic algebraic operations.  Similarly, current compositional DSMs (cDSMs) focus almost entirely on phrases made of two or more content words (e.g., adjective-noun or verb-noun combinations) and completely ignore grammatical words, to the point that even the test set of transitive sentences proposed by Grefenstette and Sadrzadeh (2011) contains only Tarzan-style statements with determiner-less subjects and objects: "table show result", "priest say mass", etc. Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments.  Grefenstette and Sadrzadeh (2011) learn matrices for verbs in a categorical model.
A Monolingual Tree-based Translation Model for Sentence Simplification In this paper, we consider sentence sim plification as a special form of translation with the complex sentence as the source and the simple sentence as the target. We propose a Tree-based Simplification Model (TSM), which, to our knowledge, is the first statistical simplification model covering splitting, dropping, reorderingand substitution integrally. We also de scribe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia. The evaluation shows that our model achieves better readability scores than a set of baseline systems. When compared against current state of-the-art methods (Zhu et al, 2010) our model yields significantly simpler output that is both grammatical and meaning preserving. Zhu et al (2010) also use Wikipedia to learn a sentence simplification model which is able to perform four rewrite operations, namely substitution, reordering, splitting, and deletion. In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. We evaluated our model on the same dataset used in Zhu et al (2010), an aligned corpus of MainEW and SimpleEW sentences. However, we refrained from doing so as Zhu et al (2010) show that Moses performs poorly, it can not model rewrite operations that split sentences or drop words and in most cases generates output identical. AlignILP is most different from the reference, followed by Zhu et al (2010) and RevILP. Zhu et al (2010) is the least grammatical model. Finally, RevILP preserves the meaning of the target as well as SimpleEW, whereas Zhu et al yields the most distortions. Our results also show that a more general model not restricted to specific rewrite operations like Zhu et al (2010) obtains superior results and has better coverage. Zhu et al (2010), for example, use a tree-based simplification model which uses techniques from statistical machine translation (SMT) with this data set. This system can handle sentence splitting operations and the authors use both automatic and human evaluation and show an improvement over the results of Zhu et al (2010) on the same data set, but they have to admit that learning from parallel bi-text is not as efficient as learning from revision histories of the Wiki-pages. Our work extends recent work by Zhu et al (2010) that also examines Wikipedia/Simple English Wikipedia as a data-driven, sentence simplification task. Our approach performs significantly better than two different text compression approaches, including T3, and better than previous approaches on a similar data set (Zhu et al, 2010). There has also been work on lexical substitution for simplification, where the aim is to substitute difficult words with simpler synonyms, derived from WordNet or dictionaries (Inui et al, 2003). Zhu et al (2010) examine the use of paired documents in English Wikipedia and Simple Wikipediafor a data-driven approach to the sentence simplification task. We follow Zhu et al (2010) and Coster and Kauchak (2011) in proposing that sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning.   While Zhu et al (2010) have demonstrated that their approach outperforms a PBMT approach in terms of Flesch Reading Ease test scores, we are not aware of any studies that evaluate PBMT for sentence simplification with human judgements.  Zhu et al (2010) learn a sentence simplification model which is able to perform four rewrite operations on the parse trees of the input sentences, namely substitution, reordering, splitting, and deletion. Zhu et al (2010) evaluate their system using BLEU and NIST scores, as well as various readability scores that only take into account the output sentence, such as the Flesch Reading Ease test and n-gram language model perplexity.
Has A Consensus NL Generation Architecture Appeared And Is It Psycholinguistically Plausible? I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other. I also compare this 'consensus architecture' among applied NLG systems with psycholinguistic knowledge about how humans speak, and argue that at least some aspects of the consensus architecture seem to be in agreement with what is known about human language production, despite the fact that psycholinguistic plausibility was not in general a goal of the developers of the surveyed systems. Superficially, the architecture of our system conforms to the standard emerged in natural anguage generation (NLG) (as expressed, for instance in Reiter, 1994) in that it includes the stages of content specification, text planning and surface generation (realization). This separation is now fairly standard and most implementations encapsulate each task in a separate module (Robin 1995), (Reiter 1994). The summarizer's architecture follows the consensus NLG architecture (Reiter, 1994), including the stages of content calculation and content planning. This assumes the existence of a separate higher-level process to produce such a representation, following the canonical pipeline architecture of a full generation system (Reiter, 1994). In the community of NLG, there is a broad consensus that the generation of natural language should be done in three major steps [Reiter, 1994]. Reiter (1994) proposed an analysis of such systems in terms of a simple three stage pipeline. Sequential processing has also been used in several NLG systems (e.g. Reiter (1994), Reiter& amp; Dale (2000)), and has been successfully used to combine standard preprocessing tasks such as part-of-speech tagging, chunking and named entity recognition (e.g. Buchholz et al (1999), Soon et al (2001)) .In this paper we address the problem of aggregating the outputs of classifiers solving different NLP tasks. Narratological aspects influence on all architectural modules [Reiter, 1994] or representation levels [Cahill et al, 2000] of NLG. The RAGS project initially set out to develop a reference architecture based on the three-stage pipeline suggested by Reiter (Reiter, 1994). In the classic natural language generation (NLG) architecture (Reiter, 1994), sentence boundary decisions are made during the sentence planning stage in which the syntactic structure and wording of sentences are decided. Although most generation systems pipeline decisions (Reiter, 1994), we believe the most efficient and flexible way to integrate constraints in sentence planning is to synchronize the decisions. Most generation systems pipeline pragmatic, semantic, lexical and syntactic decisions (Reiter, 1994). Furthermore, Reiter (1994), who reviews the architecture of some models of natural language generation, shows that psycholinguistic and engineering approaches often result in systems, which are similar in crucial respects. In this paper we ground on two of these common aspects, namely the distinction between what-to-say and how-tosay (De Smedt, Horacek& amp; Zock, 1996) and the use of a pipeline architecture, which divides the generation process& quot; into multiple modules, with information flowing in a &apos; pipeline &apos; fashion from one module to the next& quot; (Reiter, 1994). There has also been a rethinking of the traditional modular NLG architecture (Reiter, 1994).
Modeling Local Coherence: An Entity-Based Approach This paper considers the problem of automatic assessment of local coherence. We present a novel entity-based representation of discourse which is inspired by Centering Theory and can be computed automatically from raw text. We view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function. Our experiments demonstrate that the induced model achieves significantly higher accuracy than a state-of-the-art coherence model. The entity grid approach has already been applied to many applications relying on local coherence estimation: summary rating (Barzilay and Lapata, 2005), essay scoring (Burstein et al, 2010) or story generation (McIntyre and Lapata,2010). The experimental results demonstrate that our model is able to significantly outperform the state-of the-art coherence model by Barzilay and Lapata (2005), reducing the error rate of the previous approach by an average of 29% over three data sets against human upper bounds. To further refine the computation of the subsequence distribution, we follow (Barzilay and Lapata, 2005) and divide the matrix into a salient matrix and a non-salient matrix.   Barzilay and Lapata (2005) showed that their entity based model is able to distinguish a source text from its permutation accurately. The results on the Earthquakes and Accidents data are quite similar to those published in (Barzilay and Lapata, 2005) (they reported 83.4% on Earthquakes and 89.7% on Accidents), validating the correctness of our reimplementation of their method. This result supports the use of salience, in line with the conclusion drawn in (Barzilay and Lapata, 2005). The entity-based model of Barzilay and Lapata (2005) connects the local entity transition with textual coherence, while our model looks at the patterns of discourse relation transitions. This intuition has been formalized by (Barzilay and Lapata, 2005), who developed an entity-based statistical representation of local discourse and showed its usefulness for estimating coherence between sentences. Barzilay and Lapata (2005) exploited the use of the distributional and referential information of discourse entities to improve summary coherence. In order to evaluate the local coherence of the reports generated by the system, we employed an automatic coherence evaluation method introduced by Barzilay and Lapata (2005). Our model is inspired by Centering (Grosz et al, 1995) and other entity-based models of coherence (Barzilay and Lapata, 2005) in which an entity is in focus through a sequence of sentences.  We expect that features such as entity grid (Barzilay and Lapata, 2005) will improve overall algorithm performance. We present a model for discourse coherence which combines the local entity based approach of (Barzilay and Lapata, 2005) and the HMM-based content model of (Barzilay and Lee, 2004). As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively. Moreover, an accurate model can reveal information about document structure, aiding in such tasks as supervised summarization (Barzilay and Lapata, 2005). Barzilay and Lapata (2005) uses the same grid representation, but treats the transition probabilities P (ri, j |~ri, j) for each document as features for input to an SVM classifier. 
Efficient Third-Order Dependency Parsers We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they reonly Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively. Regarding dependency parsing of the English PTB, currently Koo and Collins (2010) and Zhang and Nivre (2011) hold the best results, with 93.0 and 92.9 unlabeled attachment score, respectively. For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. Previous work on graph-based dependency parsing mostly adopts linear models and perceptron-based training procedures, which lack probabilistic explanations of dependency trees and do not need to compute likelihood of labeled training 1Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve higher accuracy, but has much higher time cost (O (n4)).  Kooand Collins (2010) propose a third-order graph based parser. In the second column, [Ma08] denotes Martins et al (2008), [KC10] is Koo and Collins (2010), [Ma10] is Martins et al (2010), and [Ko10] is Koo et al (2010). S (g, h, m, s) (4) For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). We use the Model 1 version of dpo3, a state-of-the art third-order dependency parser (Koo and Collins, 2010)). For further details about the parser, see Koo and Collins (2010). The set of potential edges was pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent (Collins et al, 2008) as in Koo and Collins (2010). We also ran MST Parser (McDonald and Pereira, 2006), the Berkeley constituency parser (Petrov and Klein, 2007), and the unmodified dpo3 Model 1 (Koo and Collins, 2010) using Conversion 2 (the current recommendations) for comparison. As some important relationships were represented as siblings and some as grandparents, there was a need to develop third-order parsers which could exploit both simultaneously (Koo and Collins, 2010).  We extend Carreras (2007) graph based model with factors involving three edges similar to that of Koo and Collins (2010). Figure 5 and 6 illustrate the third order factors, which are similar to the factors of Koo and Collins (2010). While previous work using third order factors, cf. Koo and Collins (2010), was restricted to unlabeled and projective trees, our parser can produce labeled and non-projective dependency trees.  Koo and Collins (2010) further propose a third-order model that uses third-order features. Table 7 shows the performance of the graph-based systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al, 2008), Suzuki09 refers to the parser of Suzuki et al. Koo10-model1 (Koo and Collins, 2010) used the third-order features and achieved the best reported result among the supervised parsers.
Automatic Identification Of Non-Compositional Phrases Non-compositional expressions present a special challenge to NLP applications. We present a method for automatic identification of non-compositional expressions using their statistical properties in a text corpus. Our method is based on the hypothesis that when a phrase is non-composition, its mutual information differs significantly from the mutual informations of phrases obtained by substituting one of the word in the phrase with a similar word. Minipar outputs dependency trees (Lin, 1999) from the input sentences. Prior work in discovering non-compositional phrases has been carried out by Lin (1999) and Baldwin et al (2003), who also used LSA to distinguish between compositional and non compositional verb-particle constructions and noun noun compounds. Dekang Lin proposes a way to automatically identify the noncompositionality of MWEs (Lin, 1999). Least mutual information difference with similar collocations: this feature is based on Lin's work (Lin, 1999). Lin (1999) gave a corpus-based method for finding various types of non-compositional phrases, including the sort discussed in this paper. Recent work which attempts to discriminate between compositional and non-compositional MWEs include Lin (1999), who used mutual information measures identify such phrases, Bald win et al (2003), who compare the distribution of the head of the MWE with the distribution of the entire MWE, and Vallada Moiro and Tiedemann (2006), who use a word-alignment strategy to identify non-compositional MWEs making use of parallel texts. Melamed (1997) and Lin (1999) have done some research on non compositional phrases discovery. Inspired by Lin (1999), we examine the strength of association between the verb and noun constituents of the target combination and its variants, as an indirect cue to their idiomaticity. Lin (1999) assumes that a target expression is non-compositional if and only if its (I) J+ value is significantly different from that of any of the variants. Lin (1999) and Wermter and Hahn (2005) go one step further and look into a linguistic property of non-compositional compounds their lexical fixedness to identify them.  Later work such as by Lin (1999) continued this tradition. This is also the place where linguistic constraints can be applied, say to avoid non compositional phrases (Lin, 1999). Second, some n-grams themselves carry no linguistic meaning; their phrase translations can be misleading, for example non-compositional phrases (Lin, 1999). To protect against that problem, we compute the 99.99999% confidence intervals around the PMI (Lin, 1999), and use the lower bound as a measure of association. These properties can be used to identify potential idioms, for instance, by employing measures of association strength between the elements of an expression (Lin, 1999). Lin (1999) argues that non-compositional expressions need to be treated differently than other phrases in many statistical or corpus based NLP methods.  Lin (1999) defines a decision criterion for non compositional phrases based on the change in the mutual information of a phrase when substituting one word for a similar one based on an automatically constructed thesaurus. We propose a new method that compares phrases with their alternative phrases, in the spirit of Lin (1999)'s substitution approach (see Section 4.3).
Design Challenges and Misconceptions in Named Entity Recognition We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an system that achieves 90.8 on the CoNLL-2003 NER shared task, the best reported result for this dataset. The system uses the Illinois Entity Tagger (Ratinov and Roth, 2009) and Orthomatcher from the GATE framework for within a document co-reference resolution. We use the IOBES notation (Ratinov and Roth, 2009) to represent NE mentions with label sequences, thereby NER is formalized as a multi class classification problem in which a given token is classified into IOBES labels. Finally both the training and test data were sentence-segmented and word-tokenized by NLTK (Bird and Loper, 2004), dependency parsed by the Stanford Parser (Klein and Manning, 2003), and NER-tagged by the Illinois Named Entity Tagger (Ratinov and Roth, 2009) with an 18-label type set. Cited authors of each paper are extracted from the reference section and automatically identified by a named entity recognizer tuned for citation extraction (Ratinov and Roth, 2009). Ratinov and Roth (2009) also investigate design challenges for named entity recognition, and showed that other design choices, such as the representation of output labels and using features built on external knowledge, are more important than the learning model itself. In this work, we use the Brown clustering algorithm (Brown et al, 1992), which has been shown to improve performance in various NLP applications such as dependency parsing (Koo et al., 2008), named entity recognition (Ratinov and Roth, 2009), and relation extraction (Boschee et al., 2005). Ratinov and Roth (2009) have shown for the CoNLL-2003 shared task that Greedy decoding (i.e., beam search of width 1) is competitive to the widely used Viterbi algorithm while being over 100 times faster at the same time. Such mention type information as shown on the left of Figure 1 can be obtained from various sources such as dictionaries, gazetteers, rule-based systems (Stro ?tgen and Gertz, 2010), statistically trained classifiers (Ratinov and Roth, 2009), or some web resources such as Wikipedia (Ratinov et al, 2011). However, in practice, outputs from existing mention identification and typing systems can be far from ideal.  NE tags We automatically annotate the sentences with named entity (NE) tags using the named entity tagger of (Ratinov and Roth, 2009). From a sentence, we gather the following as candidate mentions: all nouns and possessive pronouns, all named entities annotated by the NE tagger (Ratinov and Roth, 2009), all base noun phrase (NP) chunks, all chunks satisfying the pattern: NP (PP NP)+, all NP constituents in the syntactic parse tree, and from each of these constituents, all substrings consisting of two or more words, provided the sub strings do not start nor end on punctuation marks. These mention candidates are then fed to our mention entity typing (MET) classifier for type prediction (more details in Section 6.3). For NER we used the Illinois Named Entity Tagger (Ratinov and Roth, 2009) on the highest setting (that achieved 90.5 F1 score on the CoNLL03 shared task). used the state-of-the-art named entity tagger of Ratinov and Roth (2009) to label the text. We also used the CBC word clusters of Pantel and Lin (2002) as additional gazetteers and Brown cluster features as used by Ratinov and Roth (2009) and Koo et al. The LBJ Tagger is based on a regularized average perceptron (Ratinov and Roth, 2009). For example, the average F1 of the Stanford NER (Finkel et al, 2005) drops from 90.8% (Ratinov and Roth, 2009) to 45.8% on tweets, while Liuetal. Our gazetteers comes from (Ratinov and Roth, 2009). As an unlabeled adaptation method to address feature sparsity, we add cluster-like features based on the gazetteers and word clustering resources used in (Ratinov and Roth, 2009) to bridge the source and target domain. For example, (Ratinov and Roth, 2009) only use the cluster-like features to address the feature sparsity problem, and (Finkel and Manning, 2009) only use target labeled data without using gazetteers and word-cluster information. The work (Ratinov and Roth, 2009) also combines their system with several document-level features.
Reinforcement Learning for Mapping Instructions to Actions In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains — Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training exam Our approach is most closely related to the reinforcement learning algorithm for mapping text instructions to commands developed by Branavan et al (2009) (see Section 4 for more detail). Previous work (Branavan et al, 2009) is only able to handle low-level instructions. This environment reward function is a simplification of the one described in Branavan et al (2009), and it performs comparably in our experiments. In addition to the look-ahead features described in Section 5.2, the policy also includes the set of features used by Branavan et al (2009). Baselines As a baseline, we compare our method against the results reported by Branavan et al (2009), denoted here as BCZB09. Performing policy-gradient with this function is equivalent to training a fully supervised, stochastic gradient algorithm that optimizes conditional likelihood (Branavan et al, 2009).  To learn from an unaligned corpus, we derive a new training algorithm that combines the Generalized Grounding Graph (G3) framework introduced by Tellex et al [2011] with the policy gradient method described by Branavan et al [2009].  We also compare against the policy gradient learning algorithm of Branavan et al (2009).
Parsing The Wall Street Journal Using A Lexical-Functional Grammar And Discriminative Estimation Techniques We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model. We report on the results of applying this system to parsing the UPenn Wall Street Journal (WSJ) treebank. The model combines full and partial parsing techniques to reach full grammar coverage on unseen data. The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models. Disambiguation performance is evaluated by measuring matches of predicate-argument relations on two distinct test sets. On a gold standard of manually annotated f-structures for a subset of the WSJ treebank, this evaluation reaches 79% F-score. An evaluation on a gold standard of dependency relations for Riezler et al (2002) report on our WSJ parsing experiments. The limitation of deterministic transfer rules has been recognized in prior work (Riezler et al, 2002).  Specifically, we parsed a dump of English Wikipedia (July 2008) with the XLE parser (Riezler et al, 2002) and extracted the following dependency relations for nouns: Verb-Subject, Verb-Object, Noun coordination, NN-compound, Adj-Mod. They are still reasonably popular today, as exemplified by major systems like PARC's XLE (Riezler et al, 2002). See e.g. Riezler et al (2002) and Zhang et al (2007) for chart based parsers which can produce fragmentary analyses. In this project, a broad-coverage LFG grammar and parser for English was employed (see Riezler et al (2002)). Alternatively, a single input parse could be selected by stochastic models such as the one described in Riezler et al (2002). Riezler et al (2002) describe a discriminative LFG parsing model that is trained on standard (syntax only) tree bank annotations by treating each tree as a full LFG analysis with an observed c-structure and hidden f-structure. For sentences out of coverage, it employs the robustness techniques (fragment parsing, 'skimming') implemented in XLE and described in Riezler et al. (2002), so that 100% of our corpus sentences receive at least some sort of analysis. Our error reduction of 51.0% also compares favorably to the 36% error reduction on English LFG parses reported in Riezler et al (2002). For our experiments, we used a stochastic parsing system for LFG that we trained on section 02-21 of the UPenn Wall Street Journal treebank (Marcus et al., 1993) by discriminative estimation of a conditional maximum-entropy model from partially labeled data (see Riezler et al. (2002)). We follow Collins' (2000) approach to discriminative reranking (see also (Riezler et al., 2002)). This was done to some extent in Riezler et al (2002) to automatically generate training data for the log-linear disambiguation component of XLE. XLE selects the most probable analysis from the potentially large candidate set by means of a stochastic disambiguation component based on a log-linear probability model (Riezler et al, 2002) that works on the packed representations. XLE selects the most probable analysis from the potentially large candidate set by means of a stochastic disambiguation component based on a log-linear (a.k.a. maximum-entropy) probability model (Riezler et al, 2002). For a more detailed description of the optimization problem and the feature-functions we use for stochastic LFG parsing see Riezler et al (2002). We have access to the entire English-language text of Wikipedia (about 2M pages) that was parsed using the XLE parser (Riezler et al, 2002), as well as an architecture for distributed data mining within this corpus, called Oceanography (Waterman, 2009). The primary linguistic analysis components are the probabilistic LFG grammar for English developed at PARC (Riezler et al., 2002), and a combination of systems for frame semantic annotation: the probabilistic Shalmaneser system for frame and role annotation (Erk and Pado, 2006), and the rule-based Detour system for frame assignment (Burchardt et al, 2005. Following Pereira and Schabes' (1992) success with partial annotations in training a model of (English) constituents generatively, their idea has been extended to discriminative estimation (Riezler et al., 2002) and also proved useful in modeling (Japanese) dependencies (Sassano, 2005).
A Practical Part-Of-Speech Tagger We present an implementation of a part-of-speech tagger based on a hidden Markov model. The methodology enables robust and accurate tagging with few resource requirements. Only a lexicon and some unlabeled training text are required. Accuracy exceeds 96%. We describe implementation strategies and optimizations which result in high-speed operation. Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment. 1 Desiderata Many words are ambiguous in their part of speech. For example, &quot;tag&quot; can be a noun or a verb. However, when a word appears in the context of other words, the ambiguity is often reduced: in &quot;a tag is a part-of-speech label,&quot; the &quot;tag&quot; can only be a noun. A tagger is a system that uses context to assign parts of speech to words. Automatic text tagging is an important first step in discovering the linguistic structure of large text corpora. Part-of-speech information facilitates higher-level analysis, such as recognizing noun phrases and other patterns in text. For a tagger to function as a practical component in a language processing system, we believe that a tagger must be: corpora contain ungrammatical constructions, isolated phrases (such as titles), and nonlinguistic data (such as tables). Corpora are also likely to contain words that are unknown to the tagger. It is desirable that a tagger deal gracefully with these situations. a tagger is to be used to analyze arbitrarily large corpora, it must be efficient—performing in time linear in the number of words tagged. Any training required should also be fast, enabling rapid turnaround with new corpora and new text genres. A should attempt to assign the correct part-of-speech tag to every word encountered. A should be able to take advantage of linguistic insights. One should be able to correct errors by supplying appropriate priori &quot;hints.&quot; It should be possible to give different hints for different corpora. effort required to retarget a tagger to new corpora, new tagsets, and new languages should be minimal. 2 Methodology 2.1 Background Several different approaches have been used for building text taggers. Greene and Rubin used a rule-based approach in the TAGGIT program [Greene and Rubin, 1971], which was an aid in tagging the Brown corpus [Francis and KuEera, 1982]. TAGGIT disambiguated 77% of the corpus; the rest was done manually over a period of several years. More recently, Koskenniemi also used a rule-based approach implemented with finite-state machines [Koskenniemi, 1990]. Statistical methods have also been used (e.g., [DeRose, [Garside al., These provide the capability of resolving ambiguity on the basis of most likely interpretation. A form of Markov model has been widely used that assumes that a word depends probabilistically on just its part-of-speech category, which in turn depends solely on the categories of the preceding two words. Two types of training (i.e., parameter estimation) have been used with this model. The first makes use of a tagged training corpus. Derouault and Merialdo use a bootstrap method for training [Derouault and Merialdo, 1986]. At first, a relatively small amount of text is manually tagged and used to train a partially accurate model. The model is then used to tag more text, and the tags are manually corrected and then used to retrain the model. Church uses the tagged Brown corpus for training [Church, 1988]. These models involve probabilities for each word in the lexicon, so large tagged corpora are required for reliable estimation. The second method of training does not require a tagged training corpus. In this situation the Baum-Welch algorithm (also known as the forward-backward algorithm) can be used [Baum, 1972]. Under this regime the model is a Markov model as state transitions (i.e., part-of-speech categories) are assumed to be unobservable. Jelinek has used this method for training a text tagger [Jelinek, 1985]. Parameter smoothing can be conachieved using the method of interpolawhich weighted estimates are taken from secondand first-order models and a uniform probability distribution [Jelinek and Mercer, 1980]. Kupiec used word equivclasses (referred to here as classes) on parts of speech, to pool data from individual words [Kupiec, 1989b]. The most common words are still represented individually, as sufficient data exist for robust estimation. 133 However all other words are represented according to the set of possible categories they can assume. In this manner, the vocabulary of 50,000 words in the Brown corpus can be reduced to approximately 400 distinct ambiguity classes [Kupiec, 1992]. To further reduce the number of parameters, a first-order model can be employed (this assumes that a word's category depends only on the immediately preceding word's category). In [Kupiec, 1989a], networks are used to selectively augment the context in a basic firstorder model, rather than using uniformly second-order dependencies. 2.2 Our approach We next describe how our choice of techniques satisfies the listed in section 1. The use of an complete flexibility in the choice of training corpora. Text from any desired domain can be used, and a tagger can be tailored for use with a particular text database by training on a portion of that database. Lexicons containing alternative tag sets can be easily accommodated without any need for re-labeling the training corpus, affording further flexibility in the use of specialized tags. As the resources required are simply a lexicon and a suitably large sample of ordinary text, taggers can be built with minimal effort, even for other languages, such as French (e.g., [Kupiec, 1992]). The use of ambiguity classes and a first-order model reduces the number of parameters to be estimated without significant reduction in accuracy (discussed in section 5). This also enables a tagger to be reliably trained using only moderate amounts of text. We have produced reasonable results training on as few as 3,000 sentences. Fewer parameters also reduce the time required for training. Relatively few ambiguity classes are sufficient for wide coverage, so it is unlikely that adding new words to the lexicon requires retraining, as their ambiguity classes are already accommodated. Vocabulary independence is achieved by predicting categories for words not in the lexicon, using both context and suffix information. Probabilities corresponding to category sequences that never occurred in the training data are assigned small, non-zero values, ensuring that the model will accept any sequence of tokens, while still providing the most likely tagging. By using the fact that words are typically associated with only a few part-ofspeech categories, and carefully ordering the computation, the algorithms have linear complexity (section 3.3). 3 Hidden Markov Modeling The hidden Markov modeling component of our tagger is implemented as an independent module following the specgiven in [Levinson et with special attention to space and time efficiency issues. Only first-order modeling is addressed and will be presumed for the remainder of this discussion. 3.1 Formalism brief, an a doubly stochastic process that generates sequence of symbols ={Sl,S2, , 1 <i< where W is some finite set of possible symbols, by composing an underlying Markov process with a state-dependent symbol generator (i.e., a Markov process with noise).' Th Markov process captures the notion of sequence depen and is described by a set of a matrix c probabilities A -= 1 < a, the probability of moving from state i to state of initial probabilities H = {70 1 < i < is the probability of starting in state i. The symbol ger erator is a state-dependent measure on V described by of symbol probabilities 1 < j < < < M = IW I is the probability ( symbol given that the Markov process is i In part-of-speech tagging, we will model word order di pendency through an underlying Markov process that 01 erates in terms of lexical tags,' yet we will only be ab to observe the sets of tags, or ambiguity classes, that ai possible for individual words. The ambiguity class of eac word is the set of its permitted parts of speech, only or of which is correct in context. Given the parameters .4, Markov modeling allows us to compute ti most probable sequence of state transitions, and hence a mostly likely sequence of lexical tags, corresponding to of ambiguity classes. In the following, identified with the number of possible .tags, and W wil the set of all ambiguity classes. Applying an HMM consists of two tasks: estimating ti parameters A, a training set; ar computing the most likely sequence of underlying sta transitions given new observations. Maximum likeliho( estimates (that is, estimates that maximize the probabili of the training set) can be found through application of z ternating expectation in a procedure known as the Baur Welch, or forward-backward, algorithm [Baum, 1972]. proceeds by recursively defining two sets of probabiliti( the forward probabilities eft+1. (i) = bi(St+i) < t < T — 1, ( = for all the backward prob bilities, i3(i) = — 1 < t < 1, ( 1.1 1 for all forward probabili the joint probability of the sequence up to tir t, S2, , the event that the Markov pr is in state i at time the backwa is the probability of seeing the sequen , ST} that the Markov process is state i at time t. It follows that the probability of t entire sequence is N N = The two systems we use are ENGCG (Karlsson et al, 1994) and the Xerox Tagger (Cutting et al, 1992). The Xerox Tagger 1, XT, (Cutting et al, 1992) is a statistical tagger made by Doug Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun in Xerox PARC. There are many POS taggers developed using different techniques for many major languages such as transformation-based error-driven learning (Brill, 1995), decision trees (Black et al, 1992), Markov model (Cutting et al, 1992), maximum entropy methods (Ratnaparkhi, 1996) etc for English. The initial phase relies on a parser that draws on the SPECIALIST Lexicon (McCray et al 1994) and the Xerox Part-of-Speech Tagger (Cutting et al 1992) to produce an underspecified categorial analysis. The phrases in our bibliographic and clinical samples were then submitted to an underspecified syntactic analysis described by Rindflesch et al (2000) that draws on a stochastic tagger (see (Cutting et al, 1992) for details) as well as the SPECIALIST Lexicon, a large syntactic lexicon of both general and medical English that is distributed with the UMLS. As a common strategy, POS guessers examine the endings of unknown words (Cutting et al 1992) along with their capitalization, or consider the distribution of unknown words over specific parts-of-speech (Weischedel et aL, 1993). The tagger used is thus one that does not need tagged and disambiguated material to be trained on, namely the XPOST originally constructed at Xerox Parc (Cutting et al 1992, Cutting and Pedersen 1993). In such cases, additional information may be coded into the HMM model to achieve higher accuracy (Cutting et al, 1992). The semi-supervised model described in Cutting et al (1992), makes use of both labeled training text and some amount of unlabeled text.  (Chanod and Tapanainen, 1995) compare two tagging frameworks for tagging French, one that is statistical, built upon the Xerox tagger (Cutting et al., 1992), and another based on linguistic constraints only. The XEROX tagger comes with a list of built-in ending guessing rules (Cutting et al,1992). This analysis depends on the SPECIALIST Lexicon and the Xerox part-of-speech tagger (Cutting et al, 1992) and provides simple noun phrases that are mapped to concepts in the UMLS Metathesaurus using MetaMap (Aronson, 2001). The prime public domain examples of such implementations include the Trigrams? n? Tags tagger (Brandts 2000), Xerox tagger (Cutting et al 1992) and LT POS tagger (Mikheev 1997). It is also possible to train statistical models using unlabeled data with the expectation maximization algorithm (Cutting et al, 1992).  It has been known for some years that good performance can be realized with partial tagging and a hidden Markov model (Cutting et al, 1992).  (Cutting et al, 1992) reported very high results (96% on the Brown corpus) for unsupervised POS tagging using Hidden Markov Models (HMMs) by exploiting hand-built tag dictionaries and equivalence classes. In the tagging literature (e.g., Cutting et al (1992)) an ambiguity class is often composed of the set of every possible tag for a word.
Machine Learning For Coreference Resolution: From Local Classification To Global Ranking In this paper, we view coreference resolution as a problem of ranking candidate partitions generated by different coreference systems. We propose a set of partition-based features to learn a ranking model for distinguishing good and bad partitions. Our approach compares favorably to two state-of-the-art coreference systems when evaluated on three standard coreference data sets. (Ng, 2005) treats coreference resolution as a problem of ranking candidate partitions generated by a set of coreference systems. The main difference between this approach and ours is that (Ng, 2005)'s approach takes coreference resolution one step further, by comparing the results of multiple systems, while our system is a single resolver; furthermore, he emphasizes the global optimization of ranking clusters obtained locally, whereas our focus is on globally optimizing the clusterization method inside the resolver. There are many different training example generation algorithms, e.g., McCarthy and Lehnert's method, Soon et als method, Ng and Cardies method (Ng, 2005). Similar to many previous works on co-reference (Ng, 2005), we cast the problem as a classification task and solve it in two steps: (1) train a classifier to determine whether two mentions are co-referent or not, and (2) use a clustering algorithm to partition the mentions into clusters, based on the pairwise predictions. To our knowledge, the best results on this dataset were obtained by the meta-classification scheme of Ng (2005). Although our train-test splits may differ slightly, the best B-Cubed F1 score reported in Ng (2005) is 69.3%, which is considerably lower than the 79.3% obtained with our method. Ng (2005) learns a meta-classifier to choose the best prediction from the output of several coreference systems. This could be incorporated in a ranking scheme, as in Ng (2005). The results are comparable to those reported in (Ng, 2005) which uses similar features and gets an F-measure of about 62% for the same data set.   MUC and B3 metrics (Ng, 2005a). Recent work has examined such models; Luo et al. (2004) using Bell trees, and McCallum and Wellner (2004) using conditional random fields, and Ng (2005) using rerankers. A third global approach is offered by Ng (2005), who proposes a global reranking over partitions generated by different coreference systems. Other work on global models of coreference (as opposed to pairwise models) has included: Luo et al (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree; McCallum and Wellner (2004) who defined several conditional random field-based models; Ng (2005) who took a reranking approach; and Culotta et al (2006) who use a probabilistic first-order logic model. Similarly, the method of (Ng, 2005) ranks base models according to their performance on separate tuning set, and then uses the highest-ranked base model for predicting on test documents. The results are comparable to those reported in (Ng, 2005) which uses similar features and gets an F-measure ranging in 50-60% for the same data set. According to Ng (2005), most learning based coreference systems can be defined by four elements: the learning algorithm used to train the coreference classifier, the method of creating training instances for the learner, the feature set used to represent a training or test instance, and the clustering algorithm used to coordinate the coreference classification decisions. This strategy has been described as best-first clustering by Ng (2005). In contrast to Ng (2005), Ng and Cardie (2002a) proposed a rule-induction system with rule pruning.
A* Parsing: Fast Exact Viterbi Parse Selection We present an extension of the classic A* search procedure to tabular PCFG parsing. The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions. We discuss various estimates and give efficient algorithms for computing them. On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%. Unlike best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation. Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time. In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I (ak), but also the outside probability O (ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for word based ITGs (Zhang and Gildea, 2005). In other words, the coarse outside score computed by the algorithm plays the same role as a heuristic in standard A* parsing (Klein and Manning, 2003). The search algorithm for the best ITG alignment, a best-first chart parsing (Charniak et al., 1998), was augmented with an A* search heuristic of quadratic complexity (Klein and Manning, 2003), resulting in significant reduction in computational complexity. Tsuruoka and Tsujii (2004) explore the frame work developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap manipulations necessary in this scheme.    A specific case of this algorithm is the A* parsing of Klein and Manning (2003) where they achieve significant speed up using carefully designed heuristic functions. We intend to explore other methods for pruning the space and agenda-based parsing, in particular A* parsing (Klein and Manning, 2003), which will allow only the most probable parts of the chart to be built, improving efficiency while still ensuring the optimal derivation is found. In addition to presenting the algorithm, we show experiments in which we extract k-best lists for three different kinds of grammars: the lexicalized grammars of Klein and Manning (2003b), the state-split grammars of Petrov et al (2006), and the tree transducer grammars of Galley et al (2006). parsing algorithm of Klein and Manning (2003c) can be formulated in terms of weighted deduction rules (Felzenszwalb and McAllester, 2007). see Klein and Manning (2003c) for details. If the heuristic is consistent, then A* guarantees that whenever an inside item comes off the agenda, its weight is its true Viterbi inside score (Klein and Manning, 2003c). We also experimented with the lexicalized parsing model described in Klein and Manning (2003b).   We used a simple but effective heuristic for these grammars, similar to the FILTER heuristic suggested in Klein and Manning (2003c). Klein and Manning (2003a) went on to describe admissible heuristics and an A* framework for parsing. The A* heuristics explored by Klein and Manning (2003a) can be seen as resulting from bounding transformations. 
Emotions From Text: Machine Learning For Text-Based Emotion Prediction In addition to information, text con tains attitudinal, and more specifically, emotional content. This paper exploresthe text-based emotion prediction prob lem empirically, using supervised machinelearning with the SNoW learning architecture. The goal is to classify the emotional affinity of sentences in the narra tive domain of children?s fairy tales, forsubsequent usage in appropriate expressive rendering of text-to-speech synthe sis. Initial experiments on a preliminarydata set of 22 fairy tales show encourag ing results over a na??ve baseline and BOW approach for classification of emotional versus non-emotional contents, with some dependency on parameter tuning. We also discuss results for a tripartite model which covers emotional valence, as well as feature set alernations. In addition, we present plans for a more cognitively soundsequential model, taking into considera tion a larger set of basic emotions. Alm et al (2005) and Francisco and Gervas (2006) worked on fairy tales. They later use this corpus to construct a reasonably accurate classifier for emotional states of sentences (Alm et al., 2005). The second is the frequency with which the character is associated with emotional language - their emotional trajectory (Alm et al 2005). These tools and resources have been already used in a large number of applications, including expressive text-to-speech synthesis (Alm et al, 2005), tracking sentiment timelines in on line forums and news (Balog et al, 2006), analysis of political debates (Carvalho et al, 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012).
A Stochastic Parts Program And Noun Phrase Parser For Unrestricted Text There are five missing brackets which are indicated as &quot;*[&quot; or &quot;1&quot;. Words with a second NP tag were identified as proper nouns in a prepass. [A/AT former/AP top/NN aide/NN] to/IN [At-torney/NP/NP General/NP/NP Edwin/NP/NP Meese/NP/NP] interceded/VBD to/TO extend/VB [an/AT aircraftNN company/NN 's/$ govern-ment/NN contract/NN] ,/, then/RB went/VBD into/IN [business/NM] with/IN [a/AT lobby-ist/NN1 [who/WPS] worked/VBD for/IN [the/AT defense/NN contractor/NN] ,/, according/IN to/IN [a/AT published/VBN report/NN] ./. [James/NP/NP E/NP./NP Jenkins/NP/NP] ,/, [a/AT one-time/JJ senior/JJ deputy/NN] to/IN [Meese/NP/NP] joined/VBD [the/AT board/NN] of/IN [directors/NNS] of/IN [Transworld/NP/NP Group/NP/NP Ltd/NP./NP] on/IN [April/NP/NP 28/CD] ,/, [1984/CD] ,/, [the/AT Chicago/NP/NP Tribune/NP/NP] reporteci/VBD in/IN [its/PP$ Tuesday/NR editions/NNS] ./. [The/AT principal/JJ figure/NN] in/IN [Trans-world/NP/NP] was/BEDZ [Richard/NP/NP Mill-man/NP/NP] ,/, [a/AT lobbyist/NN] for/IN [Fair-child/NP/NP Industries/NP/NP Inc/NP./NP] ,/, Virginia/NP/NP de fense/NN con- 142 tractor/NN] I, [the/AT Tribune/NP/NP] said/VBD .1. [A/AT federal/JJ grand/JJ jury/NN] is/BEZ in- [the/AT Fairchild/NP/NP transaction/NN] and/CC [other/AP actions/NNS] of/IN [Meese/NP/NP] and/CC [former/AP White/NP/NP House/NP/NP aide/NN Nofziger/NP/NP] in/IN [connection/NN] with/IN [Wedtech/NP/NP New/NP/NP York/NP/NP defense/NN company/NN] [that/WPS] received/VBD [$250/CD million/CD] in/IN [govemment/NN contracts/NNS] issued/VBN without/EN [competitive/JJ bidding/NN] during/IN [the/AT Reagan/NP/NP administration/NN] ./. [Jenkins/NP/NP] left/VBD [the/AT White/NP/NP House/NP/NP] in/IN [1984/CD] 1, and/CC joined/VBD [Wedtech/NP/NP] as/CS [its/PP$ director/NN] of/IN [marketing/NN *]*[ two/CD years/NNS] later/RBR .1. [Deborah/NP/NP Tucker/NP/NP] ,/, [a/AT spokeswoman/NN] for/IN [Fairchild/NP/NP] ,/, said/VBD [Friday/NR] that/CS [the/AT cornpany/NN] had/HVD been/BEN contacted/VBN by/IN [the/AT office/NN] of/IN [independent/JJ counsel/NN James/NP/NP McKay/NP/NP] and/CC [subpoenas/NNS] had/HVD been/BEN served/VBN on/IN [Fairchild/NP/NP] ./. [Tucker/NP/NP] said/VBD [the/AT investigation/NN] involving/IN [Fairchild/NP/NP] had/HVD been/BEN going/VBG on/IN [a/AT number/NN] of/IN [weeks/NNS] and/CC predates/VBZ [last/AP week/NN 's/$ exof/1N [McKay/NP/NP 's/$ investigation/NN] to/TO include/VB [Meese/NP/NP] ./. [The/AT company/NN] is/BEZ cooperating/VBG in/1N [the/AT investigation/NN] ,/, [Tucker/NP/NP] said/VBD ./. [A/AT source/NN *] close/NN***] to/IN [McKay/NP/NP] said/VBD [last/AP week/NN&quot; that/CS [Meese/NP/NP] isn't/BEZ* under/IN [cruninalaJ investigation/NN] in/IN [the/AT Fairchild/NP/NP matter/NN] ,/, but/RB is/BEZ [a/AT witness/NN] ./. [The/NP Tribune/NP/NP] said/VBD [Mill- ,/, acting/VBG as/CS [a/AT lobbyist/NN] for/1N [the/AT Chantilly/NP/NP] ,/, [Va/NP.-based/NP company/NN] ,/, went/VBD to/TO see/VB [Jenkins/NP/NP] in/IN [1982/CD] and/CC urged/VBD [him/PPO] and/CC [Meese/NP/NPI to/TO encourage/VB [the/AT Air/NP/NP Force/NP/NP] to/E0 extend/VB [the/AT production/NN] of/1/•1 [Fairchild/NP/NP 's/$ A-10/NP bomber/NN] for/IN [a/AT year/NN] ./. [Millman/NP/NP] said/VBD there/RB was/BEDZ [a/AT lucrative/JJ market/NN] in/IN [Third/NP/NP World/NP/NP countries/NNS] ,/, but/CC that/CS [Fairchild/NP/NP 's/$ chances/NNS] would/MD be/BE limited/VBN if/CS [the/AT Air/NP/NP Force/NP/NP] was/BEDZ not/* producing/VBG [the/AT plane/NN] ./. [The/AT Air/NP/NP Force/NP/NP] had/HVD decided/VBN to/TO discontinue/VB [production/NN] of/IN [the/AT A-10/NP] ,/, [a/AT 1960s-era/CD ground-support/NN attack/NN bomber/NN] at/IN [the/AT time/NN *]*[ Fairchild/NP/NP] was/BEDZ hoping/VBG to/TO sell/VB [A-10s/NP] abroad/RB j, [the/AT Tribune/NP/NP] said/VBD ./. [The/AT newspaper/NN] said/VBD [one/CD source/NN] reported/VBD that/CS after/CS [Millman/NP/NP] made/VBD [his/PPS pitch/NN] J, [Meese/NP/NP] ordered/VBD [Jenlcins/NP/NP] to/TO prepare/VB [a/AT memo/NN] on/IN [behalf/NN] of/IN [Fairchild/NP/NP] ./. [Memos/NP***] signed/VBD by/IN [Meese/NP/NP] ,/, stressing/VBG [the/AT importance/NN] of/IN [Fairchild/NP/NP 's/$ arranging/VBG sales/NNS] in/IN [Third/NP/NP World/NP/NP countries/NNS] j, were/BED sent/VBN to/IN [the/AT State/NP/NP Department/NP/NP] and/CC [the/AT Air/NP/NP Force/NP/NP] ./. [Millman/NP/NP] did/DOD not/* return/VB [telephone/NN calls/NNS] to/EN [his/PP$ office/NN1 and/CC [referral/NN numbers/NNS] [Monday/NR] ,I, [the/AT Tribune/NP/NP] said/VBD ./. We have found that if we first tag every word in the corpus with a part of speech using a method such as Church (1988) or DeRose (1988), and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with a following preposition to~in and verbs associated with a following infinitive marker to~to. This work is to be distinguished from supervised part-of-speech disambiguation systems, which use labeled training data (Church, 1988), unsupervised disambiguation systems, which use a dictionary of possible tags for each word (Merialdo, 1994), or prototype driven systems which use a small set of prototypes for each class (Haghighi and Klein, 2006). Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., Church (1988)) can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency. The first work on this topic was done back in the eighties (Church, 1988). Church's Parts of speech [Church, 1988] performs not only part-of-speech analysis, but it also identities the most simple kinds of noun phrases mostly sequences of determiners, premodifiers and nominal heads by inserting brackets around them. The appendix in [Church, 1988] lists the analysis of a small text. This is quite feasible using statistical taggers like those of Garside (1987), Church (1988) or Foster (1991) which achieve performance upwards of 97% on unrestricted text. The shallow parser constructs Verb Groups (VGs) and basic Noun Phrases (NPs), also called BaseNPs [Church 1988]. Several approaches provide similar output based on statistics (Church 1988, Zhai 1997, for example), a finite-state machine (AitMokhtar and Chanod 1997), or a hybrid approach combining statistics and linguistic rules (Voutilainen and Padro 1997). As we said at the out 211 set, we don't necessarily believe HunPos to be in any way better than TnT, and certainly the main ideas have been pioneered by DeRose (1988), Church (1988), and others long before this generation of HMM work. Instead of employing the source-channel paradigm for tagging (more or less explicitly present e.g. in (Merialdo, 1992), (Church, 1988), (Hajji, Hladk~, 1997)) used in the past (notwithstanding some exceptions, such as Maximum Entropy and rule-based taggers), we are using here a direct approach to modeling, for which we have chosen an exponential probabilistic model. Regardless of whether one is using HMMs, maximum entropy conditional sequence models, or other techniques like decision trees, most systems work in one direction through the sequence (normally left to right, but occasionally right to left ,e.g., Church (1988)). Figure 2: Change in the polarity of the sentences citing Church (1988) paper how the way a published work is perceived by the research community over time. Figure 2 shows the result of this analysis when applied to the work of Kenneth Church (1988) on part-of-speech tagging. For example, the analysis illustrated in Figure 2 shows that the work of Ken Church (1988) on part-of-speech tagging received significant positive feedback during the 1990s and until early 2000s before it started to receive more negative feedback. The high accuracy achieved by a corpus-based approach to part-of-speech tagging and noun phrase parsing, as demonstrated by (Church, 1988), has inspired similar approaches to other problems in natural language processing, including syntactic parsing and word sense disambiguation (WSD). Termight uses a part of speech tagger (Church, 1988) to identify a list of candidate terms which is then filtered by a manual pass. The multi-word terms match a small set of syntactic patterns defined by regular expressions and are found by searching a version of the document tagged with parts of speech (Church, 1988). More recently, the natural language processing community has effectively employed these models for part-of speech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al, 1993). The algorithm runs in lockstep with a part-of-speech tagger (Church, 1988), which is used for deciding possible word replacements.
Parser Adaptation and Projection with Quasi-Synchronous Grammar Features We connect two scenarios in structured parser trained on one corpus to another annotation style, and annotations from one to another. We propose quasigrammar features for these structured learning tasks. That is, we score a aligned pair of source and target trees based on local features of the trees and the alignment. Our quasi-synchronous model assigns positive probability to any alignment of any trees, in contrast to a synchronous grammar, which would insist on some form of structural parallelism. In monolingual dependency parser adaptation, we achieve high accuracy in translating among multiple annotation styles for the same sentence. On the more difficult problem of cross-lingual parser projection, we learn a dependency parser for a target language by using bilingual text, an English parser, and automatic word alignments. Our experiments show that unsupervised QG projection improves on parses trained using only highprecision projected annotations and far outperforms, by more than 35% absolute dependency accuracy, learning an unsupervised parser from raw target-language text alone. When a few target-language parse trees are available, projection gives a boost equivalent to doubling the number of target-language trees. first author would like to thank the Center for Intelligent Information Retrieval at UMass Amherst. We would also like to thank Noah Smith and Rebecca Hwa for helpful discussions and the anonymous reviewers for their suggestions for improving the paper. Following this idea, Ganchev et al (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results.  QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010).  Another projection based system is that of Smith and Eisner (2009), who report results for German (68.5%) and Spanish (64.8%) on sentences of length 15 and less inclusive of punctuation. Smith and Eisner (2009) and Li et al (2012) generated rich quasi synchronous grammar features to improve parsing performance. Smith and Eisner (2009) think of cross-language adaptation as unsupervised projection using word aligned parallel text to construct training material for the target language.  Smith and Eisner (2009) propose effective QG features for parser adaptation and projection. Smith and Eisner (2009) perform dependency projection and annotation adaptation with quasi-synchronous grammar features. 
Methods For The Qualitative Evaluation Of Lexical Association Measures This paper presents methods for a qualitative, unbiased comparison of lexical association measures and the results we have obtained for adjective-noun pairs and preposition-noun-verb triples extracted from German corpora. In our approach, we compare the entire list of candidates, sorted according to the particular measures, to a reference set of manually identified “true positives”. We also show how estimates for the very large number of hapaxlegomena and double occurrences can be inferred from random samples. We are aware of the fact that other measures of lexical association have been proposed (Evert and Krenn, 2001, and MI values were computed using Adam Berger's trigger toolkit (Berger, 1997).  Following the methodology described by Evert and Krenn (2001), German PP-verb combinations were extracted from a chunk-parsed version of the Frankfurter Rundschau Corpus.  In particular, the precision/recall value comparison between the various AMs exhibits a rather inconclusive picture in Evert and Krenn (2001) and Krenn and Evert (2001) as to whether sophisticated statistical AMs are actually more viable than frequency counting. In particular Evert and Krenn (2001) use the chi-square test which assumes independent samples and is thus not really suitable for testing the significance of differences of two or more measures which are typically run on the same set of candidates (i.e., a dependent sample). As the standard statistical AM, we selected the t-test (see also Manning and Sch¨utze (1999) for a description on its use in CE and ATR) because it has been shown to be the best-performing statisticsonly measure for CE (cf. Evert and Krenn (2001) and Krenn and Evert (2001)) and also for ATR (see Wermter and Hahn (2005)). To overcome these limitations, we use the evaluation method described by Evert and Krenn (2001). Cf. also Evert and Krenn (2001) for empirical evidence justifying the exclusion of low-frequency data. The comparison to the t-test is especially interesting because it was found to achieve the best overall precision scores in other studies (see Evert and Krenn (2001)). We also define four features that represent known collocation measures (Evert and Krenn, 2001): Point-wise mutual information (PMI); T-Score; log-likelihood; and the raw frequency of N1 N2 in the corpus. To eliminate noisy low-frequency data (cf. also Evert and Krenn (2001)), we defined different frequency cut-off thresholds, c, for the bigram, trigram and quadgram candidate sets and only considered candidates above these thresholds. We compare our P-Mod algorithm against the t-test measure, which, of all standard measures, yields the best results in general-language collocation extraction studies (Evert and Krenn, 2001), and also against the widely used C-value, which aims at enhancing the common frequency of occurrence measure by making it sensitive to nested terms (Frantzi et al, 2000). Studies on collocation extraction (e.g., by Evert and Krenn (2001)) also point out the inadequacy of such evaluation methods. The evaluation procedure used here (first suggested by Evert and Krenn (2001) for evaluating measures of lexical association) involves producing and evaluating just such a ranking. This is consistent with results reportedby Evert and Krenn (2001). Our evaluation was partly inspired by Evert and Krenn (2001). In Krenn & Evert 2001, frequency outperformed mutual information though not the t test, while in Evert and Krenn 2001, log-likelihood and the t-test gave the best results, and mutual information again performed worse than frequency.
Experiments With A Multilanguage Non-Projective Dependency Parser  Bick (2006) used the lowercased FORM if the LEMMA is not available, Corston-Oliver and Aue (2006) a prefix and Attardi (2006) a stem derived by a rule-based system for Danish, German and Swedish. Use only some components, e.g. Bick (2006) uses only case, mood and pronoun subclass and Attardi (2006) uses only gender, number, person and case. The most efficient parsers are greedy transition-based parsers, which only explore a single derivation for each input and relies on a locally trained classifier for predicting the next parser action given a compact representation of the derivation history, as pioneered by Yamada and Matsumoto (2003), Nivre (2003), Attardi (2006), and others. Speech tagger described in Dell Orletta (2009) and dependency parsed by the DeSR parser (Attardi,2006) using Support Vector Machine as learning algorithm. This model is a version of DeSR (Attardi, 2006), a deterministic classifier-based Shift/Reduce parser. Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006) .Nivre (2009) introduced a transition based non projective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time.  However, other non-projective parsers such as (Attardi, 2006) follow a constructive approach and can be analysed deductively. However, the goal of that transition is different from ours (selecting between projective and non projective parsing, rather than building some arcs in advance) and the approach is specific to one algorithm while ours is generic for example, the LEFT ARC transition can not be added to the arc-standard and arc-eager parsers, or to extensions of those like the ones by Attardi (2006) or Nivre (2009), because these already have it. Non-projective transitions that create dependency arcs between non-contiguous nodes have been used in the transition-based parser by Attardi (2006). DeSR (Attardi, 2006) is an incremental deterministic classifier-based parser. This idea is demonstrated by Attardi (2006), who proposes a transition system whose individual transitions can deal with non-projective dependencies only to a limited extent, depending on the distance in the stack of the nodes involved in the newly constructed dependency. The reported coverage in Attardi (2006) is already very high when the system is restricted to transitions of degree two or three. Table 1 gives additional statistics for treebanks from the CoNLL-X shared task (Buchholz and Marsi, 2006). We now turn to describe our variant of the transition system of Attardi (2006), which is equivalent to the original system restricted to transitions of degree two. Table 1: The number of non-projective relations of various degrees for several treebanks (training sets), as reported by the parser of Attardi (2006). We turn next to describe the equivalence between our system and the system in Attardi (2006). While in the previous sections we have described a tabular method for the transition system of Attardi (2006) restricted to transitions of degree up to two, it is possible to generalize the model to include higher degree transitions. We build upon DeSR, the shift-reduce parser described in (Attardi, 2006). (Attardi, 2006)) have been introduced for handling non-projective dependency trees: i.e., trees that can not be drawn in the plane without crossing edges. ULISSE was tested against the output of two really different data-driven parsers: the first order Maximum Spanning Tree (MST) parser (McDonald et al., 2006) and the DeSR parser (Attardi, 2006) using Support Vector Machine as learning algorithm.
What's In A Translation Rule? We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data. Galley et al (2004) extract translation rules from a large parsed parallel corpus that extend in scope to tree fragments beyond a single node; we believe that adding such larger-scale operations to the translation model is likely to significantly improve the performance of syntactically supervised alignment. In terms of tree-to-string translation rule extraction, the toolkit implements the traditional maximum likelihood algorithm using PCFG trees (Galley et al, 2004) and HPSG trees/forests (Wu et al, 2010). In particular, we implemented the GHKM algorithm as proposed by Galley et al (2004) from word-aligned tree string pairs.  For the tree-to-string model, we parsed English sentences using Stanford parser and extracted rules using the GHKM algorithm (Galley et al, 2004). Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. From word level alignments, such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages (Galley et al., 2004), or with the the word-level alignments alone without reference to external syntactic analysis (Chiang, 2005), which is the scenario we address here. algorithm (Galley et al, 2004) to forest-based by introducing non-deterministic mechanism. GHKM (Galley et al, 2004) is used to generate the baseline TTS templates based on the word alignments computed using GIZA++ and different combination methods, including union and the diagonal growing heuristic (Koehn et al, 2003). Galley et al (2004) describe how to learn hundreds of millions of tree transformation rules from a parsed, aligned Chinese/English corpus, and Galley et al (submitted) describe probability estimators for those rules. In order to test whether good translations can be generated with rules learned by Galley et al (2004), we created DerivTool as an environment for interactively using these rules as a decoder would. The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). We aligned the sentence pairs using the GIZA++ toolkit (Och and Ney, 2003) and extracted tree-to-string rules according to the GHKM algorithm (Galley et al 2004). Galley et al (2004) alleviate this modeling problem and present a method for acquiring millions of syntactic transfer rules from bilingual corpora, which we review below. We contrast our work with (Galley et al, 2004), highlight some severe limitations of probability estimates computed from single derivations, and demonstrate that it is critical to account for many derivations for each sentence pair. Finally, we show that our contextually richer rules provide a 3.63 BLEU point increase over those of (Galley et al, 2004). Galley et al (2004) present one such formalism (henceforth 'GHKM'). Formally, transformational rules ri presented in (Galley et al, 2004) are equivalent to 1-state x Rs transducers mapping a given pattern (subtree to match in pi) to a right hand side string. In this paper, we developed probability models for the multi-level transfer rules presented in (Galley et al, 2004), showed how to acquire larger rules that crucially condition on more syntactic context, and how to pack multiple derivations, including interpretations of unaligned words, into derivation forests. Typically, by using the GHKM algorithm (Galley et al 2004), translation rules are learned from word-aligned bilingual texts whose source side has been parsed by using a syntactic parser.
A Problem For RST: The Need For Multi-Level Discourse Analysis (1992). &quot;Planning text for advisory dialogues: Capturing intentional, rhetorical and attentional information.&quot; Moore and Pollack (1992) gave an example of a simple discourse. Moore and Pollack (1992) note that Rhetorical Structure Theory conflates the informational (the information being conveyed) and intentional (the effects on the reader's beliefs or attitudes) levels of discourse. Moore and Pollack (1992) have already shown that different high-level intentions yield different RS-trees. For example, for the text shown in (1 I) below, which is taken from (Moore and Pollack, 1992), one may argue from an informational perspective that A3 is a CONDITION for B3. However, the results are trees of Rhetorical Structure Theory (RST) (Mann and Thompson, 1986), and the classifiers rely on well-formedness constraints on RST trees which are too restrictive (Moore and Pollack, 1992). Moore and Pollack (1992) argue that both informational (semantic) and intentional relations can hold between clause simultaneously and independently. Finally, sometimes more than one relation can hold between two given units (Moore and Pollack, 1992).
Identifying Sources Of Opinions With Conditional Random Fields And Extraction Patterns Recent systems have been developed forsentiment classification, opinion recognition, and opinion analysis (e.g., detecting polarity and strength). We pursue another aspect of opinion analysis: identi fying the sources of opinions, emotions, and sentiments. We view this problem as an information extraction task and adopta hybrid approach that combines Con ditional Random Fields (Lafferty et al, 2001) and a variation of AutoSlog (Riloff,1996a). While CRFs model source identification as a sequence tagging task, AutoSlog learns extraction patterns. Our re sults show that the combination of these two methods performs better than either one alone. The resulting system identifies opinion sources with 79.3% precision and 59.5% recall using a head noun matching measure, and 81.2% precision and 60.6% recall using an overlap measure. (Choi et al, 2005)) and topics of opinions (Stoyanov and Cardie, 2008). We choose a window of [-2,2], as it is usually suggested by the literature (Choi et al., 2005). The features we use (Table 5) are mostly inspired by Choi et al (2005) and by the ones used for plain support vector machines (SVMs) in (Wiegand and Klakow, 2010). Choi et al (2005) and Choi et al (2006) explore conditional random fields, Wiegand and Klakow (2010) examine different combinations of convolution kernels, while Johansson and Moschitti (2010) present a re-ranking approach modeling complex relations between multiple opinions in a sentence.
Automatic Evaluation Of Summaries Using N-Gram Co-Occurrence Statistics Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results. In a recent study (Lin and Hovy, 2003a), we showed that the recall-based unigram co occurrence automatic scoring metric correlates highly with human evaluation and has high recall and precision in predicting the statistical significance of results comparing with its human counter part. Summarization evaluation is done using ROUGE-2 (R-2) (Lin and Hovy, 2003). it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003). Lin and Hovy (2003) have shown that a unigram co-occurrence statistic, computed with stop words ignored, between a summary and a set of models can be used to assign scores for a test suite that highly correlates with the scores assigned by human evaluators at DUC. Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives. Another recent study (Lin and Hovy, 2003) investigated the extent to which extractive methods may be sufficient for summarization in the single-document case. Automatic evaluation was performed with ROUGE (Lin and Hovy, 2003) using TAC-2008parameter settings. Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations. Rouge (Lin and Hovy, 2003) represents another such effort. The automatic evaluation tool, ROUGE (Lin and Hovy, 2003), is run to evaluate the quality of the generated summaries (200 words in length). ROUGE (Lin and Hovy, 2003) has been adopted as a standard evaluation metric in various summarization tasks. Although ROUGE (Lin and Hovy, 2003) is one of the most popular methods for evaluation of summaries, it may not be appropriate for the evaluation of the mediatory summary because the scoring based on N-gram in this method can not be used to consider the fairness described in Section 2. One semi automatic approach to evaluation is ROUGE (Lin and Hovy, 2003), which is primarily based on n gram co-occurrence between automatic and human summaries. We used the ROUGE evaluation approach (Lin and Hovy, 2003), which is based on n-gram co occurrence between machine summaries and ideal human summaries. Lin (Lin and Hovy, 2003) has found that ROUGE-1 and ROUGE-2 correlate well with human judgments.  Our experiments employ the aforementioned AMI meeting corpus: we compare our decision summaries to the manually generated decision abstracts for each meeting and evaluate performance using the ROUGE-1 (Lin and Hovy, 2003) text summarization evaluation metric. We use the ROUGE (Lin and Hovy, 2003) evaluation measure. Our initial experimental results show that our approach is feasible, since it produces summaries, which when evaluated against the TAC 2009 data yield ROUGE scores (Lin and Hovy, 2003) comparable to the participating systems in the Summarization task at TAC 2009. We used the standard ROUGE evaluation (Lin and Hovy, 2003) which has been also used for TAC.
Learning Question Classifiers In order to respond correctly to a free form factual question given a large collection of texts, one needs to un derstand the question to a level that allows determiningsome of the constraints the question imposes on a pos sible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer. This paper presents a machine learning approach toquestion classification. We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into finegrained classes. We show accurate results on a large col lection of free-form questions used in TREC 10. Our approach to QC follows that of (Li and Roth, 2002). We will use the TREC dataset provided by Li and Roth (2002), which assigns 6000 questions with both a coarse and a fine-grained label. This scheme is more suitable here than other common answer-typing schemata such as the one in Li and Roth (2002), which tend to focus on questions asking for factual knowledge. This is important because while large sets of existing questions can be obtained (Li and Roth, 2002), there are many fewer questions with available answers. Our experiments demonstrate that how-question specific unit lists consistently achieve higher answer identification performance than fixed-type, general purpose answer typing (which propose all numerical entities as answer candidates). For example, Li and Roth (2002) assign one of fifty possible types to a question based on features present in the question. (Li and Roth, 2002) propose a system based on SNoW. The same dataset has been used in other investigations, such as in (Li and Roth, 2002). The distribution of these 5500 training questions, with respect to its interrogative pronoun or the initial word is showed in Table 1. (Li and Roth, 2002) obtain a better performance for English, around a 92.5% in terms of accuracy. It could be also interested to test the combination between a better QC system, the current one by Li and Roth's for instance (Li and Roth, 2002), and our machine translation method. Li and Roth (2002) have developed a machine learning approach which uses the SNoW learning architecture. Compared to the over feature size of 200000 in Li and Roth (2002), our feature space is much more compact, yet turned out to be more informative as suggested by the experiments. With the increasing popularity of statistical NLP, Li and Roth (2002), Hacioglu and Ward (2003) and Zhang and Lee (2003) used supervised learning for question classification on a data set from UIUC that is now standard1. Li and Roth (2002) used a Sparse Network of Winnows (SNoW) (Khardon et al, 1999). Our findings corroborate Li and Roth (2002), who report little benefit from adding head chunk features for the fine classification task. in (Li and Roth, 2002) to our basic QA system, YourQA (Quarteroni and Manandhar, 2008) and by gathering the top 20 answer paragraphs.  Answer types are determined using classification rules similar to Li and Roth (2002). The classification scheme we propose is based on one dynamic 1 and one static layer, contrasting with previous work that uses static taxonomies (Li and Roth, 2002). 1500 of those questions come from the Li and Roth corpus (Li and Roth, 2002), 500 questions were taken from the TREC-10 questions and 100 questions were asked over the Italian Opera topic map. We followed Li and Roth (Li and Roth, 2002) to implement the features for the EAT classifier.
Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles We present a data-driven variant of the LR algorithm for dependency parsing, and extend it with a best-first search for probabil istic generalized LR dependency parsing. Parser actions are determined by a classifier, based on features that represent the current state of the parser. We apply this pars ing framework to both tracks of the CoNLL 2007 shared task, in each case taking ad vantage of multiple models trained with different learners. In the multilingual track, we train three LR models for each of the ten languages, and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme. In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-of domain training set, in a scheme similar to one iteration of co-training. This technique, first proposed by Sagae and Lavie (2006), was used in the highest scoring system in both the multilingual track (Hall et al, 2007a) and the domain adaptation track (Sagae and Tsujii, 2007). The best performing (closed class) system in the domain adaptation track used a combination of co-learning and active learning by training two different parsers on the labeled training data, parsing the unlabeled domain data with both parsers, and adding parsed sentences to the training data only if the two parsers agreed on their analysis (Sagae and Tsujii, 2007). Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Lastly we used a native dependency parser, the GENIA Dependency parser (GDep) by Sagae and Tsujii (2007). As far as pre-processing is concerned, we imported the sentence splitting, tokenization and GDep parsing results (Sagae and Tsujii, 2007) as prepared by the shared task organizers for all data sets (training, development and test). This approach is similar to the one used in (Sagae and Tsujii, 2007), which achieved the highest scores in the domain adaptation track of the CoNLL 2007 shared task (Nivre et al, 2007). The parse trees were produced by the GDep parser (Sagae and Tsujii, 2007) and supplied by the challenge organisers. While the TURKU system exploits the Stanford dependencies from the McClosky-Charniak parser (Charniak and Johnson, 2005), and the JULIELab system uses the CoNLL-like dependencies from the GDep parser (Sagae and Tsujii, 2007), the TOKYO system overlays the Shared Task data with two parsing representations, viz. Enju PAS structure (Miyao and Tsujii, 2002) and GDep parser dependencies. GDep (Sagae and Tsujii, 2007), a native dependency parser.  Sagae and Tsujii (2007) used the co-training technique to improve performance. Sagae and Tsujii (2007) presented an co training approach for dependency parsing adaptation. Dependency parsing has been performed with the GENIA dependency parser GDep (Sagae and Tsujii, 2007), which uses a best-first probabilistic shift reduce algorithm based on the LR algorithm (Knuth, 1965) and extended by the pseudo-projective parsing technique. Sagae and Tsujii (2007) generalized the standard deterministic framework to probabilistic parsing by using a best-first search strategy. For sentences in the dataset, their dependency structures are extracted using GENIA Dependency parser (Sagae and Tsujii, 2007), and phrase structure using Brown self-trained biomedical parser (McClosky, 2009).  The approaches proposed by Reichart and Rappoport (2007a) and Sagae and Tsujii (2007) can be classified as ensemble-based methods. For analysing sentence structure, we applied the mogura 2.4.1 (Matsuzaki and Miyao, 2007) and GDepbeta2 (Sagae and Tsujii, 2007) parsers. Sagae and Tsujii (2007) present a detailed description of the parsing approach used in our work, including the parsing algorithm. See Sagae and Tsujii (2007) for more information on the parser.
Probabilistic Disambiguation Models For Wide-Coverage HPSG Parsing This paper reports the development of loglinear models for the disambiguation in wide-coverage HPSG parsing. The estimation of log-linear models requires high computational cost, especially with widecoverage grammars. Using techniques to reduce the estimation cost, we trained the models using 20 sections of Penn Treebank. A series of experiments empirically evaluated the estimation techniques, and also examined the performance of the disambiguation models on the parsing of real-world sentences. Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. The atomic features and their combinations are imported from the previous work on HPSG parsing (Miyao and Tsujii, 2005). Similar results are reported in parsing (Miyao and Tsujii, 2005) while the accuracy saturated around 16,000 sentences. These results are different from those in parsing reported by Miyao and Tsujii (2005) where COMMA and SPAN especially contributed to the accuracy.  In this paper, we use an HPSG parser developed by Miyao and Tsujii (2005). In addition, the HPSG grammar is extracted from the HPSG Treebank using a corpus based procedure, and it does not necessarily cover all possible grammatical phenomena in unseen text (Miyao and Tsujii, 2005).  By running the HPSG parser described in section 2.2 on the development data without dependency constraints, we obtain similar values of LP (86.8%) and LR (85.6%) as those reported by Miyao and Tsujii (Miyao and Tsujii, 2005).  We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and an HPSG parser (Miyao and Tsujii, 2005). We first apply a deep parser (Miyao and Tsujii, 2005) and a dictionary-based term recognizer (Tsuruoka and Tsujii, 2004) to MEDLINE and obtain annotations of predicate argument structures and ontological identifiers of genes, gene products, diseases, and events. In addition, parsers that compute deeper analyses, such as predicate argument structures, have become available for the processing of real-world sentences (Miyao and Tsujii, 2005). We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures.  The HPSG parser used in this study is Ninomiya et al (2006), which is based on Enju (Miyao and Tsujii, 2005).   In the hybrid model, the probabilities of the previous model are multiplied by the super tagging probabilities instead of a preliminary probabilistic model, which is introduced to help the process of estimation by filtering unlikely lexical entries (Miyao and Tsujii, 2005). In the experiments, we compared our model with the probabilistic HPSG model of Miyao and Tsujii (2005).
Corpus-Based Identification Of Non-Anaphoric Noun Phrases Coreference resolution involves finding antecedents for anaphoric discourse entities, such as definite noun phrases. But many definite noun phrases are not anaphoric because their meaning can be understood from general world knowledge (e.g., &quot;the White House&quot; or &quot;the news media&quot;). We have developed a corpus-based algorithm for automatically identifying definite noun phrases that are non-anaphoric, which has the potential to improve the efficiency and accuracy of coreference resolution systems. Our algorithm generates lists of nonanaphoric noun phrases and noun phrase patterns from a training corpus and uses them to recognize non-anaphoric noun phrases in new texts. Using 1600 MUC-4 terrorism news articles as the training corpus, our approach achieved 78% recall and 87% precision at identifying such noun phrases in 50 test documents. More recently, Bean and Riloff (1999) proposed methods for autolnatically extracting from a corpus such special predicates, i.e., heads that correlate well with discourse novelty. Bean and Riloff (1999) developed a system for identifying discourse-new DDs that incorporates, in addition to syntax-based heuristics aimed at recognizing predicative and established DDs using post modification heuristics similar to those used by Vieira and Poesio, additional techniques for mining from corpora unfamiliar DDs including proper names, larger situation, and semantically functional. More recently, the problem has been tackled using unsupervised (e.g., Bean and Riloff (1999)) and supervised (e.g., Evans (2001), Ng and Cardie (2002a)) approaches. Bean and Riloff (1999) extracts rules from non-anaphoric noun phrases and noun phrases patterns, which are then applied to test data to identify existential noun phrases. In previous work (Bean and Riloff, 1999), we developed an unsupervised learning algorithm that automatically recognizes definite NPs that are existential without syntactic modification because their meaning is universally understood. Using this heuristic, BABAR identifies existential definite NPs in the training corpus using our previous learning algorithm (Bean and Riloff, 1999) and resolves all occurrences of the same existential NP with each another. First, a non-anaphoric NP classifier identifies definite noun phrases that are existential, using both syntactic rules and our learned existential NP recognizer (Bean and Riloff, 1999), and removes them from the resolution process. Bean and Riloff (1999) and Uryupina (2003) construct quite accurate classifiers to detect unique NPs. The system described in (Bean and Riloff, 1999) also makes use of syntactic heuristics. For the statistics-based approaches, Bean and Riloff (1999) developed a statistics-based method for automatically identifying existential definite NPs which are non-anaphoric. Bean and Riloff (1999) and Uryupina (2003) have already employed a definite probability measure in a similar way, although the way the ratio is computed is slightly different. A more fine-grained distinction is made by Bean and Riloff (1999) and Vieira and Poesio (2000) to distinguish restrictive from non-restrictive post modification by ommitting those modifiers that occur between commas, which should not be classified as chain starting. We borrow the idea of classifying definites occurring in the first sentence as chain starting from Bean and Riloff (1999).  An anaphoricity feature indicates whether an NP to be resolved is anaphoric, and is typically computed using an anaphoricity classifier (Ng, 2004), hand-crafted patterns (Daum´e III and Marcu, 2005), and automatically acquired patterns (Bean and Riloff, 1999). Non-anaphoric definite descriptions have been detected using heuristics (e.g., Vieira and Poesio (2000)) and unsupervised methods (e.g., Bean and Riloff (1999)).  
Building A Sense Tagged Corpus With Open Mind Word Expert Open Mind Word Expert is an implemented active learning system for collecting word sense tagging from the general public over the Web. It is available at http://teach-computers.org. We expect the system to yield a large volume of high-quality training data at a much lower cost than the traditional method of hiring lexicographers. We thus propose a Senseval-3 lexical sample activity where the training data is collected via Open Mind Word Expert. If successful, the collection process can be extended to create the definitive corpus of word sense information. The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted to Romanian. The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). Volunteer Contributions over the Web The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002). Due to the expensive annotation process, only a handful of manually sense-tagged corpora are available. An effort to alleviate the training data bottle neck is the Open Mind Word Expert (OMWE) project (Chklovski and Mihalcea, 2002) to collect sense-tagged data from Internet users. The annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted for multilingual annotations. Finally, in an effort related to the Wikipedia collection process, (Chklovski and Mihalcea, 2002) have implemented the Open Mind Word Expert system for collecting sense annotations from volunteer contributors over the Web. Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). The extension consisted in extending the training data set so as to include a selection of WordNet examples (full sentences containing a main verb) and the Open Mind Word Expert corpus (Chklovski and Mihalcea 2002). For the fine-grained All Words sense tagging task, which has always used WordNet, the system performance has ranged from our 59% to 65.2 (Senseval3, (Decadt et al, 2004)) to 69% (Seneval2, (Chklovski and Mihalcea, 2002)). We use SemCor1, OMWE 1.0 (Chklovski and Mihalcea, 2002), and example sentences in Word Net as the training corpus. Chklovski and Mihalcea (2002) presented another interesting proposal which turns to Web users to produce sense-tagged corpora. Open Mind Word Expert (Chklovski and Mihalcea, 2002) was a real application of active learning for WSD. Lately, many such corpora have been developed in different languages, including SemCor (Miller et al, 1993), LDC-DSO (Ng and Lee, 1996), Hinoki (Kasahara et al, 2004), and the sense annotated corpora with the help of Web users (Chklovski and Mihalcea, 2002).
Efficient Minimum Error Rate Training and Minimum Bayes-Risk Decoding for Translation Hypergraphs and Lattices Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most current state-of-theart Statistical Machine Translation (SMT) systems. The algorithms were originally to work with lists of translations, and recently extended to lattices that encode many more hypotheses typical lists. We here extend lattice-based MERT and MBR algorithms to work with hypergraphs that encode a vast number of translations produced by MT systems based on Synchronous Context Free Grammars. These algorithms are more efficient than the lattice-based versions presented earlier. We show how MERT can be employed to optimize parameters for MBR decoding. Our experiments show speedups from MERT and MBR as well as performance improvements from MBR decoding on several language pairs. We learn the weights of this consensus model using hyper graph-based minimum-error-rate training (Kumar et al, 2009). Kumar et al (2009) describes an efficient approximate algorithm for computing n-gram posterior probabilities. For each system, we report the performance of max-derivation decoding (MAX), hyper graph-based MBR (Kumar et al, 2009), and a linear version of forest-based consensus decoding (CON) (DeNero et al., 2009). Table 6 compares exact n-gram posterior computation (Algorithm 1) to the approximation described by Kumar et al (2009). We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec. The cdec MERT implementation performs inference over the decoder search space which is structured as a hyper graph (Kumar et al, 2009). These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al, 2009). MERT learns parameters from forests (Kumar et al, 2009) with 4 restarts and 8 random directions in each iteration. While other fast MBR approximations are possible (Kumar et al, 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation (1) for efficient MBR decoding over lattices. Rather than computing an error surface using k best approximations of the decoder search space, cdec's implementation performs inference over the full hyper graph structure (Kumar et al, 2009). Then, max-marginals were computed using the forward-backward algorithm and used to prune out paths that were greater than a factor of 2.3 from the best path, as recommended by Dyer. This algorithm is equivalent to the hyper graph MERT algorithm described by Kumar et al (2009). All n-gram posteriors are computed using the efficient algorithm proposed by Kumar et al (2009). We report results using the Moses implementation of Viterbi, nbest MBR and lattice MBR decoding (Kumar et al, 2009). (Kumar et al, 2009) mention that the linear approximation to BLEU used in their lattice MBR algorithm is not guaranteed to match corpus BLEU, especially on unseen test sets. However, due to the unified nature of the training and decoding criterion in our approach, the minimum risk trained weights can be plugged directly into the sampler MBR decoder, whereas lattice MBR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al, 2009). The feature weight vector w in Equation 1 is tuned by MERT over hyper graphs (Kumar et al, 2009). These approaches include the work of Kumar and Byrne (2004), which re-ranks the n best output of a MT decoder, and the work of Tromble et al (2008) and Kumar et al (2009), which does MBR decoding for lattices and hyper graphs. The line optimisation procedure can also be applied to a hyper graph representation of the hypotheses (Kumar et al, 2009). The LMERT and TGMERT optimisation algorithms are particularly suitable for this realisation of hiero in that the lattice representation avoids the need to use the hyper graph formulation of MERT given by Kumar et al (2009). More recently, this algorithm was extended to work with hyper graphs encoding a huge number of translations produced by MT systems based on Synchronous Context Free Grammars (Kumar et al, 2009).
Recognizing Stances in Online Debates This paper presents an unsupervised opinanalysis method for clasi.e., recognizing which stance a person is taking in an online debate. In order to handle the complexities of this genre, we mine the web to learn associations that are indicative of opinion stances in debates. We combine this knowledge with discourse information, and formulate the debate side classification task as an Integer Linear Programming problem. Our results show that our method is substantially better than challenging baseline methods. Following (Somasundaran and Wiebe, 2009), stance, as used in this work, refers to an overall position held by a person toward an object, idea or proposition. Our work is also different from related work in the domain of product debates (Somasundaran and Wiebe, 2009) in terms of the methodology. Some previous research looked at the related field of opinion mining, also on political discussion, as in (AbuJbara et al, 2012), (Anand et al., 2011) or (Somasundaran and Wiebe, 2009). As we aim at performing a finegrained analysis, approaches merely classifying pro or contra (like those of (Walker et al, 2012) or (Somasundaran and Wiebe, 2009) are not applicable in our case. Somasundaran and Wiebe (2009) propose an unsupervised method for classifying the stance of each contribution to an on line debate concerning the merits of competing products. Somasundaran and Wiebe (2009) presents an unsupervised opinion analysis method for debate-side classification. There have been some related works that focus on discovering the general topics and ideological perspectives in online discussions (Ahmed and Xing, 2010), placing users in support/oppose camps (Agarwal et al, 2003), and classifying user stances (Somasundaran and Wiebe, 2009).   Sentence 14 is a negative pragmatic opinion (Somasundaran and Wiebe, 2009) which can only be detected with the help of external world knowledge. Somasundaran and Wiebe (2009) used unsupervised methods to identify stances in online debates.
Features And Values The paper discusses the linguistic aspects of a new general purpose facility for computing with features. The program was developed in connection with the course I taught at the University of Texas in the fall of 1983. It is a generalized and expanded version of a system that Stuart Shieber originally designed for the PATR-II project at SRI in the spring of 1983 with later modifications by Fernando Pereira and me. Like its predecessors, the new Texas version of the &quot;DG (directed graph)&quot; package is primarily intended for representing morphological and syntactic information but it may turn out to be very useful for semantic representa Karttunen (1984) provides examples of feature structures in which a negation operator might be useful. One example of the gap between description and computational implementation is disjunctive feature representation, which became popular in feature-based grammar formalisms in the 1980s (Karttunen, 1984). The feature structure (adopted from Karttunen, 1984, p. 30) represents disjunctions by enclosing the alternatives in curly brackets ({}).
Learning Accurate Compact And Interpretable Tree Annotation We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank. Starting with a simple Xbar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals. In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves 90.2% on the Penn Treebank, higher than fully lexicalized systems. The trees are binarized (Petrov et al, 2006) and for the EM algorithm we use the initialization method described in Matsuzaki et al. (2005).  Following Petrov et al (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps. We demonstrate that likelihood-based hierarchical EM training (Petrov et al, 2006) and cluster-based language modeling methods (Goodman, 2001) are superior to both rank-based and random-projection methods. For example in the domain of syntactic parsing with probabilistic context-free grammars (PCFGs), a surprising recent result is that automatically induced grammar refinements can outperform sophisticated methods which exploit substantial manually articulated structure (Petrov et al, 2006). In this paper, we consider a much more automatic, data-driven approach to learning HMM structure for acoustic modeling, analagous to the approach taken by Petrov et al (2006) for learning PCFGs. We approximate the loss in data likelihood for a merge with the following likelihood ratio (Petrov et al, 2006).  However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser (Petrov et al, 2006)).  We have so far dealt with the adequacy of representation and we plan to test whether more sophisticated estimation (e.g., split-merge-smooth estimation as in (Petrov et al, 2006)) can obtain further improvements from the explicit representation of agreement.  We combine multiple word representations based on semantic clusters extracted from the (Brown et al, 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al, 2006) in order to improve discriminative dependency parsing in the MST Parser framework (McDonald et al, 2005). In this paper, we obtain syntactic clusters from the Berkeley parser (Petrov et al., 2006). Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al, 2006). To generate parse trees, we use the Berkeley parser (Petrov et al, 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. This enables us to compare against the results of Fowler and Penn (2010), who trained the Petrov parser (Petrov et al, 2006) on CCGbank. We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy (Petrov et al, 2006).  In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate.
The Grammar Matrix: An Open-Source Starter-Kit For The Rapid Development Of Cross-Linguistically Consistent Broad-Coverage Precision Grammars The grammar matrix is an open-source starter-kit for the development of broad- By using a type hierarchy to represent cross-linguistic generalizations and providing compatibility with other open-source tools for grammar engineering, evaluation, parsing and generation, it facilitates not only quick start-up but also rapid growth towards the wide coverage necessary for robust natural language processing and the precision parses and semantic representations necessary for natural language understanding.  The SRG is implemented within the Linguistic Knowledge Building (LKB) system (Copestake, 2002), based on the basic components of the grammar Matrix, an open source starter-kit for the development of HPSG grammars developed as part of the LinGO consortium's multilingual grammar engineering (Bender et al, 2002). What I am reporting is thus a perspective on work done primarily by Flickinger within the English Resource Grammar (ERG: Flickinger (2000)) and by Bender in the context of the Grammar Matrix (Bender et al, 2002), though I've been involved in many of the discussions. In this paper I shall present a treatment of lexical and grammatical tone and vowel length in Hausa, as implemented in an emerging bidirectional HPSG of the language based on the Lingo Grammar Matrix (Bender et al, 2002). To better understand the needs of grammar developers we carefully explored two existing grammars: the LINGO grammar matrix (Bender et al,2002), which is a basis grammar for the rapid development of cross-linguistically consistent grammars; and a grammar of a fragment of Modern Hebrew, focusing on inverted constructions (Melnik, 2006). The LinGO Grammar Matrix project (Bender et al., 2002) is situated within the DELPH-IN2 collaboration and is both a repository of reusable linguistic knowledge and a method of delivering this knowledge to a user in the form of an extensible precision implemented grammar. We share grammar libraries with the Grammar Matrix in the grammar (Bender et al, 2002) as the foundation of KRG2.  To this end, the Grammar Matrix project (Bender et al, 2002) has been developed which, through a set of questionnaires, allows grammar engineers to quickly produce a core grammar for a language of their choice. In this research, we focus particularly on the Grammar Matrix-based DELPH-IN family of grammars (Bender et al, 2002), which includes grammars of English, Japanese, Norwegian, Modern Greek, Portuguese and Korean.  However, developing such grammars has been made much more efficient with the emergence of the Grammar Matrix (Bender et al,2002).
Noun-Phrase Co-occurrence Statistics for Semi-Automatic Semantic Lexicon Construction semantic lexicons semiautomatically could be a great time saver, relative to creating them by hand. In this paper, we present an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars. Our algorithm finds more correct terms and fewer incorrect ones than previous work in this area. Additionally, the entries that are generated potentially provide broader coverage of the category than would occur to an individual coding them by hand. Our algorithm finds many terms not included within Wordnet (many more than previous algorithms), and could be viewed as an &quot;enhancer&quot; of existing broad-coverage resources. We chose three clusters produced by a program similar to Roark and Charniak (1998) except that it is based on a generative probability model and tries to classify all nouns rather than just those in pre-selected clusters. This problem is addressed by Riloff and Shepherd (1997), Roark and Charniak (1998) and more recently by Widdows and Dorow (2002). Inspired by the conjunction and appositive structures, Riloff and Shepherd (1997), Roark and Charniak (1998) used co occurrence statistics in local context to discover sibling relations. Rilo and Shepherd (Rilo and Shepherd, 1997) developed a bootstrap ping algorithm that exploits lexical co-occurrence statistics, and Roark and Charniak (Roark and Charniak, 1998) re ned this algorithm to focus more explicitly on certain syntactic structures. The main results to date in the field of automatic lexical acquisition are concerned with extracting lists of words reckoned to belong together in a particular category, such as vehicles or weapons (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998). Roark and Charniak describe a "generic algorithm" for extracting such lists of similar words using the notion of semantic similarity, as follows (Roark and Charniak, 1998). Algorithms of this type were used by Riloff and Shepherd (1997) and Roark and Charniak (1998), reporting accuracies of 17% and 35% respectively. Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roark and Charniak, 1998). Our results are an order of magnitude better than those reported by Riloff and Shepherd (1997) and Roark and Charniak (1998), who report average accuracies of 17% and 35% respectively. The experiments in (Riloff and Shepherd, 1997) were performed on the 500,000 word MUC-4 corpus, and those of (Roark and Charniak, 1998) were performed using MUC-4 and the Wall Street Journal corpus (some 30 million words). The high accuracy achieved thus questions the conclusion drawn by Roark and Charniak (1998) that parsing is invaluable. Roark and Charniak (1998) used the co-occurrence of words as features to classify nouns. The goal of extracting semantic information from text is well-established, and has encouraged work on lexical acquisition (Roark and Charniak, 1998), information extraction (Cardie, 1997), and ontology engineering (Hahn and Schnattinger, 1998). This may be explained by the fact that words appearing in conjunctions are often taxonomically similar (Roark and Charniak, 1998) and that taxonomic information is particularly useful for compound interpretation, as evidenced by the success of WordNet-based methods (see Section 5). To select seed words, we used the procedure proposed by Roark and Charniak (1998), ranking all of the head nouns in the training corpus by frequency and manually selecting the first 10 nouns that unambiguously belong to each category.  For example, Hearst (Hearst, 1992) learned hyponymy relationships by collecting words in lexico-syntactic expressions, such as NP, NP, and other NPs, and Roarkand Charniak (Roark and Charniak, 1998) generated semantically related words by applying statistical measures to syntactic contexts involving appositives, lists, and conjunctions. In previous research on semantic lexicon induction, Roark and Charniak (Roark and Charniak, 1998) showed that 3 of every 5 words learned by their system were not present in WordNet. Roark and Charniak (Roark and Charniak, 1998) followed up on this work by using a parser to explicitly capture these structures. Roark and Charniak (1998) applied this idea to extraction of words which belong to the same categories, utilizing syntactic relations such as conjunctions and appositives.
Estimation of Conditional Probabilities With Decision Trees and an Application to Fine-Grained POS Tagging We present a HMM part-of-speech tag ging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attributevectors and decomposition of the contex tual POS probabilities of the HMM into aproduct of attribute probabilities, (2) esti mation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German andCzech data, our tagger outperformed state of-the-art POS taggers. The decision tree uses different context features for the prediction of different attributes (Schmid and Laws, 2008). Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger). RFTagger (Schmid and Laws, 2008 ,pos, morph),.  The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information. Additional German tags are obtained using the RFTagger toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.). For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs. However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case. For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008). The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information. For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs. These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tag set containing approximately 800 tags. For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RF-Tagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German. The results are not directly comparable to the RF-Tagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Treebank. The lexicon supplies entries for additional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).
SemEval-2007 Task 10: English Lexical Substitution Task In this paper we describe the English Lexical Substitution task for SemEval. In the task, annotators and systems find an alternative substitute word or phrase for a target word in context. The task involves both finding the synonyms and disambiguating the context.Participating systems are free to use any lexical resource. There is a subtask which re quires identifying cases where the word isfunctioning as part of a multiword in the sen tence and detecting what that multiword is. Our systems consistently perform better when a mode exists, which makes sense because those are instances in which the annotators are in agreement (McCarthy and Navigli, 2007). Our evaluation framework is inspired by the lexical substitution task (McCarthy and Navigli, 2007), where a system attempts to generate a word (or a set of words) to replace a target word, such that the meaning of the sentence is preserved. (OOT) evaluation metrics defined by McCarthy and Navigli (2007). The first one, the LEXSUB-PARA dataset, is a small subset of the Lexical Substitution corpus (McCarthy and Navigli, 2007) that was specifically created for this task. While Dinu and Lapata (Dinu and Lapata, 2010b) did show improvement over context insensitive DIRT, this result was obtained on the verbs of the Lexical Substitution Task in SemEval (McCarthy and Navigli, 2007), which was manually created with a bias for context-sensitive substitutions. Then we will also be able to evaluate our model on the Lexical Substitution Task (McCarthy and Navigli, 2007), which has been commonly used in recent years as a benchmark for context-sensitive lexical similarity models. Recently, McCarthy and Navigli (2007) proposed the English Lexical Substitution task (hereafter referred to as LEXSUB) under the auspices of SemEval-2007. For more information on LEXSUB, see McCarthy and Navigli (2007). Within the scope of our paper, rule application is handled similarly to Lexical Substitution (McCarthy and Navigli, 2007), considering the contextual relationship between the text and the rule. SWAT-E is the best system for out often, as several of the items that were emphasized through duplication were also correct. The results are much higher than for LEXSUB (McCarthy and Navigli, 2007). The past work which is most similar to ours is derived from the lexical substitution track of SemEval 2007 (McCarthy and Navigli, 2007). In particular, this technique ranks a given list of synonyms according to a similarity metric based on the occurrences in the Web 1T 5-gram corpus, which specify n-grams frequencies in a large Web sample. This technique achieved the state-of-the-art performance on the English Lexical Substitution task at SemEval 2007 (McCarthy and Navigli, 2007). To evaluate the system's ability to come up with suitable substitutes from scratch, we use the measures designed to evaluate systems that took part in the original English lexical substitution task (McCarthy and Navigli, 2007). We evaluate our model on a paraphrase ranking task on a subset of the SemEval 2007 lexical substitution task (McCarthy and Navigli, 2007) data, and compare it to a random baseline and E&P's state of the art model. Instead, we automatically extract confusable candidates from a parallel corpus. Synonym extraction (Wu and Zhou, 2003), lexical substitution (McCarthy and Navigli, 2007) and paraphrasing (Madnani and Dorr, 2010) are related to collocation correction in the sense that they try to find semantically equivalent words or phrases. The results are reported in McCarthy and Navigli (2007) and in more detail in McCarthy and Navigli (in press). Our first experiment is carried out on the SemEval 2007 lexical substitution task dataset (McCarthy and Navigli, 2007). To evaluate the performance of our model, we use various subsets of the SemEval 2007 lexical substitution task (McCarthy and Navigli, 2007) dataset. We use it because we want to compare our model with E&P. P10 measures the percentage of gold-standard paraphrases in the top-ten list of paraphrases as ranked by the system, and can be defined as follows (McCarthy and Navigli, 2007). We explore this suggestion, implementing a lexical substitution (McCarthy and Navigli,2007) approach to dialogue generation with sentiment, using the Valentino approach and associated resources.
Learning Extraction Patterns For Subjective Expressions This paper presents a bootstrapping process that learns linguistically rich extraction patterns for subjective (opinionated) expressions. High-precision classifiers label unannotated data to automatically create a large training set, which is then given to an extraction pattern learning algorithm. The learned patterns are then used to identify more subjective sentences. The bootstrapping process learns many subjective patterns and increases recall while maintaining high precision.  Riloff and wiebe (2003) constructed a high precision classifier for contiguous sentences using the number of strong and weak subjective words in current and nearby sentences.  Although this work aims at learning only nouns, in the subsequent work, they also proposed a bootstrapping method that can deal with phrases (Riloff and Wiebe, 2003). Subjective sentences are sentences used to express opinions, evaluations, and speculations (Riloff and Wiebe, 2003).  Starting with the Romanian lexicon, we developed a lexical classifier similar to the one introduced by (Riloff and Wiebe, 2003). The major source of this list is from (Riloff and Wiebe, 2003) with additional words from other sources. Most of the previous work on sentiment lexicon construction relies on existing natural language processing tools, e.g., syntactic parsers (Wiebe, 2000), information extraction (IE) tools (Riloff and Wiebe, 2003) or rich lexical resources such as WordNet (Esuli and Sebastiani, 2006). These terms have been previously shown to be high precision for recognizing subjective sentences (Riloff and Wiebe, 2003).  We decided to expand our subjective term lists by using automatic term extraction, inspired by (Riloff and Wiebe, 2003). All trigrams are then scored according to their prevalence in relevant versus irrelevant documents (e.g. subjective vs. non-subjective sentences), following the scoring methodology of Riloff and Wiebe (2003).  Believing that the current approach may offer benefits over state-of-the-art pattern-based subjectivity detection, we also implement the AutoSlogTS method of Riloff and Wiebe (2003) for extracting subjective extraction patterns. Riloff and Wiebe (2003) performed pattern learning through bootstrapping while extracting subjective expressions. For example, Riloff and Wiebe (2003) proposed a bootstrapping process to automatically identify subjective patterns. Riloff and Wiebe (2003) extracted subjective expressions from sentences using a bootstrapping pattern learning process. Riloff and Wiebe (2003) describe a bootstrapping method to learn subjective extraction patterns that match specific syntactic templates, using a high-precision sentence-level subjectivity classifier and a large unannotated corpus. As shown in previous work, a high-precision classifier can be used to automatically generate subjectivity annotated data (Riloff and Wiebe, 2003).
Incorporating Non-Local Information Into Information Extraction Systems By Gibbs Sampling Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We how to solve this dilemma with sama simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks. Finkel et al (2005) used simulated annealing with Gibbs sampling to find a solution in a similar situation. We also conduct experiments using simulated annealing in decoding, as conducted by Finkel et al (2005) for information extraction. Finkel et al (2005) proposed a method incorporating non-local structure for information extraction. To compute the features which we extract in the next section, all instances in our data sets were part-of-speech tagged by the MXPOST tagger (Ratnaparkhi, 1996), parsed with the MaltParser, and named entity tagged with the Stanford NE tagger (Finkel et al, 2005). One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling (Finkel et al, 2005), another is (loopy) sum-product belief propagation (Smith and Eisner,2008). The results we obtained on the CoNLL03 test set were consistent with what was reported in (Finkel et al, 2005). Following (Yao et al, 2011), we filter out noisy documents and use natural language packages to annotate the documents, including NER tagging (Finkel et al, 2005) and dependency parsing (Nivre et al, 2004). These include several off-the-shelf statisical NLP tools such as the Stanford POS tagger (Toutanova and Manning, 2000), the Stanford named-entity recognizer (NER) (Finkel et al, 2005) and the Stanford Parser (Klein and Manning, 2003). We run the Stanford Named Entity Recognizer (Finkel et al, 2005) and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs. The Total column presents the number of extracted NEs and generated hypotheses and the Average column shows the average numbers per text respectively.2009), and we preprocess the data using the Stanford named-entity recognizer (Finkel et al, 2005). For the learning of patterns we used the top 64 documents retrieved by Google and to recognize the named entities in the pattern we apply several strategies, namely: 1) the Stanford's Conditional Random-Field-based named entity recognizer (Finkel et al, 2005) to detect entities of type HUMAN; 2) regular expressions to detect NUMERIC and DATE type entities; 3) gazetteers to detect entities of type LOCATION.   We used as candidates all strings labeled in the annotated data as well as all named entities found by the Stanford NER tagger for CoNLL (Finkel et al, 2005).  The named-entity features are generated by the freely available Stanford NER tagger (Finkel et al., 2005). We use the Stanford Named Entity Recognizer (Finkel et al, 2005) for this purpose. The Stanford CRF-based NER tagger was used as the monolingual component in our models (Finkel et al, 2005). For English, we use the default tagger setting from Finkel et al (2005). To determine entailment, BIUTEE performs the following main steps: Preprocessing First, all documents are parsed and processed with standard tools for named entity recognition (Finkel et al, 2005) and coreference resolution.
Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation This paper presents the results of the WMT10 and MetricsMATR10 shared which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 104 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 26 metrics. This year we also investigated increasing the number of human judgments by hiring non-expert annotators through Amazon’s The data for French, German, and Spanish are from the 2010 Workshop on Statistical Machine Translation (Callison-Burch et al, 2010). For the segment level, we followed (Callison-Burch et al, 2010) in using Kendall's rank correlation coefficient. Our choice of metrics was based on their popularity in the MT community, their performance in open competitions such as the NIST Metrics MATR challenge (NIST, 2008) and the WMT shared evaluation task (Callison-Burch et al, 2010), their availability, and their relative complementarity. On the evaluation data in (Sennrich, 2011), this system significantly outperformed MEMT (Heafield and Lavie, 2010), which was among the best-performing system combination tools at WMT 2010 (Callison-Burch et al, 2010). In this paper, we apply the same approach to a different translation scenario, namely the WMT 2011 shared task. Machine Learning methods over previously released evaluation data have been already used for tuning complex statistical evaluation metrics (e.g. SVM-Rank in Callison-Burch et al (2010)). With coefficients?= 0.60 and?= 0.23, our metric performs relatively low compared to the other metrics of WMT10 (indicatively iBLEU:?= 0.95,?= 0.39 according to Callison-Burch et al (2010). Though, it still has a position in the list, scoring better than several other reference-aware metrics (e.g.of?= 0.47 and?= 0.12 respectively) for the particular language pair. We should note that we are not capable of fully investigating this case based on the current set of experiments, because all of the systems in our data sets have shown acceptable scores (11-25 BLEU and 0.58-0.78 TERp according to Callison-Burch et al (2010)), when evaluated against reference translations. According to the system-level correlation with human judgments (Tables 1 and 2), it ranks top for the out-of-English task and very close to the top for the into-English task (Callison-Burch et al, 2010). Spearman's rank correlation coefficients on the document (system) level between all the metric sand the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al, 2008), fourth (Callison-Burch et al, 2009) and fifth (Callison-Burch et al, 2010) shared translation tasks.  To train them we use the freely available corpora: Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), CzEng0.9 (Bojar and? Zabokrtsky?, 2009), Opus (Tiedemann, 2009), DGT-TM5 and News Corpus (Callison-Burch et al, 2010), which results in more than 4 million sentence pairs for each model. Our system was tested on the News test set (Callison-Burch et al, 2010) released by the organizers of the 2010 Workshop on Statistical Machine Translation.  For instance, the corpora made available for recent machine translation evaluations are in the order of 1 billion running words (Callison-Burch et al 2010). Experiments were carried out on two corpora: TED (Paul et al 2010) and News Commentary (NC) (Callison-Burch et al 2010). Training data used for ROSE is from WMT10 (Callison-Burch et al, 2010) human judged sentences. The synonym matching is computed using WordNet (Fellbaum, 1998) and the paraphrase matching is computed using paraphrase tables (Callison-Burch et al, 2010). The Spearman's rank correlation coefficients on the document (system) level between the IBM1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al, 2008), fourth (Callison Burch et al, 2009) and fifth (Callison-Burch et al, 2010) shared translation tasks. 1249 Experiments were carried out for the system combination task of the fifth workshop on statistical machine translation (WMT10) in four directions,{Czech, French, German, Spanish} -to English (Callison-Burch et al, 2010), and we found comparable performance to the conventional confusion network based system combination in two language pairs, and statistically significant improvements in the others. We ran our experiments for the WMT10 system combination task using e four language pairs, {Czech, French, German, Spanish} -to-English (Callison-Burch et al, 2010).
Evaluating Translational Correspondence Using Annotation Projection resource acquisition. In of the Workshop on Linguistic Knowledge Acquisition and Representation: Bootstrapping Anno- Language Data. appear. I. Dan Melamed. 1998. Annotation style guide for the blinker project. Technical Report IRCS 98-06, University of Pennsylvania. Arul Menezes and Stephen D. Richardson. 2001. A best-first alignment algorithm for automatic extraction of transfer mappings from bilingual cor- In of the 39th Annual Meeting of the Association for Computational Linguistics DDMT Workshop, France. Anoop Sarkar. 2001. Applying co-training methods statistical parsing. In of NAACL, Shieber. 1994. Restricting the weakgenerative capacity of synchronous treegrammars. Intelligence, Recent work (Hwa et al, 2002) suggests that translational corresponence of linguistic structures can indeed be useful in projecting parses across languages. The dependency projection method DPA (Hwa et al, 2005) based on Direct Correspondence Assumption (Hwa et al, 2002) can be described as: if there is a pair of source words with a dependency relationship, the corresponding aligned words in target sentence can be considered as having the same dependency relationship equivalently. This idea was followed by (Hwa et al, 2002) who investigated English to Chinese projections based on the direct correspondence assumption. Previous work has primarily focused on the projection of grammatical (Yarowsky and Ngai, 2001) and syntactic information (Hwa et al, 2002). Analogously to Hwa et al (2002), we investigate whether there are indeed semantic correspondences between two languages, since there is little hope for projecting meaningful annotations in nonparallel semantic structures. Similarly to previous work (Hwa et al, 2002), we find that some mileage can be gained by assuming direct correspondence between two languages. We take as the starting point of annotation projection the direct correspondence assumption as formulated in (Hwa et al, 2002): for two sentences in parallel translation, the syntactic relationships in one language directly map the syntactic relationships in the other, and extend it to POS tags as well. Hwa et al (2002) have noticed that applying elementary linguistic transformations considerably increases precision and recall when projecting syntactic relations, at least for the English/Chinese language pair. In a different approach, Hwa et al (2002) aligned the parallel sentences using phrase based statistical MT models and then projected the alignments back to the parse trees. Many MT models implicitly make the so-called direct correspondence assumption (DCA) as defined in (Hwa et al, 2002). Our DS projection algorithm is similar to the projection algorithms described in (Hwa et al, 2002) and (Quirk et al, 2005). Following Hwa et al (2002), we looked at dependency links in the true English parses from the KTB where both the dependent and the head were linked to words on the Korean side using the intersection alignment. Since unary productions do not translate well from language to language (Hwa et al, 2002), we collapse them to their lower nodes. Fox (2002) has considered English and French, and Hwa et al (2002) investigate Chinese and English. Our hand-aligned test data were those used in Hwa et al (2002), and consisted of 48 sentence pairs also with less than 25 words in either language, for a total of 788 English words and 580 Chinese words. Hwa et al (2002) found that human translations from Chinese to English preserved only 39-42% of the unlabeled Chinese dependencies. Moreover, as stressed in previous research, using syntactic dependencies seems to be particularly well suited to coping with the problem of linguistic variation across languages (Hwa et al, 2002). A wide range of annotations from part of speech (Hi and Hwa, 2005) and chunks (Yarowsky et al, 2001) to word senses (Diab and Resnik, 2002), dependencies (Hwa et al, 2002) and semantic roles (Pado and Lapata, 2009) have been successfully transferred between languages. These sets were the data used by Hwa et al (2002). IGT's unique structure — effectively each instance consists of a bitext between English and some target language — can be easily enriched through alignment and projection (e.g., (Yarowsky and Ngai, 2001), (Hwa et al., 2002)).
On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing paper introduces decomposition a framework for deriving inference algorithms for NLP problems. The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. The approach provably solves a linear programming (LP) relaxation of the global inference problem. It to algorithms that are in that they existing decoding algorithms; in that they avoid exact algorithms for the full and often in that empirically they often recover the correct solution in spite of using an LP relaxation. We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger. Finally, in other recent work, Rush et al (2010) describe dual decomposition approaches for other NLP problems. This method is called dual decomposition (DD) (Rush et al, 2010). In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. It is a slight variation of the proof given by Rush et al (2010). Our approach is conceptually similar to that of Rush et al (2010), which combined separately trained models by enforcing agreement using global inference and solving its linear programming relaxation. It is becoming popular in the NLP community and has been shown to work effectively on several NLP tasks (Rush et al 2010). To find the minimum value, we can use a subgradient method (Rush et al 2010). The answer does not always solve the original problem Eq (2), but previous works (e.g., (Rush et al 2010)) has shown that it is effective in practice. We follows the formulation by Rush et al (2010). An example that oscillates can be constructed along lines similar to the one given by Rush et al (2010). Parsing using dual-decomposition (Rush et al, 2010) seems especially promising in this area. Alternatively, one can employ dual decomposition (Rush et al, 2010). AD resembles the subgradient based algorithm of Rush et al (2010), but it enjoys a faster convergence rate. Dual decomposition is a generic method that has been proposed for handling decoding (i.e. optimization) with such models, by decoupling the problem into two alternating steps that can each be handled by dynamic programming or other polynomial-time algorithms (Rush et al 2010), an approach that has been applied to Statistical Ma chine Translation (phrase-based (Chang and Collins, 1133 2011) and hierarchical (Rush and Collins, 2011)) among others. Though Mate scored higher overall, Berkeley's parser was better at recovering longer-distance relations, suggesting that a combined approach could perhaps work better still (Rush et al, 2010). We optimize the values of the u (i, t) variables using the same algorithm as Rush et al (2010) for their tagging and parsing problem (essentially a perceptron update). However, if it does not converge or we stop early, an approximation must be returned: following Rush et al (2010) we used the highest scoring output of the parsing submodel over all iterations.  However, we can employ dual decomposition as an approximate inference technique (Rush et al., 2010). The dual decomposition inference approach allows us to exploit this sub-graph structure (Rush et al., 2010).
ISP: Learning Inferential Selectional Preferences Semantic inference is a key component for advanced natural language understanding. However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering. This papresents a collection of methods for automatically learning admissible argument values to which an inference rule be applied, which we call and methods for filtering out incorrect inferences. We present empirical evidence of its effectiveness. Though, at least, roles of participants in the event have to be preserved by some means, such as the way presented in (Pantel et al, 2007). This work was refined by Pantel et al (2007) by assigning the x and y terms semantic types (inferential selectional preferences - ISP) based on lexical abstraction from empirically observed argument types. Most distributional methods so far extract representations from large texts, and only as a follow-on step they either 1) alter these in order to reflect a disambiguated word (such as (Erk and Pado, 2008)) or 2) directly asses the appropriateness of a similarity judgment, given a specific context (such as (Pantel et al, 2007)). Context-sensitive extensions of DIRT (Pantelet al, 2007) and (Basili et al, 2007) focus on making DIRT rules context-sensitive by attaching appropriate semantic classes to the X and Y slots of an inference rule. For this (Pantel et al, 2007) build a set of semantic classes using WordNet in one case and CBC clustering algorithm in the other; for each rule, they use the overlap of the fillers found in the input corpus as an indicator of the correct semantic classes. On a common data set (Pantel et al, 2007) and (Basili et al, 2007) achieve significant improvements over DIRT at 95% confidence level when employing the clustering methods. A couple of earlier works utilized a cluster-based model (Pantel et al, 2007) and an LSA-based model (Szpektor et al, 2008), in a selectional-preferences style approach. While most works on context-insensitive predicate inference rules, such as DIRT (Lin and Pantel, 2001), are based on word-level similarity measures, almost all prior models addressing context sensitive predicate inference rules are based on topic models (except for (Pantel et al, 2007), which was outperformed by later models). In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al (2007). Terminal nodes of the resultant structure were used as the basis for inferring semantic type restrictions, reminiscent of the use of CBC clusters (Pantel and Lin, 2002) by Pantel et al (2007), for typing the arguments of paraphrase rules. Pantel et al (2007) and Szpektor et al (2008) represented the context of such rules as the intersection of preferences of the rule's LHS and RHS, namely the observed argument instantiations or their semantic classes. To add the necessary context, ISP (Pantel et al, 2007) learned selectional preferences (Resnik, 1997) for DIRT's rules. We follow Pantel et al (2007) in using automatically-extracted semantic classes to help characterize plausible arguments. MI was also recently used for inference-rule SPs by Pantel et al (2007). As a way of enriching such a template-like knowledge, Pantel et al (2007) proposed the notion of inferential selectional preference and collected expressions that would fill those slots.  Pantel et al (2007) apply a collection of rules to filter out incorrect inferences for SP. The notion of Inferential Selectional Preference (ISP) has been introduced by Pantel et al (2007). In (Pantel et al, 2007), they augment each relation with its selectional preferences, i.e. fine-grained entity types of two arguments, to handle polysemy. This approach can be seen as a proxy to ISP (Pantel et al, 2007), since selectional preferences are one way of distinguishing multiple senses of a path.
A Stochastic Japanese Morphological Analyzer Using a Forward-DP Backward-A* N-Best  Search Algor i thm Masa.aki NAGATA NTT  Network Information Systems l~,~bor~ttorics 1-2356 Take, Yokosuka-Shi, Kanagaw~t, 238-03 Japan (tel) 4-81-468-59-2796 (fax) +81-468-59-3428 (e-mail) nagata@nttnly.ntt .  N-gram models arc usually used for scoring (Gu et al, 1991) (Nagata, 1994), but their training requires the sentences of the corpus to be manualy segmented, and even class-tagged if class-based N-gram is used, as in (Nagata, 1994).  Around 95% word segmentation accuracy is reported by using a word-based language model and the Viterbi-like dynamic programming procedure [Nagata, 1994, Takeuchi and Matsumoto, 1995, Yamamoto, 1996]. We used the Viterbi-like dynamic programming procedure described in [Nagata, 1994] to get the most likely word segmentation. Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al, 1996].  Nagata (1994) proposed a stochastic word segmenter based on a word-gram model to solve the word segmentation problem. Nagata (1994) reported an accuracy of about 97% on a test corpus in the same domain using a learning corpus of 10,945 sentences in Japanese. Fully stochastic language models (e.g. Nagata 1994), on the other hand, do not allow such manual cost manipulation and precisely for that reason, improvements in segmentation accuracy are harder to achieve. The best accuracy reported for statistical methods to date is around 95% (e.g. Nagata 1994). We use the Viterbi algorithm to find the optimal set of morphemes in a sentence and we use the method proposed by Nagata (Nagata, 1994) to search for the N best sets. An algorithm that can provide a solution for Step 2 will be a simpler version of the algorithm used to find the maximum probability solution in Japanese morphological analysis (Nagata, 1994). To find the sequence, Nagata proposed a probabilistic language model for non-segmented languages (Nagata, 1994). In this paper, we apply a dynamic programming search (Nagata, 1994) to k best MIRA.
Mot ivat ions and Methods  tbr Text Simpli f icat ion R. Chandrasekar*  Chr ist ine Doran B. Srinivas Institute for Research in l)el)artm<;nt of Deparl;mcnt of Cognitive, Science & (]cntcr for [,inguistics (]Oml)uter $?  It stands to reason that long sentences will be harder to process automatically and this reasoning has motivated the first approaches to text simplification (Chandrasekar et al, 1996). Chandrasekar et al (1996) introduced a two stage process, first transforming from sentence to syntactic tree, then from syntactic tree to new sentence;. The original motivation for sentence simplification is using it as a preprocessor to facilitate parsing or translation tasks (Chandrasekar et al., 1996). Early attempts to text simplification were based on rule-based methods where rules were designed following linguistic intuitions (Chandrasekar et al, 1996). Simplification decisions about whether to simplify a text or sentence have been studied following rule-based paradigms (Chandrasekar et al, 1996) or trainable systems (Petersen and Ostendorf,2007) where a corpus of texts and their simplifications becomes necessary. Sentence simplification can also serve to preprocess the input of other tasks, such as summarization (Knight and Marcu, 2000), parsing, machine translation (Chandrasekar et al, 1996), semantic role labeling (Vickrey and Koller, 2008) or sentence fusion (Filippova and Strube, 2008). Together with syntactic analysis and transformations similar to those of Chandrasekar et al (1996), they employed lexical simplification based on looking up synonyms in WordNet and extracting Kucera-Francis frequency from the Oxford Psycholinguistic Database (Quinlan, 1992). or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996).  Chandrasekar et al (1996) viewed text simplification as a preprocessing tool to improve the performance of their parser. Text simplification has been associated with techniques that deal not only with helping readers with reading disabilities, but also to help NLP systems (Chandrasekar et al, 1996). 
Summarizing Scientific Articles: Experiments With Relevance And Rhetorical Status Edinburgh In this article we propose a strategy for the summarization of scientific articles that concentrates on the rhetorical status of statements in an article: Material for summaries is selected in such a way that summaries can highlight the new contribution of the source article and situate it with respect to earlier work. We provide a gold standard for summaries of this kind consisting of a substantial corpus of conference articles in computational linguistics annotated with human judgments of the rhetorical status and relevance of each sentence in the articles. We present several experiments measuring our judges’ agreement on these annotations. We also present an algorithm that, on the basis of the annotated training material, selects content from unseen articles and classifies it into a fixed set of seven rhetorical categories. The output of this extraction and classification system can be viewed as a single-document summary in its own right; alternatively, it provides starting material for the generation of task-oriented and user-tailored summaries designed to give users an overview of a scientific field. We pursue a methodology based on Teufel and Moens (2002) where sentences are classified according to their argumentative role. Our methodology builds and extends the Teufel and Moens (Teufel and Moens, 2002) approach to automatic summarisation. It is also similar conceptually to content selection algorithms that have been used for text summarization (Teufel and Moens, 2002) and text generation (Sauper and Barzilay, 2009), both of which rely on finding highly-relevant passages within source texts. This approach to summarization was inspired by the process described in (Teufel and Moens, 2002). That work focused on the summarization of scientific articles to describe a new work in a way which rhetorically situates that work's contribution within the context of related prior work. The features used for the experiments reported here are inspired by previous work in text summarization on content selection (Kupiec et al, 1995), rhetorical classification (Teufel and Moens, 2002), and information ordering (Lapata, 2003). Related to this is the work by Teufel and Moens (2002) on rhetorical classification for content selection. This subset of the corpus is similar in size to the corpus reported in (Teufel and Moens, 2002): the T & M corpus consists of 80 conference articles while ours consists of 40 HOLJ documents. Ultimately, human supervision may be required as in Teufel and Moens (2002), however we can make some observations about the automatic annotation methods above. In earlier work, (Teufel and Moens, 2002) have examined the problem of summarizing scientific articles using rhetorical analysis of sentences. For instance, Teufel and Moens (2002) summarize scientific articles by selecting rhetorical elements that are commonly present in scientific abstracts. Argumentation has typically been studied in relation to summarization (Teufel and Moens, 2002). Teufel and Moens (2002) present a coding scheme for scientific argumentation in research articles that is designed for automatic summarization of human-authored text. This requires adaptation of the high-level features used in AZ (Teufel and Moens, 2002) to chemistry. Previous work on rhetorical analysis of scientific articles focus on either; 1) hierarchical discourse relations between sentences (e.g. Mann and Thompson, 1987), 2) genre analysis within a descriptive framework (e.g. Swales 1990), or 3) ZI in a flat structure and a statistical evaluation of the annotation scheme from a machine learning perspective (e.g. Teufel and Moens, 2002). We follow the lines of (Teufel and Moens, 2002) and apply ZI to the domain of biology. Teufel and Moens (2002) introduced AZ and applied it to computational linguistics papers. As these schemes are too fine grained for abstracts (some of the categories do not appear in abstracts at all), we adopt a reduced version of AZ which integrates seven categories from (Teufel and Moens, 2002) and (Mizuta et al, 2005) those which actually appear in abstracts. We know from Teufel and Moens (2002) that verb tense and voice should be useful for recognizing statements of previous work, future work and work performed in the paper. These 16 categories refer to the dimension that is discussed under problem structure in (Teufel and Moens, 2002), rather than to exclusively rhetorical zones and are viewed as types of topics. It has been successfully applied for text content such as news articles, scientific papers (Teufel and Moens, 2002) that follow a discourse structure.
Findings of the 2009 Workshop on Statistical Machine Translation j schroeder ed ac uk Abstract This paper presents the results of the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 87 machine translation systems and 22 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics. We present a new evaluation technique whereby system output is edited and judged for correctness. These systems were selected from WMT09 (Callison-Burch et al, 2009). To identify the most suitable system for our requirements, we run a set of experiments training the three models with Europarl V4 German-English (Koehn, 2005) and optimizing and testing on the News corpus (Callison-Burch et al 2009). To train our models we use the freely available corpora (when possible): Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al 2006), DGTTM3, Opus (Tiedemann, 2009), SE-Times (Tyers and Alperen, 2010), Tehran English-Persian Parallel Corpus (Pilevar et al 2011), NewsCorpus (Callison-Burch et al 2009), UN Corpus (Rafalovitch and Dale, 2009), CzEng0.9 (Bojar and Z ?abokrtsky?, 2009), English-Persian parallel corpus distributed by ELRA4 and two ArabicEnglish datasets distributed by LDC5. We trained two SMT systems, SMT content and SMTtitle, using the Europarl V4 German-English data as training corpus, and two different development sets: one made of content sentences, News Commentaries (Callison-Burch et al 2009), and the other made of news titles in the source language which were translated into English using a commercial translation system. The recent Fr-En 109 (Callison-Burch et al, 2009) corpus aggregates huge numbers of parallel French English sentences from the web. Evaluation campaigns like WMT (Callison-Burch et al, 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks. We test our metrics in the setting of the WMT 2009 evaluation task (Callison-Burch et al, 2009). Finally, we plan to repeat this experiment over other test beds with document structure, such as those from the 2009 Work shop on Statistical Machine Translation shared task (Callison-Burch et al, 2009) and the 2009 NIST MT Evaluation Campaign (Przybocki et al,2009). We use the data collected during three Workshops on Statistical Machine Translation: WMT08 (Callison Burch et al, 2008), WMT09 (Callison-Burch et al, 2009) and WMT10 (Callison-Burch et al, 2010). The results shown in the remainder of this paper are reported in terms of case insensitive BLEU which showed last year a better correlation with human judgments than case sensitive BLEU for the two languages we con sider (Callison-Burch et al, 2009). To train our models based on Moses we used the freely available corpora: Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al, 2006), Opus (Tiedemann, 2009), News Corpus (Callison-Burch et al, 2009). We train a baseline phrase-based French-English system using WMT-09 corpora (Callison-Burchetal., 2009) for training and evaluation. The main part of the corpus in this task consists of the Europarl corpus as used in the WMT evaluation (Callison-Burch et al, 2009), with some additional data collected in the scope of the project. Recently, most evaluations of machine translation systems (Callison-Burch et al, 2009) indicate that the performance of corpus-based statistical machine translation (SMT) has come up to the traditional rule-based method. There have been various evaluation metrics developed and validated for reliability in fields such as MT and summarization (Callison-Burch et al,2009). Rule-based systems could fulfill this role; they are also an attractive choice given their high quality (as judged by human evaluators) in earlier evaluations (e.g. WMT2009 (Callison-Burch et al, 2009)). While this type of evaluation has its advantages, mainly that it is fast and cheap, its correlation with human judgments is often low, especially for translation out of English (Callison-Burch et al, 2009). I mainly take advantage of this type of evaluation as part of participating with my research group in MT 13 shared tasks with large evaluation campaigns such as WMT (e.g. Callison-Burch et al (2009)). Spearman's rank correlation coefficients on the document (system) level between all the metric sand the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al, 2008), fourth (Callison-Burch et al, 2009) and fifth (Callison-Burch et al, 2010) shared translation tasks. Human judgement of rank has been chosen as the official determinant of translation quality for the 2009 Workshop on Machine Translation (Callison-Burch et al, 2009).
Detecting A Continuum Of Compositionality In Phrasal Verbs We investigate the use of an automatically acquired thesaurus for measures designed to indicate the compositionality of candidate multiword verbs, specifically English phrasal verbs identified automatically using a robust parser. We examine various measures using the nearest neighbours of the phrasal verb, and in some cases the neighbours of the simplex counterpart and show that some of these correlate significantly with human rankings of compositionality on the test set. We also show that whilst the compositionality judgements correlate with some statistics commonly used for extracting multiwords, the relationship is not as strong as that using the automatically constructed thesaurus. McCarthy et al (2003) also targeted verb-particles for a study on compositionality, and judged compositionality according to the degree of overlap in the N most similar words to the verb particle and head verb ,e.g., to determine compositionality. Building on Lin (1998), McCarthy et al (2003) measure the semantic similarity between expressions (verb particles) as a whole and their component words (verb). Similar to Lin (1999), McCarthy et al (2003) and Fazly and Stevenson (2006), our method makes use of automatically generated thesauri; the technique used to compile the thesauri differs from previous work. A similar comparison between the ranks according to Latent-SemanticAnalysis (LSA) based features and the ranks of human judges has been made by McCarthy, KellerandCaroll (McCarthy et al, 2003) for verb-particle con st ructions. McCarthy, Keller and Caroll (McCarthy et al,2003) judge compositionality according to the degree of overlap in the set of most similar words to the verb-particle and head verb. (1993) and Rooth et al (1999) referring to a direct object noun for describing verbs), or used any syn tactic relationship detected by a chunker or a parser (such as Lin (1998) and McCarthy et al (2003)).  Based on the observation (Haspelmath, 2002) that compositional collocations tend to be hyponyms of their head constituent, they propose a model which considers the semantic similarity between a collocation and its constituent words.McCarthy et al (2003) also investigate several tests for compositionality including one (simplex score) based on the observation that compositional collocations tend to be similar inmeaning to their constituent parts. Table 5 shows the results of using different similarity measures with the simplex score test and data of McCarthy et al (2003). McCarthy et al (2003) determine a continuum of compositionality of VPCs, but do not distinguish the contribution of the individual components. McCarthy et al (2003) evaluate the precision of Rasp at identifying VPCs to be 87.6% and the recall to be 49.4%. In particular, other than the re ported results of McCarthy et al (2003) targeting VPCs vs. all other analyses, we had no a priori sense of RASP? s ability to distinguish VPCs and verb-PPs. Also, we ignore the ambiguity between particles and adverbs, which is the principal reason for our evaluation being much higher than that reported by McCarthy et al (2003). In order to get a clearer sense of the impact of selectional preferences on the results, we investigated the relative performance over VPCs of varying semantic compositionality, based on 117 VPCs (f? 1) attested in the data set of McCarthy et al (2003). McCarthy et al (2003) provides compositionality judgements from three human judges, which we take the average of and bin into 11 categories (with 0= non-compositional and 10= fully compositional). The algorithm is evaluated both on 89 manually ranked MWEs and on McCarthy et als (2003) manually ranked phrasal verbs. McCarthy et al (2003) suggested that compositional phrasal verbs should have similar neighbours as for their simplex verbs. In order to evaluate our algorithm in comparison with previous work, we also tested it on the manual ranking list created by McCarthy et al (2003) . This result is comparable with or better than most measures reported by McCarthy et al (2003). 
A Discriminative Framework For Bilingual Word Alignment Bilingual word alignment forms the foun dation of most approaches to statisticalmachine translation. Current word align ment methods are predominantly based on generative models. In this paper, we demonstrate a discriminative approachto training simple word alignment mod els that are comparable in accuracy tothe more complex generative models nor mally used. These models have the theadvantages that they are easy to add fea tures to and they allow fast optimization of model parameters using small amounts of annotated data. (Moore, 2005) has proposed an approach which does not impose any restrictions on the form of model features. LLR and CLP are the word association statistics used in Moore's work (Moore, 2005). A variation of this feature was used by (Moore, 2005) in his paper. In fact, LLR can still be used for extracting positive associations by filtering in a pre-processing step words with possibly negative associations (Moore, 2005). Furthermore, to ensure that only positive association counts, we set the probability to zero if p (x, y) < p (x) p (y), where the probabilities are estimated using relative frequencies (Moore, 2005). We take advantage of this, building on our existing framework (Moore, 2005), to substantially reduce the alignment error rate (AER) we previously reported, given the same training and test data. As in our previous work (Moore, 2005), we train two models we call stage 1 and stage 2, both in the form of a weighted linear combination of feature values extracted from a pair of sentences and a proposed word alignment of them. Firstly, as denoted by Moore (2005), one needs to tune numerous parameters in order to optimize the results for a particular alignment task, which can be very time consuming.  d is an absolute discount parameter as in (Moore, 2005). (Moore, 2005) uses an averaged perceptron for training with a customized beam search. 2) Conditional link probability (Moore, 2005). For example, Moore (2005) uses statistics like log-likelihood-ratio and conditional likelihood-probability to measure word associations; Liu et al (2005) and Taskar et al (2005) use results from IBM Model 3 and Model 4, respectively. d is a discounting constant which is set to 0.4 following Moore (2005). Moore (2005) proposes a similar framework, but with more features and a different search method. In order to obtain the word alignment satisfying the ITG constraint, Wu (1997) propose a DPalgorithm, and we (Chao and Li, 2007) have transferred the constraint to four simple position judgment procedures in an explicit way, so that we can incorporate the ITG constraint as a feature into a log linear word alignment model (Moore, 2005). These models are roughly clustered into two groups: generative models, such as those pro posed by Brown et al (1993), Vogel et al (1996), and Och and Ney (2003), and discriminative models, such as those proposed by Taskar et al (2005), Moore (2005), and Blunsom and Cohn (2006). Unfortunately, as Moore (2005) points out, it is usually difficult to extend a given generative model with feature functions without changing the entire generative story. Moore (2005) likewise uses this example to motivate the need for models that support arbitrary, overlapping features. 
Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion Letter-to-phoneme conversion generally requires aligned training data of letters and phonemes. Typically, the alignments are limited to one-to-one alignments. We present a novel technique of training with many-to-many alignments. A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists. We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word. The many-to-many alignments result in significant improvements over the traditional one-to-one approach. Our system achieves state-of-the-art performance on several languages and data sets. To explore this, we tested our model in conjunction with a recent L2P system that has been shown to predict phonemes with state-of-the-art word accuracy (Jiampojamarn et al, 2007). We would like to use features that look at wide context on the input side, which is inexpensive (Jiampojamarn et al, 2007). Our system employs the many-to-many alignment described in (Jiampojamarn et al, 2007). We use the many-to-many letter phoneme alignment algorithm (Jiampojamarn et al, 2007) to map each letter to multiple phonemes (1-to-2 alignment). The M2M-aligner (Jiampojamarn et al, 2007) is based on the expectation maximization (EM) algorithm. Our input string representation for a candidate pair is formed by first aligning the source and target words using M2M-aligner (Jiampojamarn et al., 2007). DIRECTL+ (Jiampojamarn et al, 2010a) is an online discriminative training system that incorporates joint n-gram features and many-to-many alignments, which are generated by M2M-ALIGNER (Jiampojamarn et al, 2007). Advanced L2P approaches, including the joint n-gram models (Bisani and Ney, 2008) and the joint discriminative approach (Jiampojamarn et al., 2007) eliminate the one-to-one constraint entirely, allowing for linking of multiple letters to multiple phonemes. M2M-aligner (Jiampojamarn et al, 2007) is a many-to-many (M-M) alignment algorithm based on EM that allows for mapping of multiple letters to multiple phonemes. Extensions to this model are possible, for example the use of many-to-many alignments which have been shown to be very effective in letter-to-phoneme alignment tasks (Jiampojamarn et al 2007). In our lexicon, the graphemes and phonemes of each word are aligned according to the method of Jiampojamarn et al (2007). P(ht|ps,pos) is estimated from the frequency of ps, and its alignment with ht, in a version of CELEX in which the graphemic and phonemic representation of each word is many-many aligned using the method of Jiampojamarn et al (2007). In order to extract source-target character mappings, we use m2m-aligner (Jiampojamarn et al, 2007), which implements a forward-backward algorithm to sum over probabilities of possible character sequence mappings, and uses Expectation Maximization to learn mapping probabilities. One of the most popular alignment tools is m2maligner (Jiampojamarn et al, 2007), which is released as an open source software. To establish the substring alignment between katakana and Latin alphabet strings, we use the probabilistic model proposed by (Jiampojamarn et al., 2007). To balance cost and benefit for English-to-Chinese (E2C) transliteration, this work compares the one-stage method with the two-stage one, using additional features of AV (Feng et al, 2004) and M2M-aligner as an initial alignment (Jiampojamarn et al, 2007), to explore where the best investment reward is. For the alignment of supplemental data with candidate outputs, we apply M2M ALIGNER (Jiampojamarn et al, 2007). The second hybrid approach (Jiampojamarn et al, 2007) also extends instance-based classification. In the pipeline approach (Figure 1b), the input word is segmented into letter substrings by an instance-based classifier (Aha et al, 1991), which learns a letter segmentation model from many-to-many alignments (Jiampojamarn et al, 2007). We ignored one-to-one alignments included in the PRONALSYL data sets, and instead induced many-to-many alignments using the method of Jiampojamarn et al (2007).
Practical Very Large Scale CRFs Conditional Random Fields (CRFs) are a widely-used approach for supervised sequence labelling, notably due to their ability to handle large description spaces and to integrate structural dependency between labels. Even for the simple linearchain model, taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set. In this paper, we address the issue of training very large CRFs, containing up to hundreds output labels and several billion features. Efficiency stems here from the sparinduced by the use of a term. Based on our own implementation, we compare three recent proposals for implementing this regularization strategy. Our experiments demonstrate that very large CRFs can be trained efficiently and that very large models are able to improve the accuracy, while delivering compact parameter sets.  Elastic nets interpolate between L1 and L2, having been proposed by Zou and Hastie (2005) and used by Lavergne et al (2010) to regularize CRFs.     As our sequence labeling model we use the Wapiti implementation of Conditional Random Fields (Lavergne et al, 2010) with the L-BFGS optimizer and elastic net regularization with default settings.   We report results for the following hybridizations and CRF-based system using Wapiti (Lavergne et al., 2010).          We therefore treat the prediction of IS labels as a sequence labeling task. We train a CRF using wapiti (Lavergne et al, 2010), with the features outlined in Table 1.
New Figures Of Merit For Best-First Probabilistic Chart Parsing Best-first parsing methods for natural language try to parse efficiently by considering the most likely constituents first. Some figure of merit is needed by which to compare the likelihood of constituents, and the choice of this figure has a substantial impact on the efficiency of the parser. While several parsers described in the literature have used such techniques, there is little published data on their efficacy, much less attempts to judge their relative merits. We propose and evaluate several figures of merit for best-first parsing, and we identify an easily computable figure of merit that provides excellent performance on various measures and two different grammars. The (Caraballo and Charniak, 1998) article considers a number of different figures of merit for ordering the agenda, and ultimately recommends one that reduces the number of edges required for a full parse into the thousands. (Caraballo and Charniak, 1998) and [Gold98] use a figure which indicates the merit of a given constituent or edge, relative only to itself and its children but independent of the progress of the parse we will call this the edge's independent merit (IM).   Again it is reminiscent of a best-first parser (Caraballo and Charniak, 1998) in the use of an agenda and a chart, but is fundamentally different due to the fact that there is no input order.  Chitrao and Grishman (1990), Caraballo and Charniak (1998), Charniak et al (1998), and Collins (1999) describe best-first parsing, which is intended for a tabular item-based framework. Best-first parsers deal with this by allowing an upward propagation, which updates such edges' scores (Caraballo and Charniak, 1998). To situate our results, the FOMs used by (Caraballo and Charniak, 1998) require 10K edges to parse 96% of these sentences, while BF requires only 6K edges. Therefore, we were less concerned with improving efficiency, and more with the properties of this algorithm, which we consider a baseline method upon which more sophisticated techniques such as best-first parsing (Caraballo and Charniak, 1998). Similar to a best-first parser (Caraballo and Charniak, 1998), the highest scored hypothesis is expanded first. For efficiency reasons, we use a coarse-to-fine pruning scheme like that of Caraballo and Charniak (1998). It might be used to rapidly compute approximate outside-probability estimates to prioritize best-first search (e.g., Caraballo and Charniak, 1998). A quite different approach to parsing efficiency is taken in Caraballo and Charniak (1998). The results cited in Caraballo and Charniak (1998) can not be compared directly to ours, but are roughly in the same equivalence class. The standard methods of the best analysis selection (Caraballo and Charniak, 1998) usually use simple stochastic functions independent on the peculiarities of the underlying language. For instance, (Caraballo and Charniak, 1998) presents and evaluate different figures of merit in the context of best-first chart parsing. Caraballo and Charniak (1998) present best-first parsing with Figures of Merit that allows conditioning of the heuristic function on statistics of the input string.
Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora A significant portion of the world’s text is tagged by readers on social bookmarkwebsites. attribution an inherent problem in these corpora because most pages have multiple tags, but the tags do not always apply with equal specificity across the whole document. Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa. This introduces a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA’s latent topics and user tags. This allows Labeled LDA to directly learn word-tag correspondences. We demonstrate Labeled LDA’s improved expressiveness over traditional LDA with visualizations of a corpus of tagged web from Labeled LDA outperforms SVMs by more than 3 to 1 when extracting tag-specific document snippets. As a multi-label text classifier, our model is competitive with a discriminative baseline on a variety of datasets. However, one interesting result came from extending the feature space with topics derived from Latent Dirichlet Allocation (LDA) using similar methods to Ramage et al (2009).   Labeled LDA (LLDA) (Ramage et al 2009a) can be used to solve this problem. Modeling Tweets in a Latent Space: Ramage et al (2010) also use hash tags to improve the latent representation of tweets in a LDA framework, Labeled-LDA (Ramage et al, 2009), treating each hashtag as a label. Latent Dirichlet Allocation and its supervised extensions such as Labeled LDA (LLDA) (Ramage et al, 2009) and supervised LDA (sLDA) (Blei and McAuliffe, 2008) are powerful generative models that capture the underlying semantics of texts.  To address these issues we propose a distantly supervised approach which applies LabeledLDA (Ramage et al, 2009) to leverage large amounts of unlabeled data in addition to large dictionaries of entities gathered from Freebase, and combines information about an entity's context across its mentions. Distant Supervision with Topic Models: To model unlabeled entities and their possible types, we apply LabeledLDA (Ramage et al, 2009), constraining each entity's distribution over topics based on its set of possible types according to Freebase. In other models, this input is sometimes used to "fix," i.e. deterministically hold constant topic assignments (Ramage et al, 2009). To enable this, we first take class labeled data (doesn't need to be multi-class labeled data unlike (Ramage et al 2009)) and identify the discriminating features for each class. Of these models, the most related one to SeededLDA is the Labeled LDA model (Ramage et al 2009). Our purpose here is more specialized and similar to that of Labeled LDA (Ramage et al, 2009a) or FixedhLDA (Reisinger and Pas? ca, 2009) where the set of topics associated with a document is known a priori.  There have been various extensions to multi-grain (Titov and McDonald, 2008), labeled (Ramage et al, 2009), and sequential (Du et al, 2010) topic models. There have been various extensions to multi-grain (Titov and McDonald, 2008a), labeled (Ramage et al, 2009), partially-labeled (Ramage et al, 2011), constrained (Andrzejewski et al, 2009) models, etc. We present two variants of LDA that differ in the way attributes are associated with the induced LDA topics: Controled LDA (C-LDA) and Labeled LDA (L-LDA; Ramage et al (2009)). Recent work investigates ways of accommodating supervision with LDA, e.g. supervised topic models (Blei and McAuliffe, 2007), Labeled LDA (L-LDA) (Ramage et al, 2009) or DiscLDA (Lacoste-Julien et al, 2008).  L-LDA (Ramage et al, 2009) extends standard LDA to include supervision for specific target categories, yet in a different way: (i) The generative process includes a second observed variable, i.e. each document is explicitly labeled with a target category.
Introduction To The Special Issue On Computational Linguistics Using Large Corpora  An historical account of this empirical renaissance is provide in [Church and Mercer, 1993]. However, using more data usually leads to better results, or how Church and Mercer (1993) put it more data are better data?. which has commonly been used in the recent statistical NLP research (Church and Mercer, 1993).  Many of the possible cooccurrences are not observed even in a very large corpus (Church and Mercer, 1993). Much recent research in the field of natural language processing (NLP) has focused on an empirical, corpus-based approach (Church and Mercer, 1993).
Paraphrasing With Bilingual Parallel Corpora Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrasebased statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments. Bannard and Callison-Burch (2005) described a pivoting approach that can exploit bilingual parallel corpora in several languages. Bannard and Callison-Burch (2005) defined a paraphrasing probability between two phrases based on their translation probability through all possible pivot phrases as: Ppara (p1 ,p2)= sum piv Pt (piv|p1) Pt (p2|piv) where Pt denotes translation probabilies. One of the most popular methods leveraging bilingual parallel corpora is proposed by Bannard and Callison-Burch (2005). The Context-Sensitive Paraphrase Suggestion (CS-PS) model first finds a set of local paraphrases P of the input phrase K using the pivot-based method proposed by Bannard and Callison-Burch (2005).  This method was defined in (Bannard and Callison-Burch, 2005). ParaEval matches hypothesis and reference translations using paraphrases that are extracted from parallel corpora in an unsupervised fashion (Bannard and Callison-Burch, 2005). An alternative approach to paraphrase acquisition was proposed by Bannard and Callison-Burch (2005). We take advantage of transitivity of relevance to rank and filter the paraphrases generated by the pivot-based method (i.e., phrase are treated as paraphrases if they share the same translations) of Bannard and Callison-Burch (2005). For example, Bannard and Callison-Burch (2005) propose the pivot approach to generate phrasal paraphrases from an English-German parallel corpus. In this paper, we generate paraphrases adopting the pivot-based method proposed by Bannard and Callison-Burch (2005) in the first round. We first exploit the pivot-based method proposed by Bannard and Callison-Burch (2005) to populate our graph G using of candidate paraphrases cP={} from a bilingual parallel corpus B for a query phrase q. The algorithm in (Bannard and Callison-Burch, 2005) is used for this purpose by pivoting through phrases in the source and the target languages: for each source phrase, all occurrences of its target phrases are found, and all the corresponding source phrases of these target phrases are considered as the potential paraphrases of the original source phrase (Callison Burch et al, 2006). Following Bannard and Callison-Burch (2005), we identify Arabic phrases (a1) in the target corpus that are translated by at least one English phrase (e).  In this paper we examine the effectiveness of placing syntactic constraints on a commonly used paraphrasing technique that extracts paraphrases from parallel corpora (Bannard and Callison-Burch, 2005). Bannard and Callison-Burch (2005) extract paraphrases from bilingual parallel corpora. There are a number of strategies that might be adopted to alleviate this problem: Bannard and Callison-Burch (2005) rank their paraphrases with a language model when the paraphrases are substituted into a sentence. Bannard and Callison-Burch (2005) sum over multiple parallel corpora C to reduce the problems associated with systematic errors in the word alignments in one language pair. 
HMM-Based Word Alignment In Statistical Translation In this paper, we describe a new model for word alignment in statistical trans- lation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach us- es a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment prob- lem. The difference to the time align- ment HMM is that there is no monotony constraint for the possible word order- ings. We describe the details of the mod- el and test the model on several bilingual corpora. This would be more difficult in the HMM alignment model (Vogel et al., 1996). The most classical approaches are the probabilistic IBM models (Brown et al, 1993) and the HMM model (Vogel et al, 1996).  Word alignments were induced from the HMM based alignment model (Vogel et al, 1996), initialized with the bi lexical parameters of IBM Model 1 (Brown et al, 1993). Much of the additional work on generative modeling of 1 to N word alignments is based on the HMM model (Vogel et al, 1996). Extensions of the last two are included in this study together with alignments based on hidden Markov model (HMM) (Vogel et al, 1996) and inversion transduction grammars (ITG) (Wu, 1997). GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al, 1996) where EM algorithm is employed to estimate the model parameters. Any aligner such as (Al-Onaizan et al, 1999) or (Vogel et al, 1996) can be used to obtain word alignments. For each sentence in the training, three types of word alignments are created: maximum entropy alignment (Ittycheriah and Roukos, 2005), GIZA++ alignment (Och and Ney, 2000), and HMM alignment (Vogel et al, 1996). They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al, 1996). We use the IBM Model 1 (Brown et al, 1993) and the Hidden Markov Model (HMM, (Vogel et al, 1996)) to estimate the alignment model. Over the years, there have been many proposals to improve these reordering models, most notably Vogel et al (1996), which adds a first-order dependency. The starting point is the final alignment generated using GIZA++'s implementation of IBM Model 1 and the Aachen HMM model (Vogel et al, 1996). Generative word alignment models including IBM models (Brown et al, 1993) and HMM word alignment models (Vogel et al, 1996) have been widely used in various types of Statistical Machine Translation (SMT) systems. Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al, 1993) or the Hidden Markov Model (HMM) (Vogel et al,1996). Inspired by HMM word alignment (Vogel et al, 1996), our second distance measure is based on jump width. Starting with the IBM models (Brown et al,1993), researchers have developed various statistical word alignment systems based on different models, such as hidden Markov models (HMM) (Vogel et al, 1996), log-linear models (OchandNey, 2003), and similarity-based heuristic methods (Melamed, 2000). One class of particularly useful features assesses the goodness of the alignment path through the source sentence (Vogel et al., 1996). While classical approaches for word alignment are based on generative models (e.g., IBM models (Brown et al, 1993) and HMM (Vogel et al, 1996)), word alignment can also be viewed as a matching problem, where each word pair is associated with a score reflecting the desirability of aligning that pair, and the alignment is then the highest scored matching under some constraints. This data is used to train a word alignment model, such as IBM Model 1 (Brown et al, 1993) or HMM-based word alignment (Vogel et al, 1996).
A Gibbs Sampler for Phrasal Synchronous Grammar Induction We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches. Blunsom et al (2009) describe a blocked sampler following John son et al (2007) which uses the Metropolis-Hastings algorithm to correct proposal samples drawn from an approximating SCFG, however this is discounted as impractical due to the O (|f |3|e|3) complexity.  In the following experiments we compare the slice sampler and the Gibbs sampler (Blunsom et al, 2009), in terms of mixing and translation quality. We evaluate three initialisers: M4: the symmetrised output of GIZA++factorised into ITG form (as used in Blunsom et al (2009)). Following (Blunsom et al, 2009) we used a vague gamma prior (10? 4, 104), and sampled new values from a log-normal distribution whose mean was the value of the parameter, and variance was 0.3. Recent progress in better parameterisation and approximate inference (Blunsom et al., 2009) can only augment the performance of these models to a similar level as the baseline where bidirectional word alignments are combined with heuristics and subsequently used to induce translation equivalence (e.g. (Koehn et al., 2003)). Other recent approaches use Gibbs sampler for learning the SCFG by exploring a fixed grammar having pre-defined rule templates (Blunsom et al, 2008) or by reasoning over the space of derivations (Blunsom et al, 2009). We explore a subset of the space of rules being considered by (Blunsom et al, 2009) i.e., only those rules satisfying the word alignments and heuristically grown phrase alignments. For this we adopt the approach of Blunsom et al (2009b), who present a method for maintaining table counts without needing to record the table assignments for each translation decision. We apply the technique from Blunsom et al (2009a) of using multiple processors to perform approximate Gibbs sampling which they showed achieved equivalent performance to the exact Gibbs sampler. Our ArEn training data comprises several LDCcorpora, using the same experimental setup as in Blunsom et al (2009a). We use this method motivated by Gibbs Sampler (Blunsom et al, 2009) which has been used for efficiently learning rules. Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al, 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al, 2009). Our smoothing distribution of phrase pairs for all pre-terminals considers source-target phrase lengths drawn from a Poisson distribution with unit mean, drawing subsequently the words of each of the phrases uniformly from the vocabulary of each language, similar to (Blunsom et al, 2009). Recent work, e.g. by Blunsom et al (2009) and Haghihi et al. (2009) just to name a few, show that alignment models that bear closer resemblance to state-of-the art translation model consistently yields not only a better alignment quality but also an improved translation quality. Blunsom et al (2009) present a nonparametric PSCFG translation model that directly induces a grammar from parallel sentences without the use of or constraints from a word-alignment model, and Cohn and Blunsom (2009) achieve the same for tree-to-string grammars, with encouraging results on small data. Also related to STIR is previous work on bilingual grammar induction from parallel corpora using ITG (Blunsom et al, 2009).  Samplers are initialised with trees created from GIZA++ alignments constructed using a SCFG factorisation method (Blunsom et al, 2009a). For AREN experiments the language model is trained on English data as (Blunsom et al, 2009a), and for FA-EN and UR EN the English data are the target sides of the bilingual training data.
Automatic Acquisition Of Subcategorization Frames From Untagged Text This paper describes an implemented program that takes a raw, untagged text corpus as its only input (no open-class dictionary) and generates a partial list of verbs occurring in the text and the subcategorization frames (SFs) in which they occur. Verbs are detected by a novel technique based on the Case Filter of Rouvret and Vergnaud (1980). The completeness of the output list increases monotonically with the total number of occurrences of each verb in the corpus. False positive rates are one to three percent of observations. Five SFs are currently detected and more are planned. Ultimately, I expect to provide a large SF dictionary to the NLP community and to train dictionaries for specific corpora. The relatively high efficiency rate, as compared with the figures reported in (Brent, 1991), are due to the fact that Italian morphology is far more complex than English. Once a good morphologic analyzer is available (the one used in our work is very well tested, and has first described in (Russo, 1987)), problems such as verb detection, raised in (Brent, 1991), are negligible. Since (Brent 1991) there have been a considerable amount of researches focusing on verb lexicons with respective sub categorization information specified both in the field of traditional linguistics and that of computational linguistics. Finally, while statistical approaches like Brent (1991) can gather e.g. valence information from large corpora, we are more interested in full grammatical processing of individual sentences to maximally exploit each context.
TextTiling: Segmenting Text Into Multi-Paragraph Subtopic Passages Text Tiling is a technique for subdividing texts into multi-paragraph units that represent passages, or subtopics. The discourse cues for identifying major subtopic shifts are patterns of lexical co-occurrence and distribution. The algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts. Multi-paragraph subtopic segmentation should be useful for many text analysis tasks, including information retrieval and summarization. The performance with respect to identifying sentence boundaries appears to be close to that of systems aimed at identifying only sentence boundaries (Palmer and Hearst, 1997), whose accuracy is in the range of 99%. This similarity is computed by applying in the style of Hearst (1997) a cosine-based metric on the morphed segments. Foltz et al's (1998) approach is in line with the earlier TextTiling method that identifies subtopic structure in text (Hearst, 1997). Table 1 also presents the performance of a typical topic segmentation algorithm, TextTiling (Hearst, 1997). Many researchers have attempted to make use of cue phrases (Hirschberg and Litman, 1993), especially for segmentation both in prose (Hearst, 1997) and conversation (Galley et al, 2003). The best known algorithm based on this idea is TextTiling (Hearst, 1997). reference segmentation from a coder should not be trusted, given that inter-annotator agreement is often reported to be rather poor (Hearst, 1997, p. 54). (Siegel and Castellan, 1988) values for coders and automatic segmenters (Hearst, 1997, p. 56). Pairwise mean kappa scores were calculated by comparing a coder's segmentation against a reference segmentation formulated by the majority opinion strategy used in Passonneau and Litman (1993, p. 150) (Hearst, 1997, pp. 53-54). Coders often disagree in segmentation tasks (Hearst, 1997, p. 56), making it improbable that a single, correct, reference segmentation could be identified from human codings. This category choice is similar to those chosen by Hearst (1997, p. 53), who computed chance agreement in terms of the probability that coders would say that a segment boundary exists (segt), and the probability that they would not (unsegt). The best known algorithm based on this idea is TextTiling (Hearst, 1997). Automatic segmentation of discourse forms the basis for many applications, from information retrieval and text summarisation to anaphora resolution (Hearst, 1997). While agreement among annotators regarding linear segmentation has been found to be higher than 80% (Hearst, 1997), with respect to hierarchical segmentation it has been observed to be as low as 60% (Flammia and Zue, 1995). As in (Hearst, 1997), we segment each document into several 'mini-documents', each one devoted to a single topic, and then to perform location and topic-based clustering over the (now larger) set of mini-documents. So far, it has been used mainly in the context of automatic text segmentation, where a change in vocabulary is often the mark of topic change (Hearst, 1997), and, to a lesser extent, in discourse studies (see, e.g., (Foltz et al, 1998)). In this study, we use Galley et al's (2003) LCSeg algorithm, a variant of TextTiling (Hearst, 1997). To compute a baseline, we follow Kan (2003) and Hearst (1997) in using Monte Carlo simulated segments. Compared to the task of segmenting expository texts reported in Hearst (1997) with a 39.1% chance of each paragraph end being a target topic boundary, the chance of each speaker turn being a top-level or sub-topic boundary in our ICSI corpus is just 2.2% and 0.69%. For example, lexical cohesion-based algorithms, such as LCSEG (Galley et al, 2003), or its word frequency-based predecessor TextTile (Hearst, 1997) capture topic shifts by modeling the similarity of word repetition in adjacent windows.
Integrating Multiple Knowledge Sources For Detection And Correction Of Repairs In Human-Computer Dialog We have analyzed 607 sentences of spontaneous human-computer speech data containing repairs, drawn from a total corpus of 10,718 sentences. We present here criteria and techniques for automatically detecting the presence of a repair, its location, and making the appropriate correction. The criteria involve integration of knowledge from several sources: pattern matching, syntactic and semantic analysis, and acoustics. Results of testing the first stage of this model, the lexical pattern matcher, are reported in (Bear et al, 1992): 309 of 406 utterance containing nontrivial  repairs in their 10,718 utterance corpus were correctly identified, while 191 fluent utterances were incorrectly identified as containing repairs. Bear et al (1992) also speculate that acoustic information might be used to filter out false positives for candidates matching two of their lexical patterns - repetitions of single words and cases of single inserted words - but do not report such experimentation. As noted in (Bear et al, 1992), knowledge about the location of word fragments would be an invaluable cue to both detection and correction of disfluencies. For annotating speech repairs, we have extended the scheme proposed by Bear et al (1992) so that it better deals with overlapping and ambiguous repairs. Bear et al (1992) explored pattern matching, parsing and acoustic cues and concluded that multiple sources of information would be needed to detect edit dis fluencies. The SRI group (Bear et al, 1992) employed simple pattern matching techniques for detecting and correcting modification repairs. (Hindle, 1983) and (Bear et al., 1992) performed speech repair identification in their parsers, and removed the corrected material (reparandum) from consideration. This approach is similar to an experiment in (Bear et al, 1992) except that Bear et al were more interested in reducing false alarms. 
Identifying Relations for Open Information Extraction Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary. This paper shows that the output of state-ofthe-art Open IE systems is rife with uninformative and incoherent extractions. To overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs. We imthe constraints in the Open IE system, which more than doubles the area under the precision-recall curve relative previous extractors such as More than are at precision higher— compared to virtually none for earlier systems. The paper concludes with a detailed analysis Reverb (Fader et al 2011) is a state-of-the-art open domain extractor that targets verb-centric relations, which have been shown in Banko and Etzioni (2008) to cover over 70% of open domain relations. Literature on automatic relation discovery (Fader et al, 2011) has shown that verbal phrases uncover a large fraction of binary predicates while reducing the amount of noisy phrases that do not denote any relations. To this end, we used a random sample from the large scale web-based ReVerb corpus (Fader et al, 2011), comprising tuple extractions of predicate templates with their argument instantiations. Two example systems implementing this paradigm are TEXTRUN NER (Yates et al, 2007) and REVERB (Fader et al, 2011). The propositions are usually produced by an extraction method, such as TextRunner (Banko et al, 2007) or ReVerb (Fader et al, 2011). To that end, we applied the methods on a set of one billion extractions (generously provided by Fader et al (2011)) automatically extracted from the ClueWeb09 web crawl, where each extraction comprises a predicate and two arguments.  To build the rule-sets and models for the tested approaches we utilized the ReVerb corpus (Fader et al, 2011), a large scale publicly available web based open extractions data set, containing about 15 million unique template extractions. In terms of the latter, Cai and Yates (2013) and Berant et al (2013) applied pattern matching and relation intersection between Freebase relations and predicate argument triples from the ReVerb OpenIE system (Fader et al, 2011). We compare OLLIE to two state-of-the-art Open IE systems: (1) REVERB (Fader et al2011), which uses shallow syntactic processing to identify relation phrases that begin with a verb and occur between the argument phrases; 2 (2) WOEparse (Wuand Weld, 2010), which uses bootstrapping from entries in Wikipedia info-boxes to learn extraction pat terns in dependency parses. We say a schema is a textual schema if it has been extracted from free text, such as the Nell (Carlson et al, 2010) and ReVerb (Fader et al, 2011) extracted databases. MATCHER uses an API for the ReVerb Open IEsystem (Fader et al, 2011) to collect I (rT), for each rT. We obtained 155,409 positive instances from the English sentences using an off-the-shelf relation extraction system, ReVerb (Fader et al., 2011). The REVERB extractor (Fader et al 2011) on the ClueWeb09 Web corpus found over 1.4 billion noun phrases participating in textual relationships, and a sizable portion of these noun phrases are entities. Zhang and Weld (2013) is based on REVERB (Fader et al, 2011), which uses a regular expression on part-of-speech tags to produce the extractions. For convenience, we identify part-whole relations in Rule 12 based on the output produced by ReVerb (Fader et al 2011), an open information extraction system. Fader et al (2011) utilizes a confidence function. (Fader et al (2011) found that this set covers 69% of their corpus). In this paper, we present an approach for learning to map questions to formal queries over a large, open-domain database of extracted facts (Fader et al, 2011). We performed an end-to-end evaluation against a database of 15 million facts automatically extracted from general web text (Fader et al, 2011).
Semi-Supervised Training For Statistical Word Alignment We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus. We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality. Also, several models are proposed to address the problem of improving generative models with small amount of manual data, including Model 6 (Och and Ney, 2003) and the model proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). Fraser and Marcu (2006) propose an EMD algorithm, where labeled data is used for discriminative reranking. With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features. Following the lead of (Fraser and Marcu, 2006), we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines (Melamed, 1998).  EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements.  For an alignment model, most of these use the Aachen HMM approach (Vogel et al, 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006).  If human-aligned data is available, the EMD algorithm provides higher baseline alignments than GIZA++ that have led to better MT performance (Fraser and Marcu, 2006). We use the semi-supervised EMD algorithm (Fraser and Marcu, 2006b) to train the model. We compare semi-supervised LEAF with a previous state of the art semi-supervised system (Fraser and Marcu, 2006b). We ran the baseline semi-supervised system for two iterations (line 2), and in contrast with (Fraser and Marcu, 2006b) we found that the best symmetrization heuristic for this system was  union, which is most likely due to our use of fully linked alignments which was discussed at the end of Section 3. (Fraser and Marcu, 2006b) described symmetrized training of a 1-to-N log-linear model and a M-to-1 log-linear model. Examples of this line of research include Model 6 (OchandNey, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. A super set of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. (Fraser and Marcu, 2006) have proposed an algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models.
Re-Evaluation The Role Of Bleu In Machine Translation Research We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric. We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu’s correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores. Although both methods have gained mainstream acceptance and have shown good correlations with human judgments, their deficiencies have become more evident and serious as research in MT and summarization progresses (Callison-Burch et al, 2006). Callison-Burch et al (2006) point out three prominent factors: Synonyms and paraphrases are only handled if they are in the set of multiple reference translations [available]; The scores for words are equally weighted so missing out on content-bearing material brings no additional penalty; The brevity penalty is a stop-gap measure to compensate for the fairly serious problem of not being able to calculate recall. This substitution technique has shown some improvement in translation quality (Callison-Burch et al, 2006). Nevertheless, it was later found that its correlation factor with subjective evaluations (the original reason for its success) is actually not so high as first thought (Callison-Burch et al, 2006). Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. Although Callison-Burch et al (2006) have recently called into question the utility of BLEU. This is a phenomenon seen in MT, where BLEU seems to favour text that has been produced using a similar statistical n-gram language model over other symbolic models (Callison-Burch et al, 2006). This is a concern shared by all automatic evaluation metrics, and potential problems in stand-alone metrics have been analyzed (Callison-Burch et al, 2006). It has certain shortcomings for comparing different machine translation systems, especially if comparing conceptually different systems, e.g. phrase-based versus rule-based systems, as shown in (Callison-Burch et al, 2006). The insensitivity of BLEU and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al (2006), but the criticism is widespread. Callison-Burch et al (2006) report that BLEU and NIST favour n-gram-based MT models such as Pharaoh (Koehn, 2004), so the translations produced by rule-based systems score lower on the automatic evaluation, even though human judges consistently rate their output higher than Pharaoh's translation. We believe this is a satisfactory result, given the fairly good starting performance, and given that the BLEU metric is known not to be very sensitive to word order variations (Callison-Burchetal., 2006).   Callison-Burch et al (2006b) show that in general a higher BLEU score is not necessarily indicative of better translation quality. For example, Callison-Burch et al (2006) presented a number of counter-examples to the claim that BLEU agrees with human judgements. Of course, deciding between MT outputs in the general case is a problem that currently has no good solution, and is unlikely to in the near future: Bleu (and similar metrics) require one or more reference texts to distinguish between candidate outputs with the level of accuracy that they achieve, and even then they are open to substantial criticism (Callison-Burch et al, 2006). In spite of its shortcomings (Callison-Burch et al, 2006), it has been considered the standard automatic measure in the development of SMT systems (with new measures being added to it, but not substituting it, see for e.g. Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator. For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001).
Efficient Feature-based Conditional Random Field Parsing Discriminative feature-based methods are widely used in natural language processing, but sentence parsing is still dominated by generative methods. While prior feature-based dynamic programming parsers have restricted training and evaluation to artificially short sentences, we present the first general, featurerich discriminative parser, based on a conditional random field model, which has been successfully scaled to the full WSJ parsing data. Our efficiency is primarily due to the use of stochastic optimization techniques, as well as parallelization and chart prefiltering. On WSJ15, we attain a state-of-the-artF-score a 14% relative reduction in error over previous models, while being two orders of magnitude faster. On sentences of length our system achieves an F-score of a 36% relative reduction in error over a generative baseline.  Instead, we build a log-linear CRF-based dependency parser, which is similar to the CRF based constituent parser of Finkel et al (2008). Finkel et al (2008) show that SGD achieves optimal test performance with far fewer iterations than other optimization routines such as L-BFGS. Therefore, we propose a simple corpus-weighting strategy, as shown in Algorithm 1, where Db i, k is the subset of training data used in kth update and b is the batch size; k is the update step, which is adjusted following the simulated annealing procedure (Finkel et al, 2008).  We found that the SGD parameters described by Finkel et al (2008) worked equally well for our models, and, on the baseline, produced similar results to L-BFGS. Typically, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al, 2008). We directly compare asynchronous algorithms with multiprocessor synchronous mini-batch algorithms (e.g., Finkel et al, 2008) and traditional batch algorithms. Finkel et al (2008) used this approach to speed up training of a log-linear model for parsing. As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al, 2008). Finkel et al (2008) incorporated rich local features into a tree CRF model and built a competitive parser. Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005).  The model can be used for tasks like syntactic parsing (Finkel et al, 2008) and semantic role labeling (Cohn and Blunsom, 2005). In an important contrast, Koo et al (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using cluster based word-senses as intermediate abstractions in addition to POS tags (also see Finkel et al (2008)). Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008). Recently, CRF using tree structures were used in (Finkel et al, 2008) in the case of parsing. Recently, there has been an explosion of interest in Conditional Random Fields (CRFs) (Lafferty et al., 2001) for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel et al2008), syntactic chunking (Sha and Pereira, 2003) and discourse chunking (Ghosh et al2011) in Penn Discourse Treebank (Prasad et al2008). One sees this clear trend in the supervised NLP literature examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al, 2005), exponentiated gradient algorithms (Collins et al, 2008), stochastic gradient for constituency parsing (Finkel et al, 2008). Once we have converted our sentences into parse trees, we train a discriminative constituency parser similar to that of (Finkelet al, 2008).
Parsing with Compositional Vector Grammars Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%. It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.   Another application of word vectors is compositional vector grammar (Socher et al, 2013). To our best knowledge, this is the first work on this issue in SMT community; In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al, 2011a; Socher et al, 2013). To obtain DCS trees from natural language, we use Stanford CoreNLP 5 for dependency parsing (Socher et al, 2013), and convert Stanforddependencies to DCS trees by pattern matching on POS tags and dependency labels.  This initialization of the composition matrices has previously been effective for parsing (Socher et al, 2013a). Crowdflower Task First, we parse the filtered IBC sentences using the Stanford constituency parser (Socher et al, 2013a). We attempted to apply syntactically-untied RNNs (Socher et al, 2013a) to our data with the idea that associating separate matrices for phrasal categories would improve representations at high-level nodes. Socher et al (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Several researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label, such as polarity in sentiment analysis (Socher et al, 2011), syntactic category in parsing (Socher et al, 2013a) and phrase reordering pattern in SMT (Li et al, 2013). First, these parsers are among the best in the literature, with a test performance of 90.7 F 1 for the baseline Berkeley parser on the Wall Street Journal corpus (compared to 90.4 for Socher et al (2013) and 90.1 for Henderson (2004)). Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al, 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al, 2013). 
Joint Learning Improves Semantic Role Labeling Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding. This stands in stark contrast to the linguistic observation that a core argument frame is with strong dependencies between arguments. We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative loglinear models. This system achieves an reduction of all arguments core arguments over a stateof-the art independent classifier for goldstandard parse trees on PropBank. Reranking has previously been applied to semantic role labeling by Toutanova et al (2005), from which we use several features. Toutanova et al (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicate argument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation.  Accordingly, we do not maximize the probability of the entire labeled parse tree as in (Toutanova et al, 2005). To enforce this constraint, we employ the approach presented by Toutanova et al (2005).  We implemented a global reranker following Toutanova et al (2005). Statistical parsers are major components in NLP applications such as QA (Kwok et al, 2001), MT (Marcu et al, 2006) and SRL (Toutanova et al, 2005). We employ this decomposition mainly for efficiency in training: that is, the decomposition allows us to train the classification models on a subset of training examples consisting only of those phrases that have a case marker, following Toutanova et al (2005). Toutanova et al (2005) report a substantial improvement in performance on the semantic role labeling task by building a joint classifier, which takes the labels of other phrases into account when classifying a given phrase. We applied the joint classifiers in the framework of N-best re ranking (Collins, 2000), following Toutanova et al (2005). Our system, on the other hand, follows a joint approach in the spirit of Toutanova et al (2005) and performs the above steps collectively. In contrast to the work of Toutanova et al (2005) our system applies on line learning to train its parameters and exact inference to predict a collective role labelling. In addition, while the system described here is based on pipelined classification, recent research on semantic role labeling has shown that significant performance improvements can be gained by exploiting interdependencies between arguments (Toutanova et al., 2005). In a recent paper on the SRL on verbal predicates for English, (Toutanova et al, 2005) pointed out that one potential flaw in a SRL system where each argument is considered on its own is that it does not take advantage of the fact that the arguments (not the adjuncts) of a predicate are subject to the hard constraint that they do not have the same label. The system, introduced in (Toutanova et al, 2005), implements a joint model that captures dependencies among arguments of a predicate using log-linear models in a discriminative re-ranking framework. The ones denoted with asterisks (*) were not present in (Toutanova et al, 2005). We find the exact top N consistent most likely local model labelings using a simple dynamic program described in (Toutanova et al, 2005). Most of the features we use are described in more detail in (Toutanova et al, 2005). The applications range from simple classification tasks such as text classification and history-based tagging (Ratnaparkhi, 1996) to more complex structured prediction tasks such as part of-speech (POS) tagging (Lafferty et al, 2001), syntactic parsing (Clark and Curran, 2004) and semantic role labeling (Toutanova et al, 2005).
Text-Translation Alignment No Figure 4 SAT after where 254 is a translation of the latter part of 218 and the early part of 219: When a proton strikes a gas nucleus, it produces three kinds of pion, of which one kind decays into two gamma rays. The gamma rays travel close to the original trajectory of the proton, and the model predicts they will be beamed toward the earth at just two points on the pulsars orbit around the companion star. Trifft em Proton auf einen Atomkern in dieser Gashfille, werden drei Arten von Pionen erzeugt. Die neutralen Pionen zerfallen in jeweils zwei Gammaquanten, die sich beinahe in dieselbe Richtung wie das urspriingliche Proton bewegen. Nach der Modellvorstellung gibt es gerade zwei Positionen im Umlauf des Pulsars urn semen Begleitstern, bei denen die Strahlung in Richtung zum Beobachter auf der Erde ausgesandt wird. Another example is provided by English sentences 19 and 20, which appear in German as sentences 21 and 22. However the latter part of English sentence 19 is in fact transferred to sentence 22 in the German. This is also unmistakable in the final results. Notice also, in this example, that the definition of &quot;photon&quot; has become a parenthetical expression at the beginning of the second German sentence, a fact which is not reflected. The other end of the cosmic-ray energy spectrum is defined somearbitrarily: any quantum greater than electron volts arriving from space is considered a cosmic ray. The definition encompasses not only particles but also gamma-ray photons, which are quanta of electromagnetic radiation. 2 4 4 4-5 ; 136 Martin Kay and Martin Riischeisen Text-Translation Alignment Le Correctness of sentence alignment in the various passes of the algorithm. Pass Correctness Coverage Constraint in SAT of SAT by AST 1 100% 12% 4% 2 100% 47% 17% 3 100% 89% 38% 4 99.7% 96% 41% Das untere Ende des Spektrums der kosmischen Strahlen ist verhaltnismai3ig unscharf definiert. Jedes Photon (Quant der elektromagnetischen Strahlung) Teilchen mit einer Energie von mehr als Elektronenvolt, das aus dem Weltraum eintrifft, bezeichnet man als kosmischen Strahl. frequently occurred in our data that sentences that separated by colons or semicolons in the original appeared as completely distinct sentences in the German translation. Indeed, the common usage in the two languages would probably have been better represented if we had treated colons and semicolons as sentence separators, along with periods, question marks, and the like. There are, of course, situations in which these punctuation marks are used in other ways, but they are considerably less frequent and, in any case, it seems that our program would almost always make the right associations. An example involving the colon is to be found in sentence 142 of the original, translated as sentences 163 and 164: The absorption lines established a lower limit on the distance of Cygnus X-3: it must be more distant than the farthest hydrogen cloud, which is believed to lie about 37,000 light-years away, near the edge of the galaxy. dieser Absorptionslinie kann eine untere Grenze der Entvon Cygnus X bestimmen. Die Quelle mu13 jenseits am weitesten entfernten Wasserstoff-Wolke sein, also weiter als ungefahr 37000 Lichtjahre entfernt, am Rande der Milchstrai3e. sentence 197, containing semicolon, is translated by German sentences 228 and 229: The estimate is conservative; because it is based on the gamma rays observed arriving at the earth, it does not take into account the likelihood that Cygnus X emits cosmic rays in all directions. Dies ist eine vorsichtige Absch5tzung. Sie ist nur aus den Gammastrahlen-Daten abgeleitet, die auf der Erde gemessen werden; daI3 Cygnus X-3 wahrscheirtlich kosmische Strahlung in alle Richtungen aussendet, ist dabei noch nicht beriicksichtigt. 137 Computational Linguistics Volume 19, Number 1 German Sentence No CA e• tc•' x • 10 20 30 40 50 English Sentence No Figure 5 alignment of the first of the test texts: true alignment (dots) and hypothesis of the SAT after the first pass (circles) and after the second pass (crosses). Table 3 summarizes the accuracy of the algorithm as a function of the number of passes. The (thresholded) SAT is evaluated by two criteria: the number of correct by the total number alignments, and—since the SAT does not necessarily give an alignment for every sentence—the coverage, i.e., the number of sentences with at least one entry relative to the total number of sentences. An alignment is said to be correct if the SAT contains exactly the numbers of the sentences that are complete or partial translations of the original sentence. The coverage of 96% of the SAT in pass 4 is as much as one would expect, since the remaining nonaligned sentences are one-zero alignments, most of them due to the German subheadings that are not part of the English version. The table also shows that the AST always provides a significant number of candidates for alignment with each sentence before a pass: the fourth column gives the number of true sentence alignments relative to the total number of candidates in the AST. Recall that the final alignment is always a subset of the hypotheses in the AST in every preceding pass. Figure 5 shows the true sentence alignment for the first 50 sentences (dots), and how the algorithm discovered them: in the first pass, only a few sentences are set into correspondence (circles); after the second pass (crosses) already almost half of the correspondences are found. Note that there are no wrong alignments in the first two passes. In the third pass, almost all of the remaining alignments are found (for the first 50 sentences in the figure: all), and a final pass usually completes the alignment. Our algorithm produces very favorable results when allowed to converge gradually. Processing time in the original LISP implementation was high, typically several hours for each pass. By trading CPU time for memory massively, the time needed by a C++ implementation on a Sun 4/75 was reduced to 1.7 mm for the first pass, 0.8 mm for the second, and 0.5 min for the third pass in an application to this pair of articles. (Initialization, i.e., reading the files and building up the data structures, takes another 0.6 min in the beginning.) It should be noted that a naive implementation of 138 Martin Kay and Martin Roscheisen Text-Translation Alignment the algorithm without using the appropriate data structures can easily lead to times that are a factor of 30 higher and do not scale up to larger texts. The application of our method to a text that we put together from the Hansard corpus had essentially no problem in identifying the correct sentence alignment in a process of five passes. The alignments for the first 1000 sentences of the English text were checked by hand, and seven errors were found; five of them occurred in sentences where sentence boundaries were not correctly identified by the program of periods that did not mark a sentence boundary and were identified such by very simple preprocessing program. The other two errors involved two short sentences for which the SAT did not give an alignment. Processing time increased essentially linearly (per pass): the first pass took 8.3 min, the second 3.2 mm, and it further decreased until the last pass, which took 2.1 min. (Initialization took 4.2 min.) Note that the error rate depends crucially on the kind of &quot;annealing schedule&quot; used: if the thresholds that allow a word pair in the WAT to influence the SAT are lowered too fast, only a few passes are needed, but accuracy deteriorates. For example, in an application where the process terminated after only three passes, the accuracy was only in the eighties (estimated on the basis of the first 120 sentences of the English Hansard text checked by hand). Since processing time after the first pass is usually already considerably lower, we have found that a high accuracy can safely be attained when more passes are allowed than are actually necessary. In order to evaluate the sensitivity of the algorithm to the lengths of the texts that are to be aligned, we applied it to text samples that ranged in length from 10 to 1000 sentences, and examined the accuracy of the WAT after the first pass; that is, more precisely, the number of word pairs in the WAT that are valid translations relative to the total number of word pairs with a similarity of not less than 0.7 (the measurements are cross-validated over different texts). The result is that this accuracy increases asymptotically to 1 with the text length, and is already higher than 80% for text length of 100 sentences (which sufficient to reach an almost perfect alignment in the end). Roughly speaking, the accuracy is almost 1 for texts longer than 150 sentences, and around 0.5 for text length in the lower range from 20 to 60. In other words, texts of a length of more than 150 sentences are suitable to be processed in this way; text fragments shorter than 80 sentences do not have a high proportion of correct word pairs in the first WAT, but further experiments showed that the final alignment for texts of this length is, on average, again almost perfect: the drawback of a less accurate initial WAT is apparently largely compensated for by the fact that the AST is also narrower for these texts; however, the variance in the alignment accuracies is significantly higher. 5. Related Work Since we addressed the text translation alignment problem in 1988, a number of researchers, among them Gale and Church (1991) and Brown, Lai, and Mercer (1991), have worked on the problem. Both methods are based on the observation that the length of text unit is highly correlated to the length of the translation of this unit, no matter whether length is measured in number of words or in number of characters (see Figure 6). Consequently, they are both easier to implement than ours, though not necessarily more efficient. The method of Brown, Lai, and Mercer (1991) is based on a hidden Markov model for the generation of aligned pairs of corpora, whose parameters are estimated from a large text. For an application of this method to the Canadian Hansard, good results are reported. However, the problem was also considerably facilitated by the way the implementation made use of Hansard-specific comments 139 Linguistics Volume 19, Number German: Length in wards CO German: Length in chars 40 60 80 120 200 400 600 800 English: Length in words English: Length in chars Figure 6 Lengths of Aligned Paragraphs are Correlated: Robust regression between lengths of aligned paragraphs. Left: length measured in words. Right: length measured in characters. and annotations: these are used in a preprocessing step to find anchors for sentence alignment such that, on average, there are only ten sentences in between. Moreover, corpus is for the near literalness of its translations, and it is therefore unclear to what extent the good results are due to the relative ease of the problem. This would be an important consideration when comparing various algorithms; when the algorithms are actually applied, it is clearly very desirable to incorporate as much prior knowledge (say, on potential anchors) as possible. Moreover, long texts can almost always be expected to contain natural anchors, such as chapter section headings, at which to make an priori Gale and Church (1991) note that their method performed considerably better when lengths of sentences were measured in number of characters instead of in number of words. Their method is based on a probabilistic model of the distance between and a dynamic programming algorithm is used to minimize the total aligned Their implementation assumes that character in one language gives rise to, on average, one character in the other language.' In our texts, one character in English on average gives rise to somewhat more than 1.2 characters in German, and the correlation between the lengths (in characters) of aligned paragraphs in the two languages was with 0.952 lower than the 0.991 that are menin Gale and Church (1991), which supports our impression that the we used are hard texts to align, but it is not clear to what extent this would deteriorate the results. In applications to economic reports from the Union Bank of Switzerland, the method performs very well on simple alignments (one-to-one, oneto-two), but has at the moment problems with complex matches. The method has the 8 Recall that, in a similar way, we assumed in our implementation that one sentence in one language gives rise to, on average, n/rn sentences in the other language (see first footnote in Section 2.3). 140 Martin Kay and Martin Riischeisen Text-Translation Alignment advantage of associating a score with pairs of sentences so that it is easy to extract a subset for which there is a high likelihood that the alignments are correct. Given the simplicity of the methods proposed by Brown, Lai, and Mercer and Gale and Church, either of them could be used as a heuristic in the construction of the initial AST in our algorithm. In the current version, the number of candidate sentence pairs that are considered in the first pass near the middle of a text contributes disproportionally to the cost of the computation. In fact, as we remarked earlier, the of this step is proposed modification would effectively make it linear. 6. Future Work For most practical purposes, the alignment algorithm we have described produces very satisfactory results, even when applied to relatively free translations. There are doubtless many places in which the algorithm itself could be improved. For example, it is clear that the present method of building the SAT favors associations between long sentences, and this is not surprising, because there is more information in long sentences. But we have not investigated the extent of this bias and we do not therefore know it as appropriate. The present algorithm rests on being able to identify one-to-one associations between certain words, notably technical terms and proper names. It is clear from a brief inspection of Table 2 that very few correspondences are noticed among everyday words and, when they are, it is usually because those words also have precise technical uses. The very few exceptions include &quot;only&quot;/&quot;nur&quot; and 'the&quot;/&quot;die-.&quot; The pair &quot;per&quot;/&quot;pro&quot; might also qualify but if the languages afford any example of a scientific preposition, this is surely it. The most interesting further developments would be in the direction of loosening up this dependence on one-to-one associations both because this would present a very significant challenge and also because we are convinced that our present method identifies essentially all the significant one-to-one associations. There are two obvious kinds of looser associations that could be investigated. One would consist of connections between a single vocabulary item in one language and two or more in the other, or even between several items in one language and several in the other. The other would involve connections—one–one, one–many, or many–many—between phrases or recurring sequences. We have investigated the first of these enough to satisfy ourselves that there is latent information on one-to-many associations in the text, and that it can be revealed by suitable extensions of our methods. However, it is clear that the combinatorial problems associated with this approach are severe, and pursuing it would require much fine tuning of the program and designing much more effective ways of indexing the most important data structures. The key to reducing the combinatorial explosion probably lies in using tables of similarities such as those the present algorithm uses to suggest combinations of items that would be worth considering. If such an approach could be made efficient enough, it is even possible that it would provide a superior way of solving the problem for which our heuristic methods of morphological analysis were introduced. Its superiority would come from the fact that it would not depend on words being formed by concatenation, but would also accommodate such phenomena as umlaut, ablaut, vowel harmony, and the nonconcatenative process of Semitic morphology. The problems of treating recurring sequences are less severe. Data structures, such as the Patricia tree (Knuth 1973; pp. 490-493) provide efficient means of identifying all such sequences and, once identified, the data they provide could be added to 141 Computational Linguistics Volume 19, Number 1 the WAT much as we now add the results of morphological analysis. Needless to say, this would only allow for uninterrupted sequences. Any attempt to deal with discontinuous sequences would doubtless also involve great combinatorial problems. These avenues for further development are intriguing and would surely lead to interesting results. But it is unlikely that they would lead to much better sets of associations among sentences than are to be found in the SATs that our present program produces, and it was mainly these results that we were interested in from the outset. The other avenues we have mentioned concern improvements in the WAT which, for us, was always a secondary interest. Word forms are treated as instances of one and the same word if either their actual or normalised forms are equal (Kay and Roscheisen,1993). The morphology algorithm proposed by Kay and Roscheisen (1993) is applied for splitting potential suffixes and prefixes and for obtaining the normalised word forms. Kay and Roscheisen (1993) tried lexical methods for sentence alignment. Several automatic sentence alignment approaches have been proposed based on sentence length (Brown et al, 1991) and lexical information (Kay and Roscheisen, 1993).
Driving Semantic Parsing from the World&rsquo;s Response Current approaches to semantic parsing, the task of converting text to a formal meaning representation, rely on annotated training data mapping sentences to logical forms. Providing this supervision is a major bottleneck in scaling semantic parsers. This paper presents a new learning paradigm aimed at alleviating the supervision burden. We develop two novel learning algorithms capable of predicting complex structures which only rely on a binary feedback signal based on the context of an external world. In addition we reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers. Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available. Recent work by Clarke et al (2010) and Liang et al. Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers. In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance. To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments. Due to space consideration, we provide a brief description (see (Clarke et al, 2010) for more details). We restrict the possible assignments to the decision variables, forcing the resulting output formula to be syntactically legal, for example by restricting active variables to be type consistent, and forcing the resulting functional composition to be acyclic and fully connected (we refer the reader to (Clarke et al, 2010) for more details). However, when trained using the noisy supervision, our method achieves substantially more accurate translations than a state-of-the-art semantic parser (Clarke et al, 2010) (specifically, 80.0% in F-Score compared to an F-Score of 66.7%). It continues sampling a specification tree for each text specification until it finds one which successfully reads all of the input examples. The second baseline Aggressive is a state-of the-art semantic parsing framework (Clarke et al, 2010). The framework repeatedly predicts hidden structures (specification trees in our case) using a structure learner, and trains the structure learner based on the execution feedback of its predictions. As in Clarke et al (2010), we obviate the need for annotated logical forms by considering the end-to-end problem of mapping questions to answers. At the same time, representations such as FunQL (Kate et al, 2005), which was used in Clarke et al (2010), are simpler but lack the full expressive power of lambda calculus. We first compare our system with Clarke et al (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs. One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liangetal2011) or even a binary correct/incorrect signal (Clarke et al2010).
Semiring Parsing We synthesize work on parsing algorithms, deductive parsing, and the theory of algebra applied to formal languages into a general system for describing parsers. Each parser performs abstract computations using the operations of a semiring. The system allows a single, simple representation to be used for describing parsers that compute recognition, derivation forests, Viterbi, n-best, inside values, and other values, simply by substituting the operations of different semirings. We also show how to use the same representation, interpreted differently, to compute outside values. The system can be used to describe a wide variety of parsers, including Earley's algorithm, tree adjoining grammar parsing, Graham Harrison Ruzzo parsing, and prefix value computation.  Figure 3 presents the deductive inference rules (Goodman, 1999) for our generation algorithm. We presume that readers are familiar with declarative descriptions of inference algorithms, as well as with semiring parsing (Goodman, 1999). Goodman (1999) shows how a parsing logic can be combined with various semirings to compute different kinds of information about the input. Under the various derivation semirings (Goodman, 1999). To the best of our knowledge, Logic CT is the first published translation logic to be compatible with all of the semirings catalogued by Goodman (1999). Borrowing terms from parsing semirings (Goodman, 1999), a packed forest is composed of additive forest nodes and multiplicative forest nodes. Goodman (1999) augmented such logic programs with semiring weights, giving an algebraic explanation for the intuitive connections among classes of algorithms with the same logical structure. This is often done using back pointers, but can also be accomplished by representing the most probable proof for each theorem in its entirety as part of the semiring value (Goodman, 1999). This is similar to the k-best semiring defined by Goodman (1999). Goodman (1999) handles this situation more carefully, though our version is more likely to be used in practice for both the Viterbi proof and k-best proof semirings. Solvers have been proposed by Goodman (1999), by Klein and Manning (2001) using a hypergraph representation, and by Eisner et al (2005).  PTREE still propagates in O (n3) time: simply change the first-order parser's semiring (Goodman, 1999) to use max instead of sum. However, a more general framework to specify these algorithms is semiring-weighted parsing (Goodman, 1999). Following Goodman (1999), we present our lattice parser as a deductive proof system in Figure 2. Deductive logic (Pereira and Warren, 1983), extended with semirings (Goodman, 1999), is an established formal ism used in parsing. we write an inference rule with antecedents on the top line and consequent on the second line, following Goodman (1999) and Shieber et al (1995). This suggests a useful generalization: semiring-weighted deduction (Goodman, 1999). Goodman (1999) describes semirings for the Viterbi derivation, k-best Viterbi derivations, derivation forest, and number of paths.
Training Tree Transducers Many probabilistic models for natural language are now written in terms of hierarchical tree structure. Tree-based modeling still lacks many of the standard tools taken for granted in (finitestate) string-based modeling. The theory of tree transducer automata provides a possible framework to draw on, as it has been worked out in an extensive literature. We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers. Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. In our experiments, we use a translation model based on T2S tree transducers (Graehl and Knight,2004), constructed using the Travatar toolkit (Neu big, 2013). Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. In this section, we define the formal machinery of our recursive transformation model as a special case of xRs transducers (Graehl and Knight, 2004) that has only one state, and each rule is linear (L) and non-deleting (N) with regarding to variables in the source and target sides (henth the name 1-xRLNs). Graehl and Knight (2004) present an implementation that runs in time O (V log V+ E) using the method described in Algorithm 5 to ensure that every hyperedge is visited only once (assuming the priority queue is implemented as a Fibonaaci heap; for binary heap, it runs in O ((V+ E) log V)). Graehl and Knight (2004) described the use of tree transducers for natural language processing and addressed the training problems for this kind of transducers. Graehl and Knight (2004) proposed the use of target tree-to-source-string transducers (xRs) to model translation. The recomposed templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004). It is appealing to model the transformation of pi into f using tree-to-string (xRs) transducers, since their theory has been worked out in an extensive literature and is well understood (see, e.g., (Graehl and Knight, 2004)). While it is infeasible to enumerate the millions of derivations in each forest, Graehl and Knight (2004) demonstrate an efficient algorithm. Graehl and Knight (2004) and Melamed (2004), propose methods based on tree-to-tree mappings. WXTTs have been proposed by Graehl and Knight (2004) and Knight (2007) and are rooted in similar devices introduced earlier in the formal language literature (Arnold and Dauchet, 1982). Note that our semantics is equivalent to the classical term rewriting semantics, which is presented by Graehl and Knight (2004) and Graehl et al (2008), for example. The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006). The standard inside outside algorithm (Graehl and Knight, 2004) can be used to compute the expected counts of the TTS templates. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Graehl and Knight (2004) describe methods for training tree transducers. Such an algorithm is presented by Graehl and Knight (2004). We initially attempted to use the top-down DERIV algorithm of Graehl and Knight (2004), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring dead ends. The actual running of EM iterations (which directly implements the TRAIN algorithm of Graehl and Knight (2004)).
GATE: A Framework And Graphical Development Environment For Robust NLP Tools And Applications peg,. pyscropyrssexa pa...Inn/bee(Esrey:,we6onee nocnemere)seressartba 43csrassartsmeuroto-ssoprbonourseLproronparserrna flaw) sror spe6yer — ,r1 Transducerloacleo a erkirlE.FOL. *I PD.021 fP21111“enwialitneI r Figure 2: Unicode text in Gate2 witnessed on the software scene with the emergence of Unicode as a universal standard for representing textual data. GATE supports multilingual data processing using Unicode as its default text encoding. It also provides a means of entering text in various languages, using virtual keyboards where the language is not supported by the underlying operating platform. (Note that although Java represents characters as Unicode, it doesn't support input in many of the languages covered by Unicode.) Currently 28 languages are supported, and more are planned for future releases. Because GATE is an open architecture, new virtual keyboards can be defined by the users and added to the system as needed. For displaying the text, GATE relies on the rendering facilities offered by the Java implementation for the platform it runs on. Figure 2 gives an example of text in various languages displayed by GATE. The ability to handle Unicode data, along with the separation between data and implementation, allows LE systems based on GATE to be ported to new languages with no additional overhead apart from the development of the resources needed for the specific language. These facilities have been developed as part of the EMILLE project (McEnery et al., 2000), which focuses on the construction a 63 million word electronic corpus of South Asian languages. 3 Applications One of GATE's strengths is that it is flexible and robust enough to enable the development of a wide range of applications within its framework. In this section, we describe briefly some of the NLP applications we have developed using the GATE architecture. 3.1 MUSE The MUSE system (Maynard et al., 2001) is a multi-purpose Named Entity recognition system which is capable of processing texts from widely different domains and genres, thereby aiming to reduce the need for costly and time-consuming adaptation of existing resources to new applications and domains. The system aims to identify the parameters relevant to the creation of a name recognition system across different types of variability such as changes in domain, genre and media. For example, less formal texts may not follow standard capitalisation, punctuation and spelling formats, which can be a problem for many generic NE systems. Current evaluations with this system average around 93% precision and 95% recall across a variety of text types. 3.2 ACE The MUSE system has also been adapted to take part in the current ACE (Automatic Content Extraction) program run by NIST. This requires systems to perform recognition and tracking tasks of named, nominal and pronominal entities and their mentions across three types of clean news text (newswire, broadcast news and newspaper) and two types of degraded news text (OCR output and ASR output). 3.3 MUMIS The MUMIS (MUltiMedia Indexing and Searching environment) system uses Information Extraction components developed within GATE to produce formal annotations about essential events in football video programme material. This IE system comprises versions of the tokenisation, sentence detection, POS-tagging, and semantic tagging modules developed as part of GATE's standard resources, but also includes morphological analysis, full syntactic parsing and discourse interpretation modules, thereby enabling the production of annotations over text encoding structural, lexical, syntactic and semantic information. The semantic tagging module currently achieves around 91% precision and 76% recall, a significant improvement on a baseline named entity recognition system evaluated against it. 4 Processing Resources Provided with GATE is a set of reusable processing resources for common NLP tasks. (None of them are definitive, and the user can replace and/or extend them as necessary.) These are packaged together to form ANNIE, A Nearly- New IE system, but can also be used individually or coupled together with new modules in order to create new applications. For example, many other NLP tasks might require a sentence splitter and POS tagger, but would not necessarily require resources more specific to IE tasks such as a named entity transducer. The system is in use for a variety of IE and other tasks, sometimes in combination with other sets of application-specific modules. ANNIE consists of the following main processing resources: tokeniser, sentence splitter, POS tagger, gazetteer, finite state transducer (based on GATE's built-in regular expressions over annotations language (Cunningham et al., 2002)), orthomatcher and coreference resolver. The resources communicate via GATE's annotation API, which is a directed graph of arcs bearing arbitrary feature/value data, and nodes rooting this data into document content (in this case text). text into simple tokens, such as numbers, punctuation, symbols, and words of different types (e.g. with an initial capital, all upper case, etc.). The aim is to limit the work of the tokeniser to maximise efficiency, and enable greater flexibility by placing the burden of analysis on the grammars. This means that the tokeniser does not need to be modified for different applications or text types. splitter a cascade of finitestate transducers which segments the text into sentences. This module is required for the tagger. Both the splitter and tagger are domainand application-independent. a modified version of the Brill tagger, which produces a part-of-speech tag as an annotation on each word or symbol. Neither the splitter nor the tagger are a mandatory part of the NE system, but the annotations they produce can be used by the grammar (described below), in order to increase its power and coverage. of lists such as cities, organisations, days of the week, etc. It not only consists of entities, but also of names of useful as typical company designators (e.g. 'Ltd:), titles, etc. The gazetteer lists are compiled into finite state machines, which can match text tokens. tagger of handcrafted rules written in the JAPE (Java Annotations Pattern Engine) language (Cunningham et al., 2002), which describe patterns to match and annotations to be created as a result. JAPE is a version of CPSL (Common Pattern Specification Language) (Appelt, 1996), which provides finite state transduction over annotations based on regular expressions. A JAPE grammar consists of a set of phases, each of which consists of a set of pattern/action rules, and which run sequentially. Patterns can be specified by describing a specific text string, or annotations previously created by modules such as the tokeniser, gazetteer, or document format analysis. Rule prioritisation (if activated) prevents multiple assignment of annotations to the same text string. another optional module for the IE system. Its primary objective is to perform co-reference, or entity tracking, by recognising relations between entities. It also has a secondary role in improving named entity recognition by assigning annotations to previously unclassified names, based on relations with existing entities. identity relations between entities in the text. For more details see (Dimitrov, 2002). 4.1 Implementation The implementation of the processing resources is centred on robustness, usability and the clear distinction between declarative data representations and finite state algorithms The behaviour of all the processors is completely controlled by external resources such as grammars or rule sets, which makes them easily modifiable by users who do not need to be familiar with programming languages. The fact that all processing resources use finite-state transducer technology makes them quite performant in terms of execution times. Our initial experiments show that the full named entity recognition system is capable of processing around 2.5KB/s on a PITT 450 with 256 MB RAM (independently of the size of the input file; the processing requirement is linear in relation to the text size). Scalability was tested by running the ANNIE modules over a randomly chosen part of the British National Corpus (10% of all documents), which contained documents of up to 17MB in size. 5 Language Resource Creation Since many NLP algorithms require annotated corpora for training, GATE's development environment provides easy-to-use and extendable facilities for text annotation. In order to test their usability in practice, we used these facilities to build corpora of named entity annotated texts for the MUSE, ACE, and MUMIS applications. The annotation can be done manually by the user or semi-automatically by running some processing resources over the corpus and then correcting/adding new annotations manually. Depending on the information that needs to be annotated, some ANNIE modules can be used or adapted to bootstrap the corpus annotation task. For example, users from the humanities created a gazetteer list with 18th century place names in London, which when supplied to the ANNIE gazetteer, allows the automatic annotation of place information in a large collection of 18th century court reports from the Old Bailey in London. Since manual annotation is a difficult and error-prone task, GATE tries to make it simple to use and yet keep it flexible. To add a new annotation, one selects the text with the mouse (e.g., &quot;Mr. Clever&quot;) and then clicks on the desired annotation type (e.g., Person), which is shown in the list of types on the right-handside of the document viewer (see Figure 1). If however the desired annotation type does not already appear there or the user wants to associate more detailed information with the annotation (not just its type), then an annotation editing dialogue can be used. 6 Evaluation A vital part of any language engineering application is the evaluation of its performance, and a development environment for this purpose would not be complete without some mechanisms for its measurement in a large number of test cases. GATE contains two such mechanisms: an evaluation tool (AnnotationDiff) which enables automated performance measurement and visualisation of the results, and a benchmarking tool, which enables the tracking of a system's progress and regression testing. 6.1 The AnnotationDiff Tool Gate's AnnotationDiff tool enables two sets of annotations on a document to be compared, in order to either compare a system-annotated text with a reference (hand-annotated) text, or to compare the output of two different versions of the system (or two different systems). For each annotation type, figures are generated for precision, recall, F-measure and false positives. The AnnotationDiff viewer displays the two sets of annotations, marked with different colours (similar to 'visual diff' implementations such as in the MKS Toolkit or TkDiff). Annotations in the key set have two possible colours depending on their state: white for annotations which have a compatible (or partially compatible) annotation in the response set, and orange for annotations which are missing in the response set. Annotations in the response set have three possible colours: green if they are compatible with the key annotation, blue if they Figure 3: Fragment of results from benchmark tool are partially compatible, and red if they are spurious. In the viewer, two annotations will be positioned on the same row if they are co-extensive, and on different rows if not. 6.2 Benchmarking tool GATE's benchmarking tool differs from the AnnotationDiff in that it enables evaluation to be carried out over a whole corpus rather than a single document. It also enables tracking of the system's performance over time. The tool requires a clean version of a corpus (with no annotations) and an annotated reference corpus. First of all, the tool is run in generation mode to produce a set of texts annotated by the system. These texts are stored for future use. The tool can then be run in three ways: 1. Comparing the annotated set with the reference set; 2. Comparing the annotated set with the set produced by a more recent version of the system resources (the latest set); 3. Comparing the latest set with the reference set. In each case, performance statistics will be provided for each text in the set, and overall statistics for the entire set, in comparison with the reference set. In case 2, information is also provided about whether the figures have increased or decreased in comparison with the annotated set. The annotated set can be updated at any time by rerunning the tool in generation mode with the latest version of the system resources. Furthermore, the system can be run in verbose mode, where for each figure below a certain threshold (set by the user), the non-coextensive annotations (and their corresponding text) will be displayed. The output of the tool is written to an HTML file in tabular form, as shown in Figure 3. Current evaluations for the MUSE NE system are producing average figures of 90-95% Precision and Recall on a selection of different text types (spoken transcriptions, emails etc.). The default ANNIE system produces figures of between 80-90% Precision and Recall on news texts. This figure is lower than for the MUSE system, because the resources have not been tuned to a specific text type or application, but are intended to be adapted as necessary. Work on resolution of anaphora is currently averaging 63% Precision and 45% Recall, although this work is still very much in progress, and we expect these figures to improve in the near future. 7 Related Work GATE draws from a large pool of previous work on infrastructures, architectures and development environments for representing and processing language resources, corpora, and annotations. Due to space limitations here we will discuss only a small subset. For a detailed review and its use for deriving the desiderata for this architecture see (Cunningham, 2000). Work on standard ways to deal with XML data is relevant here, such as the LT XML work at Edinburgh (Thompson and McKelvie, 1997), as is work on managing collections of documents and their formats, e.g. (Brugman et al., 1998; Grishman, 1997; Zajac, 1998). We have also drawn from work on representing information about text and speech, e.g. (Brugman et al., 1998; Mikheev and Finch, 1997; Zajac, 1998; Young et al., 1999), as well as annotation standards, such as the ATLAS project (an architecture for linguistic annotation) at LDC (Bird et kirlactunerkterripNerl ABC19980430.1830.0858.sgm Annotation tope. , GPE RecallIncreaseon Atonaltmarked iron 06371426571426571la10 type Organization 1.0 increaseon hurnan-markedto 1 0 09444444444444444 Mug 07, limEll.ncreaseon tom0345to 07, 14153ING ANNOTATIONSIt To automatetetteABC Ir.NNOTATKMISinTo embroil,bath PARTIALLYCORRECT Pl4071&quot;ATIC*15nhe automateIDA, PratotationType Annotation type Person Precision increase on human-marked from 08947368421052632 lc 09444444444444444 03444444444444444 al., 2000). Our approach is also related to work on user interfaces to architectural facilities such as development environments, e.g. (Brugman et al., 1998) and to work on comparing different versions of information, e.g. (Sparck-Jones and Galliers, 1996; Paggio, 1998). This work is particularly novel in that it addresses the complete range of issues in NLP application development in a flexible and extensible way, rather than focusing just on some particular aspect of the development process. In addition, it promotes robustness, re-usability, and scalability as important principles that help with the construction of practical NLP systems. 8 Conclusions In this paper we have described an infrastructure for language engineering software which aims to assist the develeopment of robust tools and resources for NLP. One future direction is the integration of processing resources which learn in the background while the user is annotating corpora in GATE's visual environment. Currently, statistical models can be integrated but need to be trained separately. We are also extending the system to handle language generation modules, in order to enable the construction of applications which require language production in addition to analysis, e.g. intelligent report generation from IE data. Perspective GATE (Cunningham et al, 2002a) is an architecture, a framework and a development environment for human language technology modules and applications. The GATE API (Application Programming Interface) is fully documented in Javadoc and also examples are given in the comprehensive User Guide (Cunningham et al, 2002b). JAPE is a version of CPSL (Common Pattern Specification Language) (Appelt, 1996) and is used to describe patterns to match and annotations to be created as a result (for further details see (Cunningham et al, 2002b)).  The automatic alignments have then been manually corrected through a graphical editing tool within the GATE framework (Cunningham et al, 2002). The GATE system (Cunningham et al., 2002) is used to tag named entities, which are categorized as <Person>, <Organization>, <Location> and <Date>. In order to produce the gold standard annotations in GerManC-GS we used the GATE platform, which facilitates automatic as well as manual annotation (Cunningham et al 2002). Linguistic analysis of textual input is carried out using the General Architecture for Text Engineering (GATE) - a framework for the development and deployment of language processing technology in large scale (Cunningham et al, 2002). The algorithm was implemented using the GATE NLP framework (Cunningham et al, 2002) and texts preprocessed using the tokeniser, sentence splitter, and part-of-speech (POS) tagger provided with GATE. These named entities are tagged with GATE (Cunningham et al, 2002). The linguistic component uses the infrastructure and the following resources from GATE (Cunningham et al, 2002): tokenizer, sentence splitter, part-of-speech tagger, morphological analyzer and VPchunker. We developed a set of algorithms along with existing NLP tools (GATE (Cunningham et al, 2002) etc.) for this task. For named-entity recognition, we use GATE (Cunningham et al, 2002), augmented with named entity lists for locations, food types, restaurant names, and food subtypes (e.g. pizza), scraped from the we8there web pages. To this end, the GATE Gazetteer (Cunningham et al., 2002) was used, and only entities recognized by it automatically were considered. Our system is based on the GATE natural language processing framework (Cunningham et al, 2002) and it uses the ANNIE IE system included in the standard GATE distribution for text tokenization, sentence splitting and part-of-speech tagging. Similarly, we define additional features using the gazetteers from GATE, (Cunningham et al., 2002) namely, countries, person first/last names, trigger words;. An INIT is defined as a dated and located subject-verb-object triple, relying mostly on syntactical analyses from the MINIPAR parser (Lin, 1998) and linguistic annotations from the GATE information extraction engine (Cunningham et al., 2002). These patterns are implemented as regular expressions using the JAPE language (Cunningham et al, 2002). Similar mechanisms have also been proposed in other architectures to help heterogeneous linguistic modules to communicate through a common XML interface (see Cunningham et al,2002, Blache and Gunot, 2003). Some other systems are frameworks for performing generic tasks in one area of focus such as NLTK (Bird and Loper, 2004) and GATE (Cunningham et al, 2002) for Natural Language Processing; Pajek (Batagelj and Mrvar, 2003) and Guess (Adar, 2006) for Network Analysis and Visualization; and Lemur for Language Modeling and Information Retrieval.
Learning the Scope of Hedge Cues in Biomedical Texts Identifying hedged information in biomedical literature is an important subtask in information extraction because it would be misleading to extract speculative information as factual information. In this paper we present a machine learning system that finds the scope of hedge cues in biomedical texts. The system is based on a similar system that finds the scope of negation cues. We show that the same scope finding approach can be applied to both negation and hedging. To investigate the robustness of the approach, the system is tested on the three subcorpora of the BioScope corpus that represent different text types. The BioScope corpus has been used to train and evaluate automatic classifiers (e.g. Ozgur and Radev (2009) and Morante and Daelemans (2009)) with promising results. Using BioScope for training and evaluation, Morante and Daelemans (2009) developed a scope detector following a supervised sequence labeling approach while Ozgur and Radev (2009) developed a rule-based system that exploits syntactic patterns. Most systems following Morante and Daelemans (2009) used three class labels (F) IRST, (L) AST and NONE. The following summarizes the steps we took to achieve this goal. Similarly to previous work in hedge cue detection (Morante and Daelemans, 2009), we first convert the task into a sequential labeling task based on the BIO scheme, where each word in a hedge cue is labeled as B-CUE, I-CUE, or O, indicating respectively the labeled word is at the beginning of a cue, inside of a cue, or outside of a hedge cue; this is similar to the tagging scheme from the CoNLL-2001 shared task. To tackle the hedge cue detection problem posed by the CoNLL-2010 shared task, we utilized a classifier for sequential labeling following previous work (Morante and Daelemans, 2009). In Morante and Daelemans (2009), the hedge detection task is solved as two consecutive classification tasks. As the baseline classifier, we use the Cue Dictionary proposed in Morante and Daelemans (2009), classifying each occurrence of those words as a cue. Also, the task of hedge detection (Morante and Daelemans, 2009) can be solved separately, in the original sentences, after the interacting pairs have been found. Parts of the system are similar to that of Morante and Daelemans (2009) both make use of machine learning to tag tokens as being in a cue or a scope. The presence of potential clause ending words, used by Morante and Daelemans (2009), is included as a feature type with values: whereas, but, although, nevertheless, notwithstanding, how ever, consequently, hence, therefore, thus, instead, otherwise, alternatively, furthermore, moreover, since. Vincze et al (2008) created a publicly available annotated corpus of biomedical papers, abstracts and clinical data called BioScope, parts of which were also used as training data for the CoNLL10 shared task, building on the dataset and annotation scheme used for evaluation by Medlock and Briscoe (2007). Morante and Daelemans (2009) use the BioScope corpus to approach the problem of identifying cues and scopes via supervised machine learning. Morante and Daelemans (2009) presented a meta-learning system that finds the scope of hedge cues in biomedical texts. Shallow linguistic features are introduced in their experiments. Morante and Daelemans (2009) present their research on identifying hedge cues and their scopes. More experiments could be found in their paper (Morante and Daelemans, 2009). The main difference between the two systems is that Morante and Daelemans (2009) perform the second phase with a machine learner, whereas Ozgur and Radev (2009) perform the second phase witha rule-based system that exploits syntactic information. The approach to resolving the scopes of hedge cues that we present in this paper is similar to the approach followed in Morante and Daelemans (2009) in that the task is modelled in the same way. A difference between the two systems is that this system uses only one classifier to solve Task 2, whereas the system described in Morante and Daelemans (2009) used three classifiers and a metalearner. Another difference is that the system in Morante and Daelemans (2009) used shallow syntactic features, whereas this system uses features from both shallow and dependency syntax. It is not really possible to compare the scores obtained in this task to existing research previous to the CoNLL-2010 Shared Task, namely the results obtained by Ozgur and Radev (2009) on the BioScope corpus with a rule-based system and byMorante and Daelemans (2009) on the same corpus with a combination of classifiers. Morante and Daelemans (2009) report percentage of correct scopes for the full text data set (42.37), obtained by training on the abstracts data set, whereas the results presented in Table 5 are reported in Fmeasures and obtained in by training and testing on other corpora.
Learning Dependency Translation Models As Collections Of Finite-State Head Transducers The paper defines weighted head transducers, finite-state machines that perform middle-out string transduction. These transducers are strictly more expressive than the special case of standard leftto-right finite-state transducers. Dependency transduction models are then defined as collections of weighted head transducers that are applied hierarchically. A dynamic programming search algorithm is described for finding the optimal transduction of an input string with respect to a dependency transduction model. A method for automatically training a dependency transduction model from a set of input-output example strings is presented. The method first searches for hierarchical alignments of the training examples guided by correlation statistics, and then constructs the transitions of head transducers that are consistent with these alignments. Experimental results are given for applying the training method to translation from English to Spanish and Japanese. Wu (1997) and Alshawi et al (2000) showed statistical models based on syntactic structure. Dependency trees were found to correspond better across translation pairs than constituent trees by Fox (2002), and form the basis of the machine translation systems of Alshawi et al (2000). For a different approach that is based on dependency tree transformations, see Alshawi et al (2000). Yamada and Knight (2000, 2001) and Alshawi et al (2000) have effectively extended such syntactic transduction models to fully functional SMT systems, based on channel model tree transducers and finite state head transducers respectively. Methods such as (Wu, 1997), (Alshawi et al, 2000) and (Lopez et al, 2002) employ a synchronous parsing procedure to constrain a statistical alignment. Alshawi et al (2000) and Hwa et al (2005) explore transfer of deeper syntactic structure: dependency grammars. Other statistical machine translation systems such as (Wu, 1997) and (Alshawi et al, 2000) also produce a tree given a sentence. The latter are small and simple (Alshawi et al, 2000): tree nodes are words, and there need be no other structure to recover or align. However, the binary-branching SCFGs used by Wu (1997) and Alshawi et al (2000) are strictly less powerful than STSG. Methods such as (Wu, 1997), (Alshawi et al, 2000) and (Lopez et al, 2002) employ a synchronous parsing procedure to constrain a statistical alignment. (Alshawi et al, 2000) represents each production in parallel dependency trees as a finite-state transducer. Along similar lines, Alshawi et al (2000) treat translation as a process of simultaneous induction of source and target dependency trees using head transduction; again, no separate parser is used. Although hybrid approaches, such as dependency grammars augmented with phrase-structure information (Alshawi et al., 2000), can do re-ordering easily. Alshawi et al (2000) also presented a two-level arranged word ordering and chunk ordering by a hierarchically organized collection of finite state transducers. Wu (1997) showed that restricting word-level alignments between sentence pairs to observe syntactic bracketing constraints significantly reduces the complexity of the alignment problem and allows a polynomial-time solution. Alshawi et al (2000) also induce parallel tree structures from unbracketed parallel text, modeling the generation of each node's children with a finite-state transducer. In a somewhat related manner, Alshawi et al (2000) use cascaded head automata to derive dependency trees, but leave the nature of the cascading under-formalized. It is described in Alshawi et al (2000b). Wu (1997) and Alshawi et al (2000) used unsupervised learning on parallel text to induce syntactic analysis that was useful for their respective applications in phrasal translation extraction and speech translation, though not necessarily similar to what a human annotator would select. (Alshawi et al, 2000) extended the tree-based approach by representing each production in parallel dependency trees as a finite-state transducer. The input to our algorithm is a corpus consisting of pairs of sentences related by an hierarchical alignment (Alshawi et al, 2000).
Multilingual Dependency Analysis With A Two-Stage Discriminative Parser present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages. The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages. The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph. We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis.  Introduce through post-processing, e.g. through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al, 2006). Table 5 shows the official results for submitted parser outputs. The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006). Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences. The highest score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al (2006) with a LAS of 87.34 based on the TIGER tree bank, but we want to stress that these results are not comparable due to different data sets (and a different policy regarding the inclusion of punctuation). The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP) and labeled F-score (LF). McDonald et al (2006) use an additional algorithm. Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al, 2007b) and MST Parser (McDonald et al, 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and tree banks (McDonald and Nivre, 2007). In fact, our approach can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)'s parser, (McDonald et al., 2006)'s parser, and so on. But whereas the spanning tree parser of McDonald et al (2006) and the pseudo-projective parser of Nivre et al (2006) achieve this performance only with special pre or post-processing, the approach presented here derives a labeled non-projective graph in a single incremental process and hence at least has the advantage of simplicity. Moreover, it has better time complexity than the approximate second-order spanning tree parsing of McDonald et al (2006), which has exponential complexity in the worst case (although this does not appear to be a problem in practice). McDonald et al (2006) use post-processing for non-projective dependencies and for labeling. As described in (McDonald et al, 2006), we treat the labeling of dependencies as a sequence labeling problem. ULISSE was tested against the output of two really different data driven parsers: the first order Maximum Spanning Tree (MST) parser (McDonald et al., 2006) and the DeSR parser (Attardi, 2006) using Support Vector Machine as learning algorithm. It should be noted that McDonald et al (2006) use a richer feature set that is incomparable to our features.  The dependency parsers that we compare are the deterministic shift-reduce MaltParser (Nivre et al, 2007) and the second-order minimum spanning tree algorithm based MstParser (McDonald et al, 2006).  Entries marked with are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al (2006), Martins et al (2008), Martins et al (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different). The specific graph-based model studied in this work is that presented by McDonald et al (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation. More precisely, dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f (i, j, l) Rk, where f is typically a binary feature vector over properties of the arc as well as the surrounding input (McDonald et al, 2005a; McDonald et al, 2006).
Estimators For Stochastic Unification-Based Grammars Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars. We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical In previous work on log-linear models for LFG by Johnson et al (1999), pseudo likelihood estimation from annotated corpora has been introduced and experimented with on a small scale. The basic properties employed in our models are similar to the properties of Johnson et al (1999) which incorporate general linguistic principles into a log-linear model. The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Johnson and Riezler (2000). For these we use the 13 feature schemas described in Charniak and Johnson (2005), which were inspired by earlier work in discriminative estimation techniques, such as Johnson et al (1999) and Collins (2000). To overcome this problem, we penalize the objective function by adding a Gaussian prior (a term proportional to the squared norm as suggested in (Johnson et al, 1999)). The method is related to the boosting approach to ranking problems (Freund et al. 1998), the Markov Random Field methods of (Johnson et al 1999), and the boosting approaches for parsing in (Collins 2000). It does this using the re ranking methodology described in Collins (2000), using a Maximum Entropy model with Gaussian regularization as described in Johnson et al (1999). A Gaussian prior also handles the problem of "pseudo-maximal" features (Johnson et al, 1999). For example, Johnson et al (1999) and Riezler et al (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. Following (Johnson et al, 1999), a conditional ME model of the probabilities of trees {t1 ... tn} for a string s, and assuming a set of feature functions {f1 ... fm} with corresponding weights {λ1 ... λm}, is defined as. As is now standard for feature-based grammars, we use log-linear models for parse selection (Johnson et al, 1999). As is now standard for feature-based grammars, we mainly use log-linear models for parse selection (Johnson et al, 1999).  The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Riezler et al (2000), esp. since they use the same evaluation criteria than we use. L(w) is the 'conditionalized' likelihood of the training data X p (Johnson et al, 1999), computed as L (w)= lQ Ni=1 p w (r i js i). When statistically modelling linguistic phenomena of one sort or another, researchers typically train log linear models to the data (for example (Johnson et al., 1999)).  The results we report are with the Gaussian prior regularization term described in (Johnson et al, 1999). As in Johnson et al (1999) we trained the model by maximizing the conditional likelihood of the preferred analyses and using a Gaussian prior for smoothing (Chen and Rosenfeld, 1999). This disambiguation decision seems to require common world knowledge or it might be addressable with addition of knowledge about parallel structures ((Johnson et al, 1999) add features measuring parallelism).
Speed And Accuracy In Shallow And Deep Stochastic Parsing This paper reports some experiments that compare the accuracy and performance of two stochastic parsing systems. The currently popular Collins parser is a shallow parser whose output contains more detailed semanticallyrelevant information than other such parsers. The XLE parser is a deep-parsing system that couples a Lexical Functional Grammar to a loglinear disambiguation component and provides much richer representations theory. We measured the accuracy of both systems against a gold standard of the PARC 700 dependency bank, and also measured their processing times. We found the deep-parsing system to be more accurate than the Collins parser with only a reduction in parsing Accuracy for XLE is not given, because the results reported by Kaplan et al (2004) compare labeled functional dependencies drawn from LFG f-structures with equivalents derived automatically from Collins outputs. For estimation and best-parse searching, efficient dynamic programming techniques over features forests are employed (see Kaplan et al (2004)). Kaplan et al (2004) report high parsing speeds for a deep parsing system which uses an LFG grammar: 1.9 sentences per second for 560 sentences from section 23 of the Penn Treebank. We have developed a parsing system that explores this space, in the vein of systems like (Kaplan et al, 2004). We use the same 560 sentence subset from the DepBank utilised by Kaplan et al (2004) in their study of parser accuracy and efficiency. Kaplan et al (2004) compare the Collins (2003) parser with the Parc LFG parser by mapping LFG F structures and Penn Treebank parses into DepBank dependencies, claiming that the LFG parser is considerably more accurate with only a slight reduction in speed. Kaplan et al (2004) clearly invested considerable time and expertise in mapping the output of the Collins parser into the DepBank dependencies, but they also note that 'This conversion was relatively straightforward for LFG structures. In the case of Kaplan et al (2004), the testing procedure would include running their con version process on Section 23 of the Penn Treebank and evaluating the output against DepBank. Note that statistical parsers can equally suffer from this problem, see e.g. (Kaplan et al., 2004). they can be used to disprefer (actually ignore) rarely-applicable rules, in order to reduce parse time (Kaplan et al 2004). The second extrapolation is to the LFG XLE parser (Kaplan et al 2004) for English, consisting of a highly developed symbolic parser and grammar, an OT-based preference component, and a stochastic back end to select among remaining alternative parser outputs. The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al (2004). The disadvantage of such parsers is that they are typically not very efficient, parsing a few sentences per second on commodity hardware (Kaplan et al, 2004). Currently our best automatically induced grammars achieve 80.97% f-score for f structures parsing section 23 of the WSJ part of the Penn-II tree bank and evaluating against the DCU1051 and 80.24% against the PARC 700 Dependency Bank (King et al, 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al, 2004).  We achieve between 77.68% and 80.24% against the PARC 700 following the experiments in (Kaplan et al, 2004). Against the PARC 700, the hand-crafted LFG grammar reported in (Kaplan et al, 2004) achieves an f score of 79.6%. Evaluating against the PARC 700 Dependency Bank, the P-PCFG achieves an f-score of 80.24%, an overall improvement of approximately 0.6% on the result reported for the best hand-crafted grammars in (Kaplan et al, 2004). Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The data is divided into two sets, a 140-sentence development set and a test set of 560 sentences (Kaplan et al, 2004).
Precision And Recall Of Machine Translation  Another example of a loss function in this class is the MTeval metric introduced in Melamed et al (2003). We used the following n-gram-based metrics: BLEU (Papineni et al, 2002), NIST (Dod ding ton, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al, 2003), METEOR (Banerjee and Lavie, 2005). GTM General Text Matching (Melamed et al, 2003) calculates word overlap between a reference and a solution, without double counting duplicate words. Table 2 reports the translation performance as measured by BLEU (Papineni et al,2002), GTM (Melamed et al, 2003) and ME TEOR2 (Banerjee and Lavie, 2005) for Apertium and the three systems presented in the previous section, as well as the size of the phrase table and the amount of unknown words in the test set. Melamed et al (2003) used unigram F-measure to estimate machine translation quality and showed that unigram F-measure was as good as BLEU. We have selected a representative set of 22 metric variants corresponding to six different families: BLEU (Papineni et al, 2001), NIST (Dodding ton, 2002), GTM (Melamed et al, 2003), Per (Leusch et al, 2003) , WER (NieBen et al, 2000) and ROUGE (Lin and Och, 2004a). Third, we computed the correspondence between each hypothesis sentence and each of its corresponding reference sentences using an approximation to maximum matching (Melamed et al, 2003). Other well-known metrics are WER (NieBen et al, 2000), NIST (Doddington, 2002), GTM (Melamed et al, 2003), ROUGE (Lin and Och, 2004a), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al, 2006), just to name a few. We have computed the BLEU score (accumulated up to 4-grams) (Papineni et al, 2001), the NIST score (accumulated up to 5-grams) (Doddington, 2002), the General Text Matching (GTM) F-measure (e= 1, 2) (Melamed et al, 2003), and the METEOR measure (Banerjee and Lavie, 2005). Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. Melamed et al (2003) argued, at the time of introducing the GTM metric, that Pearson correlation coefficients can be affected by scale properties, and suggested, in order to avoid this effect, to use the non-parametric Spearman correlation coefficients instead. Melamed et al (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Meteor (Banerjee and Lavie, 2005), Precision and Recall (Melamed et al, 2003), and other such automatic metrics may also be affected to a greater or lesser degree because they are all quite rough measures of translation similarity, and have inexact models of allowable variation in translation. For evaluation we have selected a set of 8 metric variants corresponding to seven different families: BLEU (n= 4) (Papineni et al, 2001), NIST (n= 5) (Lin and Hovy, 2002), GTM F1-measure (e= 1, 2) (Melamed et al, 2003), 1-WER (NieBen et al, 2000), 1-PER (Leusch et al, 2003), ROUGE (ROUGE-S*) (Lin and Och, 2004) and METEOR3 (Banerjee and Lavie, 2005). Melamed et al (2003) argued, at the time of introducing the GTM metric, that Pearson correlation coefficients can be affected by scale properties. each of its corresponding reference sentences using an approximation to maximum matching (Melamed et al, 2003). At this time, the realizations are also scored using the General Text Matcher method (GTM) (Melamed et al, 2003), by comparing them to the original sentence. Melamed et al (2003) used unigram F-measure to estimate machine translation quality and showed that unigram F measure was as good as BLEU. Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. Metrics in the Rouge family allow for skip n-grams (Lin and Och,2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used.
Experiments with a Higher-Order Projective Dependency Parser We present experiments with a dependency parsing model defined on rich factors. Ourmodel represents dependency trees with factors that include three types of relations be tween the tokens of a dependency and theirchildren. We extend the projective pars ing algorithm of Eisner (1996) for our case,and train models using the averaged perceptron. Our experiments show that considering higher-order information yields signifi cant improvements in parsing accuracy, but comes at a high cost in terms of both timeand memory consumption. In the multi lingual exercise of the CoNLL-2007 shared task (Nivre et al, 2007), our system obtains the best accuracy for English, and the second best accuracies for Basque and Czech. Carreras (2007) extends the first-order model to incorporate a sum over scores for pairs of adjacent arcs in the tree, yielding a second-order model. Carreras (2007) employs his own extension of Eisner's algorithm for the case of projective trees and second-order models that include head grandparent relations.    To reap the benefits of these advances, we use a higher-order projective dependency parsing algorithm (Carreras, 2007) which is an extension of the span-based parsing algorithm (Eisner, 1996), for syntactic dependency parsing. For more details of the second-order parsing algorithm, see (Carreras, 2007). We employ, as a basis for our parser, the second order maximum spanning tree dependency parsing algorithm of Carreras (2007). The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent, and the an edge to a grandchild. Johansson and Nugues (2008) reported training times of 2.4 days for English with the high-order parsing algorithm of Carreras (2007). It consists of the second order parsing algorithm of Carreras (2007), the non-projective approximation algorithm (McDonald and Pereira, 2006), the passive aggressive support vector machine, and a feature extraction component. The second order algorithm of Carreras (2007) uses in addition to McDonaldand Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent as well as the edge from the dependents to a grandchild. The dependency parser is based on Carreras's algorithm (Carreras, 2007) and second order spanning trees.  Carreras (2007) introduced the left-most and right-most grandchild as factors. We use the factor model of Carreras (2007) as starting point for our experiments, cf. Section 4. We extend Carreras (2007) graph based model with factors involving three edges similar to that of Koo and Collins (2010). We start with the model introduced by Carreras (2007). Second order factors of Carreras (2007). 
Role of Verbs in Document Analysis representations of context for the detection and corof malapropisms. An electronic lexical database and some of its applications. Verbs are very important sources of knowledge in many language engineering tasks, and the relationships among verbs appear to play a major role in the organization and use of this knowledge: Knowledge about verb classes is crucial for lexical acquisition in support of language generation and machine translation (Dorr, 1997), and document classification (Klavans and Kan, 1998). Levin-inspired classes have been used in several NLP tasks, such as Machine Translation (Dorr, 1997) and Document Classification (Klavans and Kan, 1998). The problems of non-nominal terms (Klavans and Kan, 1998), term variation (Jacquemin et al, 1997), and relevant contexts (Maynard and Ananiadou, 1998), can be considered for improving the performance. Verb classifications have, in fact, been used to support many natural language processing (NLP) tasks, such as language generation, machine translation (Dorr, 1997), document classification (Klavans and Kan, 1998), word sense disambiguation (Dorr and Jones, 1996) and sub categorization acquisition (Korhonen, 2002). Knowledge about verb classes is crucial for lexical acquisition in support of language generation and machine translation (Dolt, 1997) and document classification (Klavans and Kan, 1998), yet manual classification of large numbers of verbs is a difficult and resource intensive task (Levm, 1993 Miller et al , 1990, Dang et al, 1998). Out of the large syntactic constituents of a sentence, e.g. noun phrases, verb phrases, and prepositional phrases, we assume that noun phrases (NPs) carry the most contentful information about the document, even if sometimes the verbs are important too, as reported in the work by [Klavans and Kan (1998)]. In document classification, Klavans and Kan (1998) demonstrate that document type is correlated with the presence of many verbs of a certain EVCA class (Levin 1993).
A Linear Programming Formulation For Global Inference In Natural Language Tasks Given a collection of discrete random variables representing outcomes of learned local predictors in natural language, e.g., named entities and relations, we seek an optimal global assignment to the variables in the presence of general (non-sequential) constraints. Examples of these constraints include the type of arguments a relation can take, and the mutual activity of different relations, etc. We develop a linear programming formulation for this problem and evaluate it in the context of simultaneously learning named entities and relations. Our approach allows us to efficiently incorporate domain and task specific constraints at decision time, resulting in significant improvements in the accuracy and the “human-like” quality of the inferences. A similar method was used for entity/relation recognition (Roth and Yih, 2004). Supervised methods such as (Culotta and Sorensen, 2004) and (Roth and Yih, 2004) provide only a partial solution, as there are many possible relations and entities of interest for a given domain, and such approaches require new annotated data each time a new relation or entity type is needed. Our model for disentanglement fits into the general class of graph partitioning algorithms (Roth and Yih, 2004) which have been used for a variety of tasks inNLP, including the related task of meeting segmentation (Malioutov and Barzilay, 2006). Roth and Yih (2004) use log probabilities as weights. Our problem formulation and use of ILP are based on both (Roth and Yih, 2004) and (Barzilay and Lapata, 2006). Also, we minimize rather than maximize due to the fact we transform the model probabilities with? log (like Roth and Yih (2004)). Roth and Yih (2004) use ILP to deal with the joint inference problem of named entity and relation identification. We phrase the inference task as an integer linear program (ILP) following the approach developed in Roth and Yih (2004). The approach we develop in this paper follows the one proposed by Roth and Yih (2004) of training individual models and combining them at inference time. Roth and Yih (2004) formulated the problem of extracting entities and relations as an integer linear program, allowing them to use global structural constraints at inference time even though the component classifiers were trained independently. Roth and Yih (2004) combined information from named entities and semantic relation tagging, adopting a similar overall goal but using a quite different approach based on linear programming. 804 task, for which Integer Linear Programming (ILP) introduced to NLP by Roth and Yih (2004) and successfully applied by Denis and Baldridge (2007 ) to the task of jointly inferring anaphoricity and determining the antecedent would be appropriate. Roth and Yih (2004) advocated ILP as a general solution for a number of NLP tasks that re quire combining multiple classifiers and which the traditional pipeline architecture is not appropriate, such as entity disambiguation and relation extraction. Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata,2006), and agreement constraints between word alignment directions (Ganchev et al, 2008) or various parsing models (Koo et al, 2010). We borrow the data and the setting from (Roth and Yih, 2004). Refer to (Roth and Yih, 2004) for more statistics on this data and a list of all the type constraints used. Integer linear programs have already been successfully used in related fields including semantic role labelling (Punyakanok et al, 2004), relation and entity classification (Roth and Yih, 2004), sentence compression (Clarke and Lapata, 2008) and dependency parsing (Martins et al, 2009). Recently, [Roth and Yih, 2004] applied an ILP model to the task of the simultaneous assignment of semantic roles to the entities mentioned in a sentence and recognition of the relations holding between them. Roth and Yih (2004) also described a classification-based framework in which they jointly learn to identify named entities and relations. (Roth and Yih, 2004) suggests a model in which global constraints are taken into account in a later stage to fix mistakes due to the pipeline.
Sentence Level Discourse Parsing Using Syntactic And Lexical Information We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees. The models use syntactic and lexical features. A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-ofthe-art decision-based discourse parser. A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance. SEE allowed the judges to step through predefined units of the model summary (elementary discourse units/EDUs) (Soricut and Marcu, 2003) and for each unit of that summary, mark the sentences in the peer summary that expressed [all (4), most (3), some (2), hardly any (1) or none (0)] of the content in the current model summary unit. Texts were segmented into clauses using SPADE (Soricut and Marcu, 2003) with some heuristic post-processing. Our model was trained and tested on RST-DT (2002) and achieves a performance of up to 86.12% F-Score, which is comparable to Soricut and Marcu (2003). Most of the current work on discourse processing focuses on sentence-level text organization (Soricut and Marcu, 2003). Since segmentation is the first stage of discourse parsing, quality discourse segments are critical to building quality discourse representations (Soricut and Marcu, 2003). Soricut and Marcu (2003) construct a statistical discourse segmenter as part of their sentence-level discourse parser (SPADE), the only implementation available for our comparison. Soricut and Marcu (2003) use syntactic features to identify sentence-internal RST structure. The test set includes only sentences for which our English parser (Soricut and Marcu, 2003) could produce a parse tree, which effectively excluded a few very long sentences. One exception is Marcu's work (Marcu, 1997, 1999) (see also Soricut and Marcu (2003) for constructing discourse structures for individual sentences). Within Rhetorical Structure Theory (RST), Soricut and Marcu (2003) have developed two probabilistic models for identifying clausal elementary discourse units and generating discourse trees at the sentence level. Most of the current work on discourse processing focuses on sentence-level text organization (Soricut and Marcu, 2003). their relation edges are obtained from the Spade system described in Soricut and Marcu (2003). Soricut and Marcu (2003) also build up RST sentential trees to use in discourse parsing. Though statistical methods have been used to induce such trees (Soricut and Marcu, 2003), they are not used for ordering and other text-structuring tasks. (Soricut and Marcu, 2003) and (Polanyi et al., 2004) implement models to perform discourse parsing. A discourse tree (Soricut and Marcu, 2003). Soricut and Marcu (2003) introduce a statistical discourse segmenter, which is trained on RST DT to label words with boundary or no-boundary labels. Like Soricut and Marcu (2003), they formulate the discourse segmentation task as a binary classification problem of deciding whether a word is the boundary or no-boundary of EDUs. Soricut and Marcu (2003) and Subba and Di Eugenio (2007) use boundary labels, which are assigned to words at the end of EDUs. SPADE is the work of Soricut and Marcu (2003).
Detection of Grammatical Errors Involving Prepositions This paper presents ongoing work on the detection of preposition errors of non-native speakers of English. Since prepositions account for a substantial proportion of all grammatical errors by ESL (English as a Second Language) learners, developing an NLP application that can reliably detect these types of errors will provide an invaluable learning resource to ESL students. To address this problem, we use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays. Although our work is preliminary, we achieve a precision of 0.8 with a recall of 0.3. More recently, Han et al (2004, 2006) use a maximum entropy classifier to propose article corrections in TESOL essays, while Izumi et al (2003) and Chodorow et al (2007) present techniques of automatic preposition choice modeling. Chodorow et al (2007) present numbers on an independently developed system for detection of preposition error in non-native English. For a first human evaluation of our system prototype, we decided to Chodorow et al (2007) evaluate their system on. Chodorow et al (2007) employed a maximum entropy model to estimate the probability of 34 prepositions based on 25 local context features ranging from words to NP/VP chunks. Studies that focus on providing automatic correction, however, mainly deal with errors that derive from closed-class words, such as articles (Han et al, 2004) and prepositions (Chodorow et al., 2007). (Chodorow et al, 2007) present a system for detecting errors in English prepositions using machine learning. Chodorow and Leacock (2000) and Chodorow et al (2007) argue that precision-oriented is better, but they do not give any concrete reason. Chodorow et al (2007) instead treat it as a classification problem and employed a maximum entropy classifier. The work of Chodorow et al (2007) and T & C 08 treat the tasks of preposition selection and error detection as a classification problem. A context is represented by 25 lexical features and 4 combination features: Lexical Token and POS n-grams in a 2 word window around the preposition, plus the head verb in the preceding verb phrase (PV), the head noun in the preceding noun phrase (PN) and the head noun in the following noun phrase (FN) when available (Chodorow et al, 2007). Research on automatic grammar correction has been conducted on a number of different parts-of speech, such as articles (Knight and Chander, 1994) and prepositions (Chodorow et al, 2007). Automatic error detection has been performed on other parts-of-speech, e.g., articles (Knight and Chander, 1994) and prepositions (Chodorow et al,2007). We extend our previous work (Chodorow et al., 2007) by experimenting with combination features, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al, 1994). Second, we discuss drawbacks in current methods of annotating ESL data and evaluating error detection systems, which are not limited to preposition errors. The baseline system (described in (Chodorow et al, 2007)) performed at 79.8% precision and 11.7% recall. While this improvement may seem small, it is in part due to the difficulty of the problem, but also the high baseline system score that was established in our prior work (Chodorow et al., 2007). For instance, in our previous work (Chodorow et al, 2007), we found that when our system's output was compared to judgments of two different raters, there was a 10% difference in precision and a 5% difference in recall. Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (Gamon et al 2008, 2010, DeFelice and Pulman 2007, 2008, Han et al 2006, Chodorow et al 2007, Tetreault and Chodorow 2008). Similarly, web-based models built on Google Web1T 5-gram Corpus (Bergsma et al., 2009) achieve better results when compared to a maximum entropy model that uses a corpus 10,000 times smaller (Chodorow et al, 2007). Chodorow et al (2007) present an approach to preposition error detection which also uses a model based on a maximum entropy classifier trained on a set of contextual features, together with a rule-based filter. Preposition errors are common among new English speakers (Chodorow et al, 2007).
Categorial Unification Grammars Categorial unification grammars (CUGs) embody the essential properties of both unification and categorial grammar formalisms. Their efficient and uniform way of encoding linguistic knowledge in well-understood and widely used representations makes them attractive for computational applications and for linguistic research. In this paper, the basic concepts of CUGs and simple examples of their application will be presented. It will be argued that the strategies and potentials of CUGs justify their further exploration i the wider context of research on unification grammars. Approaches to selected linguistic phenomena such as long-distance dependencies, adjuncts, word order, and extraposition are discussed. In this paper we present an algorithm for generating sentences using unification categorial grammars (UCGs, Zeevat et al 1987) but which extends to any categorial grammar with unification (e.g. categorial unification grammars, Uszkoreit 1986, Karttunen 1987). The lexical types are organised into an inheritance hierarchy, constrained by expressions of a simple feature based category description language, inspired by previous attempts to integrate categorial grammars and unification-based grammars, e.g. Uszkoreit (1986) and Zeevat et al (1987). Unificationbased versions of Categorial Grammar, known as CUG or UCG, have attracted considerable attention recently (see, for instance, Uszkoreit, 1986, Karttunen, 1986, Bouma, 1988, Bouma et al, 1988, and Calder et al, 1988). Higher order versions of categorial grammars like the ones being produced in the CUG/UCG-frameworks. The family of grammar models that are based on such formalisms include Generalized Phrase Structure Grammar (GPSG) [Gazdar et al 1985], Lexical Functional Grammar (LFG) [Bresnan 1982], Functional Unification Grammar (bUG) [Kay 1984], Head-Driven Phrase Structure Grammar (I-IPSG) [Pollard and Sag 1988], and Categorial Unification Grammar (CUG) [Karttunen 1986, Uszkoreit 1986, Zeevat et al 1987]. Use of unification (a core operation in HPSG) in CG dates at least as far back as Karttunen (1986, 1989), Uszkoreit (1986), and Zeevat (1988).
A Language Modeling Approach To Predicting Reading Difficulty We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling. We derive a measure based on an extension of multinomial naïve Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage. The resulting classifier is not specific to any particular subject and can be trained with relatively little labeled data. We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures. We show that with minimal changes, the classifier may be retrained for use with French Web documents. For both English and French, the classifier maintains consistently good correlation with labeled grade level (0.63 to 0.79) across all test sets. Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words). Si and Callan (2001) and Collins-Thompson and Callan (2004) have demonstrated the use of language models is more robust for web documents and passages.  In order to adjust search result presentation to the user's reading ability, we estimate the reading difficulty of each retrieved document using the Smoothed Unigram Model, a variation of a Multinomial Bayes classifier (Collins-Thompson and Callan, 2004). The latter has been found to be more effective as the former when approaching the reading level of subjects in primary and secondary school age (Collins-Thompson and Callan, 2004). Collins-Thompson and Callan (2004) adopted a similar approach and used a smoothed unigram model to predict the grade levels of short passages and web documents. First, a language modeling approach generally gives much better accuracy for Web documents and short passages (Collins-Thompson and Callan, 2004). However, there are other important criteria for the user besides relevance, such as readability (Collins-Thompson and Callan, 2004), novelty (Harman, 2003), and authority (Kleinberg, 1998).  Advanced NLP-based readability metrics developed so far typically deal with English, with a few attempts devoted to other languages, namely French (Collins-Thompson and Callan, 2004), Portuguese (Aluisio et al, 2010) and German (Bruck, 2008). Collins-Thompson and Callan (2004) used a smoothed unigram language model to predict the grade reading levels of web page documents and short passages. As a learning model, we use unigram language modelling introduced in (Collins-Thompson and Callan, 2004) to model the reading level of subjects in primary and secondary school. In some of the early works on statistical readability assessment, Si and Callan (2001) and Collins-Thompson and Callan (2004) reported the impact of using unigram language models to estimate the grade level of a given text. The widely used Flesch-Kincaid Grade Level index is based on the average number of syllables per word and the average sentence length in a passage of text (Kincaid et al, 1975) (as cited in (Collins-Thompson and Callan, 2004)). classifier to better capture the variance in word usage across grade levels (Collins-Thompson and Callan, 2004). Other related work has used models of vocabulary (Collins-Thompson and Callan, 2004). Early work on automatic readability analysis framed the problem as a classification task: creating multiple classifiers for labeling a text as being one of several elementary school grade levels (Collins-Thompson and Callan, 2004). It is also the last formula published for French L1, if we except the adaptation of the model by Collins-Thompson and Callan (2004) to French.
Word Sense Discrimination By Clustering Contexts In Vector And Similarity Spaces This paper systematically compares unsupervised word sense discrimination techniques that cluster instances of a target word that occur in raw text using both vector and similarity spaces. The context of each instance is represented as a vector in a high dimensional feature space. Discrimination is achieved by clustering these context vectors directly in vector space and also by finding pairwise similarities among the vectors and then clustering in similarity space. We employ two different representations of the context in which a target word occurs. First order context vectors represent the context of each instance of a target word as a vector of features that occur in that context. Second order context vectors are an indirect representation of the context based on the average of vectors that represent the words that occur in the context. We evaluate the discriminated clusters by carrying out experiments ussense–tagged instances of 24 words and the well known corpora.  The first one to propose this idea of context-group discrimination was Schutze (1998), and many researchers followed a similar approach to sense induction (Purandare and Pedersen, 2004). Common operations include altering feature weights and dimensionality reduce Document-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al, 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al, 2009) Approximation Models Random Indexing (Sahlgren et al, 2008) Reflective Random Indexing (Cohen et al, 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al, 2006) Incremental Semantic Analysis (Baroni et al, 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010). (Purandare and Pedersen, 2004), use clustering to discover the different meanings of a word in a corpus. This finding coincides to that of Purandare and Pedersen (2004) and Pedersen (2010) who found that with large amounts of data, first-order vectors perform better than second-order vectors, but second-order vectors are a good option when large amounts of data are not available. A detailed description can be found in Purandare and Pedersen (2004). The method is described in Purandare and Pedersen (2004). Purandare and Pedersen (2004) report that this method generally performed better where there was a reasonably large amount of data available (i.e., several thousand contexts).  Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data. Schutze's (1998) approach has been implemented in the SenseClusters program (Purandare and Pedersen, 2004), which also incorporates some interesting variations on and extensions to the original algorithm. First, Purandare and Pedersen (2004) defend the use of bigram features instead of simple word features. (Purandare and Pedersen,2004 ,p.2) and will be used throughout this paper. Purandare and Pedersen (2004) used a log likelihood test to select their features, probably because of the intuition that candidate words whose occurrence depends on whether the ambiguous word occurs will be indicative of one of the senses of the ambiguous word and hence useful for disambiguation (Schutze, 1998 ,p.102). In fact, results on word sense discrimination (Purandare and Pedersen, 2004) suggest that first order representations are more effective with larger number of context than second order methods. Pedersen et al.(2006) and Purandare and Pedersen (2004 ) integrate second-order co-occurrence of words into the similarity function. These hubs are used as a representation of the senses induced by the system, the same way that clusters of examples are used to represent senses in clustering approaches to WSD (Purandare and Pedersen, 2004). Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data and is not included in the table. Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch ?utze, 1998), and the Information Bottleneck (Niu et al., 2007).
Applied Text Generation  Following [Rambow and Korelsky, 1992], we call this additional planning task sentence planning (even though some operations may cross sentence boundaries). Tasks in the generation process have been divided into three stages (Rambow and Korelsky,1992): the text planner has access only to information about communicative goals, the dis course context, and semantics, and generates a non-linguistic representation of text structure and content. This paper addresses the area of text generation known as micro planning [Levelt 1989, Panaget 1994, Huang and Fiedler 1996], or sentence planning [Rambow and Korelsky 1992]. The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS (Iordanskaja et al, 1992), and JOYCE (Rambow and Korelsky, 1992). The second approach is natural language generation (NLG), which customarily divides the generation process into three modules (Rambow and Korelsky, 1992): (1) Text Planning, (2) Sentence Planning, and (3) Surface Realization.
Automatic Identification Of Word Translations From Unrelated English And German Corpora Algorithms for the alignment of words in translated texts are well established. However, only recently new approaches have been proposed to identify word translations from non-parallel or even unrelated texts. This task is more difficult, because most statistical clues useful in the processing of parallel texts cannot be applied to non-parallel texts. Whereas for parallel texts in some studies up to 99% of the word alignments have been shown to be correct, the accuracy for non-parallel texts has been around 30% up to now. The current study, which is based on the assumption that there is a correlation between the patterns of word co-occurrences in corpora of different languages, makes a significant improvement to about 72% of word translations identified correctly. On the other hand, the alternative approach using comparable or unrelated text corpora were studied by Rapp (1999) and Fung et al (1998). For instance, good results are obtained from large corpora several million words for which the accuracy of the proposed translation is between 76% (Fung, 1998) and 89% (Rapp, 1999) for the first 20 candidates. This can be accomplished as in Rapp (1999) and Schafer and Yarowsky (2002) by creating bag-of-words context vectors around both the source and target language words and then projecting the source vectors into the (English) target space via the current small translation dictionary. Here, a standard technique of estimating bilingual term correspondences from com parable corpora (e.g., Fung and Yee (1998) and Rapp (1999)) is employed. For example, Rapp (1999) filtered out bilingual term pairs with low monolingual frequencies (those below 100 times), while Fung and Yee (1998) restricted candidate bilingual term pairs to be pairs of the most frequent 118 unknown words. The approach we investigate for identifying term translations in comparable corpora is similar to (Rapp, 1999) and many others. Complex linguistic tools such as terminological extractors (Daille and Morin,2005), parsers (Yu and Tsujii, 2009) or lemma tizers (Rapp, 1999) are sometimes used. Context length can be based on a number of units, for instance 3 sentences (Daille and Morin, 2005), windows of 3 (Rapp, 1999) or 25 words (Prochasson et al, 2009), etc. As already noted, most authors use the log-likelihood ratio to measure the association between collocates; some, like (Rapp, 1999), informally compare the performance of a small number of association measures, or combine the results obtained with different association measures (Daille and Morin, 2005). Expand the dictionary of step 3 using comparable corpora as proposed in a study by Rapp (1999). From a previous pilot study (Rapp, 1999) it can be expected that this methodology achieves an accuracy in the order of 70%, which means that only a relatively modest amount of manual post editing is required. (Rapp, 1999) and (Koehn and Knight, 2002) extract new word translations from non-parallel corpus.  To identify the context terms CT (WS) of a source word WS, as in (Rapp, 1999), we use log likelihood ratio (LL) Dunning (1993). Using large, unrelated English and German corpora (with 163m and 135mwords) and a small German-English bilingual dictionary (with 22k entires), Rapp (1999) demonstrated that reasonably accurate translations could be learned for 100 German nouns that were not contained in the seed bilingual dictionary. We extend the vector space approach of Rapp (1999) to compute similarity between phrases in the source and target languages.  One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional co-locational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). Some successful combinations are cos CP (Schuetze and Pedersen, 1997), Lin PMI (Lin, 1998), City LL (Rapp, 1999), and Jensen Shannon divergence of conditional probabilities (JSD CP). The counts can be collected in positional (Rapp, 1999) or non-positional way (count all the word occurrences within the sliding window).
Extracting Parallel Sub-Sentential Fragments From Non-Parallel Corpora We present a novel method for extracting parallel sub-sentential fragments from comparable, non-parallel bilingual corpora. By analyzing potentially similar sentence pairs using a signal processinginspired approach, we detect which segments of the source sentence are translated into segments in the target sentence, and which are not. This method enables us to extract useful machine translation training data even from very non-parallel corpora, which contain no parallel sentence pairs. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system.  We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GI ZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. Munteanu and Marcu (2006) first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. They are neither parallel nor comparable because we cannot even extract a small number of parallel sentence pairs from this monolingual data using the method of (Munteanu and Marcu, 2006). Munteanu and Marcu (2006) proposed a method for extracting parallel sub sentential fragments from very non-parallel bilingual corpora. Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or sub sentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora.  Similarly, Munteanu and Marcu (2006) propose a method to extract sub sentential fragments from non-parallel corpora.  Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or sub sentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. The approach that is closest to our work is that of Munteanu and Marcu (2006): They use standard information retrieval together with simple word-based translation for CLIR, and extract phrases from the retrieval results using a clean bilingual lexicon and an averaging filter. Our first technique resembles the technique of Munteanu and Marcu (2006) who also perform phrase extraction by combining clean alignment lexica for initial signals with heuristics to smooth alignments for final fragment extraction. The first attempt to detect sub-sentential fragments from comparable sentences is (Munteanuand Marcu, 2006).  Munteanu and Marcu (2006) extract sub sentential translation pairs from comparable corpora using the log-likelihood-ratio of word translation probability. We mainly follow our previous approach (Wang and CallisonBurch, 2011), which is a modified version of an approach by Munteanu and Marcu (2006) on translation fragment extraction. It also can be quantified as the rate of successful extraction of translation equivalents by automated tools, such as proposed in Munteanu and Marcu (2006). At last, the goal that we aim to exploit monolingual corpora to help MT is in-spirit similar to the goal of using non-parallel corpora to help MT as aimed in a large amount of work (see Munteanu and Marcu (2006) and references therein).
Web-Scale Distributional Similarity and Entity Set Expansion Computing the pairwise semantic similarity between all words on the Web is a computationally challenging task. Parallelization and optimizations are necessary. We propose a highly scalable implementation based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web. The pairwise similarity between 500 million terms is computed in 50 hours using 200 quad-core nodes. We apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size, corpus quality, seed composition and seed size. We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia. Full algorithmic details are presented in (Pantel et al, 2009).  KE dis alone, a state-of-the-art distributional system implementing (Pantel et al, 2009), where the Ranker assigns scores to instances using the similarity score returned by KE dis alone. Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion (Pantel et al,2009), part-of-speech (POS) tagging and chunking (Huang and Yates, 2009), ontology learning (Curran, 2005), computing semantic textual similarity (Besanc? on et al, 1999), and lexical inference (Kotlerman et al, 2012). To score relevant words not appearing in the database (due to incompleteness of the database or lexical variations), GUSP uses DASH (Pantel et al, 2009) to provide additional word-pair scoring based on lexical distributional similarity computed over general text corpora (Wikipedia in this case). Since case information is important for parsers and taggers, we first true cased the sentences using DASH (Pantel et al, 2009), which stores the case for each phrase in Wikipedia. Pantel et al (2009) discusses the issue of seed set size in detail, concluding that 5-20 seed words are often required for good performance. Recently, (Vyaset al, 2009) proposed an automatic system for improving the seeds generated by editors (Pantel et al, 2009). As mentioned above, (Etzioni et al, 2005) report that seed set composition affects the correctness of the harvested instances, and (Pantel et al, 2009) observe an increment of 42% precision and 39% recall between the best and worst performing seed sets for the task of entity set expansion. According to (Pantel et al, 2009) 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found. We computed the distributional similarity between arguments using (Pantel et al, 2009) over a large crawl of the Web (described in Section 4.1). Our last feature is the distributional similarity scores of Pantel et al (2009), as trained over Wikipedia. Because human evaluation of word similarities is very difficult and costly, we conducted automatic evaluation in the set expansion setting, following previous studies such as Pantel et al (2009). To obtain examples of multiple semantic categories, we utilized selected Wikipedia listOf pages from (Pantel et al, 2009) and augmented these with our own manually defined categories, such that each list contained at least ten distinct examples occurring in our corpus. The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al, 2009). Some prior studies use every word in a document/sentence as the features, such as the distributional approaches (Pantel et al, 2009). CL-Web: A state-of-the-art open domain method based on features extracted from the Web documents data set (Pantel et al, 2009). As (Pantel et al, 2009) show, picking seeds that yield high numbers of different terms is difficult. Given the seeds set S, a seeds centroid vector is produced using the surrounding word contexts (see below) of all occurrences of all the seeds in the corpus (Pantel et al 2009). This combination was also used in (Pantel et al, 2009).
Improving Machine Learning Approaches To Coreference Resolution We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC- 6 and MUC-7 coreference resolution data sets — F-measures of 70.4 and 63.4, respectively. Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge. See Ng and Cardie (2002) for a detailed description of the features. We now show that the search problem in (2) can equivalently be solved by the more intuitive best first decoder (Ng and Cardie, 2002), rather than using the CLE decoder. The use of latent antecedents goes back to the work of Yu and Joachims (2009), although the idea of determining meaningful antecedents for mentions can be tracedback to Ng and Cardie (2002) who used a rule based approach. One way to utilize the semantic compatibility is to take it as a feature under the single-candidate learning model as employed by Ng and Cardie (2002). In the testing phase, we used the best-first clustering as in Ng and Cardie (2002). We test on the ANC Test set (1291 instances) also used in Bergsma (2005) (highest resolution accuracy reported: 73.3%), the anaphora labelled portion of AQUAINT used in Cherry and Bergsma (2005) (1078 instances, highest accuracy: 71.4%), and the anaphoric pronoun subset of the MUC7 (1997) coreference evaluation formal test set (169 instances, highest precision of 62.1 reported on all pronouns in (Ng and Cardie, 2002)). Ng and Cardie (2002) expanded the feature set of Soon et al (2001) from 12 to 53 features. Ng and Cardie (2002) split this feature into several primitive features, depending on the type of noun phrases. Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. Although there is empirical evidence (e.g. Ng and Cardie 2002a, 2004) that coreference resolution might be further improved with proper anaphoricity information, its contribution is still somewhat disappointing and lacks systematic evaluation. Ng and Cardie (2002a) employed various domain-independent features in identifying anaphoric NPs and showed how such information can be incorporated into a coreference resolution system. In contrast to Ng (2005), Ng and Cardie (2002a) proposed a rule-induction system with rule pruning. It is the same as most prior work in the literature, including Soon et al (2001) and Ng and Cardie (2002b). In the literature, besides the training instance extraction methods proposed by Soon et al (2001) and Ng and Cardie (2002b) as discussed in Section 2, McCarthy and Lehnert (1995) used all possible pairs of training instances. Plenty of machine learning algorithms such as Decision tree (Ng and Cardie, 2002), maximum entropy model, logistic regression (Bjorkelund and Nugues, 2011), Support Vector Machines, have been used to solve this problem. At first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g. Ng and Cardie (2002), Morton (2000)). Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al (2003), McCallum and Wellner (2003)). Our general approach to source coreference resolution is inspired by the state-of-the-art performance of one such approach to coreference resolution, which relies on a rule learner and single-link clustering as described in Ng and Cardie (2002). We use the features introduced by Ng and Cardie (2002) for the task of coreference resolution. We develop a novel method for partially supervised clustering, which is motivated by the success of a rule learner (RIPPER) for coreference resolution (Ng and Cardie, 2002).
Minimum Bayes-Risk Decoding For Statistical Machine Translation We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation. This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance. We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences. We report the performance of the MBR decoders on a Chinese-to-English translation task. Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions. Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc. This solution is often referred to as the Minimum Bayes Risk (MBR) solution (Kumar and Byrne,2004). This list is then rescored using Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004). The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system 's translations relative to the model 's distribution over possible translations (Kumar and Byrne, 2004). Consensus decoding procedures select translations for a single system with a minimum Bayes risk (MBR) (Kumar and Byrne, 2004). In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an N best list of translations produced by a first pass decoder (Kumar and Byrne, 2004). For each system, we report the performance of max-derivation decoding (MAX) and 1000-best3 MBR decoding (Kumar and Byrne, 2004). Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al, 2002). We experimented with two decoding settings: (1) monotone at punctuation reordering (Tillmannand Ney, 2003), and (2) minimum Bayes risk decoding (Kumar and Byrne, 2004). Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004). We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk reranking (Kumar and Byrne, 2004). Modifying the multitask objective to incorporate application-specific loss/decoding, such as Minimum Bayes Risk (Kumar and Byrne, 2004). This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al, 2000). To these systems we added minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004). Moses Baseline: We trained a Moses system (Koehn et al, 2007) with the following settings: maximum sentence length 80, grow-diag-final and symmetrization of GIZA++ alignments, an interpolated KneserNey smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-felexicalized reordering, sparse lexical and domain features (Hasler et al, 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huangand Chiang, 2007) and the no-reordering-over punctuation heuristic. Our baseline translation system uses Viterbi decoding while our final system uses segment-level Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists using 1 BLEU as the loss function. With large training data, moving to a 5-gram language model, increasing the cube pruning pop limit to 1000, and using Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists collectively show a slight improvement. Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the N-best list translations. Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumarand Byrne, 2004).
Identifying Word Translations In Non-Parallel Texts Common algorithms for sentence and word-alignment allow the automatic identification of word translations from parallel texts. This study suggests that the identification of word translations should also be possible with non-parallel and even unrelated texts. The method proposed is based on the assumption that there is a correlation between the patterns of word cooccurrences in texts of different languages. A similar idea is later applied by (Rapp 1995) to show the plausibility of correlations between words in non-parallel text. Theoretically, it is possible to use these methods to build a translation lexicon from scratch [Rapp, 1995].     Rapp (1995), Grefenstette (1998), Fung and Lo (1998), and Kaji (2003) derived bilingual lexicons or word senses from such corpora. Rapp (1995) proposed a computationally demanding matrix permutation method which maximizes a similarity between co-occurrence matrices in two languages. In order to expand the dictionaries using a set of monolingual comparable corpora, the basic approach pioneered by Fung and McKeown (1997) and Rapp (1995, 1999) is to be further developed and refined in the second phase of the project as to obtain a practical tool that can be used in an industrial context. In particular, we extend the long line of work on inducing translation lexicons (beginning with Rapp (1995)) and propose to use multiple independent cues present in monolingual texts to estimate lexical and phrasal translation probabilities for large, MT-scale phrase-tables. Rapp (1995) was the first to propose using non-parallel texts to learn the translations of words. Previous research on bilingual lexicon induction learned translations only for a small number of high frequency words (e.g. 100 nouns in Rapp (1995), 1,000 most frequent words nouin Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al (2008)). Building a dictionary from scratch is not possible this way or at least computationally un-feasible (see Rapp, 1995). Approaches for lexicon extraction from comparable corpora have been proposed that use the bag of-words model to find words that occur in similar lexical contexts (Rapp, 1995). One of the first works in the area of comparable corpora mining was based on word co-occurrence based approach (Rapp, 1995). The idea to acquire translation candidates based on comparable and unrelated corpora comes from (Rapp, 1995). Rapp (1995) suggested his idea about the usage of context vectors in order to find the words that are the translation of each other in comparable corpora. The underlying assumption is that translations of words that are related in one language are also related in the other language (Rapp 1995). In this situation, Rapp (1995) proposed using a clue different from the three mentioned above: His co-occurrence clue is based on the assumption that there is a correlation between co occurrence patterns in different languages. By presupposing a lexicon of seed words, she avoids the prohibitively expensive computational effort encountered by Rapp (1995).
Use Of Support Vector Learning For Chunk Identification  the number of votes for the class obtained through the pairwise voting is used as the certain score for beam search with width 5 (Kudo and Matsumoto,2000a). In the CoNLL-2000 shared task, we achieved the accuracy of 93.48 using IOB2-F representation (Kudo and Matsumoto, 2000b) 5. For SVM, we used the YAMCHA (Kudo and Matsumoto, 2000) sequence labeling system,1 which uses the TinySVM package for classification. For our experiments, we used TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and test software. Kudo and Matsumoto (2000) used an SVM based algorithm, and achieved NP chunking results of 93.72% precision, 94.02% recall and 93.87 F-measure for the same shared task data, using only the words and their PoS tags. They used the same features as Kudo and Matsumoto (2000), and achieved over-all chunking performance of 92.06% precision, 92.09% recall and 92.08 F-measure (The results for NP chunks alone were not reported). We use TinySVM2 along with YamCha3 (Kudo and Matsumoto (2000, 2001)) as the SVM training and test software. We used the TinySVM toolkit (Kudo and Matsumoto, 2000), with a degree 2 polynomial kernel. Following Pradhan et al (2003), we used tinySVM along with YamCha (Kudo and Matsumoto 2000, 2001) as the SVM training and test software. This type of sequential tagging method regard as a chunking procedure (Kudo and Matsumoto, 2000) at sentence level. Kudo and Matsumoto (Kudo and Matsumoto, 2000) applied SVMs to English chunking and achieved the best performance in the CoNLL00 shared task (Sang and Buchholz, 2000). Thissetting is shown to produce good results for sequence labeling tasks in previous work (Kudo and Matsumoto, 2000), and is what most end-users of SVM classifiers are likely to use. In (Goldberg et al, 2006), we established that the task is not trivially transferable to Hebrew, but reported that SVM based chunking (Kudo and Matsumoto, 2000) performs well. Kudo and Matsumoto (2000) used SVM as a classification engine and achieved an F Score of 93.79 on the shared task NPs. We use TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and classification software. Even with a more feasible method, pairwise (Kreel, 1998), which is employed in (Kudo and Matsumoto, 2000), we can not train a classifier in a reasonable time, because we have a large number of samples that belong to the non-entity class in this formulation. Support Vector Machines (SVMs) (Cortes and Vapnik, 1995) are powerful methods for learning a classifier, which have been applied successfully to many NLP tasks such as base phrase chunking (Kudo and Matsumoto, 2000) and part-of-speech tagging (Nakagawa et al, 2001). We use YamCha (Kudo and Matsumoto, 2000) to perform phrase chunking.
Comparing Automatic And Human Evaluation Of NLG Systems We consider the evaluation problem in Language Generation and results for evaluating several systems with similar functionality, including a knowledge-based generator and several statistical systems. We compare evaluation results for these systems by human domain experts, human non-experts, and several automatic evaluation metrics, inand We that correlate best with human judgments, but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone. We conthat automatic evaluation of systems has considerable potential, in particular where high-quality reference texts and only a small number of human evaluators are available. However, in general it is probably best for automatic evaluations to be supported by human-based evaluations, or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain. In our work (Belz and Reiter, 2006), we used several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts. As we showed previously (Belz and Reiter, 2006) that there are significant inter-subject differences in ratings, one thing we want to determine is how many subjects are needed to get reliable and reproducible results. A previous study by Belz and Reiter (2006) has demonstrated that automatic metrics can correlate highly with human ratings if the training dataset is of high quality. Data-to-texts systems have been evaluated in a number of ways, including human ratings (the most common technique) (Reiter et al, 2005), BLEU-like scores against human texts (Belz and Reiter, 2006), post-edit analyses (Sripada et al, 2005), and persuasive effectiveness (Carenini and Moore, 2006). The two automatic metrics used in the evaluations, NIST and BLEU have been shown to correlate highly with expert judgments (Pearson correlation coefficients 0.82 and 0.79 respectively) in this domain (Belz and Reiter, 2006). The first was an experiment with 9 subjects experienced in reading marine forecasts (Belz and Reiter, 2006), the second is a new experiment with 14 similarly experienced subjects. However, the important thing to keep in mind is that compression of a given sentence is a problem for which there are usually multiple solutions (Belz and Reiter, 2006). This suggests that, for this generation task, the data in the corpus can indeed be treated as a gold standard - unlike, for example, the corpus used by Belz and Reiter (2006), where the human judges sometimes preferred generated output to the corpus data. Belz and Reiter (2006) and Reiter and Belz (2009) describe comparison experiments between the automatic evaluation of system output and human (expert and non-expert) evaluation of the same data (English weather forecasts). While it is found that BLEU and NIST correlate quite well with human judgments in evaluating NLG systems (Belz and Reiter, 2006), it is best to support these figures with human evaluation, which we did on a small scale. The two automatic metrics used in the evaluations, NIST2 and BLEU3, have been shown to correlate well with expert judgments (Pearson's r= 0.82 and 0.79 respectively) in the SUMTIME domain (Belz and Reiter, 2006). When Belz and Reiter (2006) performed a similar study comparing natural-language generation systems that used different text-planning strategies, they also found similar results: automated measures tended to favour majority-choice strategies, while human judges preferred those that made weighted choices. it may be that subjects actually prefer the generated facial displays to the displays in the corpus, as was found by Belz and Reiter (2006). Belz and Reiter (2006) carry out a comparison of automatic evaluation metrics against human domain experts and human non-experts in the domain of weather forecast statements. The variability of the generated texts ranges from a close similarity to slightly shorter - not an uncommon (Belz and Reiter, 2006), but not necessarily detrimental, observation for NLG systems (van Deemter et al, 2005).
A High-Performance Semi-Supervised Learning Method For Text Chunking In machine learning, whether one can build a more accurate classifier by using data is an important issue. Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear. This paper presents a novel semi-supervised method that employs a learning paradigm which we call The idea is to find “what good classifiers are like” by learning from thousands of automatically generated auxiliary classification problems on unlabeled data. By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem. The method produces performance higher than the previous best results on CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (English and German). Ando and Zhang (2005) independently used this phrase, for a semi-supervised, cross-task learner that differs from our unsupervised, cross-instance learner.  Ando and Zhang (2005) utilized a multi task learner within their semi-supervised algorithm to learn feature representations which were useful across a large number of related tasks.  For a more complete description, see (Ando and Zhang, 2005a).  An important observation in (Ando and Zhang, 2005a) is that the binary classification problems used to derive theta are not necessarily those problems we are aiming to solve.  Assuming there are k target problems and m auxiliary problems, it is shown in (Ando and Zhang, 2005a) that by performing one round of minimization, an approximate solution of theat can be obtained from (4) by the following algorithm:. This is a simplified version of the definition in (Ando and Zhang, 2005a), made possible because the same theta is used for all auxiliary problems.     To avoid terminological confusion, we refer throughout the paper to a specific structural learning method, alternating structural optimization (ASO) (Ando and Zhang, 2005a). Pivot features correspond to the auxiliary problems of Ando and Zhang (2005a). We follow Ando and Zhang (2005a) and use the modified Huber loss. For both computational and statistical reasons, though, we follow Ando and Zhang (2005a) and compute a low-dimensional linear approximation to the pivot predictor space. Ando and Zhang (2005a) describe several free paramters and extensions to ASO, and we briefly address our choices for these here. As in Ando and Zhang (2005a), we observed that setting h between 20 and 100 did not change results significantly, and a lower dimensionality translated to faster run-time.
Bleu: A Method For Automatic Evaluation Of Machine Translation Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is for quick or frequent As an overall decoding performance measure, we used the BLEU metric (Papineni et al, 2002). For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al, 2002). We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003). The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). BLEU (Papineni et al 2002) is a system for automatic evaluation of machine translation. The baseline score using all phrase pairs was 59.11 (BLEU, Papineni et al, 2002) with a 95% confidence interval of [57.13, 61.09]. We utilize BLEU (Papineni et al, 2002) for the automatic evaluation of MT quality in this paper. We performed 4 runs of 10-fold cross validation, and measured the performance of the learned generators using the BLEU score (Papineni et al, 2002) and the NIST score (Doddington, 2002). We employ the phrase-based SMT framework (Koehn et al, 2003), and use the Moses toolkit (Koehn et al, 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al, 2002), using a single reference translation.  BLEU (Papineni et al, 2002b; Papineni et al, 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. Many widely used metrics like Bleu (Papineni et al, 2002) and Ter (Snover et al, 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like Meteor. A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score (Papineni et al, 2002) on a development set (Och and Ney, 2002). The results show a statistically-significant (p < 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al, 2004a) scores. Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics: BLEU (Papineni et al, 2002) and NIST (Doddington, 2002). To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score (Papineni et al, 2002) (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output. To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision (Hori and Furui, 2000b) for summarization through word extraction, ROUGE (Lin and Hovy, 2003) for abstracts, and BLEU (Papineni et al, 2002) for machine translation. For both systems, we report BLEU scores (Papineni et al, 2002) on untokenized, recapitalized output. For a certain bilingual test dataset d, we consider a set of observations Od={ (x1 ,y1), (x2 ,y2) ... (xn ,yn)}, where yi is the performance on d (measured using BLEU (Papineni et al, 2002)) of a translation model trained on a parallel corpus of size xi.
D-PATR:  A Deve lopment  Env i ronment fo r  Un i f i ca t ion -Based  Grammars Lauri Karttunen Artificial Intelligence Center SRI International 333 Ravenswood Avenue Menlo Park, CA 94025 USA and Center for the Study of Language and Information Stanford University 1 Introduction I)-PATR is a development environment for unification-based grammars on Xerox l i00 series work stations.  Instead, it takes care of fillers and gaps through a threading& quot; technique (Karttunen 1986:77). Consider the following rules taken from D-PATR (Karttunen, 1986). The formst is that of D-PATR/Karttunen 1986. Particularly well known examples are reported in Kaplan (1983) (see also Kiparsky, 1985), Shieber (1984), Evans (1985), Phillips and Thompson (1985), Jensen et al (1986) and Karttunen (1986). Karttunen (1986) and Shieber (1986) describe systems in which FSs may be modified by default statements in such a way that this property does not automatically hold.
Part-Of-Speech Tagging In Context We present a new HMM tagger that exploits context on both sides of a word to be tagged, and evaluate it in both the unsupervised and supervised case. Along the way, we present the first comprehensive comparison of unsupervised methods for part-of-speech tagging, noting that published results to date have not been comparable across corpora or lexicons. Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable. Finally, we show how this new tagger achieves state-of-the-art results in a supervised, non-training intensive framework. While replicating earlier experiments, Banko and Moore (2004) discovered that performance was highly dependent on cleaning tag dictionaries using statistics gleaned from the tokens. Second, the expectation maximization algorithm for bi tag HMMs is efficient and has been shown to be quite effective for acquiring accurate POS taggers given only a lexicon (tag dictionary) and certain favorable conditions (Banko and Moore, 2004). As Banko and Moore (2004) discovered when 5 Note that the POS tag information is not used in these experiments, except for by the C& amp; C tagger. To consider the effect of the CCG-based initialization for lexicons with differing ambiguity, I use tag cutoffs that remove any lexical entry containing a category that appears with a particular word less than X% of the time (Banko and Moore, 2004), as well as using no cutoffs at all. However, as Banko and Moore (2004) point out, the accuracy achieved by these unsupervised methods depends strongly on the precise nature of the supervised training data (in their case, the ambiguity of the tag lexicon available to the system), which makes it more difficult to understand the behaviour of such systems. Banko and Moore (2004) compared unsupervised HMM and transformation-based taggers trained on the same portions of the Penn Treebank, and showed that the quality of the lexicon used for training had a high impact on the tagging results. Duh and Kirchhoff (2005) presented a minimally supervised approach to tagging for dialectal Arabic (Colloquial Egyptian), based on a morphological analyzer for Modern Standard Arabic and unlabeled texts in a number of dialects. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Author Language Average accuracy Toutanova et al (2003) English 97.24% Banko and Moore (2004) English 96.55% Dandapat and Sarkar (2006) Bengali 84.37% Rao et al (2007) Hindi 76.34% Bengali 72.17% Telegu 53.17% Rao and Yarowsky (2007) Hindi 70.67% Bengali 65.47% Telegu 65.85% Sastry et al (2007) Hindi 69.98% Bengali 67.52% Telegu 68.32% Ekbal et al (2007) Hindi 71.65% Bengali 80.63% Telegu 53.15% Ours Assamese 85.64% and searched in the affix-probability table. As for contextualized lexical probabilities, our extension is very similar to Banko and Moore (2004) who use P (wi|ti? 1, ti ,ti+1) lexical probabilities and found, on the Penn Treebank, that incorporating more context into an HMM when estimating lexical probabilities improved accuracy from 95.87% to 96.59%. One difficulty with their approach ,noted by Banko and Moore (2004), is the treatment of unseen words: their method requires a full dictionary that lists what tags are possible for each word. If we follow Banko and Moore (2004) and construct a full (no OOV) morphological lexicon from the tagged version of the test corpus, we obtain 96.95% precision where theirs was 96.59%. The reason why Banko and Moore (2004) get less than HunPos is not because their system is inherently worse, but rather because it lacks the engineering hacks built into TnT and HunPos. Smith and Eisner (2005) employ a contrastive estimation tech 1As (Banko and Moore, 2004) point out, unsupervised tagging accuracy varies wildly depending on the dictionary employed. We define the unsupervised part-of-speech (POS) tagging problem as predicting the correct part-of speech tag of a word in a given context using an unlabeled corpus and a dictionary with possible word? tag pairs0 The performance of an unsupervised POS tagging system depends highly on the quality of the word7ujh tag dictionary (Bankoand Moore, 2004). by Banko and Moore (2004), these works made use of filtered dictionaries: dictionaries in which only relatively probable analyses of a given word are preserved. Later, Banko and Moore (2004) observed that earlier unsupervised HMM-EM results were artificially high due to use of Optimized Lexicons, in which only frequent-enough analyses of each word were kept. In this paper, we show how these strategies may becombined straightforwardly to produce improvements on the task of learning super taggers from lexicons that have not been filtered in any way.1 We demonstrate their cross-lingual effectiveness on CCGbank (English) and the Italian CCG-TUT 1See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. We use the standard splits of the data used in semi-supervised tagging experiments (e.g.Banko and Moore (2004)): sections 0-18 for training, 19-21 for development, and 22-24 for test.CCG-TUT. Banko and Moore (2004) showed that unsupervised tagger ac curacies on English degrade from 96% to 77% if the lexicon is not constrained such that only high frequency tags exist in the POS-set for each word. Some of these are annotation errors in the tree bank (Banko and Moore, 2004, Figure 2): such (mis)taggings can severely degrade the accuracy of part-of-speech disambiguators, without additional supervision (Banko and Moore, 2004,? 5, Table 1).
Functional Unification Grammar: A Formalism For Machine Translation Arg = (Sense Var) P2 = Arg = (Sense Var)1 This, in turn, is readily interpretable as a description of the logical expression Vq.dog(q)AP(g) It remains to provide verbs with a sense that provides a suitable for is, for (Sense Prop P2 Pred). An example would be the following: Cat = V Lex = barks Tense = Pres Pers = 3 Subj = {Num = Sing Anim = + Obj = NONE = [P2 = [Fred = bark]]] IV Conclusion It has not been possible in this paper to give more than an impression of how an experimental machine translation system might be constructed based on FUG. I hope, however, that it has been possible to convey something of the value of monotonic systems for this purpose. Implementing FUG in an efficient way requires skill and a variety of little known techniques. However, the programs, though subtle, are not large and, once written, they provide the grammarian and lexicographer with an emmense wealth of expressive devices. Any system implemented strictly within this framework will be reversible in the sense that, if it translates from language A to language B the, to the same extent, translates from B to A. If the set among the translations it delivers for a, then a will be among the translations of each of I of no system that comes close to providing these advantages and I know of no facility provided for in any system proposed hitherto that it not subsumable under FUG = Some work has touched on shared knowledge of lexical semantics [aacobs, 1985, Steinacker and Buchberger, 19831 and on grammatical framework suitable for bidirectional systems [Kay, 1984]. As mentioned, FCG organises the information about an utterance in feature structures, similar to other feature-structure based formalisms (as first introduced by Kay (Kay, 1984)) but with some important differences. The family of grammar models that are based on such formalisms include Generalized Phrase Structure Grammar (GPSG) [Gazdar et al 1985], Lexical Functional Grammar (LFG) [Bresnan 1982], Functional Unification Grammar (bUG) [Kay 1984], Head-Driven Phrase Structure Grammar (I-IPSG) [Pollard and Sag 1988], and Categorial Unification Grammar (CUG) [Karttunen 1986, Uszkoreit 1986, Zeevat et al 1987]. The text planner is implemented as a Functional Unification Grammar (Kay 1984) in FUF (Elhadad 1993).
One Sense Per  D iscourse William A. Gale Kenneth W. Church David Yarowsky AT&T Bell Laboratories 600 Mountain Avenue Murray Hill NJ 07974-0636 1.  Fortunately, we have found in (Gale et al, 1992) that the agreement rate can be very high (96.8%), which is well above the baseline, under very different experimental conditions. We originally designed the experiment in Gale et al (1992) to test the hypothesis that multiple uses of a polysemous word tend to have the same sense within a common discourse. We followed the well-known postulate (Gale et al, 1992) that all occurrences of a word in the same discourse tend to have the same sense (one sense per discourse), in order to decrease the annotator workload. In order to improve the bootstrapping performance, we use the heuristic one tag per domain for multi word NE in addition to the one sense per discourse principle [Gale et al 1992]. Apply the 'one sense per discourse' principle [Gale et al1992] for each disambiguated location name to propagate the selected sense to its other mentions within a document. It has been claimed (by Gale et al 1992) on the basis of corpus analysis that to a very large extent a word keeps the same meaning throughout a text. This is a reasonable restriction supported by empirical evidence (see also (Gale et al 1992)). For example, if two NPs are the same string in a given document, then it is more likely than not that they have the same semantic subtype according to the 'one sense per discourse' hypothesis (Gale et al., 1992). In this case we cannot accept the general assumption of one sense per discourse (Gale et al, 1992), because words such as line, large in English or kljuch in Russian can function in the same discourse in a totally different sense. In the original one sense per discourse study, Gale et al (1992b) considered a sample of 9 polysemous English words.  While these numbers are not as high as the 94% agreement reported by Gale et al (1992b) in their empirical study, they still strongly support the one translation per discourse hypothesis. If more than one match of the same term is found in a document, we assume one sense-per-discourse (Gale et al, 1992) and jointly extract features for all matches of the term. Addressing word sense disambiguation, Gale et al (1992) introduced the idea of a word sense located at the discourse-level and observed a 93 strong one-sense-per-discourse tendency, i.e. several occurrences of a polysemous word form have a tendency to belong to the same semantic class within one discourse. Nonetheless, there are a non-negligible number of cases in which the one sense per discourse assumption (Gale et al, 1992) does not hold. This effect of "meaning consistency" also known as the principle of "one sense per discourse" has been applied in word sense disambiguation with quite some success (Gale et al., 1992). This property tends to hold for correct classifiers (Gale et al, 1992a), at least for homonyms.   
Extracting Relations With Integrated Information Using Kernel Methods Entity relation detection is a form of information extraction that finds predefined relations between pairs of entities in text. This paper describes a relation detection approach that combines clues from different levels of syntactic processing using kernel methods. Information from three different levels of processing is considered: tokenization, sentence parsing and deep dependency analysis. Each source of information is represented by kernel functions. Then composite kernels are developed to integrate and extend individual kernels so that processing errors occurring at one level can be overcome by information from other levels. We present an evaluation of these methods on the 2004 ACE relation detection task, using Support Vector Machines, and show that each level of syntactic processing contributes useful information for this task. When evaluated on the official test data, our approach produced very competitive ACE value scores. We also compare the SVM with KNN on different kernels. Also using kernel methods and support vector machines, (Zhao and Grishman, 2005) combine clues from different levels of syntactic information and applies composite kernels to integrate the individual kernels. Although Zhao and Grishman (2005) defined a number of kernels for relation extraction, the method is essentially similar to feature-based methods.   Bigrams: A bigram feature (Zhao and Grishman, 2005) can be represented by a subgraph consisting of two connected nodes from the sequence representation, where each node is labeled with the token.  Zhao and Grishman (2005) defined several feature based composite kernels to integrate diverse features for relation extraction and achieved the F-measure of 70.4 on the 7 relation types of the ACE RDC 2004 corpus.  Since Zhao and Grishman (2005) use a 5-fold cross-validation on a subset of the 2004 data (newswire and broadcast news domains, containing 348 documents and 4400 relation instances), for comparison, we use the same setting (5-fold cross-validation on the same subset of the 2004 data, but the 5 partitions may not be the same) for the ACE 2004 data.   This kernel is formally defined in (Zhao and Grishman, 2005). Zhao and Grishman (2005) reported that adding local information to deep syntactic information improved IE results. The main concerns here (see e.g. (Zhao and Grishman, 2005)) are the extraction of large quantities of facts, generally coupled with machine learning approaches. Bigram of the words between the two mentions: This was extracted by both Zhao and Grishman (2005) and Jiang and Zhai (2007), aiming to provide more order information of the tokens between the two mentions.  This is a strong constraint on the matching of syntax so it is not surprising that the model has good precision but very low recall on the ACE corpus (Zhao and Grishman, 2005). Zhao and Grishman (2005) define a feature based composite kernel to integrate diverse features. Zhao and Grishman (2005) also evaluated their algorithm on the ACE corpus and got good performance. Zhao and Grishman (2005) define several feature-based composite kernels to capture diverse linguistic knowledge and achieve the F-measure of 70.4 on the 7 relation types in the ACE RDC 2004 corpus.
Batch Tuning Strategies for Statistical Machine Translation There has been a proliferation of recent work on SMT tuning algorithms capable of handling larger feature sets than the traditional MERT approach. We analyze a number of these algorithms in terms of their sentencelevel loss functions, which motivates several new approaches, including a Structured SVM. We perform empirical comparisons of eight different tuning strategies, including MERT, in a variety of settings. Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options. (Cherry and Foster, 2012), which is closer to the usual loss used for max-margin in machine learing. Cherry and Foster (2012) have concurrently performed a similar analysis. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). We are currently in the process of implementing and testing other parameter tuning methods (in addition to manual tuning and PRO), specifically lattice-based minimum error rate training (Macherey et al, 2008) and batch MIRA (Cherry and Foster, 2012). As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al, 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. Second, we ran the k-best batchMIRA (kbMIRA) (Cherry and Foster, 2012) implementation in Moses. Cherry and Foster (2012) reported the same result, and their implementation is available in Moses. These works use radically different experimental setups, and to our knowledge only (Cherry and Foster, 2012) and this work compare to at least two high dimensional baselines. We tune with the k-best batch MIRA algorithm (Cherry and Foster, 2012). See (Cherry and Foster, 2012) for details on objectives.
An Unsupervised Approach To Recognizing Discourse Relations We present an unsupervised approach to discourse relations of hold between arbitrary spans of texts. We show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases. A similar approach has been advocated for the interpretation of discourse relations by Marcu and Echihabi (2002). Apart from the fact that we present an alternative model, our work differs from Marcu and Echihabi (2002) in two important ways. Inspired by Marcu and Echihabi (2002), to construct relatively low noise discourse instances for unsupervised methods using cue phrases, we grouped the 13 relations into the following 5 relations: Contrast is a union of Antithesis, Concession, Otherwise and Contrast from RST. Cue-phrase-based patterns could find only limited number of discourse instances with high precision (Marcu and Echihabi, 2002). Nouns (except for named entities) and verbs were most representative words in discourse recognition (Marcu and Echihabi, 2002).  Presently, there exist methods for learning oppositional terms (Marcu and Echihabi, 2002) and paraphrase learning has been thoroughly studied, but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution. Some of existing works attempt to perform relation recognition without hand-annotated corpora (Marcu and Echihabi, 2002), (Sporleder and Lascarides, 2008) and (Blair-Goldensohn, 2007). (Marcu and Echihabi, 2002) used a pattern based approach to extract instances of discourse relations such as Contrast and Elaboration from unlabeled corpora. There are other efforts that attempt to extend the work of (Marcu and Echihabi, 2002). (Saito et al., 2006) followed the method of (Marcu and Echihabi, 2002) and conducted experiments with combination of cross-argument word pairs and phrasal patterns as features to recognize implicit relations between adjacent sentences in a Japanese corpus. (Blair-Goldensohn, 2007) extended the work of (Marcu and Echihabi, 2002) by refining the training and classification process using parameter optimization, topic segmentation and syntactic parsing. To overcome the shortage of manually annotated training data, (Marcu and Echihabi, 2002) proposed a pattern-based approach to automatically generate training data from raw corpora. (Sporleder and Lascarides, 2008) conducted a study of the pattern-based approach presented by (Marcu and Echihabi, 2002) and showed that the model built on synthetical implicit data has not generalize well on natural implicit data. Previous work (Marcu and Echihabi, 2002) and (Sporleder and Lascarides, 2008) adopted predefined pattern-based approach to generate synthetic labeled data, where each predefined pattern has one discourse relation label. (Marcu and Echihabi 2002) proposed a method to identify discourse relations between text segments using Naive Bayes classifiers trained on a huge corpus. When we consider the frequency of discourse relations, i.e. 43% for ELABORATION, 32% for CONTRAST etc., the weighted accuracy was 53% using only lexical information, which is comparable to the similar experiment by (Marcu and Echihabi 2002) of 49.7%. An unsupervised approach was proposed to recognize discourse relations in (Marcu and Echihabi, 2002), which extracts discourse relations that hold between arbitrary spans of text making use of cue phrases. We adopt the approach of Marcu and Echihabi (2002), using a small set of patterns to build relation models, and extend their work by refining the training and classification process using parameter optimization, topic segmentation and syntactic parsing. We draw on and extend the work of Marcu and Echihabi (2002).
A Graph Model For Unsupervised Lexical Acquisition This paper presents an unsupervised method forassembling semantic knowledge from a part-of speech tagged corpus using graph algorithms. The graph model is built by linking pairs of words which participate in particular syntacticrelationships. We focus on the symmetric relationship between pairs of nouns which occur to gether in lists. An incremental cluster-building algorithm using this part of the graph achieves82% accuracy at a lexical acquisition task, evaluated against WordNet classes. The model naturally realises domain and corpus specific am biguities as distinct components in the graph surrounding an ambiguous word. This problem is addressed by Riloff and Shepherd (1997), Roark and Charniak (1998) and more recently byWiddows and Dorow (2002). The senses of a word may then be discovered using graph clustering techniques (Widdows and Dorow, 2002), or algorithms such as HyperLex (Veronis, 2004) or Pagerank (Agirre et al, 2006). In concept acquisition, pattern-based methods were shown to outperform LSA by a large margin (Widdows and Dorow, 2002).  The work by (Widdows and Dorow, 2002) on lexical acquisition is similar to ours because they also use graph structures to learn semantic classes. This is the same general observation exploited by (Widdows and Dorow, 2002), who try to find graph regions that are more connected internally than externally. The major guideline in this part of the evaluation was to compare our results with previous work having a similar goal (Widdows and Dorow, 2002).  The only difference from the (Widdows and Dorow, 2002) experiment is the usage of pairs rather than single words. This was not needed in (Widdows and Dorow, 2002) because they had directly accessed the word graph, which may be an advantage in some applications. The Russian evaluation posed a bit of a problem because the Russian WordNet is not readily available and its coverage is rather small. Fortunately, the subject list is such that WordNet words (Widdows and Dorow, 2002) also reports results for an LSA-based clustering algorithm that are vastly inferior to the pattern-based ones. This metric was reported to be 82% in (Widdows and Dorow, 2002). In order to identify such useful patterns, for each pattern we build a graph following (Widdows and Dorow, 2002). The graph is constructed from a node for each content word, and a directed arc from the node x to y if the corresponding content words appear in the pattern such that x precedes y. In the context of graph-based clustering of words, Widdows and Dorow (2002) used a graph model for unsupervised lexical acquisition. Widdows and Dorow (2002) use a graph model for unsupervised lexical acquisition. In order to identify symmetric patterns, for each pattern we define a pattern graph G (P), as proposed by (Widdows and Dorow, 2002). The concept discovery algorithmis essentially the same as used by (Davidov and Rappoport, 2006) and has some similarity with the one used by (Widdows and Dorow, 2002). We have also performed an indirect comparison to (Widdows and Dorow, 2002). While there is a significant number of other related studies on concept acquisition (see Section 2), Most are supervised and/or use language-specific tools. All of these corpora were also used by (Davidov and Rappoport, 2006) and BNC was used in similar settings by (Widdows and Dorow, 2002). This also allows indirect comparison to several other studies, thus (Widdows and Dorow, 2002) reports results for an LSA-based clustering algorithm that are vastly inferior to the pattern-based ones.
  (Xia and McCord, 2004) describe an approach for translation from French to English, where reordering rules are acquired automatically. Our method differs from that of (Xia and McCord, 2004) in a couple of important respects. Xia and McCord (2004) extracted reordering rules automatically from bilingual corpora for English-to-French translation; Collins et al (2005) used linguistically-motivated clause restructuring rules for German-to-English translation; Li et al (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-to-English translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al (2009) utilized a dependency parser with several hand-labeled precedence rules for re ordering English to subject-object-verb order like Korean and Japanese. While our framework can be applied to any translation system in which it is possible to derive a token-level alignment from the input source tokens to the output target tokens, it is of particular practical interest when applied to a system that performs reordering as a preprocessing step (Xia and McCord, 2004). (Xia and McCord, 2004), and perform these two operations in separate steps, the latter conditioned on the former, Birch et al (2010) a remaking a much stronger assumption when they perform these simulations: they are assuming that lexical choice and word order are entirely independent. Generally reordering rules are applied to the source language, but there have been attempts at target side reordering as well (Na et al, 2009). Reordering rules can be based on different levels of linguistic annotation, such as POS-tags (Niehues and Kolss, 2009), chunks (Zhang et al, 2007) or parse trees (Xia and McCord, 2004). Preprocessing approaches can use either hand-written rules targeting known language differences (e.g. Collins et al (2005), Li et al (2009)), or automatically learnt rules (e.g. Xia and McCord (2004), Zhang et al (2007b)), which are basically language independent. In future work our novel approach might allow to make use of lexicalized reorder rules as in (Xia and McCord, 2004) or syntactic rules as in (Wang et al, 2007). To the best of our knowledge, the authors of (Xia and McCord, 2004) were the first to address this problem in the statistical MT paradigm. (Xia and McCord, 2004) present a similar approach, with a notable difference: the re-ordering rules are automatically learned from aligning parse trees for both the source and target sentences. Many such systems exist, with results being mixed; we review several here. Xia and McCord (2004) (English-to-French translation, using automatically-extracted reordering rules) train on the Canadian Hansard. The most notable models are given by Xia and McCord (2004), Collins et al (2005), Li et al (2007) and Wang et al (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatically extracted the rewriting patterns. The technique proposed in this study is most similar to the one proposed for French-to-English translation task in Xia and McCord (2004), where the authors present a hybrid system for French English translation based on the principle of automatic rewrite patterns extraction using a parse tree and phrase alignments. As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or pre ordering (Xia and McCord, 2004). In particular, systems that use source language syntax allow for the handling of long distance reordering without large increases in The first author is now affiliated with the Nara Institute of Science and Technology. Figure 1: An example with a source sentence F re ordered into target order F, and its corresponding target sentence E. D is one of the BTG derivations that can produce this ordering.the pre-ordering approach to machine translation (Xia and McCord, 2004), which performs translation as a two step process of reordering and translation (Figure 1). Preprocessing approaches involving the use of a syntactic parse of the source sentence to change the word order to more closely match the word order of the target language have been studied by Niessen and Ney (2004), Xia and McCord (2004), Dra ?bek and Yarowsky (2004), Collins et al. Syntax-based reordering rules can be used as a preprocessing step for PB-SMT (and other approaches), to decrease the word-order and syntactic distortion between the source and target languages (Xia and McCord, 2004). The underlying formalisms used has been quite broad and include simple formalisms such as ITGs (Wu, 1997), hierarchical synchronous rules (Chiang, 2005), string to tree models by (Galley et al, 2004) and (Galley et al, 2006), synchronous CFG models such (Xia and McCord, 2004) (Yamada and Knight, 2001), synchronous Lexical Functional Grammar inspired approaches (Probst et al, 2002) and others. Some of the previous approaches include (Collins et al, 2005), (Xia and McCord, 2004).
Determining Term Subjectivity And Term Orientation For Opinion Mining mining a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses. To aid the extraction of opinions from text, recent work has tackled the issue determining the “subjective” terms contained in text, i.e. deciding whether a term that carries opinionated content has a positive or a negative connotation. This is believed to be of key importance for identifying the orientation of documents, i.e. determining whether a document expresses a positive or negative opinion about its subject matter. We contend that the plain determination of the orientation of terms is not a realistic problem, since it starts from the nonrealistic assumption that we already know whether a term is subjective or not; this would imply that a linguistic resource that marks terms as “subjective” or “objective” is available, which is usually not the case. In this paper we confront the task of deciding whether a given term has a positive or a negative connotation, no subjective connotation at this problem thus subsumes the problem of desubjectivity problem of determining orientation. We tackle this problem by testing three different variants of a semi-supervised method previously proposed for orientation detection. Our results show that determining subjectivity is a much harder problem than determining orientation alone. We have also tested a more complex version of e, with ei scores obtained from release 1.0 of Senti-WordNet (Esuli and Sebastiani, 2006b). Esuli and Sebastiani (2006) used the method to cover objective (N) cases. These terms and prior knowledge of their polarity could be used as features in a supervised classification framework to determine the sentiment of the opinionated text (E.g., (Esuli and Sebastiani, 2006)). Another evaluation resource (Esuli and Sebastiani, 2006) is resorted to in order to recover the evaluation values of all the hypernyms for a particular verb. Esuli and Sebastiani (2006) did a classification experiment for creating lexica for opinion mining, for instance, and the importance of lexical information for event extraction in Biomedical texts has been addressed in Fillmore et al (2006). Esuli and Sebastiani (2006) also address this problem testing three different variants of a semi-supervised method, and classify the input into positive, negative or neutral. The importance of neutral category is also discussed in other literatures (Esuli and Sebastiani, 2006). Researchers have found this at various levels of analysis, including the manual annotation of phrases (Takamura et al, 2006), sentiment classification of phrases (Wilson et al, 2005), sentiment tagging of words (Andreevskaia and Bergler, 2006b), and sentiment tagging of word senses (Esuli and Sebastiani, 2006a).
Using WordNet-Based Context Vectors To Estimate The Semantic Relatedness Of Concepts In this paper, we introduce a WordNetbased measure of semantic relatedness by combining the structure and content of WordNet with co–occurrence information derived from raw text. We use the co–occurrence information along with the definitions to build vectors corresponding to each concept in Word- Net. Numeric scores of relatedness are assigned to a pair of concepts by measuring the cosine of the angle between their respective gloss vectors. We show that this measure compares favorably to other measures with respect to human judgments of semantic relatedness, and that it performs well when used in a word sense disambiguation algorithm that relies on semantic relatedness. This measure is flexible in that it can make comparisons between any two concepts without regard to their part of speech. In addition, it can be adapted to different domains, since any plain text corpus can be used to derive the co–occurrence information. Patwardhan and Pedersen (2006) create aggregate co-occurrence vectors for a WordNet sense by adding the co-occurrence vectors of the words in its WordNet gloss. An extension by Patwardhan and Pedersen (2006) differentiated context word senses and extended shorter glosses with related glosses in WordNet.  One possible explanation for the unsuitability of the measures of (Patwardhan and Pedersen, 2006) for the coordinate similarity task could be based on how context is defined when building context vectors. Other measures have been proposed that utilize the text in WordNet's definitional glosses, such as Extended Lesk (Banerjee and Pedersen, 2003) and later the Gloss Vectors (Patwardhan and Pedersen, 2006) method. It is worth noting that in their experiments, (Patwardhan and Pedersen, 2006) report that the Vector method has rank correlation coefficients of .91 and .90 for MC and RG, respectively, which are also top performing values.  To take account of these similarities WordNet-based similarity measures are used (Patwardhan and Pedersen, 2006). Patwardhan and Pedersen (2006) introduce a vector measure to determine the relatedness between pairs of concepts. The system generated rankings were compared with gold standard data created via Amazon Mechanical Turk.The Duluth systems relied on the Gloss Vec tor measure of semantic relatedness (Patwardhanand Pedersen, 2006) as implemented in WordNet: :Similarity (Pedersen et al, 2004). Gloss Vectors measure (Patwardhan and Pedersen, 2006) is calculated as a cosine (9) between context vectors vi and vj of concepts ci and cj. Patwardhan and Pedersen (2006) evaluate six knowledge-based measures on the task of word sense disambiguation and report the same result. Mohammad and Hirst (2006) and Patwardhan and Pedersen (2006) argued that word sense ambiguity is a key reason for the poor performance of traditional distributional measures, and they proposed hybrid approaches that are distributional in nature, but also make use of information in lexical resources such as published thesauri and WordNet. Effectively, this formalizes the notion that two concepts related to a third concept is also semantically related, which is similar to the hypothesis proposed by Patwardhan and Pedersen (2006) in their method based on second-order context vectors. In addition to WktWiki, we operate with 2 baseline measures relying on WordNet glosses available in a WORDNET::SIMILARITY package: Gloss Vectors (Patwardhan and Pedersen, 2006). For this work, we used the Context Vector measure (Patwardhan and Pedersen, 2006). (Patwardhan and Pedersen, 2006) cosine of the angle between the co-occurrence vector computed from the definitions around the two synsets. For each pair of nodes (u, v) in the graph, we compute the semantic similarity score (using WordNet) between every pair of dependency relation (rel: a, b) in u and v as: s (u, v)= reli u ,relj v reli=relj WN (ai ,aj) WN (bi ,bj), where rel is a relation type (e.g., nsubj) and a, b are the two arguments present in the dependency relation (b does not exist for some relations). WN (wi ,wj) is defined as the WordNet similarity score between words wi and wj. The edge weights are then normalized across all edges in the 2There exists various semantic relatedness measures based on WordNet (Patwardhan and Pedersen, 2006).
Structural Ambiguity And Lexical Relations We propose that ambiguous prepositional phrase attachment can be resolved on the basis of the relative strength of association of the preposition with noun and verb, estimated on the basis of word distribution in a large corpus. This work suggests that a distributional approach can be effective in resolving parsing problems that apparently call for complex reasoning. (Hindle and Rooth, 1991) had shown the use of dependency in Prepositional Phrase disambiguation, and the experimental results reported in (Hock en maier, 2003) demonstrate that a language model which encodes a rich notion of predicate argument structure (e.g. including long-range relations arising through coordination) can significantly improve the parsing performances. Selectional preference has been shown to be useful for, e.g., resolving ambiguous attachments (Hindle and Rooth, 1991), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). But it is harder because it does not provide the predictor with all the information needed to solve many doubtful cases; (Hindle and Rooth, 1991) found that human arbiters consistently reach a higher agreement when they are given the entire sentence rather than just the four words concerned. (Hindle and Rooth, 1991) first proposed solving the prepositional attachment task with the help of statistical information, and also defined the prevalent formulation as a binary decision problem with three words involved.
Chinese Segmentation with a Word-Based Perceptron Algorithm Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary. Discriminatively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding finding the highest scoring segmentation. In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences. The generalized perceptron algorithm is used for discriminative training, and we use a beamsearch decoder. Closed tests on the first and show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora. On the three corpora, it also outperformed the word-based perceptron model of Zhang and Clark (2007).  Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does not treat word segmentation as character sequence labeling.  (Zhang and Clark, 2007) uses perceptron (Collins, 2002) to generate word candidates with both word and character features. All of the above models, except (Zhang and Clark, 2007), adopt the character-based discriminative approach. It shows that both our joint-plus model and joint model exceed (or are comparable to) almost all e state-of-the-art systems across all corpora, except (Zhang and Clark, 2007) at PKU (ucvt.). In that special case, (Zhang and Clark, 2007) outperforms the joint-plus model by 0.3% on F score (0.4% for the joint model).  More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002). We use the publicly available Stanford CRF segmenter (Tseng et al, 2005) as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model. We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the given dataset.  For decoding, Zhang and Clark (2007) used a beam search algorithm to get approximate solutions, and Sarawagi and Cohen (2004) introduced a Viterbi style algorithm for exact inference. This is different from the experiments reported in (Zhang and Clark, 2007).  For the decoding, a beam search decoding method (Zhang and Clark, 2007) is used. The feature templates in (Zhao et al, 2006) and (Zhang and Clark, 2007) are used in training the CRFs model and Perceptrons model, respectively. We built a two-stage baseline system, using the per ceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segment or to refer to the segment or from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).
A Statistical Approach To The Semantics Of Verb-Particles This paper describes a distributional approach to the semantics of verb-particle (e.g. We report first on a framework for implementing and evaluating such models. We then go on to report on the implementation of some techniques for using statistical models acquired from corpus data to infer the meaning of verb-particle constructions. Bannard et al (2003) extended this research in looking explicitly at the task of classifying verb-particles as being compositional or not. Semantically, the compositionality of MWEs is gradual, ranging from fully compositional to idiomatic (Bannard et al, 2003). They are also extremely diverse: for example ,onthe semantic dimension alone, MWEs cover an en tire spectrum, ranging from frozen, fixed idioms to free combinations of words (Bannard et al, 2003). Note that although Lin characterizes his work as detecting non-compositionality, we agree with Bannard et al (2003) that it is better thought of as tapping into productivity.plore whether the light verbs themselves show different patterns in terms of how they are used semi productively in these constructions. Bannard (2005), extending work by Bannard et al (2003), instead considers the extent to which the verb and particle each contribute semantically to the VPC. Semantically, the compositionality of MWEs is gradual, ranging from fully com positional to idiomatic (Bannard et al, 2003). Bannard et al (2003), on the other hand, look at the separate contribution of the verb and particle, but assume that a binary decision on the compositionality of each is sufficient.  They do not fall cleanly into mutually exclusive classes, but populate the continuum between the two extremes (Bannard et al, 2003). Possible alternatives for dealing with this issue are discussed by both Bannard et al (2003) and McCarthy et al (2003). Bannard et al (2003) tested techniques using statistical models to infer the meaning of verb-particle constructions (VPCs), focus 2 In this lexicon, many MWEs are encoded as templates,. 
A New Statistical Parser Based On Bigram Lexical Dependencies This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy. Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Maximum likelihood estimation is used to estimate P(wROOT|S) on training data set with the smoothing method used in (Collins, 1996). To overcome the data sparseness problem, we not only apply the smoothing method used in (Collins, 1996) for a lexicalized head to back off it to its part-of-speech, but also assign a very small value to P (cpi|wi) when there is no cpi modifying wi in the constructed case patterns. (9) The smoothing method used in (Collins, 1996) is applied during estimation. Extending on the notion of a Base NP, introduced by Collins (1996), we mark any nonterminal that dominates only preterminals as Base. For the discourse structure analysis, we suggest a statistical model with discourse segment boundaries (DSBs) similar to the idea of gaps suggested for a statistical parsing (Collins (1996)). for the Penn Treebank (Marcus et al, 1994), and Collins (1996), whose constituent parser is internally based on probabilities of bi lexical dependencies, i.e. dependencies between two words. In general, the likelihood of a head-dependent relation decreases as distance increases (Collins, 1996). model (Collins, 1996) to Japanese dependency analysis. It is therefore necessary to either discard infrequent rules, do manual editing, use a different rule format such as individual dependencies (Collins, 1996) or gain full linguistic control and insight by using a hand written grammar, each of which sacrifices total completeness. To generate the training examples forthe classifier, we generate a parse tree for every sentence in the SENSEVAL-3 training data, using the Collins (1996) statistical parser. This is a modified version of the backed-off smoothing used by Collins (1996) to alleviate sparse data problems. We generate our training data from the Wall Street Journal Section of the Penn Tree Bank (PTB), by transforming it to projective dependency structures, following (Collins, 1996), and extracting rules from the result. We first transform the PTB into projective dependencies structures following (Collins, 1996). This lexicalization procedure is commonly used in statistical parsing (Collins, 1996) and produces a dependency tree. While the model of Collins (1996) is technically unsound (Collins, 1999), our aim at this stage is to demonstrate that accurate, efficient wide-coverage parsing is possible with CCG, even with an over-simplified statistical model. The estimation method is based on Collins (1996). Relationship: in this paper, we not only use the label of the constituent label as Collins (1996), but also use some well-known context in parsing to define the head-modifier relationship r (.), including the POS of the modifier m, the POS of the head h, the dependency direction d, the parent label of the dependency label l, the grandfather label of the dependency relation p, the POS of adjacent siblings of the modifier s. However, in order to utilize some syntax information between the pair of words, we adopt the syntactic distance representation of (Collins, 1996), named Collins distance for convenience. Capturing question or answer dependencies can be cast as a straightforward process of mapping syntactic trees to sets of binary head modifier relationships, as first noted in (Collins, 1996).
Personalizing PageRank for Word Sense Disambiguation In this paper we propose a new graphbased method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation. Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets. We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet. In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster. They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), with two variants. A natural next step for this research would be to couple sense distribution estimation and the detection of unattested senses with evidence from the context, using topics or other information about the local context (e.g. Agirre and Soroa (2009)) to carry out unsupervised WSD of individual token occurrences of a given word. Moreover, jcn+wmfvec produces similar results to state of-the-art unsupervised systems on SE02, 61.92% F-mearure in (Guo and Diab, 2010) using WN1.7.1, and SE03, 57.4% in (Agirre and Soroa, 2009) using WN1.7. PPR2 (Agirre et al, 2010): The system uses the UMLS meta thesaurus as a lexical knowledge graph and executes the Personalized PageRank, a state-of-the-art graph-based method, on the graph (Agirre and Soroa, 2009). Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) over a graph representing WordNet to disambiguate ambiguous words by taking their context into consideration. Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) with WordNet as the lexical knowledge base (LKB) to perform WSD. Tool: We used UKB tool 3 (Agirre and Soroa,2009) which provides an implementation of personalized PageRank.  In (Agirre and Soroa, 2009), a comparative analysis of different graph-based models over two well known WSD benchmarks is reported. In particular, a variant called Personalized PageRank (PPR) is proposed (Agirre and Soroa, 2009) that tries to trade-off between the amount of the employed lexical information and the overall efficiency. In (Agirre and Soroa, 2009), a possible, and more accurate alternative, is also presented called PPRword2word (PPRw2w) where a different personalization vector is used for each word in a sentence. The intuition is that distributional evidence is able to cover the gap between word oriented usages of the PPR as for the PPRw2w defined in (Agirre and Soroa, 2009), and its sentence oriented counterpart. Many algorithms (as well as the one proposed by (Agirre and Soroa, 2009)) initialize the ranks of the vertex at a uniform value (usually 1/N for a graph with N vertices). In order to address the above problems, in line with the notion of topic-sensitive PageRank, a personalized PageRank approach has been recently devised (Agirre and Soroa, 2009) as discussed in the next section. In (Agirre and Soroa, 2009), a novel use of PageRank for word sense disambiguation is presented. The alternative proposed in (Agirre and Soroa, 2009) allows a more static use of the full LKB. A word oriented version of the algorithm is also proposed in (Agirre and Soroa, 2009). This approach to the personalized PageRank is termed word-by-word or PPRw2w version in (Agirre and Soroa, 2009). The key idea in (Agirre and Soroa, 2009) is to adapt the matrix initialization step in order to exploit the available contextual evidence. In line with the results reported in (Agirre and Soroa, 2009), experiments against two different WordNet versions, 1.7 and 3.0, have been carried out.
Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs We present a novel approach to weakly supervised semantic class learning from the web, using a single powerful hyponym pattern combined with graph structures, which capture two properties associated with pattern-based Ina candidate is it was discovered many times by other instances in the pattern. A candidate is if it frequently leads to the discovery of other instances. Together, these two measures capture not only frequency of occurrence, but also cross-checking that the candidate occurs both near the class name and near other class members. We developed two algorithms that begin with just a class name and one seed instance and then automatically generate a ranked list of new class instances. We conducted experiments on four semantic classes and consistently achieved high accuracies. (Kozareva et al, 2008) proposed the use of a doubly-anchored hyponym pattern and a graph to represent the links between hyponym occurrences in these patterns. We are particularly interested in the usage of recursive patterns for the learning of semantic relations not only because it is a novel method, but also because recursive patterns of the DAP fashion are known to: (1) learn concepts with high precision compared to singly-anchored pat terns (Kozareva et al, 2008), (2) use only one seed instance for the discovery of new previously unknown terms, and (3) harvest knowledge with minimal supervision. With the same goal, Kozareva et al (2008) apply similar textual patterns to the web. Similarly, (Kozareva et al,2008) evaluated only a small number (a few hundreds) of harvested instances. In our experiments, we use the doubly-anchored lexico-syntactic patterns and bootstrapping algorithm introduced by (Kozareva et al., 2008) and (Hovy et al, 2009). In response, many automatic and semi-automatic methods of creating sets of named entities have been proposed, some are supervised (Zhou and Su, 2001), unsupervised (Pantel and Lin 2002, Nadeau et al 2006), and others semi-supervised (Kozareva et al 2008). Approaches in the first category use lexical-syntactic formulation to define patterns, either manually (Kozareva et al, 2008) or automatically (Girju et al, 2006), and apply those patterns to mine instances of the patterns. One approach for taxonomy deduction is to use explicit expressions (Iwaska et al, 2000) or lexical and semantic patterns such as is a (Snow et al, 2004), similar usage (Kozareva et al, 2008), synonyms and antonyms (Lin et al, 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. (Kozareva et al., 2008) introduced a bootstrapping scheme using the doubly-anchored pattern (DAP) that is guided through graph ranking. To assess how well our algorithm compares with previous semantic class learning methods, we compared our results to those of (Kozareva et al, 2008). Consequently, we can compare the results produced by the first iteration of our algorithm (before intermediate concepts are learned) to those of (Kozareva et al, 2008) for the Animal and People categories, and then compare again after 10 bootstrapping iterations of intermediate concept learning. Bootstrapping with intermediate concepts produces nearly 5 times as many basic-level concepts and instances than (Kozareva et al, 2008) obtain, while maintaining similar levels of precision. To group adjectives, we use a bootstrapping technique (Kozareva et al 2008) that learns which adjectives tend to co-occur, and groups these together to form an at tribute class. Kozareva et al (2008) use a boot strapping approach that extends the fixed-pattern approach of Hearst (1992) in two intriguing ways. The approach we describe here is most similar to that of Kozareva et al (2008). Kozareva et al (2008) test their approach on relatively simple and objective categories like states, countries (both closed sets), singers and fish (both open, the former more so than the latter), but not on complex categories in which members are tied both to a general category, like food, and to a stereotypical property, like sweet (Veale and Hao, 2007). Following Kozareva et al (2008), we can either indulge in reckless bootstrapping, which ignores the question of noise until all bootstrapping is finished, or we can apply a noise filter after each incremental step. Kozareva et al (2008) and Navigli et al (2011) both develop systems that create taxonomies end to-end, i.e., discover the terms, their relations, and how these are hierarchically organized. Similarly to Kozareva et al (2008) and Navigli et al (2011), our model operates over a graph whose nodes represent terms and edges their relationships. We apply boot strapping (Kozareva et al 2008) on the word graphs by manually selecting 10 seeds for concrete and abstract words (see Table 10).
Inducing Multilingual Text Analysis Tools via Robust Projection across Aligned Corpora David Yarowsky Dept.  Others have exploited the automatic transfer of some already existing annotated resource in a different medium or language (such as the translingual projection of part-of-speech tags, syntactic bracketing and inflectional morphology in Yarowsky et al (2001), requiring no direct supervision in the foreign language). That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al, 2001). In particular, prior work on translingual part-of-speech tagger projection via parallel bilingual corpora (e.g. Yarowsky et al, 2001) has been limited to inducing part-of-speech taggers in second languages (such as French or Czech) that only assign tags at the granularity of their source language (i.e. 49 the Penn Treebank-granularity distinctions from English). Yarowsky et al (2001) performed early work in the cross-lingual projection of part-of-speech tag annotations from English to French and Czech, by way of word-aligned parallel bilingual corpora. Our cross-lingual POS tag projection process is similar to Yarowsky et al (2001). We used the methods of Yarowsky et al (2001) to develop a core part-of-speech tagger for French, based only on the projected core tags, and used this as a basis for fine-grained tags. It is important because a word-aligned corpus is typically used as a first step in order to identify phrases or templates in phrase-based Machine Translation (Och et al., 1999), (Tillmann and Xia, 2003), (Koehn et al., 2003, sec. 3), or for projecting linguistic annotation across languages (Yarowsky et al, 2001). Early studies of cross-lingual annotation projection were accomplished for lexically-based tasks; for example part-of-speech tagging (Yarowsky and Ngai, 2001), named-entity tagging (Yarowsky et al, 2001), and verb classification (Merlo et al, 2002). Of significant interest is the porting of annotations across languages: for example, Yarowsky et al 2001 present a method for automatic tagging of English and the projection of the tags to other languages; however, these tags do not include semantics. Morphological analyzers, noun-phrase chunkers, POS taggers, etc., have also been developed for resource deficient languages by exploiting translated or parallel text (Yarowsky et al, 2001). Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al, 2001), and statistical machine translation. While the use of parallel resources is rather familiar in a wide range of NLP domains, such as statistical machine translation (Koehn, 2005) or annotation projection (Yarowsky et al, 2001), our work shows that they can be exploited forvery specific problems that arise in deep linguistic analysis (see Section 4). Yarowsky et al (2001) introduced a new method for developing a Part-of-Speech tagger by projecting tags across aligned corpora. However, in our research study, new challenges arose because our RRL data are automatically annotated, which is different from what has been reported in the research works we have mentioned before, i.e. (Yarowsky et al., 2001) and (Klementiev and Roth, 2006), where gold annotated data were used.  Yarowsky et al (2001) describe a successful method consisting of (i) automatic annotation of English texts, (ii) cross language projection of annotations onto target language texts, and (iii) induction of noise-robust taggers for the target language. Although potentially useful as a proxy for semantic equivalence, automatically induced alignments are often noisy, thus leading to errors in annotation projection (Yarowsky et al., 2001). Although phrase based approaches to SMT tend to be robust to word alignment errors (Lopez and Resnik, 2006), improving word-alignment is still useful for other NLP research that is more sensitive to alignment quality, e.g., projection of information across parallel corpora (Yarowsky et al, 2001). In alignment work, the foundational work is Yarowsky et al's induction of projections across aligned corpora (Yarowsky et al, 2001), most successfully adapted to cross-linguistic syntactic parsing (Hwa et al, 2005). 
Generalized Inference With Multiple Semantic Role Labeling Systems result with joint inference on the development set. Overall results on the development and test sets are shown in Table 1. Table 2 shows the results of individual systems and the improvement gained by the joint inference on the development set. 4 Conclusions We present an implementation of SRL system which composed of four stages—1) pruning, 2) argument identification, 3) argument classification, and 4) inference. The inference provides a natural way to take the output of multiple argument classifiers and combines them into a coherent predicate-argument output. Significant improvement in overall SRL performance through this inference is illustrated. Acknowledgments We are grateful to Dash Optimization for the free academic use of Xpress-MP. This research is sup Its architecture is based on the top system in the 2005 CoNLL shared task (Koomen et al, 2005), modified to process raw text using lower level processors but maintaining 6 good real time performance. Details of them can be found in (Koomen et al, 2005). Only recently we have been able to test Koomen et al (2005) SRL tool. This module is re trained in our SRC experiments, using parameters described in (Koomen et al, 2005). Combining the output from several different systems has been shown to be beneficial (Koomen et al, 2005). These were first annotated with semantic roles using a state of-the-art semantic role labeling system (Koomen et al, 2005). Koomen et al (2005) adopted the outputs of multiple SRL systems (each on a single parse tree) and combined them into a coherent predicate argument output by solving an optimization problem. Basedon that work, Koomen et al (2005) combined several SRL outputs using ILP method. On the other hand, probabilistic inference processes, which have been successfully used for SRL (Koomen et al, 2005), mandate that each individual candidate argument be associated with its raw activation, or confidence, in the given model. Koomen et al (2005) used a 2 layer architecture similar to ours. Koomen et al (2005) combined several SRL outputs using ILP method.
Overview of Genia Event Task in BioNLP Shared Task 2011 The Genia event task, a bio-molecular event extraction task, is arranged as one of the main tasks of BioNLP Shared Task 2011. As its second time to be arranged for community-wide focused efforts, it aimed to measure the advance of the community since 2009, and to evaluate generalization of the technology to full text papers. After a 3-month system development period, 15 teams submitted their performance results on test cases. The results show the community has made a significant advancement in terms of both performance improvement and generalization. The BioNLP Shared Task 2011 (BioNLP ST 11) (Kim et al, 2011a), the follow-up event to the BioNLP 09 Shared Task (Kim et al, 2009), was organized from August 2010 (sample data release) to March 2011. For compatibility with the BioNLP ST 09 and its repeat as the GE task in 2011 (Kim et al, 2011b), the REL task training/development/test set division of the GENIA corpus abstracts matches that of the BioNLP ST 09 data. The GE task (Kim et al, 2011) preserves the task definition of BioNLP-ST 2009, arranged based on the Genia corpus (Kim et al, 2008). The Infectious Diseases (ID) task of the BioNLP Shared Task 2011 (Kim et al, 2011a) is an information extraction task focusing on the biomolecular mechanisms of infectious diseases. These are a superset of those targeted in the BioNLP ST 09 and its repeat, the 2011 GE task (Kim et al, 2011b). Nevertheless, extraction performance for the top systems is comparable to the state-of-the-art results for the established BioNLP ST 09 task (Miwa et al, 2010) as well as its repetition as the 2011 GE task (Kim et al, 2011b), where the highest overall result for the primary evaluation criteria was also 56% F score for the FAUST system (Riedel et al, 2011). The five top-ranking systems participated also in the GE task (Kim et al, 2011b), which involves a subset of the ID extraction targets. We describe the Stanford entry to the BioNLP 2011 shared task on biomolecular event extraction (Kim et al, 2011a). We participated in the BioNLP-ST 2011 (Kim et al, 2011a), and applied a graph matching-based approach (Liu et al, 2010) to tackling the Task 1 of the GE NIA event extraction (GE) task (Kim et al, 2011b), and the core task of the Epigenetics and Post-translational Modifications (EPI) task (Ohta et al, 2011), two main tasks of the BioNLP-ST 2011. This section presents our results on the GE and the EPI tasks (Kim et al, 2011b; Ohta et al, 2011) respectively. Different experimental methods in processing the obtained event rules are described for the purpose of improving the precision of both tasks and increasing the recall of the EPI task. This paper presents the UMass entry to the BioNLP 2011 shared task (Kim et al, 2011a). For Genia (GE) Task 1 (Kim et al, 2011b) we achieve the second best results.  There are 4 event extraction tracks: in addition to the GENIA track that again focuses on transcription factors (Kim et al, 2011b), the epigenetics and post translational modification track (EPI) focuses on events relating to epigenetic change, such as DNAmethylation and hi stone modification, as well as other common post-translational protein modifications (Ohta et al, 2011), whereas the infectious diseases track (ID) focuses on bio-molecular mechanisms of infectious diseases (Pyysalo et al, 2011a). The BioNLP 2011 shared task GENIA Task1 (BioNLP11ST-GE1) (Kim et al, 2011) focuses on extracting events from abstracts and full papers. Domain event extraction has been advanced in particular by the BioNLP Shared Task (ST) events (Kim et al, 2011a; Kim et al, 2011b), which have introduced common task settings, datasets, and evaluation criteria for event extraction. By the primary evaluation criteria, the highest performance achieved in the 2009 task was 51.95% F-score, and a 57.46% F score was reached in the comparable 2011 task (Kim et al, 2011b). This BioNLP ST 2009 formulation of the event extraction task was followed also in three 2011 main tasks: the GE (Kim et al, 2011c), ID (Pyysalo et al,2011a) and EPI (Ohta et al, 2011) tasks. Finally, we noted a number of cases that we judged to be errors in the gold annotation; the number is broadly in line with the reported inter-annotator agreement for the data (see e.g. Ohta et al (2011)). While there is an unavoidable subjective component to evaluations such as this, we note that a similar evaluation performed following the BioNLP Shared Task 2009 using test set data reached broadly comparable results (Kim et al, 2011a). The first alternative criterion has also been previously considered in the GE task evaluation (Kim et al., 2011c); the latter has, to the best of our knowledge, not been previously considered in domain event extraction.
Named Entity Transliteration With Comparable Corpora In this paper we investigate Chinesename transliteration using compacorpora where texts in the two languages deal in some of the same topics — and therefore share references to named entities — but are not translations of each other. We present two distinct methods for transliteration, one approach using phonetic transliteration, and the second using the temporal distribution of candidate pairs. Each of these approaches works quite well, but by combining the approaches one can achieve even better results. We then propose a novel score propagation method that utilizes the co-occurrence of transliteration pairs within document pairs. This propagation method achieves further improvement over the best results from the previous step. In our work, we adopt the method proposed in (Tao and Zhai, 2005) and apply it to the problem of transliteration; note that (Tao and Zhai, 2005) compares several different metrics for time correlation, as we also note below and see (Sproat et al., 2006). However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth, 2009), (Goldwasser and Roth, 2008), (Klementiev and Roth, 2006), (Sproat et al, 2006), (Udupa et al,2009b). These include studying the query logs (Brill et al, 2001), unrelated corpora (Rapp, 1999), and comparable corpora (Sproat et al 2006). An initial PSM is bootstrapped using limited prior knowledge such as a small amount of transliterations, which may be obtained by exploiting co-occurrence information (Sproat et al, 2006). In transliteration extraction, mining translations or transliterations from the ever-growing multilingual Web has become an active research topic, for example, by exploring query logs (Brill et al., 2001) and parallel (Nie et al, 1999) or comparable corpora (Sproat et al, 2006). (Sproat et al 2006) have compared names from comparable and contemporaneous English and Chinese texts, scoring matches by training a learning algorithm to compare the phonemic representations of the names in the pair, in addition to taking into account the frequency distribution of the pair over time. For example, (Sproat et al, 2006) presents a supervised system that achieves a MRR score of 0.89, when evaluated over a dataset consisting of 400 English NE and 627 Chinese words. (Sproat et al 2006) report an oracle accuracy of 85%, but it depends on the source of the candidate transliterations.
Memory-Based Dependency Parsing This paper reports the results of experimentsusing memory-based learning to guide a de terministic dependency parser for unrestricted natural language text. Using data from a small treebank of Swedish, memory-based classifiers for predicting the next action of the parser are constructed. The accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parserguide is evaluated by parsing the held-out por tion of the treebank. The evaluation shows thatmemory-based learning gives a signficant im provement over a previous probabilistic modelbased on maximum conditional likelihood esti mation and that the inclusion of lexical features improves the accuracy even further. Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004). The parsing methodology investigated here has previously been applied to Swedish, where promising results were obtained with a relatively smalltreebank (approximately 5000 sentences for training), resulting in an attachment score of 84.7% and a labeled accuracy of 80.6% (Nivre et al, 2004). For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003). Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004). Models similar to model 2 have been found to work well for datasets with a rich annotation of dependency types, such as the Swedish dependency tree bank derived from Einarsson (1976), where the extra part-of-speech features are largely redundant (Nivre et al, 2004). These settings are the result of extensive experiments partially reported in Nivre et al (2004). The portion of words that are assigned the correct head and dependency type (or no head if the word is a root) (Nivre et al, 2004). Indirect support for this assumption can be gained from previous experiments with Swedish data, where al most the same accuracy (85% unlabeled attachment score) has been achieved with a tree bank which is much smaller but which contains proper dependency annotation (Nivre et al, 2004). This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivreet al, 2004). It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English).  Based on results from previous optimization experiments (Nivre et al., 2004), we use the modified value difference metric (MVDM) to determine distances between instances, and distance-weighted class voting for determining the class of a new instance. Compared to the state of the art in dependency parsing, the unlabeled attachment scores obtained for Swedish with model? 5, for both MBL and SVM, are about 1 percentage point higher than the results reported for MBL by Nivre et al (2004). In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al, 2004) and English (Nivre and Scholz, 2004). More details on the memory-based prediction can be found in Nivre et al (2004) and Nivre and Scholz (2004).  To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al, 2004), SVMs (Nivre et al, 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b). We build an ISBN model of dependency parsing using the parsing order proposed in (Nivre et al,2004). However, instead of performing deterministic parsing as in (Nivre et al, 2004), we use this ordering to define a generative history-based model, by integrating word prediction operations into the set of parser actions. Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al, 2004).
Scaling To Very Very Large Corpora For Natural Language Disambiguation The amount of readily available on-line text has reached hundreds of billions of words and continues to grow. Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less. In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used. We are fortunate that for this particular application, correctly labeled training data is free. Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost. However, corpus size is no longer a limiting factor: whereas up to now people have typically worked with corpora of around one million words, it has become feasible to build much larger document collections; for example, Banko and Brill (2001) report on experiments with a one billion word corpus. Banko and Brill, (2001) also found this trend for the task of confusion set disambiguation on corpora of up to one billion words. Self-training with bagging: The general self training with bagging algorithm (Banko and Brill, 2001) is presented in Table 6 and illustrated in Figure 7 (a). Figure 7 (b) illustrates the algorithm and Figure 8 describes the algorithm, also known as committee based active-learning (Banko and Brill, 2001). So the creation of more annotated data is necesssary and will certainly cause major improvements of current WSD systems and NLP systems in general (see also (Banko and Brill, 2001)). Banko and Brill (2001) suggested that the development of very large training corpora may be more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora. Banko and Brill (2001) report on confusion set disambiguation experiments where they apply relatively simple learning methods to a one billion word training corpus. Recent work has replicated the Banko and Brill (2001) results on the much more complex task of automatic thesaurus extraction, showing that contextual statistics, collected over a very large corpus, significantly improve system performance (Curran and Moens, 2002). In related work (Ng and Cardie, 2003), we compare the performance of the Blum and Mitchell co training algorithm with that of two existing single view bootstrapping algorithms - self-training with bagging (Banko and Brill, 2001) and EM (Nigam et al., 2000) - on coreference resolution, and show that single-view weakly supervised learners are a viable alternative to co-training for the task. Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). It has been well-established that statistical models improve as the size of the training data increases (Banko and Brill 2001a, 2001b). We start by using web counts for two generation tasks for which the use of large data sets has shown promising results: (a) target language candidate selection for machine translation (Grefenstette, 1998) and (b) context sensitive spelling correction (Banko and Brill, 2001a, b). Self-training with bagging: The general self training with bagging algorithm (Banko and Brill, 2001). As expected, word alignment, like many other NLP tasks (Banko and Brill, 2001), highly benefits from large amounts of training data. Additionally, for certain applications in natural language processing (NLP), it has been noted that the particular algorithms or feature sets used tend to become irrelevant as the size of the corpus increases (Banko and Brill 2001). More recently, Banko and Brill (2001) have advocated for the creative use of very large text collections as an alternative to sophisticated algorithms and hand-built resources. Banko and Brill (2001) show that even using a very simple algorithm, the results continue to improve log-linearly with more training data, even out to a billion words. In this paper we describe two techniques - surface features and paraphrases - that push the ideas of Banko and Brill (2001) and Lapata and Keller (2004) farther, enabling the use of statistics gathered from very large corpora in an unsupervised manner. We regard the results in Figure 2 as a companion to Banko and Brill (2001)'s work on exponentially increasing the amount of labeled training data. As others have pointed out (Banko and Brill, 2001), with enough data the complex algorithms with their tricks cease to have an advantage over the simpler methods.
An Empirical Study Of Smoothing Techniques For Language Modeling We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data corpus versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods. Smoothing typically adds considerable computational complexity to the system since multiple models need to be estimated and applied together, and it is often considered a black art (Chen and Goodman, 1996). Constant restoring is similar to the additive smoothing, which isused to solve the zero-frequency problem of language models (Chen and Goodman, 1996). We refer the reader to (Chen and Goodman, 1996) for a survey of the discounting methods for n-gram models. I used a smoothing method loosely based on the one-count method given in (Chen and Goodman, 1996). The language model is a statistical 4-gram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using only English sentences in the parallel training data. This was done using the SRI Language Modelling toolkit (Stolcke, 2002) employing linear interpolation and modified Kneser Ney discounting (Chen and Goodman, 1996). Alternatively, smoothing techniques (Chen and Goodman, 1996) redistribute the probabilities, taking into account previously unseen word sequences. In particular, instead of back-off, smoothing techniques could be investigated to reduce the impact of zero probability problems (Chen and Goodman, 1996). This is a general issue in statistical Natural Language Processing (NLP) and many possible remedies have been proposed in the literature, such as, for instance, using smoothing techniques (Chen and Goodman, 1996), or working with linguistically enriched, or more abstract, representations. Smoothing techniques, such as Kneser-Ney and Witten-Bell back off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off to lower order distributions, thus providing an estimate for the probability of these unseen events. Conventional smoothing techniques, such as Kneser Ney and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events. Sophisticated smoothing techniques like modified Kneser-Ney and Katz smoothing (Chen and Goodman, 1996) smooth together the predictions of unigram, bigram, trigram, and potentially higher n-gram sequences to obtain accurate probability estimates in the face of data sparsity. A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Giga word corpus (Parker et al, 2009). We use one-count smoothing (Chen and Goodman, 1996), where (ti) is based on the number of words that occur with ti once: (ti)= |wi: C (ti ,wi)= 1|. Modified Knesser-Ney smoothed (Chen and Goodman, 1996) n-gram language models are built from the monolingual English data using the SRI language modeling toolkit (Stolke, 2002). A conventional ngram language model of the target language provides the third component of the system. In all our experiments, we used 4-gram reordering models and bilingual tuple models built using Kneser-Ney back off (Chen and Goodman, 1996). Even with limited context, the parameter space can be quite sparse and requires sophisticated techniques for reliable probability estimation (Chen and Goodman, 1996). We used one-count smoothing (Chen and Goodman, 1996). When training all the language models, modified Kneser-Ney smoothing (Chen and Goodman, 1996) for n-grams is used. We constructed a 5-gram language model from the provided English News monolingual training data as well as the English side of the parallel corpus using the SRI language modeling toolkit with modified Kneser Ney smoothing (Chen and Goodman, 1996).
Domain-Specific Sense Distributions And Predominant Sense Acquisition Distributions of the senses of words are often highly skewed. This fact is exploitedby word sense disambiguation (WSD) sys tems which back off to the predominant sense of a word when contextual clues arenot strong enough. The domain of a doc ument has a strong influence on the sensedistribution of words, but it is not feasi ble to produce large manually annotated corpora for every domain of interest. In this paper we describe the construction of three sense annotated corpora in different domains for a sample of English words.We apply an existing method for acquiring predominant sense information automatically from raw text, and for our sam ple demonstrate that (1) acquiring suchinformation automatically from a mixeddomain corpus is more accurate than de riving it from SemCor, and (2) acquiringit automatically from text in the same do main as the target domain performs best by a large margin. We also show that for an all words WSD task this automatic method is best focussed on words that are salient to the domain, and on words with a different acquired predominant sense in that domain compared to that acquired from a balanced corpus. Recent results on the SPORTS and FINANCE gold standard dataset (Koeling et al 2005) show that domain WSD can achieve accuracy in the 50 60% ballpark when a state-of-the-art algorithm such as Personalized PageRank is paired with a distributional approach (Agirre et al 2009) or with semantic model vectors acquired for many domains (Navigli et al 2011). We also experimented with the gold standard produced by Koeling et al (2005). We first test the proposed method over the tasks of predominant sense learning and sense distribution induction, using the WordNet-tagged dataset of Koeling et al (2005), which is made up of 3 collections of documents: a domain-neutral corpus (BNC), and two domain-specific corpora (SPORTS and FINANCE). We first notice that, despite the coarser-grained senses of Macmillan as compared to WordNet, the upper bound WSD accuracy using Macmillan is comparable to that of the WordNet-based datasets over the balanced BNC, and quite a bit lower than that of the two domain corpora of Koeling et al (2005). The relative occurrence of unlisted/unclear senses in the datasets of Koeling et al (2005) is comparable to UKWAC.
Efficient Parsing For Bilexical Context-Free Grammars And Head Automaton Grammars stochastic parsers use grammars, where each word type idiosyncratically prefers particular complements with parhead words. We present parsing algorithms for two bilexical formalisms, improvthe prior upper bounds of For a comspecial case that was known to allow (Eisner, 1997), we present an algorithm with an improved grammar constant. For certain lexicalized context-free models we even obtain higher time complexities when the size of the grammar is not to be considered as a parameter (Eisner and Satta, 1999). We can reduce the generative power of context free transduction grammars by a syntactic restriction that corresponds to the bi lexical context-free grammars (Eisner and Satta, 1999). While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bi lexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999). However, if we restrict u to be in U, as we do in the above, then maximizing c(u) over U can be solved using the bi lexical parsing algorithm from Eisner and Satta (1999). The algorithmic complexity of (Wu, 1996) is O (n3+4 (m1)), though Huang et al (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O (n3+3 (m1)), i.e., O(n3m). While it seems difficult to improve the asymptotic running time of the Eisner algorithm beyond what is presented in (Eisner and Satta, 1999), McDonald et al. Since we are interested in projecting the fractional parse onto the space of projective spanning trees, we can simply employ a dynamic programming parsing algorithm (Eisner and Satta, 1999) where the weight of each edge is given as the fraction of the edge variable. The new model, which bilexicalizes within languages, allows us to use the "hook trick" (Eisner and Satta, 1999) and therefore reduces complexity. In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the non projective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. Throughout this paper we will use split bi lexical grammars, or SBGs (Eisner, 2000), a notationally simpler variant of split head-automaton grammars, or SHAGs (Eisner and Satta, 1999). On this point, see (Eisner and Satta, 1999, and footnote 6). First, we observe without details that we can easily achieve this by starting instead with the algorithm of Eisner (2000), rather than Eisner and Satta (1999), and again refusing to add long tree dependencies. The obvious reduction for unsplit head automaton grammars, say, is only O(n4) O (n3k), following (Eisner and Satta, 1999). In the case of dependency parsers it is also possible to use grammars (Eisner and Satta, 1999), but many algorithms use a data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bi lexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing after all, there are no phrasal constituents to consider. Folding introduces new intermediate items, perhaps exploiting the distributive law; applications include parsing speedups such as (Eisner and Satta, 1999), as well as well-known techniques for speeding up multi-way database joins, constraint programming, or marginalization of graphical models. In the sections that follow, we frame various dependency models as a particular variety of CFGs known as split-head bi lexical CFGs (Eisner and Satta, 1999). The scoring follows the parametrization of a weighted split head-automaton grammar (Eisner and Satta, 1999). As shown by Eisner (Eisner and Satta,1999) the dynamic programming algorithms for bi lexicalized PCFGs require O (m3) states, so a n-best parser would require O (nm3) states. The best known parsing algorithm for such a model is O (n3) (Eisner and Satta, 1999).
Automatic Discovery Of Non-Compositional Compounds In Parallel Data Automatic segmentation of text into minimal content-bearing units is an unsolved problem even for languages like English. Spaces between words offer an easy first approximation, but this approximation is not good enough for machine translation (MT), where many word sequences are not translated word-for-word. This paper presents an efficient automatic method for discovering sequences of words that are translated as a unit. The method proceeds by comparing pairs of statistical translation models induced from parallel texts in two languages. It can discover hundreds of noncompositional compounds on each iteration, and constructs longer compounds out of shorter ones. Objective evaluation on a simple machine translation task has shown the method's potential to improve the quality of MT output. The method makes few assumptions about the data, so it can be applied to parallel data other than parallel texts, such as word spellings and pronunciations. Melamed (1997b), however, proposes a method for the recognition of multi word compounds in bi texts that is based on the predictive value of a translation model.   Instead, we develop a new scoring criterion, based on Melamed (1997). Melamed (1997) and Lin (1999) have done some research on non compositional phrases discovery. Some applications that would benefit from knowing this distinction are machine translation (Imamura et al, 2003), finding paraphrases (Bannardand Callison-Burch, 2005), (multilingual) information retrieval (Melamed, 1997a), etc. Melamed (1997b) measures the semantic entropy of words using bi texts. Melamed (1997a) investigates various techniques to identify non-compositional compounds in parallel data. This measure is equivalent to translational entropy (Melamed, 1997b). Some of the most sophisticated work on this aspect of problem again seems to be that of Melamed (1997). Melamed (1997) investigates techniques for identifying non-compositional compounds in English-French parallel corpora and emphasises that translation models that take non compositional compounds into account are more accurate.
Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon Even though considerable attention has been given to semantic orientation of words and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons. In this paper, we show how we create a high-quality, moderate-sized emotion lexicon using Mechanical Turk. In addition to questions about emotions evoked by terms, we show how the inclusion of a word choice question can discourage malicious data entry, help identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help obtain annotations at sense level (rather than at word level). We perform an extensive analysis of the annotations to better understand the distribution of emotions evoked by terms of different parts of speech. We identify which emotions tend to be evoked simultaneously by the same term and show that certain emotions indeed go hand in hand. Since we were additionally interested in determining colour signatures for emotions (Section 7), we chose to annotate all of the 10,170 word-sense pairs that Mohammad and Turney (2010) used to create their word emotion lexicon. For example, Mohammad and Turney (2010) study the affects words can evoke in people's minds, while Bollen et al (2011) study various moods ,e.g., tension, depression, beyond simple dichotomy of positive and negative sentiment.  In this work, we make this transition more explicit and intentional, by introducing a novel connotation lexicon. Mohammad and Turney (2010) focussed on emotion evoked by common words and phrases. Two main differences are: (1) our work aims to discover even more subtle association of words with sentiment, and (2) we present a nearly unsupervised approach, while Mohammad and Turney (2010) explored the use of Mechanical Turk to build the lexicon based on human judgment. For instance, our co-occurrence method is an adaption of a technique applied in sentiment analysis (Turney and Littman, 2003), which has recently been shown to work for formality (Brooke et al, 2010), a dimension of stylistic variation that seems closely related to readability. Taboada et al (2011) validate their sentiment lexicon using crowd sourced judgments of the relative polarity of pairs of words, and in fact crowd sourcing has been applied directly to the creation of emotion lexicons (Mohammad and Turney, 2010). Mohammad and Turney (2010) created a crowd sourced term emotion association lexicon consisting of associations of over 10,000 word-sense pairs with eight emotions joy, sadness, anger, fear, trust, disgust, surprise, and anticipation argued to be the basic and prototypical emotions (Plutchik, 1980). The Mohammad and Turney (2010) lexicon also has associations with positive and negative polarity. According to Mohammad and Turney (2010), adverbs and adjectives are some of the most emotion-inspiring terms. Mohammad and Turney (2010) compiled emotion an notations for about 4000 words with eight emotions (six of Ekman, trust, and anticipation). The questions are phrased exactly as described in Mohammad and Turney (2010). The questions were phrased exactly as described in Mohammad and Turney (2010). If an annotator answers Q1 incorrectly, then in formation obtained from the remaining questions is discarded. Emotional connotation works exactly in the same way, but in this case word-emotion associations are taken from (Mohammad and Turney, 2010).
Discriminative Reranking For Machine Translation This paper describes the application of discriminative reranking techniques to the problem of machine translation. For each sentence in the source language, we obtain from a baseline statistical machine translation system, a ranked best list of candidate translations in the target language. We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation. This approach has successfully been applied in numerous Natural Language Processing (NLP) tasks including syntactic parsing (Collins and Koo, 2005), semantic parsing (Ge and Mooney, 2006), machine translation (Shen et al, 2004), spoken language understanding (Dinarelli et al, 2012), etc. Shen et al (2004) compared different algorithms for tuning the log-linear weights in a re ranking framework and achieved results comparable to the standard minimum error rate training. (Och et al, 2004) and (Shen et al, 2004) describe the use of syntactic features in reranking the output of a full translation system, but the syntactic features give very small gains. The labels for the SVM are derived as in (Shen et al, 2004), where top 10% of hypotheses by smoothed sentence-BLEU is ranked before the bottom 90%. A perceptron like algorithm that handles global features in the context of re-ranking is also presented in (Shen et al., 2004). Moreover, our ranking model is related to reranking (Shen et al, 2004) in SMT as well. Work on discriminative reranking has been reported before by Och and Ney (2002), Och et al (2004), and Shen et al (2004). Traditionally, n-best rerankers (Shen et al, 2004) have applied expensive analysis after the translation process, on both the source and target side, though they suffer from being limited to whatever is on the n-best list (Hasan et al, 2007). It was also used also in (Shen et al, 2004) to re-rank different candidates of the same hypothesis for machine translation. The same reranking schema has been used also in (Shen et al, 2004) for reranking different candidate hypotheses for machine translation. Our modified training procedure is related to the discriminative re-ranking procedure presented in (Shen et al., 2004). Shen et al (2004) and Och et al (2004) presented approaches to re-rank the output of the decoder using syntactic information.
Transformation-Based-Error-Driven Learning And Natural Language Processing: A Case Study In Part-Of-Speech Tagging Recently, there has been a rebirth of empiricism in the field of natural language processing. Manual encoding of linguistic information is being challenged by automated corpus-based learning as a method of providing a natural language processing system with linguistic knowledge. Although corpus-based approaches have been successful in many different areas of natural language processing, it is often the case that these methods capture the linguistic information they are modelling indirectly in large opaque tables of statistics. This can make it difficult to analyze, understand and improve the ability of these approaches to model underlying linguistic behavior. In this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge. This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part-of-speech tagging. In (Brill, 1995) a system of rules which uses both ending-guessing and more morphologically motivated rules is described. Brill (Brill, 1995) outlines a transformation-based learner which learns guessing rules from a pre-tagged training corpus. The other tagger was the rule-based tagger of Brill (Brill, 1995). The task is typically addressed as a sequential tagging problem; one notable exception is the work of Brill (1995), who proposed non-sequential transformation-based learning. For example, Dojchinova and Mihov (2004) mapped their initial tag set of 946 tags to just 40, which allowed them to achieve 95.5% accuracy using the transformation-based learning of Brill (1995), and 98.4% accuracy using manually crafted linguistic rules. We used Brill's tagger (Brill, 1995) and Memory-Based Shallow Parser (Daelemansetal., 1999) to analyze English sentences. A third, clean-up pass is then performed to partially disambiguate the identified WordNet glosses with Brill's part-of-speech tagger (Brill, 1995), which performs with up to 95% accuracy, and eliminates errors introduced into the list by part-of-speech ambiguity of some words acquired in pass 1 and from the seed list. According to (Brill, 1995), a Transformation-Based Error-Driven learning application is defined by 1. The initial annotation scheme 2. The space of allowable transformations 3. The iterative algorithm for choosing a transformation sequence. For example, in a part-of-speech tagging task, the initial an notation may assign each token its most likely tag without any regard to context (Brill, 1995). The addition of a look-ahead searcher has been suggested (Brill, 1995), but we have not seen it implemented in a research context, likely due to the fact that a straightforward implementation of the concept would at minimum square the amount of time required for training. Many alternatives suggest themselves to expand the options, including maximum entropy models, which have been previously successfully applied to, inter alia, sentence boundary detection (Reynar and Ratnaparkhi, 1997), and transformation-based learning, as used in part-of-speech tagging and statistical parsing applications (Brill, 1995). TBL is a machine learning approach that has been employed to solve a number of problems in natural language processing; most famously, it has been used for part-of-speech tagging (Brill, 1995). The most notable of these include the trigram HMM tagger (Brants, 2000), maximum entropy tagger (Ratna park hi, 1996), transformation-based tagger (Brill, 1995), and cyclic dependency networks (Toutanova et al, 2003). A class sequence example Transformation-based learning is a symbolic machine learning method, introduced by (Eric Brill, 1995). There has been a modest amount of previous work on improving probabilistic decision lists, as well as a fair amount of work in related fields, especially in transformation-based learning (Brill, 1995). For instance, in part-of-speech tagging (Brill, 1995), when the tag of one word is changed, it changes the answers to questions for nearby words. First, since probabilistic decision lists are probabilistic analogs of TBLs, we compared to TBL (Brill,1995). Partly inheriting from (Brill, 1995), we applied error-driven learning to filter prefixes in Sp and suffixes in Ss. In line with our assumption of raw text to extract over, we use the Brill tagger (Brill, 1995) to automatically tag the WSJ, rather than making use of the manual POS annotation provided in the Penn Treebank. The expanded set of results are summarised in Table 1, for Transformation Based Learning (TBL) (Brill, 1995).
A Stochastic Finite-State Word-Segmentation Algorithm For Chinese apos;How do you say octopus in Japanese?' (a) EL *AT, X,FP tEl 'EV Plausible Segmentation ri4-wen2 zhangl-yu2 zen3-me0 shuol 'Japanese' octopus' 'how' 'say' Implausible Segmentation ri4 wen2-zhangl yu2 zen3-me0 shuol 'Japan' essay' 'fish' how' 'say' Figure 1 A Chinese sentence in (a) illustrating the lack of word boundaries. In (b) is a plausible segmentation for this sentence; in (c) is an implausible segmentation. orthographic words are thus only a starting point for further analysis and can only be regarded as a useful hint at the desired division of the sentence into words. Whether a language even has orthographic words is largely dependent on the writing system used to represent the language (rather than the language itself); the &quot;orthographic is not universal. Most that use Roman, Armenian, or Semitic scripts, and many use scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writing system, including Chinese and Japanese, as well as Indian-derived writing systems languages like Thai, do not delimit orthographic Put another way, written Chinese simply lacks orthographic words. In Chinese text, individual characters of the script, to which we shall refer by their traditional of are written one after another with no intervening spaces; a Chinese is shown in Figure Partly as a result of this, the notion &quot;word&quot; has never played a role in Chinese philological tradition, and the idea that Chinese lacks anything analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984). Twentieth-century linguistic work on Chinese (Chao 1968; Li and Thompson 1981; Tang 1988,1989, inter alia) has revealed the incorrectness of this traditional view. All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other a word in Chinese may correspond to one or more symbols in the orthog- 1 For a related approach to the problem of word-segmention in Japanese, see Nagata (1994), inter alia. Chinese AV&quot;character'; this is the same word as Japanese 3 Throughout this paper we shall give Chinese examples in traditional orthography, followed by a Romanization into the scheme; numerals following each pinyin syllable represent tones. Examples will usually be accompanied by a translation, plus a morpheme-by-morpheme gloss given in parentheses whenever the translation does not adequately serve this purpose. In the pinyin transliterations a dash (-) separates syllables that may be considered part of the same phonological word; spaces are used to separate plausible phonological words; and a plus sign (4-) is used, where relevant, to indicate morpheme boundaries of interest. 378 Sproat, Shih, Gale and Chang Word-Segmentation for Chinese is a fairly uncontroversial case of a monographemic word, 1131S country) 'China' a fairly uncontroversial case of a digraphemic word. The relevance of the distinction between, say, phonological words say, dictionary words is shown by an example like *1:1 131 zhonglren2-min2 gong4-he2-guo2 people republic) 'People's Republic of China.' Arguably this consists of about three phonological words. On the other hand, in a translation system one probably wants to treat this string as a single dictionary word since it has a conventional and somewhat unpredictable translation into English. if one wants to any purpose—from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide. For example, suppose one is building a TTS system for Mandarin Chinese. For that application, at a minimum, one would want to know the phonological word boundaries. Now, for this application one might be tempted to simply bypass the segmentation problem and pronounce the text character-by-character. However, there are several reasons why this approach will not in general work: 1. Many hanzi have more than one pronunciation, where the correct depends upon word affiliation: Erg is pronounced is a prenominal modification marker, but the word is normally but a person's given name. 2. Some phonological rules depend upon correct word segmentation, including Third Tone Sandhi (Shih 1986), which changes a 3 (low) tone a 2 (rising) tone before another 3 tone: flao3 shu3.1 becomes [ ], than [ 1, the first applies within the word blocking its phrasal application. 3. In various dialects of Mandarin certain phonetic rules apply at the word level. For example, in Northern dialects (such as Beijing), a full tone (1, 2, 3, or 4) is changed to a neutral tone (0) in the final syllable of many melon' is often pronounced The high 1 tone of would not normally neutralize in this fashion if it were functioning as a word on its own. 4. TTS systems in general need to do more than simply compute the pronunciations of individual words; they also need to compute intonational phrase boundaries in long utterances and assign relative prominence to words in those utterances. It has been shown for English (Wang and Hirschberg 1992; Hirschberg 1993; Sproat 1994, inter alia) that grammatical part of speech provides useful information for these tasks. Given that part-of-speech labels are properties of words rather than morphemes, it follows that one cannot do part-of-speech assignment without having access to word-boundary information. Making the reasonable assumption that similar information is relevant for solving these problems in Chinese, it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation. The points enumerated above are particularly related to TTS, but analogous arguments can easily be given for other applications; see for example Wu and Tseng's (1993) discussion of the role of segmentation in information retrieval. There are thus some very good reasons why segmentation into words is an important task. 379 Computational Linguistics Volume 22, Number 3 A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented. For novel texts, no lexicon that consists simply of a list of word entries will ever be entirely satisfactory, since the list will inevitably omit many constructions that should be considered words. Among these are words derived by various productive processes, including: Morphologically derived words such (student+plural) 'students,' which is derived by the affixation of the affix the noun Personal names such as KUM Enlai.' Of course, we can expect famous names like Zhou Enlai's to be in many dictionaries, names such as EA* name of the second author of this paper, will not be found in any dictionary. Transliterated foreign names such as 'Malaysia.' Again, famous place names will most likely be found in the but less well-known names, such as (as in the New Jersey town name 'New Brunswick') will not generally be found. In this paper we present a stochastic finite-state model for segmenting Chinese text into words, both words found in a (static) lexicon as well as words derived via the above-mentioned productive processes. The segmenter handles the grouping of hanzi into words and outputs word pronunciations, with default pronunciations for hanzi it cannot group; we focus here primarily on the system's ability to segment text appropriately (rather than on its pronunciation abilities). The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers. It also incorporates the Good-Turing method (Baayen 1989; Church and Gale 1991) in estimating the likelihoods of previously unseen constructions, including morphological derivatives and personal names. We will evaluate of the segmentation, as well as the performance. This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text. Finally, this effort is part of a much larger program that we are undertaking to develop stochastic finite-state methods for text analysis with applications to TTS and other areas; in the final section of this paper we will briefly discuss this larger program so as to situate the work discussed here in a broader context. [Sproat et al, 1996] built a word uni gram model using the Viterbi re-estimation whose initial estimates were derived from the frequencies in the corpus of the strings of each word in the lexicon. [Sproat et al, 1996] wrote lexical rules for each productive morphological process, such as plural noun formation, Chinese personal names, and transliterations of foreign words. We used a simple greedy algorithm described in [Sproat et al, 1996]. [Sproat et al, 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus. Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al, 1996]. Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al, 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese. In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996). One example of such approaches is Sproat et al (1996), which is based on weighted finite-state transducers (FSTs). Sproat et al (1996) also studied such problems (with the same example) and uses weighted FSTs to deal with the affixation. As described in Sproat et al (1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name. Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al, 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting. Chinese According to Sproat et al (1996), most prior work in Chinese segmentation has exploited lexical knowledge bases.   Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996). Another difference from our model is the rule-based sub-model, which uses a dictionary-based forward maximum match method described by Sproat et al (1996). As discussed elsewhere (Sproat et al, 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese. The actual implementation of the weighted finite state transducer by Sproat et al (1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use. what Sproat et al (1996) implemented was simply a token unigram scoring function. First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al, 1996).
Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not. This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure. We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines. Another popular task in SMT is domain adaptation (Foster et al, 2010). In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010). Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010). Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models. Our main technical contributions are as follows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation. Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) extend this approach by weighting individual phrase pairs. Foster et al (2010) combine the two, applying linear interpolation to combine the instance weighted out-of-domain model with an in-domain model. Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE? EN and 1:5 for HT? EN); Previous research has been performed with ratios of 1:100 (Foster et al 2010) or 1:400 (Axelrod et al 2011). We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translation models. In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT. Our technique for setting ? m is similar to that outlined in Foster et al (2010). For efficiency and stability, we use the EM algorithm to find ?, rather than L-BFGS as in (Foster et al., 2010). Foster et al (2010), however, uses a different approach to select related sentences from OUT. Foster et al (2010) propose a similar method for machine translation that uses features to capture degrees of generality. As in (Foster et al, 2010), this approach works at the level of phrase pairs. The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010). Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and report a decrease in performance. Foster et al (2010) further perform this on extracted phrase pairs, not just sentences. To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al, 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al, 2010), which also show further improvement for new phrase feature learning in our experiments.
Modeling Local Coherence: An Entity-Based Approach This article proposes a novel framework for representing and measuring local coherence. Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text. The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities. We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment. Barzilay and Lapata (2008) presented early work in investigating the use of discourse to distinguish abridged from original encyclopedia articles. We adopt Barzilay and Lapata (2008)'s entity based local coherence model to represent a document by an entity grid, and extract local transitions among entities in continuous discourse constituents. A prominent example is the entity-based model by Barzilay and Lapata (2008). Adapted from the introduction to Barzilay and Lapata (2008). We follow Barzilay and Lapata (2008) and use the Fisher Sign test. The entity-based coherence model, proposed by Barzilay and Lapata (2008), is one of the most popular statistical models of inter-sentential coherence, and learns coherence properties similar to those employed by Centering Theory (Grosz et al, 1995). Their local model of discourse coherence is based on the entity-grid (Barzilay and Lapata, 2008), as well as on the lexicalized IBM model (see Section 4.6 above); we have experimented with both, and showed that they have a minimal effect on grading performance with the FCE dataset. First, we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata (2008) more than grammatical role and simple clausal order information do, particularly when manual annotations of this information are not available. Barzilay and Lapata (2008) introduce the entity grid as a method of representing the coherence of a document. In Barzilay and Lapata (2008), an entity grid is constructed for each document, and is represented as a matrix in which each row represents a sentence, and each column represents an entity. We test a version of the entity grid representation augmented with topological fields in a sentence ordering experiment corresponding to Experiment 1 of Barzilay and Lapata (2008). This set is larger than the set that was used in Experiment 1 of Barzilay and Lapata (2008), which consists of 400 documents in two English subcorpora on earthquakes and accidents respectively. Barzilay and Lapata (2008) found that grammatical role improves performance in this task for an English corpus. The results we obtain are higher than the results for the English corpora of Barzilay and Lapata (2008) (87.2% on the Earthquakes corpus and 90.4% on the Accidents corpus), but this is probably due to corpus differences as well as the availability of perfect coreference information in our experiments. Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. We extend the original entity-based coherence model (Barzilay and Lapata, 2008) by learning from more fine-grained coherence preferences in training data. We show that our multiple-rank model outperforms B & L's basic model on two tasks, sentence ordering and summary coherence rating, evaluated on the same datasets as in Barzilay and Lapata (2008). For entity extraction, Barzilay and Lapata (2008) had two conditions: Coreference + and Coreference -. Two evaluation tasks for Barzilay and Lapata (2008)'s entity-based model are sentence ordering and summary coherence rating. Barzilay and Lapata (2008) experimented on two datasets: news articles on the topic of earthquakes (Earthquakes) and narratives on the topic of aviation accidents (Accidents).
Parallel Implementations of Word Alignment Tool Training word alignment models on large corpora is a very time-consuming processes. This paper describes two parallel implementations of GIZA++ that accelerate this word alignment process. One of the implementations runs on computer clusters, the other runs on multi-processor system using multi-threading technology. Results show a near-linear speedup according to the number of CPUs used, and alignment quality is preserved. The experiments led on the alignment methods were evaluated on the development corpus using MGIZA++ (Gao and Vogel, 2008), a multi-thread version of GIZA++ (Och and Ney, 2003) which also allows previously trained IBM alignments models to be applied on the development and test corpora. We word-aligned the corpus with MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of the standard word alignment tool GIZA++ (Och and Ney, 2003). The parallel corpus was then word-aligned using MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). We extend the multi-thread GIZA++ (Gao and Vogel, 2008) to load the alignments from a modified corpus file. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). This speeds up the process and corrects an error of GIZA++ that can appear with rare words. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). This speeds up the process and corrects an error of GIZA++ that can appear with rare words. We use an extended version of MGIZA++ (Gaoand Vogel, 2008) to perform the constrained semi supervised word alignment. is the translation probability score (as the one given for instance by GIZA++ (Gao and Vogel, 2008)). Then, we applied a parallel version of GIZA++ (Gao and Vogel, 2008) that gave us the translation dictionaries of content words only (nouns, verbs, adjective and adverbs) at word form level. We will continue exploration on these directions. The extended GIZA++ is released to the research community as a branch of MGIZA++ (Gao and Vogel, 2008), which is available online. In this section, we will explain how to build a transliteration module on the extracted transliteration pairs and how to integrate it into MGIZA++ (Gao and Vogel, 2008) by interpolating it with the t table probabilities of the IBM models and the HMM model. Unidirectional word alignments were provided by MGIZA++ (Gao and Vogel, 2008), then symmetrized with the grow-diag-final-and heuristic (Koehn et al, 2005). For the word alignments, we chose MGIZA (Gaoand Vogel, 2008), using seven threads per MGIZA instance, with the parallel option ,i.e. one MGIZA in stance per pair direction running in parallel. This is largest publicly available parallel corpus, and it does strain computing resources, for instance forcing us to use multi-threaded GIZA++ (Gao and Vogel, 2008). Table 7 shows the gains obtained from using this corpus in both the translation model and the language model opposed to a baseline system trained with otherwise the same settings. The parallel sentences are aligned using MGIZA++ (Gao and Vogel, 2008) and then the proposed rule extraction algorithm was used in extracting the SRL-aware SCFG rules. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. Word alignment scores: source-target and target-source MGIZA++ (Gao and Vogel, 2008) force-alignment scores using IBM Model 4 (Och and Ney, 2003). The system translates from cased French to cased English; at no point do we lowercase data. The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003). We use an extended version of MGIZA++ (Gaoand Vogel, 2008) to perform the constrained semi supervised word alignment. For this, we create a parallel corpus consisting of n translation hypotheses and n copies of the corresponding source text, both lowercased and detokenized. We compute the word alignment with MGIZA++ (Gao and Vogel, 2008), based on the word alignment model from the primary corpus that we have previously saved to disk. After training a phrase table from the word aligned corpus with Moses, the lexical weights and translation probabilities are rescored, using the sufficient statistics (i.e. the word, phrase and word/phrase pair counts) of both the primary and the secondary corpus.
The Design Of A Computer Language For Linguistic Information A considerable body of accumulated knowledge about the design of languages for communicating information to computers has been derived from the subfields of programming language design and semantics. It has been the goal of the PATR group at SRI to utilize a relevant portion of this knowledge in implementing tools to facilitate communication of linguistic information to computers. The PATR-TI formalism is our current computer language for encoding linguistic information. This paper, a brief overview of that formalism, attempts to explicate our design decisions in terms of a set of properties that effective computer languages should incorporate. Indeed, we have demonstrated the feasibility (Alshawi et al, 1985) of driving a parsing system directly from the information available in LDOCE by constructing dictionary entries for the PATR-H system (Shieber, 1984). We borrow the terminology and notation of PATR-II (Shieber, 1984), a minimal constraint-based formalism that extends context-free grammar. Nearly all existing unification grammars of this kind use either term unification (the kind of unification used in resolution theorem provers, and hence provided as a primitive in PROLOG) or some version of the graph unification proposed by Kay (1985) and Shieber (1984). It is not, unfortunately, possible to keep it close to both FUG and PATR (Shieber 1984), but it should be possible for readers familiar with PATR to see roughly what the relation between the two is.  Since formalisms (i) are used in the family of the PATR parsing systems (Shieber, 1984), hereafter they will be called PATR-like formalisms.
Deterministic Dependency Parsing Of English Text This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time. When trainedand evaluated on the Wall Street Journal sec tion of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%. Unlikemost previous systems, the parser produces la beled dependency graphs, using as arc labels a combination of bracket labels and grammaticalrole labels taken from the Penn Treebank II annotation scheme. The best overall accuracy ob tained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels). Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al (2005). In this paper we fill a gap in the CCG literature by developing a shift reduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English). Memory-based learning (MBL), which is based on the idea that learning is the simple storage of experiences in memory and that solving a new problem is achieved by reusing solutions from similar previously solved problems (Daelemans and Van den Bosch, 2005), has been used primarily by Nivre et al (2004), Nivre and Scholz (2004), and Sagae and Lavie (2005). To constrain our reorderings, we first produce a parse tree, using a dependency parser similar to that of Nivre and Scholz (2004). Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear. For instance, Nivre and Scholz presented a deterministic dependency parser trained by memory-based learning (Nivre and Scholz, 2004). We built a parser based on the deterministic algorithm of Nivre and Scholz (Nivre and Scholz, 2004) as a base dependency parser.  The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz,2004) has been shown to offer state-of-the-art accuracy (Nivre et al, 2006) with high efficiency due to a greedy search strategy. This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack. Nivre's parser has been tested for Swedish (Nivre et al, 2004), English (Nivre and Scholz, 2004), Czech (Nivre and Nilsson, 2005), Bulgarian (Marinov and Nivre, 2005) and Chinese Cheng et al (2005), while McDonald? s parser has been applied to English (McDonald et al, 2005a), Czech (McDonald et al, 2005b) and, very recently, Danish (McDonald and Pereira, 2006). Nivre and Scholz (2004) uses this term with reference to Yamada and Matsumoto (2003), whose parser has to find all children of a token before it can attach that token to its head. We will refer to this as bottom-up-trees. Johansson and Nugues (2006) describe a non-deterministic implementation to the dependency parser outlined by Nivre and Scholz (2004), where they apply an n-best beam search strategy. For a highly constrained unification-based formalism like HPSG, a deterministic parsing strategy could frequently lead to parse failures. We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. That algorithm, in turn, is similar to the dependency parsing algorithm of Nivre and Scholz (2004), but it builds a constituent tree and a dependency tree simultaneously. The dependency structure for Thai is more flexible than some languages like Japanese (Sekine et al, 2000), Turkish (Eryigit and Oflazer, 2006), while it is close to Chinese (Cheng et al, 2005) and English (Nivre and Scholz, 2004). Compared with greedy local-search (Nivre and Scholz, 2004), the use of a beam allows the parser to explore a larger search space and delay difficult ambiguity-resolving decisions by considering multiple items in parallel. Features Used for Selecting Reduce The features used in (Nivre and Scholz, 2004) to define a state transition are basically obtained from the two target words wi and wj, and their related words. There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007).
Evaluating The Accuracy Of An Unlexicalized Statistical Parser On The PARC DepBank We evaluate the accuracy of an unlexicalized statistical parser, trained on 4K treebanked sentences from balanced data and tested on the PARC DepBank. We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly tuned without reliance on large in-domain manuallyconstructed treebanks. This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure. The comparison of systems using DepBank is not straightforward, so we extend and validate DepBank and highlight a number of representation and scoring issues for relational evaluation schemes. Briscoe and Carroll (2006) discuss issues raised by this re annotation. Briscoe and Carroll (2006) show that the system has equivalent accuracy to the PARC XLE parser when the morphosyntactic features in the original DepBank gold standard are taken into account. To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) re annotation of DepBank (King et al, 2003), as described in Clark and Curran (2007a). This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure. Four sets are English text: jh5 described in Section 3; trec consisting of questions from TREC and included in the tree banks released with the ERG; a00 which is taken from the BNC and consists of fact sheets and newsletters; and depbank, the 700 sentences of the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006) taken from the Wall Street Journal. The test sets were POS-tagged and lemmatized using RASP (Briscoe and Carroll, 2006). The lemmatization was done by RASP (Briscoe and Carroll, 2006). We use the Briscoe and Carroll (2006) version of DepBank, a 560 sentence subset used to evaluate the RASP parser. For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al, 2003) in the style of Briscoe and Carroll (2006). The first, Wiki 300, for testing accuracy, consists of 300 sentences manually annotated with grammatical relations (GRs) in the style of Briscoe and Carroll (2006).  Of course, it is always possible to look at accuracy figures by dependency type in order to understand what a parser is good at, as recommended by Briscoe and Carroll (2006), but it is also desirable to have a single score reflecting the overall accuracy of a parser, which means that the construction's overall contribution to the score is relevant.  C&C parser provides CCG predicate-argument dependencies and Briscoe and Carroll (2006) style grammatical relations. In this paper we evaluate a CCG parser (Clarkand Curran, 2004b) on the Briscoe and Carrollversion of DepBank (Briscoe and Carroll, 2006). Briscoe and Carroll (2006) re annotated this resource using their GRs scheme, and used it to evaluate the RASP parser. For the gold standard we chose the version of Dep Bank re annotated by Briscoe and Carroll (2006), consisting of 700 sentences from Section 23 of the Penn Treebank. The B&C scheme is similar to the original DepBank scheme (King et al, 2003), but overall contains less grammatical detail; Briscoe and Carroll (2006) describes the differences. The GRs are described in Briscoe and Carroll (2006) and Briscoe et al (2006). The GRs are arranged in a hierarchy, with those in Table 1 at the leaves; a small number of more general GRs subsume these (Briscoe and Carroll, 2006).
Sentence Compression Beyond Word Deletion In this paper we generalise the sentence compression task. Rather than sim ply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion. We present a new corpus that is suitedto our task and a discriminative tree-to tree transduction model that can naturallyaccount for structural and lexical mis matches. The model incorporates a novelgrammar extraction method, uses a lan guage model for coherent output, and canbe easily tuned to a wide range of compres sion specific loss functions. Different from prior research, Cohn and Lapata (2008) achieved sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process.  For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al, 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996). Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions). Although abstractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009). Cohn and Lapata (2008) have also developed an abstractive version of T3, which was reported to outperform the original, extractive T3 in meaning preservation; there was no statistically significant difference in grammaticality. Finally, Nomoto (2009) presented a two-stage extractive method. Our approach follows Cohn and Lapata (2008), who expand the task to include substitutions, insertions and reorderings that are automatically learned from parallel texts. We compared the Gibbs sampling compressor (GS) against a version of maximum a posteriori EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM). The comparison system described by Cohn and Lapata (2008) attempts to solve a more general problem than ours, abstractive sentence compression. In our experiments with the publicly available SVM system we used all except para phrasal rules extracted from bilingual corpora (Cohn and Lapata, 2008). For example, a different compression model could incorporate rewriting rules to enable compressions that go beyond word deletion, as in Cohn and Lapata (2008). Another future direction is to extend our ILP formulations to more sophisticated models that go beyond word deletion, like the ones proposed by Cohn and Lapata (2008).  Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001). The first abstractive compression method was proposed by Cohn and Lapata (2008). The dataset that Cohn and Lapata (2008) used to learn transduction rules consists of 570 pairs of source sentences and abstractive compressions. We did not use source sentences from the other 30 documents, because they were used by Cohn and Lapata (2008) to build their abstractive dataset (Section 2), from which we drew source sentences for our dataset. This work introduces a model free approach to sentence compression, which grew out of ideas from Nomoto (2008), and examines how it com pares to a state-of-art model intensive approach known as Tree-to-Tree Transducer, or T3 (Cohn and Lapata, 2008). 
Labeled Pseudo-Projective Dependency Parsing With Support Vector Machines We use SVM classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion. Non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser. We present evaluation results and an error analysis focusing on Swedish and Turkish. The pseudo-projective approach (Nivre and Nilsson, 2005): Transform non-projective training trees to projective ones but encode the information necessary to make the inverse transformation in the DEPREL, so that this inverse transformation can also be carried out on the test trees (Nivre et al, 2006). Table 5 shows the official results for submitted parser outputs. The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006). Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences. Firstto Third-Order Features The feature templates of first to third-order features are mainly drawn from previous work on graph based parsing (McDonald and Pereira, 2006), transition-based parsing (Nivre et al, 2006) and dual decomposition-based parsing (Martins et al, 2011). includes the most accurate parsers among Nivre et al (2006), McDonald et al (2006), Martins et al (2010), Martins et al (2011), Martins et al (2013), Koo et al (2010), Rush and Petrov (2012), Zhang and McDonald (2012) and Zhang et al (2013). Both methods are sometimes combined (Nivre et al, 2006b). We represented features with a parameter format partly inspired by MaltParser (Nivre et al, 2006a). These parameters are identical to Nivre et al (2006b) to enable a comparison of the scores. We evaluated the feature candidates on a development set using the labeled and unlabeled attachment scores (LAS and UAS) that we computed with the eval.pl script from CoNLL-X. Following (Nivre et al, 2006), the encoding scheme called Head in (Nivre and Nilsson, 2005) was used to encode the original non-projective dependencies in the labels of the projectivized dependency tree. We used the base feature model defined in (Nivre et al, 2006) for all the languages but Arabic, Chinese, Czech, and Turkish. For Arabic, Chinese, and Czech, we used the same feature models used in the CoNLL-X 948 shared task by (Nivre et al, 2006), and for Turkish we used again the base feature model but extended it with a single feature: the part-of-speech tag of the token preceding the current top of the stack. Talbanken05 is a Swedish tree bank converted to dependency format, containing both written and spoken language (Nivre et al, 2006a). For each token, Talbanken05 contains information on word form, part of speech, head and dependency relation, as well as various morphosyntactic and/orlexical semantic features. As our baseline, we use the settings optimized for Swedish in the CoNLL-X shared task (Nivre et al,2006b), where this parser was the best performing parser for Swedish. MaltParser (Nivre et al, 2006) is a language independent system for data-driven dependency parsing which is freely available. It is based on a deterministic parsing strategy in combination with tree bank-induced classifiers for predicting parsing actions. For the training of the Malt parser model that we use in the stacking experiments, we use learner and parser settings identical to the ones optimized for German in the CoNLL-X shared task (Nivre et al, 2006). We use Nivre et al, (2006)'s dependency parser. Unigram label information has been used in MaltParser (Nivre et al, 2006a; Nivre, 2006). In principle, it is restricted to projective dependency forests, but it can be used in conjunction with the pseudo-projective transformation (Nivre et al2006) in order to capture a restricted subset of non projective forests. Table 4: Comparison of the parsing accuracy (LAS and UAS, excluding punctuation) of Nivre's arc-eager parser with projective buffer transitions (NE+LBA/RBA) and the parser with the pseudo-projective transformation (Nivre et al2006) decide between both with the restricted feature in formation available for buffer nodes. To further put the obtained results into context, Table 4 compares the performance of the arc-eager parser with the projective buffer transition most suitable for each dataset with the results obtained by the parser with the pseudo-projective transformation by Nivre et al2006) in the CoNLL-X shared task, one of the top two performing systems in that event.
Concise Integer Linear Programming Formulations for Dependency Parsing We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linear program. Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data. In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearlyprojective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods. ILPs have since been used successfully in many NLP applications involving complex structures Punyakanok et al (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al (2009) for dependency parsing and several others.  Finally, we follow Martins et al (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head).   Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. Although Martins et al (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs. The multicommodity flow formulation was introduced by Martins et al (2009a), along with some of the parts considered here. We present a unified view of two state-of-the art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al (2009). In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al, 2009). The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). Recall that (i) Smith and Eisner (2008) proposed a afactor graph (Fig. 1) in which they run loopy BP, and that (ii) Martins et al (2009) approximate parsing as the solution of a linear program. Let zA , hzaia∈A; the local agreement constraints at the TREE factor (see Table 1) are written as zA? Ztree (x), where Ztree (x) is the arborescence polytope, i.e., the convex hull of all incidence vectors of dependency trees (Martins et al, 2009). Figure 3: Details of the factor graph underlying the parser of Martins et al (2009). We now turn to the concise integer LP formulation of Martins et al (2009). (20) This is exactly the LP relaxation considered by Mar tins et al (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features. They also considered a configuration with non-projectivity features which fire if an arc is non-projective. That configuration can also be obtained here if variables{ n? h, m?} are To be precise, the constraints of Martins et al (2009) are recovered after eliminating the path variables, via Eqs. In sum, although the approaches of Smith and Eisner (2008) and Martins et al (2009) look very different, in reality both are variational approximations emanating from Prop.  Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). Past work in dependency parsing considered either (i) a few "large" components, such as trees and head automata (Smith and Eisner, 2008; Koo et al., 2010), or (ii) many "small" components, coming from a multi-commodity flow formulation (Martins et al, 2009, 2011).
Noun Phrase Coreference As Clustering This paper introduces a new, unsupervised algorithm for noun phrase coreference resolution. It differs from existing methods in that it views coreference resolution as a clustering task. In an evaluation on the MUC-6 coreference resolution corpus, the algorithm achieves an F-measure of 53.6%, placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation. More importantly, the clustering approach outperforms the only MUC-6 system to treat coreference resolution as a learning problem. The clustering algorithm appears to provide a flexible mechanism for coordinating the application of context-independent and context-dependent constraints and preferences for accurate partitioning of noun phrases into coreference equivalence classes. Cardie and Wagstaff (1999) combined the use of WordNet with proper name gazetteers in order to obtain information on the compatibility of coreferential NPs in their clustering algorithm. Cardie and Wagstaff (1999) describe an unsupervised clustering approach to noun phrase coreference resolution in which features are assigned to single noun phrases only. The feature semantic class used by Cardie and Wagstaff (1999) seems to be a domain-dependent one which can only be used for the MUC domain and similar ones. Cardie and Wagstaff (1999) report a performance of 53.6% F-measure (evaluated according to Vilain et al (1995)). However, the cost is the decrease in performance to about 53% F-measure on the same data (Cardie and Wagstaff, 1999) which may be unsuitable for a lot of tasks. Although approaches to coreference resolution that rely only on clustering could easily enforce transitivity (as in Cardie and Wagstaff (1999)), they have not performed as well as state-of-the-art approaches to coreference.  Cardie and Wagstaff (1999) have proposed an unsupervised approach which also incorporates cluster information into consideration. The closest comparable unsupervised system is Cardie and Wagstaff (1999) who use pairwise NP distances to cluster document mentions. Similarly, approaches to coreference resolution (Cardie and Wagstaff, 1999) use clustering to identify groups of references to the same entity. The system of Cardie and Wagstaff (1999) uses the node distance in WordNet (with an upper limit of 4) as one component in the distance measure that guides their clustering algorithm. For the first subtask we use the same set of features as in Cardie and Wagstaff (1999). Coreference resolution on text datasets is well studied (e.g., (Cardie and Wagstaff, 1999)). We employ a set of verbal features that is similar to the features used by state-of-the-art coreference resolution systems that operate on text (e.g., (Cardieand Wagstaff, 1999)). Coreference resolution is often performed in two phases: a binary classification phase, in which the likelihood of coreference for each pair of noun phrases is assessed; and a partitioning phase, in which the clusters of mutually coreferring NPs are formed, maximizing some global criterion (Cardie and Wagstaff, 1999). The verbal features that we have included are a representative sample from the literature (e.g., (Cardie and Wagstaff, 1999)). An unsupervised approach to the resolution of definite NPs was applied by Cardie and Wagstaff (1999). Cardie and Wagstaff (1999) describe an unsupervised clustering approach to noun phrase coreference resolution in which features are assigned to single noun phrases only. The feature semantic class used by Cardie and Wagstaff (1999) seems to be a domain-dependent one which can only be used for the MUC domain and similar ones. Cardie and Wagstaff (1999) report a performance of 53.6% F-measure (evaluated according to Vilain et al (1995)).
Mining The Web For Bilingual Text STRAND (Resnik, 1998) is a languageindependent system for automatic discovery of text in parallel translation on the World Wide Web. This paper extends the preliminary STRAND results by adding automatic language identification, scaling up by orders of magnitude, and formally evaluating performance. The most recent end-product is an automatically acquired parallel corpus comprising 2491 English-French document pairs, approximately 1.5 million words per language. This is implemented by Resnik and described in detail in [Resnik, 1999], calculates the similarity between all the words' senses of words in a set. Finally, I show that this algorithm performs competitively with the approach of Resnik (1999), in which only Association for Computational Linguistics. The STRAND system (Resnik, 1999), for example, uses structural markup information from the pages, without looking at their content, to attempt to align them. These were the same pairs for which human evaluations were carried out by Resnik (1999). The STRAND scores are similar to those published by Resnik (1999). It is capable of pulling parallel texts out of a large multilingual collection, and it rivals the performance of structure-based approaches to pair classification (Resnik, 1999), having better agreement with human judges. Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest. The lack of large-scale parallel corpora no doubt has impeded progress in this direction, although attempts have been made to mine parallel corpora from the Web (Resnik, 1999). First, parallel corpora, especially accurately aligned parallel corpora are rare, although attempts have been made to mine them from the Web (Resnik, 1999). Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the web to generate corpora for languages where electronic resources are scarce, while Resnik (1999) describes a method for mining the web for bilingual texts.  Resnik (1999) mined comparable corpora on the assumption that the pages which are comparable of each other share a similar structure (headers, paragraphs, etc.) when text is presented in many languages in the Web. Grefenstette (1999) used the Web for example-based ma chine translation; Kilgarriff (2001) investigated the type of noise in Web data; Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) used it as an additional resource for word sense disambiguation; Resnik (1999) mined the Web for bilingual texts; Turney (2001) used Web frequency counts to compute information retrieval-based mutual-information scores. Mining the Web for bilingual text (Resnik, 1999) is not likely to provide sufficient quantities of high quality data. For example, Table 5 shows the abstract of Resnik (1999) and 5 selected sentences that cite it in AAN. Many research ideas have exploited the Web in unsupervised or weakly supervised algorithms for natural language processing (e.g., Resnik (1999)). In Resnik (1999), the Web is harvested in search of pages that are available in two languages, with the aim of building parallel corpora for any pair of target languages. Mining the Web for bilingual text (Resnik, 1999) is not likely to provide sufficient quantities of high quality data. The STRAND system of (Resnik, 1999), uses structural markup information from the pages, without looking at their content, to attempt to align them. Mining the Web for bilingual text (Resnik, 1999) is not likely to provide sufficient quantities of high quality data.
A Topic Model for Word Sense Disambiguation We develop latent Dirichlet alocation with WORDNET (LDAWN), an unsupervised probabilistic topic model that includes word sense as a hidden variable. We develop a probabilistic posterior inference algorithm for simultaneously disambiguating a corpusand learning the domains in which to consider each word. Using the WORDNET hierarchy, we embed the construction of Ab ney and Light (1999) in the topic model and show that automatically learned domainsimprove WSD accuracy compared to alter native contexts. Work such as (Cai et al, 2007) or (Boyd-Graber et al., 2007) use the document-level topics extracted with Latent Dirichlet Allocation (LDA) as indicators of meanings for word sense disambiguation. Abney and Light (1999) used tree structured multinomials to model selectional restrictions, which was later put into a Bayesian context for topic modeling (Boyd-Graber et al, 2007). Boyd-Graber et al (2007) integrate a model of random walks on the WordNet graph into an LDA topic model to build an unsupervised word sense disambiguation system. Boyd-Graber et al (2007) describe a related topic model, LDAWN, for word sense disambiguation that adds an intermediate layer of latent variables Z on which the Markov model parameters are conditioned. Gibbs sampling updates for LDAWN are given in Boyd-Graber et al (2007). Moreover, SemCor is also the dataset used in (Boyd-Graber et al, 2007), where a WordNet based topic model for WSD is introduced. STM10 achieves similar results as in LDAWN (Boyd-Graber et al, 2007) which was specifically designed for WSD. Boyd-Graber et al (2007) are the first to integrate semantics into the topic model framework. Previous approaches using topic models for sense disambiguation either embed topic features in a supervised model (Cai et al, 2007) or rely heavily on the structure of hierarchical lexicons such as WordNet (Boyd-Graber et al, 2007). In another unsupervised system, Boyd-Graber et al (2007) enhance the basic LDA algorithm by incorporating WordNet senses as an additional latent variable. This dataset, which consists of 320,000 articles, is significantly larger than SemCor, which is the dataset used by Boyd-Graber et al (2007). Instead, we appeal to tree-based extensions of the Dirichlet distribution, which has been used to induce correlation in semantic ontologies (Boyd-Graberetal., 2007) and to encode clustering constraints (Andrzejewski et al, 2009). We took these frequencies and propagated them through the multilingual hierarchy, following LDAWN's (Boyd-Graber et al., 2007) formulation of information content (Resnik, 1995) as a Bayesian prior. LDAWN (Boyd-Graber et al 2007) models sets of words for the word sense disambiguation task.
The Third PASCAL Recognizing Textual Entailment Challenge We would like to thank the people and organizations that made these sources available for the challenge. In addition, we thank Idan Szpektor and Roy Bar Haim from Bar-Ilan University for their assistance and advice, and Valentina Bruseghini CELCT for managing the RTE-3 We would also like to acknowledge the people and organizations involved in creating and annotating the data: Pamela Forner, Errol Hayman, Cameron Fordyce from CELCT and Courtenay Hendricks, Adam Savel and Annika Hamalainen This work was supported in part by the IST Programme of the European Community, under the Network of IST-2002- 506778. We wish to thank the managers of the PASCAL challenges program, Michele Sebag and Florence d’Alche-Buc, for their efforts and support, which made this challenge possible. We also thank David Askey, who helped manage the RTE 3 website. This type of reasoning has been identified as a core semantic inference paradigm by the generic Textual Entailment framework (Giampiccolo et al, 2007). The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al,2007). Recognizing textual entailment is to determine whether a sentence (sometimes a short paragraph) can entail the other sentence (Giampiccolo et al, 2007). The average accuracy of the systems in the RTE-3 challenge is around 61% (Giampiccolo et al, 2007). Of course, if examples were also annotated with explanations in a consistent format, this could form the basis of a new evaluation of the kind essayed in the pilot study in (Giampiccolo et al, 2007). We used the data from three recognizing textual entailment challenge: RTE2 (Bar-Haim et al, 2006), RTE3 (Giampiccolo et al, 2007), and RTE5, along with the standard split between training and test sets. For example, the best systems in RTE2 and RTE3 (Giampiccolo et al, 2007) have an accuracy 10% higher than the others but they generally use resources that are not publicly available. These features include whether the two strings are the same, two terms have the same stem, the similarity between the two terms either based on WordNet or distributional statistics (Lin, 1998). To learn the alignment model for nouns, we annotated the noun alignments for the development data used in PASCAL RTE-3 Challenge (Giampiccolo et al., 2007) and trained a logistic regression model based on the features in Table 1. A first step towards a more comprehensive notion of entailment was taken with RTE-3 (Giampiccolo et al,2007), when paragraph-length texts were first included and constituted 17% of the texts in the test set. Typical examples of such relations are given in (Giampiccolo et al, 2007) or those holding between question and answer. As a second application-oriented evaluation we measured the contributions of our (filtered) Wikipedia resource and WordNet to RTE inference (Giampiccolo et al, 2007). In RTE-3 (Giampiccolo et al, 2007), where some paragraph-long texts were included, inter sentential relations became relevant for correct inference. For semantically oriented tools such as SRL systems, it is important to also assess their results w.r.t. the task which they are meant support namely reasoning: Do the semantic representations built by SRL help in making the correct inferences? Can they be used, for instance, to determine whether a given sentence answers a given question? or whether the content of one sentence follow from that another? As explained in (Giampiccolo et al, 2007), entailment recognition is a first, major step towards answering these questions.  see the reports on RTE-1 (Dagan et al, 2005), RTE-2 (Bar-Haim et al, 2006), RTE-3 (Giampiccolo et al, 2007), the RTE-3 PILOT (Voorhees, 2008), RTE-4 (Giampicolo et al, 2008), and RTE-5 (TAC, 2009) The problem of bias is quite general and widely known. Given text T and hypothesis H, the task consists on determining whether or not H can be inferred by T (Giampiccolo et al, 2007). CSR axioms Several examples of the RTE3challenge can be solved by applying CSR (Table 5). The rest of this section depicts the axioms involved in detecting entailment for each pair. The RTE main task addressed this issue by including a candidate entailment pair in the test set only if multiple annotators agreed on its disposition (Giampiccolo et al, 2007). RTE organizers reported an agreement rate of about 88% among their annotators for the two-way task (Giampiccolo et al, 2007). Second, the ranking may be combined with target language information in order to choose the best translation, thus improving translation quality. We position the problem of generating alternative texts for translation within the Textual Entailment (TE) framework (Giampiccolo et al, 2007). Textual Entailment (TE) has recently become a prominent paradigm for modeling semantic inference, capturing the needs of a broad range of text understanding applications (Giampiccolo et al., 2007).
Nonconcatenative Finite-State Morphology the last few years, so called in general, and morphology in particular, have become widely accepted as paradigms for the computational treatment of morphology. Finite-state morphology appeals to the notion of a finite-state transducer, which is simply a classical finite-state automaton whose transitions are labeled with pairs, rather than with single symbols. The automaton operates on a pair of tapes and advances over a given transition if the current symbols on the tapes match the pair on the transition. One member of the pair of symbols on a transition can be the designated null symbol, which we will write c. When this appears, the corresponding tape is not examined, and it does not advance as the machine moves to the next state. Finite-state morphology originally arose out of a desire to provide ways of analyzing surface forms using grammars expressed in terms of systems of ordered rewriting rules. Kaplan and Kay (in preparation) observed, that finite-state transducers could be used to mimic a large class of rewriting rules, possibly including all those for phonology. The importance of came from two considerations. First, transducers are indifferent as to the direction in which they are applied. In other words, they can be used with equal facility to translate between tapes, in either direction, to accept or reject pairs of tapes, or to generate pairs of tapes. Second, a pair of transducers with one tape in common is equivalent to a single transducer operating on the remaining pair of tapes. A simple algorithm exists for constructing the transition diagram for composite machine given those of the original pair. By repeated application of this algorithm, it is therefore possible to reduce a cascade of transducers, each linked to the next by a common tape, to a single transducer which accepts exactly the same pair of tapes as was accepted by the original cascade as a whole. From these two facts together, it follows that an arbitrary ordered set of rewriting rules can be modeled by a finite-state transducer which can be automatically constructed from them and which serves as well for analyzing surface forms as for generating them from underlying lexical strings. A transducer obtained from an ordered set of in the way just outlined is a level the sense that mediates directly between lexical and surface forms without ever the intermediate forms would arise in the course of applying the original rules by one. The term morphology, is used a more restricted way, to apply to a system in which no intermediate forms are posited, even in the original grammatical formalism. The writer of a grammar using a two-level formalism never needs to think in terms of any representations other than the lexical and the surface ones. What he does is to specify, using one formalism or another, a set of transducers, each of which mediates directly between these forms and each of which restricts the allowable pairs of strings in some way. The pairs that the system as a whole accepts are those are those that are rejected by none of the component transducers, modulo certain assumptions about way in they interact, whose details need not concern us. Once again, there is a formal procedure that can be used to combine set transducers that make up such a system 2 into a single automaton with the same overall so that the final result indistinguishable form that obtained from a set of ordered rules. However it is an advantage of parallel machines that they can be used with very little loss of efficiency without combining them in this way. While it is not the purpose of this paper to explore the formal properties of finite-state transducers, a brief excursion may be in order at this point to forestall a possible objection to the claim that a parallel configuration of transducers can be combined into a single one. On the face of it, this cannot generally be so because there is generally no finite-state transducer that will accept the intersection of the sets of tape pairs accepted by an arbitrary set of transducers. It is, for example, easy to design a transducer that will map a string of x's onto the same number of y's followed by an arbitrary number of z's. It is equally easy to design one that maps a string of x's onto the same number of z's preceded by an arbitrary number of x's. The intersection of these sets contains those pairs with some number of x's on one tape, and that same number of y's followed by the same number of z's on the other tape. The set of second tapes therefore contains a context-free language which it is clearly not within the power of any finite-state device to generate. Koskenniemi overcame this objection in his original work by adopting the view that all the transducers in the parallel configuration should share the same pair or read-write heads. The effect of this is to insist that they not only accept the same pairs of tapes, but that they agree on the particular sequence of symbol pairs that must be rehearsed in the course of accepting each of them. Kaplan has been able to put a more formal construction on this in the following way Let the empty symbols appearing in the pairs labeling any transition in the transducers be replaced by some ordinary symbol not otherwise part of the alphabet. The new set of transducers derived in way clearly do not accept the same tapes as the original ones did, but there is an algorithm for constructing a single finite-state transducer that will accept the intersection of the pairs they all accept. Suppose, now, that this configuration of parallel transducers is put in series with two other standard transducers, one which carries the real empty symbol onto its surrogate, and everything else onto itself, and another transducer that carries the surrogate onto the real empty symbol, then the resulting configuration accepts just the desired set of languages, all of which are also acceptable by single transducers that can be algorithmically derived form the originals. It may well appear that the systems we have been considering properly belong to finite-state phonology or graphology, and not to morphology, properly construed. Computational linguists have indeed often been guilty of some carelessness in their use of this terminology. But it is not hard to see how it could have arisen. The first step in any process that treats natural text is to recognize the words it contains, and this generally involves analyzing each of them in terms of a constituent set of formatives of some kind. Most important among the difficulties that this entails are those having to do with the different shapes that formatives assume in different environments. In other words, the principal difficulties of morphological analysis are in fact phonological or graphological. The inventor of two-level morphology, Kimmo Koskenniemi, is fact provided a finite-state account not just of morphophonemics (or morphographemics), but also of morphotactics. He took it that the allowable set of words simply constituted a regular set of morheme sequences. This is probably the more controversial part of his proposal, but it is also the less technically elaborate, and therefore the one that has attracted less attention. As a result, the term &quot;two-level morphology&quot; has come to be commonly accepted as applying to any system of word recognition that involves two-level, finite-state, phonology or graphology. The approach to nonconcatenative morphology to be outlined in this paper will provide a more unified treatment of morphophonemics and morphotactics than has been usual 3 I shall attempt to show how a two-level account might be given of nonconcatenative morphological phenomena, particularly those exhibited in the Semitic languages. The approach I intend to take is inspired, not only by finite-state morphology, broadly construed, but equally by autosegmental phonology as proposed by Goldsmith (1979) and the autosegmental morphology of McCarthy (1979) All the data that I have used in this work is taken from McCarthy (1979) and my debt to him will be clear throughout. forms that can be constructed on the basis of each of the stems shown. However, there is every reason to suppose that, though longer and greatly more complex in detail, that enterprise would not require essentially different mechanisms from the ones I shall describe. The overall principles on which the material Table is are clear from a fairly cursory inspection Each form contains the letters &quot;ktb&quot; somewhere in it. This is the root of the verb meaning &quot;write&quot;. By replacing these three letters with other appropriately chosen Perfective Imperfective Participle Active Passive Active I katab kutib aktub II kattab kuttib ukattib III kaatab kuutib ukaatib IV ?aktab ?uktib u?aktib V takattab tukuttib atakattab VI takaatab tukuutib atakaatab VII nkatab nkutib ankatib VIII ktatab ktutib aktatib IX ktabab aktabib X staktab stuktib astaktib XI ktaabab aktaabib XII ktawtab aktawtib XIII ktawwab aktawwib XIV ktanbab aktanbib XV ktanbay aktanbiy Passive maktuub mukattab mukaatab mu?aktab mutakattab mutakaatab munkatab muktatab muktabib mustaktab muktaabib muktawtib muktawwib muktanbib muktanbiy Table I I take it as my task to describe how the members of a paradigm like the one in Table I might be generated and recognized effectively and efficiently, and in such a way as to capture profit from the linguistic generalizations inherent in it. Now this is a slightly artificial problem because the forms in Table I are not in words, but only verb stems. To get the verb forms that would be found in Arabic text, we should have to expand the table very considerably to show the inflected sequences of three consonants, we would obtain corresponding paradigms for other roots. With some notable exceptions, the columns of the table contain stems with the same sequence of vowels. of these is known as a as the headings of the columns show, these can serve to distinguish perfect from imperfective, active from passive, and the like. Each row of the table is characterized by a particular pattern according to which the vowels and consonants alternate. In other words, it is characteristic of a given row 4 that the vowel in a particular position is long or short, or that a consonant is simple or geminate, or that material in one syllable is repeated in the following one. McCarthy refers to each of these as a template, term which I shall take over. Each of them adds a particular semantic component to the basic verb, making it reflexive, causative, or whatever. Our problem, will therefore involve designing an abstract device capable of combining components of these three kinds into a single sequence. Our solution will take the form of a set of one or more finite-state transducers that will work in parallel like those of Koskenniemmi(1983), but on four tapes rather than just two. There will not be space, in this paper, to give a detailed account, even of all the material in Table I, not to mention problems that would arise if we were to consider the full range of Arabic roots. What I do hope to do, however, is to establish a theoretical framework within which solutions to all of these problems could be developed. We must presumably expect the transducers we construct to account for the Arabic data to have transition functions from states and quadruples of symbols to states. In other words, we will be able to describe them with transition diagrams whose edges are labeled with a vector of four symbols. When the automaton moves from one state to another, each of the four tapes will advance over the symbol corresponding to it on the transition that sanctions the move. shall allow myself extensions to this basic scheme which will enhance the perspicuity and economy of the formalism without changing its essential character. In particular, these extensions will leave us clearly within the domain of finite-state devices. The extensions have to do with separating the process of reading or writing a symbol on a tape, from advancing the tape to the next position. The quadruples that label the transitions in the transducers we shall be constructing will be elements each consisting of two parts, a symbol, and an instruction concerning the movement of the tape I shall use the following notation for this. A unadorned symbol will be read in the traditional way, namely, as requiring the tape on which that symbol appears to move to the next position as soon as it has been read or written. If the symbol is shown in brackets, on the other hand, the tape will not advance, and the quadruple specifying the next following transition will therefore clearly have to be one that specifies the same symbol for that tape, since the symbol will still be under the read-write head when that transition is taken. With this convention, it is natural to dispense with the e symbol in favor of the notation &quot;[1&quot;, that is, an unspecified symbol over which the corresponding tape does not advance. A symbol can also be written in braces, in which case the corresponding tape will move if the symbol under the read-write head is the last one on the tape. This is intended to capture the of autosegmental morphology, that is, the principal according to which the last item in a string may be reused when required to fill several positions. particular set of quadruples, or made up of symbols, with or without brackets or will constitute the the automata, and the &quot;useful&quot; alphabet must be the same for all the automata because none of them can move from one state to another unless the make an exactly parallel Not surprisingly, a considerable amount of information about the language is contained just in the constitution of the alphabet. Indeed, a single machine with one state which all transitions both leave and enter will generate a nontrivial subset of the material in Table I. An example of the steps involved in generating a form that depends only minimally on information in a transducer is given in table The eight step are labeled (a) - (h). For each one, a box is shown enclosing the symbols currently under the read-write heads. The tapes move under the heads from the right and then continue to the left No symbols are shown to the right on the bottom tape, because we are assuming that the operation chronicled in these diagrams is one in which a surface form is being 5 (a) V t b [1 (e) k t {b} CCVVCVC V VCCV V C C a a {al a [1 a ak t a (b) t b t [] V a a VVCVC [] VCCVC V V a ak tab (c) (g) V C VC VC VCCVC a a [] a •■■• [] ak t a b (d) k t [] (h) k t b V C C V a C VC V VCCVCVC a a i a k t a a ak t ab i b Table II generated. The bottom tape—the one containing the surface form—is therefore being written and it is for this reason that nothing appears to the right. The other three tapes, in the order shown, contain the root, the prosodic template, and the vocalism. To the right of the tapes, the frame is shown which sanctions the move that will be made to advance from that position to the next. No such frame is given for the last configuration for the obvious reason that this represents the end of the process. move from (a) to (b) sanctioned by a frame in which the root consonant is ignored. There must be a &quot;V&quot; on the template tape and an &quot;a&quot; in the current position of the vocalism. However, the vocalism tape will not move when the automata move to their next states. Finally, there will be an &quot;a&quot; on the tape containing the surface form. In summary, given that the prosodie template calls for a vowel, the next vowel in the vocalism has been copied to the surface. Nondeterministically, the device predicts that this same contribution from the vocalism will also be required to fill a later position. The move from (b) to (c) is sanctioned by a in which the is ignored. The template requires a consonant and the frame accordingly specifies the same consonant on both the root and the surface tapes, advancing both of them. A parallel move, differing only in the identity of the consonant, is made from (c) to (d). The move from (d) to (e) is similar to that from (a) to (b) except that, this time, the vocalism tape does advance. The nondeterministic prediction that is being made in this case is that there will be no further slots for the &quot;a&quot; to fill. Just what it is that makes this the &quot;right&quot; move is a matter to which we shall return. The move from (e) to (f) 6 differs from the previous two moves over root consonants in that the &quot;b&quot; is being &quot;spread&quot;. In other words, the root tape does not move, and this possibility is allowed on the specific grounds that it is the last symbol on the tape. Once again, the automata are making a nondeterministic decision, this time that there will be another consonant called for later by the prosodic template and which it will be possible to fill only if this last entry on the root tape does not move away. The moves from (f) to (g) and from (g) to (h) are like those from (d) to (e) and (b) to (c) respectively. Just what is the force of the remark, made from time to time in this commentary, that a move is made These are all situations in which some other move was, in fact, open to the transducers but where the one displayed was carefully chosen to be the one that would lead to the correct result. Suppose that, instead of leaving the root tape stationary in the move from (e) to (f), it had been allowed to advance using a frame parallel to the one used in the moves from (b) to (c) and (c) to (d), a frame which it is only reasonable to assume must exist for all consonants, including &quot;b&quot;. The move from (f) to (g) could still have been made in the same way, but this would have led to a configuration in which a consonant was required by the prosodic template, but none was available from the root. A derivation cannot be allowed to count as complete until all tapes are exhausted, so the automata have reached impasse. We must assume that, when this happens, the automata are able to return to a preceding situation in which an essentially arbitrarily choice was made, and try a different alternative. Indeed, we must assume that a general backtracking strategy is in effect, which ensures that all allowable sequences of choices are explored. Now consider the nondeterministic choice that was made in the move from (a) to (b), as contrasted with the one made under essentially circumstances from (d) to the vocalism tape had advanced in the first of these situations, but not in the second, we should presumably have been able to generate the putative form &quot;aktibib&quot;, which does not exist. This can be excluded only if we assume that there is a transducer that disallows this sequence of events, or if the frames available for &quot;i&quot; are not the same as those for &quot;a&quot;. We are, in fact, making the latter assumption, on the grounds that the vowel &quot;i&quot; occurs only in the final position of Arabic verb stems. now, the forms in rows V of table I. In each of these, the middle consonant of the root is geminate in the surface. This is not a result of spreading as we have described it, spreading occurs with the last consonant of a root. If the prosodic template for row II is &quot;CVCCVC&quot;, how is that we do not get forms like &quot;katbab&quot; and &quot;kutbib&quot; beside the ones shown? This is a problem that is overcome in McCarthy's autosegmental account only at considerable cost. Indeed, is is a deficiency of that formalism that the only mechanisms available in it to account for gemination are as complex as they are, given how common the phenomenon is. Within the framework proposed here, gemination is provided for in a very natural way. Consider the following pair of frame schemata, in and arbitrary consonant: [c] [1 [1 First of these the one that was used for the consonants in the above example except in the situation for the first occurrence of &quot;b&quot;, where is was being spread into the final two consonantal positions of the form. The second frame differs from this is two respects. First, the prosodic template contains the hitherto unused symbol &quot;G&quot;. for &quot;geminate&quot;, and second, the root tape is not advanced. Suppose, now, that the the prosodic template for forms like &quot;kattab&quot; is not &quot;CVCCVC&quot;, but &quot;CVGCVC&quot;. It will be possible to discharge the &quot;G&quot; only if the root template does not advance, so that the following &quot;C&quot; in the template can only cause the same consonant to be into the word time. The sequence &quot;GC&quot; in a prosodic template is therefore an idiom for consonant gemination. 7 Needless to say, McCarthy's work, on which this paper is based, is not interesting simply for the fact that he is able to achieve an adequate description of the data in table I, but also for the claims he makes about the way that account extends to a wider class of phenomena, thus achieving a measure of explanatory power. In particular, he claims that it extends to roots with two and four consonants. Consider, in particular, the following sets of forms: ktanbab dhanraj kattab dahraj takattab tadahraj Those in the second column are based on the root /dhrj/. In the first column are the corresponding forms of /ktb/. The similarity in the sets of corresponding forms is unmistakable. They exhibit the same patterns of consonants and vowels, differing only in that, whereas some consonant appears twice in the forms in column one, the consonantal slots are all occupied by different segments in the forms on the right. For these purposes, the &quot;n&quot; of the first pair of forms should be ignored since it is contributed by the prosodic template, and not by the root. consonantal slot in the prosodic template only in the case of the shorter form. The structure of the second and third forms is equally straighforward, but it is less easy to see how our machinery could account for them. Once again, the template calls for four root consonants and, where only three are provided, one must do double duty. But in this case, the effect is achieved through gemination rather than spreading so that the gemination mechanism just outlined is presumably in play. That mechanism makes no provision for gemination to be invoked only when needed to fill slots in the prosodic template that would otherwise remain empty. If the mechanism were as just described, and the triliteral forms were &quot;CVGCVC&quot; and &quot;tVCVGCVC&quot; respectively, then the quadriliteral forms would have to be generated on a different base. It is in cases like this, of which there in fact many, that the finite-state transducers play a substantive role. What is required in this case is a transducer that allows the root tape to remain stationary while the template tape moves over a &quot;G&quot;, provided no spreading will be allowed to occur later to fill consonantal slots that would Fig. 1 a triliteral and a quadriliteral root, otherwise he unclaimed. extra consonants spread not geminate G-geminate 0-simple no spread no spread no spread the first pair are exactly as one would expect—the final root consonant is spread to fill the final required, then the first priority must he to let them occupy the slots marked with a &quot;0&quot; in the 8 template. Fig. 1 shows a schema for the transition diagram of a transducer that has this effect. I call it a &quot;schema&quot; only because each of the edges shown does duty for a number of actual transitions. The machine begins in the &quot;start&quot; state and continues to return there so long as no frame is encountered involving a &quot;G&quot; on the template tape. A &quot;G&quot; transition causes a nondeterministic choice. If the root tape moves at the same time as the &quot;G&quot; is scanned, the transducer goes into its &quot;no-spread&quot; state, to which it continues to return so long as every move over a &quot;C&quot; on the prosodic tape is accompanied by a move over a consonant on the root tape. In other words, it must be possible to complete the process without spreading consonants. The other alternative is that the transducer should enter the &quot;geminate&quot; state over a transition over a in the template with the root tape remaining stationary. The transitions at the &quot;geminate&quot; state allow both spreading and nonspreading transitions. In summary, spreading can occur only if the transducer never leaves the &quot;start&quot; state and there is no &quot;G&quot; in the template, or there is a &quot;G&quot; on the template which does not trigger gemination. A &quot;G&quot; can fail to trigger gemination only when the root contains enough consonants to fill all the requirements that the template makes for them. One quadriliteral case remains to be accounted for, namely the following: ktaabab dharjaj According to the strategy just elaborated, we should have expected the quadriliteral form to been &quot;dhaaraj&quot;. But, this form contains a slot that is used for vowel lengthening with triliteral roots, and as consonantal position for quadriliterals. We must therefore presumably take it that the prosodic template for this form is something like &quot;CCVXCVC&quot; where &quot;X&quot; is a segment, but not specified as either vocalic or consonantal. This much is in line with the proposal that McCarthy himself makes The question is, when should be filled by a vowel, and when by a consonant? The data in Table I is, of course, insufficient to answer question. but a plausible answer that strongly suggests itself is that the &quot;X&quot; slot prefers a consonantal filler that would result in gemination. If this is true, then it is another case where the notion of gemination, though not actually exemplified in the form, plays a central role. Supposing that the analysis is correct, the next question is, how is it to be implemented. The most appealing answer would be to make &quot;X&quot; the exact obverse of &quot;G&quot;, when filled with a consonant. In other words, when a root consonant fills such a slot, the root tape must advance so that the same consonant will no longer be to fill the next position. The that the next root consonant would simply be a repetition of the current one would be excluded if we were to take over from autosegmental phonology and morphology, some version of th Contour Principle (OCP) 1979) which disallows repeated segments except in the prosodic template and in the surface string. McCarthy points out the roots like /smnn/, which appear to violate the OCP can invariably be reanalyzed as biliteral roots like /sm/ and, if this is done, our analysis, like his, goes through. The OCP does seem likely to cause some trouble when we come to treat one of the principal remaining problems, namely that of the forms in row I of table I. It turns out that the vowel that appears in the second syllable of these forms is not provided by the vocalism, but by the root. The vowel that appears in the perfect is generally different from the one that appears in the imperfect, and four different pairs are possible. pair that is used with given root is an idiosyncratic property of that root. One possibility is, therefore, that we treat the traditional triliteral roots as consisting not simply of three consonants, but as three consonants with a vowel intervening between the second and third, for a total of four segments. This flies in the face of traditional wisdom. It also counter to one of the intuitions autosegmental phonology which would have it that particular phonological features can be represented on at most one lexical tier, or tape. The intuition is that these tiers or tapes each contain a record or a particular kind of 9 gesture; the hearer's point of view, it is as though they contained a record of the signal received from a receptor that was attuned only to certain features. If we wish to maintain this model, there are presumably two alternatives open to us. Both involve assuming that roots are represented on at least two tapes in parallel, with the consonants separate from the vowel. According to one alternative, the root vowel would be written on the same tape as the vocalism; according to the other, it would be on a tape of its own. Unfortunately, neither alternative makes for a particularly happy solution. No problem arises from the proposal that a given morpheme should, in general, be represented on more than one lexical tape. However, the idea that the vocalic material associated with a root should appear on a special tape, reserved for it alone, breaks the clean lines of the system as so far presented in two ways. First, it spearates material onto two tapes, specifically the new one and the vocalism, on purely lexical grounds, having nothing to do with their phonetic or phonological constitution, and this runs counter to the idea of tapes as records of activity on phonetically specialized receptors. It is also at least slightly troublesome in that that newly introduced tape fills no function except in the generation of the first row of the table. Neither of these arguments is conclusive, and they could diminish considerably in force as a wider range of data was considered. Representing the vocalic contribution of the root on the same tape as the vacalism would avoid both of these objections, but would require that vocalic contribution to be recorded either before or after the vocalism itself. Since the root vowel affects the latter part of the root, it seems reasonable that it should be positioned to the right. Notice, however, that this is the only instance in which we have had to make any assumptions about the relative ordering of the morphemes that contribute to a stem. Once again, it may be possible to assemble further evidence reflecting on some such ordering, but I do not see it in these data. It is only right that I should point out the difficulty of accounting satisfactorily for the vocalic contribution of verbal roots. It is only right that I should also point out that the autosegmental solution fares no better on this score, resorting, as it must, to rules that access essentially non-phonological properties of the morphemes involved. By insisting that what I called the a morpheme should by, by definition, be its only contribution to phonological processes, I have cut myself off from such ex machina. Linguists in general, and computational linguists in particular, do well to employ finite-state devices wherever possible. They are theoretically appealing because they are computational weak and best understood from a mathematical point of view. They are computationally appealing because they make for simple, elegant, and highly efficient implementaions. In this paper, I hope I have shown how they can be applied to a problem in nonconcatenative morphology which seems initially to require heavier machinary.  One early approach, suggested by Kay (1987) and later pursued in different variants by Kiraz (1994, 2000) among others, was to, instead of modeling morphology along the more traditional finite-state transducer, modeling it with a n-tape automaton, where tapes would carry precisely this interleaving that is called for in Semitic interdigitation. Kay (1987) devised a framework with which each of the auto segmental tiers is assigned a tape in a multi-tape finite state machine, with an additional tape for the surface form. A strictly finite-state mechanism has a number of problems in covering natural language morphology, as has been recognized earlier (e.g. Kay (1987)). Kay (1987) proposes a framework with which each of the auto segmental tiers is assigned a tape in a multi-tape finite state machine, with an additional tape for the surface form.
Towards History-based Grammars: Using Richer Models for Probabil ist ic Parsing* Ezra Black Fred Jelinek John Lafferty David M. Magerman Robert Mercer Salim Roukos IBM T.  J .  History-based parsing uses features of the parsing history to predict the next parser action (Black et al,1992). Many current parsers fall into the class of history based grammars (Black et al, 1992). History-based feature models for predicting the next parser action (Black et al, 1992).  History-based parsing models rely on features of the derivation history to predict the next parser action (Black et al, 1992).
