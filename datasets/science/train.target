Finding Parts In Very Large Corpora We present a method for extracting parts of objects from wholes (e.g."speedometer" from "car"). Given a very large corpus our method finds part words with 55% accuracy for the top 50 words as ranked by the system. The part list could be scanned by an end-user and added to an existing ontology (such as WordNet), or used as a part of a rough semantic lexicon. To filter out attributes that are regarded as qualities (like driving ability) rather than parts (like steering wheels), we remove words ending with the suffixes -ness, -ing, and -ity.
The Mathematics Of Statistical Machine Translation: Parameter Estimation We describe a series of five statistical models of the translation process and give algorithms, for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus. Our model for Statistical machine translation (SMT) is focused on word to word translation and was based on the noisy channel approach.
Optimizing Chinese Word Segmentation for Machine Translation Performance Previous work has shown that Chinese word segmentation is useful for machine translation to English, yet the way different segmentation strategies affect MT is still poorly understood. In this paper, we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better MT performance. We find that other factors such as segmentation consistency and granularity of Chinese “words” can be more important for machine translation. Based on these findings, we implement methods inside a conditional random field segmenter that directly optimize segmentation granularity with respect to the MT task, providing an improvement of 0.73 BLEU. We also show that improving segmentation consistency using external lexicon and proper noun features yields a 0.32 BLEU increase. We develop the CRF-based Stanford Chinese segmenter that is trained on the segmentation of the Chinese Treebank for consistency. We enhance a CRF s segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence.
The viability of web-derived polarity lexicons We examine the viability of building large polarity lexicons semi-automatically from the web. We begin by describing a graph propagation framework inspired by previous work on constructing polarity lexicons from lexical graphs (Kim and Hovy, 2004; Hu and Liu, 2004; Esuli and Sabastiani, 2009; BlairGoldensohn et al., 2008; Rao and Ravichandran, 2009). We then apply this technique to build an English lexicon that is significantly larger than those previously studied. Crucially, this web-derived lexicon does not require WordNet, part-of-speech taggers, or other language-dependent resources typical of sentiment analysis systems. As a result, the lexicon is not limited to specific word classes – e.g., adjectives that occur in WordNet – and in fact contains slang, misspellings, multiword expressions, etc. We evaluate a lexicon derived from English documents, both qualitatively and quantitatively, and show that it provides superior performance to previously studied lexicons, including one derived from WordNet. We construct a graph where the nodes are 20 million candidate words or phrases, selected using a set of heuristics including frequency and mutual information of word boundaries.
Combining Lexical Syntactic And Semantic Features With Maximum Entropy Models For Information Extraction Extracting semantic relationships between entities is challenging because of a paucity of annotated data and the errors induced by entity detection modules. We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text. Our system obtained competitive results in the Automatic Content Extraction (ACE) evaluation. Here we present our general approach and describe our ACE results. We use two kinds of features: syntactic ones and word based ones, for example, the path of the given pair of NEs in the parse tree and the word n-gram between NEs. We obtain improvement in results when we combine a variety of features. We achieved the F-measure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus.
Similarity of Semantic Relations There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. This article introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) The patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM. We develop a corpus based approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms. We describe a method (Latent Relational Analysis) that extracts subsequence patterns for noun pairs from a large corpus, using query expansion to increase the recall of the search and feature selection and dimensionality reduction to reduce the complexity of the feature space.
Further Meta-Evaluation of Machine Translation This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish. We report the translation quality of over 30 diverse translation systems based on a large-scale manual evaluation involving hundreds of hours of effort. We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the correlation with human judgments at both the system-level and at the sentence-level. We validate our manual evaluation methodology by measuring intra- and inter-annotator agreement, and collecting timing information. Thus, the human an notation for the WMT 2008 dataset was collected in the form of binary pairwise preferences that are considerably easier to make. Traditionally, human ratings for MT quality have been collected in the form of absolute scores on a five or seven-point Likert scale, but low reliability numbers for this type of annotation have raised concerns.
Soft Syntactic Constraints for Hierarchical Phrased-Based Translation In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic analysis, versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data. A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then finding appropriate ways to soften that commitment. We present an approach that explores the tradeoff from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language. We obtain substantial improvements in performance for translation from Chinese and Arabic to English. We revise this method by distinguishing different constituent syntactic types, and defined features for each type to count whether a phrase matches or crosses the syntactic boundary. We find that their constituent constraints are sensitive to language pairs.
A Unification Method For Disjunctive Feature Descriptions Although disjunction has been used in several unification-based grammar formalisms, existing methods of unification have been unsatisfactory for descriptions containing large quantities of disjunction, because they require exponential time. This paper describes a method of unification by successive approximation, resulting in better average performance. The general problem of unifying two disjunctive feature structures is non-polynomial in the number of disjunctions. We present a technique which, for every set of n conjoined disjunctions, checks the consistency first of single disjuncts against the definite part of the description, then that of pairs and so on up ton-tuples for full consistency.
Using Corpus Statistics And WordNet Relations For Sense Identification Corpus-based approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneck. We show how knowledge-based techniques can be used to open the bottleneck by automatically locating training corpora. We describe a statistical classifier that combines topical context with local cues to identity a word sense. The classifier is used to disambiguate a noun, a verb, and an adjective. A knowledge base in the form of WordNet's lexical relations is used to automatically locate training examples in a general text corpus. Test results are compared with those from manually tagged training examples. We present a method to obtain sense-tagged examples using monosemous relatives.
Mildly Non-Projective Dependency Structures Syntactic parsing requires a fine balance between expressivity and complexity, so that naturally occurring structures can be accurately parsed without compromising efficiency. In dependency-based parsing, several constraints have been proposed that restrict the class of permissible structures, such as projectivity, planarity, multi-planarity, well-nestedness, gap degree, and edge degree. While projectivity is generally taken to be too restrictive for natural language syntax, it is not clear which of the other proposals strikes the best balance between expressivity and complexity. In this paper, we review and compare the different constraints theoretically, and provide an experimental evaluation using data from two treebanks, investigating how large a proportion of the structures found in the treebanks are permitted under different constraints. The results indicate that a combination of the well-nestedness constraint and a parametric constraint on discontinuity gives a very good fit with the linguistic data.
Automatic Labeling Of Semantic Roles We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles, such as Agent or Patient, or more domain-specific semantic roles, such as Speaker, Message, and Topic. The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and its position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible fillers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classifiers. Our system achieves 82% accuracy in identifying the semantic role of presegmented constituents. At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall. Our study also allowed us to compare the usefulness of different features and feature combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data. We propose the first SRL model on FrameNet.
Generative Models For Statistical Parsing With Combinatory Categorial Grammar This paper compares a number of generative probability models for a wide-coverage Combinatory Categorial Grammar (CCG) parser. These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations. According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to the figures given by Collins (1999) for a linguistically less expressive grammar. In contrast to Gildea (2001), we find a significant improvement from modeling word-word dependencies. The CCG combinatory rules are encoded as rule instances, together with a number of additional rules which deal with punctuation and type-changing. The dependency features are defined in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features.
Corpus Statistics Meet The Noun Compound: Some Empirical Results A variety of statistical methods for noun compound analysis are implemented and compared. The results support two main conclusions. First, the use of conceptual association not only enables a broad coverage, but also improves the accuracy. Second, an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents, even though the latter is more prevalent in the literature. We propose an unsupervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus. We test both adjacency and dependency models on 244 compounds extracted from Grolier's encyclopedia, a corpus of 8 million words.
Building A Large Annotated Corpus Of English: The Penn Treebank
The Automated Acquisition Of Topic Signatures For Text Summarization In order to produce a good summary, one has to identify the most relevant portions of a given text. We describe in this paper a method for automatically training topic signatures -- sets of related words, with associated weights, organized around head topics and illustrate with signature we created with 6,194 TREC collection texts over 4 selected topics. We describe the possible integration of topic signatures with ontologies and its evaluaton on an automated text summarization system. We first introduced topic signatures which are topic relevant terms for summarization.
Improving Machine Translation Performance By Exploiting Non-Parallel Corpora We present a novel method for discovering parallel sentences in comparable, non-parallel corpora. We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other. Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus. Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available. We use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles. We filter out negative examples with high length difference or low word overlap (based on a bilingual dictionary). We define features primarily based on IBM Model 1 alignments (Brown et al, 1993).
Introduction To The Special Issue On Word Sense Disambiguation: The State Of The Art We present a very concise survey of the history of ideas used in word sense disambiguation. In general, the various WSD approaches of the past can be divided into two types, i.e., data and knowledge-based approaches. We argue that word sense ambiguity is a central problem for many established HLT applications (for example Machine Translation, Information Extraction and Information Retrieval).
Providing A Unified Account Of Definite Noun Phrases In Discourse Linguistic theories typically assign various linguistic phenomena to one of the categories, syntactic, semantic, or pragmatic, as if the phenomena in each category were relatively independent of those in the others. However, various phenomena in discourse do not seem to yield comfortably to any account that is strictly a syntactic or semantic or pragmatic one. This paper focuses on particular phenomena of this sort - the use of various referring expressions such as definite noun phrases and pronouns - and examines their interaction with mechanisms used to maintain discourse coherence. Even a casual survey of the literature on definite descriptions and referring expressions reveals not only defects in the individual accounts provided by theorists (from several different disciplines), but also deep confusions about the roles that syntactic, semantic, and pragmatic factors play in accounting for these phenomena. The research we have undertaken is an attempt to sort out some of these confusions and to create the basis for a theoretical framework that can account for a variety of discourse phenomena in which all three factors of language use interact. The major premise on which our research depends is that the concepts necessary for an adequate understanding of the phenomena in question are not exclusively either syntactic or semantic or pragmatic. The next section of this paper defines two levels of discourse coherence and describes their roles in accounting for the use of singular definite noun phrases. To illustrate the integration of factors in explaining the uses of referring expressions, their use on one of these levels, i.e., the local one, is discussed in Sections 3 and 4. This account requires introducing the notion of the centers of a sentence in a discourse, a notion that cannot be defined in terms of factors that are exclusively syntactic or semantic or pragmatic. In Section 5, the interactions of the two levels with these factors and their effects on the uses of referring expressions in discourse are discussed. To resolve referring expression, we develop centering theory.
Unsupervised Learning of Narrative Schemas and their Participants We describe an unsupervised system for learning narrative schemas, coherent sequences or sets of events (arrested(POLICE,SUSPECT), convicted(JUDGE, SUSPECT)) whose arguments are filled with participant semantic roles defined over words (JUDGE = {judge, jury, court}, POLICE = {police, agent, authorities}). Unlike most previous work in event structure or semantic role learning, our system does not use supervised techniques, hand-built knowledge, or predefined classes of events or roles. Our unsupervised learning algorithm uses coreferring arguments in chains of verbs to learn both rich narrative event structure and argument roles. By jointly addressing both tasks, we improve on previous results in narrative/frame learning and induce rich frame-specific semantic roles. We describe a process to induce a partially ordered set of events related by a common protagonist by using an unsupervised distributional method to learn relations between events sharing co referring arguments, followed by temporal classification to induce partial order.
Name Tagging With Word Clusters And Discriminative Training We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus. Cluster membership is encoded in features that are incorporated in a discriminatively trained tagging model. Active learning is used to select training examples. We evaluate the technique for named-entity tagging. Compared with a state-of-the-art HMM-based name finder, the presented technique requires only 13% as much annotated data to achieve the same level of performance. Given a large annotated training set of 1,000,000 words, the technique achieves a 25% reduction in error over the state-of-the-art HMM trained on the same material. We use prefixes of the Brown cluster hierarchy to produce clusterings of varying granularity. We use the Brown algorithm for clustering (Brown et al 1992).
Effects Of Adjective Orientation And Gradability On Sentence Subjectivity Subjectivity is a pragmatic, sentence-level feature that has important implications for text processing applications such as information extraction and information retrieval. We study the effects of dynamic adjectives, semantically oriented adjectives, and gradable adjectives on a simple subjectivity classifier, and establish that they are strong predictors of subjectivity. A novel trainable method that statistically combines two indicators of gradability is presented and evaluated, complementing existing automatic techniques for assigning orientation labels. Unlike nouns, many adjectives are inherently subjective, and the number of adjectives in texts correlates with human judgements of their subjectivity. We report a statistical correlation between the number of adjectives in a text and human judgments of subjectivity. We show that automatically detected gradable adjectives are a useful feature for subjectivity classification.
Minimized Models for Unsupervised Part-of-Speech Tagging We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values. We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems in both settings. We achieve the best results (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data. We propose a rigid mechanism for modeling sparsity that minimizes the size of tagging grammar as measured by the number of transition types. To avoid the need for manually pruning the tag dictionary, we propose that low-probability tags might be automatically filtered from the tag dictionary through a model minimization procedure applied to the raw text and constrained by the full tag dictionary.
A Polynomial-Time Algorithm For Statistical Machine Translation We introduce a polynomial-time algorithm for statistical machine translation. This algorithm can be used in place of the expensive, slow best-first search strategies in current statistical translation architectures. The approach employs the stochastic bracketing transduction grammar (SBTG) model we recently introduced to replace earlier word alignment channel models, while retaining a bigram language model. The new algorithm in our experience yields major speed improvement with no significant loss of accuracy. We test our algorithm on Chinese-English translation.
An Unsupervised Morpheme-Based HMM For Hebrew Morphological Disambiguation Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text. When the word is ambiguous (there are several possible analyses for the word), a disambiguation procedure based on the word context must be applied. This paper deals with morphological disambiguation of the Hebrew language, which combines morphemes into a word in both agglutinative and fusional ways. We present an unsupervised stochastic model - the only resource we use is a morphological analyzer - which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language. We present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules (which are quite restricted in Hebrew) helps in the disambiguation. We adapt HMM algorithms for learning and searching this text representation, in such a way that segmentation and tagging can be learned in parallel in one step. Results on a large scale evaluation indicate that this learning improves disambiguation for complex tag sets. Our method is applicable to other languages with affix morphology. We provide for each word not only the PoS, but also full morphological features, such as Gender, Number, Person, Construct, Tense, and the affixes' properties. We present a lattice-based modification of the BaumWelch algorithm to handle the segmentation ambiguity.
The Importance Of Supertagging For Wide-Coverage CCG Parsing This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis. The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training. It also dramatically increases the speed of the parser. We show that large increases in speed can be obtained by tightly integrating the supertagger with the CCG grammar and parser. This is the first work we are aware of to successfully integrate a supertagger with a full parser which uses an automatically extracted grammar. We also further reduce the derivation space using constraints on category combination. The result is an accurate wide-coverage CCG parser which is an order of magnitude faster than comparable systems for other linguistically motivated formalisms. Our scores give an indication of how supertagging accuracy corresponds to overall dependency recovery. We describe two log-linear parsing models for CCG: a normal-form derivation model and a dependency model. The CCG parsing consists of two phases: first the supertagger assigns the most probable categories toeach word, and then the small number of combinatory rules, plus the type-changing and punctuation rules, are used with the CKY algorithm to build a packed chart. We propose a method for integrating the supertagger with the parser: initially a small number of categories is assigned to each word, and more categories are requested if the parser can not find a spanning analysis.
Overview of BioNLP Shared Task 2011 The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams. Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully fully generalize in various aspects. The BioNLP 2011 Shared Task series generalized this defining a series of tasks involving more text types, domains and target event types.
Word-Sense Disambiguation Using Statistical Methods We describe a statistical technique for assigning senses to words. An instance of a word is assigned a sense by asking a question about the context in which the word appears. The question is constructed to have high mutual information with the translation of that instance in another language. When we incorporated this method of assigning senses into our statistical machine translation system, the error rate of the system decreased by thirteen percent. We propose a word-sense disambiguation algorithm to disambiguate English translations of French target words based on the single most imformative context feature. We perform unsupervised word alignment and determine the most appropriate translation for a target word from a set of contextual features.
Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data. In this paper, we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar. Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees. We first introduce our approach to inducing such a grammar from parallel corpora. Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer. We introduce a polynomial time decoding algorithm for the model. We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software. The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality. Our approach requires some assumptions on the level of isomorphism (lexical and/or structural) between two languages. We present a translation model based on Synchronous Dependency Insertion Grammar (SDIG), which handles some of the non-isomorphism but requires both source and target dependency structures.
Joshua: An Open Source Toolkit for Parsing-Based Machine Translation We describe Joshua, an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French-English translation task. We develop the syntax-based MT system Joshua, which implements dynamic programming algorithms for second-order expectation semi rings (Li and Eisner, 2009) to efficiently compute the gradients needed for optimization.
Incrementality In Deterministic Dependency Parsing Deterministic dependency parsing is a robust and efficient approach to syntactic parsing of unrestricted natural language text. In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable within this framework. However, we also show that it is possible to minimize the number of structures that require non-incremental processing by choosing an optimal parsing algorithm. This claim is substantiated with experimental evidence showing that the algorithm achieves incremental parsing for 68.9% of the input when tested on a random sample of Swedish text.
SemEval-2007 Task 15: TempEval Temporal Relation Identification The TempEval task proposes a simple way to evaluate automatic extraction of temporal relations. It avoids the pitfalls of evaluating a graph of inter-related labels by defining three subtasks that allow pairwise evaluation of temporal relations. The task not only allows straightforward evaluation, it also avoids the complexities of full temporal parsing. Temporal information processing is a topic of natural language processing boosted by our evaluation campaign TempEval. TempEval07 (Verhagen et al, 2007) integrated 14 TLINK relations into three: before, after, and.
A Procedure For Quantitatively Comparing The Syntactic Coverage Of English Grammars We define PARSEVAL measures for parsing: labelled precision, labelled recall, and labelled F-measure (Prec., Rec., and F1, respectively), which are based on the number of non-terminal items in the parser's output that match those in the gold-standard parse.
Generating Phrasal and Sentential Paraphrases: A Survey of Data-Driven Methods The task of paraphrasing is inherently familiar to speakers of all languages. Moreover, the task of automatically generating or extracting semantic equivalences for the various units of language — words, phrases, and sentences — is an important part of natural language processing (NLP) and is being increasingly employed to improve the performance of several NLP applications. In this article, we attempt to conduct a comprehensive and application-independent survey of data-driven phrasal and sentential paraphrase generation methods, while also conveying an appreciation for the importance and potential use of paraphrases in the field of NLP research. Recent work done in manual and automatic construction of paraphrase corpora is also examined. We also discuss the strategies used for evaluating paraphrase generation techniques and briefly explore some future trends in paraphrase generation. We survey a variety of data driven paraphrasing techniques, categorizing them based on the type of data that they use.
Tree-To-String Alignment Template For Statistical Machine Translation We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models. We perform derivation-level combination for mixing different types of translation rules within one derivation. We also add non-syntactic PBSMT - phrase-based statistical machine translation - phrases into our tree-to-string translation system.
Confidence Estimation For Machine Translation We present a detailed study of confidence estimation for machine translation. Various methods for determining whether MT output is correct are investigated, for both whole sentences and words. Since the notion of correctness is not intuitively clear in this context, different ways of defining it are proposed. We present results on data from the NIST 2003 Chinese-to-English MT evaluation. We introduce a sentence level QE system where an arbitrary threshold is used to classify the MT output as good or bad. We study sentence and word level features for translation error prediction.
Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchronization This paper presents the first round of the task on Cross-lingual Textual Entailment for Content Synchronization, organized within SemEval-2012. The task was designed to promote research on semantic inference over texts written in different languages, targeting at the same time a real application scenario. Participants were presented with datasets for different language pairs, where multi-directional entailment relations (forward, backward, bidirectional, no entailment) had to be identified. We report on the training and test data used for evaluation, the process of their creation, the participating systems (10 teams, 92 runs), the approaches adopted and the results achieved.
Pronunciation Modeling For Improved Spelling Correction This paper presents a method for incorporating word pronunciation information in a noisy channel model for spelling correction. The proposed method builds an explicit error model for word pronunciations. By modeling pronunciation similarities between words we achieve a substantial performance improvement over the previous best performing models for spelling correction. We consider a pronunciation variation model to generate multiple pronunciations for each canonical pronunciation in a pronouncing dictionary. We extend Brill and Moore (2000) to consider edits over both letter sequences and sequences of phones in the pronunciations of the word and misspelling. We use the noisy channel model approach to determine the types and weights of edit operations. Since a spelling correction model needs to rank candidate words rather than candidate pronunciations, we derive an error model that determines the probability that a word w was spelled as the non-word r based on their pronunciations.
Unsupervised Models For Named Entity Classification This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classifier. However, we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple "seed" rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type. We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98). We extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree.
Distinguishing Word Senses In Untagged Text This paper describes an experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text. The methods described in this paper, McQuitty's similarity analysis, Ward's minimum-variance method, and the EM algorithm, assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text. These methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs. Overall, the most accurate of these procedures is McQuitty's similarity analysis in combination with a high dimensional feature set. we propose a (dis) similarity based discrimination approach that computes (dis) similarity among each pair of instances of the target word.
A Hierarchical Phrase-Based Model For Statistical Machine Translation We present a statistical phrase-based translation model that uses hierarchical phrases - phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system. We use the k-best parsing algorithm in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU. We note that whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items. To better leverage syntactic constraint yet still allow non-syntactic translations, we introduce a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. Our hierarchical phrase models for machine translation is an evolution from the traditional word (Brown et al, 1993) and phrase based (Koehn et al, 2003a) models.
Mixture-Model Adaptation for SMT We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components. We investigate a number of variants on this approach, including cross-domainversus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to. The best methods achieve gains of approximately one BLEU percentage point over a state-of-the art non-adapted baseline system. We conclude that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log linearly. We interpolate the in and general-domain phrase tables together, assigning either linear or log-linear weights to the entries in the tables before combining overlapping entries; this is now standard practice.
CorMet: A Computational Corpus-Based Conventional Metaphor Extraction System CorMet is a corpus-based system for discovering metaphorical mappings between concepts. It does this by finding systematic variations in domain-specific selectional preferences, which are inferred from large, dynamically mined Internet corpora. Metaphors transfer structure from a source domain to a target domain, making some concepts in the target domain metaphorically equivalent to concepts in the source domain. The verbs that select for a concept in the source domain tend to select for its metaphorical equivalent in the target domain. This regularity, detectable with a shallow linguistic analysis, is used to find the metaphorical interconcept mappings, which can then be used to infer the existence of higher-level conventional metaphors. Most other computational metaphor systems use small, hand-coded semantic knowledge bases and work on a few examples. Although CorMet’s only knowledge base is WordNet (Fellbaum 1998) it can find the mappings constituting many conventional metaphors and in some cases recognize sentences instantiating those mappings. CorMet is tested on its ability to find a subset of the Master Metaphor List (Lakoff, Espenson, and Schwartz 1991). The CorMet system dynamically mines domain specific corpora to find less frequent usages and identifies conceptual metaphors. We show how statistical analysis can automatically detect and extract conventional metaphors from corpora, though creative metaphors still remain a tantalizing challenge.
Polylingual Topic Models Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts. Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages. We introduce a polylingual topic model that discovers topics aligned across multiple languages. We explore the model’s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages. We retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations. We show that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) does not degrade significantly. We extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles). Polylingual topic models learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic.
Multiple Aspect Ranking Using the Good Grief Algorithm We address the problem of analyzing multiple related opinions in a text. For instance, in a restaurant review such opinions may include food, ambience and service. We formulate this task as a multiple aspect ranking problem, where the goal is to produce a set of numerical scores, one for each aspect. We present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks. This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast. We prove that our agreement-based joint model is more expressive than individual ranking models. Our empirical results further confirm the strength of the model: the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model. We combine an agreement model based on contrastive RST relations with a local aspect (or target) model to make a more informed overall decision for sentiment classification.
A Multi-Pass Sieve for Coreference Resolution Most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features. This approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones. To overcome this problem, we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision. Each tier builds on the previous tier’s entity cluster output. Further, our model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster. This cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time. The framework is highly modular: new coreference modules can be plugged in without any change to the other modules. In spite of its simplicity, our approach outperforms many state-of-the-art supervised and unsupervised models on several standard corpora. This suggests that sieve-based approaches could be applied to other NLP tasks. Our rule based model obtains competitive result with less time. The candidate antecedents for the pronoun are ordered based on a notion of discourse salience that favors syntactic salience and document proximity. We develop accurate unsupervised systems that exploit simple but robust linguistic principles.
The Kappa Statistic: A Second Look In recent years, the kappa coefficient of agreement has become the de facto standard for evaluating intercoder agreement for tagging tasks. In this squib, we highlight issues that affect κ and that the community has largely neglected. First, we discuss the assumptions underlying different computations of the expected agreement component of κ. Second, we discuss how prevalence and bias affect the κ measure.
Dependency Treelet Translation: Syntactically Informed Phrasal SMT We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation. This method requires a source language dependency parser, target language word segmentation and an unsupervised word alignment component. We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model. We describe an efficient decoder and show that using these tree-based models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser. Our treelet-based SMT system is trained on about 4.6M parallel sentence pairs from diverse sources including bilingual books, dictionaries and web publications. We extend paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on treelet pairs. We demonstrate the success of using fragments of a target language's grammar, treelets, to improve performance in phrasal translation.
A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English In this paper, we present an approach to the automatic identification and correction of preposition and determiner errors in non-native (L2) English writing. We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing. In the context of automated preposition and determiner error correction in L2 English, we note that the process is often disrupted by misspellings.
Concept Discovery From Text Broad-coverage lexical resources such as WordNet are extremely useful. However, they often include many rare senses while missing domain-specific senses. We present a clustering algorithm called CBC (Clustering By Committee) that automatically discovers concepts from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the cluster. We proceed by assigning elements to their most similar cluster. Evaluating cluster quality has always been a difficult task. We present a new evaluation methodology that is based on the editing distance between output clusters and classes extracted from WordNet (the answer key). Our experiments show that CBC outperforms several well-known clustering algorithms in cluster quality. Mutual information (MI) is an information theoric measure and has been used in our method for  clustering words.
The Generative Lexicon In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories and outline a theory of lexical semantics embodying a notion of cocompositionality and type coercion, as well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives, I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop a theory of Qualia Structure, a representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the larger lexical knowledge base through a theory of lexical inheritance. This provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole. We propose the Generative Lexicon Theory (GLT), which can be said to take advantage of both linguistic and conceptual approaches, providing a framework which arose from the integration of linguistic studies and of techniques found in AI.
Bayesian Learning of a Tree Substitution Grammar Tree substitution grammars (TSGs) offer many advantages over context-free grammars (CFGs), but are hard to learn. Past approaches have resorted to heuristics. In this paper, we learn a TSG using Gibbs sampling with a nonparametric prior to control subtree size. The learned grammars perform significantly better than heuristically extracted ones on parsing accuracy.
A Generative Constituent-Context Model For Improved Grammar Induction We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts. Parameter search with EM produces higher quality analyses than previously exhibited by unsupervised systems, giving the best published unsupervised parsing results on the ATIS corpus. Experiments on Penn treebank sentences of comparable length show an even higher F1 of 71% on non-trivial brackets. We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model. We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task. We induce parts-of-speech from the full WSJ tree bank together with additional WSJ newswire.
A Smorgasbord Of Features For Statistical Machine Translation We describe a methodology for rapid experimentation in statistical machine translation which we use to add a large number of features to a baseline system exploiting features from a wide range of levels of syntactic representation. Feature values were combined in a log-linear model to select the highest scoring candidate translation from an n-best list. Feature weights were optimized directly against the BLEU evaluation metric on held-out data. We present results for a small selection of features at each level of syntactic representation. At the 2003 Johns Hopkins summer workshop on statistical machine translation, a large number of features were tested to discover which ones could improve a state-of-the-art translation system, and the only feature that produced a 'truly significant improvement' was the Model 1 score. The effects of integrating syntactic structure into a state-of-the-art statistical machine translation system are investigated.
Fluency Adequacy or HTER? Exploring Different Human Judgments with a Tunable MT Metric Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance. Different types of human judgments, such as Fluency, Adequacy, and HTER, measure varying aspects of MT performance that can be captured by automatic MT metrics. We explore these differences through the use of a new tunable MT metric: TER-Plus, which extends the Translation Edit Rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases. TER-Plus was shown to be one of the top metrics in NIST's Metrics MATR 2008 Challenge, having the highest average rank in terms of Pearson and Spearman correlation. Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments. We extend the TER algorithm in a similar fashion to produce a new evaluation metric, TER plus (TERp), which allows tuning of the edit costs in order to maximize correlation with human judgment.
Robust Applied Morphological Generation In practical natural language generation systems it is often advantageous to have a separate component that deals purely with morphological processing. We present such a component: a fast and robust morphological generator for English based on finite-state techniques that generates a word form given a specification of the lemma, part-of-speech, and the type of inflection required. We describe how this morphological generator is used in a prototype system for automatic simplification of English newspaper text, and discuss practical morphological and orthographic issues we have encountered in generation of unrestricted text within this application.
A Corpus-Based Approach For Building Semantic Lexicons Semantic knowledge can be a great asset to natural language processing systems, but it is usually hand-coded for each application. Although some semantic information is available in general-purpose knowledge bases such as WordNet and Cyc, many applications require domain-specific lexicons that represent words and categories for a particular topic. In this paper, we present a corpus-based method that can be used to build semantic lexicons for specific categories. The input to the system is a small set of seed words for a category and a representative text corpus. The output is a ranked list of words that are associated with the category. A user then reviews the top-ranked words and decides which ones should be entered in the semantic lexicon. In experiments with five categories, users typically found about 60 words per category in 10-15 minutes to build a core semantic lexicon. We find that nouns in conjunctions or appositives tend to be semantically related. We suggest using conjunction and appositive data to cluster nouns; we approximate this data by looking at the nearest NP on each side of a particular NP. We also give credit for words associated with but not belonging to a particular category.
Automatically Constructing A Lexicon Of Verb Phrase Idiomatic Combinations We investigate the lexical and syntactic flexibility of a class of idiomatic expressions. We develop measures that draw on such linguistic properties, and demonstrate that these statistical, corpus-based measures can be successfully used for distinguishing idiomatic combinations from non-idiomatic ones. We also propose a means for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation. To measure fixedness, we use statistical measures of lexical, syntactic, and overall fixedness. We come up with a dozen possible syntactic forms for verb-object pairs (based on passivization, determiner, and object pluralization) and use a corpus based statistical measure to determine the canonical form (s).
Finding Terminology Translations From Non-Parallel Corpora We present a statistical word feature, the Word Relation Matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across language groups. Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures. Word Relation Matrices are then mapped across the corpora to find translation pairs. Translation accuracies are around 30% when only the top candidate is counted. Nevertheless, top 20 candidate output give a 50.9% average increase in accuracy on human translator performance. In our work, a translation model applied to a pair of unrelated languages (English/Japanese) with a random selection of test words, many of them multi-word terms, gives a precision around 30% when only the top candidate is proposed.
Espresso: Leveraging Generic Patterns For Automatically Harvesting Semantic Relations In this paper, we present Espresso, a weakly-supervised, general-purpose, and accurate algorithm for harvesting semantic relations. The main contributions are: i) a method for exploiting generic patterns by filtering incorrect instances using the Web; and ii) a principled measure of pattern and instance reliability enabling the filtering algorithm. We present an empirical comparison of Espresso with various state of the art systems, on different size and genre corpora, on extracting various general and specific relations. Experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision. In the pattern induction step, our system computes a reliability score for each candidate pattern based on the weighted pointwise mutual information, PMI, of the pattern with all instances extracted so far. We induce specific reliable patterns in a bootstrapping manner for entity relation extraction. Our minimally-supervised Espresso algorithm is initialized with a single set that mixes seeds of heterogeneous types, such as leader-panel and oxygen-water, which respectively correspond to the member-of and sub-quantity-of relations in the taxonomy of Keet and Artale (2008).
Hierarchical Phrase-Based Translation   We present a statistical machine translation model that uses hierarchical phrases—phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations. Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation. We describe our system’s training and decoding methods in detail, and evaluate it for translation speed and translation accuracy. Using BLEU as a metric of translation accuracy, we find that our system performs significantly better than the Alignment Template System, a state-of-the-art phrase-based system. The hierarchical phrase-based model makes an advance of statistical machine translation by employing hierarchical phrases, which not only uses phrases to learn local translations but also uses hierarchical phrases to capture reorderings of words and subphrases which can cover a large scope.
Evaluating Content Selection In Summarization: The Pyramid Method We present an empirically grounded method for evaluating content selection in summarization. It incorporates the idea that no single best model summary for a collection of documents exists. Our method quantifies the relative importance of facts to be conveyed. We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference. We propose a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents.
Efficient Parsing Of Highly Ambiguous Context-Free Grammars With Bit Vectors An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences. The parser uses bit-vector operations to parallelise the basic parsing operations. The parser is particularly useful when all analyses are needed rather than just the most probable one. We apply the Viterbi algorithm, exploiting its ability to deal with highly-ambiguous grammars.
Feature-Rich Part-Of-Speech Tagging With A Cyclic Dependency Network We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result We present a supervised conditional Markov Model part-of-speech tagger (CMM) which exploited information coming from both left and right contexts.
Detecting Text Similarity Over Short Passages: Exploring Linguistic Feature Combinations Via Machine Learning We present a new composite similarity metric that combines information from multiple linguistic indicators to measure semantic distance between pairs of small textual units. Several potential features are investigated and an optimal combination is selected via machine learning. We discuss a more restrictive definition of similarity than traditional, document-level and information retrieval-oriented, notions of similarity, and motivate it by showing its relevance to the multi-document text summarization problem. Results from our system are evaluated against standard information retrieval techniques, establishing that the new method is more effective in identifying closely related textual units. At the level of short passages or sentences, we go beyond N-gram, taking advantage of WordNet synonyms, as well as ordering and distance between shared words.
Statistical Decision-Tree Models For Parsing Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to text-processing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing n-gram modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATTER achieves 86% precision, 86% recall, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% pre- cision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length. We create FTB-UC-DEP, a depenency tree bank derived from FTB-UC using the technique of head propagation rules. We find that lexicalization substantially improves performance compared to an unlexicalized baseline model such as a probabilistic context-free grammar.
Identifying Semantic Roles Using Combinatory Categorial Grammar We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar. This system performs at least as well as a system based on a traditional Treebank parser, and outperforms it on core argument roles. We find that using features extracted from a Combinatory Categorical Grammar representation improves semantic labeling performance on core arguments.
Mining WordNet For A Fuzzy Sentiment: Sentiment Tag Extraction From WordNet Glosses Many of the tasks required for semantic tagging of phrases and texts rely on a list of words annotated with some semantic features. We present a method for extracting sentiment-bearing adjectives from WordNet using the Sentiment Tag Extraction Program (STEP). We did 58 STEP runs on unique non-intersecting seed lists drawn from manually annotated list of positive and negative adjectives and evaluated the results against other manually annotated lists. The 58 runs were then collapsed into a single set of 7,813 unique words. For each word we computed a Net Overlap Score by subtracting the total number of runs assigning this word a negative sentiment from the total of the runs that consider it positive. We demonstrate that Net Overlap Score can be used as a measure of the words degree of membership in the fuzzy category of sentiment: the core adjectives, which had the highest Net Overlap scores, were identified most accurately both by STEP and by human annotators, while the words on the periphery of the category had the lowest scores and were associated with low rates of inter-annotator agreement. We find that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets. Non-neutral adjectives were extracted from WordNet and assigned fuzzy sentiment category membership/centrality scores and tags. WordNet synonyms, antonyms, and glosses are used to iteratively expand a list of seeds.
Clause Restructuring For Statistical Machine Translation We describe a method for incorporating syntactic information in statistical machine translation systems. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the parse tree, effectively reordering the surface string on the source language side of the translation system. The goal of this step is to recover an underlying word order that is closer to the target language word-order than the original string. The reordering approach is applied as a pre-processing step in both the training and decoding phases of a phrase-based statistical MT system. We describe experiments on translation from German to English, showing an improvement from 25.2% Bleu score for a baseline system to 26.8% Bleu score for the system with reordering, a statistically significant improvement. We present sign test to measure the siginificance of score improvement in BLUE. We note that it is not clear whether the conditions required by bootstrap resampling are met in the case of BLUE, and recommend the sign test instead. We use six hand-crafted reordering rules targeting the placement of verbs, subjects, particles and negation.
From Discourse Structures To Text Summaries We describe experiments that show that the concepts of rhetorical analysts and nuclearity can be used effectively for determining the most important units in a text. We show how these concepts can be implemented and we discuss results that we obtained with a discourse-based summarization program.
Word Association Norms Mutual Information And Lexicography The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor.) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand :mbjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words. In our work, the significance of an association (x, y) is measured by the mutual information I (x, y) ,i.e. the probability of observing x and y together, compared with the probability of observing x and y independently.
Chart Generation Charts constitute a natural uniform architecture for parsing and generation provided string position is replaced by a notion more appropriate to logical forms and that measures are taken to curtail generation paths containing semantically incomplete phrases. We propose to reduce the number of constituents build during realisation by only considering for combination constituents with non overlapping semantics and compatible indices. We propose a chart based generation process which takes packed representations as input and generates all paraphrases without expanding first into disjunctive normal form.
Stochastic Lexicalized Inversion Transduction Grammar For Alignment We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training. Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences. We present a model in which the nonterminals are lexicalized by English and foreign language word pairs so that the inversions are dependent on lexical information on the left hand side of synchronous rules. We propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. Tic-tac-toe pruning algorithm uses dynamic programming to compute inside and outside scores for a span pair in O (n4).
Word Representations: A Simple and General Method for Semi-Supervised Learning If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/ We show that monolingual word clusters are broadly applicable as features in monolingual models for linguistic structure prediction.
The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task This paper presents the task definition, resources, participation, and comparative results for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name. We consider the problem of disambiguating person names in a Web searching scenario. The goal of the Web People Search task is to assign Web pages to groups, where each group contains all (and only those) pages that refer to one unique entity. Our Web Persona Search (WePS) task has created a benchmark dataset.
Accurate Information Extraction From Research Papers Using Conditional Random Fields With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance. This paper employs Conditional Random Fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers. The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. This paper makes an empirical exploration of several factors, including variations on Gaussian, exponential and hyperbolic-L1 priors for improved regularization, and several classes of features and Markov order. On a standard benchmark data set, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs. CORA consists of two collections: a set of research paper headers annotated for entities such as title, author, and institution; and a collection of references annotated with BibTeX fields such as journal, year, and publisher.
Semantic-Head-Driven Generation We present an algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applicable. In particular, unlike a previous bottom-up generator, it allows use of semlantically nonmonotonic grammars, yet unlike top-down methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion. We introduce a head-driven algorithm for generating from logical forms.
Contextual Dependencies In Unsupervised Word Segmentation Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages, and may shed light on how humans learn to segment speech. We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively. The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation. We also show that previous probabilistic models rely crucially on sub-optimal search procedures. We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level. We use hierarchical Dirichlet processes (HDP) to induce contextual word models.
A Statistical Model For Domain-Independent Text Segmentation We propose a statistical method that finds the maximum-probability segmentation of a given text. This method does not require training data because it estimates probabilities from the given text. Therefore, it can be applied to any text in any domain. An experiment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system. We model the problem of TS as a problem of finding the minimum cost path in a graph and therefore adopts a dynamic programming algorithm. We introduce one of the first probabilistic approaches using Dynamic Programming (DP) called U00.
On Coreference Resolution Performance Metrics The paper proposes a Constrained Entity-Alignment F-Measure (CEAF) for evaluating coreference resolution. The metric is computed by aligning reference and system entities (or coreference chains) with the constraint that a system (reference) entity is aligned with at most one reference (system) entity. We show that the best alignment is a maximum bipartite matching problem which can be solved by the Kuhn-Munkres algorithm. Comparative experiments are conducted to show that the widely-known MUC F-measure has serious flaws in evaluating a coreference system. The proposed metric is also compared with the ACE-Value, the official evaluation metric in the Automatic Content Extraction (ACE) task, and we conclude that the proposed metric possesses some properties such as symmetry and better interpretability missing in the ACE-Value. We use a Bell tree to score and store the searching path.
Experiments Using Stochastic Search For Text Planning Marcu has characterised an important and difficult problem in text planning: given a set of facts to convey and a set of rhetorical relations that can be used to link them together, how can one arrange this material so as to yield the best possible text? We describe experiments with a number of heuristic Search methods for this task. We investigate the problem of determining a discourse tree for a set of elementary speech acts which are partially constrained by rhetorical relations. We advocate genetic algorithms as an alternative to exhaustively searching for the optimal ordering of descriptions of museum artefacts.
Lexical Semantic Techniques For Corpus Analysis In this paper we outline a research program for computational linguistics, making extensive use of text corpora. We demonstrate how a semantic framework for lexical knowledge can suggest richer relationships among words in text beyond that of simple co-occurrence. The work suggests how linguistic phenomena such as metonymy and polysemy might be exploitable for semantic tagging of lexical items. Unlike with purely statistical collocational analyses, the framework of a semantic theory allows the automatic construction of predictions about deeper semantic relationships among words appearing in collocational systems. We illustrate the approach for the acquisition of lexical information for several classes of nominals, and how such techniques can fine-tune the lexical structures acquired from an initial seeding of a machine-readable dictionary. In addition to conventional lexical semantic relations, we show how information concerning lexical presuppositions and preference relations can also be acquired from corpora, when analyzed with the appropriate semantic tools. Finally, we discuss the potential that corpus studies have for enriching the data set for theoretical linguistic research, as well as helping to confirm or disconfirm linguistic hypotheses. we present an interesting framework for the acquisition of semantic relations from corpora not only relying on statistics, but guided by theoretical lexicon principles. We show how statistical techniques, such as mutual information measures can contribute to automatically acquire lexical information regarding the link between a noun and a predicate. We use generalized syntactic patterns for extracting qualia structures from a partially parsed corpus.
Log-Linear Models For Word Alignment We present a framework for word alignment based on log-linear models. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible additional variables. Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information. In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features. Our experiments show that log-linear models significantly outperform IBM translation models. We present a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment.
Building Deep Dependency Structures Using A Wide-Coverage CCG Parser This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures. The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies. A set of dependency structures used for training and testing the parser is obtained from a treebank of CCG normal-form derivations, which have been derived (semi-) automatically from the Penn Treebank. The parser correctly recovers over 80% of labelled dependencies, and around 90% of unlabelled dependencies. We provide examples showing how heads can fill dependency slots during a derivation, and how long-range dependencies can be recovered through unification of co-indexed head variables. We define predicate argument structure for CCG in terms of the dependencies that hold between words with lexical functor categories and their arguments.
On the Complexity of Non-Projective Data-Driven Dependency Parsing In this paper we investigate several non-projective parsing algorithms for dependency parsing, providing novel polynomial time solutions under the assumption that each dependency decision is independent of all the others, called here the edge-factored model. We also investigate algorithms for non-projective parsing that account for non-local information, and present several hardness results. This suggests that it is unlikely that exact non-projective dependency parsing is tractable for any model richer than the edge-factored model. We claim that the main obstacle is that non-projective parsing is NP-hard beyond arc-factored models.
Indexing With WordNet Synsets Can Improve Text Retrieval The classical, vector space model for text retrieval is shown to give better results (up to 29% better in our experiments) if WordNet synsets are chosen as the indexing space, instead of word forms. This result is obtained for a manually disambiguated test collection (of queries and documents) derived from the SEMCOR semantic concordance. The sensitivity of retrieval performance to (automatic) disambiguation errors when indexing documents is also measured. Finally, it is observed that if queries are not disambiguated, indexing by synsets performs (at best) only as good as standard word indexing. We point out some more weaknesses of WordNet for Information Retrieval purposes, in particular the lack of domain information and the fact that sense distinctions are excessively fine-grained for the task.
First- and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation Forests Many statistical translation models can be regarded as weighted logical deduction. Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs). We then introduce a novel second-order expectation semiring, which computes second-order statistics (e.g., the variance of the hypothesis length or the gradient of entropy). This second-order semiring is essential for many interesting training paradigms such as minimum risk, deterministic annealing, active learning, and semi-supervised learning, where gradient descent optimization requires computing the gradient of entropy or risk. We use these semirings in an open-source machine translation toolkit, Joshua, enabling minimum-risk training for a benefit of up to 1.0 BLEU point. We consider minimum risk training using a linearly decomposable approximation of BLEU. The sufficient statistics for graph expected BLEU can be computed using expectation semirings. We extend the work of Smith and Eisner and obtain much better estimates of feature expectations by using a packed chart instead of an n-best list. We perform expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007).
Application-driven Statistical Paraphrase Generation Paraphrase generation (PG) is important in plenty of NLP applications. However, the research of PG is far from enough. In this paper, we propose a novel method for statistical paraphrase generation (SPG), which can (1) achieve various applications based on a uniform statistical model, and (2) naturally combine multiple resources to enhance the PG performance. In our experiments, we use the proposed method to generate paraphrases for three different applications. The results show that the method can be easily transformed from one application to another and generate valuable and interesting paraphrases. We present a sentence paraphrasing method that can be configured for different tasks, including a form of sentence compression.
On-Demand Information Extraction At present, adapting an Information Extraction system to new topics is an expensive and slow process, requiring some knowledge engineering for each new topic. We propose a new paradigm of Information Extraction which operates 'on demand' in response to a user's query. On-demand Information Extraction (ODIE) aims to completely eliminate the customization effort. Given a user's query, the system will automatically create patterns to extract salient relations in the text of the topic, and build tables from the extracted information using paraphrase discovery technology. It relies on recent advances in pattern discovery, paraphrase discovery, and extended named entity tagging. We report on experimental results in which the system created useful tables for many topics, demonstrating the feasibility of this approach.
Part-Of-Speech Tagging With Neural Networks Text corpora which are tagged with part-of-speech in- formation are useful in many areas of linguistic research. In this paper, a new part-of-speech tagging method based on neural networks (Net-Tagger) is presented and its performance is compared to that of a HMM-tagger (Cutting et al. , 1992) and a trigram-based tagger (Kempe, 1993). It is shown that the Net-Tagger performs as well as the trigram-based tagger and better than the HMM-tagger. The correct rate of tagging has reached 95%, in part by using a very large amount of training data.
ROUGE: A Package For Automatic Evaluation Of Summaries ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.
Biomedical Named Entity Recognition Using Conditional Random Fields And Rich Feature Sets 
Use Of Deep Linguistic Features For The Recognition And Labeling Of Semantic Arguments We use deep linguistic features to predict semantic roles on syntactic arguments, and show that these perform considerably better than surface-oriented features. We also show that predicting labels from a light-weight parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features. We argue that deep linguistic features harvested from FrameNet are beneficial for the successful assignment of PropBank roles to constituents. We use LTAG-based decomposition of parse trees for SRL. Instead of using the typical parse tree features used in SRL models, we use the path within the elementary tree from the predicate to the constituent argument.
Interpretation As Abduction An approach to abductive inference developed in the TACITUS project has resulted in a dramatic simplification of how the problem of interpreting texts is conceptualized. Its use in solving the local pragmatics problems of reference, compound nominals, syntactic ambiguity, and metonymy is described and illustrated. It also suggests an elegant and thorough integration of syntax, semantics, and pragmatics.
An Error-Driven Word-Character Hybrid Model for Joint Chinese Word Segmentation and POS Tagging In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging. Our word-character hybrid model offers high performance since it can handle both known and unknown words. We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an error-driven policy that delivers such balance by acquiring examples of unknown words from particular errors in a training corpus. We describe an efficient framework for training our model based on the Margin Infused Relaxed Algorithm (MIRA), evaluate our approach on the Penn Chinese Treebank, and show that it achieves superior performance compared to the state-of-the-art approaches reported in the literature. We separate the processing of known words and unknown words, and uses a set of segmentation tags to represent the segmentation of characters.
Online Learning Of Approximate Dependency Parsing Algorithms In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word. We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms. We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish. We propose a second-order graph-based dependency parsing model which incorporates features from the two kinds of subtrees. We use the Viterbi decoding algorithm to achieve O (n3) parsing time. We show that non-projective dependency parsing with horizontal Markovization is FNP-hard. We define a second-order dependency parsing model in which interactions between adjacent siblings are allowed.
From Grammar To Lexicon: Unsupervised Learning Of Lexical Syntax Imagine a language that is completely unfamiliar; the only means of studying it are an ordinary grammar book and a very large corpus of text. No dictionary is available. How can easily recognized, surface grammatical facts be used to extract from a corpus as much syntactic information as possible about individual words? This paper describes an approach based on two principles. First, rely on local morpho-syntactic cues to structure rather than trying to parse entire sentences. Second, treat these cues as probabilistic rather than absolute indicators of syntactic structure. Apply inferential statistics to the data collected using the cues, rather than drawing a categorical conclusion from a single occurrence of a cue. The effectiveness of this approach for inferring the syntactic frames of verbs is supported by experiments on an English corpus using a program called Lerner. Lerner starts out with no knowledge of content words--it bootstraps from determiners, auxiliaries, modals, prepositions, pronouns, complementizers, coordinating conjunctions, and punctuation. Our study is focused on large-scaled automatic acquisition of subcategorization frames (SCF).
Unsupervised Discovery Of Morphemes We present two methods for unsupervised segmentation of words into morpheme-like units. The model utilized is especially suited for languages with a rich morphology, such as Finnish. The first method is based on the Minimum Description Length (MDL) principle and works online. In the second method, Maximum Likelihood (ML) optimization is used. The quality of the segmentations is measured using an evaluation method that compares the segmentations produced to an existing morphological analysis. Experiments on both Finnish and English corpora show that the presented methods perform well compared to a current state-of-the-art system. Our method is based on jointly minimizing the size of the morph codebook and the encoded size of all the word forms using the minimum description length MDL cost function.
Efficient Normal-Form Parsing For Combinatory Categorial Grammar Under categorial grammars that have powerful rules like composition, a simple n-word sentence can have exponentially many parses. Generating all parses is inefficient and obscures whatever true semantic ambiguities are in the input. This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar, by means of an efficient, correct, and easy to implement normal-form parsing technique. The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses; that is, spurious ambiguity (as carefully defined) is shown to be both safely and completely eliminated. We provide a safe and complete parsing algorithm which can return non-NF derivations when necessary to preserve an interpretation if composition is bounded or the grammar is restricted in other ways.
Cues And Control In Expert-Client Dialogues We conducted an empirical analysis into the relation between control and discourse structure. We applied control criteria to four dialognes and identified 3 levels of discourse structure. We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control. Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not. We define initiative as being held by the speaker who is driving the conversation at any point in the conversation. We propose rules for tracking initiative based on utterance types: for example, statements, proposals and questions show initiative while answers and acknowledgements do not.
Generalizing Word Lattice Translation Word lattice decoding has proven useful in spoken language translation; we argue that it provides a compelling model for translation of text genres, as well. We show that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous context-free grammar-based models. Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models. Our experiments evaluating the approach demonstrate substantial gains for Chinese-English and Arabic-English translation. In our model, several different segmenters for Chinese are combined to create the lattice. All of the systems we present use the lattice input format to Moses (Dyer et al, 2008), including the baselines which do not need them.
Feature Forest Models for Probabilistic HPSG Parsing Probabilistic modeling of lexicalized grammars is difficult because these grammars exploit complicated data structures, such as typed feature structures. This prevents us from applying common methods of probabilistic modeling in which a complete structure is divided into substructures under the assumption of statistical independence among sub-structures. For example, part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsing is split into applications of CFG rules. These methods have relied on the structure of the target problem, namely lattices or trees, and cannot be applied to graph structures including typed feature structures. This article proposes the feature forest model as a solution to the problem of probabilistic modeling of complex data structures including typed feature structures. The feature forest model provides a method for probabilistic modeling without the independence assumption when probabilistic events are represented with feature forests. Feature forests are generic data structures that represent ambiguous trees in a packed forest structure. Feature forest models are maximum entropy models defined over feature forests. A dynamic programming algorithm is proposed for maximum entropy estimation without unpacking feature forests. Thus probabilistic modeling of any data structures is possible when they are represented by feature forests. This article also describes methods for representing HPSG syntactic structures and predicate–argument structures with feature forests. Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing. The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed.
Improving Translation Quality by Discarding Most of the Phrasetable It is possible to reduce the bulk of phrase-tables for Statistical Machine Translation using a technique based on the significance testing of phrase pair co-occurrence in the parallel corpus. The savings can be quite substantial (up to 90%) and cause no reduction in BLEU score. In some cases, an improvement in BLEU is obtained at the same time although the effect is less pronounced if state-of-the-art phrase table smoothing is employed. We use Fisher's exact test. We filter out statistically unreliable translation pairs.
Meaningful Clustering Of Senses Helps Boost Word Sense Disambiguation Performance Fine-grained sense distinctions are one of the major obstacles to successful Word Sense Disambiguation. In this paper, we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies, namely the Oxford Dictionary of English. We assess the quality of the mapping and the induced clustering, and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task. In our coarse-grained task, the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses. We present an automatic approach for mapping between sense inventories; here similarities in gloss definition and structured relations between the two sense inventories are exploited in order to map between WordNet senses and distinctions made within the coarser-grained Oxford English Dictionary. We argue that automatically creating new alignments is difficult because of word ambiguities, different granularities of senses, or language specific conceptualizations.
A Metalearning Approach to Processing the Scope of Negation Finding negation signals and their scope in text is an important subtask in information extraction. In this paper we present a machine learning system that finds the scope of negation in biomedical texts. The system combines several classifiers and works in two phases. To investigate the robustness of the approach, the system is tested on the three subcorpora of the BioScope corpus representing different text types. It achieves the best results to date for this task, with an error reduction of 32.07% compared to current state of the art results. we describe a method for improving resolution of the scope of negation by combining IGTREE, CRF, and Support Vector Machines (SVM) (Morante and Daelemans, 2009). we pioneered the research on negation scope finding by formulating it as a chunking problem, which classifies the words of a sentence as being inside or outside the scope of a negation signal.
Hierarchical Phrase-Based Translation with Suffix Arrays A major engineering challenge in statistical machine translation systems is the efficient representation of extremely large translation rulesets. In phrase-based models, this problem can be addressed by storing the training data in memory and using a suffix array as an efficient index to quickly lookup and extract rules on the fly. Hierarchical phrase-based translation introduces the added wrinkle of source phrases with gaps. Lookup algorithms used for contiguous phrases no longer apply and the best approximate pattern matching algorithms are much too slow, taking several minutes per sentence. We describe new lookup algorithms for hierarchical phrase-based translation that reduce the empirical computation time by nearly two orders of magnitude, making on-the-fly lookup feasible for source phrases with gaps. The basis of our method is to look for the occurrences of continuous substrings using a Suffix Array, and then intersect them to find the occurrences of discontinuous substrings.
A Statistical Parser For Czech This paper considers statistical parsing of Czech, which differs radically from English in at least two respects: (1) it is a highly inflected language, and (2) it has relatively free word order. These differences are likely to pose new problems for techniques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results - 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text. We use a transformed tree bank from the Prague Dependency Treebank for constituent parsing on Czech.
Joint Determination of Anaphoricity and Coreference Resolution using Integer Programming Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution. In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. This joint ILP formulation provides f-score improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets. By using joint inference for anaphoricity and coreference, we avoid cascade-induced errors without the need to separately optimize the threshold.
Building A Large-Scale Annotated Chinese Corpus In this paper we address issues related to building a large-scale Chinese corpus. We try to answer four questions: (i) how to speed up annotation, (ii) how to maintain high annotation quality, (iii) for what purposes is the corpus applicable, and finally (iv) what future work we anticipate.
(Meta-) Evaluation of Machine Translation This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back. We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process. We measured timing and intra- and inter-annotator agreement for three types of subjective evaluation. We measured the correlation of automatic evaluation metrics with human judgments. This meta-evaluation reveals surprising facts about the most commonly used methodologies. We show that ranking sentences gives higher inter-annotator agreement than scoring adequacy and fluency.
Minimizing Manual Annotation Cost In Supervised Training From Corpora Corpus-based methods for natural language processing often use supervised training, requiring expensive manual annotation of training corpora. This paper investigates methods for reducing annotation cost by sample selection. In this approach, during training the learning program examines many unlabeled examples and selects for labeling (annotation) only those that are most informative at each stage. This avoids redundantly annotating examples that contribute little new information. This paper extends our previous work on committee-based sample selection for probabilistic classifiers. We describe a family of methods for committee-based sample selection, and report experimental results for the task of stochastic part-of-speech tagging. We find that all variants achieve a significant reduction in annotation cost, though their computational efficiency differs. In particular, the simplest method, which has no parameters to tune, gives excellent results. We also show that sample selection yields a significant reduction in the size of the model used by the tagger. We use HMMs for POS tagging and find that selective samplying of sentences can significantly reduce the number of samples required to achieve desirable tag accuracies. We use the vote entropy metric, the entropy of the distribution of labels assigned to an example by the ensemble of classifiers, to estimate the disagreement within an ensemble.
A Clustering Approach For Nearly Unsupervised Recognition Of Nonliteral Language In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques. TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies. It also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning. We adapt a word-sense disambiguation algorithm to our task and augment it with multiple seed set learners, a voting schema, and additional features like SuperTags and extra-sentential context. Detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4%. Using the TroFi algorithm, we also build the TroFi Example Base, an extensible resource of annotated literal/nonliteral examples which is freely available to the NLP research community. For scoring, Literal recall is defined as (correct literals in literal cluster/ total correct literals); Literal precision is defined as (correct literals in literal cluster/ size of literal cluster). We model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two automatically constructed seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set.
Exploiting Diversity In Natural Language Processing: Combining Parsers Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy. Two general approaches are presented and two combination techniques are described for each approach. Both parametric and non-parametric models are explored. The resulting parsers surpass the best previously published performance results for the Penn Treebank. We improve the best parser's F-measure of 89.7 to 91.3, using naive Bayes voting on the Penn TreeBank constituent structures (16% error reduction). Regarding the system combination study, we propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees. We perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined.
Models For The Semantic Classification Of Noun Phrases This paper presents an approach for detecting semantic relations in noun phrases. A learning algorithm, called semantic scattering, is used to automatically label complex nominals, genitives and adjectival noun phrases with the corresponding semantic relation. We propose a 35 class scheme to classify relations in various phrases. We propose a method called semantic scattering for interpreting NCs.
A Syntactic Approach To Discourse Semantics A correct structural analysis of a discourse is a prerequisite for understanding it. This paper sketches the outline of a discourse grammar which acknowledges several different levels of structure. This grammar, the "Dynamic Discourse Model", uses an Augmented Transition Network parsing mechanism to build a representation of the semantics of a discourse in a stepwise fashion, from left to right, on the basis of the semantic representations of the individual clauses which constitute the discourse. The intermediate states of the parser model the intermediate states of the social situation which generates the discourse. The paper attempts to demonstrate that a discourse may indeed be viewed as constructed by means of sequencing and recursive nesting of discourse constituents. It gives rather detailed examples of discourse structures at various levels, and shows how these structures are described in the framework proposed here.
Bidirectional Inference With The Easiest-First Strategy For Tagging Sequence Data This paper presents a bidirectional inference algorithm for sequence labeling problems such as part-of-speech tagging, named entity recognition and text chunking. The algorithm can enumerate all possible decomposition structures and find the highest probability sequence together with the corresponding decomposition structure in polynomial time. We also present an efficient decoding algorithm based on the easiest-first strategy, which gives comparably good performance to full bidirectional inference with significantly lower computational cost. Experimental results of part-of-speech tagging and text chunking show that the proposed bidirectional inference methods consistently outperform unidirectional inference methods and bidirectional MEMMs give comparable performance to that achieved by state-of-the-art learning algorithms including kernel support vector machines. We propose easiest-first deterministic decoding.
Fast Decoding And Optimal Decoding For Machine Translation A good decoding algorithm is critical to the success of any statistical machine translation system. The decoder’s job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them). Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions. In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem. We compare translations obtained by a multi-stack decoder and a greedy hill-climbing algorithm against those produced by an optimal integer programming decoder that treats decoding as a variant of the traveling-salesman problem.
An Empirical Study of Semi-supervised Structured Conditional Models for Dependency Parsing This paper describes an empirical study of high-performance dependency parsers based on a semi-supervised learning approach. We describe an extension of semi-supervised structured conditional models (SS-SCMs) to the dependency parsing problem, whose framework is originally proposed in (Suzuki and Isozaki, 2008). Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with another semi-supervised approach, described in (Koo et al., 2008). The second extension is to apply the approach to second-order parsing models, such as those described in (Carreras, 2007), using a two-stage semi-supervised learning approach. We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections: the Penn Treebank for English, and the Prague Dependency Treebank for Czech. Our best results on test data in the above datasets achieve 93.79% parent-prediction accuracy for English, and 88.05% for Czech. We present a very effective semi-supervised approach in which features from multiple generative models estimated on unlabeled data are combined in a discriminative system for structured prediction.
Learning To Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment We address the text-to-text generation problem of sentence-level paraphrasing — a phenomenon distinct from and more difficult than word- or phrase-level paraphrasing. Our approach applies multiple-sequence alignment to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented by word lattice pairs and automatically determines how to apply these patterns to rewrite new sentences. The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems. We propose to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. We construct lattices over paraphrases using an iterative pairwise multiple sequence alignment (MSA) algorithm. We propose a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations. We present an approach for generating sentence level paraphrases, learning structurally similar patterns of expression from data and identifying paraphrasing pairs among them using a comparable corpus.
Word Sense Disambiguation With Very Large Neural Networks Extracted From Machine Readable Dictionaries In this paper, we describe a means for automatically building very large neural networks (VLNNs) from definition texts in machine-readable dictionaries, and demonstrate the use of these networks for word sense disambiguation. Our method brings together two earlier, independent approaches to word sense disambiguation: the use of machine-readable dictionaries and spreading and activation models. The automatic construction of VLNNs enables real-size experiments with neural networks for natural language processing, which in turn provides insight into their behavior and design and can lead to possible improvements. We apply conventional spreading activation approaches to word sense disambiguation.
Non-Projective Dependency Parsing Using Spanning Tree Algorithms We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al., 2003; McDonald et al., 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies. The key idea is to build a complete graph consisting of tokens of the sentence where each edge is weighted by a learned scoring function.
The Web As A Baseline: Evaluating The Performance Of Unsupervised Web-Based Models For A Range Of NLP Tasks Previous work demonstrated that web counts can be used to approximate bigram frequencies, and thus should be useful for a wide variety of NLP tasks. So far, only two generation tasks (candidate selection for machine translation and confusion-set disambiguation) have been tested using web-scale data sets. The present paper investigates if these results generalize to tasks covering both syntax and semantics, both generation and analysis, and a larger range of n-grams. For the majority of tasks, we find that simple, unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a large corpus. However, in most cases, web-based models fail to outperform more sophisticated state-of-the-art models trained on small corpora. We argue that web-based models should therefore be used as a baseline for, rather than an alternative to, standard models. Our web-based unsupervised model classifies noun noun instances based on Lauer's list of 8 prepositions and uses the web as the training corpus.
A Simple Pattern-Matching Algorithm For Recovering Empty Nodes And Their Antecedents This paper describes a simple pattern matching algorithm for recovering empty nodes and identifying their co-indexed antecedents in phrase structure trees that do not contain this information. The patterns are minimal connected tree fragments containing an empty node and all other nodes co-indexed with it. This paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure, which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a gold standard corpus. Evaluating the algorithm on the output of Charniak’s parser (Charniak, 2000) and the Penn treebank (Marcus et al., 1993) shows that the pattern matching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity. We propose an algorithm that is able to find long-distance dependencies, as a post processing step, after parsing. While Charniak's parser does not generate empty category information, we have developed an algorithm that extracts patterns from the Treebank which can be used to insert empty categories into the parser's output. It is the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees.
A Phrase-Based Statistical Model For SMS Text Normalization Short Messaging Service (SMS) texts behave quite differently from normal written texts and have some very special phenomena. To translate SMS texts, traditional approaches model such irregularities directly in Machine Translation (MT). However, such approaches suffer from customization problem as tremendous effort is required to adapt the language model of the existing translation system to handle SMS text style. We offer an alternative approach to resolve such irregularities by normalizing SMS texts before MT. In this paper, we view the task of SMS normalization as a translation problem from the SMS language to the English language and we propose to adapt a phrase-based statistical MT model for the task. Evaluation by 5-fold cross validation on a parallel SMS normalized corpus of 5000 sentences shows that our method can achieve 0.80702 in BLEU score against the baseline BLEU score 0.6958. Another experiment of translating SMS texts from English to Chinese on a separate SMS text corpus shows that, using SMS normalization as MT preprocessing can largely boost SMS translation performance from 0.1926 to 0.3770 in BLEU score. We also use Phrase-based SMT techniques on character level. We use a phrase-based statistical machine translation model by splitting sentences into their k most probable phrases.
Comlex Syntax: Building A Computational Lexicon We describe the design of Complex Syntax, a computational lexicon providing detailed syntactic information for approximately 38,000 English headwords. We consider the types of errors which arise in creating such a lexicon, and how such errors can be measured and controlled. Our COMLEX syntax dictionary provides verb subategorization information and syntactic paraphrases, but they are indexed by words thus not suitable to use in generation directly.
Extracting Paraphrases From A Parallel Corpus While paraphrasing is critical both for interpretation and generation of natural language, current systems use manual or semi-automatic methods to collect paraphrases. We present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple English translations of the same source text. Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases. We incorporate part-of-speech information and other morphosyntactic clues into our co-training algorithm.
Language Independent Named Entity Recognition Combining Morphological And Contextual Evidence Identifying and classifying personal, geographic, institutional or other names in a text is an important task for numerous applications. This paper describes and evaluates a language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns captured in hierarchically smoothed trie models. The algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required language-specific information, tokenizers or tools. We consider one sense per discourse phenomenon, the tendency of terms to have a fixed meaning within a single document. We take a character-level approach to named entity recognition (NER) using prefix and suffix tries. The bootstrapping stage uses the initial or current entity assignments to estimate the class conditional distributions for both entities and contexts along their trie paths, and then re-estimates the distributions of the contexts/entity-candidates to which they are linked, recursively, until all accessible nodes are reached.
Mining Very-Non-Parallel Corpora: Parallel Sentence And Lexicon Extraction Via Bootstrapping And EM We present a method capable of extracting parallel sentences from far more disparate “very-non-parallel corpora” than previous “comparable corpora” methods, by exploiting bootstrapping on top of IBM Model 4 EM. Step 1 of our method, like previous methods, uses similarity measures to find matching documents in a corpus first, and then extracts parallel sentences as well as new word translations from these documents. But unlike previous methods, we extend this with an iterative bootstrapping framework based on the principle of “find-one-get-more”, which claims that documents found to contain one pair of parallel sentences must contain others even if the documents are judged to be of low similarity. We re-match documents based on extracted sentence pairs, and refine the mining process iteratively until convergence. This novel “find-one-get-more” principle allows us to add more parallel sentences from dissimilar documents, to the baseline set. Experimental results show that our proposed method is nearly 50% more effective than the baseline method without iteration. We also show that our method is effective in boosting the performance of the IBM Model 4 EM lexical learner as the latter, though stronger than Model 1 used in previous work, does not perform well on data from very-non-parallel corpus.
Disambiguation Of Proper Names In Text Identifying the occurrences of proper names in text and the entities they refer to can be a difficult task because of the many-to-many mapping between names and their referents. We analyze the types of ambiguity -- structural and semantic -- that make the discovery of proper names difficult in text, and describe the heuristics used to disambiguate names in Nominator, a fully-implemented module for proper name recognition developed at the IBM T.J. Watson Research Center. We use hand-written rules and knowledge bases to classify proper names into broad categories.
Discriminative Reranking For Natural Language Parsing This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75% F-measure, a 13% relative decrease in F-measure error over the baseline model’s score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative—in terms of both simplicity and efficiency—to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation. We show that applying reranking techniques to the n-best output of a base parser can improve parsing performance. We propose a method only updates values of features co-occurring with a rule feature on examples at each iteration.
The Stanford Typed Dependencies Representation This paper examines the Stanford typed dependencies representation, which was designed to provide a straightforward description of grammatical relations for any user who could benefit from automatic text understanding. For such purposes, we argue that dependency schemes must follow a simple design and provide semantically contentful information, as well as offer an automatic procedure to extract the relations. We consider the underlying design principles of the Stanford scheme from this perspective, and compare it to the GR and PARC representations. Finally, we address the question of the suitability of the Stanford scheme for parser evaluation. Stanford dependencies provide a simple description of relations between pairs of words in a sentence.
An Improved Error Model For Noisy Channel Spelling Correction The noisy channel model has been applied to a wide range of problems, including spelling correction. These models consist of two components: a source model and a channel model. Very little research has gone into improving the channel model for spelling correction. This paper describes a new channel model for spelling correction, based on generic string to string edits. Using this model gives significant performance improvements compared to previously proposed models. We present an improved error model for noisy channel spelling correction that goes beyond single insertions, deletions, substitutions, and transpositions. We show that adding a source language model increases the accuracy significantly. We characterise the error model by computing the product of operation probabilities on slice-by-slice string edits. We introduce a model that worked on character sequences, not only on character level, and was conditioned on where in the word the sequences occurred.
Transition-based Dependency Parsing with Rich Non-local Features Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations. In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems. In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transition-based parsing and rivaling the best results overall. For the Chinese Treebank, they give a signficant improvement of the state of the art. An open source release of our parser is freely available. We develop the feature template for the arc-eager model.
Empirical Methods For Compound Splitting Compounded words are a challenge for NLP applications such as machine translation (MT). We introduce methods to learn splitting rules from monolingual and parallel corpora. We evaluate them against a gold standard and measure their impact on performance of statistical MT systems. Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task. We present a method requiring no linguistically motivated morphological analysis to split compounds. We split German compound words, based on the frequency of the words in the potential decompositions.
Adaptation Of Maximum Entropy Capitalizer: Little Data Can Help A Lot A novel technique for maximum “a posteriori” (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented. The technique is applied to the problem of automatically capitalizing uniformly cased text. Automatic capitalization is a practically relevant problem: speech recognition output needs to be capitalized; also, modern word processors perform capitalization among other text proofing algorithms such as spelling correction and grammar checking. Capitalization can be also used as a preprocessing step in named entity extraction or machine translation. A “background” capitalizer trained on 20 M words of Wall Street Journal (WSJ) text from 1987 is adapted to two Broadcast News (BN) test sets – one containing ABC Primetime Live text and the other NPR Morning News/CNN Morning Edition text – from 1996. The “in-domain” performance of the WSJ capitalizer is 45% better relative to the 1-gram baseline, when evaluated on a test set drawn from WSJ 1994. When evaluating on the mismatched “out-of-domain” test data, the 1-gram baseline is outperformed by 60% relative; the improvement brought by the adaptation technique using a very small amount of matched BN data – 25–70k words – is about 20–25% relative. Overall, automatic capitalization error rate of 1.4% is achieved on BN data. The performance gain obtained by employing our adaptation technique using a tiny amount of out-of-domain training data on top of the background data is striking: as little as 0.14 M words of in-domain data brings more improvement than using 10 times more background training data (from 2 M words to 20 M words). we proposed method for transfer learning in Maximum Entropy models involves modifying the mu's of this Gaussian prior. we use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data.
Unsupervised Learning Of Disambiguation Rules For Part Of Speech Tagging In this paper we describe an unsupervised learning algorithm for automatically training a rule-based part of speech tagger without using a manually tagged corpus. We compare this algorithm to the Baum-Welch algorithm, used for unsupervised training of stochastic taggers. Next, we show a method for combining unsupervised and supervised rule-based training algorithms to create a highly accurate tagger using only a small amount of manually tagged text. We present a rule-based part-of-speech tagger for unsupervised training corpus. We propose a method to acquire context-dependent POS disambiguation rules and created an accurate tagger, even from a very small annotated text by combining supervised and unsupervised learning.
Word-Sense Disambiguation Using Statistical Models Of Roget's Categories Trained On Large Corpora This paper describes a program that disambignates English word senses in unrestricted text using statistical models of the major Roget's Thesaurus categories. Roget's categories serve as approximations of conceptual classes. The categories listed for a word in Roger's index tend to correspond to sense distinctions; thus selecting the most likely category provides a useful level of sense disambiguation. The selection of categories is accomplished by identifying and weighting words that are indicative of each category when seen in context, using a Bayesian theoretical framework. Other statistical approaches have required special corpora or hand-labeled training examples for much of the lexicon. Our use of class models overcomes this knowledge acquisition bottleneck, enabling training on unrestricted monolingual text without human intervention. Applied to the 10 million word Grolier's Encyclopedia, the system correctly disambiguated 92% of the instances of 12 polysemous words that have been previously studied in the literature. We rely on the intuition that the senses of words are hinted at by their contextual information. From the perspective of a generative process, neighboring words of a target are generated by the target's underlying sense.
A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based On Minimum Cuts Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as "thumbs up" or "thumbs down". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints. We created a movie-review dataset for opinion detection. We argue that subjectivity detection performed prior to the sentiment analysis leads to better results in the latter. We show that sentence level classification can improve document level analysis. In our subjectivity detection method, soft local consistency constraints are created between every sentence in a document and inference is solved using a min-cut algorithm.
Role Of Word Sense Disambiguation In Lexical Acquisition: Predicting Semantics From Syntactic Cues This paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources. We describe two experiments: one which ignored word-sense distinctions, resulting in 6.3% accuracy for semantic classification of verbs based on (Levin, 1993); and one which exploited word-sense distinctions, resulting in 97.9% accuracy. These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses. Finally, we show that we can provide effective acquisition techniques for novel word senses using a combination of online sources. We show that if we were given the perfect knowledge of the possible syntactic frames, verbs can be classified into the correct classes almost perfectly.
The Proposition Bank: An Annotated Corpus Of Semantic Roles The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated. We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty "trace" categories of the treebank. As proposition banks are semantically annotated versions of a Penn-style tree bank, they provide consistent semantic role labels across different syntactic realizations of the same verb.
Preemptive Information Extraction Using Unrestricted Relation Discovery We are trying to extend the boundary of Information Extraction (IE) systems. Existing IE systems require a lot of time and human effort to tune for a new scenario. Preemptive Information Extraction is an attempt to automatically create all feasible IE systems in advance without human intervention. We propose a technique called Unrestricted Relation Discovery that discovers all possible relations from texts and presents them as tables. We present a preliminary system that obtains reasonably good results. We apply NER, coreference resolution and parsing to a corpus of newspaper articles to extract two-place relations between NEs. We rely further on supervised methods, defining features over a full syntactic parse, and exploit multiple descriptions of the same event in newswire to identify useful relations. Preemptive IE is a paradigm that first groups documents based on pairwise vector clustering, then applies additional clustering to group entities based on document clusters.
Applying Conditional Random Fields To Japanese Morphological Analysis This paper presents Japanese morphological analysis based on conditional random fields (CRFs). Previous work in CRFs assumed that observation sequence (word) boundaries were fixed. However, word boundaries are not clear in Japanese, and hence a straightforward application of CRFs is not possible. We show how CRFs can be applied to situations where word boundary ambiguity exists. CRFs offer a solution to the long-standing problems in corpus-based or statistical Japanese morphological analysis. First, flexible feature designs for hierarchical tagsets become possible. Second, influences of label and length bias are minimized. We experiment CRFs on the standard testbed corpus used for Japanese morphological analysis, and evaluate our results using the same experimental dataset as the HMMs and MEMMs previously reported in this task. Our results confirm that CRFs not only solve the long-standing problems but also improve the performance over HMMs and MEMMs. we studied Japanese word segmentation and POS tagging using conditional random fields (CRFs) and rule based unknown word processing.
Investigating GIS And Smoothing For Maximum Entropy Taggers This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (Gis) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar. Our supertagger finds the single most probable category sequence given the sentenc and uses additional features defined in terms of the previously assigned categories.
Learning To Recognize Features Of Valid Textual Entailments This paper advocates a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment. Current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text, using a locally decomposable matching score. We argue that there are significant weaknesses in this approach, including flawed assumptions of monotonicity and locality. Instead we propose a pipelined approach where alignment is followed by a classification step, in which we extract features representing high-level characteristics of the entailment problem, and pass the resulting feature vector to a statistical classifier trained on development data. We report results on data from the 2005 Pascal RTE Challenge which surpass previously reported results for alignment-based systems. We emphasize that there is more to inferential validity than close lexical or structural correspondence: negations, models, non-factive and implicative verbs, and other linguistic constructs can affect validity in ways hard to capture in alignment.
Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages We introduce a novel precedence reordering approach based on a dependency parser to statistical machine translation systems. Similar to other preprocessing reordering approaches, our method can efficiently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding. For a set of five subject-object-verb (SOV) order languages, we show significant improvements in BLEU scores when translating from English, compared to other reordering approaches, in state-of-the-art phrase-based SMT systems. We show that translation between subject-verb-object (English) and subject-object-verb (Pashto) languages can be improved by reordering the source side of the parallel data. On Web text, we report significant improvements applying one set of hand-crafted rules to translation from English to each of five SOV languages: Korean, Japanese, Hindi, Urdu and Turkish.
A Comparison of Vector-based Representations for Semantic Composition In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods. We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication. Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora. We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection. The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method. We compute a weighted linear combination of the embeddings for words that appear in the document to be classified. We compare count and predict representations as input to composition functions. For paraphrase detection, we use cosine similarity between sentence pairs together with two shallow similarity cues: word overlap between the two sentences and difference in sentence length. Add and mult attained the top performance with the simple models for both figures of merit.
A Boosting Algorithm For Classification Of Semi-Structured Text The focus of research in text classification has expanded from simple topic identification to more challenging tasks such as opinion/modality identification. Unfortunately, the latter goals exceed the ability of the traditional bag-of-word representation approach, and a richer, more structural representation is required. Accordingly, learning algorithms must be created that can handle the structures observed in texts. In this paper, we propose a Boosting algorithm that captures sub-structures embedded in texts. The proposal consists of i) decision stumps that use subtrees as features and ii) the Boosting algorithm which employs the subtree-based decision stumps as weak learners. We also discuss the relation between our algorithm and SVMs with tree kernel. Two experiments on opinion/modality classification confirm that subtree features are important. We adopt the BACT learning algorithm to effectively learn subtrees useful for both antecedent identification and zero pronoun detection.
The Second International Chinese Word Segmentation Bakeoff The second international Chinese word segmentation bakeoff was held in the summer of 2005 to evaluate the current state of the art in word segmentation. Twenty three groups submitted 130 result sets over two tracks and four different corpora. We found that the technology has improved over the intervening two years, though the out-of-vocabulary problem is still of paramount importance. In the Second International Chinese Word Segmentation Bakeoff, two of the highest scoring systems in the closed track competition were based on a CRF model.
Chunking With Support Vector Machines We apply Support Vector Machines (SVMs) to identify English base phrases (chunks). SVMs are known to achieve high generalization performance even with input data of high dimensional feature spaces. Furthermore, by the Kernel principle, SVMs can carry out training with smaller computational overhead independent of their dimensionality. We apply weighted voting of 8 SVMs-based systems trained with distinct chunk representations. Experimental results show that our approach achieves higher accuracy than previous approaches. In this paper, we develop an SVMs-based chunking tool YamCha.
Tree Kernel-Based Relation Extraction with Context-Sensitive Structured Parse Tree Information This paper proposes a tree kernel with context-sensitive structured parse tree information for relation extraction. It resolves two critical problems in previous tree kernels for relation extraction in two ways. First, it automatically determines a dynamic context-sensitive tree span for relation extraction by extending the widely-used Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT. Second, it proposes a context-sensitive convolution tree kernel, which enumerates both context-free and context-sensitive sub-trees by considering their ancestor node paths as their contexts. Moreover, this paper evaluates the complementary nature between our tree kernel and a state-of-the-art linear kernel. Evaluation on the ACE RDC corpora shows that our dynamic context-sensitive tree span is much more suitable for relation extraction than SPT and our tree kernel outperforms the state-of-the-art Collins and Duffy's convolution tree kernel. It also shows that our tree kernel achieves much better performance than the state-of-the-art linear kernels. Finally, it shows that feature-based and tree kernel-based methods much complement each other and the composite kernel can well integrate both flat and structured features. Our composite kernel depends partially on a full parse, and partially on a collection of shallow syntactic features.
Scaling Web-Based Acquisition Of Entailment Relations Paraphrase recognition is a critical step for natural language interpretation. Accordingly, many NLP applications would benefit from high coverage knowledge bases of paraphrases. However, the scalability of state-of-the-art paraphrase acquisition approaches is still limited. We present a fully unsupervised learning algorithm for Web-based extraction of entailment relations, an extended model of paraphrases. We focus on increased scalability and generality with respect to prior work, eventually aiming at a full scale knowledge base. Our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the Web for related syntactic entailment templates. Experiments show promising results with respect to the ultimate goal, achieving much better scalability than prior Web-based methods. we describe the TEASE method for extracting entailing relation templates from the Web.
An Improved Extraction Pattern Representation Model For Automatic IE Pattern Acquisition Several approaches have been described for the automatic unsupervised acquisition of patterns for information extraction. Each approach is based on a particular model for the patterns to be acquired, such as a predicate-argument structure or a dependency chain. The effect of these alternative models has not been previously studied. In this paper, we compare the prior models and introduce a new model, the Subtree model, based on arbitrary subtrees of dependency trees. We describe a discovery procedure for this model and demonstrate experimentally an improvement in recall using Subtree patterns. Our method consists of three phases to learn extraction patterns from the source documents for a scenario specified by the user. We use frequent dependency subtrees as measured by TF*IDF to identify named entities and IE patterns important for a given domain. We also propose representations for IE patterns which extends the SVO representation.
Loosely Tree-Based Alignment For Machine Translation We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length. This is done by adding a new subtree cloning operation to either tree-to-string or tree-to-tree alignment algorithms. We found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model than with a tree-to-tree model using two trees. We train a system on parallel constituent trees from the Korean-English Treebank, evaluating agreement with hand-annotated word alignments. The "clone" operation allows words to be aligned even in cases of radically mismatched trees, at a cost in the probability of the alignment.
SPMT: Statistical Machine Translation With Syntactified Target Language Phrases We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a human-based quality metric that ranks translations on a scale from 1 to 5.
Three Heads Are Better Than One Machine translation (MT) systems do not currently achieve optimal quality translation on free text, whatever translation method they employ. Our hypothesis is that the quality of MT will improve if an MT environment uses output from a variety of MT systems working on the same text. In the latest version of the Pan-gloss MT project, we collect the results of three translation engines -- typically, sub-sentential chunks -- in a chart data structure. Since the individual MT systems operate completely independently, their re- sults may be incomplete, conflicting, or redundant. We use simple scoring heuristics to estimate the quality of each chunk, and find the highest-score sequence of chunks (the "best cover"). This paper describes in detail the combining method, presenting the algorithm and illustrations of its progress on one of many actual translations it has produced. It uses dynamic programming to efficiently compare weighted averages of sets of adjacent scored component translations. The current system operates primarily in a human-aided MT mode. The translation delivery system and its associated post-editing aide are briefly described, as is an initial evaluation of the usefulness of this method. Individual MT engines will be reported separately and are not, therefore, described in detail here. We produce the first MEMT system by combining outputs from three different MT engines based on their knowledge of the inner workings of the engines. We develop a multi-engine MT system, which builds a chart using the translation units inside each input system and then uses a chart walk algorithm to find the best cover of the source sentence.
Max-Margin Parsing We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines. Our formulation uses a factorization analogous to the standard dynamic programs for parsing. In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates. Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness. We provide an efficient algorithm for learning such models and show experimental evidence of the model’s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar. we suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems.
A Fast And Portable Realizer For Text Generation Systems We release a surface realizer, RealPro, and it is  intended as off-the-shelf plug-in realizer. Our RealPro surface realizer which produces a surface linguistic utterance.
Online Learning of Relaxed CCG Grammars for Parsing to Logical Form We consider the problem of learning to parse sentences to lambda-calculus representations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG). A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar - for example allowing flexible word order, or insertion of lexical items - with learned costs. We also present a new, online algorithm for inducing a weighted CCG. Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match figure reported by He and Young (2006). We develop ATIS dataset for semantic parsing. We develop a set which includes features that are sensitive to lexical choices and the structure of the logical form that is constructed. We introduce the standard application, composition and coordination combinators, as well as type-shifting rules to model spontaneous, unedited text.
Get Out The Vote: Determining Support Or Opposition From Congressional Floor-Debate Transcripts We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation. To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another. We find that the incorporation of such information yields substantial improvements over classifying speeches in isolation. We present a method based on support vector machines to determine whether the speeches made by participants represent support or opposition to proposed legislation, using transcripts of U.S. congressional floor debates.
Forest Reranking: Discriminative Parsing with Non-Local Features Conventional n-best reranking techniques often suffer from the limited scope of the nbest list, which rules out many potentially good alternatives. We instead propose forest reranking, a method that reranks a packed forest of exponentially many parses. Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank. Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank. We show that the use of non-local features does in fact contribute substantially to parser performance. To prune the packed forests, we use inside and outside probabilities to compute the distance of the best derivation that traverses a hyper edge away from the globally best derivation.
Temporal Ontology And Temporal Reference A semantics of temporal categories in language and a theory of their use in defining the temporal relations between events both require a more complex structure on the domain underlying the meaning representations than is commonly assumed. This paper proposes an ontology based on such notions as causation and consequence, rather than on purely temporal primitives. A central notion in the ontology is that of an elementary event-complex called a "nucleus." A nucleus can be thought of as an association of a goal event, or "culmination," with a "preparatory process" by which it is accomplished, and a "consequent state," which ensues. Natural-language categories like aspects, futurates, adverbials, and when-clauses are argued to change the temporal/aspectual category of propositions under the control of such a nucleic knowledge representation structure. The same concept of a nucleus plays a central role in a theory of temporal reference, and of the semantics of tense, which we follow McCawley, Partee, and Isard in regarding as an anaphoric category. We claim that any manageable formalism for natural language temporal descriptions will have to embody such an ontology, as will any usable temporal database for knowledge about events which is to be interrogated using natural language. We describe temporal expressions relating to changes of state.
GermaNet - A Lexical-Semantic Net For German We present the lexical-semantic net for German "GermaNet" which integrates conceptual ontological information with lexical semantics, within and across word classes. It is compatible with the Princeton WordNet but integrates principle-based modifications on the constructional and organizational level as well as on the level of lexical and conceptual relations. GermaNet includes a new treatment of regular polysemy, artificial concepts and of particle verbs. It furthermore encodes cross-classification and basic syntactic information, constituting an interesting tool in exploring the interaction of syntax and semantics. The development of such a large scale resource is particularly important as German up to now lacks basic online tools for the semantic exploration of very large corpora. GermaNet is a large lexical database, where words are associated with POS in formation and semantic sorts, which are organized in a fine-grained hierarchy.
Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger. We describe how the voted perceptron can be used to train maximum-entropy style taggers and also give a discussion of the theory behind the perceptron algorithm applied to ranking tasks. Voted perceptron training attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the best-scoring labeling of that instance according to the current model.
Improved Word-Level System Combination for Machine Translation Recently, confusion network decoding has been applied in machine translation system combination. Due to errors in the hypothesis alignment, decoding may result in ungrammatical combination outputs. This paper describes an improved confusion network based method to combine outputs from multiple MT systems. In this approach, arbitrary features may be added log-linearly into the objective function, thus allowing language model expansion and re-scoring. Also, a novel method to automatically select the hypothesis which other hypotheses are aligned against is proposed. A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including TER, BLEU and METEOR. The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods. We use the tercom script (Snover et al, 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another. We propose a multiple confusion network (CN) or super-network framework, where we use each of all individual system results as the backbone to build CNs based on an alignment metric, TER. Each word in the confusion network is associated with a word posterior probability.
Inducing Translation Lexicons Via Diverse Similarity Measures And Bridge Languages This paper presents a method for inducing translation lexicons between two distant languages without the need for either parallel bilingual corpora or a direct bilingual seed dictionary. The algorithm successfully combines temporal occurrence similarity across dates in news corpora, wide and local cross-language context similarity, weighted Levenshtein distance, relative frequency and burstiness similarity measures. These similarity measures are integrated with the bridge language concept under a robust method of classifier combination for both the Slavic and Northern Indian language families. We induce translation lexicons for languages without common parallel corpora using a bridge language that is related to the target languages. We create bag-of-words context vectors around both the source and target language words and then project the source vectors into the target space via the current small translation dictionary.
Automatic Extraction Of Subcategorization From Corpora We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount. We use a grammar and a sophisticated parsing tool for argument-adjunct distinction.
Improving Word Representations via Global Context and Multiple Word Prototypes Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models. Our representation is designed to capture word sense disambiguation.
Why Generative Phrase Models Underperform Surface Heuristics We investigate why weights from generative models underperform heuristic estimates in phrase-based machine translation. We first propose a simple generative, phrase-based model and verify that its estimates are inferior to those given by surface statistics. The performance gap stems primarily from the addition of a hidden segmentation variable, which increases the capacity for overfitting during maximum likelihood training with EM. In particular, while word level models benefit greatly from re-estimation, phrase-level models do not: the crucial difference is that distinct word alignments cannot all be correct, while distinct segmentations can. Alternate segmentations rather than alternate alignments compete, resulting in increased determinization of the phrase table, decreased generalization, and decreased final BLEU score. We also show that interpolation of the two methods can result in a modest increase in BLEU score. We try a different generative phrase translation model analogous to IBM word-translation Model 3 (Brown et al., 1993), and again find that the standard model outperforms their generative model. We explore estimation using EM of phrase pair probabilities under a conditional translation model based on the original source-channel formulation. We conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator.
Assigning Function Tags To Parsed Text It is generally recognized that the common non-terminal labels for syntactic constituents (NP, VP, etc.) do not exhaust the syntactic and semantic information one would like about parts of a syntactic tree. For example, the Penn Treebank gives each constituent zero or more 'function tags' indicating semantic roles and other related information not easily encapsulated in the simple constituent labels. We present a statistical algorithm for assigning these function tags that, on text already parsed to a simple-label level, achieves an F-measure of 87%, which rises to 99% when considering 'no tag' as a valid choice. As an alternative to hard coded heuristics, we propose to recover the Penn functional tags automatically.
A Simple Approach To Building Ensembles Of Naive Bayesian Classifiers For Word Sense Disambiguation This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. Despite the simplicity of this approach, empirical results disambiguating the widely studied nouns line and interest show that such an ensemble achieves accuracy rivaling the best previously published results. We present an ensemble of eighty-one Naive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features.
An Efficient Augmented-Context-Free Parsing Algorithm An efficient parsing algorithm for augmented context-free grammars is introduced, and its application to on-line natural language interfaces discussed. The algorithm is a generalized LR parsing algorithm, which precomputes an LR shift-reduce parsing table (possibly with multiple entries) from a given augmented context-free grammar. Unlike the standard LR parsing algorithm, it can handle arbitrary context-free grammars, including ambiguous grammars, while most of the LR efficiency is preserved by introducing the concept of a "graph-structured stack". The graph-structured stack allows an LR shift-reduce parser to maintain multiple parses without parsing any part of the input twice in the same way. We can also view our parsing algorithm as an extended chart parsing algorithm efficiently guided by LR parsing tables. The algorithm is fast, due to the LR table precomputation. In several experiments with different English grammars and sentences, timings indicate a five- to tenfold speed advantage over Earley's context-free parsing algorithm. The algorithm parses a sentence strictly from left to right on-line, that is, it starts parsing as soon as the user types in the first word of a sentence, without waiting for completion of the sentence. A practical on-line parser based on the algorithm has been implemented in Common Lisp, and running on Symbolics and HP AI workstations. The parser is used in the multi-lingual machine translation project at CMU. Also, a commercial on-line parser for Japanese language is being built by Intelligent Technology Incorporation, based on the technique developed at CMU.
Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model’s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines. we introduce a semi-supervised approach that uses recursive autoencoders to learn the hierarchical structure and sentiment distribution of a sentence.
METEOR: An Automatic Metric For MT Evaluation With Improved Correlation With Human Judgments We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machine-produced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-by-segment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigram-precision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.
An Empirical Evaluation Of Knowledge Sources And Learning Algorithms For Word Sense Disambiguation In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data. Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations. The learning algorithms evaluated include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree algorithms. We present empirical results showing the relative contribution of the component knowledge sources and the different learning algorithms. In particular, using all of these knowledge sources and SVM (i.e., a single learning algorithm) achieves accuracy higher than the best official scores on both SENSEVAL-2 and SENSEVAL-1 test data. Our feature set consists of the following four types: local context n-grams of nearby words, global context from all the words in the given context, parts-of-speech n-grams of nearby words and syntactic information obtained from parser output.
TextRank: Bringing Order Into Texts In this paper, we introduce TextRank - a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications. In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks. we propose TextRank, which is one of the most well-known graph based approaches to key phrase extraction. we propose the TextRank model to rank key words based on the co-occurrence links between words.
The Alignment Template Approach To Statistical Machine Translation A phrase-based statistical machine translation approach — the alignment template approach — is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source–channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German–English speech Verbmobil task, we analyze the effect of various system components. On the French–English Canadian Hansards task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese–English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems. We describe a phrase-extract algorithm for extracting phrase pairs from a sentence pair annotated with a 1-best alignment.
Phrasal Cohesion And Statistical Machine Translation There has been much interest in using phrasal movement to improve statistical machine translation. We explore how well phrases cohere across two languages, specifically English and French, and examine the particular conditions under which they do not. We demonstrate that while there are cases where coherence is poor, there are many regularities which can be exploited by a statistical machine translation system. We also compare three variant syntactic representations to determine which one has the best properties with respect to cohesion. We measure phrasal cohesion in gold standard alignments by counting crossings. We compare tree-bank parser style analyses, a variant with flattened VPs and dependency structures.
Algorithms for Deterministic Incremental Dependency Parsing Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a nonprojective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework. We give a systematic description of the arc-standard and arc-eager algorithms, currently two popular transition-based parsing methods for word-level dependency parsing.
Unsupervised Part-Of-Speech Tagging Employing Efficient Graph Clustering An unsupervised part-of-speech (POS) tagging system that relies on graph clustering methods is described. Unlike in current state-of-the-art approaches, the kind and number of different tags is generated by the method itself. We compute and merge two partitionings of word graphs: one based on context similarity of high frequency words, another on log-likelihood statistics for words of lower frequencies. Using the resulting word clusters as a lexicon, a Viterbi POS tagger is trained, which is refined by a morphological component. The approach is evaluated on three different languages by measuring agreement with existing taggers. We directly compare the tagger output to supervised taggers for English, German and Finnish via information-theoretic measures. We conceptualize a network of words that capture the word co-occurrence patterns. We cluster the most frequent 10,000 words using contexts formed from the most frequent 150-200 words.
Weakly Supervised Named Entity Transliteration And Discovery From Multilingual Comparable Corpora Named Entity recognition (NER) is an important part of many natural language processing tasks. Current approaches often employ machine learning techniques and require supervised data. However, many languages lack such resources. This paper presents an (almost) unsupervised learning algorithm for automatic discovery of Named Entities (NEs) in a resource free language, given a bilingual corpora in which it is weakly temporally aligned with a resource rich language. NEs have similar time distributions across such corpora, and often some of the tokens in a multi-word NE are transliterated. We develop an algorithm that exploits both observations iteratively. The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration. Seeded with a small number of transliteration pairs, our algorithm discovers multi-word NEs, and takes advantage of a dictionary (if one exists) to account for translated or partially translated NEs. We evaluate the algorithm on an English-Russian corpus, and show high level of NEs discovery in Russian. We explore the use of a perceptron-based ranking model for the purpose of finding name transliterations across comparable corpora. Our feature space consists of n-gram pairs from the two languages. We find that the projection of NER tags is easier in comparison to projecting other types of annotations such as POS-tags and BPC. We introduce a Russian data set comprised of temporally aligned news articles.
Corpus-Based Induction Of Syntactic Structure: Models Of Dependency And Constituency We present a generative model for the unsupervised learning of dependency structures. We also describe the multiplicative combination of this dependency model with a model of linear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data. Our contributions include the generative Dependency Model with Valence (DMV). We argue that consistent syntactic representations are desirable in the evaluation of unsupervised syntactic parsers.
SemEval-2010 Task 14: Word Sense Induction &#x26;Disambiguation This paper presents the description and evaluation framework of SemEval-2010 Word Sense Induction & Disambiguation task, as well as the evaluation results of 26 participating systems. In this task, participants were required to induce the senses of 100 target words using a training set, and then disambiguate unseen instances of the same words using the induced senses. System answers were evaluated in: (1) an unsupervised manner by using two clustering evaluation measures, and (2) a supervised manner in a WSD task. In constructing the dataset we use WordNet to first randomly select one sense of the word and then construct a set of words in relation to the first word's chosen synset.
A Unigram Orientation Model For Statistical Machine Translation In this paper, we present a unigram segmentation model for statistical machine translation where the segmentation units are blocks: pairs of phrases without internal structure. The segmentation model uses a novel orientation component to handle swapping of neighbor blocks. During training, we collect block unigram counts with orientation: we count how often a block occurs to the left or to the right of some predecessor block. The orientation model is shown to improve translation performance over two models: 1) no block re-ordering is used, and 2) the block swapping is controlled only by a language model. We show experimental results on a standard Arabic-English translation task. This work introduces lexical features for distortion modeling.
WordNet 2 - A Morphologically And Semantically Enhanced Resource This paper presents an on-going project intended to enhance WordNet morphologically and semantically. The motivation for this work steams from the current limitations of WordNet when used as a linguistic knowledge base. We envision a software tool that automatically parses the conceptual defining glosses, attributing part-of-speech tags and phrasal brackets. The nouns, verbs, adjectives and adverbs from every definition are then disambiguated and linked to the corresponding synsets. This increases the connectivity between synsets allowing the retrieval of topically related concepts. Furthermore, the tool transforms the glosses, first into logical forms and then into semantic forms. Using derivational morphology new links are added between the synsets. We propose a scheme for attaching sense tags to predicates within the framework of transforming WordNet glosses into a logical form. The eXtended WordNet is a publicly available version of WordNet in which (among other things) each term occurring in a WordNet gloss (except those in example phrases) is lemmatized and mapped to the synset in which it belongs.
A Composite Kernel To Extract Relations Between Entities With Both Flat And Structured Features This paper proposes a novel composite kernel for relation extraction. The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples. The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction. Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features. Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction. We use a convolution tree kernel (CTK, Collins and Duffy, 2001) to investigate various structured information for relation extraction and find that the Shortest Path enclosed Tree (SPT) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus.
met*: A Method For Discriminating Metonymy And Metaphor By Computer The met* method distinguishes selected examples of metonymy from metaphor and from literalness and anomaly in short English sentences. In the met* method, literalness is distinguished because it satisfies contextual constraints that the nonliteral others all violate. Metonymy is discriminated from metaphor and anomaly in a way that [1] supports Lakoff and Johnson's (1980) view that in metonymy one entity stands for another whereas in metaphor one entity is viewed as another, [2] permits chains of metonymies (Reddy 1979), and [3] allows metonymies to co-occur with instances of either literalness, metaphor, or anomaly. Metaphor is distinguished from anomaly because the former contains a relevant analogy, unlike the latter. The met* method is part of Collative Semantics, a semantics for natural language processing, and has been implemented in a computer program called meta5. Some examples of meta5's analysis of metaphor and metonymy are given. The met* method is compared with approaches from artificial intelligence, linguistics, philosophy, and psychology. We use selectional preference violation technique to detect metaphors. We developed a system called met*, capable of discriminating between literalness, metonymy, metaphor and anomaly. We build a system met*, which is designed to distinguish both metaphor and metonymy from literal text, providing special techniques for processing these instances of figurative language. We developed a system called met*, capable of discriminating between literalness, metonymy, metaphor and anomaly.
Extracting Parallel Sentences from Comparable Corpora using Document Level Alignment The quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training. In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). One resource not yet thoroughly explored is Wikipedia, an online encyclopedia containing linked articles in many languages. We advance the state of the art in parallel sentence extraction by modeling the document level alignment, motivated by the observation that parallel sentence pairs are often found in close proximity. We also include features which make use of the additional annotation given by Wikipedia, and features using an automatically induced lexicon model. Results for both accuracy in sentence extraction and downstream improvement in an SMT system are presented. We report significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model.
Bootstrapping POS-Taggers Using Unlabelled Data This paper investigates booststrapping part-of-speech taggers using co-training, in which two taggers are iteratively re-trained on each other's output. Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set. We investigate selecting examples by directly maximising tagger agreement on unlabelled data, a method which has been theoretically and empirically motivated in the co-training literature. Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably out-performs self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. We report positive results with little labeled training data but negative results when the amount of labeled training data increases. We define self-training as a procedure in which a tagger is retrained on its own labeled cache at each round.
Survey Article: Inter-Coder Agreement for Computational Linguistics This article is a survey of methods for measuring agreement among corpus annotators. It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff’s alpha as well as Scott’s pi and Cohen’s kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappa-like measures in computational linguistics, may be more appropriate for many corpus annotation tasks — but that their use makes the interpretation of the value of the coefficient even harder. A comprehensive overview of methods for measuring the inter-annotator agreement in various areas of computational linguistics was given in this work.
Recovering Implicit Information This paper describes the SDC PUNDIT, (Prolog UNDerstands Integrated Text), system for processing natural language messages. PUNDIT, written in Prolog, is a highly modular system consisting of distinct syntactic, semantic and pragmatics components. Each component draws on one or more sets of data, including a lexicon, a broad-coverage grammar of English, semantic verb decompositions, rules mapping between syntactic and semantic constituents, and a domain model. This paper discusses the communication between the syntactic, semantic and pragmatic modules that is necessary for making implicit linguistic information explicit. The key is letting syntax and semantics recognize missing linguistic entities as implicit entities, so that they can be labelled as such, and referenee resolution can be directed to find specific referents for the entities. In this way the task of making implicit linguistic information explicit becomes a subset of the tasks performed by reference resolution. The success of this approach is dependent on marking missing syntactic constituents as elided and missing semantic roles as ESSENTIAL so that reference resolution can know when to look for referents. We propose the first attempt for the automatic annotation of implicit semantic roles. We make one of the earliest attempts to automatically recover extra-sentential arguments.
Automatically Extracting And Representing Collocations For Language Generation Collocational knowledge is necessary for language generation. The problem is that collocations come in a large variety of forms. They can involve two, three or more words, these words can be of different syntactic categories and they can be involved in more or less rigid ways. This leads to two main difficulties: collocational knowledge has to be acquired and it must be represented flexibly so that it can be used for language generation. We address both problems in this paper, focusing on the acquisition problem. We describe a program, Xtract, that automatically acquires a range of collocations from large textual corpora and we describe how they can be represented in a flexible lexicon using a unification based formalism.
Contextual Word Similarity And Estimation From Sparse Data In recent years there is much interest in word cooccurrence relations, such as n-grams, verb-object combinations, or cooccurrence within a limited context. This paper discusses how to estimate the probability of cooccurrences that do not occur in the training data. We present a method that makes local analogies between each specific unobserved cooccurrence and other cooccurrences that contain similar words, as determined by an appropriate word similarity metric. Our evaluation suggests that this method performs better than existing smoothing methods, and may provide an alternative to class based models. We argue that using a relatively small number of classes to model the similarity between words may lead to substantial loss of information. Clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time.
Fast Unsupervised Incremental Parsing This paper describes an incremental parser and an unsupervised learning algorithm for inducing this parser from plain text. The parser uses a representation for syntactic structure similar to dependency links which is well-suited for incremental parsing. In contrast to previous unsupervised parsers, the parser does not use part-of-speech tags and both learning and parsing are local and fast, requiring no explicit clustering or global optimization. The parser is evaluated by converting its output into equivalent bracketing and improves on previously published results for unsupervised parsing from plain text. Our incremental parsing approach uses a novel representation called common cover links, which can be converted to constituent brackets. Though punctuation is usually entirely ignored in unsupervised parsing research, we use phrasal punctuation - punctuation symbols that often mark phrasal boundaries within a sentence.
Improving IBM Word Alignment Model 1 We investigate a number of simple methods for improving the word-alignment accuracy of IBM Model 1. We demonstrate reduction in alignment error rate of approximately 30% resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters. A limitation of IBM Model 1 is that each word in the target sentence can be generated by at most one word in the source sentence. We also suggested adding multiple empty words to the target sentence for IBM Model 1. Our method also alleviates another related limitation by enabling translation between contiguous words across the query and documents.
Joint Unsupervised Coreference Resolution with Markov Logic Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data. Some unsupervised approaches have been proposed (e.g., Haghighi and Klein (2007)), but they are less accurate. In this paper, we present the first unsupervised approach that is competitive with supervised ones. This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals. On MUC and ACE datasets, our model outperforms Haghigi and Klein’s one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models. We empirically report that global approaches achieve performance better than the ones based on incrementally processing a text. Our method is based on the entity-mention model. In the predicate nominative construction, the object of a copular verb (forms of the verb be) is constrained to corefer with its subject.
The ICSI Meeting Recorder Dialog Act (MRDA) Corpus We describe a new corpus of over 180,000 hand-annotated dialog act tags and accompanying adjacency pair annotations for roughly 72 hours of speech from 75 naturally-occurring meetings. We provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data.
A TAG-Based Noisy-Channel Model Of Speech Repairs This paper describes a noisy channel model of speech repairs, which can identify and correct repairs in speech transcripts. A syntactic parser is used as the source model, and a novel type of TAG-based transducer is the channel model. The use of TAG is motivated by the intuition that the reparandum is a "rough copy" of the repair. The model is trained and tested on the Switchboard disfluency-annotated corpus. Noisy channel models do well on the disfluency detection task. Although the standard noisy channel model performs well, a log linear re-ranker can be used to increase performance. Our TAG system achieves a high EDIT-F score, largely as a result of its explicit tracking of overlapping words between reparanda and alterations.
Text Segmentation Based On Similarity Between Words This paper proposes a new indicator of text structure, called the lexical cohesion profile (LCP), which locates segment boundaries in a text. A text segment is a coherent scene; the words in a segment are linked together via lexical cohesion relations. LCP records mutual similarity of words in a sequence of text. The similarity of words, which represents their cohesiveness, is computed using a semantic network. Comparison with the text segments marked by a number of subjects shows that LCP closely correlates with the human judgments. LCP may provide valuable information for resolving anaphora and ellipsis. We find that using a domain independent source of knowledge for text segmentation doesn't necessarily lead to better results than work that is based only on word distribution in texts.
Phrasetable Smoothing For Statistical Machine Translation We discuss different strategies for smoothing the phrasetable in Statistical MT, and give results over a range of translation settings. We show that any type of smoothing is a better idea than the relative-frequency estimates that are often used. The best smoothing techniques yield consistent gains of approximately 1% (absolute) according to the BLEU metric.
Empirical Studies On The Disambiguation Of Cue Phrases Cue phrases are linguistic expressions such as now and well that function as explicit indicators of the structure of a discourse. For example, now may signal the beginning of a subtopic or a return to a previous topic, while well may mark subsequent material as a response to prior material, or as an explanatory comment. However, while cue phrases may convey discourse structure, each also has one or more alternate uses. While incidentally may be used sententially as an adverbial, for example, the discourse use initiates a digression. Although distinguishing discourse and sentential uses of cue phrases is critical to the interpretation and generation of discourse, the question of how speakers and hearers accomplish this disambiguation is rarely addressed. This paper reports results of empirical studies on discourse and sentential uses of cue phrases, in which both text-based and prosodic features were examined for disambiguating power. Based on these studies, it is proposed that discourse versus sentential usage may be distinguished by intonational features, specifically, pitch accent and prosodic phrasing. A prosodic model that characterizes these distinctions is identified. This model is associated with features identifiable from text analysis, including orthography and part of speech, to permit the application of the results of the prosodic analysis to the generation of appropriate intonational features for discourse and sentential uses of cue phrases in synthetic speech. In the literature, there is still no consistent definition for discourse markers. We find that into national phrasing and pitch accent play a role in disambiguating cue phrases, and hence in helping determine discourse structure.
A Syntactic Analysis Method Of Long Japanese Sentences Based On The Detection Of Conjunctive Structures This paper presents a syntactic analysis method that first detects conjunctive structures in a sentence by checking parallelism of two series of words and then analyzes the dependency structure of the sentence with the help of the information about the conjunctive structures. Analysis of long sentences is one of the most difficult problems in natural language processing. The main reason for this difficulty is the structural ambiguity that is common for conjunctive structures that appear in long sentences. Human beings can recognize conjunctive structures because of a certain, but sometimes subtle, similarity that exists between conjuncts. Therefore, we have developed an algorithm for calculating a similarity measure between two arbitrary series of words from the left and the right of a conjunction and selecting the two most similar series of words that can reasonably be considered as composing a conjunctive structure. This is realized using a dynamic programming technique. A long sentence can be reduced into a shorter form by recognizing conjunctive structures. Consequently, the total dependency structure of a sentence can be obtained by relatively simple head-dependent rules. A serious problem concerning conjunctive structures, besides the ambiguity of their scopes, is the ellipsis of some of their components. Through our dependency analysis process, we can find the ellipses and recover the omitted components. We report the results of analyzing 150 Japanese sentences to illustrate the effectiveness of this method. we propose a method to detect conjunctive structures by calculating similarity scores between two sequences of bunsetsus. we propose a similarity-based method to resolve both of the two tasks for Japanese. we propose a Japanese parsing method that included coordinate structure detection.
Dependency-based Syntactic&#x2013;Semantic Analysis with PropBank and NomBank This paper presents our contribution in the closed track of the 2008 CoNLL Shared Task (Surdeanu et al., 2008). To tackle the problem of joint syntactic-semantic analysis, the system relies on a syntactic and a semantic subcomponent. The syntactic model is a bottom-up projective parser using pseudo-projective transformations, and the semantic model uses global inference mechanisms on top of a pipeline of classifiers. The complete syntactic-semantic output is selected from a candidate pool generated by the subsystems. The system achieved the top score in the closed challenge: a labeled syntactic accuracy of 89.32%, a labeled semantic F1 of 81.65, and a labeled macro F1 of 85.49. Our system use two 30 different subsystems to handle verbal and nominal predicates, respectively. We present importance of capturing non-local dependencies of core arguments in predicate-argument structure analysis. In our work, the impact of different grammatical representations on the task of frame-based shallow semantic parsing is studied and the poor lexical generalization problem is outlined.
Sentence Reduction For Automatic Text Summarization We present a novel sentence reduction system for automatically removing extraneous phrases from sentences that are extracted from a document for summarization purpose. The system uses multiple sources of knowledge to decide which phrases in an extracted sentence can be removed, including syntactic knowledge, context information, and statistics computed from a corpus which consists of examples written by human professionals. Reduction can significantly improve the conciseness of automatic summaries. We study a new method to remove extraneous phrase from sentences by using multiple source of knowledge to decide which phrase in the sentences can be removed. In our approach, decisions about which material to include/delete in the sentence summaries do not rely on relative frequency information on words, but rather on probability models of subtree deletions that are learned from a corpus of parses for sentences and their summaries.
Reliable Measures For Aligning Japanese-English News Articles And Sentences We have aligned Japanese and English news articles and sentences to make a large parallel corpus. We first used a method based on cross-language information retrieval (CLIR) to align the Japanese and English articles and then used a method based on dynamic programming (DP) matching to align the Japanese and English sentences in these articles. However, the results included many incorrect alignments. To remove these, we propose two measures (scores) that evaluate the validity of alignments. The measure for article alignment uses similarities in sentences aligned by DP matching and that for sentence alignment uses similarities in articles aligned by CLIR. They enhance each other to improve the accuracy of alignment. Using these measures, we have successfully constructed a large-scale article and sentence alignment corpus available to the public. We build an automatically sentence aligned Japanese/English Yomiuri newspaper corpus consisting of 180K sentence pairs. We use the BM25 similarity measure.
Applying Co-Training Methods To Statistical Parsing We propose a novel Co-Training method for statistical parsing. The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text. The algorithm iteratively labels the entire data set with parse trees. Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data. Our co-training a mostly unsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other.
Limitations Of Co-Training For Natural Language Learning From Large Datasets Co-Training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data. This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data. This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels. Using base noun phrase bracketing as a case study, we find that co-training reduces by 36% the difference in error between classifiers and supervised clastrained on a labeled version all available data. However, degradation in the quality of the bootstrapped data arises as an obstacle to further improvement. To address this, we propose a moderately supervised variant of cotraining in which a human corrects the mistakes made during automatic labeling. Our analysis suggests that corrected co-training and similar moderately supervised methods may help cotraining scale to large natural language learning tasks. We show that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a high performing model from being learned.
The English All-Words Task We describe our experience in preparing the sense-tagged corpus used in the English all-words task and we tabulate the scores.
Japanese Dependency Structure Analysis Based On Support Vector Machines This paper presents a method of Japanese dependency structure analysis based on Support Vector Machines (SVMs). Conventional parsing techniques based on Machine Learning framework, such as Decision Trees and Maximum Entropy Models, have difficulty in selecting useful features as well as finding appropriate combination of selected features. On the other hand, it is well-known that SVMs achieve high generalization performance even with input data of very high dimensional feature space. Furthermore, by introducing the Kernel principle, SVMs can carry out the training in high-dimensional spaces with a smaller computational cost independent of their dimensionality. We apply SVMs to Japanese dependency structure identification problem. Experimental results on Kyoto University corpus show that our system achieves the accuracy of 89.09% even with small training data (7958 sentences). We introduce a new type of feature called dynamic features which are created dynamically during the parsing process.
Chinese Syntactic Reordering for Statistical Machine Translation Syntactic reordering approaches are an effective method for handling word-order differences between source and target languages in statistical machine translation (SMT) systems. This paper introduces a reordering approach for translation from Chinese to English. We describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order. The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English in terms of their word order. We evaluated the reordering approach within the MOSES phrase-based SMT system (Koehn et al., 2007). The reordering approach improved the BLEU score for the MOSES system from 28.52 to 30.86 on the NIST 2006 evaluation data. We also conducted a series of experiments to analyze the accuracy and impact of different types of reordering rules. Our rule set substantially decreases the total times of rule application about 60%, compared with a constituent-based approach. Chinese ordering differs from English mainly in clause ordering.
Bilingual Parsing With Factored Estimation: Using English To Parse Korean We describe how simple, commonly understood statistical models, such as statistical dependency parsers, probabilistic context-free grammars, and word-to-word translation models, can be effectively combined into a unified bilingual parser that jointly searches for the best English parse, Korean parse, and word alignment, where these hidden structures all constrain each other. The model used for parsing is completely factored into the two parsers and the TM, allowing separate parameter estimation. We evaluate our bilingual parser on the Penn Korean Treebank and against several baseline systems and show improvements parsing Korean with very limited labeled data. we proposed to merge an English parser, a word alignment model, and a Korean PCFG parser trained from a small number of Korean parse trees under a unified log linear model.
Enforcing Transitivity in Coreference Resolution A desirable quality of a coreference resolution system is the ability to handle transitivity constraints, such that even if it places high likelihood on a particular mention being coreferent with each of two other mentions, it will also consider the likelihood of those two mentions being coreferent when making a final assignment. This is exactly the kind of constraint that integer linear programming (ILP) is ideal for, but, surprisingly, previous work applying ILP to coreference resolution has not encoded this type of constraint. We train a coreference classifier over pairs of mentions, and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments. We present results on two commonly used datasets which show that enforcement of transitive closure consistently improves performance, including improvements of up to 3.6% using the b3 scorer, and up to 16.5% using cluster f-measure. We present a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier.
Knowledge-Free Induction Of Inflectional Morphologies We propose an algorithm to automatically induce the morphology of inflectional languages using only text corpora and no human input. Our algorithm combines cues from orthography, semantics, and syntactic distributions to induce morphological relationships in German, Dutch, and English. Using CELEX as a gold standard for evaluation, we show our algorithm to be an improvement over any knowledge-free algorithm yet proposed. We use latent semantic analysis to find prefixes, suffixes and circumfixes in German, Dutch and English.
Minimum Error Rate Training In Statistical Machine Translation Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria. A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text. In this paper, we analyze various training criteria which directly optimize translation quality. These training criteria make use of recently proposed automatic evaluation metrics. We describe a new algorithm for efficient training an unsmoothed error count. We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure. In our model, feature weights are tuned with Minimum Error Rate Training (MERT) to maximize BLEU.
A Tale of Two Parsers: Investigating and Combining Graph-based and Transition-based Dependency Parsing Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations. We study both approaches under the framework of beam-search. By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods. More importantly, we propose a beam-search-based parser that combines both graph-based and transition-based parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers. Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuracies of 92.1% and 86.2%, respectively. We define head rules to convert phrase structures into dependency structures. We combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explore the addition of graph based features to a transition-based parser.
Findings of the 2011 Workshop on Statistical Machine Translation This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics. We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics. This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake. We also conducted a pilot 'tunable metrics' task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality. Despite avoiding language-specific resources and using only the training data provided by the workshop, an extensive manual evaluation determined that the outputs produced were of significantly higher quality than both statistical and rule-based systems that made use of language-specific resources.
Two Languages are Better than One (for Syntactic Parsing) We show that jointly parsing a bitext can substantially improve parse quality on both sides. In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them. Features include monolingual parse scores and various measures of syntactic divergence. Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables. The resulting bitext parser outperforms state-of-the-art monolingual parser baselines by 2.5 F at predicting English side trees and 1.8 F at predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation. In bitext parsing, we use feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy. We use word alignment density features which measure how well the aligned entity pair matches up with alignments from an independent word aligner.
Automatic Acquisition Of Hyponyms From Large Text Corpora We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to attgment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested. We find individual pairs of hypernyms and hyponyms from text using pattern-matching techniques.
Surface Grammatical Analysis For The Extraction Of Terminological Noun Phrases LEXTER is a software package for extracting terminology. A corpus of French language texts on any subject field is fed in, and LEXTER produces a list of likely terminological units to be submitted to an expert to be validated. To identify the terminological units, LEXTER takes their form into account and proceeds in two main stages : analysis, parsing. In the first stage, LEXTER uses a base of rules designed to identify frontier markers in view to analysing the texts and extracting maximal-length noun phrases. In the second stage, LEXTER parses these maximal-length noun phrases to extract subgroups which by virtue of their grammatical structure and their place in the maximal-length noun phrases are likely to be terminological units. In this article, the type of analysis used (surface grammatical analysis) is highlighted, as the methodological approach adopted to adapt the rules (experimental approach). We present a surface-syntactic analyser that extracts maximal length noun phrases mainly sequences of determiners, premodifiers, nominal heads, and certain kinds of post modifying prepositional phrases and adjectives from French texts for terminology applications. Our method relies purely on linguistic information, namely morpho-syntactic features of term candidates.
Text Chunking Using Transformation-Based Learning Eric Brill introduced transformation-based learning and showed that it can do part-of-speech tagging with fairly high accuracy. The same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive "baseNP" chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 92% for baseNP chunks and 88% for somewhat more complex chunks that partition the sentence. Some interesting adaptations to the transformation-based learning approach are also suggested by this application. We formalize chunking as a classification task, in which each word is classified as the (B)eginning, (I)nside or (O) outside of a chunk. We pioneer the machine learning techniques to chunking problem.
A Conditional Random Field Word Segmenter for Sighan Bakeoff 2005 We present a Chinese word segmentation system submitted to the closed track of Sighan bakeoff 2005. Our segmenter was built using a conditional random field sequence model that provides a framework to use a large number of linguistic features such as character identity, morphological and character reduplication features. Because our morphological features were extracted from the training corpora automatically, our system was not biased toward any particular variety of Mandarin. Thus, our system does not overfit the variety of Mandarin most familiar to the system's designers. Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR). We develop the Stanford Chinese word segmenter.
Chinese Part-Of-Speech Tagging: One-At-A-Time Or All-At-Once? Word-Based Or Character-Based? Chinese part-of-speech (POS) tagging assigns one POS tag to each word in a Chinese sentence. However, since words are not demarcated in a Chinese sentence, Chinese POS tagging requires word segmentation as a prerequisite. We could perform Chinese POS tagging strictly after word segmentation (one-at-a-time approach), or perform both word segmentation and POS tagging in a combined, single step simultaneously (all-at- once approach). Also, we could choose to assign POS tags on a word-by-word basis, making use of word features in the surrounding context (word-based), or on a character-by-character basis with character features (character-based). This paper presents an in-depth study on such issues of processing architecture and feature representation for Chinese POS tagging, within a maximum entropy framework. We found that while the all-at-once, character-based approach is the best, the one-at-a-time, character-based approach is a worthwhile compromise, performing only slightly worse in terms of accuracy, but taking shorter time to train and run. As part of our investigation, we also built a state-of-the-art Chinese word segmenter, which outperforms the best SIGHAN 2003 word segmenters in the closed track on 3 out of 4 test corpora.
A Comparative Study On Reordering Constraints In Statistical Machine Translation In statistical machine translation, the generation of a translation hypothesis is computationally expensive. If arbitrary word-reorderings are permitted, the search problem is NP-hard. On the other hand, if we restrict the possible word-reorderings in an appropriate way, we obtain a polynomial-time search algorithm. In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints. This comparison includes a theoretical discussion on the permitted number of reorderings for each of these constraints. We show a connection between the ITG constraints and the since 1870 known Schroder numbers. We evaluate these constraints on two tasks: the Verbmobil task and the Canadian Hansards task. The evaluation consists of two parts: First, we check how many of the Viterbi alignments of the training corpus satisfy each of these constraints. Second, we restrict the search to each of these constraints and compare the resulting translation hypotheses. The experiments will show that the baseline ITG constraints are not sufficient on the Canadian Hansards task. Therefore, we present an extension to the ITG constraints. These extended ITG constraints increase the alignment coverage from about 87% to 96%. We show that ITG constraints yield significantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). We introduce a normal form ITG which avoids over-counting.
The Tradeoffs Between Open and Traditional Relation Extraction Traditional Information Extraction (IE) takes a relation name and hand-tagged examples of that relation as input. Open IE is a relation-independent extraction paradigm that is tailored to massive and heterogeneous corpora such as the Web. An Open IE system extracts a diverse set of relational tuples from text without any relation-specific input. How is Open IE possible? We analyze a sample of English sentences to demonstrate that numerous relationships are expressed using a compact set of relation-independent lexico-syntactic patterns, which can be learned by an Open IE system. What are the tradeoffs between Open IE and traditional IE? We consider this question in the context of two tasks. First, when the number of relations is massive, and the relations themselves are not pre-specified, we argue that Open IE is necessary. We then present a new model for Open IE called O-CRF and show that it achieves increased precision and nearly double the recall than the model employed by TEXTRUNNER, the previous state-of-the-art Open IE system. Second, when the number of target relations is small, and their names are known in advance, we show that O-CRF is able to match the precision of a traditional extraction system, though at substantially lower recall. Finally, we show how to combine the two types of systems into a hybrid that achieves higher precision than a traditional extractor, with comparable recall. We use a Conditional Random Field (CRF) classifier to perform Open Relation Extraction, improving by more than 60% the F-score achieved by the Naive Bayes model in the TextRunner system. Our system is trained using a CRF classifier on S-V-O tuples from a parsed corpus as positive examples, and tuples that violate phrasal structure as negative ones.
Bayesian Unsupervised Topic Segmentation This paper describes a novel Bayesian approach to unsupervised topic segmentation. Unsupervised systems for this task are driven by lexical cohesion: the tendency of well-formed segments to induce a compact and consistent lexical distribution. We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation. This contrasts with previous approaches, which relied on hand-crafted cohesion metrics. The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems. Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets. We also show that both an entropy-based analysis and a well-known previous technique can be derived as special cases of the Bayesian framework. We present a dynamic program for linear segmentation. If the actual number of segments is known and only a linear discourse structure is acceptable, then a single move, shift of the segment border, is sufficient. We find the richer model beneficial for a meetings corpus but not for a textbook.
Lexical Cohesion Computed By Thesaural Relations As An Indicator Of The Structure Of Text In text, lexical cohesion is the result of chains of related words that contribute to the continuity of lexical meaning. These lexical chains are a direct result of units of text being 'about the same thing,' and finding text structure involves finding units of text that are about the same thing. Hence, computing the chains is useful, since they will have a correspondence to the structure of the text. Determining the structure of text is an essential step in determining the deep meaning of the text. In this paper, a thesaurus is used as the major knowledge base for computing lexical chains. Correspondences between lexical chains and structural elements are shown to exist. Since the lexical chains are computable, and exist in non-domain-specific text, they provide a valuable indicator of text structure. The lexical chains also provide a semantic context for interpreting words, concepts, and sentences. We propose the idea of using lexical chains as indicators of lexical cohesion. We propose the concept of Lexical Chains to explore the discourse structure of a text.
Inversion Transduction Grammar for Joint Phrasal Translation Modeling We present a phrasal inversion transduction grammar as an alternative to joint phrasal translation models. This syntactic model is similar to its flat-string phrasal predecessors, but admits polynomial-time algorithms for Viterbi alignment and EM training. We demonstrate that the consistency constraints that allow flat phrasal models to scale also help ITG algorithms, producing an 80-times faster inside-outside algorithm. We also show that the phrasal translation tables produced by the ITG are superior to those of the flat joint phrasal model, producing up to a 2.5 point improvement in BLEU score. Finally, we explore, for the first time, the utility of a joint phrasal translation model as a word alignment method. We use synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences.
Towards The Automatic Identification Of Adjectival Scales: Clustering Adjectives According To Meaning In this paper we present a method to group adjectives according to their meaning, as a first step towards the automatic identification of adjectival scales. We discuss the properties of adjectival scales and of groups of semantically related adjectives and how they imply sources of linguistic knowledge in text corpora. We describe how our system exploits this linguistic knowledge to compute a measure of similarity between two adjectives, using statistical techniques and without having access to any semantic information about the adjectives. We also show how a clustering algorithm can use these similarities to produce the groups of adjectives, and we present results produced by our system for a sample set of adjectives. We conclude by presenting evaluation methods for the task at hand, and analyzing the significance of the results obtained. We learn attributes by clustering adjectives that denote values of the same attribute.
Automatic Labeling Of Semantic Roles We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data. We split the task into two sequential subtasks: first, argument recognition decides for each instance whether it bears a semantic role or not; then, argument labelling assigns a label to instances recognised as role-bearers. We present a system that uses completely syntactic features to classify the Frame Elements in a sentence in the FrameNet database.
Efficient Support Vector Classifiers For Named Entity Recognition Named Entity (NE) recognition is a task in which proper nouns and numerical information are extracted from documents and are classified into categories such as person, organization, and date. It is a key technology of Information Extraction and Open-Domain Question Answering. First, we show that an NE recognizer based on Support Vector Machines (SVMs) gives better scores than conventional systems. However, off-the-shelf SVM classifiers are too inefficient for this task. Therefore, we present a method that makes the system substantially faster. This approach can also be applied to other similar tasks such as chunking and part-of-speech tagging. We also present an SVM-based feature selection method and an efficient training method. We propose Kernel Expansion that is used to transform the d-degree polynomial kernel based classifier into a linear one, with a modified decision function. We propose an XQK (eXpand the Quadratic Kernel) which can make their Named-Entity recognizer drastically fast.
Japanese Dependency Analysis Using Cascaded Chunking In this paper, we propose a new statistical Japanese dependency parser using a cascaded chunking model. Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model, which is not always efficient or scalable. We propose a new method that is simple and efficient, since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side. Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency. Our cascaded chunking model does not require the probabilities of dependencies and parses a sentence deterministically.
Noun Classification From Predicate-Argument Structures A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described. The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification. We make use of a mutual-information based metric to determine noun similarity. We use thenotion of distributional similarity, i.e., two words with similar meanings will be used in similar contexts.
Simple Semi-supervised Dependency Parsing We present a simple and effective semi-supervised method for training dependency parsers. We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus. We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions. For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuracy of 92.02% to 93.16%, and in the case of Czech unlabeled second-order parsing, we improve from a baseline accuracy of 86.13% to 87.13%. In addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance. We show that for syntactic dependency parsing, combining Brown cluster features with word forms or POS tags yields high accuracy even with little training data. We propose to use word clusters as features to improve graph-based statistical dependency parsing for English and Czech.
Distributional Clustering Of English Words We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical "soft" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data. We make use of deterministic annealing to cluster verb-argument pairs into classes of verbs and nouns.
SemEval-2010 Task 13: TempEval-2 Tempeval-2 comprises evaluation tasks for time expressions, events and temporal relations, the latter of which was split up in four subtasks, motivated by the notion that smaller subtasks would make both data preparation and temporal relation extraction easier. Manually annotated data were provided for six languages: Chinese, English, French, Italian, Korean and Spanish. One of the tasks of this workshop is to determine the temporal relation between an event and a time expression in the same sentence.
Word Sense And Subjectivity Subjectivity and meaning are both important properties of language. This paper explores their interaction, and brings empirical evidence in support of the hypotheses that (1) subjectivity is a property that can be associated with word senses, and (2) word sense disambiguation can directly benefit from subjectivity annotations. We study the distinction between objectivity and subjectivity in each different sense of a word, and their empirical effects in the context of sentiment analysis. We provide evidence that word sense labels, together with contextual subjectivity analysis, can be exploited to improve performance in word sense disambiguation. We show that even reliable subjectivity clues have objective senses. We show that subjectivity annotations can be helpful for word sense disambiguation when a word has distinct subjective senses and objective senses. We conduct a study on human annotation of 354 words senses with polarity and report a high inter-annotator agreement. We define subjective expressions as words and phrases being used to express mental and emotional states, such as speculations, evaluations, sentiments, and beliefs.
An Algorithm For Generating Quantifier Scopings The syntactic structure of a sentence often manifests quite clearly the predicate-argument structure and relations of grammatical subordination. But scope dependencies are not so transparent. As a result, many systems for representing the semantics of sentences have ignored scoping or generated scopings with mechanisms that have often been inexplicit as to the range of scopings they choose among or profligate in the scopings they allow. This paper presents, along with proofs of some of its important properties, an algorithm that generates scoped semantic forms from unscoped expressions encoding predicate-argument structure. The algorithm is not profligate as are those based on permutation of quantifiers, and it can provide a solid foundation for computational solutions where completeness is sacrificed for efficiency and heuristic efficacy. We extend this formalism to support operators (such as not) and present an enumeration algorithm that is more efficient than the naive wrapping approach. We presented an algorithm to generate quantifier scopings from a representation of predicate-argument relations and the relations of grammatical subordination. We introduce an algorithm for generating all possible quantifier scopings.
Sentiment Analysis Using Support Vector Machines With Diverse Information Sources This paper introduces an approach to sentiment analysis which uses support vector machines (SVMs) to bring together diverse sources of potentially pertinent information, including several favorability measures for phrases and adjectives and, where available, knowledge of the topic of the text. Models using the features introduced are further combined with unigram models which have been shown to be effective in the past (Pang et al., 2002) and lemmatized versions of the unigram models. Experiments on movie review data from the Internet Movie Database demonstrate that hybrid SVMs which combine unigram-style feature-based SVMs with those based on real-valued favorability measures obtain superior performance, producing the best results yet published using this data. Further experiments using a feature set enriched with topic information on a smaller dataset of music reviews hand-annotated for topic are also reported, the results of which suggest that incorporating topic information into such models may also yield improvement.
Machine Transliteration It is challenging to translate names and technical terms across languages with different alphabets and sound inventories. These items are commonly transliterated, i.e., replaced with approximate phonetic equivalents. For example, computer in English comes out as (konpyuutaa) in Japanese. Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries. We describe and evaluate a method for performing backwards transliterations by machine. This method uses a generative model, incorporating several distinct stages in the transliteration process. We propose to compose a set of weighted finite state transducers to solve the problem of back-transliteration from Japanese Katakana to English.
Distant supervision for relation extraction without labeled data Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACE-style algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large un-labeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression. Additionally, researchers have tried to automatically extract examples for supervised learning from resources such as Wikipedia (Weld et al,2008) and databases (Mintz et al, 2009), or attempted open information extraction (IE) (Banko et al, 2007) to extract all possible relations. Distant supervision (DS) can automatically gather labeled data by heuristically aligning entities in text with those in a knowledge base.
CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word–word dependencies. The resulting corpus, CCGbank, includes 99.4% of the sentences in the Penn Treebank. It is available from the Linguistic Data Consortium, and has been used to train wide coverage statistical parsers that obtain state-of-the-art rates of dependency recovery. In order to obtain linguistically adequate CCG analyses, and to eliminate noise and inconsistencies in the original annotation, an extensive analysis of the constructions and annotations in the Penn Treebank was called for, and a substantial number of changes to the Treebank were necessary. We discuss the implications of our findings for the extraction of other linguistically expressive grammars from the Treebank, and for the design of future treebanks. The CCGbank-style dependency is a directed graph of head-child relations labelled with the head's lexical category and the argument slot filled by the child. CCGbank is a corpus of CCG derivations that was semiautomatically converted from the Wall Street Journal section of the Penn treebank.
Introduction To The Special Issue On The Web As Corpus The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists’ playground. This special issue of Computational Linguistics explores ways in which this dream is being explored. It is natural to question the appropriateness of web data for research purposes, because web data is inevitably noisy and search engines themselves can introduce certain idiosyncracies which can distort results.
An Empirical Approach To Conceptual Case Frame Acquisition Conceptual natural language processing systems usually rely on case frame instantiation to recognize events and role objects in text. But generating a good set of case frames for a domain is time-consuming, tedious, and prone to errors of omission. We have developed a corpus-based algorithm for acquiring conceptual case frames empirically from unannotated text. Our algorithm builds on previous research on corpus-based methods for acquiring extraction patterns and semantic lexicons. Given extraction patterns and a semantic lexicon for a domain, our algorithm learns semantic preferences for each extraction pattern and merges the syntactically compatible patterns to produce multi-slot case frames with selectional restrictions. The case frames generate more cohesive output and produce fewer false hits than the original extraction patterns. Our system requires only preclassified training texts and a few hours of manual review to filter the dictionaries, demonstrating that conceptual case frames can be acquired from unannotated text without special training resources. our Conceptual Case Frame Acquisition project, extraction patterns, a domain semantic lexicon, and a list of conceptual roles and associated semantic categories for the domain are used to produce multiple-slot case frames with selectional restrictions.
Collective Information Extraction With Relational Markov Networks Most information extraction (IE) systems treat separate potential extractions as independent. However, in many cases, considering influences between different potential extractions could improve overall accuracy. Statistical methods based on undirected graphical models, such as conditional random fields (CRFs), have been shown to be an effective approach to learning accurate IE systems. We present a new IE method that employs Relational Markov Networks (a generalization of CRFs), which can represent arbitrary dependencies between extractions. This allows for "collective information extraction" that exploits the mutual influence between possible extractions. Experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach. We present AImed, a corpus for the evaluation of PPI extraction systems.
Recognizing Text Genres With Simple Metrics Using Discriminant Analysis A simple method for categorizing texts into pre-determined text genre categories using the statistical standard technique of discriminant analysis is demonstrated with application to the Brown corpus. Discriminant analysis makes it possible to use a large number of parameters taht may be specific for a certain corpus or information streatm, and combine them into a small number of function, with the parameters weighted on bais of how useful they are for discriminating text genres. An application to information retrieval is discussed. We word length as an indicator of formality for applications such as genre classification.
Structural Disambiguation With Constraint Propagation We present a new grammatical formalism called Constraint Dependency Grammar (CDG) in which every grammatical rule is given as a constraint on word-to-word modifications. CDG parsing is formalized as a constraint satisfaction problem over a finite domain so that efficient constraint-propagation algorithms can be employed to reduce structural ambiguity without generating individual parse trees. The weak generative capacity and the computational complexity of CDG parsing are also discussed. Our constraint dependency grammar maps to the notation of constratin satisfaction techniques.
A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model In this paper, we propose a novel string-to-dependency algorithm for statistical machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set. We presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically.
Early Results For Named Entity Recognition With Conditional Random Fields Feature Induction And Web-Enhanced Lexicons
Intricacies Of Collins Parsing Model This article documents a large set of heretofore unpublished details Collins used in his parser, such that, along with Collins’ (1999) thesis, this article contains all information necessary to duplicate Collins’ benchmark results. Indeed, these as-yet-unpublished details account for an 11% relative increase in error from an implementation including all details to a clean-room implementation of Collins’ model. We also show a cleaner and equally well-performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about Collins’ parser. We not only analyze the effect of the unpublished details, but also reanalyze the effect of certain well-known details, revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought. Finally, we perform experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech. The results suggest that the power of Collins-style parsing models did not lie primarily with the use of bilexical dependencies as was once thought, but in lexico-structural dependencies, that is, predicting syntactic structures conditioning on head words. We show that bilexical-information is used in only 1.49% of the decisions in Collins' Model-2 parser, and that removing this information results in an exceedingly small drop in performance.
A Comparison Of Algorithms For Maximum Entropy Parameter Estimation Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. However, the flexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Surprisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limited-memory variable metric algorithm outperformed the other choices. We introduce the open-source Toolkit for Advanced Discriminative Model which uses a limited-memory variable metric.
A Novel Use Of Statistical Parsing To Extract Information From Text Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations. Our rule-based methods employ a number of linguistic rules to capture relation patterns. We take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations. We integrate various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. We combine entity recognition, parsing, and relation extraction into a jointly-trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly-labeled data is unavailable.
Counter-Training In Discovery Of Semantic Patterns This paper presents a method for unsupervised discovery of semantic patterns. Semantic patterns are useful for a variety of text understanding tasks, in particular for locating events in text for information extraction. The method builds upon previously described approaches to iterative unsupervised pattern acquisition. One common characteristic of prior approaches is that the output of the algorithm is a continuous stream of patterns, with gradually degrading precision. Our method differs from the previous pattern acquisition algorithms in that it introduces competition among several scenarios simultaneously. This provides natural stopping criteria for the unsupervised learners, while maintaining good precision levels at termination. We discuss the results of experiments with several scenarios, and examine different aspects of the new procedure. We develop Counter-Trainin for detecting negative rules for a specific domain or a specific class by learning from multiple domains or classes at the same time. We use predicate-argument (SVO) model, which allows subtrees containing only a verb and its direct subject and object as extraction pattern candidates.
Learning Non-Isomorphic Tree Mappings For Machine Translation Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings. Unlike previous statistical formalisms (limited to isomorphic trees), synchronous TSG allows local distortion of the tree topology. We reformulate it to permit dependency trees, and sketch EM/Viterbi algorithms for alignment, training, and decoding. We argue that if the parse tree of source sentence is provided, decoding (for tree-to-string and tree-to-tree models) can also be cast as a tree-parsing problem. We consider synchronous tree substitution grammar, a formalism that can account for structural mismatches, and is trained discriminatively.
Bootstrapping Statistical Parsers From Small Datasets We present a practical co-training method for bootstrapping statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences. Experimental results show that unlabelled sentences can be used to improve the performance of statistical parsers. In addition, we consider the problem of bootstrapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material. We show that bootstrapping continues to be useful, even though no manually produced parses from the target domain are used. We examine self-training for PCFG parsing in the small seed case (< 1k labeled data). We report either minor improvements or significant damage from using self-training for parsing. We find degradation using a lexicalized tree adjoining grammar parser and minor improvement using Collins lexicalized PCFG parser; however, this gain was obtained only when the parser was trained on a small labeled set.
Cooking Up Referring Expressions This paper describes the referring expression generation mechanisms used in EPICURE, a computer program which produces natural language descriptions of cookery recipes. Major features of the system include: an underlying ontology which permits the representation of non-singular entities; a notion of discriminatory power, to determine what properties should be used in a description; and a PATR-like unification grammar to produce surface linguistic strings. We produce a description entailing the minimal number of attributes possible at the price of suffering NP-hard complexity. Our algorithm attempts to build a minimal distinguishing description by always selecting the most discriminatory property available. We define minimality as the proportion of descriptions produced by a system that are maximally brief. We propose a solution to the problem of generating definite descriptions that evoke a discourse entity already introduced in the context.
Syntactic Constraints on Paraphrases Extracted from Parallel Corpora We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type. This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method. We show how the monolingual context of a sentence to paraphrase can be used to improve the quality of the acquired paraphrases. Human evaluators are asked to score each pair of an original sentence and a paraphrased sentence with the following two 5-point scale grades: Grammaticality: whether the paraphrased sentence is grammatical, Meaning: whether the meaning of the original sentence is properly retained by the paraphrased sentence. A problem of phrase-based methods to paraphrase or term variation acquisition is the fact that a large proportion of the term variations or paraphrases proposed by the system are superior sub-strings of the original term. We automatically acquire paraphrase dictionary.
Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web’s natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multiinstance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint — for example they cannot extract the pair Founded(Jobs, Apple) and CEO-of(Jobs, Apple). This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Freebase. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level. We apply the greedy inference algorithm. We use  multiple deterministic-OR constraint to train a sentential relation extractor.
GEMINI: A Natural Language System For Spoken-Language Understanding We report a syntactic and semantic coverage of 86% for the DARPA Airline reservation corpus. Gemini is an expressive formalism in which to write formal grammars. We present Gemini natural language parser/generator, which attempts to parse the speech recognition output.
Syntax Augmented Machine Translation Via Chart Parsing We present translation results on the shared task ”Exploiting Parallel Texts for Statistical Machine Translation” generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshop’s baseline system. Our translation system is available open-source under the GNU General Public License. In our work, syntax is successfully integrated into hierarchical SMT. We start with a complete set of phrases as extracted by traditional PBMT heuristics, and then annotated the target side of each phrasal entry with the label of the constituent node in the target-side parse tree that subsumes the span. We use broken syntactic fragments to augment their grammars to increase the rule coverage; while we learn optimal tree fragments transformed from the original ones via a generative framework, they enumerate the fragments available from the original trees without learning process.
Efficient Deep Processing Of Japanese We present a broad coverage Japanese grammar written in the HPSG formalism with MRS semantics. The grammar is created for use in real world applications, such that robustness and performance issues play an important role. It is connected to a POS tagging and word segmentation tool. This grammar is being developed in a multilingual context, requiring MRS structures that are easily comparable across languages. Our hand-crafted Japanese HPSG grammar, JACY, provides semantic information as well as linguistically motivated analysis of complex constructions.
Using Multiple Knowledge Sources For Word Sense Discrimination This paper addresses the problem of how to identify the intended meaning of individual words in unrestricted texts, without necessarily having access to complete representations of sentences. To discriminate senses, an understander can consider a diversity of information, including syntactic tags, word frequencies, collocations, semantic context, role-related expectations, and syntactic restrictions. However, current approaches make use of only small subsets of this information. Here we will describe how to use the whole range of information. Our discussion will include how the preference cues relate to general lexical and conceptual knowledge and to more specialized knowledge of collocations and contexts. We will describe a method of combining cues on the basis of their individual specificity, rather than a fixed ranking among cue-types. We will also discuss an application of the approach in a system that computes sense tags for arbitrary texts, even when it is unable to determine a single syntactic or semantic representation for some sentences. We are one of the first to use multiple kinds of features for word sense disambiguation in the semantic interpretation system, TRUMP. We describe a study of different sources useful for word sense disambiguation, including morphological information.
Learning Bilingual Lexicons from Monolingual Corpora We present a method for learning bilingual translation lexicons from monolingual corpora. Word types in each language are characterized by purely monolingual features, such as context counts and orthographic substrings. Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings. We show that high-precision lexicons can be learned in a variety of language pairs and from a range of corpus types. We only use a small-sized bilingual lexicon containing 100 word pairs as seed lexicon. The availability of parsers is a more stringent constraint, but our results suggest that more basic NLP methods may be sufficient for bilingual lexicon extraction. In this work, we have used a set of seed translations (unlike e.g., Haghighi et al (2008)). We present a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account.
PARADISE: A Framework For Evaluating Spoken Dialogue Agents This paper presents PARADISE (PARAdigm for Dialogue System Evaluation), a general framework for evaluating spoken dialogue agents. The framework decouples task requirements from an agent's dialogue behaviors, supports comparisons among dialogue strategies, enables the calculation of performance over subdialogues and whole dialogues, specifies the relative contribution of various factors to performance, and makes it possible to compare agents performing different tasks by normalizing for task complexity. We identify three factors which carry an influence on the performance of SDSs: agent factors (mainly related to the dialogue and the system itself), task factors (related to how the SDS captures the task it has been developed for) and environmental factor (e.g. factors related to the acoustic environment and the transmission channel. We aim to evaluate diaglogue agent strategies by relating overall user satisfaction to other metrics such as task success, efficiency measure and qualitative measures.
A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence. These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance. Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity. Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far. Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach. We demonstrate that the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text.
Evaluating Discourse Processing Algorithms In order to take steps towards establishing a methodology for evaluating Natural Language systems, we conducted a case study. We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues. We present the quantitative results of hand-simulating these algorithms, but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general. We illustrate the general difficulties encoun- tered with quantitative evaluation. These are problems with: (a) allowing for underlying assumptions, (b) determining how to handle underspecifications, and (c) evaluating the contribution of false positives and error chaining. We refer to error chaining as the case in which a pronoun x correctly says that it is coreferent with another pronoun y while the program misidentifies the antecedent of y.
The Structure And Performance Of An Open-Domain Question Answering System This paper presents the architecture, operation and results obtained with the LASSO Question Answering system developed in the Natural Language Processing Laboratory at SMU. To find answers, the system relies on a combination of syntactic and semantic techniques. The search for the answer is based on a novel form of indexing called paragraph indexing. A score of 55.5% for short answers and 64.5% for long answers was achieved at the TREC-8 competition. We transform a natural language question into an IR query. We select as keywords all named entities that were recognized as proper nouns.
Supertagging: An Approach To Almost Parsing In this paper, we have proposed novel methods for robust parsing that integrate the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques. Our thesis is that the computation of linguistic structure can be localized iflexical items are associated with rich descriptions (supertags) that impose complex constraints in a local context. The supertags are designed such that only those elements on which the lexical item imposes constraints appear within a given supertag. Further, each lexical item is associated with as many supertags as the number of different syntactic contexts in which the lexical item can appear. This makes the number of different descriptions for each lexical item much larger than when the descriptions are less complex, thus increasing the local ambiguity for a parser. But this local ambiguity can be resolved by using statistical distributions of supertag co-occurrences collected from a corpus of parses. We have explored these ideas in the context of the Lexicalized Tree-Adjoining Grammar (LTAG) framework. The supertags in LTAG combine both phrase structure information and dependency information in a single representation. Supertag disambiguation results in a representation that is effectively a parse (an almost parse), and the parser need "only" combine the individual supertags. This method of parsing can also be used to parse sentence fragments such as in spoken utterances where the disambiguated supertag sequence may not combine into a single structure. We indicate that, correct disambiguation with supertagging, i.e., assignment of lexical entries before parsing, enable effective LTAG (Lexicalized Tree-Adjoining Grammar) parsing.
Entity-Based Cross-Document Core f erencing Using the Vector Space Model Cross-document coreference occurs when the same person, place, event, or concept is discussed in more than one text source. Computer recognition of this phenomenon is important because it helps break "the document boundary" by allowing a user to examine information about a particular entity from multiple text sources at the same time. In this paper we describe a cross-document coreference resolution algorithm which uses the Vector Space Model to resolve ambiguities between people having the same name. In addition, we also describe a scoring algorithm for evaluating the cross-document coreference chains produced by our system and we compare our algorithm to the scoring algorithm used in the MUC-6 (within document) coreference task.   We proposed entity-based cross-document co-referencing which uses co-reference chains of each document to generate its summary and then use the summary rather than the whole article to select informative words to be the features of the document.
Decision Lists For Lexical Ambiguity Resolution: Application To Accent Restoration In Spanish And French This paper presents a statistical decision procedure for lexical ambiguity resolution. The algorithm exploits both local syntactic patterns and more distant collocational evidence, generating an efficient, effective, and highly perspicuous recipe for resolving a given ambiguity. By identifying and utilizing only the single best disambiguating evidence in a target context, the algorithm avoids the problematic complex modeling of statistical dependencies. Although directly applicable to a wide class of ambiguities, the algorithm is described and evaluated in a realistic case study, the problem of restoring missing accents in Spanish and French text. Current accuracy exceeds 99% on the full task, and typically is over 90% for even the most difficult ambiguities. We note that lemmatization allows for more compact and generalizable data by clustering all inflected forms of an ambiguous word together. The strategy we adopt to model syntagmatic relations is to provide bigrams and trigrams of collocated words as features to descrive local contexts.
WordNet::Similarity - Measuring The Relatedness Of Concepts WordNet::Similarity is a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts (or synsets). It provides six measures of similarity, and three measures of relatedness, all of which are based on the lexical database WordNet. These measures are implemented as Perl modules which take as input two concepts, and return a numeric value that represents the degree to which they are similar or related. Extensive research concerning the integration of semantic knowledge into NLP for the English language has been arguably fostered by the emergence of WordNet::Similarity package. The measures vary from simple edge-counting to attempt to factor in peculiarities of the network structure by considering link direction, relative path, and density, such as vector, lesk, hso, lch, wup, path, res, lin and jcn.
Toward Memory-Based Translation An essential problem of example-based translation is how to utilize more than one translation example for translating one source sentence. This 1)aper proposes a method to solve this problem. We introduce the representation, called matching expression, which represents the combination of fragments of translation examples. The translation process consists of three steps: (1) Make the source matching expression from the source sentence. (2) Transfer the source matching expression into the target matching expression. (3) Construct the target sentence from the target matching expression. This mechanism generates some candidates of translation. To select, the best translation out of them, we define the score of a translation. We combine a measure of structural similarity with a measure of word distance in order to obtain the overall distance measure that is used for matching.
A Computational Model Of Language Performance: Data Oriented Parsing Data Oriented Parsing (DOP) is a model where no abstract rules, but language experiences in the form of an analyzed corpus, constitute the basis for language processing. Analyzing a new input means that the system attempts to find find the most probable way to reconstruct the input out of fragments that already exist in the corpus. Disambiguation occurs as a side-effect. DOP can be implemented by using conventional parsing strategies. In this work, super strong equivalence relations between other stochastic grammars are studied. We show that conventional context-free parsing techniques can be used in creating a parse forest for a sentence in DOP1.
Three Generative Lexicalized Models For Statistical Parsing In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96). We provide a 29-million word parsed corpus from the Wall Street Journal.
LexPageRank: Prestige In Multi-Document Text Summarization Multidocument extractive summarization relies on the concept of sentence centrality to identify the most important sentences in a document. Centrality is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank. In this model, a sentence connectivity matrix is constructed based on cosine similarity. If the cosine similarity between two sentences exceeds a particular predefined threshold, a corresponding edge is added to the connectivity matrix. We provide an evaluation of our method on DUC 2004 data. The results show that our approach outperforms centroid-based summarization and is quite successful compared to other summarization systems. we propose Lex PageRank, which is an approach for computing sentence importance based on the concept of eigenvector centrality.
Towards Terascale Semantic Acquisition Although vast amounts of textual data are freely available, many NLP algorithms exploit only a minute percentage of it. In this paper, we study the challenges of working at the terascale. We present an algorithm, designed for the terascale, for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method. We focus on the accuracy of these two systems as a function of processing time and corpus size. We propose a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performance and efficiency. We extend is-a relation acquisition towards terascale, and automatically identified hypernym patterns by minimal edit distance. We propose, in the scenario of extracting is-a relations, one pattern-based approach and compared it with a baseline syntactic distributional similarity method (called syntactic co-occurrence in their paper).
The NomBank Project: An Interim Report This paper describes NomBank, a project that will provide argument structure for instances of common nouns in the Penn Treebank II corpus. NomBank is part of a larger effort to add additional layers of annotation to the Penn Treebank II corpus. The University of Pennsylvania’s PropBank, NomBank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text. This paper describes the NomBank project in detail including its specifications and the process involved in creating the resource. we provide coarse annotations for some of the possessive con st ructions in the Penn Treebank, but only those that meet their criteria.
One Sense Per Collocation Previous work [Gale, Church and Yarowsky, 1992] showed that with high probability a polysemous word has one sense per discourse. In this paper we show that for certain definitions of collocation, a polysemous word exhibits essentially only one sense per collocation. We test this empirical hypothesis for several definitions of sense and collocation, and discover that it holds with 90-99% accuracy for binary ambiguities. We utilize this property in a disambiguation algorithm that achieves precision of 92% using combined models of very local context. We define collocation as a co-occurrence of two words in a defined relation. In order to analyze and compare the behavior of several kinds of collocations, we use a measure of entropy as well as the results obtained when tagging heldout data with the collocations organized as decision lists. We find that the objects of verbs play a more dominant role than their subjects in WSD and nouns acquire more stable disambiguating information from their noun or adjective modifiers.
Char Align: A Program For Aligning Parallel Texts At The Character Level There have been a number of recent papers on aligning parallel texts at the sentence level, e.g., Brown et al (1991), Gale and Church (to appear), Isabelle (1992), Kay and R/Ssenschein (to appear), Simard et al (1992), Warwick-Armstrong and Russell (1990). On clean inputs, such as the Canadian Hansards, these methods have been very successful (at least 96% correct by sentence). Unfortunately, if the input is noisy (due to OCR and/or unknown markup conventions), then these methods tend to break down because the noise can make it difficult to find paragraph boundaries, let alone sentences. This paper describes a new program, charalign, that aligns texts at the character level rather than at the sentence/paragraph level, based on the cognate approach proposed by Simard et al. We show that cheap alignment of text segments is possible by exploiting orthographic cognates. Char_align is designed for language pairs that share a common alphabet.
Cut And Paste Based Text Summarization We present a cut and paste based text summarizer, which uses operations derived from an analysis of human written abstracts. The summarizer edits extracted sentences, using reduction to remove inessential phrases and combination to merge resuiting phrases together as coherent sentences. Our work includes a statistically based sentence decomposition program that identifies where the phrases of a summary originate in the original document, producing an aligned corpus of summaries and articles which we used to develop the summarizer. We first extract sentences, then remove redundant phrases, and use (manual) recombination rules to produce coherent output. We manually analyze 30 human-written summaries, and find that 19% of sentences can not be explained by cut-and-paste operations from the source text.
An Efficient Probabilistic Context-Free Parsing Algorithm That Computes Prefix Probabilities We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities. Probabilities (a) and (b) are computed incrementally in a single left-to-right pass over the input. Our algorithm compares favorably to standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars by making use of Earley's top-down control structure. It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs. An Earley chart is used for keeping track of all derivations that are consistent with the input.
Edge-Based Best-First Chart Parsing Best-first probabilistic chart parsing attempts to parse efficiently by working on edges that are judged "best" by some probabilistic figure of merit (FOM). Recent work has used probabilistic context-free grammars (PCFGs) to assign probabilities to constituents, and to use these probabilities as the starting point for the FOM. This paper extends this approach to using a probabilistic FOM to judge edges (incomplete constituents), thereby giving a much finer-grained control over parsing effort. We show how this can be accomplished in a particularly simple way using the common idea of binarizing the PCFG. The results obtained are about a factor of twenty improvement over the best prior results - that is, our parser achieves equivalent results using one twentieth the number of edges. Furthermore we show that this improvement is obtained with parsing precision and recall levels superior to those achieved by exhaustive parsing. We introduce overparsing as a technique to improve parse accuracy by continuing parsing after the first complete parse tree is found.
Structured Models for Fine-to-Coarse Sentiment Analysis In this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Inference in the model is based on standard sequence classification techniques using constrained Viterbi to ensure consistent solutions. The primary advantage of such a model is that it allows classification decisions from one level in the text to influence decisions at another. Experiments show that this method can significantly reduce classification error relative to models trained in isolation. We showed that jointly learning fine-grained (sentence) and coarse-grained (document) sentiment improves predictions at both levels.
Ontological Promiscuity To facilitate work in discourse interpretation, the logical form of English sentences should be both close to English and syntactically simple. In this paper I propose a logical notation which is first-order and nonintensional, and for which semantic translation can be naively compositional. The key move is to expand what kinds of entities one allows in one's ontology, rather than complicating the logical notation, the logical form of sentences, or the semantic translation process. Three classical problems - opaque adverbials, the distinction between de re and de ditto belief reports, and the problem of identity in intensional contexts - are examined for the difficulties they pose for this logical notation, and it is shown that the difficulties can be overcome. The paper closes with a statement about the view of semantics that is presupposed by this approach. We advocate for an ontologically promiscuous representation that includes a wide variety of types of entities. We argue that semantic representations for natural language need not be higher-order in that ontological promiscuity can solve the problem.
Distributed Word Clustering for Large Scale Class-Based Language Modeling in Machine Translation In statistical language modeling, one technique to reduce the problematic effects of data sparsity is to partition the vocabulary into equivalence classes. In this paper we investigate the effects of applying such a technique to higher-order n-gram models trained on large corpora. We introduce a modification of the exchange clustering algorithm with improved efficiency for certain partially class-based models and a distributed version of this algorithm to efficiently obtain automatic word classifications for large vocabularies (>1 million words) using such large training corpora (>30 billion tokens). The resulting clusterings are then used in training partially class-based language models. We show that combining them with word-based n-gram models in the log-linear model of a state-of-the-art statistical machine translation system leads to improvements in translation quality as indicated by the BLEU score. We introduce the predictive class bigram model.
HHMM-Based Chinese Lexical Analyzer ICTCLAS This document presents the results from Inst. of Computing Tech., CAS in the ACL SIGHAN-sponsored First International Chinese Word Segmentation Bake-off. The authors introduce the unified HHMM-based frame of our Chinese lexical analyzer ICTCLAS and explain the operation of the six tracks. Then provide the evaluation results and give more analysis. Evaluation on ICTCLAS shows that its performance is competitive. Compared with other system, ICTCLAS has ranked top both in CTB and PK closed track. In PK open track, it ranks second position. ICTCLAS BIG5 version was transformed from GB version only in two days; however, it achieved well in two BIG5 closed tracks. Through the first bakeoff, we could learn more about the development in Chinese word segmentation and become more confident on our HHMM-based approach. At the same time, we really find our problems during the evaluation. The bakeoff is interesting and helpful.
Language Independent NER Using A Maximum Entropy Tagger Named Entity Recognition (NER) systems need to integrate a wide variety of information for optimal performance. This paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy. The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch. We condition the label of a token at a particular position on the label of the most recent previous instance of that same token in a prior sentence of the same document. Our named entity recogniser is run on pos-tagged and chunked documents in the corpus to identify and extract named entities as potential topics.
Multi-Source Transfer of Delexicalized Dependency Parsers We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-the-art performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. We show that part-of-speech tags contain significant amounts of information for unlabeled dependency parsing. We demonstrate an alternative to grammar induction by projecting reference parse trees from languages that have annotations to ones that are resource-poor. Tree banks in other languages can still serve as a kind of proxy for learning which features generally transfer useful in formation. We demonstrate that projecting from a single oracle chosen language can lead to good parsing performance.
Using Universal Linguistic Knowledge to Guide Grammar Induction We present an approach to grammar induction that utilizes syntactic universals to improve dependency parsing across a range of languages. Our method uses a single set of manually-specified language-independent rules that identify syntactic dependencies between pairs of syntactic categories that commonly occur across languages. During inference of the probabilistic model, we use posterior expectation constraints to require that a minimum proportion of the dependencies we infer be instances of these rules. We also automatically refine the syntactic categories given in our coarsely tagged input. Across six languages our approach outperforms state-of-the-art unsupervised methods by a significant margin. Our system is  weakly supervised, in which manually defined universal syntactic rules (USR) are used to constrain a probabilistic Bayesian model.
Large Language Models in Machine Translation This paper reports on the benefits of large-scale statistical language modeling in machine translation. A distributed infrastructure is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. It is capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases. 5-gram word language models in English are trained on a variety of monolingual corpora. In the case of language models, we often have to remove low-frequency words because of a lack of computational resources, since the feature space of k grams tends to be so large that we sometimes need cutoffs even in a distributed environment. To scale LMs to larger corpora with higher-order dependencies we consider distributed language models that scale more readily. Stupid back off smoothing is significantly more efficient to train and deploy in a distributed framework than a context dependent smoothing scheme such as Kneser-Ney. We show that each doubling of the training data from the news domain (used to build the language model) leads to improvements of approximately 0.5 BLEU points. We used 1500 machines for a day to compute the relative frequencies of n-grams from 1.8TB of web data.
The Penn Discourse TreeBank 2.0. We present the second version of the Penn Discourse Treebank, PDTB-2.0, describing its lexically-grounded annotations of discourse relations and their two abstract object arguments over the 1 million word Wall Street Journal corpus. We describe all aspects of the annotation, including (a) the argument structure of discourse relations, (b) the sense annotation of the relations, and (c) the attribution of discourse relations and each of their arguments. We list the differences between PDTB-1.0 and PDTB-2.0. We present representative statistics for several aspects of the annotation in the corpus. we present The Penn Discourse Treebank (PDTB) ,such a corpus which provides a discourse-level annotation on top of the Penn Treebank, following a predicate argument approach (Webber, 2004).
Stochastic Attribute-Value Grammars Probabilistic analogues of regular and context-free grammars are well known in computational linguistics, and currently the subject of intensive research. To date, however, no satisfactory probabilistic analogue of attribute-value grammars has been proposed: previous attempts have failed to define an adequate parameter-estimation algorithm. In the present paper, I define stochastic attribute-value grammars and give an algorithm for computing the maximum-likelihood estimate of their parameters. The estimation algorithm is adapted from Della Pietra, Della Pietra, and Lafferty (1995). To estimate model parameters, it is necessary to compute the expectations of certain functions under random fields. In the application discussed by Della Pietra, Della Pietra, and Lafferty (representing English orthographic constraints), Gibbs sampling can be used to estimate the needed expectations. The fact that attribute-value grammars generate constrained languages makes Gibbs sampling inapplicable, but I show that sampling can be done using the more general Metropolis-Hastings algorithm. We proposes a Markov Random Field or log linear model for SUBGs.
Stanfordâ€™s Multi-Pass Sieve Coreference Resolution System at the CoNLL-2011 Shared Task This paper details the coreference resolution system submitted by Stanford at the CoNLL2011 shared task. Our system is a collection of deterministic coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information. All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. We participated in both the open and closed tracks and submitted results using both predicted and gold mentions. Our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track. The Stanford coreference resolver, which won the CoNLL-2011 shared task on coreference resolution, adopts the once-popular rule-based approach, resolving pronouns simply with rules that encode the aforementioned traditional linguistic constraints on coreference, such as the Binding constraints and gender and number agreement. The infrequency of occurrences of difficult pronouns in these standard evaluation corpora by no means undermines their significance, however.
Improving Accuracy In Word Class Tagging Through The Combination Of Machine Learning Systems We examine how differences in language models, learned by different data-driven systems performing the same NLP task, can be exploited to yield a higher accuracy than the best individual system. We do this by means of experiments involving the task of morphosyntactic word class tagging, on the basis of three different tagged corpora. Four well-known tagger generators (hidden Markov model, memory-based, transformation rules, and maximum entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second-stage classifiers. All combination taggers outperform their best component. The reduction in error rate varies with the material in question, but can be as high as 24.3% with the LOB corpus. We report on accuracy of arounf 97% with in-domain training data for POS tagging using the Penn Treebank.
Text And Knowledge Mining For Coreference Resolution Traditionally coreference is resolved by satisfying a combination of salience, syntactic, semantic and discourse constraints. The acquisition of such knowledge is time-consuming, difficult and error-prone. Therefore, we present a knowledge minimalist methodology of mining coreference rules from annotated text corpora. Semantic consistency evidence, which is a form of knowledge required by coreference, is easily retrieved from WordNet. Additional consistency knowledge is discovered by a meta-bootstrapping algorithm applied to unlabeled texts. We use paths through Wordnet, using not only synonym and is-a relations, but also parts, morphological derivations, gloss texts and polysemy, which are weighted with a measure based on the relation types and number of path elements. The path patterns in WordNet are utilized to compute the semantic consistency between NPs.
Fast Methods For Kernel-Based Text Analysis Kernel-based learning (e.g., Support Vector Machines) has been successfully applied to many hard problems in Natural Language Processing (NLP). In NLP, although feature combinations are crucial to improving performance, they are heuristically selected. Kernel methods change this situation. The merit of the kernel methods is that effective feature combination is implicitly expanded without loss of generality and increasing the computational costs. Kernel-based text analysis shows an excellent performance in terms in accuracy; however, these methods are usually too slow to apply to large-scale text analysis. In this paper, we extend a Basket Mining algorithm to convert a kernel-based classifier into a simple and fast linear classifier. Experimental results on English BaseNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel-based classifiers. We propose polynomial kernel inverted (PKI). PKI - Inverted Indexing, stores for each feature the support vectors in which it appears. The PKE approach uses a basket mining approach to prune many features from the expansion. An extension of the PrefixSpan algorithm (Pei et al, 2001) is used to efficiently mine the features in a low degree polynomial kernel space.
Exploiting Semantic Role Labeling WordNet And Wikipedia For Coreference Resolution In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns. We show that including predicate argument pairs as features improved the performance of a coreference resolver. We train a model for classifying whether two mentions are co-referring or not. We suggest to mine semantic relatedness from Wikipedia, which can deal with the data sparseness problem suffered by using WordNet.
Supervised Noun Phrase Coreference Research: The First Fifteen Years The research focus of computationalcoreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago. We argue that the so called mention-pair model suffers from several design flaws which originate from the locally confined perspective of the model.
Multi-Prototype Vector-Space Models of Word Meaning Current vector-space models of lexical semantics create a single “prototype” vector to represent the meaning of a word. However, due to lexical ambiguity, encoding word meaning with a single vector is problematic. This paper presents a method that uses clustering to produce multiple “sense-specific” vectors for each word. This approach provides a context-dependent vector representation of word meaning that naturally accommodates homonymy and polysemy. Experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models. We introduce a multi-prototype VSM where word sense discrimination is first applied by clustering contexts, and then prototypes are built using the contexts of the sense-labeled words.
Unsupervised Semantic Role Labeling We present an unsupervised method for labelling the arguments of verbs with their semantic roles. Our bootstrapping algorithm makes initial unambiguous role assignments, and then iteratively updates the probability model on which future assignments are based. A novel aspect of our approach is the use of verb, slot, and noun class information as the basis for backing off in our probability model. We achieve 50–65% reduction in the error rate over an informed baseline, indicating the potential of our approach for a task that has heretofore relied on large amounts of manually generated training data. we present an unsupervised method for labeling the arguments of verbs with their semantic roles. we perform unsupervised semantic role labeling by using hand-crafted verb lexicons to replace supervised semantic role training data.
Machine Transliteration Of Names In Arabic Texts We present a transliteration algorithm based on sound and spelling mappings using finite state machines. The transliteration models can be trained on relatively small lists of names. We introduce a new spelling-based model that is much more accurate than state-of-the-art phonetic-based models and can be trained on easier-to-obtain training data. We apply our transliteration algorithm to the transliteration of names from Arabic into English. We report on the accuracy of our algorithm based on exact-matching criterion and based on human-subjective evaluation. We also compare the accuracy of our system to the accuracy of human translators. We transliterate named entities in Arabic text to English by combining phonetic-based and spelling-based models, and re-ranking candidates with full-name web counts, named entities co-reference, and contextual web counts. We show that the use of outside linguistic resources such as WWW counts of transliteration candidates can greatly boost transliteration accuracy. Out spelling-based model directly maps English letter sequences into Arabic letter sequences with associated probability that are trained on a small English/Arabic name list without the need for English pronunciations.
Single Malt or Blended? A Study in Multilingual Parser Optimization We describe a two-stage optimization of the MaltParser system for the ten languages in the multilingual track of the CoNLL 2007 shared task on dependency parsing. The first stage consists in tuning a single-parser system for each language by optimizing parameters of the parsing algorithm, the feature model, and the learning algorithm. The second stage consists in building an ensemble system that combines six different parsing strategies, extrapolating from the optimal parameters settings for each language. When evaluated on the official test sets, the ensemble system significantly outperforms the single-parser system and achieves the highest average labeled attachment score. We extend the two-stage approach to a three-stage architecture where the parser and labeler generate an n-best list of parses which in turn is reranked. We point out that the official results for Chinese contained a bug, and the true performance of our system is actually much higher. We implement a left-to-right arc-eager parsing model in a way that the parser scan through an input sequence from left to right and the right dependents are attached to their heads as soon as possible.
Overview of BioNLP&rsquo;09 Shared Task on Event Extraction Even though considerable attention has been given to semantic orientation of words and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons. In this paper, we show how we create a high-quality, moderate-sized emotion lexicon using Mechanical Turk. In addition to questions about emotions evoked by terms, we show how the inclusion of a word choice question can discourage malicious data entry, help identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help obtain annotations at sense level (rather than at word level). We perform an extensive analysis of the annotations to better understand the distribution of emotions evoked by terms of different parts of speech. We identify which emotions tend to be evoked simultaneously by the same term and show that certain emotions indeed go hand in hand. The BioNLP 09 Shared Task on Event Extraction, the first large scale evaluation of biomedical event extraction systems, drew the participation of 24 groups and established a standard event representation scheme and datasets. The BioNLP 09 Shared Task is the first shared task that provided a consistent data set and evaluation tools for extraction of such biological relations.
A Noisy-Channel Approach To Question Answering We introduce a probabilistic noisy-channel model for question answering and we show how it can be exploited in the context of an end-to-end QA system. Our noisy-channel system outperforms a state-of-the-art rule-based QA system that uses similar resources. We also show that the model we propose is flexible enough to accommodate within one mathematical framework many QA-specific resources and techniques, which range from the exploitation of WordNet, structured, and semi-structured databases to reasoning, and paraphrasing. We have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. Another form of combining strategies for advanced QA is proposed: (1) a knowledge-based Q/A implementation based on syntactic/semantic processing is combined using a maximum-entropy framework with (2) a statistical noisy-channel algorithm for Q/A and (3) a pattern-based approach that learn from Web data.
Top Accuracy and Fast Dependency Parsing is not a Contradiction In addition to a high accuracy, short parsing and training times are the most important properties of a parser. However, parsing and training times are still relatively long. To determine why, we analyzed the time usage of a dependency parser. We illustrate that the mapping of the features onto their weights in the support vector machine is the major factor in time complexity. To resolve this problem, we implemented the passive-aggressive perceptron algorithm as a Hash Kernel. The Hash Kernel substantially improves the parsing times and takes into account the features of negative examples built during the training. This has lead to a higher accuracy. We could further increase the parsing and training speed with a parallel feature extraction and a parallel parsing algorithm. We are convinced that the Hash Kernel and the parallelization can be applied successful to other NLP applications as well such as transition based dependency parsers, phrase structrue parsers, and machine translation. We show that the Hash Kernel improves parsing speed and accuracy since the parser uses additionally negative features. The Mateparser is an efficient second order dependency parser that models the interaction between siblings as well as grandchildren.
An Annotation Scheme For Free Word Order Languages We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme. The resulting scheme reflects a stratificational notion of language, and makes only minimal assumptions about the interrelation of the particular representational strata. We release the NEGRA corpus, a hand parsed corpus of German newspaper text containing approximately 20,000 sentences.
Transformation Based Learning In The Fast Lane Transformation-based learning has been successfully employed to solve many natural language processing problems. It achieves state-of-the-art performance on many natural language processing tasks and does not overtrain easily. However, it does have a serious drawback: the training time is often intorelably long, especially on the large corpora which are often used in NLP. In this paper, we present a novel and realistic method for speeding up the training time of a transformation-based learner without sacrificing performance. The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems: a standard transformation-based learner, and the ICA system (Hepple, 2000). The results of these experiments show that our system is able to achieve a significant improvement in training time while still achieving the same performance as a standard transformation-based learner. This is a valuable contribution to systems and algorithms which utilize transformation-based learning at any part of the execution. we propose fnTBL toolkit, which implements several optimizations in rule learning to drastically speed up the time needed for training.
Investigating Regular Sense Extensions based on Intersective Levin Classes In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases. We see verb classes as the key to making generalizations about regular extensions of meaning. Current approaches to English classification, Levin classes and WordNet, have limitations in their applicability that impede their utility as general classification schemes. We present a refinement of Levin classes, intersective sets, which are a more fine-grained classification and have more coherent sets of syntactic frames and associated semantic components. We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the original Levin classes. We also have begun to examine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties. We show that multiple listings could in some cases be interpreted as regular sense extensions and defined intersective Levin classes, which are a more fine-grained, syntactically and semantically coherent refinement of basic Levin classes. We argue that the use of syntactic frames and verb classes can simplify the definition of different verb senses.
Fully Unsupervised Discovery of Concept-Specific Relationships by Web Mining We present a web mining method for discovering and enhancing relationships in which a specified concept (word class) participates. We discover a whole range of relationships focused on the given concept, rather than generic known relationships as in most previous work. Our method is based on clustering patterns that contain concept words and other words related to them. We evaluate the method on three different rich concepts and find that in each case the method generates a broad variety of relationships with good precision. We introduce the use of term frequency patterns for relationship discovery. As a pre-requisite to extracting relations among pairs of classes, our method extracts class instances from unstructured Web documents, by submitting pairs of instances as queries and analyzing the contents of the top 1,000 documents returned by a Web search engine. We propose a method for unsupervised discovery of concept specific relations, requiring initial word seeds.
Vector-based Models of Semantic Composition This paper proposes a framework for representing the meaning of phrases and sentences in vector space. Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments. We propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression.
Improved Source-Channel Models For Chinese Word Segmentation This paper presents a Chinese word segmentation system that uses improved source-channel models of Chinese sentence generation. Chinese words are defined as one of the following four types: lexicon words, morphologically derived words, factoids, and named entities. Our system provides a unified approach to the four fundamental features of word-level Chinese language processing: (1) word segmentation, (2) morphological analysis, (3) factoid detection, and (4) named entity recognition. The performance of the system is evaluated on a manually annotated test set, and is also compared with several state-of-the-art systems, taking into account the fact that the definition of Chinese words often varies from system to system.
Parser Combination By Reparsing We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers. We apply this idea to dependency and constituent parsing, generating results that surpass state-of-the-art accuracy levels for individual parsers. We introduce a threshold for the constituent count and search for the tree with the largest count number from all the possible constituent combinations. We combine five parsers to obtain a score of 92.1, whereas the best single parser obtains a score of 91.0.
A Mention-Synchronous Coreference Resolution Algorithm Based On The Bell Tree This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained. We apply beam search at test time, but use a static assignment of antecedents and learns log-linear model using batch learning. We use a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree. To cope with computational complexity, we heuristically search for the most probable partition by performing a beam search through a Bell tree. We show that one can obtain a very high MUC score simply by lumping all mentions together. We applied the ANY predicate to generate cluster-level features for their entity-mention model, which did not perform as well as the mention-pair model.
Supertagged Phrase-Based Statistical Machine Translation Until quite recently, extending Phrase-based Statistical Machine Translation (PBSMT) with syntactic structure caused system performance to deteriorate. In this work we show that incorporating lexical syntactic descriptions in the form of supertags can yield significantly better PBSMT systems. We describe a novel PBSMT model that integrates supertags into the target language model and the target side of the translation model. Two kinds of supertags are employed: those from Lexicalized Tree-Adjoining Grammar and Combinatory Categorial Grammar. Despite the differences between these two approaches, the supertaggers give similar improvements. In addition to supertagging, we also explore the utility of a surface global grammaticality measure based on combinatory operators. We perform various experiments on the Arabic to English NIST 2005 test set addressing issues such as sparseness, scalability and the utility of system subcomponents. Our best result (0.4688 BLEU) improves by 6.1% relative to a state-of-the- art PBSMT model, which compares very favourably with the leading systems on the NIST 2005 task.
Wide-Coverage Semantic Representations From A CCG Parser This paper shows how to construct semantic representations from the derivations produced by a wide-coverage CCG parser. Unlike the dependency structures returned by the parser itself, these can be used directly for semantic interpretation. We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text. We believe this is a major step towards wide-coverage semantic interpretation, one of the key objectives of the field of NLP. We present an algorithm that learns CCG lexicons with semantics but requires fully specified CCG derivations in the training data. We consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon.
Alignment By Agreement We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions. We use discriminative SMT on large training data, training 1.5 million features on 67,000 sentences.
Generating Referring Expressions: Boolean Extensions Of The Incremental Algorithm This paper brings a logical perspective to the generation of referring expressions, addressing the incompleteness of existing algorithms in this area. After studying references to individual objects, we discuss references to sets, including Boolean descriptions that make use of negated and disjoined properties. To guarantee that a distinguishing description is generated whenever such descriptions exist, the paper proposes generalizations and extensions of the Incremental Algorithm of Dale and Reiter (1995).
Centering: A Framework For Modeling The Local Coherence Of Discourse This paper concerns relationships among focus of attention, choice of referring expression, and perceived coherence of utterances within a discourse segment. It presents a framework and initial theory of centering intended to model the local component of attentional state. The paper examines interactions between local coherence and choice of referring expressions; it argues that differences in coherence correspond in part to the inference demands made by different types of referring expressions, given a particular attentional state. It demonstrates that the attentional state properties modeled by centering can account for these differences. Our centering model uses a ranking of discourse entities realized in particular sentence sand computes transitions between adjacent sentences to provide insight in the felicity of texts. Our centering theory postulates strong links between the center of attention in comprehension of adjacent sentences and syntactic position and form of reference. Our centering Theory is an entity-based theory of local coherence, which claims that certain entities mentioned in an utterance are more central than others and that this property constrains a speaker's use of certain referring expressions. Our centering Theory is an influential framework for modelling entity coherence in computational linguistics in the last two decades.
Language Model Based Arabic Word Segmentation We approximate Arabic’s rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix* (* denotes zero or more occurrences of a morpheme). Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus. The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input. The language model is initially estimated from a small manually segmented corpus of about 110,000 words. To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus. The resulting Arabic word segmentation system achieves around 97% exact match accuracy on a test corpus containing 28,449 word tokens. We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest. We demonstrate a technique for segmenting Arabic text and use it as a morphological processing step in machine translation.
Disambiguating Nouns Verbs And Adjectives Using Automatically Acquired Selectional Preferences Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information. We evaluate WSD using selectional preferences acquired for English adjective–noun, subject, and direct object grammatical relationships with respect to a standard test corpus. The selectional preferences are specific to verb or adjective classes, rather than individual word forms, so they can be used to disambiguate the co-occurring adjectives and verbs, rather than just the nominal argument heads. We also investigate use of the one-sense-per-discourse heuristic to propagate a sense tag for a word to other occurrences of the same word within the current document in order to increase coverage. Although the preferences perform well in comparison with other unsupervised WSD systems on the same corpus, the results show that for many applications, further knowledge sources would be required to achieve an adequate level of accuracy and coverage. In addition to quantifying performance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance. We report that the Word-Class Model performs well in unsupervised WSD.
Using A Semantic Concordance For Sense Identification This paper proposes benchmarks for systems of automatic sense identification. A textual corpus in which open-class words had been tagged both syntactically and semantically was used to explore three statistical strategies for sense identification: a guessing heuristic, a most-frequent heuristic, and a co-occurrence heuristic. When no information about sense-frequencies was available, the guessing heuristic using the numbers of alternative senses in WordNet was correct 45% of the time. When statistics for sense-frequancies were derived from a semantic concordance, the assumption that each word is used in its most frequently occurring sense was correct 69% of the time; when that figure was calculated for polysemous words alone, it dropped to 58%. And when a co-occurence heuristic took advantage of prior occurrences of words together in the same sentences, little improvement was observed. The semantic concordance is still too small to estimate the potential limits of a co-occurrence heuristic. We prepare a sense-tagged corpus SEMCOR containing a substantial subset of the Brown corpus tagged with the refined senses of WORDNET.
Automatic Detection Of Text Genre As the text databases available to users become larger and more heterogeneous, genre becomes increasingly important for computational linguistics as a complement to topical and structural principles of classification. We propose a theory of genres as bundles of facets, which correlate with various surface cues, and argue that genre detection based on surface cues is as successful as detection based on deeper structural properties. We believe that parsing and word-sense disambiguation can also benefit from genre classification. We avoid structured markers since they require tagged or parsed text and replace them with character-level markers (e.g., punctuation mark counts) and derivative markers, i.e., ratios and variation measures derived from measure of lexical and character-level markers.
Named Entity Recognition Through Classifier Combination This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions. When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data. We test different methods for combining the results of four systems and found that robust risk minimization works best.
Dialogue Act Modeling For Automatic Tagging And Recognition Of Conversational Speech We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech-act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, and APOLOGY. Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence. The dialogue model is based on treating the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as observations emanating from the model states. Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram. The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act. We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy. Models are trained and evaluated using a large hand-labeled database of 1,155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech. We achieved good dialogue act labeling accuracy (65% based on errorful, automatically recognized words and prosody, and 71% based on word transcripts, compared to a chance baseline accuracy of 35% and human accuracy of 84%) and a small reduction in word recognition error. We use HMMs as a general model of discourse with an application to speech acts (or dialog acts) in conversations.
Dynamic Programming for Linear-Time Incremental Parsing Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a major problem: the search is greedy and only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming. We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging “equivalent” stacks based on feature values. Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy. Better search also leads to better learning, and our final parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster. In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). The essential idea in our calculation is to delegate (in the computation of the Viterbi score) the scoring of sh transitions to the inference rules for la/ra.
Why Doesn't EM Find Good HMM POS-Taggers? This paper investigates why the HMMs estimated by Expectation-Maximization (EM) produce such poor results as Part-of-Speech (POS) taggers. We find that the HMMs estimated by EM generally assign a roughly equal number of word tokens to each hidden state, while the empirical distribution of tokens to POS tags is highly skewed. This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution. We investigate Gibbs Sampling (GS) and Variational Bayes (VB) estimators and show that VB converges faster than GS for this task and that VB significantly improves 1-to-1 tagging accuracy over EM. We also show that EM does nearly as well as VB when the number of hidden HMM states is dramatically reduced. We also point out the high variance in all of these estimators, and that they require many more iterations to approach convergence than usually thought. we demonstrate good performance on unsupervised part-of-speech tagging (using a dictionary) with a Bayesian model.
Headline Generation Based On Statistical Translation Extractive summarization techniques cannot generate document summaries shorter than a single sentence, something that is often required. An ideal summarization system would understand each document and generate an appropriate summary directly from the results of that understanding. A more practical approach to this problem results in the use of an approximation: viewing summarization as a problem analogous to statistical machine translation. The issue then becomes one of generating a target document in a more concise language from a source document in a more verbose language. This paper presents results on experiments using this approach, in which statistical models of the term selection and term ordering are jointly applied to produce summaries in a style learned from a training corpus. We approximate the length distribution with a Gaussian. We draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization.
A Rule-Based Approach To Prepositional Phrase Attachment Disambiguation In this paper, we describe a new corpus-based approach to prepositional phrase attachment disambiguation, and present results comparing performance of this algorithm with other corpus-based approaches to this problem. We train a transformation-based learning algorithm on 12,766 quadruples from WSJ. We use the supervised transformation-based learning method and lexical and conceptual classes derived from WordNet, achieving 82% precision on 500 randomly selected examples.
Tuning Support Vector Machines For Biomedical Named Entity Recognition We explore the use of Support Vector Machines (SVMs) for biomedical named entity recognition. To make the SVM training with the available largest corpus, the GENIA corpus, tractable, we propose to split the non-entity class into sub-classes, using part-of-speech information. In addition, we explore new features such as word cache and the states of an HMM trained by unsupervised learning. Experiments on the GENIA corpus show that our class splitting technique not only enables the training with the GENIA corpus but also improves the accuracy. The proposed new features also contribute to improve the accuracy. We compare our SVM-based recognition system with a system using Maximum Entropy tagging method. For protein name recognition we achieve scores of 0.492, 0.664 and 0.565 for precision, recall and f-score respectively. We use a feature set containing lexical information, POS tags, affixes and their combinations in order to recognise and classify terms into a set of general biological classes used within the GENIA project (GENIA, 2003).
TextRunner: Open Information Extraction on the Web Traditional information extraction systems have focused on satisfying precise, narrow, pre-specified requests from small, homogeneous corpora. In contrast, the TEXTRUNNER system demonstrates a new kind of information extraction, called Open Information Extraction (OIE), in which the system makes a single, data-driven pass over the entire corpus and extracts a large set of relational tuples, without requiring any human input. (Banko et al., 2007) TEXTRUNNER is a fully-implemented, highly scalable example of OIE. TEXTRUNNER's extractions are indexed, allowing a fast query mechanism. Our first public demonstration of the TEXTRUNNER system shows the results of performing OIE on a set of 117 million web pages. It demonstrates the power of TEXTRUNNER in terms of the raw number of facts it has extracted, as well as its precision using our novel assessment mechanism. And it shows the ability to automatically determine synonymous relations and objects using large sets of extractions. We have built a faster user interface for querying the results. We provide an online demo of TextRunner.
Dependency Parsing by Belief Propagation We formulate dependency parsing as a graphical model with the novel ingredient of global constraints. We show how to apply loopy belief propagation (BP), a simple and effective tool for approximate learning and inference. As a parsing algorithm, BP is both asymptotically and empirically efficient. Even with second-order features or latent variables, which would make exact parsing considerably slower or NP-hard, BP needs only O(n3) time with a small constant factor. Furthermore, such features significantly improve parse accuracy over exact first-order methods. Incorporating additional features would increase the runtime additively rather than multiplicatively. We can encapsulate common dynamic programming algorithms within special-purpose factors to efficiently globally constrain variable configurations. DEP-TREE is a global combinatorial factor which attaches to all Link (i, j) variables and similarly contributes a factor of 1 iff the configuration of Link variables forms a valid projective dependency graph.
Mixed Initiative In Dialogue: An Investigation Into Discourse Segmentation Conversation between two people is usually of MIXED-INITIATIVE, with CONTROL over the conversation being transferred from one person to another. We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns. The application of the control rules lets us derive domain-independent discourse structures. The derived structures indicate that initiative plays a role in the structuring of discourse. In order to explore the relationship of control and initiative to discourse processes like centering, we analyze the distribution of four different classes of anaphora for two data sets. This distribution indicates that some control segments are hierarchically related to others. The analysis suggests that discourse participants often mutually agree to a change of topic. We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types. These differences can be explained in terms of collaborative planning principles. We find that as initiative passes back and fourth between discourse participants, control over the conversation similarly transfers from one speaker to another. We develope utterance-based rules for allocation of control.
Minimum Risk Annealing For Training Log-Linear Models When training the parameters for a natural language system, one would prefer to minimize 1-best loss (error) on an evaluation set. Since the error surface for many natural language problems is piecewise constant and riddled with local minima, many systems instead optimize log-likelihood, which is conveniently differentiable and convex. We propose training instead to minimize the expected loss, or risk. We define this expectation using a probability distribution over hypotheses that we gradually sharpen (anneal) to focus on the 1-best hypothesis. Besides the linear loss functions used in previous work, we also describe techniques for optimizing nonlinear functions such as precision or the BLEU metric. We present experiments training log-linear combinations of models for dependency parsing and for machine translation. In machine translation, annealed minimum risk training achieves significant improvements in BLEU over standard minimum error training. We also show improvements in labeled dependency parsing. We use a linearization technique to approximate the expectation of log BLEU score. We present a deterministic annealing training procedure, whose objective is to minimize the expected error (together with the entropy regularization technique). We observe test set gains by minimum risk annealing, which incorporates a temperature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface.
CogNIAC: High Precision Coreference With Limited Knowledge And Linguistic Resources This paper presents a high precision pronoun resolution system that is capable of greater than 90% precision with 60% and better recall for some pronouns. It is suggested that the system is resolving a sub-set of anaphors that do not require general world knowledge or sophisticated linguistic processing for successful resolution. The system does this by being very sensitive to ambiguity, and only resolving pronouns when very high confidence rules have been satisfied. The system is capable of 'noticing' ambiguity because it requires that there be a unique antecedent within a salience ranking, and the salience rankings are not total orders, i.e. two or more antecedents can be equally salient. Given the nature of the systems rules, it is very likely that they are largely domain independent and that they reflect processing strategies used by humans for general language comprehension. The system has been evaluated in two distinct experiments which support the overall validity of the approach. our method, CogNiac is a knowledge poor approach to anaphora resolution based on a set of high confidence rules which are successively applied over the pronoun under consideration.
Synchronous Tree-Adjoining Grammars The unique properties of Tree-adjoining grammars (TAG) present a challenge for the application of TAGs beyond the limited confines of syntax, for instance, to the task of semantic interpretation or automatic translation of natural language. We present a variant of TAGs, called synchronous TAGs, which characterize correspondences between languages. The formalism's intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language, or to their translates in another natural language; in summary, we intend it to allow TAGs to be used beyond their role in syntax proper. We discuss the application of synchronous TAGs to concrete examples, mentioning primarily in passing some computational issues that arise in its interpretation. Synchronous Tree Adjoining Grammars is introduced primarily for semantics but will be later also proposed for translation. A synchronous derivation process for the two syntactic structures of both languages suggests the level of cross-lingual isomorphism between the two trees (e.g. Synchronous Tree Adjoining Grammars.
A Study On Convolution Kernels For Shallow Statistic Parsing In this paper we have designed and experimented novel convolution kernels for automatic classification of predicate arguments. Their main property is the ability to process structured representations. Support Vector Machines (SVMs), using a combination of such kernels and the flat feature kernel, classify PropBank predicate arguments with accuracy higher than the current argument classification state-of-the-art. Additionally, experiments on FrameNet data have shown that SVMs are appealing for the classification of semantic roles even if the proposed kernels do not produce any improvement. We can label semantic roles by classifying the smallest subtree that includes the predicate with one of its arguments, i.e. the so called PAF structure. Our convolution kernel is characterized by two aspects: the semantic space of the subcategorization structures and the kernel function that measure their similarities.
Is It Harder To Parse Chinese Or The Chinese Treebank? We present a detailed investigation of the challenges posed when applying parsing models developed against English corpora to Chinese. We develop a factored-model statistical parser for the Penn Chinese Treebank, showing the implications of gross statistical differences between WSJ and Chinese Treebanks for the most general methods of parser adaptation. We then provide a detailed analysis of the major sources of statistical parse errors for this corpus, showing their causes and relative frequencies, and show that while some types of errors are due to difficult ambiguities inherent in Chinese grammar, others arise due to treebank annotation practices. We show how each type of error can be addressed with simple, targeted changes to the independence assumptions of the maximum likelihood-estimated PCFG factor of the parsing model, which raises our F1 from 80.7% to 82.6% on our development set, and achieves parse accuracy close to the best published figures for Chinese parsing. We argue that a careful error classification can reveal possible improvements. Noun/verb mis-taggings are a frequent error case for PCFG parsing on PCTB data, compounded in Chinese by the lack of function words and morphology. There are many linguistic differences between Chinese and English, as well as structural differences between their corresponding tree banks, and some of these make it a harder task to parse Chinese.
Head-Driven Statistical Models For Natural Language Parsing This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models. We propose to generate the head of a phrase first and then generate its sisters using Markovian processes, thereby exploiting head/sister-dependencies.
Word Identification For Mandarin Chinese Sentences Chinese sentences are composed with string of characters without blanks to mark words. However the basic unit for sentence parsing and understanding is word. Therefore the first step of processing Chinese sentences is to identify the words. The difficulties of identifying words include (l) the identification of complex words, such as Determinative-Measure, reduplications, derived words etc., (2) the identification of proper names,(3) resolving the ambiguous segmentations. In this paper, we propose the possible solutions for the above difficulties. We adopt a matching algorithm with 6 different heuristic rules to resolve the ambiguities and achieve a 99.77% of the success rate. The statistical data supports that the maximal matching algorithm is the most effective heuristics. we propose the forward maximum matching algorithm.
Aligning A Parallel English-Chinese Corpus Statistically With Lexical Criteria We describe our experience with automatic alignment of sentences in parallel English-Chinese texts. Our report concerns three related topics: (1) progress on the HKUST English-Chinese Parallel Bilingual Corpus; (2) experiments addressing the applicability of Gale and Church's (1991) length-based statistical method to the task of alignment involving a non-Indo-European language; and (3) an improved statistical method that also incorporates domain-specific lexical cues. We find that the lengths of English and Chinese texts are not as highly correlated as in the French-English task, leading to lower success rates for length-based aligners.
Exploiting Syntactic and Shallow Semantic Kernels for Question Answer Classification We study the impact of syntactic and shallow semantic information in automatic classification of questions and answers and answer re-ranking. We define (a) new tree structures based on shallow semantics encoded in Predicate Argument Structures (PASs) and (b) new kernel functions to exploit the representational power of such structures with Support Vector Machines. Our experiments suggest that syntactic information helps tasks such as question/answer classification and that shallow semantics gives remarkable contribution when a reliable set of PASs can be extracted, e.g. from answers. Our shallow semantic representations, bearing a more compact information, can prevent the sparseness of deep structural approaches and the weakness of BOW models.
Indirect-HMM-based Hypothesis Alignment for Combining Outputs from Machine Translation Systems This paper presents a new hypothesis alignment method for combining outputs of multiple machine translation (MT) systems. An indirect hidden Markov model (IHMM) is proposed to address the synonym matching and word ordering issues in hypothesis alignment. Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the parameters of the IHMM are estimated indirectly from a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty. The IHMM-based method significantly outperforms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets. Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the 2008 NIST Open MT Evaluation. we propose using an indirect hidden Markov model (IHMM) for pairwise alignment of system outputs.
Improvements In Automatic Thesaurus Extraction The use of semantic resources is common in modern NLP systems, but methods to extract lexical semantics have only recently begun to perform well enough for practical use. We evaluate existing and new similarity metrics for thesaurus extraction, and experiment with the trade-off between extraction performance and efficiency. We propose an approximation algorithm, based on canonical attributes and coarse and fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty. We show that synonymy extraction for lexical semantic resources using distributional similarity produces continuing gains in accuracy as the volume of input data increases. We demonstrate that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality. We find the JACCARD measure and the TTEST weight to have the best performance in our comparison of distance measures.
A Regression Model of Adjective-Noun Compositionality in Distributional Semantics In this paper we explore the computational modelling of compositionality in distributional models of semantics. In particular, we model the semantic composition of pairs of adjacent English Adjectives and Nouns from the British National Corpus. We build a vector-based semantic space from a lemmatised version of the BNC, where the most frequent A-N lemma pairs are treated as single tokens. We then extrapolate three different models of compositionality: a simple additive model, a pointwise-multiplicative model and a Partial Least Squares Regression (PLSR) model. We propose two evaluation methods for the implemented models. Our study leads to the conclusion that regression-based models of compositionality generally out-perform additive and multiplicative approaches, and also show a number of advantages that make them very promising for future research. Our approach to the semantic composition of adjectives with nouns draws on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. Our main innovation is to use the co-occurrence vectors of corpus-observed ANs to train a supervised composition model. We look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically.
Scalable Inference And Training Of Context-Rich Syntactic Translation Models Statistical MT has made great progress in the last few years, but current translation models are weak on re-ordering and target language fluency. Syntactic approaches seek to remedy these problems. In this paper, we take the framework for acquiring multi-level syntactic translation rules of (Galley et al., 2004) from aligned tree-string pairs, and present two main extensions of their approach: first, instead of merely computing a single derivation that minimally explains a sentence pair, we construct a large number of derivations that include contextually richer rules, and account for multiple interpretations of unaligned words. Second, we propose probability estimates and a training procedure for weighting these rules. We contrast different approaches on real examples, show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated, and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules. We use xRS formalism to allow for the use of translation rules that have multi-level target tree annotations and discontinuous source language phrases. Our rule composing method composes two or more minimal GHKM or SPMT rules having shared states to form larger rules.
A Compression-Based Algorithm For Chinese Word Segmentation Chinese is written without using spaces or other word delimiters. Although a text may be thought of as a corresponding sequence of words, there is considerable ambiguity in the placement of boundaries. Interpreting a text as a sequence of words is beneficial for some information retrieval and storage tasks: for example,full-text search, word-based compression, and keyphrase extraction. We describe a scheme that infers appropriate positions for word boundaries using an adaptive language model that is standard in text compression. It is trained on a corpus of presegmented text, and when applied to new text, interpolates word boundaries so as to maximize the compression obtained. This simple and general method performs well with respect to specialized schemes for Chinese language segmentation. Our n-gram generative language modeling based approach does not use domain knowledge.
Probabilistic Text Structuring: Experiments With Sentence Ordering Ordering information is a critical task for natural language generation applications. In this paper we propose an approach to information ordering that is particularly suited for text-to-text generation. We describe a model that learns constraints on sentence order from a corpus of domain-specific texts and an algorithm that yields the most likely order among several alternatives. We evaluate the automatically generated orderings against authored texts from our corpus and against human subjects that are asked to mimic the model's task. We also assess the appropriateness of such a model for multi-document summarization. We build a conditional model of words across adjacent sentences, focusing on words in particular semantic roles. We proposed an algorithm that computes the probability of two sentences being adjacent for ordering sentences. As the features, we propose the Cartesian product of content words in adjacent sentences.
Multi-Criteria-Based Active Learning For Named Entity Recognition In this paper, we propose a multi-criteria-based active learning approach and effectively apply it to named entity recognition. Active learning targets to minimize the human annotation efforts by selecting examples for labeling. To maximize the contribution of the selected examples, we consider the multiple criteria: informativeness, representativeness and diversity and propose measures to quantify them. More comprehensively, we incorporate all the criteria using two selection strategies, both of which result in less labeling cost than single-criterion-based method. The results of the named entity recognition in both MUC-6 and GENIA show that the labeling cost can be reduced by at least 80% without degrading the performance. We consider Active Learning (AL) for entity recognition based on Support Vector Machines. Our diversity-motivated intra-stratum sampling scheme considers K-diverse neighbors and aims to maximize the training utility of all seeds from a stratum.
Template-Based Information Extraction without the Templates Standard algorithms for template-based information extraction (IE) require predefined template schemas, and often labeled data, to learn to extract their slot fillers (e.g., an embassy is the Target of a Bombing template). This paper describes an approach to template-based IE that removes this requirement and performs extraction without knowing the template structure in advance. Our algorithm instead learns the template structure automatically from raw text, inducing template schemas as sets of linked events (e.g., bombings include detonate, set off, and destroy events) associated with semantic roles. We also solve the standard IE task, using the induced syntactic patterns to extract role fillers from specific documents. We evaluate on the MUC-4 terrorism dataset and show that we induce template structure very similar to hand-created gold structure, and we extract role fillers with an F1 score of .40, approaching the performance of algorithms that require full knowledge of the templates. We acquire event words from an external resource, group the event words to form event scenarios, and group extraction patterns for different event roles.
Randomised Language Modelling for Statistical Machine Translation A Bloom filter (BF) is a randomised data structure for set membership queries. Its space requirements are significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability. Here we explore the use of BFs for language modelling in statistical machine translation. We show how a BF containing n-grams can enable us to use much larger corpora and higher-order models complementing a conventional n-gram LM within an SMT system. We also consider (i) how to include approximate frequency information efficiently within a BF and (ii) how to reduce the error rate of these models by first checking for lower-order sub-sequences in candidate n-grams. Our solutions in both cases retain the one-sided error guarantees of the BF while taking advantage of the Zipf-like distribution of word frequencies to reduce the space requirements. We present a scheme for associating static frequency information with a set of n-grams in a BF efficiently.
Measuring The Semantic Similarity Of Texts This paper presents a knowledge-based method for measuring the semantic-similarity of texts. While there is a large body of previous work focused on finding the semantic similarity of concepts and words, the application of these word-oriented methods to text similarity has not been yet explored. In this paper, we introduce a method that combines word-to-word similarity metrics into a text-to-text metric, and we show that this method outperforms the traditional text similarity metrics based on lexical matching. we proposed a hybrid method by combining six existing knowledge-based methods.
The Berkeley FrameNet Project FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, "Tools for Lexicon Building"). The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics. The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between "frame elements" and their syntactic realizations (e.g. grammatical function, phrase type, and other syntactic traits). This report will present the project's goals and workflow, and information about the computational tools that have been adapted or created in-house for this work. We present the FrameNet project in which we havee been developing a frame-semantic lexicon for the core vocabulary of English.
Cheap and Fast â€“ But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon’s Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense. We compare the quality of labels produced by non-expert Turkers against those made by experts for a variety of NLP tasks and find that they required only four responses per item to emulate expert annotations. We show that obtaining multiple low quality labels (through Mechanical Turk) can approach high-quality editorial labels. We work with a majority rule where ties are broken uniformly at random and report an observed agreement (accuracy) between the majority rule and the gold standard of 89.7.
A Maximum Entropy Model For Prepositional Phrase Attachment We construct a benchmark dataset of 27,937 pp-attachment quadruples extracted from the Wall Street Journal corpus. We train a maximum entropy model on (v, n1, p, n2) quadruples extracted from the Wall Street Journal corpus and achieve 81.6% accuracy. Our maximum entropy approach uses the mutual information clustering algorithm.
A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation Inspired by previous preprocessing approaches to SMT, this paper proposes a novel, probabilistic approach to reordering which combines the merits of syntax and phrase-based SMT. Given a source sentence and its parse tree, our method generates, by tree operations, an n-best list of reordered inputs, which are then fed to standard phrase-based decoder to produce the optimal translation. Experiments show that, for the NIST MT-05 task of Chinese-to-English translation, the proposal leads to BLEU improvement of 1.56%. We use a maximum entropy system to learn reordering rules for binary trees (i.e., whether to keep or reorder for each node). We model reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation.
Combining Distributional And Morphological Information For Part Of Speech Induction In this paper we discuss algorithms for clustering words into classes from unlabelled text using unsupervised algorithms, based on distributional and morphological information. We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages. We propose a perplexity based test for the quality of the POS induction algorithm. We find that many-to-1 accuracy has several defects.
Centroid-Based Summarization Of Multiple Documents: Sentence Extraction Utility-Based Evaluation And User Studies We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization. Our centroid-based extractive summarizer scores sentences based on sentence-level and inter-sentence features which indicate the quality of the sentence as a summary sentence.
Broad-Coverage Sense Disambiguation And Information Extraction With A Supersense Sequence Tagger In this paper we approach word sense disambiguation and information extraction as a unified tagging problem. The task consists of annotating text with the tagset defined by the 41 Wordnet supersense classes for nouns and verbs. Since the tagset is directly related to Wordnet synsets, the tagger returns partial word sense disambiguation. Furthermore, since the noun tags include the standard named entity detection classes – person, location, organization, time, etc. – the tagger, as a by-product, returns extended named entity information. We cast the problem of supersense tagging as a sequential labeling task and investigate it empirically with a discriminatively-trained Hidden Markov Model. Experimental evaluation on the main sense-annotated datasets available, i.e., Semcor and Senseval, shows considerable improvements over the best known "first-sense" baseline. Our supersense tagger annotates text with a 46-label tag set of WNSS categories.
A Statistical Approach To Language Translation An approach to automatic translation is outlined that utilizes techniques of statistical information extraction from large data bases. The method is based on the availability of pairs of large corresponding texts that are translations of each other. In our case, the texts are in English and French. Fundamental to the technique is a complex glossary of correspondence of fixed locutions. The steps of the proposed translation process are: (1) Partition the source text into a set of fixed locutions. (2) Use the glossary plus contextual information to select the corresponding set of fixed locutions into a sequence forming the target sentence. (3) Arrange the words of the target fixed locutions into a sequence forming the target sentence. We have developed statistical techniques facilitating both the automatic creation of the glossary, and the performance of the three translation steps, all on the basis of an alignment of corresponding sentences in the two texts. While we are not yet able to provide examples of French / English translation, we present some encouraging intermediate results concerning glossary creation and the arrangement of target word sequences. We develop the so-called IBM models, implementing a set of elementary operations, such as movement, duplication and translation, that independently act on individual words in the source sentence. In general a statistical machine translation system is composed of three components: a language model, a translation model, and a decoder. The language model tells how probable a given sentence is in the source language, the translation model indicates how likely it is that a particular target sentence is a translation of a given source sentence, and the decoder is what actually takes a source sentence as input and produces its translation as output.
Guided Learning for Bidirectional Sequence Classification In this paper, we propose guided learning, a new learning framework for bidirectional sequence classification. The tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single Perceptron like learning algorithm. We apply this novel learning algorithm to POS tagging. It obtains an error rate of 2.67% on the standard PTB test set, which represents 3.3% relative error reduction over the previous best result on the same data set, while using fewer features. Our model is competitive to CRF in tagging accuracy but requires much less training time. We develop new algorithms based on the easiet-first strategy and the perceptron algorithm.
Prototype-Driven Learning For Sequence Models We investigate prototype-driven learning for primarily unsupervised sequence modeling. Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label. This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model. On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work. For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints. We also compare to semi-supervised learning and discuss the system's error trends. Prototype-driven learning (PDL) optimizes the joint marginal likelihood of data labeled with prototype input features for each label. We ask the user to suggest a few prototypes (examples) for each class and use those as features.
Improving Statistical Machine Translation Using Word Sense Disambiguation We show for the first time that incorporating the predictions of a word sense disambiguation system within a typical phrase-based statistical machine translation (SMT) model consistently improves translation quality across all three different IWSLT Chinese-English test sets, as well as producing statistically significant improvements on the larger NIST Chinese-English MT task - and moreover never hurts performance on any test set, according not only to BLEU but to all eight most commonly used automatic evaluation metrics. Recent work has challenged the assumption that word sense disambiguation (WSD) systems are useful for SMT. Yet SMT translation quality still obviously suffers from inaccurate lexical choice. In this paper, we address this problem by investigating a new strategy for integrating WSD into an SMT system, that performs fully phrasal multi-word disambiguation. Instead of directly incorporating a Senseval-style WSD system, we redefine the WSD task to match the exact same phrasal translation disambiguation task faced by phrase-based SMT systems. Our results provide the first known empirical evidence that lexical semantics are indeed useful for SMT, despite claims to the contrary. We provide a machine translation system with the WSD probabilities for a phrase translation as extra features in a log-linear model. We use rich context features based on position, syntax and local collocations to dynamically adapt the lexicons for each sentence and facilitate the choice of longer phrases. We use a state-of-the-art WSD engine (a combination of naive Bayes, maximum entropy, boosting and Kernel PCA models) to dynamically determine the score of a phrase pair under consideration and, thus, let the phrase selection adapt to the context of the sentence.
Exploiting Parallel Texts For Word Sense Disambiguation: An Empirical Study A central problem of word sense disambiguation (WSD) is the lack of manually sense-tagged data required for supervised learning. In this paper, we evaluate an approach to automatically acquire sense-tagged training data from English-Chinese parallel corpora, which are then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task. Our investigation reveals that this method of acquiring sense-tagged data is promising. On a subset of the most difficult SENSEVAL-2 nouns, the accuracy difference between the two approaches is only 14.0%, and the difference could narrow further to 6.5% if we disregard the advantage that manually sense-tagged data have in their sense coverage. Our analysis also highlights the importance of the issue of domain dependence in evaluating WSD programs. We address word sense disambiguation by manually annotating WordNet senses with their translation in the target language (Chinese), and then automatically extracting labeled examples for word sense disambiguation by applying the IBM Models to a bilingual corpus. When several senses of an English word are translated by the same Chinese word, we can collapse these senses to obtain a coarser-grained, lumped sense inventory.
Moses: Open Source Toolkit for Statistical Machine Translation We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks. Our Moses decoder implements the factored phrase-based translation model.
Revisiting Readability: A Unified Framework for Predicting Text Quality We combine lexical, syntactic, and discourse features to produce a highly predictive model of human readers' judgments of text readability. This is the first study to take into account such a variety of linguistic factors and the first to empirically demonstrate that discourse relations are strongly associated with the perceived quality of text. We show that various surface metrics generally expected to be related to readability are not very good predictors of readability judgments in our Wall Street Journal corpus. We also establish that readability predictors behave differently depending on the task: predicting text readability or ranking the readability. Our experiments indicate that discourse relations are the one class of features that exhibits robustness across these two tasks. We propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality. When readability is targeted towards adult competent language users a more prominent role is played by discourse features. Five annotators have assessed the overall text quality of each article on a scale from 1 to 5. We find non-significant correlation for the mean number of words per sentence and the mean number of characters per word.
A Maximum Entropy Approach To Natural Language Processing The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing. We propose a gain-informed selection method. .
Automatic Evaluation And Uniform Filter Cascades For Inducing N-Best Translation Lexicons This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources. The knowledge sources are cast as filters, so that any subset of them can be cascaded in a uniform framework. A new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades. The best filter cascades improve lexicon quality by up to 137% over the plain vanilla statistical method, and approach human performance. Drastically reducing the size of the training corpus has a much smaller impact on lexicon quality when these knowledge sources are used. This makes it practical to train on small hand-built corpora for language pairs where large bilingual corpora are unavailable. Moreover, three of the four filters prove useful even when used with large training corpora. We use the Longest Common Subsequence Ratio (LCSR) to measure similarity.
Message Understanding Conference-6: A Brief History We have recently completed the sixth in a series of "Message Understanding Conferences" which are designed to promote and evaluate research in information extraction. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted. We describe some of the motivations for the new format and briefly discuss some of the results of the evaluations. We demostrate that grammar-based IE systems can be effective in many scenarios. We introduce name recognition and classification tasks.
Guiding Semi-Supervision with Constraint-Driven Learning Over the last few years, two of the main research directions in machine learning of natural language processing have been the study of semi-supervised learning algorithms as a way to train classifiers when the labeled data is scarce, and the study of ways to exploit knowledge and global information in structured learning tasks. In this paper, we suggest a method for incorporating domain knowledge in semi-supervised learning algorithms. Our novel framework unifies and can exploit several kinds of task specific constraints. The experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks. We introduce constraint driven learning, CoDL. We use constraints at multiple levels, such as sentence-level constraints to specify field boundaries and global constraints to ensure relation-level consistency.
A Critique And Improvement Of An Evaluation Metric For Text Segmentation The Pk evaluation metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms. However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size distribution. We propose a simple modification to the Pk metric that remedies these problems. This new metric—called WindowDiff — moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text. As a measure for segmentation quality we develop WindowDiff, which only evaluates segment boundaries not the labels assigned to them.
Generating Natural Language Summaries From Multiple On-Line Sources We present a methodology for summarization of news about current events in the form of briefings that include appropriate background (historical) information. The system that we developed, SUMMONS, uses the output of systems developed for the DARPA Message Understanding Conferences to generate summaries of multiple documents on the same or related events, presenting similarities and differences, contradictions, and generalizations among sources of information. We describe the various components of the system, showing how information from multiple articles is combined, organized into a paragraph, and finally, realized as English sentences. A feature of our work is the extraction of descriptions of entities such as people and places for reuse to enhance a briefing. We combine work in information extraction and natural language processing.
Adding Noun Phrase Structure to the Penn Treebank The Penn Treebank does not annotate within base noun phrases (NPs), committing only to at structures that ignore the complexity of English NPs. This means that tools trained on Treebank data cannot learn the correct internal structure of NPs. This paper details the process of adding gold-standard bracketing within each noun phrase in the Penn Treebank. We then examine the consistency and reliability of our annotations. Finally, we use this resource to determine NP structure using several statistical approaches, thus demonstrating the utility of the corpus. This adds detail to the Penn Treebank that is necessary for many NLP applications. Our annotation scheme inserts NML and JJP brackets to describe the correct NP structure. We use NE tags during the annotation process, as we find that NER based features will be helpful in a statistical model.
Attention Intentions And The Structure Of Discourse In this paper we explore a new theory of discourse structure that stresses the role of purpose and processing in discourse. In this theory, discourse structure is composed of three separate but interrelated components: the structure of the sequence of utterances (called the linguistic structure), a structure of purposes (called the intentional structure), and the state of focus of attention (called the attentional state). The linguistic structure consists of segments of the discourse into which the utterances naturally aggregate. The intentional structure captures the discourse-relevant purposes, expressed in each of the linguistic segments as well as relationships among them. The attentional state is an abstraction of the focus of attention of the participants as the discourse unfolds. The attentional state, being dynamic, records the objects, properties, and relations that are salient at each point of the discourse. The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases, referring expressions, and interruptions. The theory of attention, intention, and aggregation of utterances is illustrated in the paper with a number of example discourses. Various properties of discourse are described, and explanations for the behavior of cue phrases, referring expressions, and interruptions are explored. This theory provides a framework for describing the processing of utterances in a discourse. Discourse processing requires recognizing how the utterances of the discourse aggregate into segments, recognizing the intentions expressed in the discourse and the relationships among intentions, and tracking the discourse through the operation of the mechanisms associated with attentional state. This processing description specifies in these recognition tasks the role of information from the discourse and from the participants' knowledge of the domain. We proposed a theory of discourse structure to account for why an utterance was said and what was meant by it.
Supervised Models for Coreference Resolution Traditional learning-based coreference resolvers operate by training a mention-pair classifier for determining whether two mentions are coreferent or not. Two independent lines of recent research have attempted to improve these mention-pair classifiers, one by learning a mention-ranking model to rank preceding mentions for a given anaphor, and the other by training an entity-mention classifier to determine whether a preceding cluster is coreferent with a given mention. We propose a cluster-ranking approach to coreference resolution that combines the strengths of mention rankers and entity-mention models. We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution. Experimental results on the ACE data sets demonstrate its superior performance to competing approaches. In each query we include a null-cluster instance, to allow joint learning of discourse-new detection. We show that the CR model is stronger than the MP model. Our cluster ranking model proceeds in a left-to-right fashion and adds the current discourse old mention to the highest scoring preceding cluster.
Extraposition Grammars Extraposition grammars are an extension of definite clause grammars, and are similarly defined in terms of logic clauses. The extended formalism makes it easy to describe left extraposition of constituents, an important feature of natural language syntax. Whereas head grammars provide for an account of verb fronting and cross-serial dependencies, we, introducing extraposition grammars, is focused on displacement of noun phrases in English.
Divide and Conquer: Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora We address the creation of cross-lingual textual entailment corpora by means of crowdsourcing. Our goal is to define a cheap and replicable data collection methodology that minimizes the manual work done by expert annotators, without resorting to preprocessing tools or already annotated monolingual datasets. In line with recent works emphasizing the need of large-scale annotation efforts for textual entailment, our work aims to: i) tackle the scarcity of data available to train and evaluate systems, and ii) promote the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality. We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators. The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German.
A Joint Source-Channel Model For Machine Transliteration Most foreign names are transliterated into Chinese, Japanese or Korean with approximate phonetic equivalents. The transliteration is usually achieved through intermediate phonemic mapping. This paper presents a new framework that allows direct orthographical mapping (DOM) between two different languages, through a joint source-channel model, also called n-gram transliteration model (TM). With the n-gram TM model, we automate the orthographic alignment process to derive the aligned transliteration units from a bilingual dictionary. The n-gram TM under the DOM framework greatly reduces system development effort and provides a quantum leap in improvement in transliteration accuracy over that of other state-of-the-art machine learning algorithms. The modeling framework is validated through several experiments for English-Chinese language pair. We find that English-to-Chinese transliteration without Chinese phonemes outperforms that with Chinese phonemes. Our grapheme-based approach, which treats transliteration as statistical machine translation problem under monotonic constraint, aims to obtain a direct orthographical mapping (DOM) to reduce possible errors introduced in multiple conversions. Phoneme-based approaches are usually not good enough, because name entities have various etymological origins and transliterations are not always decided by pronunciations. Many transliterated words are proper names, whose pronunciation rules may vary depending on the language of origin. Our direct orthographic mapping, making use of individual Chinese graphemes, tends to overcome the problem and model the character choice directly.
MBT: A Memory-Based Part Of Speech Tagger-Generator We introduce a memory-based approach to part of speech tagging. Memory-based learning is a form of supervised learning based on similarity-based reasoning. The part of speech tag of a word in a particular context is extrapolated from the most similar cases held in memory. Supervised learning approaches are useful when a tagged corpus is available as an example of the desired output of the tagger. Based on such a corpus, the tagger-generator automatically builds a tagger which is able to tag new text the same way, diminishing development time for the construction of a tagger considerably. Memory-based tagging shares this advantage with other statistical or machine learning approaches. Additional advantages specific to a memory-based approach include (i) the relatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii) explanation capabilities, (iv) flexible integration of information in case representations, (v) its non-parametric nature, (vi) reasonably good results on unknown words without morphological analysis, and (vii) fast learning and tagging. In this paper we show that a large-scale application of the memory-based approach is feasible: we obtain a tagging accuracy that is on a par with that of known statistical approaches, and with attractive space and time complexity properties when using IGTree, a tree-based formalism for indexing and searching huge case bases. The use of IGTree has as additional advantage that optimal context size for disambiguation is dynamically computed. Our tagger uses a very fine-grained tag set.
Lexical Normalisation of Short Text Messages: Makn Sens a #twitter Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words. Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn't require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter. We use a classifier to detect ill formed words, and then generate correction candidates based on morphophonemic similarity.
Coping With Syntactic Ambiguity Or How To Put The Block In The Box On The Table Sentences are far more ambiguous than one might have thought. There may be hundreds, perhaps thousands, of syntactic parse trees for certain very natural sentences of English. This fact has been a major problem confronting natural language processing, especially when a large percentage of the syntactic parse trees are enumerated during semantic/pragmatic processing. In this paper we propose some methods for dealing with syntactic ambiguity in ways that exploit certain regularities among alternative parse trees. These regularities will be expressed as linear combinations of ATN networks, and also as sums and products of formal power series. We believe that such encoding of ambiguity will enhance processing, whether syntactic and semantic constraints are processed separately in sequence or interleaved together. The number of possible binary-branching parses of a sentence is defined by the Catalan number, an exponential combinatoric function.
A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches This paper presents and compares WordNet-based and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses. We derive a WordNet-based measure using PageRank and combined it with several corpus based vector space models using SVMs. Examining the relations between the words in each pair, we further split this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the former contains synonyms, antonyms, identical words and hyponyms/hypernyms and the latter capture other word relations.
The Complexity of Phrase Alignment Problems Many phrase alignment models operate over the combinatorial space of bijective phrase alignments. We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard. On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient.
Domain Adaptation with Active Learning for Word Sense Disambiguation When a word sense disambiguation (WSD) system is trained on one domain but applied to a different domain, a drop in accuracy is frequently observed. This highlights the importance of domain adaptation for word sense disambiguation. In this paper, we first show that an active learning approach can be successfully used to perform domain adaptation of WSD systems. Then, by using the predominant sense predicted by expectation-maximization (EM) and adopting a count-merging technique, we improve the effectiveness of the original adaptation process achieved by the basic active learning approach. We perform supervised domain adaptation on a manually selected subset of 21 nouns from the DSO corpus. We notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning.
The CoNLL 2007 Shared Task on Dependency Parsing The Conference on Computational Natural Language Learning features a shared task, in which participants train and test their learning systems on the same data sets. In 2007, as in 2006, the shared task has been devoted to dependency parsing, this year with both a multilingual track and a domain adaptation track. In this paper, we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages. In addition, we characterize the different approaches of the participating systems, report the test results, and provide a first analysis of these results. We note that languages with free word order and high morphological complexity are the most difficult for dependency parsing. Morphologically rich languages present new challenges, as the use of state of the art parsers for more configurational and non-inflected languages like English does not reach similar performance levels in languages like Basque, Greek or Turkish.
Probabilistic Top-Down Parsing And Language Modeling This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition. The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling. A lexicalized probabilistic top-down parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers. A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity. Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model. A small recognition experiment also demonstrates the utility of the model. Our parser works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning. At each word in the string, our top-down parser provides access to the weighted set of partial analyses in the beam.
Calibrating Features For Semantic Role Labeling This paper takes a critical look at the features used in the semantic role tagging literature and show that the information in the input, generally a syntactic parse tree, has yet to be fully exploited. We propose an additional set of features and our experiments show that these features lead to fairly significant improvements in the tasks we performed. We further show that different features are needed for different subtasks. Finally, we show that by using a Maximum Entropy classifier and fewer features, we achieved results comparable with the best previously reported results obtained with SVM models. We believe this is a clear indication that developing features that capture the right kind of information is crucial to advancing the stateof-the-art in semantic analysis.
Latent Semantic Analysis For Text Segmentation This paper describes a method for linear text segmentation that is more accurate or at least as accurate as state-of-the-art methods (Utiyama and Isahara, 2001; Choi, 2000a). Inter-sentence similarity is estimated by latent semantic analysis (LSA). Boundary locations are discovered by divisive clustering. Test results show LSA is a more accurate similarity measure. We use all vocabulary words to compute low-dimensional document vectors.
Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning. Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences. We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations. We present an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method. We initialise lexical weights in their learning algorithm using corpus-wide alignment statistics across words and meaning elements.
Domain Adaptation With Structural Correspondence Learning Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resource-rich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger. Following (Blitzer et al, 2006), we present an application of structural correspondence learning to non-projective dependency parsing (McDonald et al, 2005). Our approach is to train a separate out-of-domain parser, and use this to generate additional features on the supervised and unsupervised in-domain data. We introduce SCL that is one feature representation approach that has been effective on certain high-dimensional NLP problems, including part-of-speech tagging and sentiment classification. We apply the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. We append the source domain labeled data with predicted pivots (i.e. words that appear in both the source and target domains) to adapt a POS tagger to a target domain.
Using Predicate-Argument Structures For Information Extraction In this paper we present a novel, customizable IE paradigm that takes advantage of predicate-argument structures. We also introduce a new way of automatically identifying predicate argument structures, which is central to our IE paradigm. It is based on: (1) an extended set of features; and (2) inductive decision tree learning. The experimental results prove our claim that accurate predicate-argument structures enable high quality IE results. We apply semantic parsing to capture the predicate-argument sentence structure.
Factored Translation Models We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level - may it be linguistic markup or automatically generated word classes. In a number of experiments we show that factored translation models lead to better translation performance, both in terms of automatic scores, as well as more grammatical coherence. Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data. We also propose frameworks for the simultaneous use of different word-level representations. We propose a tight integration of morpho syntactic information into the translation model, where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. We generalise the phrase-based model's representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search. Factored translation models facilitate a more data-oriented approach to agreement modeling.
A Simple and Effective Hierarchical Phrase Reordering Model While phrase-based statistical machine translation systems currently deliver state-of-the- art performance, they remain weak on word order changes. Current phrase reordering models can properly handle swaps between adjacent phrases, but they typically lack the ability to perform the kind of long-distance reorderings possible with syntax-based systems. In this paper, we present a novel hierarchical phrase reordering model aimed at improving non-local reorderings, which seamlessly integrates with a standard phrase-based system with little loss of computational efficiency. We show that this model can successfully handle the key examples often used to motivate syntax-based systems, such as the rotation of a prepositional phrase around a noun phrase. We contrast our model with reordering models commonly used in phrase-based systems, and show that our approach provides statistically significant BLEU point gains for two language pairs: Chinese-English (+0.53 on MT05 and +0.71 on MT08) and Arabic-English (+0.55 on MT05). Our hierarchical orientation model captures non-local phrase reordering by a shift reduce algorithm. We introduce a deterministic shift-reduce parser into decoding, so that the decoder always has access to the largest possible previous block, given the current translation history. We introduce three orientation models for lexicalized reordering: word-based, phrase-based and hierarchical orientation model.
Senseval-3 Task: Automatic Labeling Of Semantic Roles The SENSEVAL-3 task to perform automatic labeling of semantic roles was designed to encourage research into and use of the FrameNet dataset. The task was based on the considerable expansion of the FrameNet data since the baseline study of automatic labeling of semantic roles by Gildea and Jurafsky. The FrameNet data provide an extensive body of "gold standard" data that can be used in lexical semantics research, as the basis for its further exploitation in NLP applications. Eight teams participated in the task, with a total of 20 runs. Discussions among participants during development of the task and the scoring of their runs contributed to a successful task. Participants used a wide variety of techniques, investigating many aspects of the FrameNet data. They achieved results showing considerable improvements from Gildea and Jurafsky’s baseline study. Importantly, their efforts have contributed considerably to making the complex FrameNet dataset more accessible. They have amply demonstrated that FrameNet is a substantial lexical resource that will permit extensive further research and exploitation in NLP applications in the future. we conduct an evaluation exercise in the Senseval-3 workshop.
Is Knowledge-Free Induction Of Multiword Unit Dictionary Headwords A Solved Problem? We seek a knowledge-free method for inducing multiword units from text corpora for use as machine-readable dictionary headwords. We provide two major evaluations of nine existing collocation-finders and illustrate the continuing need for improvement. We use Latent Semantic Analysis to make modest gains in performance, but we show the significant challenges encountered in trying this approach. We show that WordNet is as effective an evaluation resource as the web for MWE detection methods, despite its inherent size limitations and static nature. We compare the semantic vector of a phrase and the vectors of its component words in two ways: one includes the phrases's contexts in the construction of the semantic vectors of the parts and one does not.
METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more commonly used Bleu metric. It is one of several automatic metrics used in this year’s shared task within the ACL WMT-07 workshop. This paper recaps the technical details underlying the metric and describes recent improvements in the metric. The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and German, in addition to English. In an MT evaluation setting, sense clusters have been integrated into an Mt evaluation metric (METEOR) and brought about an increase of the metric's correlation with human judgments of translation quality in different languages.
The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts. The motivation behind this task was that distinguishing factual and uncertain information in texts is of essential importance in information extraction. The CoNLL-2010 shared task, aimed at detecting uncertainty cues in texts, focused on these phrases in trying to determine whether sentences contain uncertain information. The goal of the CoNLL 2010 Shared Task is to develop linguistic scope detectors as well.
Improvements In Phrase-Based Statistical Machine Translation In statistical machine translation, the currently best performing systems are based in some way on phrases or word groups. We describe the baseline phrase-based translation system and various refinements. We describe a highly efficient monotone search algorithm with a complexity linear in the input sentence length. We present translation results for three tasks: Verb-mobil, Xerox and the Canadian Hansards. For the Xerox task, it takes less than 7 seconds to translate the whole test set consisting of more than 10K words. The translation results for the Xerox and Canadian Hansards task are very promising. The system even outperforms the alignment template system. In our approach smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table.
Statistical Sentence Condensation Using Ambiguity Packing And Stochastic Disambiguation Methods For Lexical-Functional Grammar We present an application of ambiguity pack- ing and stochastic disambiguation techniques for Lexical-Functional Grammars (LFG) to the domain of sentence condensation. Our system incorporates a linguistic parser/generator for LFG, a transfer component for parse reduction operating on packed parse forests, and a maximum-entropy model for stochastic output selection. Furthermore, we propose the use of standard parser evaluation methods for automatically evaluating the summarization quality of sentence condensation systems. An experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings. Overall summarization quality of the proposed system is state-of-the-art, with guaranteed grammaticality of the system output due to the use of a constraint-based parser/generator. We present a discriminative sentence compressor over the output of an LFG parser that is a packed representation of possible compressions. We apply linguistically rich LFG grammars to a sentence compression system.
Probabilistic CFG With Latent Annotations This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA. This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables. Fine-grained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm. Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared. In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F1, sentences <= 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection. We use a markovized grammar to get a better unannotated parse forest during decoding, but we do not markovize the training data. We right-binarize the tree bank data to construct grammars with only unary and binary productions.
Exploiting Shallow Linguistic Information For Relation Extraction From Biomedical Literature We propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information. We use a combination of kernel functions to integrate two different information sources: (i) the whole sentence where the relation appears, and (ii) the local contexts around the interacting entities. We performed experiments on extracting gene and protein interactions from two different data sets. The results show that our approach outperforms most of the previous methods based on syntactic and semantic information. In addition to word features, we extract shallow linguistic information such as POS tag, lemma, and orthographic features of tokens for PPI extraction.
A Linear Observed Time Statistical Parser Based On Maximum Entropy Models This paper presents a statistical parser for natural language that obtains a parsing accuracy--roughly 87% precision and 86% recall--which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall. We introduce the idea of oracle re-ranking: suppose there exists a perfect re-ranking scheme that magically picks the best parse that has the highest F-score among the top k parses for each sentence. We take a pipeline approach in that the classifiers are trained on individual decisions rather than on the overall quality of the parser, and chained to yield the global structure.
Parsing Strategies With 'Lexicalized' Grammars: Application To Tree Adjoining Grammars In this paper we present a general parsing strategy that arose from the development of an Earley-type parsing algorithm for TAGs (Schabes and Joshi 1988) and from recent linguistic work in TAGs (Abeille 1988). In our approach elementary structures are associated with their lexical heads. These structures specify extended domains of locality (as compared to a context-free grammar) over which constraints can be stated. These constraints either hold within the elementary structure itself or specify what other structures can be composed with a given elementary structure. We state the conditions under which context-free based grammars can be 'lexicalized' without changing the linguistic structures originally produced. We argue that even if one extends the domain of locality of CFGs to trees, using only substitution does not give the freedom to choose the head of each structure. We show how adjunction allows us to 'lexicalize' a CFG freely. We then show how a 'lexicalized' grammar naturally follows from the extended domain of locality of TAGs and present some of the linguistic advantages of our approach. A novel general parsing strategy for 'lexicalized' grammars is discussed. In a first stage, the parser builds a set structures corresponding to the input sentence and in a second stage, the sentence is parsed with respect to this set. The strategy is independent of the linguistic theory adopted and of the underlying grammar formalism. However, we focus our attention on TAGs. Since the set of trees needed to parse an input sentence is supposed to be finite, the parser can use in principle any search strategy. Thus, in particular, a top-down strategy can be used since problems due to recursive structures are eliminated. The parser is also able to use non-local information to guide the search. We then explain how the Earley-type parser for TAGs can be modified to take advantage of this approach. Lexicalized grammars offer significant parsing benefits as the number of applications of productions (i.e., derivation steps) is clearly bounded by the length of the input string.
Extracting Product Features And Opinions From Reviews Consumers are often forced to wade through many on-line reviews in order to make an informed product choice. This paper introduces OPINE, an unsupervised information-extraction system which mines reviews in order to build a model of important product features, their evaluation by reviewers, and their relative quality across products. Compared to previous work, OPINE achieves 22% higher precision (with only 3% lower recall) on the feature extraction task. OPINE's novel use of relaxation labeling for finding the semantic orientation of words in context leads to strong performance on the tasks of finding opinion phrases and their polarity. Our dictionary-based method utilizes Wikipedia to find an entry page for a phrase or a single term in a query. We not only analyze polarity of opinions regarding product features but also rank opinions based on their strength. We present a method that identifies product features for using corpus statistics, WordNet relations and morphological cues. The relevance ranking and extraction was performed with Pointwise Mutual Information.
Co-Training for Cross-Lingual Sentiment Classification The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classification. However, there are many freely available English sentiment corpora on the Web. This paper focuses on the problem of cross-lingual sentiment classification, which  leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data. Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are  considered as two independent views of the classification problem. We propose a co-training approach to making use of unlabeled  Chinese data. Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers. The proposed co-regression algorithm can make full use of both the features in the source language and the features in the target language in a unified framework. We propose to use ensemble method to train better Chinese sentiment classification model on English labeled data and their Chinese translation. We leveraged an available English corpus for Chinese sentiment classification by using the co-training approach to make full use of both English and Chinese features in a unified framework.
Using Emoticons To Reduce Dependency In Machine Learning Techniques For Sentiment Classification Sentiment Classification seeks to identify a piece of text according to its author's general feeling toward their subject, be it positive or negative. Traditional machine learning techniques have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the training and test data with respect to topic. This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with training data labeled with emoticons, which has the potential of being independent of domain, topic and time. In sentiment analysis research, we use emoticons in newsgroup articles to extract instances relevant for training polarity classifiers. We find that when authors of an electronic communication use an emotion, they are effectively marking up their own text with an emotional state.
Lexicalized Markov Grammars for Sentence Compression We present a sentence compression system based on synchronous context-free grammars (SCFG), following the successful noisy-channel approach of (Knight and Marcu, 2000). We define a head-driven Markovization formulation of SCFG deletion rules, which allows us to lexicalize probabilities of constituent deletions. We also use a robust approach for tree-to-tree alignment between arbitrary document-abstract parallel corpora, which lets us train lexicalized models with much more data than previous approaches relying exclusively on scarcely available document-compression corpora. Finally, we evaluate different Markovized models, and find that our selected best model is one that exploits head-modifier bilexicalization to accurately distinguish adjuncts from complements, and that produces sentences that were judged more grammatical than those generated by previous work.
Recognizing Contextual Polarity In Phrase-Level Sentiment Analysis This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline. We propose supervised learning, dividing the resources into prior polarity and context polarity. Our experiments indicate that lexicon-lookup approaches to subjectivity analysis will have limited success on general texts. We manually construct polarity lexicon, in which each entry is annotated with its degree of subjectivity (strong, weak), as well as its sentiment polarity (positive, negative and neutral). Our MPQA lexicon contains separate lexicons for subjectivity clues, intensifiers and valence shifters, which are used for identifying opinion roots, modifiers and negation words.
Efficient Unsupervised Discovery Of Word Categories Using Symmetric Patterns And High Frequency Words We present a novel approach for discovering word categories, sets of words sharing a significant aspect of their meaning. We utilize meta-patterns of high-frequency words and content words in order to discover pattern candidates. Symmetric patterns are then identified using graph-based measures, and word categories are created based on graph clique sets. Our method is the first pattern-based method that requires no corpus annotation or manually provided seed patterns or words. We evaluate our algorithm on very large corpora in two languages, using both human judgments and WordNet-based evaluation. Our fully unsupervised results are superior to previous work that used a POS tagged corpus, and computation time for huge corpora are orders of magnitude faster than previously reported. We show that pairs of words that often appear together in symmetric patterns tend to belong to the same class (that is, they share some notable aspect of their semantics).
Introduction To The CoNLL-2000 Shared Task: Chunking We give background information on the data sets, present a general overview of the systems that have taken part in the shared task and briefly discuss their performance. The dataset is extracted from the WSJ Penn Tree bank and contains 211,727 training examples and 47,377 test instances.
Multitext Grammars And Synchronous Parsers Multitext Grammars (MTGs) generate arbitrarily many parallel texts via production rules of arbitrary length. Both ordinary MTGs and their bilexical subclass admit relatively efficient parsers. Yet, MTGs are more expressive than other synchronous formalisms for which parsers have been described in the literature. The combination of greater expressive power and relatively low cost of inference makes MTGs an attractive foundation for practical models of translational equivalence. We present algorithms for synchronous parsing with more complex grammars, discussing how to parse grammars with greater than binary branching and lexicalization of synchronous grammars. We discuss the applicability of the so-called hook trick for parsing bilexical multi text grammars.
Extracting Semantic Orientations Of Words Using Spin Model We propose a method for extracting semantic orientations of words: desirable or undesirable. Regarding semantic orientations as spins of electrons, we use the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. We also propose a criterion for parameter selection on the basis of magnetization. Given only a small number of seed words, the proposed method extracts semantic orientations with high accuracy in the experiments on English lexicon. The result is comparable to the best value ever reported. We build lexical network from not only co-occurrence but other resources including thesaurus. We determine term orientation (for Japanese) according to a spin model, i.e. a physical model of a set of electrons each endowed with one between two possible spin directions, and where electrons propagate their spin direction to neighbouring electrons until the system reaches a stable configuration.
Sentiment Analysis of Twitter Data We examine sentiment analysis on Twitter data. The contributions of this paper are: (1) We introduce POS-specific prior polarity features. (2) We explore the use of a tree kernel to obviate the need for tedious feature engineering. The new features (in conjunction with previously proposed features) and the tree kernel perform approximately at the same level, both outperforming the state-of-the-art baseline. In our work a study was conducted on a reduced corpus of tweets labelled manually.
Experiments in Domain Adaptation for Statistical Machine Translation The special challenge of the WMT 2007 shared task was domain adaptation. We took this opportunity to experiment with various ways of adapting a statistical machine translation systems to a special domain (here: news commentary), when most of the training data is from a different domain (here: European Parliament speeches). This paper also gives a description of the submission of the University of Edinburgh to the shared task. Factored translation models is used for the integration of domain adaptation. We use two language models and two translation models: one in-domain and other out-of-domain to adapt the system. We learn mixture weights for language models trained with in-domain and out of-domain data respectively by minimizing the perplexity of a tuning (development) set and interpolating the models.
Which Side Are You On? Identifying Perspectives At The Document And Sentence Levels In this paper we investigate a new problem of identifying the perspective from which a document is written. By perspective we mean a point of view, for example, from the perspective of Democrats or Republicans. Can computers learn to identify the perspective of a document? Not every sentence is written strongly from a perspective. Can computers learn to identify which sentences strongly convey a particular perspective? We develop statistical models to capture how perspectives are expressed at the document and sentence levels, and evaluate the proposed models on articles about the Israeli-Palestinian conflict. The results show that the proposed models successfully learn how perspectives are reflected in word usage and can identify the perspective of a document with high accuracy. We use hierarchical Bayesian modelling for opinion modelling (Lin et al, 2006). Our experiments were conducted in political debate corpus (Lin et al 2006). We explore relationships between sentence-level and document-level classification for a stance-like prediction task. We introduce implicit sentiment a topic of study in computational linguistics under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science.
Information Fusion In The Context Of Multi-Document Summarization We present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents. Our approach is unique in its usage of language generation to reformulate the wording of the summary. We observe for that task of multi-document summarization of news articles that extraction may be inappropriate because it may produce summaries which are overly verbose or biased towards some sources.
Models Of Translational Equivalence Among Words Parallel texts (bitexts) have properties that distinguish them from other kinds of parallel data. First, most words translate to only one other word. Second, bitext correspondence is typically only partial - many words in each text have no clear equivalent in the other text. This article presents methods for biasing statistical translation models to reflect these properties. Evaluation with respect to independent human judgments has confirmed that translation models biased in this fashion are significantly more accurate than a baseline knowledge-free model. This article also shows how a statistical translation model can take advantage of preexisting knowledge that might be available about particular language pairs. Even the simplest kinds of language-specific knowledge, such as the distinction between content words and function words, are shown to reliably boost translation model performance on some tasks. Statistical models that reflect knowledge about the model domain combine the best of both the rationalist and empiricist paradigms. We measure the orthographic similarity using longest common subsequence ratio (LCSR). We define a direct association as an association between two words where the two words are indeed mutual translations. We propose Competitive Linking Algorithm (CLA) to align the words to construct confusion network. We use competitive linking to greedily construct matchings where the pair score is a measure of word-to-word association. We argue that there are ways to determine the boundaries of some multi-words phrases, allowing to treat several words as a single token.
Statistical Phrase-Based Translation We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems. We propose STIR, as a pre-ordering step in a state of-the-art phrase-based translation system from English to Japanese.
Word Reordering And A Dynamic Programming Beam Search Algorithm For Statistical Machine Translation In this article, we describe an efficient beam search algorithm for statistical machine translation based on dynamic programming (DP). The search algorithm uses the translation model presented in Brown et al. (1993). Starting from a DP-based solution to the traveling-salesman problem, we present a novel technique to restrict the possible word reorderings between source and target language in order to achieve an efficient search algorithm. Word reordering restrictions especially useful for the translation direction German to English are presented. The restrictions are generalized, and a set of four parameters to control the word reordering is introduced, which then can easily be adopted to new translation directions. The beam search procedure has been successfully tested on the Verbmobil task (German to English, 8,000-word vocabulary) and on the Canadian Hansards task (French to English, 100,000-word vocabulary). For the medium-sized Verbmobil task, a sentence can be translated in a few seconds, only a small number of search errors occur, and there is no performance degradation as measured by the word error criterion used in this article. In our work, a beam-search algorithm used for TSP is adapted to work with an IBM-4 word-based model and phrase-based model respectively.
New Ranking Algorithms For Parsing And Tagging: Kernels Over Discrete Structures And The Voted Perceptron This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm. We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the “all subtrees” (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence. We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data. Convolution kernels are used to implicitly define a tree substructure space. The tree kernel is proposed for syntactic parsing reranking. Tree kernels evaluate the similarity between two trees in terms of their overlap, generally measured as the number of common substructures.
A Model-Theoretic Coreference Scoring Scheme This note describes a scoring scheme for the coreference task in MUC6. It improves on the original approach l by: (1) grounding the scoring scheme in terms of a model ; (2) producing more intuitive recall and precision scores ; and (3) not requiring explicit computation of the transitive closure of coreference. The principal conceptual difference is that we have moved from a syntactic scoring model based on following coreference links to an approach defined by the model theory of those links. In brief, the scheme operates by comparing the equivalence classes defined by the links in the key and the response, rather than the links themselves (thus, this is only well defined for identity links, at the moment). These classes are of course the models of the IDENT equivalence relation, and this strategy is preferable for a number of reasons, one being that the scores are independent of the particular links used to encode the equivalence relation. The scores themselves are obtained by determining the minimal perturbations to the response that are required to transform its corresponding equivalence classes into those of the key. Specifically, the recall (respectively precision) error terms are found by calculating the least number of links that need to be added to the respons e (respectively the key) in order to have the classes align. Although at first blush this seems combinatorially explosive, due to references to minimal spanning subsets of the equivalence relation, it turns out it can be accomplished with a very simple counting scheme. we introduce the link-based MUC evaluation metric for the MUC-6 and MUC 7 co reference tasks.
Identifying Word Correspondences In Parallel Texts Motivated by the need to reduce on the memory requirement and to insure robustness in estimation of probability, we propose an alternative algorithm in which probabilities are not estimated and stored for all word pairs. We propose a statistic to measure the strength of correlation between source and target words. We use the phi2 statistics as the correspondence level of the word pairs and show that it was more effective than the mutual information.
Shallow Parsing With Conditional Random Fields Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods. We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model. Improved training methods based on modern optimization algorithms were critical in achieving these results. We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models CRFs have been applied with impressive empirical results to the tasks of noun phrase chunking in this work.
An Unsupervised Method For Word Sense Tagging Using Parallel Corpora We present an unsupervised method for word sense disambiguation that exploits translation correspondences in parallel corpora. The technique takes advantage of the fact that cross-language lexicalizations of the same concept tend to be consistent, preserving some core element of its semantics, and yet also variable, reflecting differing translator preferences and the influence of context. Working with parallel corpora introduces an extra complication for evaluation, since it is difficult to find a corpus that is both sense tagged and parallel with another language; therefore we use pseudo-translations, created by machine translation systems, in order to make possible the evaluation of the approach against a standard test set. The results demonstrate that word-level translation correspondences are a valuable source of information for sense disambiguation. We present an unsupervised approach to WSD that exploits translational correspondences in parallel corpora that are artificially created by applying commercial MT systems on a sense-tagged English corpus. Cross-language tagging is the goal and we present a method for word sense tagging both the source and target texts of parallel bilingual corpora with the WordNet sense inventory.
CCG Supertags in Factored Statistical Machine Translation Combinatorial Categorial Grammar (CCG) supertags present phrase-based machine translation with an opportunity to access rich syntactic information at a word level. The challenge is incorporating this information into the translation process. Factored translation models allow the inclusion of supertags as a factor in the source or target language. We show that this results in an improvement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings. we exploit factored phrase-based translation models to associate each word with a supertag, which contains most of the information needed to build a full parse.
A Systematic Comparison Of Various Statistical Alignment Models We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented. The trial and test data had been manually aligned at the word level, noting particular pairs of words either as 'sure' or 'possible' alignments.
Inducing Syntactic Categories By Context Distribution Clustering This paper addresses the issue of the automatic induction of syntactic categories from unannotated corpora. Previous techniques give good results, but fail to cope well with ambiguity or rare words. An algorithm, context distribution clustering (CDC), is presented which can be naturally extended to handle these problems. We apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters. In our bootstrapping approach we first cluster the most distributionally reliable words and then incrementallly augment each cluster with words that are distributionally similar to those already in the cluster.
Discriminative Word Alignment With Conditional Random Fields In this paper we present a novel approach for inducing word alignments from sentence aligned data. We use a Conditional Random Field (CRF), a discriminative model, which is estimated on a small supervised training set. The CRF is conditioned on both the source and target texts, and thus allows for the use of arbitrary and overlapping features over these data. Moreover, the CRF has efficient training and decoding processes which both find globally optimal solutions. We apply this alignment model to both French-English and Romanian-English language pairs. We show how a large number of highly predictive features can be easily incorporated into the CRF, and demonstrate that even with only a few hundred word-aligned training sentences, our model improves over the current state-of-the-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively.
Learning Semantic Correspondences with Less Supervision A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state. To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty--Robocup sportscasting, weather forecasts (a new domain), and NFL recaps. We propose a probabilistic generative approach to produce a Viterbi alignment between NL and MRs. We use a hierarchical semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts.
A Simple Rule-Based Part Of Speech Tagger Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods. In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another. Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rule-based tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below. Our rule based POS tagging methods extract rules from training corpus and use these rules to tag new sentence. We also show that assigning the most common part of speech for each lexical item gives a baseline of 90% accuracy.
A Uniform Architecture For Parsing And Generation The use of a single grammar for both parsing and generation is an idea with a certain elegance, the desirability of which several researchers have noted. In this paper, we discuss a more radical possibility: not only can a single grammar be used by different processes engaged in various "directions" of processing, but one and the same language-processing architecture can be used for processing the grammar in the various modes. In particular, parsing and generation can be viewed as two processes engaged in by a single parameterized theorem prover for the logical interpretation of the formalism. We discuss our current implementation of such an architecture, which is parameterized in such a way that it can be used for either purpose with grammars written in the PATR formalism. Furthermore, the architecture allows fine tuning to reflect different processing strategies, including parsing models intended to mimic psycholinguistic phenomena. This tuning allows the parsing system to operate within the same realm of efficiency as previous architectures for parsing alone, but with much greater flexibility for engaging in other processing regimes. We state that to guarantee completeness in using a precomputed entry in the chart, the entry must subsume the formula being generated top-down.
Generalizing Case Frames Using A Thesaurus And The MDL Principle A new method for automatically acquiring case frame patterns from large corpora is proposed. In particular, the problem of generalizing values of a case frame slot for a verb is viewed as that of estimating a conditional probability distribution over a partition of words, and a new generalization method based on the Minimum Description Length (MDL) principle is proposed. In order to assist with efficiency, the proposed method makes use of an existing thesaurus and restricts its attention to those partitions that are present as "cuts" in the thesaurus tree, thus reducing the generalization problem to that of estimating a "tree cut model" of the thesaurus tree. An efficient algorithm is given, which provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL. Case frame patterns obtained by the method were used to resolve PP-attachment ambiguity. Experimental results indicate that the proposed method improves upon or is at least comparable with existing methods. We use a minimum description length-based algorithm to find an optimal tree cut over WordNet for each classification problem, finding improvements over both lexical association (Hindle and Rooth, 1993) and conceptual association, and equaling the transformation-based results. We propose a model in which the appropriate cut c is selected according to the MinimumDescription Length principle; this principle explicitly accounts for the trade-off between generalisation and accuracy by minimising a sum of model description length and data description length.
The Reliability Of A Dialogue Structure Coding Scheme This paper describes the reliability of a dialogue structure coding scheme based on utterance function, game structure, and higher-level transaction structure that has been applied to a corpus of spontaneous task-oriented spoken dialogues. We computed agreement on a coarse segmentation level that was constructed on the top of finer segments, by determining how well coders agreed on where the coarse segments started, and, for agreed starts, by computing how coders agreed on where coarse segments ended.
Inducing A Semantically Annotated Lexicon Via EM-Based Clustering We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evalutated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries. We test 3000 random verb-noun pairs, requiring the erbs and nouns to appear between 30 and 3000 times in training. We use soft clustering to form classes for generalization and do not take recourse to any hand-crafter resources in our approach to selectional preference induction.
Retrieving Collocations From Text: Xtract Natural languages are full of collocations, recurrent combinations of words that co-occur more often than expected by chance and that correspond to arbitrary word usages. Recent work in lexicography indicates that collocations are pervasive in English; apparently, they are common in all types of writing, including both technical and nontechnical genres. Several approaches have been proposed to retrieve various types of collocations from the analysis of large samples of textual data. These techniques automatically produce large numbers of collocations along with statistical figures intended to reflect the relevance of the associations. However, none of these techniques provides functional information along with the collocation. Also, the results produced often contained improper word associations reflecting some spurious aspect of the training corpus that did not stand for true collocations. In this paper, we describe a set of techniques based on statistical methods for retrieving and identifying collocations from large textual corpora. These techniques produce a wide range of collocations and are based on some original filtering methods that allow the production of richer and higher-precision output. These techniques have been implemented and resulted in a lexicographic tool, Xtract. The techniques are described and some results are presented on a 10 million-word corpus of stock market news reports. A lexicographic evaluation of Xtract as a collocation retrieval tool has been made, and the estimated precision of Xtract is 80 %. We develop Xtract, a term extraction system. We propose a statistical model by measuring the spread of the distribution of co occurring pairs of words with higher strength. In terms of practical MWE identification systems, we propose a well known approach that uses a set of techniques based on statistical methods, calculated from word frequencies, to identify MWEs in corpora.
Two Statistical Parsing Models Applied To The Chinese Treebank This paper presents the first-ever results of applying statistical parsing models to the newly-available Chinese Treebank. We have employed two models, one extracted and adapted from BBN's SIFT System (Miller et al. , 1998) and a TAG-based parsing model, adapted from (Chiang, 2000). On sentences with < 40 words, the former model performs at 69% precision, 75% recall, and the latter at 77% precision and 78% recall. Our parser operates at word-level with the assumption that input sentences are pre-segmented.
Better Evaluation for Grammatical Error Correction We present a novel method for evaluating grammatical error correction. The core of our method, which we call MaxMatch (M2), is an algorithm for efficiently computing the sequence of phrase-level edits between a source sentence and a system hypothesis that achieves the highest overlap with the gold-standard annotation. This optimal edit sequence is subsequently scored using F1 measure. We test our M2 scorer on the Helping Our Own (HOO) shared task data and show that our method results in more accurate evaluation for grammatical error correction. We propose an alternative evaluation scheme which operates in terms of tokens rather than character offsets.
Local Phrase Reordering Models For Statistical Machine Translation We describe stochastic models of local phrase movement that can be incorporated into a Statistical Machine Translation (SMT) system. These models provide properly formulated, non-deficient, probability distributions over reordered phrase sequences. They are implemented by Weighted Finite State Transducers. We describe EM-style parameter re-estimation procedures based on phrase alignment under the complete translation model incorporating reordering. Our experiments show that the reordering model yields substantial improvements in translation performance on Arabic-to-English and Chinese-to-English MT tasks. We also show that the procedure scales as the bitext size is increased. We present a polynomial-time strategy. We define two local reordering models for their Translation Template Model (TTM): In the first one, called MJ-1, only adjacent phrases are allowed to swap, and the movement has to be done within a window of 2.
Semantic Role Labeling Using Different Syntactic Views Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels. In this paper we present a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classifiers. We show improvements on this system by: i) adding new features including features extracted from dependency parses, ii) performing feature selection and calibration and iii) combining parses obtained from semantic parsers trained using different syntactic views. Error analysis of the baseline system showed that approximately half of the argument identification errors resulted from parse errors in which there was no syntactic constituent that aligned with the correct argument. In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements. We combine systems that are based on phrase-structure parsing, dependency parsing, and shallow parsing. We use the Constituent, Predicate, and Predicate-Constituent related features for the kernel, resulting in the best performance. We combine the outputs of multiple parsers to extract reliable syntactic information, which is translated into features for a machine learning experiment in assigning semantic roles.
Integrating Graph-Based and Transition-Based Dependency Parsers Previous studies of data-driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference. In this paper, we show how these results can be exploited to improve parsing accuracy by integrating a graph-based and a transition-based model. By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task. We first show how the MST Parser (McDonald et al, 2005) and the Malt Parser (Nivre et al, 2007) could be improved by stacking each parser on the predictions of the other. In the previous work, Nivre and McDonald (2008) have integrated MST Parser and Malt Parser by feeding one parser's output as features into the other.
Discourse Segmentation By Human And Automated Means The need to model the relation between discourse structure and linguistic features of utterances is almost universally acknowledged in the literature on discourse. However, there is only weak consensus on what the units of discourse structure are, or the criteria for recognizing and generating them. We present quantitative results of a two-part study using a corpus of spontaneous, narrative monologues. The first part of our paper presents a method for empirically validating multiutterance units referred to as discourse segments. We report highly significant results of segmentations performed by naive subjects, where a commonsense notion of speaker intention is the segmentation criterion. In the second part of our study, data abstracted from the subjects' segmentations serve as a target for evaluating two sets of algorithms that use utterance features to perform segmentation. On the first algorithm set, we evaluate and compare the correlation of discourse segmentation with three types of linguistic cues (referential noun phrases, cue words, and pauses). We then develop a second set using two methods: error analysis and machine learning. Testing the new algorithms on a new data set shows that when multiple sources of linguistic knowledge are used concurrently, algorithm performance improves. We describe an experiment where seven untrained annotators were asked to find discourse segments in a corpus of transcribed narratives about a movie.
Distributional Memory: A General Framework for Corpus-Based Semantics Research into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus. As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems. In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes. Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods. The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature. We use a representation based on third order tensors and provide a general framework for distributional semantics in which it is possible to represent several aspects of meaning using a single data structure.
Dependency Grammar Induction via Bitext Projection Constraints Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poor languages. The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext. We consider generative and discriminative models for dependency grammar induction that use word-level alignments and a source language parser (English) to constrain the space of possible target trees. Unlike previous approaches, our framework does not require full projected parses, allowing partial, approximate transfer through linear expectation constraints on the space of distributions over trees. We consider several types of constraints that range from generic dependency conservation to language-specific annotation rules for auxiliary verb analysis. We evaluate our approach on Bulgarian and Spanish CoNLL shared task data and show that we consistently outperform unsupervised methods and can outperform supervised learning for limited training data. We use the posterior regularization (PR) approach in which a supervised English parser is used to generate constraints that are projected using a parallel corpus and used to regularize a target language parser.
A Discriminative Global Training Algorithm For Statistical MT This paper presents a novel training algorithm for a linearly-scored block sequence translation model. The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder. No translation, language, or distortion model probabilities are used as in earlier work on SMT. Therefore our method, which employs less domain specific knowledge, is both simpler and more extensible than previous approaches. Moreover, the training procedure treats the decoder as a black-box, and thus can be used to optimize any decoding scheme. The training algorithm is evaluated on a standard Arabic-English translation task. We use a BLEU oracle decoder for discriminative training of a local reordering model. We use a perceptron style algorithm for training a large number of features. We compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple, local reordering models. We present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations.
SemEval-2007 Task 07: Coarse-Grained English All-Words Task This paper presents the coarse-grained English all-words task at SemEval-2007. We describe our experience in producing a coarse version of the WordNet sense inventory and preparing the sense-tagged corpus for the task. We present the results of participating systems and discuss future directions. We show that the performance of WSD systems clearly indicates that WSD is not easy unless one adopts a coarse-grained approach, and then systems tagging all words at best perform a few percentage points above the most frequent sense heuristic (Navigli et al, 2007).
Bayesian Word Sense Induction Sense induction seeks to automatically identify word senses directly from a corpus. A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning. Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word's contexts into different classes, each representing a word sense. Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words. The Bayesian framework provides a principled way to incorporate a wide range of features beyond lexical co-occurrences and to systematically assess their utility on the sense induction task. The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset. Our latent variable formulation serves as a foundation for more robust models of other linguistic phenomena. We extract pseudo documents from a 10-word window centered on the corresponding word token for each word type. We combine different feature sets using a probabilistic Word Sense Induction model and find that only some combinations produced an improved system.
Named Entity Recognition Using An HMM-Based Chunk Tagger This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities. Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature. In this way, the NER problem can be resolved effectively. Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively. It shows that the performance is significantly better than reported by any other machine-learning system. Moreover, the performance is even consistently better than those based on handcrafted rules. Our named entity recognition system recognizes various types of MUC-style named entities such as organization, location, person, date, time, money and percentage.
Two Languages Are More Informative Than One This paper presents a new approach for resolving lexical ambiguities in one language using statistical data on lexical relations in another language. This approach exploits the differences between mappings of words to senses in different languages. We concentrate on the problem of target word selection in machine translation, for which the approach is directly applicable, and employ a statistical model for the selection mechanism. The model was evaluated using two sets of Hebrew and German examples and was found to be very useful for disambiguation.
Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms. Using statistical machine translation techniques, a semantic parser based on a synchronous context-free grammar augmented with lambda-operators is learned given a set of training sentences and their correct logical forms. The resulting parser is shown to be the best-performing system so far in a database query domain. We demonstrate that our meaning representation language outperforms FUNQL, another meaning representation language.
Conundrums in Noun Phrase Coreference Resolution: Making Sense of the State-of-the-Art We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions, the assumptions made in evaluation methodologies, and inherent differences in text corpora. First, we examine three subproblems that play a role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection. We measure the impact of each subproblem on coreference resolution and confirm that certain assumptions regarding these subproblems in the evaluation methodology can dramatically simplify the overall task. Second, we measure the performance of a state-of-the-art coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets. We show that the co-reference resolution problem can be separated into different parts ac cording to the type of the mention.
Using Encyclopedic Knowledge For Named Entity Disambiguation We present a new method for detecting and disambiguating named entities in open domain text. A disambiguation SVM kernel is trained to exploit the high coverage and rich structure of the knowledge encoded in an online encyclopedia. The resulting model significantly outperforms a less informed baseline. We measure similarity between the textual context of the NE mention and the Wikipedia categories of the candidate. We use context matching to link noun phrase subjects into Wikipedia.
A Discriminative Matching Approach To Word Alignment We present a discriminative, large-margin approach to feature-based matching for word alignment. In this framework, pairs of word tokens receive a matching score, which is based on features of that pair, including measures of association between the words, distortion between their positions, similarity of the orthographic form, and so on. Even with only 100 labeled training examples and simple features which incorporate counts from a large unlabeled corpus, we achieve AER performance close to IBM Model 4, in much less time. Including Model 4 predictions as features, we achieve a relative AER reduction of 22% in over intersected Model 4 alignments. We use a large margin approach by factoring the structure level constraints to constraints at the level of an alignment link. We use a one-to-one constraint, where words in either sentence can participate in at most one link. We cast the problem of alignment as a maximum weight bipartite matching problem, where nodes correspond to the words in the two sentences.
Automatic Verb Classification Based On Statistical Distributions Of Argument Structure Automatic acquisition of lexical knowledge is critical to a wide range of natural language processing tasks. Especially important is knowledge about verbs, which are the primary source of relational information in a sentence--the predicate-argument structure that relates an action or state to its participants (i.e., who did what to whom). In this work, we report on supervised learning experiments to automatically classify three major types of English verbs, based on their argument structure--specifically, the thematic roles they assign to participants. We use linguistically-motivated statistical indicators extracted from large annotated corpora to train the classifier, achieving 69.8% accuracy for a task whose baseline is 34%, and whose expert-based upper bound we calculate at 86.5%. A detailed analysis of the performance of the algorithm and of its errors confirms that the proposed features capture properties related to the argument structure of the verbs. Our results validate our hypotheses that knowledge about thematic relations is crucial for verb classification, and that it can be gleaned from a corpus by automatic means. We thus demonstrate an effective combination of deeper linguistic knowledge with the robustness and scalability of statistical techniques. We work with a Decision Tree and selected linguistic cues to classify English verbs into three classes: unaccusative, unergative and object-drop.
An Evaluation Exercise For Word Alignment This paper presents the task definition, resources, participating systems, and comparative results for the shared task on word alignment, which was organized as part of the HLT/NAACL 2003 Workshop on Building and Using Parallel Texts. The shared task included Romanian-English and English-French sub-tasks, and drew the participation of seven teams from around the world. We present a small dataset of 447 pairs of non-overlapping sentences which can be used to evaluate the performance of word-alignment systems.
Joint Word Segmentation and POS Tagging Using a Single Perceptron For Chinese POS tagging, word segmentation is a preliminary step. To avoid error propagation and improve segmentation by utilizing POS information, segmentation and tagging can be performed simultaneously. A challenge for this joint approach is the large combined search space, which makes efficient decoding very hard. Recent research has explored the integration of segmentation and POS tagging, by decoding under restricted versions of the full combined search space. In this paper, we propose a joint segmentation and POS tagging model that does not impose any hard constraints on the interaction between word and POS information. Fast decoding is achieved by using a novel multiple-beam search algorithm. The system uses a discriminative statistical model, trained using the generalized perceptron algorithm. The joint model gives an error reduction in segmentation accuracy of 14.6% and an error reduction in tagging accuracy of 12.2%, compared to the traditional pipeline approach. We use an approximate decoding algorithm that keeps track of a set of partially built structures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning.
A Statistical Semantic Parser That Integrates Syntax And Semantics We introduce a learning semantic parser, SCISSOR, that maps natural-language sentences to a detailed, formal, meaning- representation language. It first uses an integrated statistical parser to produce a semantically augmented parse tree, in which each non-terminal node has both a syntactic and a semantic label. A compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation. We evaluate the system in two domains, a natural-language database interface and an interpreter for coaching instructions in robotic soccer. We present experimental results demonstrating that SCISSOR produces more accurate semantic representations than several previous approaches. we introduced an approach, SCISSOR, where the composition of meaning representations is guided by syntax.
The Parallel Grammar Project We report on the Parallel Grammar (ParGram) project which uses the XLE parser and grammar development platform for six languages: English, French, German, Japanese, Norwegian, and Urdu. The ParGram English LFG is a hand-crafter broad-coverage grammar develope d with the XLE platform.
Statistical Models for Unsupervised Prepositional Phrase Attachment We present several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task. Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information. It is therefore less resource-intensive and more portable than previous corpus-based algorithm proposed for this task. We present results for prepositional phrase attachment in both English and Spanish. We first assume noun attachment for all of-PPs and then apply our disambiguation methods to all remaining PPs.
Japanese Discourse And The Process Of Centering This paper has three aims: (1) to generalize a computational account of the discourse process called CENTERING, (2) to apply this account to discourse processing in Japanese so that it can be used in computational systems for machine translation or language understanding, and (3) to provide some insights on the effect of syntactic factors in Japanese on discourse interpretation. We argue that while discourse interpretation is an inferential process, syntactic cues constrain this process; we demonstrate this argument with respect to the interpretation of ZEROS, unexpressed arguments of the verb, in Japanese. The syntactic cues in Japanese discourse that we investigate are the morphological markers for grammatical TOPIC, the postposition wa, as well as those for grammatical functions such as SUBJECT, ga, OBJECT, o and OBJECT2, ni. In addition, we investigate the role of speaker's EMPATHY, which is the viewpoint from which an event is described. This is syntactically indicated through the use of verbal compounding, i.e. the auxiliary use of verbs such as kureta, kita. Our results are based on a survey of native speakers of their interpretation of short discourses, consisting of minimal pairs, varied by one of the above factors. We demonstrate that these syntactic cues do indeed affect the interpretation of ZEROS, but that having previously been the TOPIC and being realized as a ZERO also contributes to the salience of a discourse entity. We propose a discourse rule of ZERO TOPIC ASSIGNMENT, and show that CENTERING provides constraints on when a ZERO can be interpreted as the ZERO TOPIC. We propose forward center ranking for Japanese.
Translating Named Entities Using Monolingual And Bilingual Resources Named entity phrases are some of the most difficult phrases to translate because new phrases can appear from nowhere, and because many are domain specific, not to be found in bilingual dictionaries. We present a novel algorithm for translating named entity phrases using easily obtainable monolingual and bilingual resources. We report on the application and evaluation of this algorithm in translating Arabic named entities to English. We also compare our results with the results obtained from human translations and a commercial system for the same task. We show that use of outside linguistic resources such as WWW counts of transliteration candidates can greatly boost transliteration accuracy. A spelling-based model is described that directly maps English letter sequences into Arabic letter sequences with associated probability that are trained on a small English/Arabic name list without the need for English pronunciations. The phonetics-based and spelling-based models have been linearly combined into a single transliteration model. We use Web statistics information to validate the translation candidates generated by language model, and obtained the accuracy of 72.6% in Arabic-English OOV word translation.
Contrastive Estimation: Training Log-Linear Models On Unlabeled Data Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and named-entity extraction (McCallum and Li, 2003). CRFs are log-linear, allowing the incorporation of arbitrary features into the model. To train on unlabeled data, we require unsupervised estimation methods for log-linear models; few exist. We describe a novel approach, contrastive estimation. We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computationally efficient. Applied to a sequence labeling problem - POS tagging given a tagging dictionary and unlabeled text - contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features. To define an approximation to the partition function for log-linear estimation, our contrastive estimation uses a technique to generate local neighborhoods of a parses. We generate negative evidence for the contrastive estimation method by moving or removing a word in a sentence. We define coarse grained POS tags on PTB. We initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. We attempt to identify negative examples that are semantically similar to the positive ones in order to improve the discriminative power of the classifier. We show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods. The contrastive estimation technique we propose is globally normalized and thus capable of dealing with arbitrary features.
Computing Consensus Translation For Multiple Machine Translation Systems Using Enhanced Hypothesis Alignment This paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (MT) systems. The outputs are combined and a possibly new translation hypothesis can be generated. Similarly to the well-established ROVER approach of (Fiscus, 1997) for combining speech recognition hypotheses, the consensus translation is computed by voting on a confusion network. To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering. The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment. The proposed alignment and voting approach was evaluated on several machine translation tasks, including a large vocabulary task. The method was also tested in the framework of multi-source and speech translation. On all tasks and conditions, we achieved significant improvements in translation quality, increasing e.g. the BLEU score by as much as 15% relative. We align synonyms and different morphological forms of words to each other but this is done implicitly, relying on the parallel text to learn word alignments. We use pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model. Different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). We propose using a statistical word alignment algorithm as a more robust way of aligning (monolingual) outputs into a confusion network for system combination.
An Efficient Method For Determining Bilingual Word Classes In statistical natural language processing we always face the problem of sparse data. One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling. In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation. We develop an optimization criterion based on a maximum-likelihood approach and describe a clustering algorithm. We will show that the usage of the bilingual word classes we get can improve statistical machine translation. We show improvements on perplexity of bilingual corpus, and word translation accuracy using a template-based translation model. We describe a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words.
Graph-Based Generation Of Referring Expressions This article describes a new approach to the generation of referring expressions. We propose to formalize a scene (consisting of a set of objects with various properties and relations) as a labeled directed graph and describe content selection (which properties to include in a referring expression) as a subgraph construction problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The current approach has four main advantages: (1) Graph structures have been studied extensively, and by moving to a graph perspective we get direct access to the many theories and algorithms for dealing with graphs; (2) many existing generation algorithms can be reformulated in terms of graphs, and this enhances comparison and integration of the various approaches; (3) the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches. One of the strengths of the Graph-Based Algorithm is its ability to generate expressions that involve relations between objects, and these include spatial ones (next to, on top of, etc.).
Experimental Support for a Categorical Compositional Distributional Model of Meaning Modeling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it. The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments. The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences. Our model matches the results of its competitors in the first experiment, and betters them in the second. The general improvement in results with increase in syntactic complexity showcases the compositional power of our model. We suggest that relational word vectors live in a space the dimensionality of which be a function of the arity of the relation.
A Monolingual Tree-based Translation Model for Sentence Simplification In this paper, we consider sentence simplification as a special form of translation with the complex sentence as the source and the simple sentence as the target. We propose a Tree-based Simplification Model (TSM), which, to our knowledge, is the first statistical simplification model covering splitting, dropping, reordering and substitution integrally. We also describe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia. The evaluation shows that our model achieves better readability scores than a set of baseline systems. We use a tree-based simplification model which uses techniques from statistical machine translation (SMT) with this data set. We examine the use of paired documents in English Wikipedia and Simple Wikipediafor a data-driven approach to the sentence simplification task. We propose sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning.
Has A Consensus NL Generation Architecture Appeared And Is It Psycholinguistically Plausible? I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other. I also compare this 'consensus architecture' among applied NLG systems with psycholinguistic knowledge about how humans speak, and argue that at least some aspects of the consensus architecture seem to be in agreement with what is known about human language production, despite the fact that psycholinguistic plausibility was not in general a goal of the developers of the surveyed systems. Most generation systems pipeline pragmatic, semantic, lexical and syntactic decisions. We show that psycholinguistic and engineering approaches often result in systems, which are similar in crucial respects.
Modeling Local Coherence: An Entity-Based Approach This paper considers the problem of automatic assessment of local coherence. We present a novel entity-based representation of discourse which is inspired by Centering Theory and can be computed automatically from raw text. We view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function. Our experiments demonstrate that the induced model achieves significantly higher accuracy than a state-of-the-art coherence model. To further refine the computation of the subsequence distribution, we divide the matrix into a salient matrix and a non-salient matrix. We show that our entity based model is able to distinguish a source text from its permutation accurately. We exploit the use of the distributional and referential information of discourse entities to improve summary coherence.
Efficient Third-Order Dependency Parsers We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they require only O(n4) time. Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively. The set of potential edges is pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent.
Automatic Identification Of Non-Compositional Phrases Non-compositional expressions present a special challenge to NLP applications. We present a method for automatic identification of non-compositional expressions using their statistical properties in a text corpus. Our method is based on the hypothesis that when a phrase is non-composition, its mutual information differs significantly from the mutual informations of phrases obtained by substituting one of the word in the phrase with a similar word. We use LSA to distinguish between compositional and non compositional verb-particle constructions and noun noun compounds. We define a decision criterion for non compositional phrases based on the change in the mutual information of a phrase when substituting one word for a similar one based on an automatically constructed thesaurus.
Design Challenges and Misconceptions in Named Entity Recognition We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset. We use the IOBES notation to represent NE mentions with label sequences, thereby NER is formalized as a multi class classification problem in which a given token is classified into IOBES labels. We investigate design challenges for named entity recognition, and showed that other design choices, such as the representation of output labels and using features built on external knowledge, are more important than the learning model itself. We have shown for the CoNLL-2003 shared task that Greedy decoding (i.e., beam search of width 1) is competitive to the widely used Viterbi algorithm while being over 100 times faster at the same time.
Reinforcement Learning for Mapping Instructions to Actions In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains -- Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples. We show that performing policy-gradient with this function is equivalent to training a fully supervised, stochastic gradient algorithm that optimizes conditional likelihood.
Parsing The Wall Street Journal Using A Lexical-Functional Grammar And Discriminative Estimation Techniques We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model. We report on the results of applying this system to parsing the UPenn Wall Street Journal (WSJ) treebank. The model combines full and partial parsing techniques to reach full grammar coverage on unseen data. The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models. Disambiguation performance is evaluated by measuring matches of predicate-argument relations on two distinct test sets. On a gold standard of manually annotated f-structures for a subset of the WSJ treebank, this evaluation reaches 79% F-score. An evaluation on a gold standard of dependency relations for Brown corpus data achieves 76% F-score. We describe a discriminative LFG parsing model that is trained on standard (syntax only) tree bank annotations by treating each tree as a full LFG analysis with an observed c-structure and hidden f-structure. XLE selects the most probable analysis from the potentially large candidate set by means of a stochastic disambiguation component based on a log-linear probability model that works on the packed representations.
A Practical Part-Of-Speech Tagger We present an implementation of a part-of-speech tagger based on a hidden Markov model. The methodology enables robust and accurate tagging with few resource requirements. Only a lexicon and some unlabeled training text are required. Accuracy exceeds 96%. We describe implementation strategies and optimizations which result in high-speed operation. Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment. Our semi-supervised model makes use of both labeled training text and some amount of unlabeled text. We train statistical models using unlabeled data with the expectation maximization algorithm. We report very high results (96% on the Brown corpus) for unsupervised POS tagging using Hidden Markov Models (HMMs) by exploiting hand-built tag dictionaries and equivalence classes.
Machine Learning For Coreference Resolution: From Local Classification To Global Ranking In this paper, we view coreference resolution as a problem of ranking candidate partitions generated by different coreference systems. We propose a set of partition-based features to learn a ranking model for distinguishing good and bad partitions. Our approach compares favorably to two state-of-the-art coreference systems when evaluated on three standard coreference data sets. We emphasize the global optimization of ranking clusters obtained locally. We cast coreference resolution as a classification task and solve it in two steps: (1) train a classifier to determine whether two mentions are co-referent or not, and (2) use a clustering algorithm to partition the mentions into clusters, based on the pairwise predictions. Our method ranks base models according to their performance on separate tuning set, and then uses the highest-ranked base model for predicting on test documents. Most learning based coreference systems can be defined by four elements: the learning algorithm used to train the coreference classifier, the method of creating training instances for the learner, the feature set used to represent a training or test instance, and the clustering algorithm used to coordinate the coreference classification decisions.
A* Parsing: Fast Exact Viterbi Parse Selection We present an extension of the classic A* search procedure to tabular PCFG parsing. The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions. We discuss various estimates and give efficient algorithms for com- puting them. On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%. Unlike best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation. Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time. We describe admissible heuristics and an A*framework for parsing.
Emotions From Text: Machine Learning For Text-Based Emotion Prediction In addition to information, text contains attitudinal, and more specifically, emotional content. This paper explores the text-based emotion prediction problem empirically, using supervised machine learning with the SNoW learning architecture. The goal is to classify the emotional affinity of sentences in the narrative domain of children's fairy tales, for subsequent usage in appropriate expressive rendering of text-to-speech synthesis. Initial experiments on a preliminary data set of 22 fairy tales show encouraging results over a narrative baseline and BOW approach for classification of emotional versus non-emotional contents, with some dependency on parameter tuning. We also discuss results for a tripartite model which covers emotional valence, as well as feature set alternations. In addition, we present plans for a more cognitively sound sequential model, taking into consideration a larger set of basic emotions.
A Stochastic Parts Program And Noun Phrase Parser For Unrestricted Text Our part of speech tagger can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency. Our part-of-speech tagger performs not only part-of-speech analysis, but it also identities the most simple kinds of noun phrases mostly sequences of determiners, premodifiers and nominal heads by inserting brackets around them.
Parser Adaptation and Projection with Quasi-Synchronous Grammar Features We connect two scenarios in structured learning: adapting a parser trained on one corpus to another annotation style, and projecting syntactic annotations from one language to another. We propose quasi-synchronous grammar (QG) features for these structured learning tasks. That is, we score an aligned pair of source and target trees based on local features of the trees and the alignment. Our quasi-synchronous model assigns positive probability to any alignment of any trees, in contrast to asynchronous grammar, which would insist on some form of structural parallelism. In monolingual dependency parser adaptation, we achieve high accuracy in translating among multiple annotation styles for the same sentence. On the more difficult problem of cross-lingual parser projection, we learn a dependency parser for a target language by using bilingual text, an English parser, and automatic word alignments. Our experiments show that unsupervised QG projection improves on parses trained using only high-precision projected annotations and far outperforms, by more than 35% absolute dependency accuracy, learning an unsupervised parser from raw target-language text alone. When a few target-language parse trees are available, projection gives a boost equivalent to doubling the number of target-language trees. We think of cross-language adaptation as unsupervised projection using word aligned parallel text to construct training material for the target language.
Methods For The Qualitative Evaluation Of Lexical Association Measures This paper presents methods for a qualitative, unbiased comparison of lexical association measures and the results we have obtained for adjective-noun pairs and preposition-noun-verb triples extracted from German corpora. In our approach, we compare the entire list of candidates, sorted according to the particular measures, to a reference set of manually identified "true positives". We also show how estimates for the very large number of hapaxlegomena and double occurrences can be inferred from random samples. We extract German PP-verb combinations from a chunk-parsed version of the Frankfurter Rundschau Corpus. We use four collocation measures: Point-wise mutual information (PMI); T-Score; log-likelihood; and the raw frequency of N1 N2 in the corpus. The t-test measure, which, of all standard measures, yields the best results in general-language collocation extraction studies.
Experiments With A Multilanguage Non-Projective Dependency Parser We present a deterministic classifier-based Shift/Reduce parser. We develop DeSR, an incremental deterministic classifier-based parser. We propose a transition system whose individual transitions can deal with non-projective dependencies only to a limited extent, depending on the distance in the stack of the nodes involved in the newly constructed dependency.
What's In A Translation Rule? We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora. We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data. We describe how to learn hundreds of millions of tree transformation rules from a parsed, aligned Chinese/English corpus, and we describe probability estimators for those rules. Our translation rules are learned from word-aligned bilingual texts whose source side has been parsed by using a syntactic parser.
A Problem For RST: The Need For Multi-Level Discourse Analysis We note that Rhetorical Structure Theory conflates the informational (the information being conveyed) and intentional (the effects on the reader's beliefs or attitudes) levels of discourse. We argue that both informational (semantic) and intentional relations can hold between clause simultaneously and independently.
Identifying Sources Of Opinions With Conditional Random Fields And Extraction Patterns Recent systems have been developed for sentiment classification, opinion recognition, and opinion analysis (e.g., detecting polarity and strength). We pursue another aspect of opinion analysis: identifying the sources of opinions, emotions, and sentiments. We view this problem as an information extraction task and adopt a hybrid approach that combines Conditional Random Fields (Lafferty et al., 2001) and a variation of AutoSlog (Riloff, 1996a). While CRFs model source identification as a sequence tagging task, AutoSlog learns extraction patterns. Our results show that the combination of these two methods performs better than either one alone. The resulting system identifies opinion sources with 79.3% precision and 59.5% recall using a head noun matching measure, and 81.2% precision and 60.6% recall using an overlap measure.
Automatic Evaluation Of Summaries Using N-Gram Co-Occurrence Statistics Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results. We are the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives. We propose ROUGE, a semi automatic approach,  which is primarily based on n gram co-occurrence between automatic and human summaries.
Learning Question Classifiers In order to respond correctly to a free form factual question given a large collection of texts, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer. This paper presents a machine learning approach to question classification. We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into fine-grained classes. We show accurate results on a large collection of free-form questions used in TREC 10. We assign one of fifty possible types to a question based on features present in the question. We have developed a machine learning approach which uses the SNoW learning architecture.
Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles We present a data-driven variant of the LR algorithm for dependency parsing, and extend it with a best-first search for probabilistic generalized LR dependency parsing. Parser actions are determined by a classifier, based on features that represent the current state of the parser. We apply this parsing framework to both tracks of the CoNLL 2007 shared task, in each case taking advantage of multiple models trained with different learners. In the multilingual track, we train three LR models for each of the ten languages, and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme. In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-of-domain training set, in a scheme similar to one iteration of co-training. We use a combination of co-learning and active learning by training two different parsers on the labeled training data, parsing the unlabeled domain data with both parsers, and adding parsed sentences to the training data only if the two parsers agreed on their analysis. We generalize the standard deterministic framework to probabilistic parsing by using a best-first search strategy.
Probabilistic Disambiguation Models For Wide-Coverage HPSG Parsing This paper reports the development of log-linear models for the disambiguation in wide-coverage HPSG parsing. The estimation of log-linear models requires high computational cost, especially with wide-coverage grammars. Using techniques to reduce the estimation cost, we trained the models using 20 sections of Penn Treebank. A series of experiments empirically evaluated the estimation techniques, and also examined the performance of the disambiguation models on the parsing of real-world sentences. Our HPSG parser computes deeper analyses, such as predicate argument structures. We also introduce a hybrid model, where the probabilities of the previous model are multiplied by the super tagging probabilities instead of a preliminary probabilistic model, to help the process of estimation by filtering unlikely lexical entries.
Corpus-Based Identification Of Non-Anaphoric Noun Phrases Coreference resolution involves finding antecedents for anaphoric discourse entities, such as definite noun phrases. But many definite noun phrases are not anaphoric because their meaning can be understood from general world knowledge (e.g., "the White House" or "the news media"). We have developed a corpus-based algorithm for automatically identifying definite noun phrases that are non-anaphoric, which has the potential to improve the efficiency and accuracy of coreference resolution systems. Our algorithm generates lists of non-anaphoric noun phrases and noun phrase patterns from a training corpus and uses them to recognize non-anaphoric noun phrases in new texts. Using 1600 MUC-4 terrorism news articles as the training corpus, our approach achieved 78% recall and 87% precision at identifying such noun phrases in 50 test documents. We develop a system for identifying discourse-new DDs that incorporates, in addition to syntax-based heuristics aimed at recogznizing predicative and established DDs, additional techniques for mining from corpora unfamiliar DDS including proper names, larger situation and semantically functional. We develop an unsupervised learning algorithm that automatically recognizes definite NPs that are existential without syntactic modification because their meaning is universally understood.
Building A Sense Tagged Corpus With Open Mind Word Expert Open Mind Word Expert is an implemented active learning system for collecting word sense tagging from the general public over the Web. It is available at http://teach-computers.org. We expect the system to yield a large volume of high-quality training data at a much lower cost than the traditional method of hiring lexicographers. We thus propose a Senseval-3 lexical sample activity where the training data is collected via Open Mind Word Expert. If successful, the collection process can be extended to create the definitive corpus of word sense information. Finally, in an effort related to the Wikipedia collection process, we implemente the Open Mind Word Expert system for collecting sense annotations from volunteer contributors over the Web. we presented another interesting proposal which turns to Web users to produce sense-tagged corpora.
Efficient Minimum Error Rate Training and Minimum Bayes-Risk Decoding for Translation Hypergraphs and Lattices Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most current state-of-the-art Statistical Machine Translation (SMT) systems. The algorithms were originally developed to work with N-best lists of translations, and recently extended to lattices that encode many more hypotheses than typical N-best lists. We here extend lattice-based MERT and MBR algorithms to work with hypergraphs that encode a vast number of translations produced by MT systems based on Synchronous Context Free Grammars. These algorithms are more efficient than the lattice-based versions presented earlier. We show how MERT can be employed to optimize parameters for MBR decoding. Our experiments show speedups from MERT and MBR as well as performance improvements from MBR decoding on several language pairs. We describe an efficient approximate algorithm for computing n-gram posterior probabilities.
Recognizing Stances in Online Debates This paper presents an unsupervised opinion analysis method for debate-side classification, i.e., recognizing which stance a person is taking in an online debate. In order to handle the complexities of this genre, we mine the web to learn associations that are indicative of opinion stances in debates. We combine this knowledge with discourse information, and formulate the debate side classification task as an Integer Linear Programming problem. Our results show that our method is substantially better than challenging baseline methods. We define stance as an overall position held by a person toward an object, idea or proposition.
Features And Values The paper discusses the linguistic aspects of a new general purpose facility for computing with features. The program was developed in connection with the course I taught at the University of Texas in the fall of 1983. It is a generalized and expanded version of a system that Stuart Shieber originally designed for the PATR-II project at SRI in the spring of 1983 with later modifications by Fernando Pereira and me. Like its predecessors, the new Texas version of the "DG {directed graph}" package is primarily intended for representing morphological and syntactic information but it may turn out to be very useful for semantic representations too. We provide examples of feature structures in which a negation operator might be useful.
Learning Accurate Compact And Interpretable Tree Annotation We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank. Starting with a simple X-bar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals. In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2% on the Penn Treebank, higher than fully lexicalized systems. We use hierarchical EM training. We show that in the domain of syntactic parsing with probabilistic context-free grammars, automatically induced grammar refinements can outperform sophisticated methods which exploit substantial manually articulated structure. We introduce split-merge-smooth estimation.
The Grammar Matrix: An Open-Source Starter-Kit For The Rapid Development Of Cross-Linguistically Consistent Broad-Coverage Precision Grammars The grammar matrix is an open-source starter-kit for the development of broad-coverage HPSGs. By using a type hierarchy to represent cross-linguistic generalizations and providing compatibility with other open-source tools for grammar engineering, evaluation, parsing and generation, it facilitates not only quick start-up but also rapid growth towards the wide coverage necessary for robust natural language processing and the precision parses and semantic representations necessary for natural language understanding. Our LinGO Grammar Matrix project is both a repository of reusable linguistic knowledge and a method of delivering this knowledge to a user in the form of an extensible precision implemented grammar.
Noun-Phrase Co-occurrence Statistics for Semi-Automatic Semantic Lexicon Construction Generating semantic lexicons semi-automatically could be a great time saver, relative to creating them by hand. In this paper, we present an algorithm for extracting potential entries for a category from an online corpus, based upon a small set of exemplars. Our algorithm finds more correct terms and fewer incorrect ones than previous work in this area. Additionally, the entries that are generated potentially provide broader coverage of the category than would occur to an individual coding them by hand. Our algorithm finds many terms not included within Wordnet (many more than previous algorithms), and could be viewed as an "enhancer" of existing broad-coverage resources. We use co-occurrence statistics in local context to discover sibling relations. Our experiments were performed using the MUC-4 and Wall Street Journal corpuses (about 30 million words). To select seed words we rank all of the head nouns in the training corpus by frequency and manually select the first 10 nouns that unambiguously belong to each category. We find that 3 of every 5 words learned by our system are not present in WordNet.
Estimation of Conditional Probabilities With Decision Trees and an Application to Fine-Grained POS Tagging We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state-of-the-art POS taggers. Our fine-grained tag set contains approximately 800 tags.
SemEval-2007 Task 10: English Lexical Substitution Task In this paper we describe the English Lexical Substitution task for SemEval. In the task, annotators and systems find an alternative substitute word or phrase for a target word in context. The task involves both finding the synonyms and disambiguating the context. Participating systems are free to use any lexical resource. There is a subtask which requires identifying cases where the word is functioning as part of a multiword in the sentence and detecting what that multiword is. In the lexical substitution task, a system attempts to generate a word (or a set of words) to replace a target word, such that the meaning of the sentence is preserved. We establish a benchmark for context-sensitive lexical similarity models.
Learning Extraction Patterns For Subjective Expressions This paper presents a bootstrapping process that learns linguistically rich extraction patterns for subjective (opinionated) expressions. High-precision classifiers label unannotated data to automatically create a large training set, which is then given to an extraction pattern learning algorithm. The learned patterns are then used to identify more subjective sentences. The bootstrapping process learns many subjective patterns and increases recall while maintaining high precision. We construct a high precision classifier for contiguous sentences using the number of strong and weak subjective words in current and nearby sentences. We introduce a bootstrapping method to learn subjective extraction patterns that match specific syntactic templates using a high-precision sentence-level subjectivity classifier and a large unannotated corpus.
Incorporating Non-Local Information Into Information Extraction Systems By Gibbs Sampling Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks.
Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation This paper presents the results of the WMT10 and Metrics MATR10 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 104 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 26 metrics. This year we also investigated increasing the number of human judgments by hiring non-expert annotators through Amazon's Mechanical Turk. We release the News test set in the 2010 Workshop on Statistical Machine Translation.
Evaluating Translational Correspondence Using Annotation Projection Recently, statistical machine translation models have begun to take advantage of higher level linguistic structures such as syntactic dependencies. Underlying these models is an assumption about the directness of translational correspondence between sentences in the two languages; however, the extent to which this assumption is valid and useful is not well understood. In this paper, we present an empirical study that quantifies the degree to which syntactic dependencies are preserved when parses are projected directly from English to Chinese. Our results show that although the direct correspondence assumption is often too restrictive, a small set of principled, elementary linguistic transformations can boost the quality of the projected Chinese parses by 76% relative to the unimproved baseline. The dependency projection method DPA (Hwa et al, 2005) based on Direct Correspondence Assumption (Hwa et al, 2002) can be described as: if there is a pair of source words with a dependency relationship, the corresponding aligned words in target sentence can be considered as having the same dependency relationship equivalently. We align the parallel sentences using phrase based statistical MT models and then projected the alignments back to the parse trees.
On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing This paper introduces dual decomposition as a framework for deriving inference algorithms for NLP problems. The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. The approach provably solves a linear programming (LP) relaxation of the global inference problem. It leads to algorithms that are simple, in that they use existing decoding algorithms; efficient, in that they avoid exact algorithms for the full model; and often exact, in that empirically they often recover the correct solution in spite of using an LP relaxation. We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger. We use the highest scoring output of the parsing submodel over all iterations.
ISP: Learning Inferential Selectional Preferences Semantic inference is a key component for advanced natural language understanding. However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering. This paper presents ISP, a collection of methods for automatically learning admissible argument values to which an inference rule can be applied, which we call inferential selectional preferences, and methods for filtering out incorrect inferences. We evaluate ISP and present empirical evidence of its effectiveness. Context-sensitive extensions of DIRT focus on making DIRT rules context-sensitive by attaching appropriate semantic classes to the X and Y slots of an inference rule. We build a set of semantic classes using WordNet in one case and CBC clustering algorithm in the other; for each rule, we use the overlap of the fillers found in the input corpus as an indicator of the correct semantic classes. We augment each relation with its selectional preferences, i.e. fine-grained entity types of two arguments, to handle polysemy.
A Stochastic Japanese Morphological Analyzer Using A Forward-DP Backward-A* N-Best Search Algorithm We present a novel method for segmenting the input sentence into words and assigning parts of speech to the words. It consists of a statistical language model and an efficient two-pass N-best search algorithm. The algorithm does not require delimiters between words. Thus it is suitable for written Japanese. The proposed Japanese morphological analyzer achieved 95.l% recall and 94.6% precision for open text when it was trained and tested on the ATR Corpus. We propose a method to search for the N best sets.
Motivations And Methods For Text Simplification Long and complicated sentences prove to be a stumbling block for current systems relying on NL input. These systems stand to gain from methods that syntactically simplify such sentences. To simplify a sentence, we need an idea of the structure of the sentence, to identify the components to be separated out. Obviously a parser could be used to obtain the complete structure of the sentence. However, full parsing is slow prone to failure, especially on complex sentences. In this paper, we consider two alternatives to full parsing which could be used for simplification. The first approach uses a Finite State Grammar (FSG) to produce noun and verb groups while the second uses a Supertagging model to produce dependency linkages. We discuss the impact of these two input representations on the sim-plification process. We introduce a two stage process, first transforming from sentence to syntactic tree, then from syntactic tree to new sentence. Our text simplification techniques deal not only with helping readers with reading disabilities, but also to help NLP systems as a preprocessing tool.
Summarizing Scientific Articles: Experiments With Relevance And Rhetorical Status In this article we propose a strategy for the summarization of scientific articles that concentrates on the rhetorical status of statements in an article: Material for summaries is selected in such a way that summaries can highlight the new contribution of the source article and situate it with respect to earlier work. We provide a gold standard for summaries of this kind consisting of a substantial corpus of conference articles in computational linguistics annotated with human judgments of the rhetorical status and relevance of each sentence in the articles. We present several experiments measuring our judges’ agreement on these annotations. We also present an algorithm that, on the basis of the annotated training material, selects content from unseen articles and classifies it into a fixed set of seven rhetorical categories. The output of this extraction and classification system can be viewed as a single-document summary in its own right; alternatively, it provides starting material for the generation of task-oriented and user-tailored summaries designed to give users an overview of a scientific field. We examine the problem of summarizing scientific articles using rhetorical analysis of sentences. We summarize scientific articles by selecting rhetorical elements that are commonly present in scientific abstracts.
Findings of the 2009 Workshop on Statistical Machine Translation This paper presents the results of the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 87 machine translation systems and 22 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics. We present a new evaluation technique whereby system output is edited and judged for correctness. Our Fr-En 109 corpus aggregates huge numbers of parallel French English sentences from the web. We show that the performance of corpus-based statistical machine translation (SMT) has come up to the traditional rule-based method.
Detecting A Continuum Of Compositionality In Phrasal Verbs We investigate the use of an automatically acquired thesaurus for measures designed to indicate the compositionality of candidate multiword verbs, specifically English phrasal verbs identified automatically using a robust parser. We examine various measures using the nearest neighbours of the phrasal verb, and in some cases the neighbours of the simplex counterpart and show that some of these correlate significantly with human rankings of compositionality on the test set. We also show that whilst the compositionality judgements correlate with some statistics commonly used for extracting multiwords, the relationship is not as strong as that using the automatically constructed thesaurus.
A Discriminative Framework For Bilingual Word Alignment Bilingual word alignment forms the foundation of most approaches to statistical machine translation. Current word alignment methods are predominantly based on generative models. In this paper, we demonstrate a discriminative approach to training simple word alignment models that are comparable in accuracy to the more complex generative models normally used. These models have the the advantages that they are easy to add features to and they allow fast optimization of model parameters using small amounts of annotated data. LLR can still be used for extracting positive associations by filtering in a pre-processing step words with possibly negative associations. We train two models we call stage 1 and stage 2, both in the form of a weighted linear combination of feature values extracted from a pair of sentences and a proposed word alignment of them. We use statistics like log-likelihood-ratio and conditional likelihood-probability to measure word associations.
Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion Letter-to-phoneme conversion generally requires aligned training data of letters and phonemes. Typically, the alignments are limited to one-to-one alignments. We present a novel technique of training with many-to-many alignments. A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists. We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word. The many-to-many alignments result in significant improvements over the traditional one-to-one approach. Our system achieves state-of-the-art performance on several languages and data sets. The M2M-aligner is based on the expectation maximization (EM) algorithm. M2M-aligner is a many-to-many (M-M) alignment algorithm based on EM that allows for mapping of multiple letters to multiple phonemes.
Practical Very Large Scale CRFs Conditional Random Fields (CRFs) are a widely-used approach for supervised sequence labelling, notably due to their ability to handle large description spaces and to integrate structural dependency between labels. Even for the simple linear-chain model, taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set. In this paper, we address the issue of training very large CRFs, containing up to hundreds output labels and several billion features. Efficiency stems here from the sparsity induced by the use of a lscript1 penalty term. Based on our own implementation, we compare three recent proposals for implementing this regularization strategy. Our experiments demonstrate that very large CRFs can be trained efficiently and that very large models are able to improve the accuracy, while delivering compact parameter sets.
New Figures Of Merit For Best-First Probabilistic Chart Parsing Best-first parsing methods for natural language try to parse efficiently by considering the most likely constituents first. Some figure of merit is needed by which to compare the likelihood of constituents, and the choice of this figure has a substantial impact on the efficiency of the parser. While several parsers described in the literature have used such techniques, there is little published data on their efficacy, much less attempts to judge their relative merits. We propose and evaluate several figures of merit for best-first parsing, and we identify an easily computable figure of merit that provides excellent performance on various measures and two different grammars. We present best-first parsing with Figures of Merit that allows conditioning of the heuristic function on statistics of the input string.
Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora A significant portion of the world's text is tagged by readers on social bookmarking websites. Credit attribution is an inherent problem in these corpora because most pages have multiple tags, but the tags do not always apply with equal specificity across the whole document. Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa. This paper introduces Labeled LDA, a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA's latent topics and user tags. This allows Labeled LDA to directly learn word-tag correspondences. We demonstrate Labeled LDA's improved expressiveness over traditional LDA with visualizations of a corpus of tagged web pages from del.icio.us. Labeled LDA outperforms SVMs by more than 3 to 1 when extracting tag-specific document snippets. As a multi-label text classifier, our model is competitive with a discriminative baseline on a variety of datasets. L-LDA extends standard LDA to include supervision for specific target categories, and the generative process includes a second observed variable, i.e. each document is explicitly labeled with a target category.
Introduction To The Special Issue On Computational Linguistics Using Large Corpora A historical account of this empirical renaissance is provide in this work. Much recent research in the field of natural language processing (NLP) has focused on an empirical, corpus-based approach.
Paraphrasing With Bilingual Parallel Corpora Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrase-based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments. We define a paraphrasing probability between two phrases based on their translation probability through all possible pivot phrases.
HMM-Based Word Alignment In Statistical Translation In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora. We use a useful feature that assesses the goodness of the alignment path through the source sentence.
A Gibbs Sampler for Phrasal Synchronous Grammar Induction We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches. We use Gibbs sampler for learning the SCFG by reasoning over the space of derivations (Blunsom et al, 2009). We present a method for maintaining table counts without needing to record the table assignments for each translation decision. We apply the technique of using multiple processors to perform approximate Gibbs sampling which we show achieve equivalent performance to the exact Gibbs sampler.
Automatic Acquisition Of Subcategorization Frames From Untagged Text This paper describes an implemented program that takes a raw, untagged text corpus as its only input (no open-class dictionary) and generates a partial list of verbs occurring in the text and the subcategorization frames (SFs) in which they occur. Verbs are detected by a novel technique based on the Case Filter of Rouvret and Vergnaud (1980). The completeness of the output list increases monotonically with the total number of occurrences of each verb in the corpus. False positive rates are one to three percent of observations. Five SFs are currently detected and more are planned. Ultimately, I expect to provide a large SF dictionary to the NLP community and to train dictionaries for specific corpora.
TextTiling: Segmenting Text Into Multi-Paragraph Subtopic Passages TextTiling is a technique for subdividing texts into multi-paragraph units that represent passages, or subtopics. The discourse cues for identifying major subtopic shifts are patterns of lexical co-occurrence and distribution. The algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts. Multi-paragraph subtopic segmentation should be useful for many text analysis tasks, including information retrieval and summarization. We compute chance agreement in terms of the probability that coders would say that a segment boundary exists (segt), and the probability that they would not (unsegt).
Integrating Multiple Knowledge Sources For Detection And Correction Of Repairs In Human-Computer Dialog We have analyzed 607 sentences of spontaneous human-computer speech data containing repairs, drawn from a total corpus of 10,718 sentences. We present here criteria and techniques for automatically detecting the presence of a repair, its location, and making the appropriate correction. The criteria involve integration of knowledge from several sources: pattern matching, syntactic and semantic analysis, and acoustics. We are able to correctly identify 309 of 406 utterences containing nontrivial repairs, while 191 fluent utterances were incorrectly identified as containing repairs. We speculate that acoustic information might be used to filter out false positives for canditate matching. We find location of word fragments to be an invaluable cue to both the detection and correction of disfluencies.
Identifying Relations for Open Information Extraction Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary. This paper shows that the output of state-of-the-art Open IE systems is rife with uninformative and incoherent extractions. To overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs. We implemented the constraints in the REVERB Open IE system, which more than doubles the area under the precision-recall curve relative to previous extractors such as TEXTRUNNER and WOEpos. More than 30% of REVERB’s extractions are at precision 0.8 or higher compared to virtually none for earlier systems. The paper concludes with a detailed analysis of REVERB’s errors, suggesting directions for future work. We show that verbal phrases uncover a large fraction of binary predicates while reducing the amount of noisy phrases that do not denote any relations. We develop a large scale web-based ReVerb corpus, comprising tuple extractions of predicate templates with their argument instantiations. Our ReVerb corpus is a large scale publicly available web based open extractions data set, containing about 15 million unique template extractions, automatically extracted from the ClueWeb09 web crawl.
Semi-Supervised Training For Statistical Word Alignment We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus. We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality. If human-aligned data is available, the EMD algorithm provides higher baseline alignments than GIZA++ that have led to better MT performance. We combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. We pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. We propose an EMD algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models.
Re-Evaluation The Role Of Bleu In Machine Translation Research We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric. We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu's correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores. The problems of Blue include: (1) synonyms and paraphrases are only handled if they are in the set of multiple reference translations [available]; (2) The scores for words are equally weighted so missing out on content-bearing material brings no additional penalty; (3) The brevity penalty is a stop-gap measure to compensate for the fairly serious problem of not being able to calculate recall. Blue has certain shortcomings for comparing different machine translation systems, especially if comparing conceptually different systems, e.g. phrase-based versus rule-based systems. We find that BLEU and NIST favour n-gram-based MT models such as Pharaoh (Koehn, 2004), so the translations produced by rule-based systems score lower on the automatic evaluation, even though human judges consistently rate their output higher than Pharaoh's translation.
Efficient Feature-based Conditional Random Field Parsing Discriminative feature-based methods are widely used in natural language processing, but sentence parsing is still dominated by generative methods. While prior feature-based dynamic programming parsers have restricted training and evaluation to artificially short sentences, we present the first general, feature rich discriminative parser, based on a conditional random field model, which has been successfully scaled to the full WSJ parsing data. Our efficiency is primarily due to the use of stochastic optimization techniques, as well as parallelization and chart prefiltering. On WSJ15, we attain a state-of-the-art F-score of 90.9%, a 14% relative reduction in error over previous models, while being two orders of magnitude faster. On sentences of length 40, our system achieves an F-score of 89.0%, a 36% relative reduction in error over a generative baseline. In our model, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al, 2008).
Parsing with Compositional Vector Grammars Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%. It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments. Recursive neural networks, which have the ability to generate a tree structured output, have already been applied to natural language parsing, we extended them to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al, 2013).
Joint Learning Improves Semantic Role Labeling Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding. This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments. We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative log-linear models. This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a state-of-the art independent classifier for gold-standard parse trees on PropBank. We introduce a joint approach for SRL and demonstrate that a model that scores the full predicate argument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. We employ decomposition for efficiency in training: that is, the decomposition allows us to train the classification models on a subset of training examples consisting only of those phrases that have a case marker.
Text-Translation Alignment We present an algorithm for aligning texts with their translations that is based only on internal evidence. The relaxation process rests on a notion of which word in one text corresponds to which word in the other text that is essentially based on the similarity of their distributions. It exploits a partial alignment of the word level to induce a maximum likelihood alignment of the sentence level, which is in turn used, in the next iteration, to refine the word level estimate. The algorithm appears to converge to the correct sentence alignment in only a few iterations. Our morphology algorithm is applied for splitting potential suffixes and prefixes and for obtaining the normalised word forms.
Driving Semantic Parsing from the World&rsquo;s Response Current approaches to semantic parsing, the task of converting text to a formal meaning representation, rely on annotated training data mapping sentences to logical forms. Providing this supervision is a major bottleneck in scaling semantic parsers. This paper presents a new learning paradigm aimed at alleviating the supervision burden. We develop two novel learning algorithms capable of predicting complex structures which only rely on a binary feedback signal based on the context of an external world. In addition we reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers. We train systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers.
Semiring Parsing We synthesize work on parsing algorithms, deductive parsing, and the theory of algebra applied to formal languages into a general system for describing parsers. Each parser performs abstract computations using the operations of a semiring. The system allows a single, simple representation to be used for describing parsers that compute recognition, derivation forests, Viterbi, n-best, inside values, and other values, simply by substituting the operations of different semirings. We also show how to use the same representation, interpreted differently, to compute outside values. The system can be used to describe a wide variety of parsers, including Earley's algorithm, tree adjoining grammar parsing, Graham Harrison Ruzzo parsing, and prefix value computation. We show how a parsing logic can be combined with various semirings to compute different kinds of information about the input. We augment such logic programs with semiring weights, giving an algebraic explanation for the intuitive connections among classes of algorithms with the same logical structure.
Training Tree Transducers Many probabilistic models for natural language are now written in terms of hierarchical tree structure. Tree-based modeling still lacks many of the standard tools taken for granted in (finite-state) string-based modeling. The theory of tree transducer automata provides a possible framework to draw on, as it has been worked out in an extensive literature. We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-to-tree and tree-to-string transducers. We define training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. We describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers.
GATE: A Framework And Graphical Development Environment For Robust NLP Tools And Applications In this paper we present GATE, a framework and graphical development environment which enables users to develop and deploy language engineering components and resources in a robust fashion. The GATE architecture has enabled us not only to develop a number of successful applications for various language processing tasks (such as Information Extraction), but also to build and annotate corpora and carry out evaluations on the applications generated. The framework can be used to develop applications and resources in multiple languages, based on its thorough Unicode support. We include the ANNIE IE system in the standard GATE distribution for text tokenization, sentence splitting and part-of-speech tagging. We propose mechanisms to help heterogeneous linguistic modules to communicate through a common XML interface.
Learning the Scope of Hedge Cues in Biomedical Texts Identifying hedged information in biomedical literature is an important subtask in information extraction because it would be misleading to extract speculative information as factual information. In this paper we present a machine learning system that finds the scope of hedge cues in biomedical texts. The system is based on a similar system that finds the scope of negation cues. We show that the same scope finding approach can be applied to both negation and hedging. To investigate the robustness of the approach, the system is tested on the three subcorpora of the BioScope corpus that represent different text types. We develop a scope detector following a supervised sequence labeling approach. We present a meta-learning system that finds the scope of hedge cues in biomedical texts. We use shallow syntactic features.
Learning Dependency Translation Models As Collections Of Finite-State Head Transducers The paper defines weighted head transducers, finite-state machines that perform middle-outstring transduction. These transducers are strictly more expressive than the special case of standard left-to-right finite-state transducers. Dependency transduction models are then defined as collections of weighted head transducers that are applied hierarchically. A dynamic programming search algorithm is described for finding the optimal transduction of an input string with respect to a dependency transduction model. A method for automatically training a dependency transduction model from a set of input-output example strings is presented. The method first searches for hierarchical alignments of the training examples guided by correlation statistics, and then constructs the transitions of head transducers that are consistent with these alignments. Experimental results are given for applying the training method to translation from English to Spanish and Japanese. We treat translation as a process of simultaneous induction of source and target dependency trees using head transduction. We present a two-level arranged word ordering and chunk ordering by a hierarchically organized collection of finite state transducers. We induce parallel tree structures from unbracketed parallel text, modeling the generation of each node's children with a finite-state transducer.
Multilingual Dependency Analysis With A Two-Stage Discriminative Parser We present a two-stage multilingual dependency parser and evaluate it on 13 diverse languages. The first stage is based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages. The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph. We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis. We use post-processing for non-projective dependencies and for labeling. We treat the labeling of dependencies as a sequence labeling problem. The specific graph-based model studied in this work factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc.
Estimators For Stochastic Unification-Based Grammars Log-linear models provide a statistically sound framework for Stochastic "Unification-Based" Grammars (SUBGs) and stochastic versions of other kinds of grammars. We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical-Functional Grammar. We incorporate general linguistic principles into a log-linear model. We use parses generated by a LFG parser as input to a MRF approach.
Speed And Accuracy In Shallow And Deep Stochastic Parsing This paper reports some experiments that compare the accuracy and performance of two stochastic parsing systems. The currently popular Collins parser is a shallow parser whose output contains more detailed semantically relevant information than other such parsers. The XLE parser is a deep-parsing system that couples a Lexical Functional Grammar to a log-linear disambiguation component and provides much richer representations theory. We measured the accuracy of both systems against a gold standard of the PARC 700 dependency bank, and also measured their processing times. We report high parsing speeds for a deep parsing system which uses an LFG grammar: 1.9 sentences per second for 560 sentences from section 23 of the Penn Treebank.
Precision And Recall Of Machine Translation Machine translation can be evaluated using precision, recall, and the F-measure. These standard measures have significantly higher correlation with human judgments than recently proposed alternatives. More importantly, the standard measures have an intuitive interpretation, which can facilitate insights into how MT systems might be improved. The relevant software is publicly available. We formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty.
Experiments with a Higher-Order Projective Dependency Parser We present experiments with a dependency parsing model defined on rich factors. Our model represents dependency trees with factors that include three types of relations between the tokens of a dependency and their children. We extend the projective parsing algorithm of Eisner (1996) for our case, and train models using the averaged perceptron. Our experiments show that considering higher-order information yields significant improvements in parsing accuracy, but comes at a high cost in terms of both time and memory consumption. In the multilingual exercise of the CoNLL-2007 shared task (Nivre et al., 2007), our system obtains the best accuracy for English, and the second best accuracies for Basque and Czech. We extend the first-order model to incorporate a sum over scores for pairs of adjacent arcs in the tree, yielding a second-order model. Our second-order models include head grandparent relations. Our second order algorithm uses the child of the dependent occurring in the sentence between the head and the dependent as well as the edge from the dependents to a grandchild. We introduce the left-most and right-most grandchild as factors.
Role of Verbs in Document Analysis We present results of two methods for assessing the event profile of news articles as a function of verb type. The unique contribution of this research is the focus on the role of verbs, rather than nouns. Two algorithms are presented and evaluated, one of which is shown to accurately discriminate documents by type and semantic properties, i.e. the event profile. The initial method, using WordNet (Miller et al. 1990), produced multiple cross-classification of articles, primarily due to the bushy nature of the verb tree coupled with the sense disambiguation problem. Our second approach using English Verb Classes and Alternations (EVCA) Levin (1993) showed that monosemous categorization of the frequent verbs in WSJ made it possible to usefully discriminate documents. For example, our results show that articles in which communication verbs predominate tend to be opinion pieces, whereas articles with a high percentage of agreement verbs tend to be about mergers or legal cases. An evaluation is performed on the results using Kendall's tau. We present convincing evidence for using verb semantic classes as a discriminant in document classification. We demonstrate that document type is correlated with the presence of many verbs of a certain EVCA class.
A Linear Programming Formulation For Global Inference In Natural Language Tasks Given a collection of discrete random variables representing outcomes of learned local predictors in natural language, e.g., named entities and relations, we seek an optimal global assignment to the variables in the presence of general (non-sequential) constraints. Examples of these constraints include the type of arguments a relation can take, and the mutual activity of different relations, etc. We develop a linear programming formulation for this problem and evaluate it in the context of simultaneously learning named entities and relations. Our approach allows us to efficiently incorporate domain and task specific constraints at decision time, resulting in significant improvements in the accuracy and the human-like quality of the inferences. we use ILP to deal with the joint inference problem of named entity and relation identification. we applied an ILP model to the task of the simultaneous assignment of semantic roles to the entities mentioned in a sentence and recognition of the relations holding between them. we described a classification-based framework in which they jointly learn to identify named entities and relations.
Sentence Level Discourse Parsing Using Syntactic And Lexical Information We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees. The models use syntactic and lexical features. A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-of-the-art decision-based discourse parser. A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance. within Rhetorical Structure Theory (RST), we have developed two probabilistic models for identifying clausal elementary discourse units and generating discourse trees at the sentence level. We introduce a statistical discourse segmenter, which is trained on RST DT to label words with boundary or no-boundary labels.
Detection of Grammatical Errors Involving Prepositions This paper presents ongoing work on the detection of preposition errors of non-native speakers of English. Since prepositions account for a substantial proportion of all grammatical errors by ESL (English as a Second Language) learners, developing an NLP application that can reliably detect these types of errors will provide an invaluable learning resource to ESL students. To address this problem, we use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays. Although our work is preliminary, we achieve a precision of 0.8 with a recall of 0.3. Chodorow et al (2007) employed a maximum entropy model to estimate the probability of 34 prepositions based on 25 local context features ranging from words to NP/VP chunks. A context is represented by 25 lexical features and 4 combination features: Lexical Token and POS n-grams in a 2 word window around the preposition, plus the head verb in the preceding verb phrase (PV), the head noun in the preceding noun phrase (PN) and the head noun in the following noun phrase (FN) when available.
Categorial Unification Grammars Categorial unification grammars (CUGs) embody the essential properties of both unification and categorial grammar formalisms. Their efficient and uniform way of encoding linguistic knowledge in well-understood and widely used representations makes them attractive for computational applications and for linguistic research. In this paper, the basic concepts of CUGs and simple examples of their application will be presented. It will be argued that the strategies and potentials of CUGs justify their further exploration in the wider context of research on unification grammars. Approaches to selected linguistic phenomena such as long-distance dependencies, adjuncts, word order, and extraposition are discussed.
A Language Modeling Approach To Predicting Reading Difficulty We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling. We derive a measure based on an extension of multinomial naive Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage. The resulting classifier is not specific to any particular subject and can be trained with relatively little labeled data. We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures. We show that with minimal changes, the classifier may be retrained for use with French Web documents. For both English and French, the classifier maintains consistently good correlation with labeled grade level (0.63 to 0.79) across all test sets. Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words). We use a smoothed unigram language model to predict the grade reading levels of web page documents and short passages.
Word Sense Discrimination By Clustering Contexts In Vector And Similarity Spaces This paper systematically compares unsupervised word sense discrimination techniques that cluster instances of a target word that occur in raw text using both vector and similarity spaces. The context of each instance is represented as a vector in a high dimensional feature space. Discrimination is achieved by clustering these context vectors directly in vector space and also by finding pairwise similarities among the vectors and then clustering in similarity space. We employ two different representations of the context in which a target word occurs. First order context vectors represent the context of each instance of a target word as a vector of features that occur in that context. Second order context vectors are an indirect representation of the context based on the average of vectors that represent the words that occur in the context. We evaluate the discriminated clusters by carrying out experiments using sense–tagged instances of 24 SENSEVAL2 words and the well known Line, Hard and Serve sense–tagged corpora.
Applied Text Generation We divide tasks in the generation process into three stages: the text planner has access only to information about communicative goals, the discourse context, and semantics, and generates a non-linguistic representation of text structure and content. The sentence planner chooses abstract linguistic resources. It passes an abstract lexico-syntactic specification5 to the Realizer, which inflects, adds function words, and linearizes, thus producing the surface string.
Automatic Identification Of Word Translations From Unrelated English And German Corpora Algorithms for the alignment of words in translated texts are well established. However, only recently new approaches have been proposed to identify word translations from non-parallel or even unrelated texts. This task is more difficult, because most statistical clues useful in the processing of parallel texts cannot be applied to non-parallel texts. Whereas for parallel texts in some studies up to 99% of the word alignments have been shown to be correct, the accuracy for non-parallel texts has been around 30% up to now. The current study, which is based on the assumption that there is a correlation between the patterns of word co-occurrences in corpora of different languages, makes a significant improvement to about 72% of word translations identified correctly. We create bag-of-words context vectors around both the source and target language words and then project the source into the English target space via the current small translation dictionary. We filter out bilingual term pairs with low monolingual frequencies (those below 100 times). We show that accurate translations can be learned for 100 German nouns that are not contained in the seed bilingual dictionary.
Extracting Parallel Sub-Sentential Fragments From Non-Parallel Corpora We present a novel method for extracting parallel sub-sentential fragments from comparable, non-parallel bilingual corpora. By analyzing potentially similar sentence pairs using a signal processing-inspired approach, we detect which segments of the source sentence are translated into segments in the target sentence, and which are not. This method enables us to extract useful machine translation training data even from very non-parallel corpora, which contain no parallel sentence pairs. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. We first use the GI ZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. We first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. We use standard information retrieval together with simple word-based translation for cross-lingual information retrieval (CLIR), and extract phrases from the retrieval results using a clean bilingual lexicon and an averaging filter. We perform phrase extraction by combining clean alignment lexica for initial signals with heuristics to smooth alignments for final fragment extraction.
Web-Scale Distributional Similarity and Entity Set Expansion Computing the pairwise semantic similarity between all words on the Web is a computationally challenging task. Parallelization and optimizations are necessary. We propose a highly scalable implementation based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web. The pairwise similarity between 500 million terms is computed in 50 hours using 200 quadcore nodes. We apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size, corpus quality, seed composition and seed size. We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia. Our DASH stores the case for each phrase in Wikipedia. We find that 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found. Given the seeds set S, a seeds centroid vector is produced using the surrounding word contexts of all occurrences of all the seeds in the corpus.
Improving Machine Learning Approaches To Coreference Resolution We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC-6 and MUC-7 coreference resolution data sets - F-measures of 70.4 and 63.4, respectively. Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge. In the testing phase, we used the best-first clustering. We expand the feature set of Soon et al (2001) from 12 to 53 features. We propose a rule-induction system with rule pruning.
Minimum Bayes-Risk Decoding For Statistical Machine Translation We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation. This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance. We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences. We report the performance of the MBR decoders on a Chinese-to-English translation task. Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions. The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system's translations relative to the model's distribution over possible translations. In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an N best list of translations produced by a first pass decoder.
Identifying Word Translations In Non-Parallel Texts Common algorithms for sentence and word-alignment allow the automatic identification of word translations from parallel texts. This study suggests that the identification of word translations should also be possible with non-parallel and even unrelated texts. The method proposed is based on the assumption that there is a correlation between the patterns of word co-occurrences in texts of different languages. We propose a computationally demanding matrix purmutation method which maximizes a similarity between co-occurence matrices in two languages. An underlying assumption in our work is that translations of words that are related in one language are also related in the other language.
Use Of Support Vector Learning For Chunk Identification
Comparing Automatic And Human Evaluation Of NLG Systems We consider the evaluation problem in Natural Language Generation (NLG) and present results for evaluating several NLG systems with similar functionality, including a knowledge-based generator and several statistical systems. We compare evaluation results for these systems by human domain experts, human non-experts, and several automatic evaluation metrics, including NIST, BLEU, and ROUGE. We find that NIST scores correlate best (> 0.8) with human judgments, but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone. We conclude that automatic evaluation of NLG systems has considerable potential, in particular where high-quality reference texts and only a small number of human evaluators are available. However, in general it is probably best for automatic evaluations to be supported by human-based evaluations, or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain. We use several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts. We demonstrate that automatic metrics can correlate highly with human ratings if the training dataset is of high quality. The two automatic metrics used in the evaluations, NIST and BLEU have been shown to correlate highly with expert judgments (Pearson correlation coefficients 0.82 and 0.79 respectively) in this domain.
A High-Performance Semi-Supervised Learning Method For Text Chunking In machine learning, whether one can build a more accurate classifier by using unlabeled data (semi-supervised learning) is an important issue. Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear. This paper presents a novel semi-supervised method that employs a learning paradigm which we call structural learning. The idea is to find “what good classifiers are like” by learning from thousands of automatically generated auxiliary classification problems on unlabeled data. By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem. The method produces performance higher than the previous best results on CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (English and German). We utilize a multi task learner within our semi-supervised algorithm to learn feature representations which are useful across a large number of related tasks. Our structural learning method uses alternating structural optimization (ASO). For both computational and statistical reasons, we follow compute a low-dimensional linear approximation to the pivot predictor space.
Bleu: A Method For Automatic Evaluation Of Machine Translation Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. BLEU is a system for automatic evaluation of machine translation. BLEU is based on measuring string level similarity between the reference translation and translation hypothesis.
D-PATR: A Development Environment For Unification-Based Grammars We describe systems in which FSs may be modified by default statements in such a way that this property does not automatically hold.
Part-Of-Speech Tagging In Context We present a new HMM tagger that exploits context on both sides of a word to be tagged, and evaluate it in both the unsupervised and supervised case. Along the way, we present the first comprehensive comparison of unsupervised methods for part-of-speech tagging, noting that published results to date have not been comparable across corpora or lexicons. Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable. Finally, we show how this new tagger achieves state-of-the-art results in a supervised, non-training intensive framework. While replicating earlier experiments, we discover that performance was highly dependent on cleaning tag dictionaries using statistics gleaned from the tokens. We show that he expectation maximization algorithm for bi tag HMMs is efficient and quite effective for acquiring accurate POS taggers given only a lexicon (tag dictionary) and certain favorable conditions. We observe that earlier unsupervised HMM-EM results were artificially high due to use of Optimized Lexicons, in which only frequent-enough analyses of each word were kept.
Functional Unification Grammar: A Formalism For Machine Translation Functional Unification Grammar provides an opportunity to encompass within one formalism and computational system the parts of machine translation systems that have usually been treated separately, natably analysis, transfer, and synthesis. Many of the advantages of this formalism come from the fact that it is monotonic allowing data structures to grow differently as different nondeterministic alternatives in a computation are pursued, but never to be modified in any way. A striking feature of this system is that it is fundamental reversible, allowing a to translate as b only if b could translate as a.
One Sense Per Discourse It is well-known that there are polysemous words like sentence whose "meaning" or "sense" depends on the context of use. We have recently reported on two new word-sense disambiguation systems, one trained on bilingual material (the Canadian Hansards) and the other trained on monolingual material (Roget's Thesaurus and Grolier's Encyclopedia). As this work was nearing completion, we observed a very strong discourse effect. That is, if a polysemous word such as sentence appears two or more times in a well-written discourse, it is extremely likely that they will all share the same sense. This paper describes an experiment which confirmed this hypothesis and found that the tendency to share sense in the same discourse is extremely strong (98%). This result can be used as an additional source of constraint for improving the performance of the word-sense disambiguation algorithm. In addition, it could also be used to help evaluate disambiguation algorithms that did not make use of the discourse constraint. We claim on the basis of corpus analysis that to a very large extent a word keeps the same meaning throughout a text.
Extracting Relations With Integrated Information Using Kernel Methods Entity relation detection is a form of information extraction that finds predefined relations between pairs of entities in text. This paper describes a relation detection approach that combines clues from different levels of syntactic processing using kernel methods. Information from three different levels of processing is considered: tokenization, sentence parsing and deep dependency analysis. Each source of information is represented by kernel functions. Then composite kernels are developed to integrate and extend individual kernels so that processing errors occurring at one level can be overcome by information from other levels. We present an evaluation of these methods on the 2004 ACE relation detection task, using Support Vector Machines, and show that each level of syntactic processing contributes useful information for this task. When evaluated on the official test data, our approach produced very competitive ACE value scores. We also compare the SVM with KNN on different kernels. We define several feature based composite kernels to integrate diverse features for relation extraction and achieved the F-measure of 70.4 on the 7 relation types of the ACE RDC 2004 corpus. We show that adding local information to deep syntactic information improved IE results. We extract bigram of the words between the two mentions, aiming to provide more order information of the tokens between the two mentions.
Batch Tuning Strategies for Statistical Machine Translation There has been a proliferation of recent work on SMT tuning algorithms capable of handling larger feature sets than the traditional MERT approach. We analyze a number of these algorithms in terms of their sentence-level loss functions, which motivates several new approaches, including a Structured SVM. We perform empirical comparisons of eight different tuning strategies, including MERT, in a variety of settings. Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options.
An Unsupervised Approach To Recognizing Discourse Relations We present an unsupervised approach to recognizing discourse relations of CONTRAST, EXPLANATION-EVIDENCE, CONDITION and ELABORATION that hold between arbitrary spans of texts. We show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases. We use a pattern based approach to extract instances of discourse relations such as Contrast and Elaboration from unlabeled corpora. We propose a method to identify discourse relations between text segments using Naive Bayes classifiers trained on a huge corpus.
A Graph Model For Unsupervised Lexical Acquisition This paper presents an unsupervised method for assembling semantic knowledge from a part-of-speech tagged corpus using graph algorithms. The graph model is built by linking pairs of words which participate in particular syntactic relationships. We focus on the symmetric relationship between pairs of nouns which occur together in lists. An incremental cluster-building algorithm using this part of the graph achieves 82% accuracy at a lexical acquisition task, evaluated against WordNet classes. The model naturally realises domain and corpus specific ambiguities as distinct components in the graph surrounding an ambiguous word. We try to find graph regions that are more connected internally than externally.
Improving A Statistical MT System With Automatically Learned Rewrite Patterns Current clump-based statistical MT systems have two limitations with respect to word ordering: First, they lack a mechanism for expressing and using generalization that accounts for reorderings of linguistic phrases. Second, the ordering of target words in such systems does not respect linguistic phrase boundaries. To address these limitations, we propose to use automatically learned rewrite patterns to preprocess the source sentences so that they have a word order similar to that of the target langauge. Our system is a hybrid one. The basic model is statistical, but we use broad-coverage rule-based parsers in two ways - during training for learning rewrite patterns, and at runtime for reordering the source sentences. Our experiments show 10% relative improvement in Bleu measure. We describe an approach for translation from French to English, where reordering rules are acquired automatically. Our re-ordering rules are automatically learned from aligning parse trees for both the source and target sentences.
Determining Term Subjectivity And Term Orientation For Opinion Mining Opinion mining is a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses. To aid the extraction of opinions from text, recent work has tackled the issue of determining the orientation of “subjective” terms contained in text, i.e. deciding whether a term that carries opinionated content has a positive or a negative connotation. This is believed to be of key importance for identifying the orientation of documents, i.e. determining whether a document expresses a positive or negative opinion about its subject matter. We contend that the plain determination of the orientation of terms is not a realistic problem, since it starts from the nonrealistic assumption that we already know whether a term is subjective or not; this would imply that a linguistic resource that marks terms as “subjective” or “objective” is available, which is usually not the case. In this paper we confront the task of deciding whether a given term has a positive connotation, or a negative connotation, or has no subjective connotation at all; this problem thus subsumes the problem of determining subjectivity and the problem of determining orientation. We tackle this problem by testing three different variants of a semi-supervised method previously proposed for orientation detection. Our results show that determining subjectivity and orientation is a much harder problem than determining orientation alone.
Using WordNet-Based Context Vectors To Estimate The Semantic Relatedness Of Concepts In this paper, we introduce a WordNet-based measure of semantic relatedness by combining the structure and content of WordNet with co–occurrence information derived from raw text. We use the co–occurrence information along with the WordNet definitions to build gloss vectors corresponding to each concept in WordNet. Numeric scores of relatedness are assigned to a pair of concepts by measuring the cosine of the angle between their respective gloss vectors. We show that this measure compares favorably to other measures with respect to human judgments of semantic relatedness, and that it performs well when used in a word sense disambiguation algorithm that relies on semantic relatedness. This measure is flexible in that it can make comparisons between any two concepts without regard to their part of speech. In addition, it can be adapted to different domains, since any plain text corpus can be used to derive the co–occurrence information. We create aggregate co-occurrence vectors for a WordNet sense by adding the co-occurrence vectors of the words in its WordNet gloss. We introduce a vector measure to determine the relatedness between pairs of concepts.
Structural Ambiguity And Lexical Relations We propose that ambiguous prepositional phrase attachment can be resolved on the basis of the relative strength of association of the preposition with noun and verb, estimated on the basis of word distribution in a large corpus. This work suggests that a distributional approach can be effective in resolving parsing problems that apparently call for complex reasoning. We find that human arbiters constitently reach a higher agreement in propositional phrase attachment when they are given the entire sentence rather than just the four words concerned.
Chinese Segmentation with a Word-Based Perceptron Algorithm Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary. Discriminatively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding finding the highest scoring segmentation. In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences. The generalized perceptron algorithm is used for discriminative training, and we use a beam-search decoder. Closed tests on the first and second SIGHAN bakeoffs show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora. We also provide a feature template for Chinese word segmentation.
A Statistical Approach To The Semantics Of Verb-Particles This paper describes a distributional approach to the semantics of verb-particle constructions (e.g. put up, make off). We report first on a framework for implementing and evaluating such models. We then go on to report on the implementation of some techniques for using statistical models acquired from corpus data to infer the meaning of verb-particle constructions.
A New Statistical Parser Based On Bigram Lexical Dependencies This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy. We make use of a backed-off smoothing technique to alleviate sparse data problems.
Personalizing PageRank for Word Sense Disambiguation In this paper we propose a new graph-based method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation. Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets. We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet. In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster. We propose Personalized PageRank (PPR) that tries to trade-off between the amount of the employed lexical information and the overall efficiency. We initialize the ranks of the vertex at a uniform value (usually 1/N for a graph with N vertices). We present a novel use of PageRank for word sense disambiguation. The key idea is to adapt the matrix initialization step in order to exploit the available contextual evidence.
Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs We present a novel approach to weakly supervised semantic class learning from the web, using a single powerful hyponym pattern combined with graph structures, which capture two properties associated with pattern-based extractions: popularity and productivity. Intuitively, a candidate is popular if it was discovered many times by other instances in the hyponym pattern. A candidate is productive if it frequently leads to the discovery of other instances. Together, these two measures capture not only frequency of occurrence, but also cross-checking that the candidate occurs both near the class name and near other class members. We developed two algorithms that begin with just a class name and one seed instance and then automatically generate a ranked list of new class instances. We conducted experiments on four semantic classes and consistently achieved high accuracies. We introduce a bootstrapping scheme using the doubly-anchored pattern (DAP) that is guided through graph ranking.
Inducing Multilingual Text Analysis Tools Via Robust Projection Across Aligned Corpora This paper describes a system and set of algorithms for automatically inducing stand-alone monolingual part-of-speech taggers, base noun-phrase bracketers, named-entity taggers and morphological analyzers for an arbitrary foreign language. Case studies include French, Chinese, Czech and Spanish. Existing text analysis tools for English are applied to bilingual text corpora and their output projected onto the second language via statistically derived word alignments. Simple direct annotation projection is quite noisy, however, even with optimal alignments. Thus this paper presents noise-robust tagger, bracketer and lemmatizer training procedures capable of accurate system bootstrapping from noisy and incomplete initial projections. Performance of the induced stand-alone part-of-speech tagger applied to French achieves 96% core part-of-speech (POS) tag accuracy, and the corresponding induced noun-phrase bracketer exceeds 91% F-measure. The induced morphological analyzer achieves over 99% lemmatization accuracy on the complete French verbal system. This achievement is particularly noteworthy in that it required absolutely no hand-annotated training data in the given language, and virtually no language-specific knowledge or resources beyond raw text. Performance also significantly exceeds that obtained by direct annotation projection. We perform early work in the cross-lingual projection of part-of-speech tag annotations from English to French and Czech, by way of word-aligned parallel bilingual corpora.
Generalized Inference With Multiple Semantic Role Labeling Systems We present an approach to semantic role labeling (SRL) that takes the output of multiple argument classifiers and combines them into a coherent predicate-argument output by solving an optimization problem. The optimization stage, which is solved via integer linear programming, takes into account both the recommendation of the classifiers and a set of problem specific constraints, and is thus used both to clean the classification results and to ensure structural integrity of the final role labeling. We illustrate a significant improvement in overall SRL performance through this inference. we adopted the outputs of multiple SRL systems (each on a single parse tree) and combined them into a coherent predicate argument output by solving an optimization problem.
Overview of Genia Event Task in BioNLP Shared Task 2011 The Genia event task, a bio-molecular event extraction task, is arranged as one of the main tasks of BioNLP Shared Task 2011. As its second time to be arranged for community-wide focused efforts, it aimed to measure the advance of the community since 2009, and to evaluate generalization of the technology to full text papers. After a 3-month system development period, 15 teams submitted their performance results on test cases. The results show the community has made a significant advancement in terms of both performance improvement and generalization.
Named Entity Transliteration With Comparable Corpora In this paper we investigate Chinese-English name transliteration using comparable corpora, corpora where texts in the two languages deal in some of the same topics - and therefore share references to named entities - but are not translations of each other. We present two distinct methods for transliteration, one approach using phonetic transliteration, and the second using the temporal distribution of candidate pairs. Each of these approaches works quite well, but by combining the approaches one can achieve even better results. We then propose a novel score propagation method that utilizes the co-occurrence of transliteration pairs within document pairs. This propagation method achieves further improvement over the best results from the previous step. We compare names from comparable and contemporaneous English and Chinese texts, scoring matches by training a learning algorithm to compare the phonemic representations of the names in the pair, in addition to taking into account the frequency distribution of the pair over time.
Memory-Based Dependency Parsing This paper reports the results of experiments using memory-based learning to guide a deterministic dependency parser for unrestricted natural language text. Using data from a small treebank of Swedish, memory-based classifiers for predicting the next action of the parser are constructed. The accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parser guide is evaluated by parsing the held-out portion of the treebank. The evaluation shows that memory-based learning gives a signficant improvement over a previous probabilistic model based on maximum conditional likelihood estimation and that the inclusion of lexical features improves the accuracy even further.
Scaling To Very Very Large Corpora For Natural Language Disambiguation The amount of readily available on-line text has reached hundreds of billions of words and continues to grow. Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less. In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used. We are fortunate that for this particular application, correctly labeled training data is free. Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost. We suggest that the development of very large training corpora may be more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora. We show that even using a very simple algorithm, the results continue to improve log-linearly with more training data, even out to a billion words.
An Empirical Study Of Smoothing Techniques For Language Modeling We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods. Our smoothing technique can smooth together the predictions of unigram, bigram, trigram and potentially higher n-gram sequences to obtain accurate probability estimates in the face of data sparsity.
Domain-Specific Sense Distributions And Predominant Sense Acquisition Distributions of the senses of words are often highly skewed. This fact is exploited by word sense disambiguation (WSD) systems which back off to the predominant sense of a word when contextual clues are not strong enough. The domain of a document has a strong influence on the sense distribution of words, but it is not feasible to produce large manually annotated corpora for every domain of interest. In this paper we describe the construction of three sense annotated corpora in different domains for a sample of English words. We apply an existing method for acquiring predominant sense information automatically from raw text, and for our sample demonstrate that (1) acquiring such information automatically from a mixed-domain corpus is more accurate than deriving it from SemCor, and (2) acquiring it automatically from text in the same domain as the target domain performs best by a large margin. We also show that for an all words WSD task this automatic method is best focussed on words that are salient to the domain, and on words with a different acquired predominant sense in that domain compared to that acquired from a balanced corpus. Our dataset is made up of 3 collections of documents: a domain-neutral corpus (BNC), and two domain-specific corpora (SPORTS and FINANCE).
Efficient Parsing For Bilexical Context-Free Grammars And Head Automaton Grammars Several recent stochastic parsers use bilexical grammars, where each word type idiosyncratically prefers particular complements with particular head words. We present O(n4) parsing algorithms for two bilexical formalisms, improving the prior upper bounds of O(n5). For a common special case that was known to allow O(n3) parsing (Eisner, 1997), we present an O(n3) algorithm with an improved grammar constant. We show that the dynamic programming algorithms for lexicalized PCFGs require O(m3) states.
Automatic Discovery Of Non-Compositional Compounds In Parallel Data Automatic segmentation of text into minimal content-bearing units is an unsolved problem even for languages like English. Spaces between words offer an easy first approximation, but this approximation is not good enough for machine translation (MT), where many word sequences are not translated word-for-word. This paper presents an efficient automatic method for discovering sequences of words that are translated as a unit. The method proceeds by comparing pairs of statistical translation models induced from parallel texts in two languages. It can discover hundreds of non-compositional compounds on each iteration, and constructs longer compounds out of shorter ones. Objective evaluation on a simple machine translation task has shown the method's potential to improve the quality of MT output. The method makes few assumptions about the data, so it can be applied to parallel data other than parallel texts, such as word spellings and pronunciations. we propose a method for the recognition of multi word compounds in bi texts that is based on the predictive value of a translation model. we investigates techniques for identifying non-compositional compounds in English-French parallel corpora and emphasises that translation models that take non compositional compounds into account are more accurate.
Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon Even though considerable attention has been given to semantic orientation of words and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons. In this paper, we show how we create a high-quality, moderate-sized emotion lexicon using Mechanical Turk. In addition to questions about emotions evoked by terms, we show how the inclusion of a word choice question can discourage malicious data entry, help identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help obtain annotations at sense level (rather than at word level). We perform an extensive analysis of the annotations to better understand the distribution of emotions evoked by terms of different parts of speech. We identify which emotions tend to be evoked simultaneously by the same term and show that certain emotions indeed go hand in hand. We focus on emotion evoked by common words and phrases. We explore the use of Mechanical Turk to build the lexicon based on human judgment. We create a crowd sourced term emotion association lexicon consisting of associations of over 10,000 word-sense pairs with eight emotions joy, sadness, anger, fear, trust, disgust, surprise, and anticipation argued to be the basic and prototypical emotions.
Discriminative Reranking For Machine Translation This paper describes the application of discriminative reranking techniques to the problem of machine translation. For each sentence in the source language, we obtain from a baseline statistical machine translation system, a ranked n-best list of candidate translations in the target language. We introduce two novel perceptron-inspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-the-art performance in machine translation. We compare different algorithms for tuning the log-linear weights in a re-ranking framework and achieve results comparable to the standard minimum error rate training. We present approaches to re-rank the output of the decoder using syntactic information.
Transformation-Based-Error-Driven Learning And Natural Language Processing: A Case Study In Part-Of-Speech Tagging Recently, there has been a rebirth of empiricism in the field of natural language processing. Manual encoding of linguistic information is being challenged by automated corpus-based learning as a method of providing a natural language processing system with linguistic knowledge. Although corpus-based approaches have been successful in many different areas of natural language processing, it is often the case that these methods capture the linguistic information they are modelling indirectly in large opaque tables of statistics. This can make it difficult to analyze, understand and improve the ability of these approaches to model underlying linguistic behavior. In this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge. This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part-of-speech tagging. We outline a transformation-based learned which learns guessing rules from a pre-tagged training corpus. We propose non-sequential transformation-based learning. We introduce a symbolic machine learning method, a class sequence example Transformation-based learning.
A Stochastic Finite-State Word-Segmentation Algorithm For Chinese The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words. For languages like English one can assume, to a first approximation, that word boundaries are given by whitespace or punctuation. In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to "reconstruct" the word-boundary information. In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer. The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and--since the primary intended application of this model is to text-to-speech synthesis--provides pronunciations for these words. We evaluate the system's performance by comparing its segmentation 'judgments' with the judgments of a pool of human segmenters, and the system is shown to perform quite well. We built a word uni gram model using the Viterbi re-estimation whose initial estimates were derived from the frequencies in the corpus of the strings of each word in the lexicon. We proposed a method to estimate a set of initial word frequencies without segmenting the corpus.
Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not. This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure. We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines. We rank the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models. We apply linear interpolation to combine the instance weighted out-of-domain model with an in-domain model. We propose a method for machine translation that uses features to capture degrees of generality.
Modeling Local Coherence: An Entity-Based Approach This article proposes a novel framework for representing and measuring local coherence. Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text. The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities. We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment. An entity grid is constructed for each document, and is represented as a matrix in which each row represents a sentence, and each column represents an entity. We experiment on two datasets: news articles on the topic of earthquakes (Earthquakes) and narratives on the topic of aviation accidents (Accidents).
Parallel Implementations of Word Alignment Tool Training word alignment models on large corpora is a very time-consuming processes. This paper describes two parallel implementations of GIZA++ that accelerate this word alignment process. One of the implementations runs on computer clusters, the other runs on multi-processor system using multi-threading technology. Results show a near-linear speed-up according to the number of CPUs used, and alignment quality is preserved. We use a multi-threaded version of the GIZA++ tool. This speeds up the process and corrects an error of GIZA++ that can appear with rare words.
The Design Of A Computer Language For Linguistic Information A considerable body of accumulated knowledge about the design of languages for communicating information to computers has been derived from the subfields of programming language design and semantics. It has been the goal of the PART group at SRI to utilize a relevant portion of this knowledge in implementing tools to facilitate communication of linguistic information to computers. The PATR-II formalism is our current computer language for encoding linguistic information. This paper, a brief overview of that formalism, attempts to explicate our design decisions in terms of a set of properties that effective computer languages should incorporate. PATR-II is a minimal constraint-based formalism that extends context-free grammar.
Deterministic Dependency Parsing Of English Text This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time. When trained and evaluated on the Wall Street Journal section of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%. Unlike most previous systems, the parser produces labeled dependency graphs, using as arc labels a combination of bracket labels and grammatical role labels taken from the Penn Treebank II annotation scheme. The best overall accuracy obtained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels). We propose a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear. Our deterministic shift/reduce classifier-based dependency parsing approach offers state-of-the-art accuracy with high efficiency due to a greedy search strategy.
Evaluating The Accuracy Of An Unlexicalized Statistical Parser On The PARC DepBank We evaluate the accuracy of an unlexicalized statistical parser, trained on 4K treebanked sentences from balanced data and tested on the PARC DepBank. We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly tuned without reliance on large in-domain manually-constructed treebanks. This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure. The comparison of systems using DepBank is not straightforward, so we extend and validate DepBank and highlight a number of representation and scoring issues for relational evaluation schemes. We show that the system has equivalent accuracy to the PARC XLE parser when the morphosyntactic features in the original DepBank gold standard are taken into account. We provide annotation for internal NP structure. We recommend looking at accuracy figures by dependency type to understand what a parser is good at. We re annotated DepBank using GRs scheme, and used it to evaluate the RASP parser.
Sentence Compression Beyond Word Deletion In this paper we generalise the sentence compression task. Rather than simply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion. We present a new corpus that is suited to our task and a discriminative tree-to-tree transduction model that can naturally account for structural and lexical mismatches. The model incorporates a novel grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression specific loss functions. Different from prior research, we achieve sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process. We present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. Our abstractive methods sheds more light on how people compress sentences, but do not always manage to outperform extractive methods. We expand the task to include substitutions, insertions and reorderings that are automatically learned from parallel texts. We propose the first abstractive compression method.
Labeled Pseudo-Projective Dependency Parsing With Support Vector Machines We use SVM classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion. Non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser. We present evaluation results and an error analysis focusing on Swedish and Turkish. Our pseudo-projective approach transforms non-projective training trees to projective ones but encode the information necessary to make the inverse transformation in the DEPREL, so that this inverse transformation can also be carried out on the test trees (Nivre et al, 2006).
Concise Integer Linear Programming Formulations for Dependency Parsing We formulate the problem of non-projective dependency parsing as a polynomial-sized integer linear program. Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data. In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearly-projective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods. We introduce the multicommodity flow formulation.
Noun Phrase Coreference As Clustering This paper introduces a new, unsupervised algorithm for noun phrase coreference resolution. It differs from existing methods in that it views coreference resolution as a clustering task. In an evaluation on the MUC-6 coreference resolution corpus, the algorithm achieves an F-measure of 53.6%, placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation. More importantly, the clustering approach outperforms the only MUC-6 system to treat coreference resolution as a learning problem. The clustering algorithm appears to provide a flexible mechanism for coordinating the application of context-independent and context-dependent constraints and preferences for accurate partitioning of noun phrases into coreference equivalence classes. We combine the use of WordNet with proper name gazetteers in order to obtain information on the compatibility of coreferential NPs in their clustering algorithm. Approaches to coreference resolution that rely only on clustering can easily enforce transitivity. We use pairwise NP distances to cluster document mentions. Our system uses the node distance in WordNet (with an upper limit of 4) as one component in the distance measure that guides their clustering algorithm. Coreference resolution is performed in two phases: a binary classification phase, in which the likelihood of coreference for each pair of noun phrases is assessed; and a partitioning phase, in which the clusters of mutually coreferring NPs are formed, maximizing some global criterion.
Mining The Web For Bilingual Text STRAND (Resnik, 1998) is a language-independent system for automatic discovery of text in parallel translation on the World Wide Web. This paper extends the preliminary STRAND results by adding automatic language identification, scaling up by orders of magnitude, and formally evaluating performance. The most recent end-product is an automatically acquired parallel corpus comprising 2491 English-French document pairs, approximately 1.5 million words per language. We use structure markup information from pages without looking at their content to attempt to align them.
A Topic Model for Word Sense Disambiguation We develop latent Dirichlet allocation with WORDNET (LDAWN), an unsupervised probabilistic topic model that includes word sense as a hidden variable. We develop a probabilistic posterior inference algorithm for simultaneously disambiguating a corpus and learning the domains in which to consider each word. Using the WORDNET hierarchy, we embed the construction of Abney and Light (1999) in the topic model and show that automatically learned domains improve WSD accuracy compared to alternative contexts. We use the document-level topics extracted with Latent Dirichlet Allocation (LDA) as indicators of meanings for word sense disambiguation. We describe a related topic model, LDAWN, for word sense disambiguation that adds an intermediate layer of latent variables Z on which the Markov model parameters are conditioned. We integrate semantics into the topic model framework.
The Third PASCAL Recognizing Textual Entailment Challenge This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems. In creating this year’s dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios. Additionally, a pool of resources was offered so that the participants could share common tools. A pilot task was also set up, aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions. 26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges. The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al,2007). Textual Entailment (TE) has become a prominent paradigm for modeling semantic inference, capturing the needs of a broad range of text understanding applications.
Nonconcatenative Finite-State Morphology Instead of modeling morphology along the more traditional finite-state transducer, we suggest modeling it with a n-tape automaton, where tapes would carry precisely this interleaving that is called for in Semitic interdigitation. We propose a framework with which each of the auto segmental tiers is assigned a tape in a multi-tape finite state machine, with an additional tape for the surface form.
Towards History-Based Grammars: Using Richer Models For Probabilistic Parsing We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Tree-bank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error. We present history-based parsing, using features of the parsing history to predict the next parser action.
